2020.blackboxnlp-1.8,P18-2124,0,0.0902065,"Missing"
2020.blackboxnlp-1.8,2020.emnlp-main.442,1,0.872458,"Missing"
2020.blackboxnlp-1.8,N19-1423,0,0.0628855,"Missing"
2020.blackboxnlp-1.8,W18-5446,0,0.0748859,"Missing"
2020.blackboxnlp-1.8,W12-2601,0,0.0875554,"Missing"
2020.blackboxnlp-1.8,P02-1040,0,0.110996,"Missing"
2020.emnlp-main.368,N15-1151,0,0.0296389,"ss-Lingual NLU: Cross-lingual learning has a fairly short history in NLP, and has mainly been restricted to traditional NLP tasks, such as PoS tagging, morphological inflection and parsing. In contrast to these tasks, which have seen much cross-lingual attention (Plank et al., 2016; Bjerva, 2017; Kementchedjhieva et al., 2018; de Lhoneux et al., 2018), there has been relatively little work on cross-lingual NLU, partly due to lack of benchmark datasets. Existing work has mainly been focused on NLI (Agic and Schluter, 2018; Conneau et al., 2018; Zhao et al., 2020), and to a lesser degree on RE (Faruqui and Kumar, 2015; Verga et al., 2016) and QA (Abdou et al., 2019; Lewis et al., 2020). Previous research generally reports that cross-lingual learning is challenging and that it is hard to beat a machine translation baseline (e.g., Conneau et al. (2018)). Such a baseline is for instance suggested by Faruqui and Kumar (2015), where the text in the target language is automatically translated to English. We achieve competitive performance compared to a machine translation baseline (for XNLI), and propose a method that requires no training instances for the target task in the target language. Furthermore, our met"
2020.emnlp-main.368,2020.acl-main.747,0,0.148164,"Missing"
2020.emnlp-main.368,D18-1398,0,0.424643,"nd apply it directly to another language where only limited training data is available (i.e., low-resource languages). Therefore, it is essential to investigate strategies that allow one to use the large amount of training data available for English for the benefit of other languages. Meta-learning has recently been shown to be beneficial for several machine learning tasks (Koch et al., 2015; Vinyals et al., 2016; Santoro et al., 2016; Finn et al., 2017; Ravi and Larochelle, 2017; Nichol et al., 2018). For NLP, recent work has also shown the benefits of this sharing between tasks and domains (Gu et al., 2018; Dou et al., 2019; Qian and Yu, 2019). Although cross-lingual transfer with meta-learning has been investigated for machine translation (Gu et al., 2018), this paper – to best of our knowledge – is the first attempt to study meta-learning for cross-lingual natural language understanding. Our contributions are as follows: Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourc"
2020.emnlp-main.368,D18-1269,0,0.533938,"is beneficial. 1 • We propose X-MAML1 , a cross-lingual metalearning architecture, and study it for two natural language understanding tasks (Natural Language Inference and Question Answering); Introduction There are more than 7,000 languages spoken in the world, over 90 of which have more than 10 million native speakers each (Eberhard et al., 2019). Despite this, very few languages have proper linguistic resources when it comes to natural language understanding tasks (Joshi et al., 2020). Although there is growing awareness in the field, as evidenced by the release of datasets such as XNLI (Conneau et al., 2018), most NLP research still only considers English (Bender, 2019). While one solution to • We test X-MAML on cross-domain, crosslingual, standard supervised, few-shot as well as zero-shot learning, across a total of 15 languages; • We observe consistent improvements over strong models including Multilingual BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020); 1 Our code is available at https://github.com/ copenlu/X-MAML 4547 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4547–4562, c November 16–20, 2020. 2020 Association for Computati"
2020.emnlp-main.368,N19-1423,0,0.576193,"hard et al., 2019). Despite this, very few languages have proper linguistic resources when it comes to natural language understanding tasks (Joshi et al., 2020). Although there is growing awareness in the field, as evidenced by the release of datasets such as XNLI (Conneau et al., 2018), most NLP research still only considers English (Bender, 2019). While one solution to • We test X-MAML on cross-domain, crosslingual, standard supervised, few-shot as well as zero-shot learning, across a total of 15 languages; • We observe consistent improvements over strong models including Multilingual BERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020); 1 Our code is available at https://github.com/ copenlu/X-MAML 4547 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4547–4562, c November 16–20, 2020. 2020 Association for Computational Linguistics • We perform an extensive error analysis, which reveals that cross-lingual trends can partly be explained by typological commonalities between languages. 2 Meta-Learning Meta-learning tries to tackle the problem of fast adaptation to a handful of new training data instances. It discovers the structure among multipl"
2020.emnlp-main.368,D19-1112,0,0.0776258,"Missing"
2020.emnlp-main.368,D16-1264,0,\N,Missing
2020.emnlp-main.368,W18-5446,0,\N,Missing
2020.emnlp-main.368,D18-1543,1,\N,Missing
2020.emnlp-main.368,D18-1456,0,\N,Missing
2020.emnlp-main.368,D19-1077,0,\N,Missing
2020.emnlp-main.368,D18-1514,0,\N,Missing
2020.emnlp-main.368,N18-1101,0,\N,Missing
2020.emnlp-main.368,N16-1103,0,\N,Missing
2020.emnlp-main.368,2020.emnlp-main.484,0,\N,Missing
2020.emnlp-main.368,W18-0207,1,\N,Missing
2020.emnlp-main.442,C10-1004,0,0.0284291,"were quite impressive. Figure 1: Our data collection pipeline able for verification or objective observation. It is this type of state which is referred to as subjectivity (Banfield, 1982; Banea et al., 2011). Whereas subjectivity has been investigated in isolation, it can be argued that subjectivity is only meaningful given sufficient context. Regardless, much previous work has focused on annotating words (Heise, 2001), word senses (Durkin and Manning, 1989; Wiebe and Mihalcea, 2006; Akkaya et al., 2009), or sentences (Pang and Lee, 2004), with notable exceptions such as Wiebe et al. (2005); Banea et al. (2010), who investigate subjectivity in phrases in the context of a text or conversation. There is limited work that studies subjectivity using architectures that allow for contexts to be incorporated efficiently (Vaswani et al., 2017). As subjectivity relies heavily on context, and we have access to methods which can encode such context, what then of access to data which encodes subjectivity? We argue that in order to fully investigate research questions dealing with subjectivity in contexts, a large-scale dataset is needed. We choose to frame this as a QA dataset, as it not only offers the potenti"
2020.emnlp-main.442,P17-1147,0,0.0947055,"Missing"
2020.emnlp-main.442,2020.acl-main.653,0,0.0722396,"Missing"
2020.emnlp-main.442,D18-1543,1,0.899108,"Missing"
2020.emnlp-main.442,2020.blackboxnlp-1.8,1,0.761925,"argeted at understanding subjectivity. This can be attributed to how these datasets are constructed. Large-scale QA datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), CoQA (Reddy et al., 2019), MLQA (Lewis et al., 2020) are based on factual data. We are the first to attempt to create a review-based QA dataset for the purpose of understanding subjectivity. Recent work has corroborated our findings on the benefits of modelling subjectivity QA, and highlights the differences in the distributions of hidden representation between S UBJ QA and the factual SQuAD data (Muttenthaler et al., 2020). 7 Conclusion In this paper we investigate subjectivity in QA by leveraging end-to-end architectures. We release S UBJ QA, a question-answering dataset which contains subjectivity labels for both questions and answers. The dataset allows i) evaluation and development of architectures for subjective content, and ii) investigation of subjectivity and its interactions in broad and diverse contexts. We further implement a subjectivity-aware model and evaluate it, along with 4 strong baseline models. We hope this dataset opens new avenues for research on querying subjective content, and into subje"
2020.emnlp-main.442,P04-1035,0,0.547843,"iews, containing subjectivity annotations for questions and answer spans across 6 domains. 1 Introduction Subjectivity is ubiquitous in our use of language (Banfield, 1982; Quirk et al., 1985; Wiebe et al., 1999; Benamara et al., 2017), and is therefore an important aspect to consider in Natural Language Processing (NLP). For example, subjectivity can be associated with different senses of the same word. BOILING is objective in the context of hot water, but subjective in the context of a person boiling with anger (Wiebe and Mihalcea, 2006). The same applies to sentences in discourse contexts (Pang and Lee, 2004). While early work has shown subjectivity to be an important feature for low-level tasks such as word-sense disambiguation and sentiment analysis, subjectivity in NLP has not been explored in many contexts where it is prevalent. ∗ JB and NB contributed equally to this work. In recent years, there has been renewed interest in areas of NLP for which subjectivity is important, and a specific topic of interest is question answering (QA). This includes work on aspect extraction (Poria et al., 2016), opinion mining (Sun et al., 2017) and community QA (Gupta et al., 2019). Many QA systems are based o"
2020.emnlp-main.442,P18-2124,0,0.0970882,"Missing"
2020.emnlp-main.442,Q19-1016,0,0.132296,"ained traction recently with the availability of new datasets and architectures (Grail and Perez, 2018; Gupta et al., 2019; Fan et al., 2019; Xu et al., 2019b; Li et al., 2019), these are agnostic with respect to how subjectivity is expressed in the questions and the reviews. Furthermore, the datasets are either very small (< 2000 questions) or have target-specific question types (e.g., yes-no). Most QA datasets and systems focus on answering questions over factual data such as Wikipedia articles and News (Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018; Abdou et al., 2019; Reddy et al., 2019). In this work, on the other hand, we focus on QA over subjective data from reviews on product and service websites. In this work, we investigate the relation between subjectivity and question answering (QA) in the context of customer reviews. As no such QA 5480 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5480–5494, c November 16–20, 2020. 2020 Association for Computational Linguistics Contributions (i) We release a challenging QA dataset with subjectivity labels for questions and answers, spanning 6 domains; (ii) We investigate the relationshi"
2020.emnlp-main.442,N13-1008,0,0.0188715,"/www.yelp.com/dataset Given a review corpus, we extract opinions about various aspects of the items being reviewed (Opinion Extraction). Consider the following review snippets and extractions. Example 2 Review: [...] character development was quite impressive. e1 :‹‘impressive’, ‘character development’› Review: 3 stars for good power and good writing. e2 :‹‘good’, ‘writing’› In the next (Neighborhood Model Construction) step, we characterize the items being reviewed and their subjective extractions using latent features between two items. In particular, we use matrix factorization techniques (Riedel et al., 2013) to construct a neighborhood model N via a set of weights we,e0 , where each corresponds to a directed association strength between extraction e and e0 . For instance, e1 and e2 in Example 2 could have a similarity score 0.93. This neighborhood model forms the core of data collection. We select a subset of extractions from N as topics (Topic Selection) and ask crowd workers to translate them to natural language questions (Question Generation). For each topic, a subset of its neighbors from N and reviews which mention them are selected (Review Selection). In this manner, question-review pairs a"
2020.emnlp-main.442,W17-2623,0,0.310605,"e highly subjective as well. Although QA over customer reviews has gained traction recently with the availability of new datasets and architectures (Grail and Perez, 2018; Gupta et al., 2019; Fan et al., 2019; Xu et al., 2019b; Li et al., 2019), these are agnostic with respect to how subjectivity is expressed in the questions and the reviews. Furthermore, the datasets are either very small (< 2000 questions) or have target-specific question types (e.g., yes-no). Most QA datasets and systems focus on answering questions over factual data such as Wikipedia articles and News (Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018; Abdou et al., 2019; Reddy et al., 2019). In this work, on the other hand, we focus on QA over subjective data from reviews on product and service websites. In this work, we investigate the relation between subjectivity and question answering (QA) in the context of customer reviews. As no such QA 5480 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5480–5494, c November 16–20, 2020. 2020 Association for Computational Linguistics Contributions (i) We release a challenging QA dataset with subjectivity labels for questions and"
2020.emnlp-main.442,P18-4005,1,0.876594,"Missing"
2020.emnlp-main.442,P06-1134,0,0.209631,"ective answer. We release an English QA dataset (S UBJ QA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 domains. 1 Introduction Subjectivity is ubiquitous in our use of language (Banfield, 1982; Quirk et al., 1985; Wiebe et al., 1999; Benamara et al., 2017), and is therefore an important aspect to consider in Natural Language Processing (NLP). For example, subjectivity can be associated with different senses of the same word. BOILING is objective in the context of hot water, but subjective in the context of a person boiling with anger (Wiebe and Mihalcea, 2006). The same applies to sentences in discourse contexts (Pang and Lee, 2004). While early work has shown subjectivity to be an important feature for low-level tasks such as word-sense disambiguation and sentiment analysis, subjectivity in NLP has not been explored in many contexts where it is prevalent. ∗ JB and NB contributed equally to this work. In recent years, there has been renewed interest in areas of NLP for which subjectivity is important, and a specific topic of interest is question answering (QA). This includes work on aspect extraction (Poria et al., 2016), opinion mining (Sun et al."
2020.emnlp-main.442,P99-1032,0,0.642808,"Missing"
2020.emnlp-main.442,N19-1242,0,0.0487657,"Missing"
2020.emnlp-main.442,D12-1036,0,0.0299843,"7.05 0.40 -0.48 7.37 11.07 8.28 9.68 7.24 3.41 1.01 5.71 3.08 4.06 -0.01 -1.57 7.42 8.65 5.84 6.67 7.26 2.20 Average 13.35 15.40 3.69 7.93 14.27 16.19 3.63 7.84 2.05 6.34 Table 9: MTL gains/losses over the fine-tuning condition (F1 and Exact match), across subj./fact. QA. mining (Blair-Goldensohn et al., 2008), with a focus on text polarity. There is renewed intereste in incorporating subjective opinion data into general data management systems (Li et al., 2019; Kobren et al., 2019) and for querying subjective data. In this work, we revisit subjectivity in the context of review QA. Yu et al. (2012); McAuley and Yang (2016b) also use review data, as they leverage question types and aspects to answer questions. However, no prior work has modeled subjectivity explicitly using end-to-end architectures. Furthermore, none of the existing reviewbased QA datasets are targeted at understanding subjectivity. This can be attributed to how these datasets are constructed. Large-scale QA datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), CoQA (Reddy et al., 2019), MLQA (Lewis et al., 2020) are based on factual data. We are the first to attempt to create a review-based"
2020.emnlp-main.442,J17-1006,0,\N,Missing
2020.sigtyp-1.1,Q19-1038,0,0.0255088,"o Edoardo M. Ponti‡ Ekaterina Vylomova] Ryan Cotterell? Isabelle Augenstein/ • Aalborg University / University of Copenhagen  Johns Hopkins University ] University of Melbourne 2 Carnegie Mellon University Leipzig University ? ETH Z¨urich ‡ University of Cambridge jbjerva@cs.aau.dk, augenstein@di.ku.dk Abstract of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words (Ammar et al., 2016; Wada et al., 2019) or, more recently, sentences (Artetxe and Schwenk, 2019; Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by"
2020.sigtyp-1.1,N18-1083,1,0.844133,"l., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typolog"
2020.sigtyp-1.1,W18-0207,1,0.747207,"l., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typolog"
2020.sigtyp-1.1,N19-1156,1,0.642604,"al similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful f"
2020.sigtyp-1.1,P19-1382,1,0.844402,"al similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful f"
2020.sigtyp-1.1,P07-1009,0,0.847176,"Missing"
2020.sigtyp-1.1,C16-1298,0,0.0351469,"Missing"
2020.sigtyp-1.1,N19-1423,0,0.0121619,"na Vylomova] Ryan Cotterell? Isabelle Augenstein/ • Aalborg University / University of Copenhagen  Johns Hopkins University ] University of Melbourne 2 Carnegie Mellon University Leipzig University ? ETH Z¨urich ‡ University of Cambridge jbjerva@cs.aau.dk, augenstein@di.ku.dk Abstract of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words (Ammar et al., 2016; Wada et al., 2019) or, more recently, sentences (Artetxe and Schwenk, 2019; Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva a"
2020.sigtyp-1.1,J19-2006,1,0.848772,"al similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful f"
2020.sigtyp-1.1,W19-4208,1,0.845374,"ographic proximity is a challenging one. We expect that further exploration of unconstrained systems to have the most potential for predicting features in such cases, where little or nothing is known about a language. 7 Typologically Informed Sharing Cross-lingual sharing informed by typology has been investigated for, among others, parsing (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; de Lhoneux et al., 2018), language modeling (Tsvetkov et al., 2016; Ponti et al., 2019b), machine translation (Daiber et al., 2016; Ponti et al., 2018), and morphological inflection (Chaudhary et al., 2019). Many of these approaches use language embeddings with sparse features encoding WALS feature values. Oncevay et al. (2020) find that combining information from typological databases with embeddings learned during training of an NMT model can be beneficial for multilingual NMT. 6.3 Conclusions Acknowledgments This research has received funding from the Swedish Research Council under grant agreement No 2019-04129, as well as the German Research Foundation (DFG project number 408121292). Typological Probing Several recent papers study typological feature prediction as a probing task for evaluati"
2020.sigtyp-1.1,D18-1029,1,0.907351,"Missing"
2020.sigtyp-1.1,2020.sigtyp-1.3,0,0.0348564,"ure values for non-ancestral languages can be inferred individually by rerooting the tree to a related language. NUIG (Choudhary (2020), NUI Galway) submitted a constrained system with independent classifiers to predict each WALS feature. The outputs of independent classifiers are then fed into a shared encoder with feed-forward and self-attention layers in order to make use of feature correlations. Their model does not use other known features for WALS feature prediction at inference time, relying only on the 5-dimensional inputs of longitude, latitude, genus, family, and country-code. NEMO (Gutkin and Sproat (2020), Google London and Tokyo) submitted constrained systems which first computed probabilities of represented feature values across each language’s genetic (genus and family), and areal (features from languages within a 2,500 kilometre radius, computed from provided latitude and longitude with the Haversine formula), and implicational universals or rather, priors for certain features given commonly associated feature-value pairs in the data. Figure 2: Macro-averaged rankings of all submissions They compared several classifiers’ performance using these sparse features, ultimately submitting system"
2020.sigtyp-1.1,2020.acl-main.747,0,0.0779042,"Missing"
2020.sigtyp-1.1,2020.sigtyp-1.5,0,0.0676552,"Missing"
2020.sigtyp-1.1,2020.sigtyp-1.2,0,0.0945652,"Missing"
2020.sigtyp-1.1,N18-2085,1,0.860078,"Missing"
2020.sigtyp-1.1,D18-1543,1,0.916053,"Missing"
2020.sigtyp-1.1,E17-2002,0,0.0174388,"m 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known. 1 Introduction Linguistic typology is the study of structural properties of languages (Comrie, 1988; Croft, 2002; Velupillai, 2012). Approaches to the categorisation of the languages of the world according to their linguistic properties are represented by, e.g., typological features in databases such as WALS (Dryer and Haspelmath, 2013), URIEL (Littell et al., 2017), and AUTOTYP (Nichols et al., 2013), e.g. in terms of their syntax, morphology, and phonology. One example of such a typological feature is the basic word order feature in WALS. For instance, English is best described as a subject-verb-object (SVO) language, whereas Japanese is best described as a subject-object-verb (SOV) language. Once a relatively niche topic in the NLP community, studying typological features has recently risen in popularity and importance for a number 1 Proceedings of the Second Workshop on Computational Research in Linguistic Typology, pages 1–11 c Online, November 19,"
2020.sigtyp-1.1,P18-1142,1,0.887137,"Missing"
2020.sigtyp-1.1,D17-1268,0,0.394103,"t causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic prope"
2020.sigtyp-1.1,D19-1288,1,0.883931,"Missing"
2020.sigtyp-1.1,I17-1046,0,0.0877507,"gly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Campbell, 2007; Bjerva et al., 2019b). The latter stream of work has provided the inspiration for this shared task on typological feature prediction. As knowledge bases are notoriously incomplete and require manual labour from (in this case, linguistic) domain experts to create, populate and maintain, high-performance methods for autoTypological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the wor"
2020.sigtyp-1.1,N13-1126,0,0.0591155,"Missing"
2020.sigtyp-1.1,P12-1066,0,0.170714,"Missing"
2020.sigtyp-1.1,N16-1161,0,0.0680928,"Missing"
2020.sigtyp-1.1,2020.emnlp-main.368,1,0.920027,"2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and to what degree typological similarities are exploited by such models (Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zhao et al., 2020). In addition to using typology for diagnostic purposes, prior work has also found that typology can guide cross-lingual sharing (de Lhoneux et al., 2018). Finally, the relationship between typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) and language representations has been studied, which has shown that knowledge base population methods can be used to complete typological KBs (Malaviya et al., 2017; Murawaki, 2017; Bjerva and Augenstein, 2018a; Bjerva et al., 2019c), and that implications can be discovered in typological KBs (Daum´e III and Camp"
2020.sigtyp-1.1,2020.emnlp-main.187,0,0.0638701,"al for predicting features in such cases, where little or nothing is known about a language. 7 Typologically Informed Sharing Cross-lingual sharing informed by typology has been investigated for, among others, parsing (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; de Lhoneux et al., 2018), language modeling (Tsvetkov et al., 2016; Ponti et al., 2019b), machine translation (Daiber et al., 2016; Ponti et al., 2018), and morphological inflection (Chaudhary et al., 2019). Many of these approaches use language embeddings with sparse features encoding WALS feature values. Oncevay et al. (2020) find that combining information from typological databases with embeddings learned during training of an NMT model can be beneficial for multilingual NMT. 6.3 Conclusions Acknowledgments This research has received funding from the Swedish Research Council under grant agreement No 2019-04129, as well as the German Research Foundation (DFG project number 408121292). Typological Probing Several recent papers study typological feature prediction as a probing task for evaluating cross-lingual sentence encoders (Choenni and Shutova, 2020; Bjerva and Augenstein, 2018a; Nooralahzadeh et al., 2020; Zh"
2020.sigtyp-1.1,P19-1300,0,0.0206958,"J. Mielke Aditi Chaudhary2 Giuseppe G. A. Celano Edoardo M. Ponti‡ Ekaterina Vylomova] Ryan Cotterell? Isabelle Augenstein/ • Aalborg University / University of Copenhagen  Johns Hopkins University ] University of Melbourne 2 Carnegie Mellon University Leipzig University ? ETH Z¨urich ‡ University of Cambridge jbjerva@cs.aau.dk, augenstein@di.ku.dk Abstract of reasons. The field has seen considerable advances in cross-lingual transfer learning, whereby stable cross-lingual representations can be learned on massive amounts of data in an unsupervised way, be it for words (Ammar et al., 2016; Wada et al., 2019) or, more recently, sentences (Artetxe and Schwenk, 2019; Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). This naturally raises the question of what these representations encode, and some have turned to typology for potential answers (Choenni and Shutova, 2020; Zhao et al., 2020). In a similar vein, research has shown that these learned representations can be fine-tuned for supervised tasks, then applied to new languages in a few- or even zero-shot fashion with surprisingly high performance. This has raised the question of what causes such surprisingly high results, and t"
2020.sigtyp-1.1,E17-2102,0,0.458015,"t with 5k samples each. 3.2 Baselines We provide two baselines. The first is a simple lower-bound baseline based on observing feature frequencies in WALS (Baseline frequency in Figure 2). For each unobserved feature in the test set, we predict the most frequent feature value from the training set. The second uses the k-nearest neighbours (kNN) algorithm with a simple feature set to predict each unobserved feature, with k = 1 (Baseline knn-imputation in Figure 2). Each language is represented by a language vector (~l ∈ R64 ) trained as a part of a multilingual character-based language ¨ model (Ostling and Tiedemann, 2017). During inference, for a language l and unobserved feature y, we find the nearest neighbour to ~l for which y has been observed, similar to Bjerva and Augenstein (2018a,b). 1 3 Distances calculated with WALS language locations. 3.3 Submissions We received eight submissions from five teams across the constrained and unconstrained subtasks, as described below. ´ (Vastl et al. (2020), Charles University) UFAL submitted a constrained system which ensembled two approaches: first, estimating the correlation of feature values within languages enables missing feature prediction, and second, using a n"
2020.sigtyp-1.1,D15-1213,0,0.0604974,"Missing"
2021.eacl-main.38,N18-1083,1,0.930305,"Ostling and Tiedemann (2017); Ponti et al. (2019); Oncevay et al. (2020)). Notably, limited effects are usually observed from including typological features explicitly. For instance, de Lhoneux et al. (2018) observe positive cross-lingual sharing effects only in a handful of their settings. We therefore hypothesise that relevant typological information is learned as a by-product of cross-lingual training. Hence, although models do benefit from this information, it is not necessary to provide it explicitly in a high-resource scenario, where there is abundant training data. This is confirmed by Bjerva and Augenstein (2018a), who find that, e.g., language embeddings trained on a morphological task can encode morphological features from WALS. In contrast with previous work, we blind a model to typological information, by using adversarial 480 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 480–486 April 19 - 23, 2021. ©2021 Association for Computational Linguistics techniques based on gradient reversal (Ganin and Lempitsky, 2014). We evaluate on the structured prediction and classification tasks in XTREME (Hu et al., 2020), yielding a total of 40"
2021.eacl-main.38,W18-0207,1,0.942532,"Ostling and Tiedemann (2017); Ponti et al. (2019); Oncevay et al. (2020)). Notably, limited effects are usually observed from including typological features explicitly. For instance, de Lhoneux et al. (2018) observe positive cross-lingual sharing effects only in a handful of their settings. We therefore hypothesise that relevant typological information is learned as a by-product of cross-lingual training. Hence, although models do benefit from this information, it is not necessary to provide it explicitly in a high-resource scenario, where there is abundant training data. This is confirmed by Bjerva and Augenstein (2018a), who find that, e.g., language embeddings trained on a morphological task can encode morphological features from WALS. In contrast with previous work, we blind a model to typological information, by using adversarial 480 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 480–486 April 19 - 23, 2021. ©2021 Association for Computational Linguistics techniques based on gradient reversal (Ganin and Lempitsky, 2014). We evaluate on the structured prediction and classification tasks in XTREME (Hu et al., 2020), yielding a total of 40"
2021.eacl-main.38,N19-1156,1,0.928317,"the POS tagging task will suffer under this condition, whereas other tasks only to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020)."
2021.eacl-main.38,P19-1382,1,0.863597,"the POS tagging task will suffer under this condition, whereas other tasks only to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020)."
2021.eacl-main.38,2020.sigtyp-1.1,1,0.673001,"y to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020). In contrast to such post-hoc approaches, our experimental setting allows for me"
2021.eacl-main.38,J19-2006,1,0.908887,"the POS tagging task will suffer under this condition, whereas other tasks only to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020)."
2021.eacl-main.38,2020.acl-main.493,0,0.0236422,"op in performance when blinding a model to syntactic features, but we also observe that the α sharing weights in our model do not appear to correlate with linguistic similarities in this setting. Conversely, encouraging a model to consider typology, by jointly optimising it for typological feature prediction, improves performance in general. Furthermore, α weights in this scenario converge towards correlating with structural similarities of languages. This is in line with recent work which has found that m-BERT uses fine-grained syntactic distinctions in its crosslingual representation space (Chi et al., 2020). We interpret this as evidence for the fact that typology can be a necessity for modelling in NLP. Our results furthermore corroborate previous work in that we only find moderate benefits from including typological information explicitly. We expect that this to a large degree is due to the typological similarities of languages being encoded implicitly based on correlations between patterns in the input data. As low-resource languages often do not even have access to any substantial amount of raw text, but often do have annotations in WALS, we expect that using typological information can go s"
2021.eacl-main.38,P07-1009,0,0.150682,"Missing"
2021.eacl-main.38,N19-1423,0,0.0199481,"l from exploiting typology severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to typology somewhat improves performance. 1 Figure 1: A PoS tagger is exposed (or blinded with gradient reversal, −λ) to typological features. Observing α values tells us how typology affects sharing. Introduction Most languages in the world have little access to NLP technology due to data scarcity (Joshi et al., 2020). Nonetheless, high-quality multilingual representations can be obtained using only a raw text signal, e.g. via multilingual language modelling (Devlin et al., 2019). Furthermore, structural similarities of languages are to a large extent documented in typological databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath (2013)). Hence, developing models which can take use typological similarities of languages is an important direction in order to alleviate language technology inequalities. While previous work has attempted to use typological information to inform NLP models, our work differs significantly from such efforts in that we blind a model to this information. Most previous work includes language information as features"
2021.eacl-main.38,D18-1029,0,0.199366,"Missing"
2021.eacl-main.38,2020.acl-main.560,0,0.011474,"on a cross-lingual architecture in which the latent weights governing the sharing between languages is learnt during training. We show that (i) preventing this model from exploiting typology severely reduces performance, while a control experiment reaffirms that (ii) encouraging sharing according to typology somewhat improves performance. 1 Figure 1: A PoS tagger is exposed (or blinded with gradient reversal, −λ) to typological features. Observing α values tells us how typology affects sharing. Introduction Most languages in the world have little access to NLP technology due to data scarcity (Joshi et al., 2020). Nonetheless, high-quality multilingual representations can be obtained using only a raw text signal, e.g. via multilingual language modelling (Devlin et al., 2019). Furthermore, structural similarities of languages are to a large extent documented in typological databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath (2013)). Hence, developing models which can take use typological similarities of languages is an important direction in order to alleviate language technology inequalities. While previous work has attempted to use typological information to inform NL"
2021.eacl-main.38,D18-1543,1,0.903419,"Missing"
2021.eacl-main.38,D17-1268,0,0.0180311,"ypological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020). In contrast to such post-hoc approaches, our experimental setting allows for measuring the impact of typology on crosslingual sharing performance in a direct manner as part of the model architecture. Syntactic Features We first blind/expose the model to syntactic features from WALS (Dryer and Haspelmath, 2013). We take the set of word order features which are annotated for all languages in our experiments, resulting in 33 features. This includes features such as 81A: Order of Subject, Object and Ver"
2021.eacl-main.38,I17-1046,0,0.0196742,"fixing morphology. We hypothesise that mainly the POS tagging task will suffer under this condition, whereas other tasks only to some extent require morphology. Typological Prediction and Blinding We first investigate whether prohibiting or allowing access to typological features has an effect on model performance using our architecture. We hypothesise that our multilingual model will leverage signals related to the linguistic nature of a task when optimising its its sharing parameters α. There exists a growing body of work on prediction of typological features (Daum´e III and Campbell, 2007; Murawaki, 2017; Bjerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 201"
2021.eacl-main.38,2020.emnlp-main.368,1,0.834918,"jerva and Augenstein, 2018b; Bjerva et al., 2019a,b), most notably in a recent shared task on the subject (Bjerva et al., 2020). While we are inspired by this direction of research, our contribution is not concerned with the accuracy of the prediction of such features, and this is therefore not evaluated in detail in the paper. Moreover, an increasing amount of work measures the correlation of predictive performance of cross-lingual models with typological features as a way of probing what a model has learned about typology (Malaviya et al., 2017; Choenni and Shutova, 2020; Gerz et al., 2018; Nooralahzadeh et al., 2020; Zhao et al., 2020). In contrast to such post-hoc approaches, our experimental setting allows for measuring the impact of typology on crosslingual sharing performance in a direct manner as part of the model architecture. Syntactic Features We first blind/expose the model to syntactic features from WALS (Dryer and Haspelmath, 2013). We take the set of word order features which are annotated for all languages in our experiments, resulting in 33 features. This includes features such as 81A: Order of Subject, Object and Verb, which encodes what the preferred word ordering is (if any) in a transit"
2021.eacl-main.38,C16-1123,0,0.354646,"Missing"
2021.eacl-main.38,2020.emnlp-main.187,0,0.0108029,"tures (WALS, Dryer and Haspelmath (2013)). Hence, developing models which can take use typological similarities of languages is an important direction in order to alleviate language technology inequalities. While previous work has attempted to use typological information to inform NLP models, our work differs significantly from such efforts in that we blind a model to this information. Most previous work includes language information as features, by using language IDs, or language embeddings (e.g. Ammar et al. (2016); O’Horan et al. (2016); ¨ Ostling and Tiedemann (2017); Ponti et al. (2019); Oncevay et al. (2020)). Notably, limited effects are usually observed from including typological features explicitly. For instance, de Lhoneux et al. (2018) observe positive cross-lingual sharing effects only in a handful of their settings. We therefore hypothesise that relevant typological information is learned as a by-product of cross-lingual training. Hence, although models do benefit from this information, it is not necessary to provide it explicitly in a high-resource scenario, where there is abundant training data. This is confirmed by Bjerva and Augenstein (2018a), who find that, e.g., language embeddings"
2021.eacl-main.38,E17-2102,0,0.156223,"databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath (2013)). Hence, developing models which can take use typological similarities of languages is an important direction in order to alleviate language technology inequalities. While previous work has attempted to use typological information to inform NLP models, our work differs significantly from such efforts in that we blind a model to this information. Most previous work includes language information as features, by using language IDs, or language embeddings (e.g. Ammar et al. (2016); O’Horan et al. (2016); ¨ Ostling and Tiedemann (2017); Ponti et al. (2019); Oncevay et al. (2020)). Notably, limited effects are usually observed from including typological features explicitly. For instance, de Lhoneux et al. (2018) observe positive cross-lingual sharing effects only in a handful of their settings. We therefore hypothesise that relevant typological information is learned as a by-product of cross-lingual training. Hence, although models do benefit from this information, it is not necessary to provide it explicitly in a high-resource scenario, where there is abundant training data. This is confirmed by Bjerva and Augenstein (2018a"
2021.eacl-main.38,J19-3005,0,0.155697,"Missing"
2021.starsem-1.22,P19-1310,0,0.0562086,"Missing"
2021.starsem-1.22,N19-1391,0,0.0198998,"ν : L(C ν , fΘ ) X = X kfΘ (i, s) − fΘ (j, t))k22 (s,t)∈C ν (i,j)∈a(s,t) (1) where Θ are the parameters of the encoder f . As in Cao et al. (2020), we use a regularization term to avoid for the resulting (re-aligned) embeddings to drift too far away from the initial encoder state f0 : 231 R(C ν , fΘ ) = X len(t) X kfΘ (i, t) − f0 (i, t)k22 t∈C ν i=1 (2) Like for the multilingual pre-training of m-BERT and XLM-R, we fine-tune the encoder f on the concatenation of k parallel corpora to handle resourcelean languages, which is in contrast to offline alignment with language-independent rotations (Aldarmaki and Diab, 2019; Schuster et al., 2019). Assume that English is a common pivot (source language) in all our k parallel corpora. Then the following objective function orients all non-English embeddings toward English: min Θ k X L(C ν , fΘ ) + R(C ν , fΘ ) (3) ν=1 In §5, we refer to the above described realignment step as J OINT-A LIGN. 3.2 Vector space normalization We add a batch normalization layer that constrains all embeddings of different languages into a distribution with zero mean and unit variance: f (i, s) − µβ f¯(i, s) = q σβ2 +  (4) where  is a constant value for numerical stability, µβ and σβ ar"
2021.starsem-1.22,P17-1042,0,0.0327956,"ing tasks; (ii) normalizing vector spaces is surprisingly effective, rivals much more resource-intensive methods such as remapping, and leads to more consistent gains; (iii) all three techniques—vector space normalization, re-mapping and input normalization—are orthogonal and their gains often stack. This is a very important finding as it allows for improvements on a much larger scale, especially for typologically 230 2 Related Work Cross-lingual Transfer Static cross-lingual representations have long been used for effective crosslingual transfer and can even be induced without parallel data (Artetxe et al., 2017; Lample et al., 2018). In the monolingual case, static cross-lingual embeddings have recently been succeeded by contextualized ones, which yield considerably better results. The capabilities and limitations of the contextualized multilingual BERT (m-BERT) representations is a topic of vivid discourse. Pires et al. (2019) show surprisingly good transfer performance for mBERT despite it being trained without parallel data, and that transfer is better for typologically similar languages. Wu et al. (2019) show that language representations are not correctly aligned in m-BERT, but can be linearly"
2021.starsem-1.22,N18-1083,1,0.80958,"y broader sample, and shows that vector space normalization is as effective as other recently proposed fixes for m-BERT’s limitations (especially re-mapping), but is much cheaper and orthogonal to other solutions (e.g., input normalization) in that gains are almost additive. Linguistic Typology in NLP. Structural properties of many of the world’s languages can be queried via databases such as WALS (Dryer and Haspelmath, 2013). O’Horan et al. (2016); Ponti et al. (2019) suggest to inject typological information into models to bridge the performance gap between high- and low-resource languages. Bjerva and Augenstein (2018); de Lhoneux et al. (2018); Bjerva and Augenstein (2021) show that crossOriginal - m-BERT 0 50 50 0.0 0.2 0.4 0.6 Original - XLM-R 0.8 1.0 0 0.0 0.2 0.4 0.6 Remapping - XLM-R 0.8 1.0 0.90 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.90 0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 Remapping + Space Norm - XLM-R 1.0 50 50 50 0 Remapping + Space Norm - m-BERT Remapping - m-BERT 50 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.4 0.2 0.0 0.2 Matched pairs 0.4 0.6 0.8 1.0 Random pairs Figure 2: Histograms of cosine similarity scores of word pairs. lingual transfer can be more successful betwee"
2021.starsem-1.22,2021.eacl-main.38,1,0.736113,"ion is as effective as other recently proposed fixes for m-BERT’s limitations (especially re-mapping), but is much cheaper and orthogonal to other solutions (e.g., input normalization) in that gains are almost additive. Linguistic Typology in NLP. Structural properties of many of the world’s languages can be queried via databases such as WALS (Dryer and Haspelmath, 2013). O’Horan et al. (2016); Ponti et al. (2019) suggest to inject typological information into models to bridge the performance gap between high- and low-resource languages. Bjerva and Augenstein (2018); de Lhoneux et al. (2018); Bjerva and Augenstein (2021) show that crossOriginal - m-BERT 0 50 50 0.0 0.2 0.4 0.6 Original - XLM-R 0.8 1.0 0 0.0 0.2 0.4 0.6 Remapping - XLM-R 0.8 1.0 0.90 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.90 0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 Remapping + Space Norm - XLM-R 1.0 50 50 50 0 Remapping + Space Norm - m-BERT Remapping - m-BERT 50 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.4 0.2 0.0 0.2 Matched pairs 0.4 0.6 0.8 1.0 Random pairs Figure 2: Histograms of cosine similarity scores of word pairs. lingual transfer can be more successful between languages which share, e.g., morphological properties."
2021.starsem-1.22,N19-1156,1,0.902609,"For instance, according to WALS, English firmly follows the subject-verb-object (SVO) structure, whereas there is no dominant order in German. We apply this reordering in order to decrease the linguistic gap between languages. For instance, when considering English and French, we reverse all noun-adjective pairings from French to match English. This alignment is done while considering a dependency tree. We re-align according to the typological features from WALS. Since such feature annotations are available for a large amount of languages, and can be obtained automatically with high accuracy (Bjerva et al., 2019a), we expect this method to scale to languages for which basic dependencies (such as noun-adjective attachment) can be obtained automatically. In §5, we refer to the above re-alignment step as T EXT. Input normalization In addition to joint alignment and vector space normalization, we investigate decreasing crosslinguistic differences between languages via the following surface form manipulation of input texts. 232 4 4.1 Experiments Transfer tasks Cross-lingual embeddings are usually evaluated via zero-shot cross-lingual transfer for supervised text classification tasks, or via unsupervised c"
2021.starsem-1.22,J19-2006,1,0.915658,"For instance, according to WALS, English firmly follows the subject-verb-object (SVO) structure, whereas there is no dominant order in German. We apply this reordering in order to decrease the linguistic gap between languages. For instance, when considering English and French, we reverse all noun-adjective pairings from French to match English. This alignment is done while considering a dependency tree. We re-align according to the typological features from WALS. Since such feature annotations are available for a large amount of languages, and can be obtained automatically with high accuracy (Bjerva et al., 2019a), we expect this method to scale to languages for which basic dependencies (such as noun-adjective attachment) can be obtained automatically. In §5, we refer to the above re-alignment step as T EXT. Input normalization In addition to joint alignment and vector space normalization, we investigate decreasing crosslinguistic differences between languages via the following surface form manipulation of input texts. 232 4 4.1 Experiments Transfer tasks Cross-lingual embeddings are usually evaluated via zero-shot cross-lingual transfer for supervised text classification tasks, or via unsupervised c"
2021.starsem-1.22,W17-4755,0,0.011817,"infer whether a premise sentence entails, contradicts, or is neutral towards a hypothesis sentence. Conneau et al. (2018) release a multilingual NLI corpus, where the English dev and test sets of the MultiNLI corpus (Williams et al., 2018) are translated to 15 languages by crowd-workers. RFEval. This task evaluates the translation quality, i.e. similarity of a target language translation and a source language sentence. Following Zhao et al. (2020), we collect source language sentences with their system and reference translations, as well as human judgments from the WMT17 metrics shared task (Bojar et al., 2017), which contains predictions of 166 translation systems across 12 language pairs in WMT17. Each language pair has approximately 3k source sentences, each associA Typologically Varied Language Sample We evaluate multilingual representations on two sets of languages: (1) a default language set with 4 languages from the official XNLI test sets and 2 languages from the WMT17 test sets; (2) a diagnostic language set which contains 19 languages with different levels of data resources from a typologically diverse sample4 covering five language families (each with at least three languages): Austronesi"
2021.starsem-1.22,P19-4007,0,0.0589207,"Missing"
2021.starsem-1.22,D18-1269,0,0.0246154,"result in high quality target language embeddings and gives a false impression of cross-lingual abilities (Libovick´y et al., 2020). Zhao et al. (2020) use the more difficult task of reference-free machine translation evaluation (RFEval) to expose limitations of cross-lingual encoders, i.e., a failure to properly represent finegrained language aspects, which may be exploited by natural adversarial inputs such as word-by-word translations. XNLI. The goal of natural language inference (NLI) is to infer whether a premise sentence entails, contradicts, or is neutral towards a hypothesis sentence. Conneau et al. (2018) release a multilingual NLI corpus, where the English dev and test sets of the MultiNLI corpus (Williams et al., 2018) are translated to 15 languages by crowd-workers. RFEval. This task evaluates the translation quality, i.e. similarity of a target language translation and a source language sentence. Following Zhao et al. (2020), we collect source language sentences with their system and reference translations, as well as human judgments from the WMT17 metrics shared task (Bojar et al., 2017), which contains predictions of 166 translation systems across 12 language pairs in WMT17. Each languag"
2021.starsem-1.22,N19-1423,0,0.67862,"2 4 6 0.65 0.70 0.75 0.80 0.85 Language Similarity RFEval (66.9) 3 Task Performance XNLI (91.7) 4 0.90 0.95 XNLI (72.8) 2 1 0 1 2 0.0 0.5 1.0 1.5 2.0 Wikipedia articles (in millions) 2.5 Figure 1: Zero-shot performance on XNLI and RFEval vs. language similarity to English (top), and data sizes in Wikipedia (bottom). Each point is a language; brackets give the Pearson correlation of points on the xand y-axis. Zero-shot performance is based on the last layer of m-BERT and is standardized (zero mean, unit standard deviation) for better comparison. Introduction Cross-lingual text representations (Devlin et al., 2019; Conneau et al., 2019) ideally allow for transfer between any language pair, and thus hold the promise to alleviate the data sparsity problem for low-resource languages. However, until now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al"
2021.starsem-1.22,N13-1073,0,0.0394373,"Missing"
2021.starsem-1.22,D19-1006,0,0.0275513,"between languages which share, e.g., morphological properties. We draw inspiration from Wang and Eisner (2016), who use dependency statistics to generate a large collection of synthetic languages to augment training data for low-resource languages. This intuition of modifying languages based on syntactic features can also be used in order to decrease syntactic and morphological differences between languages. We go further than using syntactic features, and remove word contractions and reorder sentences based on typological information from WALS. 3 Language-Agnostic Representations Analyses by Ethayarajh (2019) indicate that random words are often assigned high cosine similarities in the upper layers of monolingual BERT. We examine this in a cross-lingual setting, by randomly selecting 500 German-English mutual word translations and random word pairs within parallel sentences from Europarl (Koehn, 2005). Fig. 2 (left) shows histograms based on the last layers of m-BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2019), respectively, which show that XLM-R wrongly assigns nearly perfect cosine similarity scores (+1) to both mutual word translations (matched word pairs) and random word pairs, wher"
2021.starsem-1.22,2005.mtsummit-papers.11,0,0.0416832,"ic features can also be used in order to decrease syntactic and morphological differences between languages. We go further than using syntactic features, and remove word contractions and reorder sentences based on typological information from WALS. 3 Language-Agnostic Representations Analyses by Ethayarajh (2019) indicate that random words are often assigned high cosine similarities in the upper layers of monolingual BERT. We examine this in a cross-lingual setting, by randomly selecting 500 German-English mutual word translations and random word pairs within parallel sentences from Europarl (Koehn, 2005). Fig. 2 (left) shows histograms based on the last layers of m-BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2019), respectively, which show that XLM-R wrongly assigns nearly perfect cosine similarity scores (+1) to both mutual word translations (matched word pairs) and random word pairs, whereas m-BERT sometimes assigns low scores to mutual translations. This reaffirms that both mBERT and XLM-R have difficulty in distinguishing matched from random word pairs. Surprisingly, vector space re-mapping does not seem to help for XLM-R, but better separates random from matched pairs for m-BER"
2021.starsem-1.22,D18-2012,0,0.0412338,"Missing"
2021.starsem-1.22,2020.emnlp-main.363,0,0.0533972,"r of m-BERT and is standardized (zero mean, unit standard deviation) for better comparison. Introduction Cross-lingual text representations (Devlin et al., 2019; Conneau et al., 2019) ideally allow for transfer between any language pair, and thus hold the promise to alleviate the data sparsity problem for low-resource languages. However, until now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al., 2019) and, more importantly, to realign them post-hoc so as to address the deficits of state-of-the-art contextualized encoders which have not seen any parallel data during training (Schuster et al., 2019; Wu and Dredze, 2019; Cao et al., 2020). However, re-mapping (i) can be costly, (ii) requires parallel data on word or sentence level, which may not be available abundantly in low-resource set1 https://github.com/AIPHES/ Language-Agnostic-Contextualized-"
2021.starsem-1.22,D18-1543,1,0.866387,"Missing"
2021.starsem-1.22,P19-1493,0,0.167712,"Missing"
2021.starsem-1.22,2020.findings-emnlp.150,0,0.0338733,"Missing"
2021.starsem-1.22,2020.lrec-1.497,0,0.0468248,"Missing"
2021.starsem-1.22,C16-1123,0,0.0581884,"Missing"
2021.starsem-1.22,J19-3005,0,0.0199628,"Missing"
2021.starsem-1.22,N19-1162,0,0.072225,"now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al., 2019) and, more importantly, to realign them post-hoc so as to address the deficits of state-of-the-art contextualized encoders which have not seen any parallel data during training (Schuster et al., 2019; Wu and Dredze, 2019; Cao et al., 2020). However, re-mapping (i) can be costly, (ii) requires parallel data on word or sentence level, which may not be available abundantly in low-resource set1 https://github.com/AIPHES/ Language-Agnostic-Contextualized-Encoders 2 We consider language similarity as the cosine similarity between the average representations of two languages over monolingual corpora from Wikipedia. 229 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 229–240 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguisti"
2021.starsem-1.22,L16-1680,0,0.0315374,"Missing"
2021.starsem-1.22,Q16-1035,0,0.0287008,"m-BERT 0 50 50 0.0 0.2 0.4 0.6 Original - XLM-R 0.8 1.0 0 0.0 0.2 0.4 0.6 Remapping - XLM-R 0.8 1.0 0.90 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.90 0 0.4 0.2 0.0 0.2 0.4 0.6 0.8 Remapping + Space Norm - XLM-R 1.0 50 50 50 0 Remapping + Space Norm - m-BERT Remapping - m-BERT 50 0.92 0.94 Matched pairs 0.96 0.98 1.00 Random pairs 0 0.4 0.2 0.0 0.2 Matched pairs 0.4 0.6 0.8 1.0 Random pairs Figure 2: Histograms of cosine similarity scores of word pairs. lingual transfer can be more successful between languages which share, e.g., morphological properties. We draw inspiration from Wang and Eisner (2016), who use dependency statistics to generate a large collection of synthetic languages to augment training data for low-resource languages. This intuition of modifying languages based on syntactic features can also be used in order to decrease syntactic and morphological differences between languages. We go further than using syntactic features, and remove word contractions and reorder sentences based on typological information from WALS. 3 Language-Agnostic Representations Analyses by Ethayarajh (2019) indicate that random words are often assigned high cosine similarities in the upper layers o"
2021.starsem-1.22,N18-1101,0,0.0109127,"et al., 2020). Zhao et al. (2020) use the more difficult task of reference-free machine translation evaluation (RFEval) to expose limitations of cross-lingual encoders, i.e., a failure to properly represent finegrained language aspects, which may be exploited by natural adversarial inputs such as word-by-word translations. XNLI. The goal of natural language inference (NLI) is to infer whether a premise sentence entails, contradicts, or is neutral towards a hypothesis sentence. Conneau et al. (2018) release a multilingual NLI corpus, where the English dev and test sets of the MultiNLI corpus (Williams et al., 2018) are translated to 15 languages by crowd-workers. RFEval. This task evaluates the translation quality, i.e. similarity of a target language translation and a source language sentence. Following Zhao et al. (2020), we collect source language sentences with their system and reference translations, as well as human judgments from the WMT17 metrics shared task (Bojar et al., 2017), which contains predictions of 166 translation systems across 12 language pairs in WMT17. Each language pair has approximately 3k source sentences, each associA Typologically Varied Language Sample We evaluate multilingu"
2021.starsem-1.22,D19-1077,0,0.0168753,"ach point is a language; brackets give the Pearson correlation of points on the xand y-axis. Zero-shot performance is based on the last layer of m-BERT and is standardized (zero mean, unit standard deviation) for better comparison. Introduction Cross-lingual text representations (Devlin et al., 2019; Conneau et al., 2019) ideally allow for transfer between any language pair, and thus hold the promise to alleviate the data sparsity problem for low-resource languages. However, until now, crosslingual systems trained on English appear to transfer poorly to target languages dissimilar to English (Wu and Dredze, 2019; Pires et al., 2019) and for which only small monolingual corpora are available (Conneau et al., 2019; Hu et al., 2020; Lauscher et al., 2020), as illustrated in Fig. 1.2 As a remedy, recent work has suggested to train representations on larger multilingual corpora (Conneau et al., 2019) and, more importantly, to realign them post-hoc so as to address the deficits of state-of-the-art contextualized encoders which have not seen any parallel data during training (Schuster et al., 2019; Wu and Dredze, 2019; Cao et al., 2020). However, re-mapping (i) can be costly, (ii) requires parallel data on"
2021.starsem-1.22,2020.acl-main.151,1,0.879145,"Missing"
2021.starsem-1.22,D19-1053,1,0.832568,"h French Italian Indonesian Dutch Portuguese German English α α γ γ η γ γ η η β α δ δ δ α β δ β β low low low low middle low middle middle middle low middle high high high middle high high high high 29.3 26.5 24.8 24.0 23.8 22.2 21.7 20.1 19.8 19.6 19.2 18.5 18.2 18.0 17.7 16.3 16.2 15.6 0.0 0.08 0.06 0.08 0.06 0.20 0.13 0.15 0.47 0.46 0.09 0.33 1.56 2.16 1.57 0.51 1.99 1.02 2.37 5.98 low low low low low middle middle middle middle middle middle high high high high high high high high ated with one human reference translation and with the automatic translations of participating systems. As in Zhao et al. (2019, 2020), we use the Earth Mover Distance to compute the distances between source sentence and target language translations, based on the semantic similarities of their contextualized cross-lingual embeddings. We refer to this score as XMoverScore (Zhao et al., 2020) and report its Pearson correlation with human judgments in our experiments. 4.2 Table 1: Languages used, with their language families: Austronesian (α), Germanic (β), Indo-Aryan (γ), Romance (δ), and Uralic (η). The cosine distances between target languages and English are measured using m-BERT. This is, however, not likely to resu"
C16-1333,W13-3520,0,0.0122335,"e code is available at https://github.com/bjerva/semantic-tagging. We represent each sentence using both a character-based representation (Sc ) and a word-based representation (Sw ). The character-based representation is a 3-dimensional matrix Sc ∈ Rs×w×dc , where s is the zero-padded sentence length, w is the zero-padded word length, and dc is the dimensionality of the character embeddings. The word-based representation is a 2-dimensional matrix Sw ∈ Rs×dw , where s is the zero-padded sentence length and dw is the dimensionality of the word embeddings. We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings. Word embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014). We also experimented using a bi-LSTM. However, we found GRUs to yield comparatively better validation data performance on semtags. We also observe better validation data performance when running two consecutive forward and backward passes before concatenating the GRU layers, rather than concatenating after each forward/backward pass as is commonplace in NLP literature. 3535 We use CNNs for character-le"
C16-1333,P14-1133,0,0.0284775,"Missing"
C16-1333,W16-4816,1,0.900908,"n’ information path in the network facilitates optimisation (He et al., 2016). ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3531 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3531–3541, Osaka, Japan, December 11-17 2016. ¨ also seen some recent use in NLP (Ostling, 2016; Conneau et al., 2016; Bjerva, 2016; Wu et al., 2016). However, no previous work has attempted to apply ResNets to NLP tagging tasks. To answer our second question, we carry out an extrinsic evaluation exercise. We investigate the effect of using semantic tags as an auxiliary loss for POS tagging. Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging. 2 2.1 Semantic Tagging Background We refer to semantic tagging, or sem-tagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentence. In the context of this paper thes"
C16-1333,W08-2222,1,0.211215,"bank (PTB) Part-of-Speech tagset (Marcus et al., 1993) does not make the necessary semantic distinctions, in addition to containing redundant information for semantic processing. Let us consider a couple of examples. There are significant differences in meaning between the determiners every (universal quantification), no (negation), and some (existential quantification), but they all receive the DT (determiner) POS label in PTB. Since determiners form a closed class, one could enumerate all word forms for each class. Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010). This might work for a single language, but it falls short when considering a multilingual setting. Furthermore, determiners like any can have several interpretations and need to be disambiguated in context. Semantic tagging does not only apply to determiners, but reaches all parts of speech. Other examples where semantic classes disambiguate are reflexive versus emphasising pronouns (both POS-tagged as PRP, personal pronoun); the comma, that could be a conjunction, disjunction, or apposition; intersective vs. subsective and privative adjectives (all POS-tagged as JJ, adjective"
C16-1333,A00-1031,0,0.0508761,"ing independent variables in our experiments: 1. character and word representations (w, ~ ~c); 2. residual bypass for character representations (~cbp ); 3. convolutional representations (Basic CNN and ResNets); 4. auxiliary loss (using coarse semtags on ST and fine semtags on UD). 3536 We compare our results to four baselines: 1. the most frequent baseline per word (MFC), where we assign the most frequent tag for a word in the training data to that word in the test data, and unseen words get the global majority tag; 2. the trigram statistic based TNT tagger offers a slightly tougher baseline (Brants, 2000); 3. the B I - LSTM baseline, running the off-the-shelf state-of-the-art POS tagger for the UD dataset (Plank et al., 2016) (using default parameters with pre-trained Polyglot embeddings); 4. we use a baseline consisting of running our own system with only a B I - GRU using word representations (w), ~ with pre-trained Polyglot embeddings. 4.1 Experiments on semantic tagging We evaluate our system on two semantic tagging (ST) datasets: our silver semtag dataset and our gold semtag dataset. For the +AUX condition we use coarse semtags as an auxiliary loss. Results from these experiments are show"
C16-1333,D15-1085,0,0.0325507,"in a space with dimensionality dc as an image of dimensionality n × dc . This view gives us additional freedom in terms of sizes of convolutional patches used, which offers more computational flexibility than using only, e.g., 4 × dc convolutions. This view is applied to all CNN variations explored in this work. A neural network is trained with respect to some loss function, such as the cross-entropy between the predicted tag probability distribution and the gold probability distribution. Recent work has shown that the addition of an auxiliary loss function can be beneficial to several tasks. Cheng et al. (2015) use a language modelling task as an auxiliary loss, as they attempt to predict the next token while performing named entity recognition. Plank et al. (2016) use the log frequency of the current token as an auxiliary loss function, and find this to improve POS tagging accuracy. Since our semantic tagging task is based on predicting fine semtags, which can be mapped to coarse semtags, we add the prediction of these coarse semtags as an auxiliary loss for the sem-tagging experiments. Similarly, we also experiment with POS tagging, where we use the fine semtags as an auxiliary information. 3.4.1"
C16-1333,P16-1160,0,0.0190157,"ork is the first to apply ResNets to NLP sequence tagging tasks. We further contribute to the literature on ResNets by introducing a residual bypass function. The intuition is to combine both deep and shallow processing, which opens a path of easy signal propagation between lower and higher layers in the network. 3.3 Modelling character information and residual bypass Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupała, 2013; 3534 Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations. Gillick et al. (2015) similarly apply an LSTM-based model using byte-level information directly. Dos Santos and Zadrozny (2014) construct character-based word-level representations by running a convolutional network over the character representations of each word. All of these approaches have in common that the character-based representation i"
C16-1333,D13-1146,1,0.363033,"matically obtained with the C&C tools (Curran et al., 2007) and then manually corrected), as well as a set of manually crafted rules to output semantic tags. Some tags related to specific phenomena were hand-corrected in a second stage. Our second dataset is smaller but equipped with gold standard semantic tags and used for testing (PMB, the Parallel Meaning Bank). It comprises a selection of 400 sentences of the English part of a parallel corpus. It has no overlap with the GMB corpus. For this dataset, we used the Elephant tokeniser, which performs word, multi-word and sentence segmentation (Evang et al., 2013). We then used the 3532 ANA PRO DEF HAS REF EMP ACT GRE ITJ HES QUE ATT QUA UOM IST REL RLI SST PRI INT SCO LOG ALT EXC NIL DIS IMP AND BUT pronoun definite possessive reflexive emphasizing greeting interjection hesitation interrogative quantity measurement intersective relation rel. inv. scope subsective privative intensifier score alternative exclusive empty disjunct./exist. implication conjunct./univ. contrast COM EQA MOR LES TOP BOT ORD DEM PRX MED DST DIS SUB COO APP MOD NOT NEC POS ENT CON ROL NAM GPE PER LOC ORG ART NAT HAP URL equative comparative pos. comparative neg. pos. superlative"
C16-1333,J93-2004,0,0.0565979,". Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging. 2 2.1 Semantic Tagging Background We refer to semantic tagging, or sem-tagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentence. In the context of this paper these units can be morphemes, words, punctuation, or multi-word expressions. The linguistic information traditionally obtained for deep processing is insufficient for fine-grained lexical semantic analysis. The widely used Penn Treebank (PTB) Part-of-Speech tagset (Marcus et al., 1993) does not make the necessary semantic distinctions, in addition to containing redundant information for semantic processing. Let us consider a couple of examples. There are significant differences in meaning between the determiners every (universal quantification), no (negation), and some (existential quantification), but they all receive the DT (determiner) POS label in PTB. Since determiners form a closed class, one could enumerate all word forms for each class. Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010). This might work for a single"
C16-1333,W16-2003,0,0.135016,"eeper networks, since keeping a ‘clean’ information path in the network facilitates optimisation (He et al., 2016). ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3531 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3531–3541, Osaka, Japan, December 11-17 2016. ¨ also seen some recent use in NLP (Ostling, 2016; Conneau et al., 2016; Bjerva, 2016; Wu et al., 2016). However, no previous work has attempted to apply ResNets to NLP tagging tasks. To answer our second question, we carry out an extrinsic evaluation exercise. We investigate the effect of using semantic tags as an auxiliary loss for POS tagging. Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging. 2 2.1 Semantic Tagging Background We refer to semantic tagging, or sem-tagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentenc"
C16-1333,P16-2067,1,0.925084,"cope with longer input sequences than vanilla RNNs. GRUs are similar to the more commonly-used Long Short-Term Memory networks (LSTMs), both in purpose and implementation (Chung et al., 2014). A bi-directional GRU is a GRU which makes both forward and backward passes over sequences, and can therefore use both preceding and succeeding contexts to predict a tag (Graves and Schmidhuber, 2005; Goldberg, 2015). Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016). We build on previous approaches by combining bi-GRUs with character representations from a basic CNN and ResNets. 3533 Figure 1: Model architecture. Left: Architecture with basic CNN char representations (~c), Middle: basic CNN with char and word representations and bypass (~cbp ∧ w), ~ Right: ResNet with auxiliary loss and residual bypass (+AUXbp ). 3.2 Deep Residual Networks Deep Residual Networks (ResNets) are built up by stacking residual units. A residual unit can be expressed as: yl = h(xl ) + F(xl , Wl ), (3) xl+1 = f (yl ), where xl and xl+1 are the input and output of the l-th layer"
C16-1333,N04-1030,0,0.0174103,"Missing"
C16-1333,P07-2009,1,\N,Missing
C16-1333,L16-1262,0,\N,Missing
D18-1543,N18-1083,1,0.545746,"a parser across languages in the context of a probabilistic parser. Options we do not explore here are learning the architecture jointly with optimizing the task objective (Misra et al., 2016; Ruder et al., 2017), or learning an architecture search model that predicts an architecture based on the properties of datasets, typically with reinforcement learning (Zoph and Le, 2017; Wong and Gesmundo, 2018; Liang et al., 2018). We also do not explore the option of sharing selectively based on more ﬁne-grained typological information about languages, which related work has indicated could be useful (Bjerva and Augenstein, 2018). Rather, we stick to sharing between languages of the same language families. The strategies explored here do not exhaust the space of possible parameter sharing strategies. For example, we completely ignore soft sharing based on mean-constrained regularisation (Duong et al., 2015). 8 Conclusions We present evaluations of 27 parameter sharing strategies for the Uppsala parser across 10 languages, representing ﬁve language pairs from ﬁve different language families. We repeated the experiment with pairs of unrelated languages. We made several observations: (a) Generally, multitask learning hel"
D18-1543,P15-1166,0,0.0238529,"evaluated on development data); B EST and W ORST are the overall best and worst sharing strategy across languages; C HAR shares only the character-based LSTM parameters; W ORD shares only the word-based LSTM parameters; A LL shares all parameters. refers to hard sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing. � (soft sharing of word parameters, hard sharing of the rest) improves parsing accuracy when training on related languages, and is especially useful in the low resource case. Similar effects have been observed in machine translation (Dong et al., 2015; Johnson et al., 2017), for example. Most studies have only explored a small number of parameter sharing strategies, however. Vilares et al. (2016) evaluate parsing with hard parameter sharing for 100 language pairs with a statistical parser. Naseem et al. (2012) proposed to selectively share subsets of a parser across languages in the context of a probabilistic parser. Options we do not explore here are learning the architecture jointly with optimizing the task objective (Misra et al., 2016; Ruder et al., 2017), or learning an architecture search model that predicts an architecture based on"
D18-1543,P15-2139,0,0.475239,"alidation data. This model is linguistically motivated and obtains signiﬁcant improvements over a mono-lingually trained baseline. We also ﬁnd that sharing transition classiﬁer parameters helps when training a parser on unrelated language pairs, but we ﬁnd that, in the case of unrelated languages, sharing too many parameters does not help. 1 Introduction The idea of sharing parameters between parsers of related languages goes back to early work in crosslingual adaptation (Zeman and Resnik, 2008), and the idea has recently received a lot of interest in the context of neural dependency parsers (Duong et al., 2015; Ammar et al., 2016; Susanto and Lu, 2017). Modern neural dependency parsers, however, use different sets of parameters for representation and scoring, and it is not clear what parameters it is best to share. The Universal Dependencies (UD) project (Nivre et al., 2016), which is seeking to harmonize the annotation of dependency treebanks across ∗ Work carried out during a stay at the University of Copenhagen. languages, has seen a steady increase in languages that have a treebank in a common standard. Many of these languages are low resource and have small UD treebanks. It seems interesting t"
D18-1543,Q16-1023,0,0.0441809,"rs is tuned. The novel architecture signiﬁcantly outperforms our monolingual baseline on our set of 10 languages. We additionally investigate parameter sharing of unrelated languages. 2 The Uppsala dependency parser The Uppsala parser (de Lhoneux et al., 2017a,b) consists of three sets of parameters; the parameters of the character-based LSTM, those of the word-based LSTM, and the parameters of the MLP that predicts transitions. The character-based LSTM produces representations for the wordbased LSTM, which produces representations for the MLP. The Uppsala parser is a transition-based parser (Kiperwasser and Goldberg, 2016), adapted to the Universal Dependencies (UD) scheme,1 and using the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). The S WAP 1 http://universaldependencies.org/ transition is used to generate non-projective dependency trees (Nivre, 2009). For an input sentence of length n with words w1 , . . . , wn , the parser creates a sequence of vectors x1:n , where the vector xi representing wi is the concatenation of a word embedding and the ﬁnal state of the character-based LSTM after proce"
D18-1543,P11-1068,0,0.107639,"Missing"
D18-1543,K17-3022,1,0.895862,"Missing"
D18-1543,W17-6314,1,0.900687,"Missing"
D18-1543,P12-1066,0,0.340049,"d sharing, ID refers to soft sharing, using an embedding of the language ID and ✗ refers to not sharing. � (soft sharing of word parameters, hard sharing of the rest) improves parsing accuracy when training on related languages, and is especially useful in the low resource case. Similar effects have been observed in machine translation (Dong et al., 2015; Johnson et al., 2017), for example. Most studies have only explored a small number of parameter sharing strategies, however. Vilares et al. (2016) evaluate parsing with hard parameter sharing for 100 language pairs with a statistical parser. Naseem et al. (2012) proposed to selectively share subsets of a parser across languages in the context of a probabilistic parser. Options we do not explore here are learning the architecture jointly with optimizing the task objective (Misra et al., 2016; Ruder et al., 2017), or learning an architecture search model that predicts an architecture based on the properties of datasets, typically with reinforcement learning (Zoph and Le, 2017; Wong and Gesmundo, 2018; Liang et al., 2018). We also do not explore the option of sharing selectively based on more ﬁne-grained typological information about languages, which re"
D18-1543,P09-1040,0,0.040354,"STM, and the parameters of the MLP that predicts transitions. The character-based LSTM produces representations for the wordbased LSTM, which produces representations for the MLP. The Uppsala parser is a transition-based parser (Kiperwasser and Goldberg, 2016), adapted to the Universal Dependencies (UD) scheme,1 and using the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). The S WAP 1 http://universaldependencies.org/ transition is used to generate non-projective dependency trees (Nivre, 2009). For an input sentence of length n with words w1 , . . . , wn , the parser creates a sequence of vectors x1:n , where the vector xi representing wi is the concatenation of a word embedding and the ﬁnal state of the character-based LSTM after processing the characters of wi . The character vector ch(wi ) is obtained by running a (bi-directional) LSTM over the characters chj (1 ≤ j ≤ m) of wi . Each input element is represented by the word-level, bi-directional LSTM, as a vector vi = B I L STM(x1:n , i). For each conﬁguration, the feature extractor concatenates the LSTM representations of core"
D18-1543,K18-2011,1,0.882793,"Missing"
D18-1543,P17-2007,0,0.118529,"ally motivated and obtains signiﬁcant improvements over a mono-lingually trained baseline. We also ﬁnd that sharing transition classiﬁer parameters helps when training a parser on unrelated language pairs, but we ﬁnd that, in the case of unrelated languages, sharing too many parameters does not help. 1 Introduction The idea of sharing parameters between parsers of related languages goes back to early work in crosslingual adaptation (Zeman and Resnik, 2008), and the idea has recently received a lot of interest in the context of neural dependency parsers (Duong et al., 2015; Ammar et al., 2016; Susanto and Lu, 2017). Modern neural dependency parsers, however, use different sets of parameters for representation and scoring, and it is not clear what parameters it is best to share. The Universal Dependencies (UD) project (Nivre et al., 2016), which is seeking to harmonize the annotation of dependency treebanks across ∗ Work carried out during a stay at the University of Copenhagen. languages, has seen a steady increase in languages that have a treebank in a common standard. Many of these languages are low resource and have small UD treebanks. It seems interesting to ﬁnd out ways to leverage the wealth of in"
D18-1543,P16-2069,0,0.0948361,"Missing"
D18-1543,I08-3008,0,0.140566,"n classiﬁer is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This model is linguistically motivated and obtains signiﬁcant improvements over a mono-lingually trained baseline. We also ﬁnd that sharing transition classiﬁer parameters helps when training a parser on unrelated language pairs, but we ﬁnd that, in the case of unrelated languages, sharing too many parameters does not help. 1 Introduction The idea of sharing parameters between parsers of related languages goes back to early work in crosslingual adaptation (Zeman and Resnik, 2008), and the idea has recently received a lot of interest in the context of neural dependency parsers (Duong et al., 2015; Ammar et al., 2016; Susanto and Lu, 2017). Modern neural dependency parsers, however, use different sets of parameters for representation and scoring, and it is not clear what parameters it is best to share. The Universal Dependencies (UD) project (Nivre et al., 2016), which is seeking to harmonize the annotation of dependency treebanks across ∗ Work carried out during a stay at the University of Copenhagen. languages, has seen a steady increase in languages that have a treeb"
D18-1543,W14-4606,0,0.0296745,"0.5 0.8 0.1 0.3 -0.1 1.0 1.4 1.0 0.8 78.8 78.2 0.6 � � ID ID ✗ � � ID ✗ ✗ ✗ � � ID ✗ av. Table 3: LAS on the test sets of the best of 9 sharing strategies and the monolingual baseline. δ is the difference between O URS AND M ONO . 6 Unrelated languages We repeated the same set of experiments with unrelated language pairs. We hypothesise that parameter sharing between unrelated language pairs will be less useful in general than with related language pairs. However, it can still be useful, it has been shown previously that unrelated languages can beneﬁt from being trained jointly. For example, Lynn et al. (2014) have shown that Indonesian was surprisingly particularly useful for Irish. The results are presented in Table 4. The table only presents part of the results, the rest can be found in the supplementary material. As expected, there is much less to be gained from sharing parameters between unrelated pairs. However, it is possible to improve the monolingual baseline by sharing some of the parameters. In general, sharing the MLP is still a helpful thing to do. It is most helpful to share the MLP and optionally one of the two other sets of parameters. Results are close to the monolingual baseline w"
D19-6128,C16-1333,1,0.894016,"Missing"
D19-6128,P17-1014,0,0.0285848,"his condition, the test set is still relatively small. Thus, the word embeddings do not have much distributional information with which to arrive at good word representations for previously out-of-vocabulary words. In RQ1, we asked for which task and dataset sizes transductive auxiliary task self-training is most beneficial. We found benefits across the board, with larger effects when the main task training set is small. 5 Related Work Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on"
D19-6128,E17-2039,1,0.851467,"Missing"
D19-6128,E17-1005,0,0.0343576,"Missing"
D19-6128,N18-1172,1,0.854624,"f the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For instance, Ruder and Plank (2017) find that similarity as well as diversity measures applied to the main vs. auxiliary task datasets as a whole are useful in selecting auxiliary tasks. In the context of sequence labelling, many combinations 256 of tasks have been explored (Søgaard and Goldberg, 2016b; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Ruder et al. (2019) present a flexible architecture, which learns which parameters to share between a main and an auxiliary task. One of the few examples where multi-task learning is combined with other methods is the semi"
D19-6128,D15-1041,0,0.0150694,"cies obtained from our training data. 2.2 We approach sequence labelling by using a variant of a bidirectional recurrent neural network, which uses both preceding and succeeding context when predicting the label of a word. This choice was made as such models at the same time obtain high performance on all three tasks and lend themselves nicely to multi-task training via hard parameter sharing. This system is based on the hierarchical bi-LSTM of Plank et al. (2016) and is implemented using DyNet (Neubig et al., 2017). On the subword-level, the LSTM is bi-directional and operates on characters (Ballesteros et al., 2015; Ling et al., 2015). Second, a context bi-LSTM operates on the word level, from which output is passed on to a classification layer. Multi-task training is approached using hard parameter sharing (Caruana, 1993). We consider T datasets, each containing pairs of input-output set ), w ∈ V , y t ∈ Lt . The input quences (w1:n , y1:n i i vocabulary V is shared across tasks, but the outputs (tagsets) Lt are task dependent. At each step in the training process we choose a random task t, followed by a randomly chosen batch of training instance. Each task is associated with an independent classificat"
D19-6128,L16-1262,0,0.0622977,"Missing"
D19-6128,N18-1202,0,0.0189821,"for self-training. Second, we choose a transductive approach, because we assume that not all auxiliary task examples will lead to equal improvements on the main task. In particular, we expect auxiliary task labels for the test instances to be most useful, since information about those instances is most relevant for the prediction of the main task labels on this data. Similarly to contextualised word representations, this offers an additional signal for the test set instances, as we obtain this through predicted auxiliary labels rather than direct encoding of the context (Devlin et al., 2018; Peters et al., 2018). 3.1 question how it compares to adding additional (expensive) gold-standard annotations for the main and the auxiliary tasks. To ensure that our findings are generalisable, we use a large sample of 56 treebanks, covering 41 languages and several domains. Although this experimental set-up would allow us to run multilingual experiments, we only train monolingual models, and aggregate results across languages and treebanks. We investigate three tasks; two of them being morpho-syntactic (POS tagging and DepRel tagging) and one being semantic (semantic tagging). In all cases, POS is the auxiliary"
D19-6128,P16-2067,0,0.0199108,"multi-task model for DepRel tagging and semantic tagging, respectively. 2 (2016). We use this task as an unsupervised auxiliary baseline, with frequencies obtained from our training data. 2.2 We approach sequence labelling by using a variant of a bidirectional recurrent neural network, which uses both preceding and succeeding context when predicting the label of a word. This choice was made as such models at the same time obtain high performance on all three tasks and lend themselves nicely to multi-task training via hard parameter sharing. This system is based on the hierarchical bi-LSTM of Plank et al. (2016) and is implemented using DyNet (Neubig et al., 2017). On the subword-level, the LSTM is bi-directional and operates on characters (Ballesteros et al., 2015; Ling et al., 2015). Second, a context bi-LSTM operates on the word level, from which output is passed on to a classification layer. Multi-task training is approached using hard parameter sharing (Caruana, 1993). We consider T datasets, each containing pairs of input-output set ), w ∈ V , y t ∈ Lt . The input quences (w1:n , y1:n i i vocabulary V is shared across tasks, but the outputs (tagsets) Lt are task dependent. At each step in the t"
D19-6128,W03-0404,0,0.107928,"ulary words. In RQ1, we asked for which task and dataset sizes transductive auxiliary task self-training is most beneficial. We found benefits across the board, with larger effects when the main task training set is small. 5 Related Work Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For"
D19-6128,D17-1038,0,0.0171906,"can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For instance, Ruder and Plank (2017) find that similarity as well as diversity measures applied to the main vs. auxiliary task datasets as a whole are useful in selecting auxiliary tasks. In the context of sequence labelling, many combinations 256 of tasks have been explored (Søgaard and Goldberg, 2016b; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Ruder et al. (2019) present a flexible architecture, which learns which parameters to share between a main and an auxiliary task. One of the few examples where multi-task learning is combined with"
D19-6128,P16-2038,0,0.159736,"05) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-task learning, most research focuses on understanding which auxiliary tasks to select, or on how to share between tasks (Søgaard and Goldberg, 2016a; Lin et al., 2019; Ruder and Plank, 2017; Augenstein et al., 2018; Ruder et al., 2019). For instance, Ruder and Plank (2017) find that similarity as well as diversity measures applied to the main vs. auxiliary task datasets as a whole are useful in selecting auxiliary tasks. In the context of sequence labelling, many combinations 256 of tasks have been explored (Søgaard and Goldberg, 2016b; Mart´ınez Alonso and Plank, 2017; Bjerva, 2017). Ruder et al. (2019) present a flexible architecture, which learns which parameters to share between a main and an auxiliary task. One of the few examples w"
D19-6128,L16-1680,0,0.0458928,"Missing"
D19-6128,P95-1026,0,0.760946,"ary rate is reduced to zero in this condition, the test set is still relatively small. Thus, the word embeddings do not have much distributional information with which to arrive at good word representations for previously out-of-vocabulary words. In RQ1, we asked for which task and dataset sizes transductive auxiliary task self-training is most beneficial. We found benefits across the board, with larger effects when the main task training set is small. 5 Related Work Self-training has been shown to be a successful learning approach (Nigam and Ghani, 2000), e.g., for word sense disambiguation (Yarowsky, 1995) or AMR parsing (Konstas et al., 2017). Samples in self-training are typically selected according to confidence (Zhu, 2005) which requires a proxy to measure it. This can be the confidence of the model (Yarowsky, 1995; Riloff et al., 2003) or the agreement of different models, as used in tri-training (Zhou and Li, 2005). Another option is curriculum learning, where selection is based on learning difficulty, increasing the difficulty during learning (Bengio et al., 2009). In contrast, we build upon the assumption that the auxiliary task examples are ones a model can be certain about. In multi-t"
D19-6128,D15-1176,0,\N,Missing
D19-6128,P19-1301,0,\N,Missing
E14-3008,P98-1013,0,0.157693,"Missing"
E14-3008,basile-etal-2012-developing,0,0.0815028,"d semantic features. 4 There are several corpora of reasonable size which include semantic annotation on some level, such as PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998), and the Penn Discourse TreeBank (Prasad et al., 2005). The combination of several levels of semantic annotation into one formalism are not common, however. Although some efforts exist, they tend to lack a level of formally grounded “deep” semantic representation which combines these layers. The Groningen Meaning Bank (GMB) contains a substantial collection of English texts with such deep semantic annotation (Basile et al., 2012a). One of its goals is to combine semantic phenomena into a single formalism, as opposed to dealing with single phenomena in isolation. This provides a better handle on explaining dependencies between various ambiguous linguistic phenomena. Manually annotating a comprehensive corpus with gold-standard semantic representations is obviously a hard and time-consuming task. Therefore, a sophisticated bootstrapping approach is used. Existing NLP tools are used to get a reasonable approximation of the target annotations to start with. Pieces of information coming from both experts (linguists) and c"
E14-3008,C02-1014,0,0.116046,"Missing"
E14-3008,W05-0509,0,0.0393507,"Missing"
E14-3008,E12-2019,0,0.123618,"d semantic features. 4 There are several corpora of reasonable size which include semantic annotation on some level, such as PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998), and the Penn Discourse TreeBank (Prasad et al., 2005). The combination of several levels of semantic annotation into one formalism are not common, however. Although some efforts exist, they tend to lack a level of formally grounded “deep” semantic representation which combines these layers. The Groningen Meaning Bank (GMB) contains a substantial collection of English texts with such deep semantic annotation (Basile et al., 2012a). One of its goals is to combine semantic phenomena into a single formalism, as opposed to dealing with single phenomena in isolation. This provides a better handle on explaining dependencies between various ambiguous linguistic phenomena. Manually annotating a comprehensive corpus with gold-standard semantic representations is obviously a hard and time-consuming task. Therefore, a sophisticated bootstrapping approach is used. Existing NLP tools are used to get a reasonable approximation of the target annotations to start with. Pieces of information coming from both experts (linguists) and c"
E14-3008,J96-1002,0,0.0567573,"Missing"
E14-3008,Y09-1024,0,0.0206145,"Missing"
E14-3008,P06-4018,0,0.00539003,"n cross validation iterations, one of these folds was left out and used for evaluation. For the sake of conciseness, averaged results over all classes are given in the comparisons of Section 6.1.1 and Section 6.1.2, whereas detailed results are only given for the best performing classifier. Note that the training data from Wordrobe is not used for the evaluations on the Switchboard corpus, as this would prohibit fair evaluation with previous work. WordNet distances We also include a feature based on WordNet distances. In this work, we use the path distance similarity measure provided in NLTK (Bird, 2006). In essence, this measure provides a score based on the shortest path that connects the senses in a hypernym/hyponym taxonomy. First, we calculate the distance to each hypernym of every given word. These distances are then summed together for each animacy class. Taking the most frequent hypernym for each animacy class gives us the following hypernyms: person.n.01, abstraction.n.06, city.n.01, time period.n.01, car.n.01, organization.n.01, artifact.n.01, animal.n.01, machine.n.01, buddy.n.01. The classifier then uses whichever of these words is closest as its WordNet feature. 5.2.4 Results 6.1"
E14-3008,N12-2002,0,0.189522,"Missing"
E14-3008,J13-4004,0,0.0154016,"grouped as inanimate, while the remaining classes are grouped as animate. The availability of this data allows us to easily exploit the annotation for a supervised learning approach (see Section 5). Relevance of animacy for NLP Although seemingly overlooked in the past, animacy has recently been shown to be an important feature for NLP. Øvrelid & Nivre (2007) found that the accuracy of a dependency parser for Swedish could be improved by incorporating a binary animacy distinction. Other work has highlighted animacy as relevant for anaphora and coreference resolution (Or˘asan and Evans, 2007; Lee et al., 2013) and verb argument disambiguation (Dell’Orletta et al., 2005). Furthermore, in English, the choices for dative alternation (Bresnan et al., 2007), between genitive constructions (Stefanowitsch, 2003), and between active and passive voice (Rosenbach, 2008) are also affected by the animacy of their constituent nouns. With this in mind, Zaenen et al. (2004) suggest that animacy, for languages such as English, is not a matter of grammatical and ungrammatical sentences, but rather of sentences being more and less felicitous. This highlights annotation of animacy as potentially particularly useful f"
E14-3008,W08-2222,0,0.0541828,"Missing"
E14-3008,D13-1006,0,0.0143148,"voice (Rosenbach, 2008) are also affected by the animacy of their constituent nouns. With this in mind, Zaenen et al. (2004) suggest that animacy, for languages such as English, is not a matter of grammatical and ungrammatical sentences, but rather of sentences being more and less felicitous. This highlights annotation of animacy as potentially particularly useful for applications such as Natural Language Generation. In spite of this, animacy appears to be rarely annotated in corpora, and thus also rather rarely used in tools and algorithms for NLP (although some recent efforts do exist, cf. Moore et al. (2013)). Furthermore, the few corpora that do include animacy in their annotation do not contain much other semantic annotation, making them less interesting for computational semanticists. 2.2 3 Related work In this section we will give an overview of previous work in animacy classification, some of which has inspired the approach presented in this paper. 3.1 Exploiting corpus frequencies A binary animacy classifier which uses syntactic and morphological features has been previously developed for Norwegian and Swedish (Øvrelid, 2005; Øvrelid, 2006; Øvrelid, 2009). The features used are based on fre"
E14-3008,nivre-etal-2006-talbanken05,0,0.0729431,"Missing"
E14-3008,W13-0215,0,0.0140569,"th experts (linguists) and crowd sourcing methods are then added in to improve the annotation. The addition of animacy annotation is done in the same manner. First, the animacy classifier will be incorporated into this toolchain. We then correct the tags for a subset of the corpus, which is also used to evaluate the classifier. Note that the classifier used in the toolchain uses a different model from the conditions where we evaluate on the Switchboard corpus. For the GMB, we include training data obtained through the crowd-sourcing game Wordrobe, which uses a subset of the data from the GMB (Venhuizen et al., 2013). Data Two annotated corpora are used in this work. A further data source is concreteness ratings obtained through manual annotation (Brysbaert et al., 2013), and is used as a feature in the classifier. These ratings were obtained for approximately 40,000 English words and two-word expressions, through the use of internet crowd-sourcing. The rating was given on a five-point scale, ranging from abstract, or language based, to concrete, or experience based (Brysbaert et al., 2013). 4.1 The NXT Switchboard Corpus Firstly, the classifier is trained and evaluated on the Switchboard corpus, as this"
E14-3008,W04-0216,0,0.100527,"Missing"
E14-3008,E06-3008,0,0.0710483,"Missing"
E14-3008,E09-1072,0,0.0182482,"the results achieved here. Quite likely, results should even improve, seeing that the added computational power of ANNs allows us to capture more interesting/deeper statistical patterns, if they exist in the data. The features used in this paper mainly revolved around semantically oriented ones, such as semantic relations from WordNet, thematic roles and, arguably, concreteness ratings. Better results could most likely be achieved if one also incorporated more syntactically oriented features, such as frequency counts from a dependency parsed corpus, as done by e.g. Bowman & Chopra (2012) and Øvrelid (2009). Other options include the use of more linguistically motivated features, such as exploiting relative pronouns (i.e. who vs. which). 8 Conclusions and future work At the beginning of this paper, we set out three aims. Firstly, we wanted to improve upon the state of the art in multi-class animacy classification. A conclusive statement to that effect is hard to make, considering that comparison was only made directly to one previous work. However, as our performance compared to this work was somewhat higher, this work certainly marks some sort of improvement. Secondly, we aimed at investigating"
E14-3008,J05-1004,0,0.245849,"Missing"
E14-3008,P09-1054,0,0.0325736,"Missing"
E14-3008,C98-1013,0,\N,Missing
E17-2039,W13-2322,0,0.0486133,"d sentences in other languages. The aim of this paper is to present a method that implements this idea in practice, by building a parallel corpus with shared formal meaning representations, that is, the Parallel Meaning Bank (PMB). Recently, several semantic resources—corpora of texts annotated with meanings—have been developed to stimulate and evaluate semantic parsing. Usually, such resources are manually or semiautomatically created, and this process is expensive since it requires training of and annotation by human annotators. The AMR banks of Abstract Meaning Representations for English (Banarescu et al., 2013) or Chinese and Czech (Xue et al., 2014) sentences, for instance, are the result of manual annotation efforts. Another example is the development of the Groningen Meaning Bank (Bos et al., 2017), a corpus of English texts annotated with formal, compositional meaning representations, which took advantage of existing semantic parsing tools, combining them with human corrections. In this paper we propose a method for producing meaning banks for several languages (English, Dutch, German and Italian), by taking advantage of translations. On the conceptual level we follow the approach of the Groning"
E17-2039,W07-1401,0,0.00916599,"or selection is that freely distributable texts are preferable over texts which are under copyright and require (paid) licensing. Besides English we chose two other Germanic languages, Dutch and German, because they are similar to English. We also include one Romance language, Italian, in order to test whether our method works for languages which are typologically more different from English. The texts in the PMB are sourced from twelve different corpora from a wide range of genres, including, among others: Tatoeba1 , NewsCommentary (via OPUS, Tiedemann, 2012), Recognizing Textual Entailment (Giampiccolo et al., 2007), Sherlock Holmes stories2 , and the Bible (Christodouloupoulos and Steedman, 2015). These corpora are divided over 100 parts in a balanced way. Initially, two of these parts, 00 and 3.1 Segmentation Text segmentation involves word and sentence boundary detection. Multiword expressions that represent constituents are treated as single tokens. Closed compound words that have a semantically transparent structure are decomposed. For example, impossible is decomposed into im and possible while Las Vegas and 2 pm are analysed as a single token. In this way we aim to assign ‘atomic’ meanings to toke"
E17-2039,E12-2019,1,0.866687,"e et al., 2014) sentences, for instance, are the result of manual annotation efforts. Another example is the development of the Groningen Meaning Bank (Bos et al., 2017), a corpus of English texts annotated with formal, compositional meaning representations, which took advantage of existing semantic parsing tools, combining them with human corrections. In this paper we propose a method for producing meaning banks for several languages (English, Dutch, German and Italian), by taking advantage of translations. On the conceptual level we follow the approach of the Groningen Meaning Bank project (Basile et al., 2012), and use some of the tools developed in it. The main reason for this choice is that we are not only interested in the final meaning of a sentence, but also in how it is derived—the compositional semantics. These derivations, based on Combinatory Categorial Grammar (CCG, Steedman, 2001), give us the means to project semantic information from one sentence to its translated counterpart. The goal of the PMB is threefold. First, it will The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four lang"
E17-2039,C16-1333,1,0.685288,"Missing"
E17-2039,D14-1107,0,0.0183364,"G makes the derivations suitable for widecoverage compositional semantics (Bos et al., 2004). CCG is also a lexicalised theory of grammar, which makes cross-lingual projection of grammatical information from source to target sentence more convenient (see Section 4). The version of CCG that we employ differs from standard CCG: in order to facilitate the crosslingual projection process and retain compositionality, type-changing rules of a CCG parser are explicated by inserting (unprojected) empty elements which have their own semantics (see the token ∅ in Figure 2). For parsing, we use EasyCCG (Lewis and Steedman, 2014), which was chosen because it is accurate, does not require part-of-speech annotation (which would require different annotation schemes for each language) and is easily adaptable to our modified grammar formalism. 3.3 3.4 Universal Semantic Tagging Symbolization The meaning representations that we use contain logical symbols and non-logical symbols. The latter are based on the words mentioned in the input text. We refer to this process as symbolization. It combines lemmatization with normalization, and To facilitate the organization of a wide-coverage semantic lexicon for cross-lingual semanti"
E17-2039,C04-1180,1,0.672748,"Missing"
E17-2039,J03-1002,0,0.00928522,"antic representation in the PMB. It is a well-studied theory from a linguistic semantic viewpoint and suitable for compositional semantics.5 Expressions in DRT, called Discourse 4 http://unece.org/cefact/codesfortrade In particular, we employ Projective DRT (Venhuizen, 2015)—an extension of DRT that accounts for presupposi5 tions, anaphora and conventional implicatures in a generalized way. 245 one-to-one heuristic, with each English sentence aligned to a non-English sentence in order, to be corrected manually. Subsequently, we automatically align words in the aligned sentences using GIZA ++ (Och and Ney, 2003). Although we use existing tools for the initial annotation of English and projection as the initial annotation of non-English documents, our aim is to train new language-neutral models. Training new models on just the automatic annotation will not yield better performance than the combination of existing tools and projection. However, we improve these models constantly by adding manual corrections to the initial automatic annotation, and retraining them. In addition, this approach lets us adapt to revisions of the annotation guidelines. 5 annotation conflicts are then slated for resolution by"
E17-2039,W15-1841,1,0.476422,"ing knowledge sources, such as WordNet (Fellbaum, 1998), Wikipedia, and UNECE codes for trade4 , to do symbolization. We are currently investigating how the performance of machine learning-based symbolizer compares to a rule-based one incorporating the lemmatizer Morpha (Minnen et al., 2001). 3.5 Representation Structures (DRSs), have a recursive structure and are usually depicted as boxes. An upper part of a DRS contains a set of referents while the lower part lists a conjunction of atomic or compound conditions over these referents (see an example of a DRS in the bottom of Figure 2). Boxer (Bos, 2015), a system that employs λ-calculus to construct DRSs in a compositional way, is used to derive meaning representations of the documents. However, the original version of Boxer is tailored to the English language. We have adapted Boxer to work with the universal semtags rather than English-specific part-ofspeech tags. Boxer also assigns VerbNet/LIRICS thematic roles (Bonial et al., 2011) to verbs so that the lexical semantics of verbs include the corresponding thematic predicates (see came in Figure 2). Hence an input to Boxer is a CCG derivation where all tokens are decorated with semtags and"
E17-2039,tiedemann-2012-parallel,0,0.00956183,"s sufficient for our purposes. Another criterion for selection is that freely distributable texts are preferable over texts which are under copyright and require (paid) licensing. Besides English we chose two other Germanic languages, Dutch and German, because they are similar to English. We also include one Romance language, Italian, in order to test whether our method works for languages which are typologically more different from English. The texts in the PMB are sourced from twelve different corpora from a wide range of genres, including, among others: Tatoeba1 , NewsCommentary (via OPUS, Tiedemann, 2012), Recognizing Textual Entailment (Giampiccolo et al., 2007), Sherlock Holmes stories2 , and the Bible (Christodouloupoulos and Steedman, 2015). These corpora are divided over 100 parts in a balanced way. Initially, two of these parts, 00 and 3.1 Segmentation Text segmentation involves word and sentence boundary detection. Multiword expressions that represent constituents are treated as single tokens. Closed compound words that have a semantically transparent structure are decomposed. For example, impossible is decomposed into im and possible while Las Vegas and 2 pm are analysed as a single to"
E17-2039,C16-1056,1,0.83036,"igure 2). 4 Cross-lingual Projection The initial annotation for Dutch, German and Italian is bootstrapped via word alignments. Each non-English text is automatically word-aligned with its English counterpart, and non-English words initially receive semtags, CCG categories and symbols based on those of their English counterparts (see Figure 2). CCG slashes are flipped as needed, and 2:1 alignments are handled through functional composition. Then, the CCG derivations and DRSs can be obtained by applying CCG’s combinatory rules in such a way that the same DRS as for the English sentence results (Evang and Bos, 2016; Evang, 2016). If the alignment is incorrect, it can be corrected manually (see Section 5). The idea behind this way of bootstrapping is to exploit the advanced state-of-the-art of NLP for English, and to encourage parallelism between the syntactic and semantic analyses of different languages. To facilitate cross-lingual projection, alignment has to be done at two levels: sentences and words. Sentence alignment is initially done with a simple Semantic Interpretation Discourse Representation Theory (DRT, Kamp and Reyle, 1993), is the semantic formalism that is used as a semantic representation"
E17-2039,xue-etal-2014-interlingua,0,0.0257014,"s paper is to present a method that implements this idea in practice, by building a parallel corpus with shared formal meaning representations, that is, the Parallel Meaning Bank (PMB). Recently, several semantic resources—corpora of texts annotated with meanings—have been developed to stimulate and evaluate semantic parsing. Usually, such resources are manually or semiautomatically created, and this process is expensive since it requires training of and annotation by human annotators. The AMR banks of Abstract Meaning Representations for English (Banarescu et al., 2013) or Chinese and Czech (Xue et al., 2014) sentences, for instance, are the result of manual annotation efforts. Another example is the development of the Groningen Meaning Bank (Bos et al., 2017), a corpus of English texts annotated with formal, compositional meaning representations, which took advantage of existing semantic parsing tools, combining them with human corrections. In this paper we propose a method for producing meaning banks for several languages (English, Dutch, German and Italian), by taking advantage of translations. On the conceptual level we follow the approach of the Groningen Meaning Bank project (Basile et al.,"
E17-2039,D13-1146,1,0.743699,"ion. Multiword expressions that represent constituents are treated as single tokens. Closed compound words that have a semantically transparent structure are decomposed. For example, impossible is decomposed into im and possible while Las Vegas and 2 pm are analysed as a single token. In this way we aim to assign ‘atomic’ meanings to tokens and avoid redundant lexical semantics. Segmentation follows an IOB-annotation scheme on the level of characters, with four labels: beginning of sentence, beginning of word, inside a word, and outside a word. We use the same statistical tokenizer, Elephant (Evang et al., 2013), for all four languages, but with language-specific models. 1 https://tatoeba.org http://gutenberg.org, http://etc.usf. edu/lit2go, http://gutenberg.spiegel.de 2 243 NP SNP He came back male come back ((SNP )(SNP ))/NP EPS at ∅ 5 o’clock ∅ 17 : 00 IST REL DIS (SNP )(SNP ) ((SNP )(SNP ))/NP NP/N at CLO N λV Gp.V G(λx.D ; px) λGVHp.V H(λx.G(λy.D ; px)) λpq.D ; (px; qx) s M anner(x, s) back(s) SNP x y e s t1 t2 come(e) T ime(e, t2 ) now(t1 ) t2 &lt; t 1 λx.D = e t1 t2 come(e) T heme(e, x) T ime(e, t2 ) now(t1 ) t2 &lt; t1 zur¨uck f¨unf Uhr = x male(x) = SNP λGp.G(λx.D ; pe) = NP λp.D ∗ px"
J19-2006,N18-1083,1,0.697995,"rds can be embedded, so can languages, by associating each language with a real-valued vector known as a language representation, which can be used to measure similarities between languages. This type of representation can be obtained by, for example, training a multilingual model for ¨ some NLP task (Johnson et al. 2017; Malaviya, Neubig, and Littell 2017; Ostling and Tiedemann 2017). The focus of this work is on the evaluation of similarities between such representations. This is an important area of work, as computational approaches to typology (Dunn et al. 2011; Cotterell and Eisner 2017; Bjerva and Augenstein 2018) have the potential to answer research questions on a much larger scale than traditional typological research (Haspelmath 2001). Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts. For instance, if the similarities between languages in an embedded language space were to be found to encode geographical distances (Figure 1), any conclusions drawn from use of these representations would not likely be of much use for most NLP tasks. The importan"
J19-2006,W17-6508,0,0.029981,"gly correlated with genealogical distance, significant differences can be observed. Romanian, as a member of the Balkan sprachbund, is distinct from the other Romance languages. The North Germanic (Danish, Swedish) and West Germanic (Dutch, German) branches are separated through considerable structural differences, with English grouped with the North Germanic languages despite its West Germanic origin. The Baltic languages (Latvian, Lithuanian) are grouped with the nearby Finnic languages (Estonian, Finnish) rather than their distant Slavic relatives. This idea has been explored previously by Chen and Gerdes (2017), who use a combination of relative frequency, length, and direction of deprels. We, by comparison, achieve an even richer representation by also taking head and dependent POS into account. 386 Bjerva et al. What Do Language Representations Really Represent? Figure 4 Correlations between similarities (Genetic, Geo., and Struct.) and language representations (Raw, Func, POS, Phrase, Deprel). Significance at p &lt; 0.001 is indicated by *. 5. Analysis of Similarities Although we are able to reconstruct phylogenetic language trees in a similar manner to previous work, we wish to investigate whether"
J19-2006,P17-1109,0,0.0302127,"ddings. Similarly to how words can be embedded, so can languages, by associating each language with a real-valued vector known as a language representation, which can be used to measure similarities between languages. This type of representation can be obtained by, for example, training a multilingual model for ¨ some NLP task (Johnson et al. 2017; Malaviya, Neubig, and Littell 2017; Ostling and Tiedemann 2017). The focus of this work is on the evaluation of similarities between such representations. This is an important area of work, as computational approaches to typology (Dunn et al. 2011; Cotterell and Eisner 2017; Bjerva and Augenstein 2018) have the potential to answer research questions on a much larger scale than traditional typological research (Haspelmath 2001). Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts. For instance, if the similarities between languages in an embedded language space were to be found to encode geographical distances (Figure 1), any conclusions drawn from use of these representations would not likely be of much use for"
J19-2006,D18-1029,0,0.102725,"Missing"
J19-2006,2005.mtsummit-papers.11,0,0.200565,"lations as, e.g., speakers from certain 195 195 nt tothe abstract away from the surface forms of distance measures provide more explanation (RQ2). distance measures provide more explanation (RQ2). 194 tigate representation learning on monolingual English sentences, which are translations are captured by language representations, or if other talksame about thecertain same issues. We therefore dtions to will talk about the issues. We therefore 196 196 as,tend e.g.,tospeakers from regions 195 distance provide more explanation from various source languages English from the (RQ2). Europarl corpus (Koehn 2005). 4.1toGenetic Distance introduce several levels ofWe abstraction: i) training onmeasures 4.1 Genetic Distance eo several levels of abstraction: i) training on 197 talk about the same issues. therefore 197 196 They use a feature-engineering approach to predict source languages and learn an 198 Following Rabinovich al. (2017), phylo4.1 etFollowing Genetic Distance several1This levels of abstraction: i) training is the exactassame used byon Rabinovich al. 198 Rabinovich et al.et(2017), we we use use phylo197 is the exact same data useddata by asRabinovich et al. tree Indo-European family using t"
J19-2006,D17-1268,0,0.260431,"Missing"
J19-2006,E17-2102,1,0.817678,"national (CC BY-NC-ND 4.0) license Computational Linguistics Volume 45, Number 2 1. Introduction Words can be represented with distributed word representations, currently often in the form of word embeddings. Similarly to how words can be embedded, so can languages, by associating each language with a real-valued vector known as a language representation, which can be used to measure similarities between languages. This type of representation can be obtained by, for example, training a multilingual model for ¨ some NLP task (Johnson et al. 2017; Malaviya, Neubig, and Littell 2017; Ostling and Tiedemann 2017). The focus of this work is on the evaluation of similarities between such representations. This is an important area of work, as computational approaches to typology (Dunn et al. 2011; Cotterell and Eisner 2017; Bjerva and Augenstein 2018) have the potential to answer research questions on a much larger scale than traditional typological research (Haspelmath 2001). Furthermore, having knowledge about the relationships between languages can help in NLP applications (Ammar et al. 2016), and having incorrect interpretations can be detrimental to multilingual NLP efforts. For instance, if the sim"
J19-2006,P17-1049,0,0.0968921,"Missing"
J19-2006,L16-1680,0,0.0311049,"Missing"
J19-2006,P15-2034,1,\N,Missing
J19-2006,Q17-1024,0,\N,Missing
K17-2012,P16-1009,0,0.0988365,"Missing"
K17-2012,W16-2002,0,0.0653565,"ining data available in the shared task (i.e., excluding Scottish Gaelic). 2 Method 1. Fully supervised, where we simply train the forward (inflection) and backward (lemmatization and morphological analysis) model jointly with shared character embeddings. 2. Semi-supervised, where supervised examples are mixed with examples where only the inflected target form is used. This form is passed first through the backward model, a greedy search to obtain a unique lemma, and finally through the forward model to reconstruct the inflected form. Background The results of the SIGMORPHON 2016 shared task (Cotterell et al., 2016) indicated that the attentional sequence-to-sequence model of Bahdanau et al. (2014) is very suitable for this task (Kann and Sch¨utze, 2016), so we use this framework as the basis of our model. Our official submission only includes results from fully supervised training (method 1), due to time constraints, but Section 5 contains a comparison between the two versions on the development set. The system architecture is shown in Figure 1 for the forward (inflection) model. The backward ∗ This work was carried out while the second author was visiting the Department of Linguistics, Stockholm Univer"
K18-3011,P17-1182,0,0.0605568,"Missing"
K18-3011,W16-2010,0,0.138201,"Missing"
K18-3011,D18-1543,1,0.841915,"Missing"
K18-3011,P16-2067,0,0.0639877,"Missing"
K18-3011,K17-2002,0,0.036597,"Missing"
K18-3011,P16-2038,0,0.0797609,"Missing"
K18-3011,W17-0225,1,0.857404,"Missing"
K18-3011,C16-1333,1,0.89768,"Missing"
N18-1083,D17-1011,0,0.0714889,"Missing"
N18-1083,W18-0207,1,0.707325,"1957), summarised in the catchy phrase You shall know a word by the company it keeps (Firth, 1957). We argue that language embeddings, or distributed representations of language, can also be theoretically motivated by Chomsky’s Principles and Parameters framework (Chomsky and Lasnik, 1993; Chomsky, 1993, 2014). Language embeddings encode languages as dense real-valued vectors, in which the dimensions are reminiscent of the parameters found in this framework. Briefly put, Chomsky argues that languages can be described in terms of principles Contributions Our work bears the most resemblance to Bjerva and Augenstein (2018), who finetune language embeddings on the task of PoS tagging, and investigate how a handful of typological properties are coded in these for four Uralic languages. We expand on this and thus contribute to previous work by: (i) introducing novel qualitative investigations of language embeddings, in addition to thorough quantitative evaluations; (ii) considering four tasks at three different typological levels; (iii) considering a far larger sample of 908 (abstract rules) and parameters (switches) which can be turned either on or off for a given language (Chomsky and Lasnik, 1993). An example o"
N18-1083,P17-1109,0,0.0851896,"eural computational models. Even so, recent work only deals with fragments of typology compared to what we consider here. Computational typology has to a large extent focused on exploiting word or morpheme alignments on the massively parallel New Testament, in approximately 1,000 languages, in order to in¨ fer word order (Ostling, 2015) or assign linguistic categories (Asgari and Sch¨utze, 2017). W¨alchli (2014) similarly extracts lexical and grammatical markers using New Testament data. Other work has taken a computational perspective on language evolution (Dunn et al., 2011), and phonology (Cotterell and Eisner, 2017; Alishahi et al., 2017). Language embeddings In this paper, we follow an approach which has seen some attention in the past year, namely the use of distributed language representations, or language embeddings. Some ¨ typological experiments are carried out by Ostling and Tiedemann (2017), who learn language embeddings via multilingual language modelling and show that they can be used to reconstruct genealogical trees. Malaviya et al. (2017) learn language embeddings via neural machine translation, and predict syntactic, morphological, and phonetic features. 3.1.1 Language embeddings as contin"
N18-1083,K17-2001,0,0.0843525,"entations by training a recurrent neural language model (Mikolov et al., 2010) simultaneously for different languages ¨ (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017). In these recurrent multilingual language models with long short-term memory cells (LSTM, Hochreiter and Schmidhuber, 1997), languages are embedded into an n-dimensional space. In order for multilingual parameter sharing to be successful in this setting, the neural network is encouraged to use the language embeddings to encode features of language. In this pa¨ per, we explore the embeddings trained by Ostling and Tiedemann (2017), both in their original state, and by further tuning them for our four tasks. These are trained by training a multilingual language model with language representations on a collection of texts from the New Testament, covering 975 languages. While other work has looked at the types of representations encoded in different layers of deep neural models (K´ad´ar et al., 2017), we choose to look at the representations only in the bottom-most embedding layer. This is motivated by the fact that we look at several tasks using different neural architectures, and want to ensure comparability between the"
N18-1083,P16-1038,0,0.0438529,"uage embedding set on the typological property relevant to that dataset. In addition, we also evaluate on a set of all typological properties in WALS. Note that the evaluation on all properties is only comparable to the evaluation on each specific property, as the set of languages under consideration differs between tasks. Dataset Class G2P ASJP SIGMORPHON UD Phonology Phonology Morphology Syntax |Ltask | 311 4,664 52 50 |Ltask ∩ Lpre | 102 824 29 27 Table 1: Overview of tasks and datasets. 5 Phonology 5.1 Grapheme-to-phoneme We use grapheme-to-phoneme (G2P) as a proxy of a phonological task (Deri and Knight, 2016; Peters et al., 2017). The dataset contains over 650,000 such training instances, for a total of 311 languages (Deri and Knight, 2016). The task is to produce a phonological form of a word, given its orthographic form and the language in which it is written. Crucially, this mapping is highly different depending on the language at hand. For instance, take the word written variation, which exists in both English and French: (English, variation) -&gt; vE@ri""eIS@n (French, variation) -&gt; vaKja""sj˜O 5.1.1 Experiments and Analysis We train a sequence-to-sequence model with attention for the task of g"
N18-1083,K17-2012,1,0.887927,"17). All PoS tagging results tasks (morphological inflection, G2P,147 and phonological the morphological system, using grapheme-toported are the average of five runs, each with reconstruction). Figure adapted with148 permissiontofrom phoneme data. ferent initialisation seeds, so as to minimise r 149 ¨ 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 128 3 Morphology Unimorph 131 132 133 134 135 136 137 138 139 140 141 142 3.2 Morphological Experiments We train a sequence-to-sequence model based ¨ on the system developed by Ostling and Bjerva (2017). The neural architecture is modified so as to include an embedded language representation. During training, the errors are also backpropagated into this embedding, meaning that the encoded representation will be fine-tuned as the task is learned. The system architecture is depicted in Figure ??. ~l 4 Phonology 143 4.1 Grapheme-to-phoneme 144 g2p data 145 146 147 148 149 6 Method and experiments We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture, based on Plank et al. (2016). The system is implemented using DyNet (Neubig et al., 2017). We train using t"
N18-1083,E17-2102,0,0.128552,"¨ Ostling, 2015; Cotterell and Eisner, 2017; Asgari and Sch¨utze, 2017; Malaviya et al., 2017; Bjerva and Augenstein, 2018). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., ¨ 2016; Ostling and Tiedemann, 2017; Malaviya et al., 2017). We hypothesise that these language embeddings encode typological properties of language, reminiscent of the features in WALS, or even of parameters in Chomsky’s Principles and Parameters framework (Chomsky, 1993). In this paper, we model languages in deep neural networks using language embeddings, considering three typological levels: phonology, morphology and syntax. We consider four NLP tasks to be representative of these levels: grapheme-tophoneme prediction and phoneme reconstruction, morphological inflection, and part-of-speech tagging. We pose three research que"
N18-1083,W17-5403,0,0.179856,"Missing"
N18-1083,P16-2067,0,0.421153,"Most Frequent Accuracy Accuracy 1.0 k-NN Most Frequent 0.4 0.2 0 200 Iterations 400 600 0.0 0 200 Iterations 400 600 Figure 2: Prediction of two morphological features in WALS with morphological language embeddings, one data point per 50 iterations. we are mainly interested in observing the language embeddings, we down-sample all training sets to 1,500 sentences (approximate number of sentences of the smallest data sets) so as to minimise any size-related effects. 7.1.1 Word-order experiments We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture based on Plank et al. (2016), detailed in Section 8. Quantitative results Table 5 contains results on WALS feature prediction using language embeddings fine-tuned on PoS tagging. We consider both the set of word order features, which are relevant for the dataset, and a set of all WALS features. Using the fine-tuned embeddings is significantly better than both the baseline and the pre-trained embeddings (p < 0.05), in both the random and the unseen conditions, indicating that the model learns something about word order typology. This can be expected, as word order features are highly relevant when assigning a PoS tag to a"
N18-1083,D17-1268,0,0.326748,"d Eisner, 2017; Asgari and Sch¨utze, 2017; Malaviya et al., 2017; Bjerva and Augenstein, 2018). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., ¨ 2016; Ostling and Tiedemann, 2017; Malaviya et al., 2017). We hypothesise that these language embeddings encode typological properties of language, reminiscent of the features in WALS, or even of parameters in Chomsky’s Principles and Parameters framework (Chomsky, 1993). In this paper, we model languages in deep neural networks using language embeddings, considering three typological levels: phonology, morphology and syntax. We consider four NLP tasks to be representative of these levels: grapheme-tophoneme prediction and phoneme reconstruction, morphological inflection, and part-of-speech tagging. We pose three research questions (RQs): A core par"
N18-1083,N16-1161,0,0.341773,"guage representations differ considerably depending on the target task. For instance, for grapheme-to-phoneme mapping, the differences between the representations for Norwegian Bokm˚al and Danish increase rapidly during training. This is due to the fact that, although the languages are typologically close to one another, they are phonologically distant. 2 3 Background 3.1 Distributed language representations There are several methods for obtaining distributed language representations by training a recurrent neural language model (Mikolov et al., 2010) simultaneously for different languages ¨ (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017). In these recurrent multilingual language models with long short-term memory cells (LSTM, Hochreiter and Schmidhuber, 1997), languages are embedded into an n-dimensional space. In order for multilingual parameter sharing to be successful in this setting, the neural network is encouraged to use the language embeddings to encode features of language. In this pa¨ per, we explore the embeddings trained by Ostling and Tiedemann (2017), both in their original state, and by further tuning them for our four tasks. These are trained by training a multilingual language mod"
N18-1083,P15-2034,0,0.0903062,"is motivated by the fact that we look at several tasks using different neural architectures, and want to ensure comparability between these. Related work Computational linguistics approaches to typology are now possible on a larger scale than ever before due to advances in neural computational models. Even so, recent work only deals with fragments of typology compared to what we consider here. Computational typology has to a large extent focused on exploiting word or morpheme alignments on the massively parallel New Testament, in approximately 1,000 languages, in order to in¨ fer word order (Ostling, 2015) or assign linguistic categories (Asgari and Sch¨utze, 2017). W¨alchli (2014) similarly extracts lexical and grammatical markers using New Testament data. Other work has taken a computational perspective on language evolution (Dunn et al., 2011), and phonology (Cotterell and Eisner, 2017; Alishahi et al., 2017). Language embeddings In this paper, we follow an approach which has seen some attention in the past year, namely the use of distributed language representations, or language embeddings. Some ¨ typological experiments are carried out by Ostling and Tiedemann (2017), who learn language em"
N18-1083,J17-4003,0,\N,Missing
N18-1083,L16-1262,0,\N,Missing
N18-1083,K17-1037,0,\N,Missing
N19-1156,E17-2102,0,0.104682,"ded, so can languages, by associating each language with a real-valued vector known as a language embedding. Training such representations as a part of a multilingual model allows us to infer similarities between languages. This is due to the fact that in order for multilingual parameter sharing to be successful in this setting, the neural network needs to use the language embeddings to encode features of the languages. Previous work has explored this type of representation learning in various tasks, such as NMT (Malaviya et al., 2017), ¨ language modelling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017), and tasks representing morphological, phonological, and syntactic linguistic levels (Bjerva and Augenstein, 2018a). In the context of computational typology, representations obtained through language modelling ¨ have been the most successful (Ostling and Tiedemann, 2017). This approach is particularly interesting since unlabelled data is available for a large portion of the world’s languages, meaning that high quality language embeddings can be obtained for more than 1,000 of the world’s languages. Language Embeddings through LMs In this work, we use a language modelling objective to pre-tra"
N19-1156,P14-1130,0,0.0321674,"ar (Breese et al., 1998) to overcome the cold start problem arising for unseen users or items at test time. The most successful one of these, in turn, is matrix factorisation, as applied in this paper, which represents users and items as (dense) vectors in the same latent feature space and measures their compatibility by taking the dot product between the two representations (Koren et al., 2009; Bokde et al., 2015). Beyond recommender systems, matrix factorisation has shown successes in a wide variety of subareas of NLP (Riedel et al., 2013; Rockt¨aschel et al., 2015; Levy and Goldberg, 2014; Lei et al., 2014; Augenstein et al., 2018). 10 Conclusion We introduce a generative model inspired by the principles-and-parameters framework, drawing on the correlations between typological features of languages to solve the novel task of typological collaborative filtering. We further show that raw text can be utilised to infer similarities between languages, thus allowing for extending the method with semi-supervised language embeddings. Acknowledgements We acknowledge the computational resources provided by CSC in Helsinki through NeIC-NLPL (www.nlpl.eu), and the support of the NVIDIA Corporation with the"
N19-1156,D17-1268,0,0.167387,"rrently often in the form of word embeddings. Similarly to how words can be embedded, so can languages, by associating each language with a real-valued vector known as a language embedding. Training such representations as a part of a multilingual model allows us to infer similarities between languages. This is due to the fact that in order for multilingual parameter sharing to be successful in this setting, the neural network needs to use the language embeddings to encode features of the languages. Previous work has explored this type of representation learning in various tasks, such as NMT (Malaviya et al., 2017), ¨ language modelling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017), and tasks representing morphological, phonological, and syntactic linguistic levels (Bjerva and Augenstein, 2018a). In the context of computational typology, representations obtained through language modelling ¨ have been the most successful (Ostling and Tiedemann, 2017). This approach is particularly interesting since unlabelled data is available for a large portion of the world’s languages, meaning that high quality language embeddings can be obtained for more than 1,000 of the world’s languages. Language Embeddings"
N19-1156,N13-1095,0,0.0145181,"s. 6 Data WALS. The World Atlas of Language Structures (WALS) is a large knowledge base of typological properties at the lexical, phonological, syntactic and semantic level on which we will run our experiments. The documentation of linguistic structure is spread throughout a wide variety of academic works, ranging from field linguistics to grammars describing the nuances of individual grammatical uses. KB creation is a laborious tasks as it involves distilling knowledge into a single, standardised resource, which, naturally, will be incomplete, prompting the need for methods to complete them (Min et al., 2013). In the case of WALS, few languages have all values annotated for all of the properties. In this section, we offer a formalisation of typological KBs to allow for our development of a probabilistic model over vectors of properties. WALS, for instance, contains n = 202 different parameters (Dryer and Haspelmath, 2013). Binarisation of WALS. Many common typological KBs, including WALS, the one studied here, contain binary as well as non-binary parameters. To deal with this, we binarise the KB as follows: Whenever there is a typological parameter that takes ≥ 3 values, e.g., ‘Feature 81A: Order"
N19-1156,I17-1046,0,0.0669546,"rvey of typology in NLP, see Ponti et al. (2018). Figure 7: Accuracy per feature group (Germanic). stein (2018a), might also be useful in our semisupervised extension of typological collaborative filtering. 9 Related Work Computational Typology The availability of unlabelled datasets for hundreds of languages permits inferring linguistic properties and categories ¨ (Ostling, 2015; Asgari and Sch¨utze, 2017). Individual prediction of typological features has been attempted in conjunction with several NLP tasks (Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b). Our work is most similar to Murawaki (2017), who presents a Bayesian approach to utilising relations between features and languages for feature prediction. However, our work differs on several important counts, as we (i) include language information obtained through unsupervised learning, which allows us to take advantage of raw data and predict features for completely unannotated languages, (ii) analyse the effects of varying amounts of known features, especially in situations with and without in-branch training data, and (iii) view the problem of typological features through the lens of parameters from principles and parameters (Chom"
N19-1156,N13-1008,0,0.312537,"oes the language admit word-final voiced obstruents? 3 A Generative Model of Typology We now seek a probabilistic formalisation of the linguistic theory presented in §2; specifically, for every language `, we seek to explain the observed binary vector of parameters π ` : πi` = 1 indicates that the ith parameter is “on” in language `. The heart of our model will be quite simple: every language ` will have a language embedding λ` ∈ Rd and every parameter will have a parameter embed` ding eπi ∈ Rd . Now, πi` ∼ sigmoid(e&gt; πi λ ). This model also takes inspiration from work in relation extraction (Riedel et al., 2013). Writing the joint distribution over the entire binary vector of parameters, we arrive at p(π ` |λ` ) = |π| Y (1)   ` sigmoid e&gt; πi λ (2) i=1 = |π| Y i=1 = |π| Y 1 ` 1 + exp(−e&gt; πi λ ) i=1 (3) 1 For an overview of differences between these schools, we refer the reader to Haspelmath (2008). `∈L Note that p(λ` ) is, spiritually at least, a universal grammar: it is the prior over what sort of languages can exist, albeit encoded as a real vector. In the parlance of principles and parameters, the prior represents the principles. Then our model parameters are Θ = {eπ1 , . . . , eπ|π |, λ1 , . . ."
N19-1156,N15-1118,0,0.0408962,"Missing"
P19-1382,D17-1011,0,0.030622,"Missing"
P19-1382,N18-1083,1,0.898096,"d of knowledge base population. Path Ranking Algorithm (PRA) is an algorithm which finds relation paths by traversing the knowledge graph, which can then be used to predict implicatures and feature values (Lao and Cohen, 2010; Lao et al., 2011).1 We train PRA using the standard hyperparameters of the existing implementation, which includes regularising with `1 = 0.001 and `2 = 0.001, as well as using negative sampling. Baseline #4: Language embeddings Although we aim to predict implications, and not only feature values, we compare with previous work on predicting typological features in WALS (Bjerva and Augenstein, 2018a). As their setup is different, we use their highest reported score as a baseline. Feature Prediction Results. Table 1 contains the results from feature prediction across the chapters outlined in WALS. Our implementation is able to predict features across categories above baseline levels. At increasing numbers of implicants, prediction power tends to increase. This is not the case for all feature categories, however. One such case is Nominal Syntax, in which performance peaks at 3 implicants. This is expected, as correlations only exist between some features, thus at a certain point access to"
P19-1382,W18-0207,1,0.510671,"d of knowledge base population. Path Ranking Algorithm (PRA) is an algorithm which finds relation paths by traversing the knowledge graph, which can then be used to predict implicatures and feature values (Lao and Cohen, 2010; Lao et al., 2011).1 We train PRA using the standard hyperparameters of the existing implementation, which includes regularising with `1 = 0.001 and `2 = 0.001, as well as using negative sampling. Baseline #4: Language embeddings Although we aim to predict implications, and not only feature values, we compare with previous work on predicting typological features in WALS (Bjerva and Augenstein, 2018a). As their setup is different, we use their highest reported score as a baseline. Feature Prediction Results. Table 1 contains the results from feature prediction across the chapters outlined in WALS. Our implementation is able to predict features across categories above baseline levels. At increasing numbers of implicants, prediction power tends to increase. This is not the case for all feature categories, however. One such case is Nominal Syntax, in which performance peaks at 3 implicants. This is expected, as correlations only exist between some features, thus at a certain point access to"
P19-1382,N19-1156,1,0.624091,"rtain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features, and then marginalising out all features other than the two under consideration. This computation is made tractable by learning a tree-structured graphical model (Figure 1) with the PC algorithm of Neapolitan (2004) and then applying the belief propagation (BP) algorithm (Pearl, 1982). We draw inspiration from manual linguistic efforts to this problem (Greenberg, 1963; Lehmann, 1978), as well as from previous computational methods (Daum´e III and Campbell, 2007; Bjerva et al., 2019a). Additionally, we provide a qualitative analysis of predicted implications, as well as performing an empirical evaluation on typological feature prediction, com3924 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3924–3930 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics paring to strong baselines. 2 From A Generative Model to Probabilistic Implications Notation. We now seek a probabilistic formalisation of typological implications. First, we will introduce the relevant notation. Let ` be a language. W"
P19-1382,P07-1009,0,0.36283,"Missing"
P19-1382,C10-1044,0,0.0875114,"Missing"
P19-1382,J19-2006,1,0.853583,"rtain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features, and then marginalising out all features other than the two under consideration. This computation is made tractable by learning a tree-structured graphical model (Figure 1) with the PC algorithm of Neapolitan (2004) and then applying the belief propagation (BP) algorithm (Pearl, 1982). We draw inspiration from manual linguistic efforts to this problem (Greenberg, 1963; Lehmann, 1978), as well as from previous computational methods (Daum´e III and Campbell, 2007; Bjerva et al., 2019a). Additionally, we provide a qualitative analysis of predicted implications, as well as performing an empirical evaluation on typological feature prediction, com3924 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3924–3930 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics paring to strong baselines. 2 From A Generative Model to Probabilistic Implications Notation. We now seek a probabilistic formalisation of typological implications. First, we will introduce the relevant notation. Let ` be a language. W"
P19-1382,D11-1049,0,0.0918847,"Missing"
P19-1382,P17-1109,1,0.846546,"typological implications. In addition to work on finding implications based on known features, there is an increasing amount of work on computational methods to discovering typological features (Ponti et al., 2018). Work in this area includes unsupervised discovery of word ¨ order (Ostling, 2015) or other linguistic features (Asgari and Sch¨utze, 2017), typological probing of language representations (Bjerva et al., 2019b; Beinborn and Choenni, 2019), and several papers attempt to predict typological features in WALS (Georgi et al., 2010; Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b; Cotterell and Eisner, 2017, 2018; Bjerva et al., 2019a). 6 Conclusions We defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further investigation by typologists. Additionally, our approach outperforms strong baselines for prediction of typological features. Acknowledgments We acknowledge the computational resources provided by CSC in Helsinki through NeIC-NLPL (www.nlpl.eu), and the support of the NVIDIA Corporation with the donation of the Titan Xp GPU used for this"
P19-1382,D18-1543,1,0.696833,"Missing"
P19-1382,N18-1004,1,0.902651,"Missing"
P19-1382,D17-1268,0,0.257826,"7), we are the first to introduce a probabilisation of typological implications. In addition to work on finding implications based on known features, there is an increasing amount of work on computational methods to discovering typological features (Ponti et al., 2018). Work in this area includes unsupervised discovery of word ¨ order (Ostling, 2015) or other linguistic features (Asgari and Sch¨utze, 2017), typological probing of language representations (Bjerva et al., 2019b; Beinborn and Choenni, 2019), and several papers attempt to predict typological features in WALS (Georgi et al., 2010; Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b; Cotterell and Eisner, 2017, 2018; Bjerva et al., 2019a). 6 Conclusions We defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further investigation by typologists. Additionally, our approach outperforms strong baselines for prediction of typological features. Acknowledgments We acknowledge the computational resources provided by CSC in Helsinki through NeIC-NLPL (www.nlpl.eu), and the support of the NVIDIA Corp"
P19-1382,P15-2034,0,0.0142401,"this direction has been manual, typological knowledge bases do exist now (Dryer and Haspelmath, 2013; Partick Littel and Levin, 2016), which allows for automated discovery of implications. Although previous computational work exists (Daum´e III and Campbell, 2007), we are the first to introduce a probabilisation of typological implications. In addition to work on finding implications based on known features, there is an increasing amount of work on computational methods to discovering typological features (Ponti et al., 2018). Work in this area includes unsupervised discovery of word ¨ order (Ostling, 2015) or other linguistic features (Asgari and Sch¨utze, 2017), typological probing of language representations (Bjerva et al., 2019b; Beinborn and Choenni, 2019), and several papers attempt to predict typological features in WALS (Georgi et al., 2010; Malaviya et al., 2017; Bjerva and Augenstein, 2018a,b; Cotterell and Eisner, 2017, 2018; Bjerva et al., 2019a). 6 Conclusions We defined the notion of probabilistic implications, and presented a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further investigati"
P19-1382,E17-2102,0,0.153608,"er workings of language and define the space of plausible languages. Universals can aid cognitive scientists examining the underlying processes of language, as there arguably is a cognitive reason for why, e.g., languages with OV ordering are postpositional (Greenberg, 1963). In the context of natural language processing (NLP), when creating synthetic data for multilingual NLP, one should consider universals to maintain the plausibility of the data (Wang and Eisner, 2016). Computational typology can furthermore be used to induce language representations, useful in, e.g., lan¨ guage modelling (Ostling and Tiedemann, 2017) and syntactic parsing (de Lhoneux et al., 2018). In this paper, we argue that the deterministic Greenbergian view of implications (Greenberg, 1963) is outdated. Instead, we suggest that a probabilistic view of implications is more suitable, and define the notion of a probabilistic typological implication as a certain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features, and then marginalising out all features other than the two under consideration. This computation is made tractable by learning a tree-structured graphic"
P19-1382,Q16-1035,0,0.0272503,"e the presence of one feature strictly implies the presence of another. Universals are important to investigate as they offer insight into the inner workings of language and define the space of plausible languages. Universals can aid cognitive scientists examining the underlying processes of language, as there arguably is a cognitive reason for why, e.g., languages with OV ordering are postpositional (Greenberg, 1963). In the context of natural language processing (NLP), when creating synthetic data for multilingual NLP, one should consider universals to maintain the plausibility of the data (Wang and Eisner, 2016). Computational typology can furthermore be used to induce language representations, useful in, e.g., lan¨ guage modelling (Ostling and Tiedemann, 2017) and syntactic parsing (de Lhoneux et al., 2018). In this paper, we argue that the deterministic Greenbergian view of implications (Greenberg, 1963) is outdated. Instead, we suggest that a probabilistic view of implications is more suitable, and define the notion of a probabilistic typological implication as a certain conditional probability distribution. We do this by first placing a joint distribution over the vector of typological features,"
S14-2114,S14-2001,0,0.127342,"t and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. Introduction The recent popularity of employing distributional approaches to semantic interpretation has also lead to interesting questions about the relationship between classic formal semantics (including its computational adaptations) and statistical semantics. A promising way to provide insight into these questions was brought forward as Shared Task 1 in the SemEval-2014 campaign for semantic evaluation (Marelli et al., 2014). In this task, a system is given a set of sentence pairs, and has to predict for each pair whether the sentences are somehow related in meaning. Interestingly, this is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations u"
S14-2114,W08-2222,1,0.883349,"hether the sentences are somehow related in meaning. Interestingly, this is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations used for textual entailment are 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1 To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&C tools. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings"
S14-2114,P07-2009,1,0.612665,"s is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations used for textual entailment are 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1 To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&C tools. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org"
S14-2114,N13-1092,0,0.217137,"works as follows: (i) produce a formal semantic representation for each sentence for a given sentence pair; (ii) translate these semantic representations into first-order logic; (iii) use off-theshelf theorem provers and model builders to check whether the first sentence entails the second, or whether the sentences are contradictory. This is essentially an improved version of the framework introduced by Bos & Markert (2006). To generate background knowledge that could assist in finding a proof we used the lexical database WordNet (Fellbaum, 1998). We also used a large database of paraphrases (Ganitkevitch et al., 2013) to alter the second sentence in case no proof was found at the first attempt, inspired by Bosma & Callison-Burch (2006). The core system reached high precision on entailment and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. Introduction The recent popularity of employing distributional approaches to semantic interpretation has also lead to interesting questions about the relationship between classic formal semantics (including its computational adaptations) a"
S16-1182,D15-1198,0,0.0769137,"-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 shows. A DRS is a first-order representation, i.e., expressible with first-order logic. Various notations are possible, but widely used are the box-like representations shown in Figure 1. Boxes display scopes of discourse referents and contain properties of and relations between discourse referents. They are recursive structures, hence a box may contain other bo"
S16-1182,W08-2222,1,0.723603,"_______ _______________ | ||x1 ||e1 s1 || ||............. ||............... || ||equipment(x1)|>|manufacture(e1) || ||_____________ ||Manner(e1,s1) || | |Theme(e1,x1) || | |complete(s1) || | |_______________ || |___________________________________| Figure 1: DRS, as produced by Boxer. (m / manufacture-01 :ARG1 (e2 / equipment :mod (a2 / all)) :ARG1-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 show"
S16-1182,W15-1841,1,0.833012,"____________ | ||x1 ||e1 s1 || ||............. ||............... || ||equipment(x1)|>|manufacture(e1) || ||_____________ ||Manner(e1,s1) || | |Theme(e1,x1) || | |complete(s1) || | |_______________ || |___________________________________| Figure 1: DRS, as produced by Boxer. (m / manufacture-01 :ARG1 (e2 / equipment :mod (a2 / all)) :ARG1-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 shows. A DRS is"
S16-1182,J16-3006,1,0.832421,"otations are possible, but widely used are the box-like representations shown in Figure 1. Boxes display scopes of discourse referents and contain properties of and relations between discourse referents. They are recursive structures, hence a box may contain other boxes. 2.2 Abstract Meaning Representations At first glance, an AMR looks quite different from a DRS. Usually, an AMR is displayed as a directed graph with a unique root (Figure 2). However, it is also possible to view an AMR as a recursive structure, and then DRS and AMR have more in common than one perhaps would initially realize (Bos, 2016). The variables in an AMR correspond to discourse referents in a DRS. The colon-prefixed symbols in an AMR are similar to the two-place relation symbols in a DRS. And the forward slashes in an AMR correspond to one-place predicates in a DRS. So the main commonalities between a DRS (as produced by Boxer) and an AMR (as used at the SEMEVAL2016 shared task) are: • both use a neo-Davidsonian event semantics; • both are recursive meaning representations; • both expect normalization of date expressions. There are also some obvious differences between DRS and AMR. Some of them are theoretical and hav"
S16-1182,P07-2009,1,0.644846,"|| ||equipment(x1)|>|manufacture(e1) || ||_____________ ||Manner(e1,s1) || | |Theme(e1,x1) || | |complete(s1) || | |_______________ || |___________________________________| Figure 1: DRS, as produced by Boxer. (m / manufacture-01 :ARG1 (e2 / equipment :mod (a2 / all)) :ARG1-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 shows. A DRS is a first-order representation, i.e., expressible with first-order logic. Va"
S16-1182,D13-1146,1,0.896308,"__________ ||Manner(e1,s1) || | |Theme(e1,x1) || | |complete(s1) || | |_______________ || |___________________________________| Figure 1: DRS, as produced by Boxer. (m / manufacture-01 :ARG1 (e2 / equipment :mod (a2 / all)) :ARG1-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 shows. A DRS is a first-order representation, i.e., expressible with first-order logic. Various notations are possible, but widely used"
S16-1182,P11-1138,0,0.0451465,"Missing"
S17-2021,S16-1101,0,0.0202194,"s for Semantic Textual Similarity Johannes Bjerva Center for Language and Cognition Groningen University of Groningen The Netherlands j.bjerva@rug.nl Abstract Arabic sentence. Previous approaches to this problem have focussed on two main approaches. On the one hand, MT approaches have been attempted (e.g. Lo et al. (2016)), which allow for monolingual similarity assessment, but suffer from the fact that involving a fully-fledged MT system severely increases system complexity. Applying bilingual word representations, on the other hand, bypasses this issue without inducing such complexity (e.g. Aldarmaki and Diab (2016)). However, bilingual approaches do not allow for taking advantage of the increasing amount of STS data available for more than one language pair. Currently, there are several methods available for obtaining high quality multilingual word representations. It is therefore interesting to investigate whether language can be ignored entirely in an STS system after mapping words to their respective representations. We investigate the utility of multilingual word representations in a crosslingual STS setting. We approach this by combining multilingual word representations with a deep neural network,"
S17-2021,S17-2001,0,0.0375963,"m is trained and evaluated on all language pairs included in the shared task (English, Spanish, Arabic, and Turkish). Although development results are promising, our system does not yield high performance on the shared task test sets. 1 ¨ Robert Ostling Department of Linguistics Stockholm University Sweden robert@ling.su.se Introduction Semantic Textual Similarity (STS) is the task of assessing the degree to which two sentences are semantically similar. Within the SemEval STS shared tasks, this is measured on a scale ranging from 0 (no semantic similarity) to 5 (complete semantic similarity) (Cer et al., 2017). Monolingual STS is an important task, for instance for evaluation of machine translation (MT) systems, where estimating the semantic similarity between a system’s translation and the gold translation can aid both system evaluation and development. The task is already a challenging one in a monolingual setting, e.g., when estimating the similarity between two English sentences. In this paper, we tackle the more difficult case of cross-lingual STS, e.g., estimating the similarity between an English and an 2 2.1 Multilingual Word Representations Multilingual Skip-gram The skip-gram model has be"
S17-2021,2005.mtsummit-papers.11,0,0.0225731,"e which words comprise the multilingual context of a word. Whereas Guo et al. (2016) only evaluate their approach on the relatively similar languages English, French and Spanish, we explore a more typological diverse case, as we apply this method to English, Spanish and Arabic. We use the same parameter settings as Guo et al. (2016). 2.2 3.2 We use all available data from previous editions of the SemEval shared tasks on (cross-lingual) STS. An overview of the available data is shown in Table 1. Language pair Learning embeddings We train 100-dimensional multilingual embeddings on the Europarl (Koehn, 2005) and UN corpora (Ziemski et al., 2016). Word alignment, which is required for the training of multilingual embeddings, is performed using the Ef¨ maral word-alignment tool (Ostling and Tiedemann, 2016). This allows us to extract a large amount of multilingual (w, c) pairs. We then use these pairs in order to learn multilingual embeddings, by applying the word2vecf tool (Levy and Goldberg, 2014). 3 3.1 Data N sentences English / English English / Spanish English / Arabic Spanish / Spanish Arabic / Arabic 3750 1000 2162 1620 1081 Table 1: Available data for (cross-lingual) STS from the SemEval s"
S17-2021,P14-2050,0,0.039148,"ditions of the SemEval shared tasks on (cross-lingual) STS. An overview of the available data is shown in Table 1. Language pair Learning embeddings We train 100-dimensional multilingual embeddings on the Europarl (Koehn, 2005) and UN corpora (Ziemski et al., 2016). Word alignment, which is required for the training of multilingual embeddings, is performed using the Ef¨ maral word-alignment tool (Ostling and Tiedemann, 2016). This allows us to extract a large amount of multilingual (w, c) pairs. We then use these pairs in order to learn multilingual embeddings, by applying the word2vecf tool (Levy and Goldberg, 2014). 3 3.1 Data N sentences English / English English / Spanish English / Arabic Spanish / Spanish Arabic / Arabic 3750 1000 2162 1620 1081 Table 1: Available data for (cross-lingual) STS from the SemEval shared task series. 4 Experiments and Results We aim to investigate whether using a multilingual input representation and shared weights allow us to ignore languages in STS. We first train and evaluate single-source trained systems (i.e. on a single language pair), and evaluate this both usMethod System architecture We use a relatively simple neural network architecture, consisting of an input l"
S17-2021,S16-1102,0,0.140904,"ual Similarity Johannes Bjerva Center for Language and Cognition Groningen University of Groningen The Netherlands j.bjerva@rug.nl Abstract Arabic sentence. Previous approaches to this problem have focussed on two main approaches. On the one hand, MT approaches have been attempted (e.g. Lo et al. (2016)), which allow for monolingual similarity assessment, but suffer from the fact that involving a fully-fledged MT system severely increases system complexity. Applying bilingual word representations, on the other hand, bypasses this issue without inducing such complexity (e.g. Aldarmaki and Diab (2016)). However, bilingual approaches do not allow for taking advantage of the increasing amount of STS data available for more than one language pair. Currently, there are several methods available for obtaining high quality multilingual word representations. It is therefore interesting to investigate whether language can be ignored entirely in an STS system after mapping words to their respective representations. We investigate the utility of multilingual word representations in a crosslingual STS setting. We approach this by combining multilingual word representations with a deep neural network,"
S17-2021,L16-1561,0,0.0242906,"ltilingual context of a word. Whereas Guo et al. (2016) only evaluate their approach on the relatively similar languages English, French and Spanish, we explore a more typological diverse case, as we apply this method to English, Spanish and Arabic. We use the same parameter settings as Guo et al. (2016). 2.2 3.2 We use all available data from previous editions of the SemEval shared tasks on (cross-lingual) STS. An overview of the available data is shown in Table 1. Language pair Learning embeddings We train 100-dimensional multilingual embeddings on the Europarl (Koehn, 2005) and UN corpora (Ziemski et al., 2016). Word alignment, which is required for the training of multilingual embeddings, is performed using the Ef¨ maral word-alignment tool (Ostling and Tiedemann, 2016). This allows us to extract a large amount of multilingual (w, c) pairs. We then use these pairs in order to learn multilingual embeddings, by applying the word2vecf tool (Levy and Goldberg, 2014). 3 3.1 Data N sentences English / English English / Spanish English / Arabic Spanish / Spanish Arabic / Arabic 3750 1000 2162 1620 1081 Table 1: Available data for (cross-lingual) STS from the SemEval shared task series. 4 Experiments and R"
S18-1058,P17-2054,1,0.902217,"Missing"
S18-1058,E17-2026,0,0.0431706,"Missing"
S18-1058,W17-0225,1,0.887854,"place-holder instead of their actual values. Since the model used both word and character representations, the characters are read in separately, although the same basic principle is followed. Every character is represented by an embedded representation, which is initialised randomly prior to training. Introduction We consider the task of identifying affect in tweets, as described in Mohammad et al. (2018). Given a tweet, the task is to predict the emotions and their corresponding intensities which the tweet portrays. Previous approaches to this task are outlined in Mohammad and Bravo-Marquez (2017). The winning team of the SemEval EmoInt 2017, presented in Goel et al. (2017), tackled a similar task as the regression task presented in this year’s SemEval Task 1. The winning system utilised an ensemble approach consisting of 5 sub-models and using a weighted average of these models to come up with the final result. This model is utilising most of the different approaches mentioned in the literature and combining them into one and with great success. Our work bears resemblance to the runner up in the SemEval EmoInt 2017, K¨oper et al. (2017), who used a comparatively simple model consistin"
S18-1058,C16-1333,1,0.897597,"Missing"
S18-1058,P15-1166,0,0.0840695,"Missing"
S18-1058,W15-4322,0,0.0657959,"Missing"
S18-1058,W17-5207,0,0.0371953,"both word and character representations, the characters are read in separately, although the same basic principle is followed. Every character is represented by an embedded representation, which is initialised randomly prior to training. Introduction We consider the task of identifying affect in tweets, as described in Mohammad et al. (2018). Given a tweet, the task is to predict the emotions and their corresponding intensities which the tweet portrays. Previous approaches to this task are outlined in Mohammad and Bravo-Marquez (2017). The winning team of the SemEval EmoInt 2017, presented in Goel et al. (2017), tackled a similar task as the regression task presented in this year’s SemEval Task 1. The winning system utilised an ensemble approach consisting of 5 sub-models and using a weighted average of these models to come up with the final result. This model is utilising most of the different approaches mentioned in the literature and combining them into one and with great success. Our work bears resemblance to the runner up in the SemEval EmoInt 2017, K¨oper et al. (2017), who used a comparatively simple model consisting of a CNN-LSTM neural network. The difference between the models presented in"
S18-1058,W17-5206,0,0.0629953,"Missing"
S18-1058,E17-1005,0,0.048576,"Missing"
S18-1058,W17-5205,0,0.0483094,"Missing"
S18-1058,S18-1001,0,0.044417,"Missing"
S18-1058,P16-2038,0,0.0793817,"Missing"
W15-3708,N13-1090,0,0.322556,"ust be taken as the analysis requires the assessment by the traditional historian. 1 Introduction Continuous space representations of words are currently the backbone of several state-of-theart approaches to problems in natural language processing. The distributional hypothesis, summarised as: ‘You shall know a word by the company it keeps’ (Firth, 1957) is the basis of many approaches for obtaining such representations. Word embeddings are an example of such a model (e.g. Collobert and Weston (2008)), and have been found to encapsulate interesting semantic properties; in a model presented by Mikolov et al. (2013b), the result when calculat−−−→ −−→ −−−−−→ −−−−→ ing KING − MAN + WOMAN, is close to QUEEN. In this work in progress within the Cassiodigitalis project, we investigate how such representations can be adapted to aid humanities researchers, using the case of late antiquity as an example. Using such a model has several advantages, such 1 github.com/bjerva/cassiodigitalis 53 Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 53–57, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics and The Asian"
W15-3708,P14-1023,0,0.0542457,"l historical characters who were Cassiodorus’ peers and competitors in cultural networks (Boethius, Symmachus), political networks (Liberius) and ecclesiastical circles (Agapetus). Furthermore we added representatives of the political forces with whom the Ostrogothic kingdom in Italy inThe core of the method used in this paper is based on the freely available word2vec tool, which can be used to quickly create high quality word embeddings based on a large corpus of text (Mikolov et al., 2013a).2 We train word2vec using parameters similar to those used for the best performing English vectors in Baroni et al. (2014). We use the continuous bag-of-words model, a window size of 5, a vector dimensionality of 400, 10 negative samples and set subsampling to 1e−5 . We further allow the model to train on the corpus over the course of 100 epochs. 3.1 Cassiodorus’ Variae 3 code.google.com/p/word2vec/ 54 cs.cmu.edu/˜dbamman/latin.html teracted and competed: apart from the Ostrogothic kings themselves (Theodoric, Athalaric, Theodahad), we have their barbarian predecessors (Alaric and Odoacer), and the Roman emperors from the Byzantine east (Anastasius, Iustinianus, Theodora). This selection of persons, along with re"
W16-1102,W14-2302,0,0.14866,"Missing"
W16-1102,W15-1402,0,0.14851,"subject dataset we only use the four features for subject-verb pairs, while in the subject-andobject dataset we use eight features, four based on the subject-verb pair and four based on the objectverb pair. Model fitting and evaluation is done using 10-fold cross validation, and l2-regularization with strength 1 is applied. In case of missing data points (e.g. no cluster available for the verb/noun), the majority class (non-metaphor) is assigned. As a baseline, we calculate the score when classifying all items as metaphor. In addition, we evaluate the effect of re-weighting examples. Beigman Klebanov et al. (2015), who also evaluated their metaphor detection system on the VUAMC, showed that re-weighting training examples can have a large impact on performance, due to the large class imbalance in the VUAMC (mostly 5 http://derczynski.com/sheffield/brown-tuning 13 Evaluation http://www.vismet.org/metcor/documentation/home.html Data BL CP LP P-LP SPS SA Subject 23.0 0.0 0.0 Object 50.8 0.0 3.2 Both 53.4 0.0 18.1 Table 1: 0.0 1.4 0.7 All Data 0.0 0.0 1.3 0.0 0.0 2.4 0.0 2.3 32.1 lectional preference information without any generalization, Features: all-metaphor BaseLine (BL), Conditional Probability (CP),"
W16-1102,J92-4003,0,0.369701,"lities or a measure like selectional association (Resnik, 1993). However, even a very large corpus will not yield full coverage of all possible pairs. This could cause the system to flag, for example ‘evoke-exhilaration’ as a selectional preference violation and thus as metaphoric, simply because this combination occurs very infrequently in the corpus. The solution to this problem is to do some sort of generalization over these verb-noun pairs. One way of doing this is to perform clustering the verbs and nouns in semantically coherent classes. We test two clustering methods: Brown clustering (Brown et al., 1992), and a novel method, which relies on k-means clustering of word embeddings. In addition to these clustering methods, we also look at using word embeddings directly in a classifier, in order to prevent the information loss inherent in a clustering approach. These approaches are compared to performance based on using the selectional preference information without any generalization and a most-frequent class baseline. 3.1 Corpus frequencies Before we can apply generalization methods over selectional preferences, we need to gather selectional preference frequencies from a large corpus of English."
W16-1102,R15-1016,0,0.0152258,"e information gain of a verb. This is an indicator of how selective the verb is in the choice of its arguments, and could thus be useful in filtering out weak-preference verbs. P (n|v) = SP S(v) = X n∈N P (n|v) ∗ log P (n|v) P (n) (2) More directly useful is the selectional association (SA) measure, which is an indication of how typical a noun is in its occurrence with a certain verb. It is defined as follows: SA(v, n) = 3 12 Selectional preference metrics 1 P (n|v) ∗ P (n|v) ∗ log (3) SP S(v) P (n) https://spacy.io 3.3 Clustering For the Brown clustering, we use the pre-trained clusters from Derczynski et al. (2015). They provide clusterings4 with different numbers of clusters, which makes it easy to find an optimal number of clusters. We use the clusters trained on 64M words of newswire text. For k-means clustering of word embeddings, we use the GloVe word embeddings (Pennington et al., 2014), with 300 dimensions, trained on a 840B word corpus and cluster the embeddings of the top 400,000 most frequent words. Clustering is done using the k-means clustering implementation from scikit-learn (Pedregosa et al., 2011), with kmeans++ initialization, a maximum of 500 iterations, and the best clustering out of"
W16-1102,J91-1003,0,0.179445,"rocessing in a semantic parsing framework, novel metaphor is the most problematic for deducing meaning, and is thus the most relevant and interesting. Another important aspect of novel metaphors is that they tend to be less frequent than conventional metaphor (and literals). This has the advantage of making one feature commonly used for metaphor detection, selectional preference violation, more effective. The idea of using selectional preference violation as an indicator of metaphoricity goes far back (Wilks, 1975; Wilks, 1978), and has been widely used in previous work on metaphor detection (Fass, 1991; Mason, 2004; Jia and Yu, 2008; Shutova et al., 2010; Neuman et al., 2013; Wilks et al., 2013). Using selectional preference violation as a heuristic works well, but has the fundamental shortcoming that selectional preferences are based on frequency; selectional preference data are obtained from a corpus by counting how often words occur together in a certain relation. Metaphor, on the other hand, is defined by basicness of meaning, and not frequency of meaning. Although these two are correlated, there are also many cases where the figurative sense of a word has become more frequent than its"
W16-1102,Y08-1020,0,0.0217837,"arsing framework, novel metaphor is the most problematic for deducing meaning, and is thus the most relevant and interesting. Another important aspect of novel metaphors is that they tend to be less frequent than conventional metaphor (and literals). This has the advantage of making one feature commonly used for metaphor detection, selectional preference violation, more effective. The idea of using selectional preference violation as an indicator of metaphoricity goes far back (Wilks, 1975; Wilks, 1978), and has been widely used in previous work on metaphor detection (Fass, 1991; Mason, 2004; Jia and Yu, 2008; Shutova et al., 2010; Neuman et al., 2013; Wilks et al., 2013). Using selectional preference violation as a heuristic works well, but has the fundamental shortcoming that selectional preferences are based on frequency; selectional preference data are obtained from a corpus by counting how often words occur together in a certain relation. Metaphor, on the other hand, is defined by basicness of meaning, and not frequency of meaning. Although these two are correlated, there are also many cases where the figurative sense of a word has become more frequent than its original, literal sense. The re"
W16-1102,J04-1002,0,0.02352,"a semantic parsing framework, novel metaphor is the most problematic for deducing meaning, and is thus the most relevant and interesting. Another important aspect of novel metaphors is that they tend to be less frequent than conventional metaphor (and literals). This has the advantage of making one feature commonly used for metaphor detection, selectional preference violation, more effective. The idea of using selectional preference violation as an indicator of metaphoricity goes far back (Wilks, 1975; Wilks, 1978), and has been widely used in previous work on metaphor detection (Fass, 1991; Mason, 2004; Jia and Yu, 2008; Shutova et al., 2010; Neuman et al., 2013; Wilks et al., 2013). Using selectional preference violation as a heuristic works well, but has the fundamental shortcoming that selectional preferences are based on frequency; selectional preference data are obtained from a corpus by counting how often words occur together in a certain relation. Metaphor, on the other hand, is defined by basicness of meaning, and not frequency of meaning. Although these two are correlated, there are also many cases where the figurative sense of a word has become more frequent than its original, lit"
W16-1102,D14-1162,0,0.0792448,"Missing"
W16-1102,shutova-teufel-2010-metaphor,0,0.37791,"Missing"
W16-1102,C10-1113,0,0.320219,"Missing"
W16-1102,N10-1147,0,0.0207306,"million verb-object triples of 7.8 million distinct types. The triples contained 175,630 verb lemma types and 1,982,512 noun lemma types. 3.2 Selectional preference information can be used in three ways: as a conditional probability, a selectional preference strength for the verb, and as selectional association between a verb and a noun. The conditional probability is defined as follows: P (n, v) c(n, v) ≈ (1) P (v) c(v) Where P (n|v) is the probability of seeing noun n occurring with verb v. This can be approximated by using the counts c from the corpus. For the other two metrics, we follow Shutova (2010) in using the metrics proposed by Resnik (1993), selectional preference strength (SPS) and selectional association (SA). Selectional preference strength is the information gain of a verb. This is an indicator of how selective the verb is in the choice of its arguments, and could thus be useful in filtering out weak-preference verbs. P (n|v) = SP S(v) = X n∈N P (n|v) ∗ log P (n|v) P (n) (2) More directly useful is the selectional association (SA) measure, which is an indication of how typical a noun is in its occurrence with a certain verb. It is defined as follows: SA(v, n) = 3 12 Selectional"
W16-1102,W13-0905,0,0.0163464,"deducing meaning, and is thus the most relevant and interesting. Another important aspect of novel metaphors is that they tend to be less frequent than conventional metaphor (and literals). This has the advantage of making one feature commonly used for metaphor detection, selectional preference violation, more effective. The idea of using selectional preference violation as an indicator of metaphoricity goes far back (Wilks, 1975; Wilks, 1978), and has been widely used in previous work on metaphor detection (Fass, 1991; Mason, 2004; Jia and Yu, 2008; Shutova et al., 2010; Neuman et al., 2013; Wilks et al., 2013). Using selectional preference violation as a heuristic works well, but has the fundamental shortcoming that selectional preferences are based on frequency; selectional preference data are obtained from a corpus by counting how often words occur together in a certain relation. Metaphor, on the other hand, is defined by basicness of meaning, and not frequency of meaning. Although these two are correlated, there are also many cases where the figurative sense of a word has become more frequent than its original, literal sense. The result of this is that metaphor detection systems based on selecti"
W16-4116,C16-1333,1,0.864903,"Missing"
W16-4116,P16-2067,0,0.0879681,"Missing"
W16-4116,wittenburg-etal-2006-elan,0,0.0503658,"Missing"
W16-4816,N10-1027,0,0.0672584,"Missing"
W16-4816,C16-1333,1,0.925164,"(RNNs). Deep residual networks (ResNets) are a recent building block for CNNs which have yielded promising results in, e.g., image classification tasks (He et al., 2015; He et al., 2016). ResNets are constructed by stacking so-called residual units. These units can be viewed as a series of convolutional layers with a ‘shortcut’ which facilitates signal propagation in the neural network. This, in turn, allows for training deeper networks more easily (He et al., 2016). In Natural Language Processing (NLP), ResNets have shown state-of-the-art performance for Semantic and Part-of-Speech tagging (Bjerva et al., 2016). However, no previous work has attempted to apply ResNets to language identification. 2 Method Several previous approaches in the DSL shared tasks have formulated the task as a two-step classification, first identifying the language group, and then the specific language (Zampieri et al., 2015). Instead of taking this approach, we formulate the task as a multi-class classification problem, with each language / dialect representing a separate class. Our system is a deep neural network consisting of a bidirectional Gated Recurrent Unit (GRU) network at the upper level, and a Deep Residual Networ"
W16-4816,W16-4802,0,0.452745,"Missing"
W16-4816,L16-1284,0,0.21629,"Missing"
W16-4816,P12-3005,0,0.0238354,"ion Language identification is an unsolved problem, certainly in the context of discriminating between very similar languages (Baldwin and Lui, 2010). This problem is tackled in the Discriminating between Similar Languages (DSL) series of shared tasks (Zampieri et al., 2014; Zampieri et al., 2015). Most successful approaches to the DSL shared task in previous years have relied on settings containing ensembles of classifiers (Goutte et al., 2016). These classifiers often use various combinations of features, mostly based on word, character, and/or byte n-grams (see, e.g., Cavnar et al. (1994), Lui and Baldwin (2012)). We are interested in exploring a single methodological aspect in the current edition of this shared task (Malmasi et al., 2016). We aim to investigate whether reasonable results for this task could be obtained by applying recently emerged neural network architectures, coupled with sub-token input representations. To address this question, we explore convolutional neural networks (CNNs) and recurrent neural networks (RNNs). Deep residual networks (ResNets) are a recent building block for CNNs which have yielded promising results in, e.g., image classification tasks (He et al., 2015; He et al"
W16-4816,W16-4801,0,0.0765527,"Missing"
W16-4816,W16-2003,0,0.0475833,"2016), e.g., the identity function (He et al., 2015), which we also 120 use in our experiments. ResNets can be intuitively understood by thinking of residual functions as paths through which information can propagate easily. This means that, in every layer, a ResNet learns more complex feature combinations, which it combines with the shallower representation from the previous layer. This architecture allows for the construction of much deeper networks. ResNets have recently been found to yield impressive performance in both image recognition and NLP tasks (He et al., 2015; ¨ He et al., 2016; Ostling, 2016; Conneau et al., 2016), and are an interesting and effective alternative to simply stacking layers. In this paper we use the assymetric variant of ResNets as described in Equation 9 in He et al. (2016): (2) xl+1 = xl + F(fˆ(xl ), Wl ). Our residual block, using dropout and batch normalization (Srivastava et al., 2014; Ioffe and Szegedy, 2015), is defined in Table 1. In the table, merge indicates the concatenation of the input of the residual block, with the output of the final convolutional layer. type patch/pool size Batch normalization + ReLu + Dropout (p = 0.5) Convolution Batch normalizat"
W16-4816,P16-2067,0,0.029364,"to cope with longer input sequences than vanilla RNNs. GRUs are similar to the more commonly-used Long Short-Term Memory networks (LSTMs), both in purpose and implementation (Chung et al., 2014). A bi-directional GRU makes both forward and backward passes over sequences, and can therefore use both preceding and succeeding contexts to predict a tag (Graves and Schmidhuber, 2005; Goldberg, 2015). Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS and semantic tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016; Bjerva et al., 2016). 2.2 Deep Residual Networks Deep Residual Networks (ResNets) are built up by stacking residual units. A residual unit can be expressed as: yl = h(xl ) + F(xl , Wl ), (1) xl+1 = f (yl ), where xl and xl+1 are the input and output of the l-th layer, Wl is the weights for the l-th layer, and F is a residual function (He et al., 2016), e.g., the identity function (He et al., 2015), which we also 120 use in our experiments. ResNets can be intuitively understood by thinking of residual functions as paths through which information can propagate easily. This means that, in every"
W16-4816,W14-5307,0,0.137129,"Missing"
W16-4816,W15-5401,0,0.194474,"Missing"
W17-0224,S16-1081,0,0.106717,"which no such data exists. We train and evaluate on five language pairs, including English, Spanish, and Arabic. We are able to train wellperforming systems for several language pairs, without any labelled data for that language pair. 1 ¨ Robert Ostling Department of Linguistics Stockholm University Sweden robert@ling.su.se Introduction Semantic Textual Similarity (STS) is the task of assessing the degree to which two sentences are semantically similar. Within the SemEval STS shared tasks, this is measured on a scale ranging from 0 (no semantic similarity) to 5 (complete semantic similarity) (Agirre et al., 2016). Monolingual STS is an important task, for instance for evaluation of machine translation (MT) systems, where estimating the semantic similarity between a system’s translation and the gold translation can aid both system evaluation and development. The task is already a challenging one in a monolingual setting, e.g., when estimating the similarity between two English sentences. In this paper, we tackle the more difficult case of cross-lingual STS, e.g., estimating the similarity between an English and an Arabic sentence. Previous approaches to this problem have focussed on two main approaches"
W17-0224,W13-3520,0,0.0281965,"anguage. Table 4: Ablation results (Pearson correlations). Columns indicate ablated language pairs, and rows indicate testing language pairs. The none column indicates no ablation, i.e., training on all three monolingual pairs. Bold indicates results when not training on the language pair evaluated on. PP PP Ablated en/en es/es ar/ar none PP Test PP P en/en es/es ar/ar 5.3 0.60 0.69 0.64 0.64 0.68 0.66 0.69 0.67 0.58 0.65 0.60 0.72 Comparison with Monolingual Representations We compare multilingual embeddings with the performance obtained using the pre-trained monolingual Polyglot embeddings (Al-Rfou et al., 2013). Training and evaluating on the same language pair yields comparable results regardless of embeddings. However, when using monolingual embeddings, every multilingual language pair combination yields poor results. 6 Discussion 7 In all cases, training on the target language pair is beneficial. We also observe that using multilingual embeddings is crucial for multilingual approaches, as monolingual embeddings naturally only yield on-par results in monolingual settings. This is due to the fact that using the shared language-agnostic input representation allows us to take advantage of linguistic"
W17-0224,S16-1101,0,0.0520833,"Missing"
W17-0224,J16-4007,0,0.0277065,"n differs/missing. J(θ ) = 4. The two sentences are mostly equivalent, but some unimportant details differ. This manner of assessing semantic content of two sentences notably does not take important semantic features such as negation into account, and can therefore be seen as complimentary to textual entailment. Furthermore, the task is highly related to paraphrasing, as replacing an n-gram with a paraphrase thereof ought to alter the semantic similarity of two sentences to a very low degree. Successful monolingual approaches in the past have taken advantage of both of these facts (see, e.g., Beltagy et al. (2016)). Approaches similar to these can be applied in cross-lingual STS, if the sentence pair is translated to a language for which such resources exist. However, involving a fullyfledged MT system increases pipeline complexity, which increases the risk of errors in cases of, e.g., mistranslations. Using bilingual word representations, in order to create truly cross-lingual systems, was explored by several systems in SemEval 2016 (Agirre et al., 2016). However, such systems are one step short of truly taking advantage of the large amounts of multilingual parallel data, and STS data, available. This"
W17-0224,P14-2050,0,0.0255699,"ish, French and Spanish, we explore a more typological diverse case, as we apply this method to English, Spanish and Arabic. We use the same parameter settings as Guo et al. (2016). 5. The two sentences are completely equivalent, as they mean the same thing. 3 ∑ (w,c)∈D 3.2 Learning embeddings We train multilingual embeddings on the Europarl and UN corpora. Word alignment is performed ¨ using the Efmaral word-alignment tool (Ostling and Tiedemann, 2016). This allows us to extract a large amount of multilingual (w, c) pairs. We then learn multilingual embeddings by applying the word2vecf tool (Levy and Goldberg, 2014). 4 4.1 Multilingual Word Representations Method System architecture We use a relatively simple neural network architecture, consisting of an input layer with pretrained word embeddings and a siamese network of fully connected layers with shared weights. In order to prevent any shift from occurring in the embeddings, we do not update these during training. The intuition here, is that we do not want the representation for, e.g., dog to be updated, which might push it further away from that of perro. We expect this to be especially important in cases where we train on a single language, and eval"
W17-0224,S16-1102,0,0.0570375,"Missing"
W17-0225,E14-4030,0,0.0295603,"x and y are all variables in the given distributions, p(x, y) is the joint probability of variable x cooccurring with variable y, and p(x) is the probability of variable x occurring at all. MI describes how much information is shared between X and Y , and can therefore be considered a measure of ‘correlation’ between tag sets. 3.1 Data 4.1 Morphosyntactic Tasks Dependency Relation Classification is the task of predicting the dependency tag (and its direction) for a given token. This is a task that has not received much attention, although it has been shown to be a useful feature for parsing (Ouchi et al., 2014). We choose to look at several instantiations of this task, as it allows for a controlled setup under a number of conditions for MTL, and since data is available for a large number of typologically varied languages. Previous work has suggested various possible instantiations of dependency relation classification labels (Ouchi et al., 2016). In this work, we use labels designed to range from highly complex and informative, to very basic ones.3 The labelling schemes used are shown in Table 1. Information Theory and MTL Entropy has in the literature been hypothesised to be related to the usefulne"
W17-0225,Q16-1031,0,0.0242819,"main task (e.g. parsing), while also learning some other auxiliary task (e.g. POS tagging). Introduction When attempting to solve a natural language processing (NLP) task, one can consider the fact that many such tasks are highly related to one another. A common way of taking advantage of this is to apply multitask learning (MTL, Caruana (1998)). MTL has been successfully applied to many linguistic sequence-prediction tasks, both syntactic and semantic in nature (Collobert and Weston, 2008; Cheng et al., 2015; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2016; Bjerva et al., 2016; Ammar et al., 2016; Plank et al., 2016). It is, however, unclear when an auxiliary task is useful, although previous work has provided some insights (Caruana, 1998; Mart´ınez Alonso and Plank, 2016). Currently, considerable time and effort need to be employed in order to experimentally investigate the usefulness of any given main task / auxiliary task combination. In this paper we wish to alleviate this process by providing a means to investigating when an auxiliary task is helpful, thus also 3 Information-theoretic Measures We wish to give an information-theoretic perspective on when an auxiliary task will be"
W17-0225,C16-1333,1,0.89848,"Missing"
W17-0225,P16-2067,0,0.0495894,"sing), while also learning some other auxiliary task (e.g. POS tagging). Introduction When attempting to solve a natural language processing (NLP) task, one can consider the fact that many such tasks are highly related to one another. A common way of taking advantage of this is to apply multitask learning (MTL, Caruana (1998)). MTL has been successfully applied to many linguistic sequence-prediction tasks, both syntactic and semantic in nature (Collobert and Weston, 2008; Cheng et al., 2015; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2016; Bjerva et al., 2016; Ammar et al., 2016; Plank et al., 2016). It is, however, unclear when an auxiliary task is useful, although previous work has provided some insights (Caruana, 1998; Mart´ınez Alonso and Plank, 2016). Currently, considerable time and effort need to be employed in order to experimentally investigate the usefulness of any given main task / auxiliary task combination. In this paper we wish to alleviate this process by providing a means to investigating when an auxiliary task is helpful, thus also 3 Information-theoretic Measures We wish to give an information-theoretic perspective on when an auxiliary task will be useful for a given ma"
W17-0225,D15-1085,0,0.0293244,"layers are shared, whereas there is one output layer per task. An RNN can thus be trained to solve one main task (e.g. parsing), while also learning some other auxiliary task (e.g. POS tagging). Introduction When attempting to solve a natural language processing (NLP) task, one can consider the fact that many such tasks are highly related to one another. A common way of taking advantage of this is to apply multitask learning (MTL, Caruana (1998)). MTL has been successfully applied to many linguistic sequence-prediction tasks, both syntactic and semantic in nature (Collobert and Weston, 2008; Cheng et al., 2015; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2016; Bjerva et al., 2016; Ammar et al., 2016; Plank et al., 2016). It is, however, unclear when an auxiliary task is useful, although previous work has provided some insights (Caruana, 1998; Mart´ınez Alonso and Plank, 2016). Currently, considerable time and effort need to be employed in order to experimentally investigate the usefulness of any given main task / auxiliary task combination. In this paper we wish to alleviate this process by providing a means to investigating when an auxiliary task is helpful, thus also 3 Information-the"
W17-0225,P16-2038,0,0.112974,"whereas there is one output layer per task. An RNN can thus be trained to solve one main task (e.g. parsing), while also learning some other auxiliary task (e.g. POS tagging). Introduction When attempting to solve a natural language processing (NLP) task, one can consider the fact that many such tasks are highly related to one another. A common way of taking advantage of this is to apply multitask learning (MTL, Caruana (1998)). MTL has been successfully applied to many linguistic sequence-prediction tasks, both syntactic and semantic in nature (Collobert and Weston, 2008; Cheng et al., 2015; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2016; Bjerva et al., 2016; Ammar et al., 2016; Plank et al., 2016). It is, however, unclear when an auxiliary task is useful, although previous work has provided some insights (Caruana, 1998; Mart´ınez Alonso and Plank, 2016). Currently, considerable time and effort need to be employed in order to experimentally investigate the usefulness of any given main task / auxiliary task combination. In this paper we wish to alleviate this process by providing a means to investigating when an auxiliary task is helpful, thus also 3 Information-theoretic Measures We wish to g"
W17-0225,E17-1005,0,0.17801,"Missing"
W17-0225,L16-1262,0,\N,Missing
W17-5025,C14-1185,0,0.108067,"g.su.se Barbara Plank CLCG University of Groningen b.plank@rug.nl Abstract although the focus in the past has been on using written text, speech transcripts and audio features have also been included in recent editions, for instance in the 2016 Computational Paralinguistics Challenge (Schuller et al., 2016). Although these aspects are combined in the NLI Shared Task 2017, with both written and spoken responses available, we only utilise written responses in this work. For a further overview of NLI, we refer the reader to Malmasi (2016). Previous approaches to NLI have used syntactic features (Bykh and Meurers, 2014), string kernels (Ionescu et al., 2014), and variations of ensemble models (Malmasi and Dras, 2017; Tetreault et al., 2013). No systems used neural networks in the 2013 shared task (Tetreault et al., 2013), hence ours is one of the first works using a neural approach for this task, along with concurrent submissions in this shared task (Malmasi et al., 2017). We present the RUG-SU team’s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep re"
W17-5025,D14-1142,0,0.121905,"Groningen b.plank@rug.nl Abstract although the focus in the past has been on using written text, speech transcripts and audio features have also been included in recent editions, for instance in the 2016 Computational Paralinguistics Challenge (Schuller et al., 2016). Although these aspects are combined in the NLI Shared Task 2017, with both written and spoken responses available, we only utilise written responses in this work. For a further overview of NLI, we refer the reader to Malmasi (2016). Previous approaches to NLI have used syntactic features (Bykh and Meurers, 2014), string kernels (Ionescu et al., 2014), and variations of ensemble models (Malmasi and Dras, 2017; Tetreault et al., 2013). No systems used neural networks in the 2013 shared task (Tetreault et al., 2013), hence ours is one of the first works using a neural approach for this task, along with concurrent submissions in this shared task (Malmasi et al., 2017). We present the RUG-SU team’s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep residual network using word and character"
W17-5025,W13-1706,0,0.12628,"written text, speech transcripts and audio features have also been included in recent editions, for instance in the 2016 Computational Paralinguistics Challenge (Schuller et al., 2016). Although these aspects are combined in the NLI Shared Task 2017, with both written and spoken responses available, we only utilise written responses in this work. For a further overview of NLI, we refer the reader to Malmasi (2016). Previous approaches to NLI have used syntactic features (Bykh and Meurers, 2014), string kernels (Ionescu et al., 2014), and variations of ensemble models (Malmasi and Dras, 2017; Tetreault et al., 2013). No systems used neural networks in the 2013 shared task (Tetreault et al., 2013), hence ours is one of the first works using a neural approach for this task, along with concurrent submissions in this shared task (Malmasi et al., 2017). We present the RUG-SU team’s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep residual network using word and character features, and a system based on a recurrent neural network. Our best system is an e"
W17-5025,W17-1201,0,0.0820421,"Missing"
W17-5025,W17-5007,0,0.123361,"ed Task 2017, with both written and spoken responses available, we only utilise written responses in this work. For a further overview of NLI, we refer the reader to Malmasi (2016). Previous approaches to NLI have used syntactic features (Bykh and Meurers, 2014), string kernels (Ionescu et al., 2014), and variations of ensemble models (Malmasi and Dras, 2017; Tetreault et al., 2013). No systems used neural networks in the 2013 shared task (Tetreault et al., 2013), hence ours is one of the first works using a neural approach for this task, along with concurrent submissions in this shared task (Malmasi et al., 2017). We present the RUG-SU team’s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep residual network using word and character features, and a system based on a recurrent neural network. Our best system is an ensemble of neural networks, reaching an F1 score of 0.8323. Although our system is not the highest ranking one, we do outperform the baseline by far. 1 Introduction Native Language Identification (NLI) is the task of identifying the nat"
W17-5025,P14-5010,0,0.0211796,"ockholm University (team RUG-SU) submission to NLI Shared Task 2017 (Malmasi et al., 2017). Neural networks constitute one of the most popular methods in natural language processing these days (Manning, 2015), but appear not to have been previously used for NLI. Our goal in this paper is therefore twofold. On the one hand, we wish to investigate how well a neural system can perform the task. On the other hand, we wish to investigate the effect of using features based on spelling errors. 2 3 External data 3.1 PoS-tagged sentences We indirectly use the training data for the Stanford PoS tagger (Manning et al., 2014), and for initialising word embeddings we use GloVe embeddings from 840 billion tokens of web data.1 3.2 Spelling features We investigate learner misspellings, which is mainly motivated by two assumptions. For one, spelling errors are quite prevalent in learners’ written production (Kochmar, 2011). Additionally, spelling errors have been shown to be influenced by phonological L1 transfer (Grigonyt˙e and Hammarberg, 2014). We use the Aspell spell checker to detect misspelled words.2 Related Work NLI is an increasingly popular task, which has been the subject of several shared tasks in recent ye"
W17-5025,W17-1219,1,0.831843,"subset of the development set (70/30 split). For the selection of systems to include in the ensemble, we use the combination of systems resulting in the highest 237 6 References Discussion In isolation, the ResNet system yields a relatively high F1 score of 80.16. This indicates that, although simpler methods yield better results for this task, deep neural networks are also applicable. However, further experimentation is needed before such a system can outperform the more traditional feature-based systems. This is in line with previous findings for the related task of language identification (Medvedeva et al., 2017; Zampieri et al., 2017). Combining all of our systems without external data yields an F1 score of 83.23, which places our system in the third best performing group of the NLI Shared Task 2017 (Malmasi et al., 2017). When adding external data, the best performing systems are those including the spelling system predictions and/or the LSTM predictions. However, the highest F1 score obtained (81.91) is lower than our best score without external resources. This can attributed to overfitting of the ensemble on the development data. It is nonetheless interesting that adding spelling features does bo"
W17-5025,W16-2003,1,0.791824,"ly dropout (p = 0.5) on the final hidden layer. Deep Residual Networks Deep residual networks, or resnets, are a class of convolutional neural networks, which consist of several convolutional blocks with skip connections in between (He et al., 2015, 2016). Such skip connections facilitate error propagation to earlier layers in the network, which allows for building deeper networks. Although their primary application is image recognition and related tasks, recent work has found deep residual networks to be useful for a range of NLP tasks. Examples of this in¨ clude morphological re-inflection (Ostling, 2016), semantic tagging (Bjerva et al., 2016), and other text classification tasks (Conneau et al., 2016). We apply resnets with four residual blocks. Each residual block contains two successive onedimensional convolutions, with a kernel size and stride of 2. Each such block is followed by an average pooling layer and dropout (p = 0.5, Srivastava et al. (2014)). The resnets are applied to several input representations: word unigrams, and character 4- to 6-grams. These input representations are first embedded into a 64-dimensional space, and trained together with the task. We do not use any pre-trai"
W17-5025,D14-1162,0,0.0806817,"sing a batch size of 50. No pre-trained embeddings were used in this model. We additionally experiment with a simple multiplayer perceptron (MLP). In contrast to CBOW it uses n-hot features (of the size of the vocabulary), PoS-tagged sentences In order to easier capture general syntactic patterns, we use a sentence-level bidirectional LSTM over tokens and their corresponding part of speech tags from the Stanford CoreNLP toolkit (Manning et al., 2014). PoS tags are represented by 64-dimensional embeddings, initialised randomly; word tokens by 300-dimensional embeddings, initialised with GloVe (Pennington et al., 2014) embeddings trained on 840 billion words of English web data from the Common Crawl project.3 3 https://nlp.stanford.edu/projects/ glove/ 4 236 http://scikit-learn.org/ Table 1: Official results for the essay task, with and without external resources (ext. res.). Setting System F1 (macro) Accuracy Baselines Random Baseline Official Baseline 0.0909 0.7100 0.0909 0.7100 01 – Resnet (w1 +c5 ) 02 – Resnet (w1 +c5 ) 03 – Ensemble (Resnet (w1 +c5 ), Resnet (c4 )) 04 – Ensemble (Resnet (w1 +c5 ), Resnet (c6 ), Resnet (c4 ), Resnet (c3 )) 05 – Ensemble (Resnet (w1 +c5 ), Resnet (c6 ), Resnet (c4 ), CBO"
W17-5043,W17-5007,0,0.156163,"n explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with exploring each of these classification methods as they pertain to the NLI Shared Task 2017 (Malmasi et al., 2017). In this paper, we explore the performance of a linear SVM trained on languageindependent character features for the NLI Shared Task 2017. Our basic system (G RONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as features. We compare this against several ensemble and meta-classifiers in order to examine how the linear system fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution. 1 Introduction Native Language I"
W17-5043,C16-1333,1,0.848832,"dropout, 20 epochs, trained with the adam optimization algorithm (Kingma and Ba, 2014) for 20 iterations with a batch size of 50. 3.3.2 Deep Residual Networks Deep residual networks (resnets) are a class of convolutional neural networks (CNNs), which consist of several convolutional blocks with skip connections in between (He et al., 2016). Such skip connections facilitate error propagation to earlier layers in the network, which allows for building deeper networks. Resnets have been shown to be useful for NLP tasks, such as text classification (Conneau et al., 2016), and sequence labelling (Bjerva et al., 2016). We applied resnets with four residual blocks in our en385 semble experiments, each containing two successive one-dimensional convolutions. Each such block is followed by an average pooling layer and dropout (p = 0.5, Srivastava et al. (2014)). The resnets were applied to several input representations: word unigrams, and character 4-6-grams. The outputs of each resnet are concatenated before passing through two fully connected layers. We trained the resnet over 50 epochs with adam, using the model with the lowest validation loss. In addition to dropout, we used weight decay for regularization"
W17-5043,C12-1025,0,0.0743422,"ect their L2 writing. At a large scale, this could be extended to enhance existing teaching pedagogies and tailor them towards students of a particular L1. NLI is another natural fit for forensic linguistics, where it can be used to detect the native language (and potentially the nationality) of an anonymous writer. NLI is typically framed as a multi-class classification problem, wherein a classifier is trained on more than two native languages simultaneously. As with many text-classification tasks, Support Vector Machines (SVM) have consistently produced the best results for the task, e.g., (Brooke and Hirst, 2012). However, other classifiers, such as Random Forests and Logistic Regression, have also been explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with e"
W17-5043,P16-2067,1,0.895832,"Missing"
W17-5043,W13-1706,0,0.128826,"students of a particular L1. NLI is another natural fit for forensic linguistics, where it can be used to detect the native language (and potentially the nationality) of an anonymous writer. NLI is typically framed as a multi-class classification problem, wherein a classifier is trained on more than two native languages simultaneously. As with many text-classification tasks, Support Vector Machines (SVM) have consistently produced the best results for the task, e.g., (Brooke and Hirst, 2012). However, other classifiers, such as Random Forests and Logistic Regression, have also been explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with exploring each of these classification methods as they pertain to the NLI Shared Task 2017 (Malmasi et al., 2017). In this pape"
W17-5043,D14-1142,0,0.431151,"Missing"
W17-5043,W13-1714,0,0.414695,"Missing"
W18-0207,Q16-1031,0,0.036842,"estigate the usefulness of explicitly modelling similarities between languages in deep neural networks using language embeddings. To do so, we view NLP tasks for multiple Uralic languages as different aspects of the same problem and model them in one model using multilingual transfer in a multi-task learning model. Multilingual models frequently follow a hard parameter sharing regime, where all hidden layers of a neural network are shared between languages, with the language either being implicitly coded in the input string (Johnson et al., 2017), given as a language ID in a one-hot encoding (Ammar et al., 2016), or as a language embedding (Östling and Tiedemann, 2017). In this paper, we both explore multilingual modelling of Uralic languages, and probe the language embeddings obtained from such modelling in order to gain novel insights about typological traits of Uralic languages. We aim to answer the following three research questions (RQs). RQ 1 To what extent is model transfer between Uralic languages for PoS tagging mutually beneficial? RQ 2 Are distributed language representations useful for model transfer between Uralic languages? RQ 3 Can we observe any explicit typological properties encoded"
W18-0207,D17-1011,0,0.127381,"s have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017). These language embeddings encode typological pr"
W18-0207,W16-4116,1,0.840006,"putational Linguistics 1 Introduction For more than two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Ös"
W18-0207,P17-1109,0,0.106631,"n two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017)."
W18-0207,P16-1038,0,0.0473904,"troduction For more than two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 20"
W18-0207,D17-1268,0,0.423471,"ith respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017). These language embeddings encode typological properties of language, re"
W18-0207,L16-1247,0,0.0134878,"se the language embeddings to encode features of language. Other work has explored learning language embeddings in the context of neural machine translation (Malaviya et al., 2017). In this work, we explore the embeddings trained by Östling and Tiedemann (2017), both in their original state, and by further tuning them for PoS tagging. 2.2 Part-of-speech tagging We use PoS annotations from version 2 of the Universal Dependencies (Nivre et al., 2016). We focus on the four Uralic languages present in the UD, namely Finnish (based on the Turku Dependency Treebank, Pyysalo et al., 2015), Estonian (Muischnek et al., 2016), Hungarian (based on the Hungarian Dependency Treebank, Vincze et al., 2010), and North Sámi (Sheyanova and Tyers, 2017). As we are mainly interested in observing the language embeddings, we down-sample all training sets to 1500 sentences (approximate number of sentences in the Hungarian data), so as to minimise any size-based effects. 2.3 Typological data In the experiments for RQ3, we attempt to predict typological features. We extract the features we aim to predict from WALS (Dryer and Haspelmath, 2013). We consider features which are encoded for all four Uralic languages in our sample. 3"
W18-0207,L16-1262,0,0.0866901,"Missing"
W18-0207,P15-2034,0,0.117523,"ciation for Computational Linguistics 1 Introduction For more than two and a half centuries, linguistic typologists have studied languages with respect to their structural and functional properties, thereby implicitly classifying languages as being more or less similar to one another, by virtue of such properties (Haspelmath, 2001; Velupillai, 2012). Although typology has a long history (Herder, 1772; Gabelentz, 1891; Greenberg, 1960, 1974; Dahl, 1985; Comrie, 1989; Haspelmath, 2001; Croft, 2002), computational approaches have only recently gained popularity (Dunn et al., 2011; Wälchli, 2014; Östling, 2015; Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings"
W18-0207,E17-2102,0,0.213918,"16; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017). These language embeddings encode typological properties of language, reminiscent of the sparse features in WALS, or even of parameters in Chomsky’s Principles and Parameters framework (Chomsky, 1993; Chomsky and Lasnik, 1993; Chomsky, 2014). In this paper, we investigate the usefulness of explicitly modelling similarities between languages in deep neural networks using language embeddings. To do so, we view NLP tasks for multiple Uralic languages as different aspects of the same problem and model them in one model using multilingual transfer in a multi-task learning m"
W18-0207,W17-5403,0,0.0880875,"Missing"
W18-0207,P16-2067,0,0.0290186,"we are mainly interested in observing the language embeddings, we down-sample all training sets to 1500 sentences (approximate number of sentences in the Hungarian data), so as to minimise any size-based effects. 2.3 Typological data In the experiments for RQ3, we attempt to predict typological features. We extract the features we aim to predict from WALS (Dryer and Haspelmath, 2013). We consider features which are encoded for all four Uralic languages in our sample. 3 Method and experiments We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture, based on Plank et al. (2016). The system is implemented using DyNet (Neubig et al., 2017). We train using the Adam optimisation algorithm (Kingma and Ba, 2014) over a maximum of 10 epochs, using early stopping. We make two modifications to the bi-LSTM architecture of Plank et al. (2016). First of all, we do not use any atomic embedded word representations, but rather use only character-based word representations. This choice was made so as to encourage the model not to rely on languagespecific vocabulary. Additionally, we concatenate a pre-trained language embedding to each word representation. That is to say, in the ori"
W18-0207,W17-0607,0,0.0141672,"context of neural machine translation (Malaviya et al., 2017). In this work, we explore the embeddings trained by Östling and Tiedemann (2017), both in their original state, and by further tuning them for PoS tagging. 2.2 Part-of-speech tagging We use PoS annotations from version 2 of the Universal Dependencies (Nivre et al., 2016). We focus on the four Uralic languages present in the UD, namely Finnish (based on the Turku Dependency Treebank, Pyysalo et al., 2015), Estonian (Muischnek et al., 2016), Hungarian (based on the Hungarian Dependency Treebank, Vincze et al., 2010), and North Sámi (Sheyanova and Tyers, 2017). As we are mainly interested in observing the language embeddings, we down-sample all training sets to 1500 sentences (approximate number of sentences in the Hungarian data), so as to minimise any size-based effects. 2.3 Typological data In the experiments for RQ3, we attempt to predict typological features. We extract the features we aim to predict from WALS (Dryer and Haspelmath, 2013). We consider features which are encoded for all four Uralic languages in our sample. 3 Method and experiments We approach the task of PoS tagging using a fairly standard bi-directional LSTM architecture, base"
W18-0207,N16-1161,0,0.342322,"Bjerva and Börstell, 2016; Deri and Knight, 2016; Cotterell and Eisner, 2017; Peters et al., 2017; Asgari and Schütze, 2017; Malaviya et al., 2017). One part of traditional typological research can be seen as assigning sparse explicit feature vectors to languages, for instance manually encoded in databases such as the World Atlas of Language Structures (WALS, Dryer and Haspelmath, 2013). A recent development which can be seen as analogous to this, is the process of learning distributed language representations in the form of dense real-valued vectors, often referred to as language embeddings (Tsvetkov et al., 2016; Östling and Tiedemann, 2017; Malaviya et al., 2017). These language embeddings encode typological properties of language, reminiscent of the sparse features in WALS, or even of parameters in Chomsky’s Principles and Parameters framework (Chomsky, 1993; Chomsky and Lasnik, 1993; Chomsky, 2014). In this paper, we investigate the usefulness of explicitly modelling similarities between languages in deep neural networks using language embeddings. To do so, we view NLP tasks for multiple Uralic languages as different aspects of the same problem and model them in one model using multilingual transf"
W18-0518,gonzalez-agirre-etal-2012-multilingual,0,0.0119783,"seen languages (for which we have no explicit output layer), we implement a further component in our neural model that we optimize to predict the language of some input from the set of available languages with explicit output layers. This is an additional output layer of our model, defined as a dense projection from the first hidden layer followed by a sigmoid. l = σ(h0 Wlid + blid ) Number of synsets As a measure of a target’s semantic ambiguity, we count the number of synsets that include it. For this, we rely on the language-specific WordNet resources for English (Fellbaum, 1998), Spanish (Gonzalez-Agirre et al., 2012) and French (Sagot and Fiˇser, 2008). For German, access to GermaNet (Hamp and Feldweg, 1997) was harder to obtain, and we instead automatically translate words from German to English and use the English WordNet.1 In case of a multi-word target, we take the mean number of synsets across the individual words. (5) During training, we then supply a ground truth language identifier ˆl as a second target variable and perform optimization under a cross-entropy loss that we add to the CWI loss. At test time, for a language without an explicit output layer, we first predict the most similar language w"
W18-0518,W18-0503,1,0.817879,"most notably random forest classifiers, which drew on a range of morphologic, semantic and psycholinguistic features, among others (Paetzold and Specia, 2016b; Ronzano et al., 2016). Yimam et al. (2017) present first work on CWI that considers languages other than English. They release a German and a Spanish dataset and present first CWI results for these languages. Notably, they also describe first cross-lingual experiments, in which they train on some language and test on another, i.e. without employing any of the common strategies for cross-lingual learning that we outline above. Recently, Bingel et al. (2018) showed promising results in predicting complex words from gaze patterns of Danish children with reading difficulties, 4 Model As outlined in Section 2, earlier work has shown the aptitude of ensemble methods for CWI, especially such ensembles that feature random forests. We further choose to address the problem in a cross-lingual fashion, for which we deem multitask learning models particularly suitable. Motivated by these observations, we devise an ensemble that comprises a number of random forests as well as feed-forward neural networks with hard parameter sharing. The random forests each c"
W18-0518,P15-4015,0,0.0240106,"task, this is the case for French, for which our system achieves the best performance. We further provide a qualitative and quantitative analysis of which words pose problems for our system. 1 RQ 1 How can multitask learning be applied to the task of cross-lingual CWI? Introduction RQ 2 How can complex words be identified in languages which are not seen during training time? Complex word identification (CWI) is the task of predicting whether a certain word might be difficult for a reader to understand and is typically used as a first step in (lexical) simplification pipelines (Shardlow, 2014; Paetzold and Specia, 2015, 2016a). This task has received significant attention from the community over the past few years, leading to two shared tasks and several other publications (Shardlow, 2013a,b). This paper presents our submission to the CWI 2018 shared task (Yimam et al., 2018), at the 13th Workshop on Innovative Use of NLP for Building Educational Applications. This task includes tracks targeting four languages: English, Spanish, German and French. For each of these languages, the task involves prediction of binary labels of whether any of a range of annotators deemed some word or phrase complex, or predicti"
W18-0518,E17-2026,1,0.841084,"include a thorough qualitative and quantitative error analysis, which shows that long and infrequent words are very likely to be complex, but that non-complex words that display these properties pose a challenge to our system. 2 2.1 Related work Multitask Learning Multitask learning (MTL) is the combined learning of several tasks in a single model (Caruana, 1997). This can be beneficial in a number of scenarios. Previous work has shown benefits, e.g., in cases where one has tasks which are closely related to one another (Bjerva, 2017a,b), when one task can help another escape a local minimum (Bingel and Søgaard, 2017), and when one has access to some unsupervised signal which can be beneficial to the task at hand (Rei, 2017). A common approach to MTL is the application of hard parameter sharing, in which some set of parameters in a model is shared between several tasks. We contribute to previous work in MTL by using a hard parameter sharing approach in which 166 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 166–174 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics we share intermediate layers between languag"
W18-0518,S16-1149,0,0.12371,"Missing"
W18-0518,P04-3031,0,0.137141,"s a measure of semantic specificity, we further consider the length of the hypernym chain of an item, i.e. the number of hypernyms that can recursively be obtained for a word. These are also obtained using WordNet, and again we average over individual words in a target. Inflectional complexity As a proxy for inflectional complexity (i.e. the number of suffixes appended to a word stem), we measure the difference in length (character count) between the surface form and the stem of a word. For this, we use language-specific instances of the Snowball stemmer (Porter, 2001) as implemented in NLTK (Bird and Loper, 2004). Features Surface features As basic surface features, we include the length of an item (in characters) and whether it is all-lowercase. Our systems build on the same set of features for all input languages, although some of these are computed from language-specific resources. This means that the distributions of values attained 1 For the translations, we used a bilingual dictionary (https://www.dict.cc/). 169 Language MAE Rank French German Spanish 0.066 0.075 0.079 1 2 3 ∆ (system) 0.012 (TMU) -0.013 (TMU) -0.007 (TMU) F1 Rank 0.7595 0.6621 0.7458 1 5 5 ∆ (system) 0.013 (TMU) -0.083 (TMU) -0"
W18-0518,petrov-etal-2012-universal,0,0.123499,"Missing"
W18-0518,W17-0225,1,0.910622,"has the disadvantage that it tends to place relatively high demands on availability of parallel text. Another frequently used approach in this context is to use machine translation (MT) so as to obtain a monolingual training set (Tiedemann et al., 2014). However, this approach necessarily increases the complexity of a system, as a fully-fledged MT system needs to be incorporated in the pipeline. Furthermore, this approach bypasses the problem of attempting to find methods or feature sets which can be successful across languages. We therefore follow previous work by, ¨ e.g. Bjerva and Ostling (2017) in that we use hard parameter sharing with language-agnostic input representations. We build upon this by leveraging language-specific resources which are widely available, such as Wikipedia dumps, and WordNet (see Section 5. 2.3 Training words is computed across all data splits. which opens up possibilities for personalized complex word identification, but it is less certain how well their method generalizes to other languages or demographics. 3 Data We use the data made available through the shared task (Yimam et al., 2018). Each training instance consists of a sentence, with a marked compl"
W18-0518,P17-1194,0,0.0262417,"o be complex, but that non-complex words that display these properties pose a challenge to our system. 2 2.1 Related work Multitask Learning Multitask learning (MTL) is the combined learning of several tasks in a single model (Caruana, 1997). This can be beneficial in a number of scenarios. Previous work has shown benefits, e.g., in cases where one has tasks which are closely related to one another (Bjerva, 2017a,b), when one task can help another escape a local minimum (Bingel and Søgaard, 2017), and when one has access to some unsupervised signal which can be beneficial to the task at hand (Rei, 2017). A common approach to MTL is the application of hard parameter sharing, in which some set of parameters in a model is shared between several tasks. We contribute to previous work in MTL by using a hard parameter sharing approach in which 166 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 166–174 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics we share intermediate layers between languages, and use one output-layer per language, thus in a sense seeing languages as tasks, similarly to Bjerva (20"
W18-0518,W17-0224,1,0.795209,"ions, although this has the disadvantage that it tends to place relatively high demands on availability of parallel text. Another frequently used approach in this context is to use machine translation (MT) so as to obtain a monolingual training set (Tiedemann et al., 2014). However, this approach necessarily increases the complexity of a system, as a fully-fledged MT system needs to be incorporated in the pipeline. Furthermore, this approach bypasses the problem of attempting to find methods or feature sets which can be successful across languages. We therefore follow previous work by, ¨ e.g. Bjerva and Ostling (2017) in that we use hard parameter sharing with language-agnostic input representations. We build upon this by leveraging language-specific resources which are widely available, such as Wikipedia dumps, and WordNet (see Section 5. 2.3 Training words is computed across all data splits. which opens up possibilities for personalized complex word identification, but it is less certain how well their method generalizes to other languages or demographics. 3 Data We use the data made available through the shared task (Yimam et al., 2018). Each training instance consists of a sentence, with a marked compl"
W18-0518,S16-1157,0,0.0399379,"hrases than English. This is further illustrated in Figure 1. In addition to the shared task data, we also use external resources in our feature representations (see Section 5). CWI Automatic complex word identification has a relatively short history as a research task, with first publications including Shardlow (2013a,b) A noticeable commonality of the highestscoring systems in the CWI 2016 shared task was the use of ensemble methods, most notably random forest classifiers, which drew on a range of morphologic, semantic and psycholinguistic features, among others (Paetzold and Specia, 2016b; Ronzano et al., 2016). Yimam et al. (2017) present first work on CWI that considers languages other than English. They release a German and a Spanish dataset and present first CWI results for these languages. Notably, they also describe first cross-lingual experiments, in which they train on some language and test on another, i.e. without employing any of the common strategies for cross-lingual learning that we outline above. Recently, Bingel et al. (2018) showed promising results in predicting complex words from gaze patterns of Danish children with reading difficulties, 4 Model As outlined in Section 2, earlier"
W18-0518,P13-3015,0,0.380255,"ystem. 1 RQ 1 How can multitask learning be applied to the task of cross-lingual CWI? Introduction RQ 2 How can complex words be identified in languages which are not seen during training time? Complex word identification (CWI) is the task of predicting whether a certain word might be difficult for a reader to understand and is typically used as a first step in (lexical) simplification pipelines (Shardlow, 2014; Paetzold and Specia, 2015, 2016a). This task has received significant attention from the community over the past few years, leading to two shared tasks and several other publications (Shardlow, 2013a,b). This paper presents our submission to the CWI 2018 shared task (Yimam et al., 2018), at the 13th Workshop on Innovative Use of NLP for Building Educational Applications. This task includes tracks targeting four languages: English, Spanish, German and French. For each of these languages, the task involves prediction of binary labels of whether any of a range of annotators deemed some word or phrase complex, or prediction of the ratio of those who did. The task further differs from previous approaches to CWI in extending the definition of the target units from the word level to multi-word"
W18-0518,W13-2908,0,0.151835,"ystem. 1 RQ 1 How can multitask learning be applied to the task of cross-lingual CWI? Introduction RQ 2 How can complex words be identified in languages which are not seen during training time? Complex word identification (CWI) is the task of predicting whether a certain word might be difficult for a reader to understand and is typically used as a first step in (lexical) simplification pipelines (Shardlow, 2014; Paetzold and Specia, 2015, 2016a). This task has received significant attention from the community over the past few years, leading to two shared tasks and several other publications (Shardlow, 2013a,b). This paper presents our submission to the CWI 2018 shared task (Yimam et al., 2018), at the 13th Workshop on Innovative Use of NLP for Building Educational Applications. This task includes tracks targeting four languages: English, Spanish, German and French. For each of these languages, the task involves prediction of binary labels of whether any of a range of annotators deemed some word or phrase complex, or prediction of the ratio of those who did. The task further differs from previous approaches to CWI in extending the definition of the target units from the word level to multi-word"
W18-0518,shardlow-2014-open,0,0.149665,". In the shared task, this is the case for French, for which our system achieves the best performance. We further provide a qualitative and quantitative analysis of which words pose problems for our system. 1 RQ 1 How can multitask learning be applied to the task of cross-lingual CWI? Introduction RQ 2 How can complex words be identified in languages which are not seen during training time? Complex word identification (CWI) is the task of predicting whether a certain word might be difficult for a reader to understand and is typically used as a first step in (lexical) simplification pipelines (Shardlow, 2014; Paetzold and Specia, 2015, 2016a). This task has received significant attention from the community over the past few years, leading to two shared tasks and several other publications (Shardlow, 2013a,b). This paper presents our submission to the CWI 2018 shared task (Yimam et al., 2018), at the 13th Workshop on Innovative Use of NLP for Building Educational Applications. This task includes tracks targeting four languages: English, Spanish, German and French. For each of these languages, the task involves prediction of binary labels of whether any of a range of annotators deemed some word or"
W18-0518,W14-1614,0,0.034255,"Missing"
W18-0518,W18-0507,0,0.207565,"? Introduction RQ 2 How can complex words be identified in languages which are not seen during training time? Complex word identification (CWI) is the task of predicting whether a certain word might be difficult for a reader to understand and is typically used as a first step in (lexical) simplification pipelines (Shardlow, 2014; Paetzold and Specia, 2015, 2016a). This task has received significant attention from the community over the past few years, leading to two shared tasks and several other publications (Shardlow, 2013a,b). This paper presents our submission to the CWI 2018 shared task (Yimam et al., 2018), at the 13th Workshop on Innovative Use of NLP for Building Educational Applications. This task includes tracks targeting four languages: English, Spanish, German and French. For each of these languages, the task involves prediction of binary labels of whether any of a range of annotators deemed some word or phrase complex, or prediction of the ratio of those who did. The task further differs from previous approaches to CWI in extending the definition of the target units from the word level to multi-word expressions, such that annotations in the training and test set spanned wider stretches o"
W18-0518,yimam-etal-2017-multilingual,0,0.0377369,"is is further illustrated in Figure 1. In addition to the shared task data, we also use external resources in our feature representations (see Section 5). CWI Automatic complex word identification has a relatively short history as a research task, with first publications including Shardlow (2013a,b) A noticeable commonality of the highestscoring systems in the CWI 2016 shared task was the use of ensemble methods, most notably random forest classifiers, which drew on a range of morphologic, semantic and psycholinguistic features, among others (Paetzold and Specia, 2016b; Ronzano et al., 2016). Yimam et al. (2017) present first work on CWI that considers languages other than English. They release a German and a Spanish dataset and present first CWI results for these languages. Notably, they also describe first cross-lingual experiments, in which they train on some language and test on another, i.e. without employing any of the common strategies for cross-lingual learning that we outline above. Recently, Bingel et al. (2018) showed promising results in predicting complex words from gaze patterns of Danish children with reading difficulties, 4 Model As outlined in Section 2, earlier work has shown the ap"
W18-3401,D15-1041,0,0.028899,"with a character-based sequence-to-sequence model. Furthermore, we experiment with different choices of external resources and corresponding auxiliary tasks and show that autoencoding can be as efficient as an auxiliary task for low-resource POS tagging as lemmatization. Finally, we evaluate our models on 34 typologically diverse languages. 2 Figure 1: Our multi-task architecture, consisting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We be"
W18-3401,E17-2026,1,0.947527,".6776(.00) .6226( - ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,W17-0225,1,0.850592,"or results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-reso"
W18-3401,C16-1333,1,0.850199,"owever, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevan"
W18-3401,D11-1005,0,0.0784096,"Missing"
W18-3401,K17-2001,0,0.389323,"- ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,P15-2044,1,0.928316,"Missing"
W18-3401,N16-1077,0,0.0302103,"Lemmatization is a task from the area of inflectional morphology. In particular, it is a special case of morphological inflection. Its goal is to map a given inflected word form to its lemma, e.g., sue˜no 7→ so˜nar. (6) Word autoencoding. For the word autoencoding task, we use the inflected forms from the SIGMORPHON 2017 shared task dataset for each respective setting. Due to identical forms for different slot in the morphological paradigm of some lemmas, we might have duplicate examples in those datasets. Sequence-to-sequence models have shown strong performances on morphological inflection (Aharoni et al., 2016; Kann and Sch¨utze, 2016; Makarov et al., 2017). Therefore, when morphological dictionaries are available, we can easily combine a neural model for lemmatization with a POS tagger, using our architecture. Our intuition for this auxiliary task is that it should be possible to include morphological information into our character-based word representations. Formally, the task can be described as follows. Let AL be a discrete alphabet for language L and let TL be a set of morphological tags for L. The morphological paradigm π of a lemma w in L is a set of pairs n o π(w) = fk [w], tk (7) Random s"
W18-3401,P15-1166,0,0.0831235,"Missing"
W18-3401,P07-1094,0,0.133713,"Missing"
W18-3401,P17-2054,1,0.900977,"Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to mention some important work here. Cross-lingual approaches have been used for a large variety of tasks, e.g., automatic speech recognition (Huang et al., 2013), entity recognition (Wang and Manning, 2014), language modeling (Ts"
W18-3401,N18-1172,1,0.839691,"nt training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to me"
W18-3401,P16-2067,1,0.917337,"ting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We believe that using subword-level auxiliary tasks to regularize the character-level encoding in hierarchical LSTMs is a flexible and efficient way to get the best of both worlds: such a model is still able to make predictions about unknown words, but the subword-level auxiliary task should prevent it from overfitting. 2.1 vc,i = conc(LSTMc,f (c1:m ), (1) LSTMc,b (cm:1 )) Second, a cont"
W18-3401,P17-1194,0,0.0202508,". Furthermore, their model is also a multi-task model, being trained jointly on predicting the POS and the log-frequency of a word. Their architecture obtained state-of-the-art results for POS tagging in several languages. Hence, in the low-resource setting considered here, we build upon the architecture developed by Plank et al. (2016), and extend it to a multi-task architecture involving sequenceto-sequence learning. Note though that in contrast to our setup, their tasks are both sequence-labeling tasks and using the same input for both tasks. The same holds true for the multi-task model by Rei (2017), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in S"
W18-3401,P17-1182,1,0.881428,"Missing"
W18-3401,P16-2090,1,0.82551,"Missing"
W18-3401,P11-2120,1,0.816951,"Missing"
W18-3401,P16-2038,1,0.707545,"o obtain better performance on a sequence classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lin"
W18-3401,Q16-1023,0,0.0435006,"individual languages. Here, we notice that autoencoders often outperform lemmatization for agglutinative languages. An explanation for this might be that agglutinative morphology is harder to learn, and the chance of overfitting on a small sample is therefore higher. Hyperparameters For all networks, we use 300-dimensional character embeddings, 64-dimensional word embeddings and 100-dimensional LSTM hidden states. Encoder and decoder LSTMs have 1 hidden layer each. For training, we use ADAM (Kingma and Ba, 2014), as well as word dropout and character dropout, each with a coefficient of 0.25 (Kiperwasser and Goldberg, 2016). Gaussian noise is added to the concatenation of the last states of the character LSTMs for POS tagging. All models are trained using early stopping, with a minimum number of 75 (single-task and low), 30 (medium) or 20 (high) epochs and a maximum number of 300 epochs, which is never reached. We stop training if we obtain no improvement for 10 consecutive epochs. The best model on the development set is used for testing. 5 Results The test results for all languages and settings are presented in Table 1. Our first observation is that using 100 words of auxiliary task data seems to be sufficient"
W18-3401,N06-1012,0,0.0581584,"), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in Sutton et al. (2006). Sutton et al. (2006) use a group lasso regularizer to prevent feature swamping. In the same way, we could also detect distributionally similar characters and use a group lasso regularizer to prevent covariate characters to swamp each other. However, this effect can potentially also hurt performance if done in an uninformed way. We intuit that this makes it also impossible for the model to learn useful similarities between characters (random string autoencod7 4.5 4.5 POS, main task POS+AE-Random, main task POS+AE-Random, aux. task POS+Lemmatization, main task POS+Lemmatization, aux. task POS+"
W18-3401,Q13-1001,0,0.0766196,"Missing"
W18-3401,D12-1127,0,0.148688,"Missing"
W18-3401,N16-1161,0,0.0651137,"Missing"
W18-3401,E17-1005,1,0.841116,"ce classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we cons"
W18-3401,Q14-1005,0,0.0607788,"Missing"
W18-3401,D13-1032,0,0.0818875,"Missing"
W18-3401,P12-1066,0,0.0687448,"Missing"
W18-3401,H01-1035,0,0.151424,"Missing"
W18-3401,N16-1004,0,0.0425229,"Missing"
