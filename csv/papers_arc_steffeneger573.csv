2021.starsem-1.22,Inducing Language-Agnostic Multilingual Representations,2021,-1,-1,2,1,994,wei zhao,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"Cross-lingual representations have the potential to make NLP techniques available to the vast majority of languages in the world. However, they currently require large pretraining corpora or access to typologically similar languages. In this work, we address these obstacles by removing language identity signals from multilingual embeddings. We examine three approaches for this: (i) re-aligning the vector spaces of target languages (all together) to a pivot source language; (ii) removing language-specific means and variances, which yields better discriminativeness of embeddings as a by-product; and (iii) increasing input similarity across languages by removing morphological contractions and sentence reordering. We evaluate on XNLI and reference-free MT evaluation across 19 typologically diverse languages. Our findings expose the limitations of these approaches{---}unlike vector normalization, vector space re-alignment and text normalization do not achieve consistent gains across encoders and languages. Due to the approaches{'} additive effects, their combination decreases the cross-lingual transfer gap by 8.9 points (m-BERT) and 18.2 points (XLM-R) on average across all tasks and languages, however."
2021.latechclfl-1.7,End-to-end style-conditioned poetry generation: What does it take to learn from examples alone?,2021,-1,-1,8,0,5481,jorg wockener,"Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"In this work, we design an end-to-end model for poetry generation based on conditioned recurrent neural network (RNN) language models whose goal is to learn stylistic features (poem length, sentiment, alliteration, and rhyming) from examples alone. We show this model successfully learns the {`}meaning{'} of length and sentiment, as we can control it to generate longer or shorter as well as more positive or more negative poems. However, the model does not grasp sound phenomena like alliteration and rhyming, but instead exploits low-level statistical cues. Possible reasons include the size of the training data, the relatively low frequency and difficulty of these sublexical phenomena as well as model biases. We show that more recent GPT-2 models also have problems learning sublexical phenomena such as rhyming from examples alone."
2021.inlg-1.32,{TUDA}-Reproducibility @ {R}epro{G}en: Replicability of Human Evaluation of Text-to-Text and Concept-to-Text Generation,2021,-1,-1,3,0,5973,christian richter,Proceedings of the 14th International Conference on Natural Language Generation,0,"This paper describes our contribution to the Shared Task ReproGen by Belz et al. (2021), which investigates the reproducibility of human evaluations in the context of Natural Language Generation. We selected the paper {``}Generation of Company descriptions using concept-to-text and text-to-text deep models: data set collection and systems evaluation{''} (Qader et al., 2018) and aimed to replicate, as closely to the original as possible, the human evaluation and the subsequent comparison between the human judgements and the automatic evaluation metrics. Here, we first outline the text generation task of the paper of Qader et al. (2018). Then, we document how we approached our replication of the paper{'}s human evaluation. We also discuss the difficulties we encountered and which information was missing. Our replication has medium to strong correlation (0.66 Spearman overall) with the original results of Qader et al. (2018), but due to the missing information about how Qader et al. (2018) compared the human judgements with the metric scores, we have refrained from reproducing this comparison."
2021.findings-acl.141,{BERT}-Defense: A Probabilistic Model Based on {BERT} to Combat Cognitively Inspired Orthographic Adversarial Attacks,2021,-1,-1,3,0,7843,yannik keller,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.eval4nlp-1.17,The {E}val4{NLP} Shared Task on Explainable Quality Estimation: Overview and Results,2021,-1,-1,4,0,2508,marina fomicheva,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,0,None
2021.emnlp-main.701,Global Explainability of {BERT}-Based Evaluation Metrics by Disentangling along Linguistic Factors,2021,-1,-1,3,0,10048,marvin kaster,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Evaluation metrics are a key ingredient for progress of text generation systems. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than BLEU or ROUGE, invented two decades ago. However, little is known what these metrics, which are based on black-box language model representations, actually capture (it is typically assumed they model semantic similarity). In this work, we use a simple regression based global explainability technique to disentangle metric scores along linguistic factors, including semantics, syntax, morphology, and lexical overlap. We show that the different metrics capture all aspects to some degree, but that they are all substantially sensitive to lexical overlap, just like BLEU and ROUGE. This exposes limitations of these novelly proposed metrics, which we also highlight in an adversarial test scenario."
2021.acl-long.129,Changes in {E}uropean Solidarity Before and During {COVID}-19: Evidence from a Large Crowd- and Expert-Annotated {T}witter Dataset,2021,-1,-1,4,0,12884,alexandra ils,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic. To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs. crowds). We use these annotations to train a BERT model with multiple data augmentation strategies. Our augmented BERT model that combines both expert and crowd annotations outperforms the baseline BERT classifier trained with expert annotations only by over 25 points, from 58{\%} macro-F1 to almost 85{\%}. We use this high-quality model to automatically label over 270k tweets between September 2019 and December 2020. We then assess the automatically labeled data for how statements related to European (anti-)solidarity discourses developed over time and in relation to one another, before and during the COVID-19 crisis. Our results show that solidarity became increasingly salient and contested during the crisis. While the number of solidarity tweets remained on a higher level and dominated the discourse in the scrutinized time frame, anti-solidarity tweets initially spiked, then decreased to (almost) pre-COVID-19 values before rising to a stable higher level until the end of 2020."
2021.acl-long.179,Better than Average: Paired Evaluation of {NLP} systems,2021,-1,-1,3,0.859917,12959,maxime peyrard,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Evaluation in NLP is usually done by comparing the scores of competing systems independently averaged over a common set of test instances. In this work, we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best, since the average, as well as alternatives such as the median, ignores the pairing arising from the fact that systems are evaluated on the same test instances. We illustrate the importance of taking the instancelevel pairing of evaluation scores into account and demonstrate, both theoretically and empirically, the advantages of aggregation methods based on pairwise comparisons, such as the Bradley{--}Terry (BT) model, a mechanism based on the estimated probability that a given system scores better than another on the test set. By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics, we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30{\%} of the setups. To facilitate the adoption of pairwise evaluation, we release a practical tool for performing the full analysis of evaluation scores with the mean, median, BT, and two variants of BT (Elo and TrueSkill), alongside functionality for appropriate statistical testing."
2020.semeval-1.22,{CMCE} at {S}em{E}val-2020 Task 1: Clustering on Manifolds of Contextualized Embeddings to Detect Historical Meaning Shifts,2020,-1,-1,3,0,15005,david rother,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper describes the system Clustering on Manifolds of Contextualized Embeddings (CMCE) submitted to the SemEval-2020 Task 1 on Unsupervised Lexical Semantic Change Detection. Subtask 1 asks to identify whether or not a word gained/lost a sense across two time periods. Subtask 2 is about computing a ranking of words according to the amount of change their senses underwent. Our system uses contextualized word embeddings from MBERT, whose dimensionality we reduce with an autoencoder and the UMAP algorithm, to be able to use a wider array of clustering algorithms that can automatically determine the number of clusters. We use Hierarchical Density Based Clustering (HDBSCAN) and compare it to Gaussian MixtureModels (GMMs) and other clustering algorithms. Remarkably, with only 10 dimensional MBERT embeddings (reduced from the original size of 768), our submitted model performs best on subtask 1 for English and ranks third in subtask 2 for English. In addition to describing our system, we discuss our hyperparameter configurations and examine why our system lags behind for the other languages involved in the shared task (German, Swedish, Latin). Our code is available at https://github.com/DavidRother/semeval2020-task1"
2020.lrec-1.205,"{PO}-{EMO}: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in {G}erman and {E}nglish Poetry",2020,54,1,2,1,5482,thomas haider,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Most approaches to emotion analysis of social media, literature, news, and other domains focus exclusively on basic emotion categories as defined by Ekman or Plutchik. However, art (such as literature) enables engagement in a broader range of more complex and subtle emotions. These have been shown to also include mixed emotional responses. We consider emotions in poetry as they are elicited in the reader, rather than what is expressed in the text or intended by the author. Thus, we conceptualize a set of aesthetic emotions that are predictive of aesthetic appreciation in the reader, and allow the annotation of multiple labels per line to capture mixed emotions within their context. We evaluate this novel setting in an annotation experiment both with carefully trained experts and via crowdsourcing. Our annotation with experts leads to an acceptable agreement of k = .70, resulting in a consistent dataset for future large scale analysis. Finally, we conduct first emotion classification experiments based on BERT, showing that identifying aesthetic emotions is challenging in our data, with up to .52 F1-micro on the German subset. Data and resources are available at https://github.com/tnhaider/poetry-emotion."
2020.conll-1.8,How to Probe Sentence Embeddings in Low-Resource Languages: On Structural Design Choices for Probing Task Evaluation,2020,-1,-1,1,1,995,steffen eger,Proceedings of the 24th Conference on Computational Natural Language Learning,0,"Sentence encoders map sentences to real valued vectors for use in downstream applications. To peek into these representations{---}e.g., to increase interpretability of their results{---}probing tasks have been designed which query them for linguistic knowledge. However, designing probing tasks for lesser-resourced languages is tricky, because these often lack largescale annotated data or (high-quality) dependency parsers as a prerequisite of probing task design in English. To investigate how to probe sentence embeddings in such cases, we investigate sensitivity of probing task results to structural design choices, conducting the first such large scale study. We show that design choices like size of the annotated probing dataset and type of classifier used for evaluation do (sometimes substantially) influence probing outcomes. We then probe embeddings in a multilingual setup with design choices that lie in a {`}stable region{'}, as we identify for English, and find that results on English do not transfer to other languages. Fairer and more comprehensive sentence-level probing evaluation should thus be carried out on multiple languages in the future."
2020.coling-main.105,Probing Multilingual {BERT} for Genetic and Typological Signals,2020,-1,-1,3,0,21002,taraka rama,Proceedings of the 28th International Conference on Computational Linguistics,0,"We probe the layers in multilingual BERT (mBERT) for phylogenetic and geographic language signals across 100 languages and compute language distances based on the mBERT representations. We 1) employ the language distances to infer and evaluate language trees, finding that they are close to the reference family tree in terms of quartet tree distance, 2) perform distance matrix regression analysis, finding that the language distances can be best explained by phylogenetic and worst by structural factors and 3) present a novel measure for measuring diachronic meaning stability (based on cross-lingual representation variability) which correlates significantly with published ranked lists based on linguistic approaches. Our results contribute to the nascent field of typological interpretability of cross-lingual text representations."
2020.coling-main.152,{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation,2020,-1,-1,2,0,21233,martin kerscher,Proceedings of the 28th International Conference on Computational Linguistics,0,We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence. We perceive of this as a new unsupervised probing task and show that it correlates well with downstream task performance. We also illustrate how the language generated from different encoders differs. We apply our approach to generate sentence analogies from sentence embeddings.
2020.codi-1.16,Evaluation of Coreference Resolution Systems Under Adversarial Attacks,2020,-1,-1,3,0,21812,haixia chai,Proceedings of the First Workshop on Computational Approaches to Discourse,0,"A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions unseen at train time. In this work, we create a new dataset based on CoNLL, which largely decreases mention overlaps in the entire dataset and exposes the limitations of published resolvers on two aspects{---}lexical inference ability and understanding of low-level orthographic noise. Our findings show (1) the requirements for embeddings, used in resolvers, and for coreference resolutions are, by design, in conflict and (2) adversarial approaches are sometimes not legitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus giving an inflated impression for the improvements."
2020.acl-main.124,{SUPERT}: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization,2020,35,0,3,0.618517,7649,yang gao,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39{\%}. Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval."
2020.acl-main.151,On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation,2020,56,0,6,1,994,wei zhao,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish {``}translationese{''}, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points."
2020.aacl-main.79,From Hero to Z{\\'e}roe: A Benchmark of Low-Level Adversarial Attacks,2020,-1,-1,1,1,995,steffen eger,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Adversarial attacks are label-preserving modifications to inputs of machine learning classifiers designed to fool machines but not humans. Natural Language Processing (NLP) has mostly focused on high-level attack scenarios such as paraphrasing input texts. We argue that these are less realistic in typical application scenarios such as in social media, and instead focus on low-level attacks on the character-level. Guided by human cognitive abilities and human robustness, we propose the first large-scale catalogue and benchmark of low-level adversarial attacks, which we dub Z{\'e}roe, encompassing nine different attack modes including visual and phonetic adversaries. We show that RoBERTa, NLP{'}s current workhorse, fails on our attacks. Our dataset provides a benchmark for testing robustness of future more human-like NLP models."
W19-4727,Semantic Change and Emerging Tropes In a Large Corpus of {N}ew {H}igh {G}erman Poetry,2019,24,0,2,1,5482,thomas haider,Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change,0,"Due to its semantic succinctness and novelty of expression, poetry is a great test-bed for semantic change analysis. However, so far there is a scarcity of large diachronic corpora. Here, we provide a large corpus of German poetry which consists of about 75k poems with more than 11 million tokens, with poems ranging from the 16th to early 20th century. We then track semantic change in this corpus by investigating the rise of tropes ({`}love is magic{'}) over time and detecting change points of meaning, which we find to occur particularly within the German Romantic period. Additionally, through self-similarity, we reconstruct literary periods and find evidence that the law of linear semantic change also applies to poetry."
W19-4308,Pitfalls in the Evaluation of Sentence Embeddings,2019,25,0,1,1,995,steffen eger,Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019),0,"Deep learning models continuously break new records across different NLP tasks. At the same time, their success exposes weaknesses of model evaluation. Here, we compile several key pitfalls of evaluation of sentence embeddings, a currently very popular NLP paradigm. These pitfalls include the comparison of embeddings of different sizes, normalization of embeddings, and the low (and diverging) correlations between transfer and probing tasks. Our motivation is to challenge the current evaluation of sentence embeddings and to provide an easy-to-access reference for future research. Based on our insights, we also recommend better practices for better future evaluations of sentence embeddings."
P19-1150,Towards Scalable and Reliable Capsule Networks for Challenging {NLP} Applications,2019,50,3,3,1,994,wei zhao,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Obstacles hindering the development of capsule networks for challenging NLP applications include poor scalability to large output spaces and less reliable routing processes. In this paper, we introduce: (i) an agreement score to evaluate the performance of routing processes at instance-level; (ii) an adaptive optimizer to enhance the reliability of routing; (iii) capsule compression and partial routing to improve the scalability of capsule networks. We validate our approach on two NLP tasks, namely: multi-label text classification and question answering. Experimental results show that our approach considerably improves over strong competitors on both tasks. In addition, we gain the best results in low-resource settings with few training instances."
N19-1129,Does My Rebuttal Matter? Insights from a Major {NLP} Conference,2019,0,0,2,0.618517,7649,yang gao,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Peer review is a core element of the scientific process, particularly in conference-centered fields such as ML and NLP. However, only few studies have evaluated its properties empirically. Aiming to fill this gap, we present a corpus that contains over 4k reviews and 1.2k author responses from ACL-2018. We quantitatively and qualitatively assess the corpus. This includes a pilot study on paper weaknesses given by reviewers and on quality of author responses. We then focus on the role of the rebuttal phase, and propose a novel task to predict after-rebuttal (i.e., final) scores from initial reviews and author responses. Although author responses do have a marginal (and statistically significant) influence on the final scores, especially for borderline papers, our results suggest that a reviewer{'}s final score is largely determined by her initial score and the distance to the other reviewers{'} initial scores. In this context, we discuss the conformity bias inherent to peer reviewing, a bias that has largely been overlooked in previous research. We hope our analyses will help better assess the usefulness of the rebuttal phase in NLP conferences."
N19-1165,Text Processing Like Humans Do: Visually Attacking and Shielding {NLP} Systems,2019,32,9,1,1,995,steffen eger,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., {``}!d10t{''}) or as a writing style ({``}1337{''} in {``}leet speak{''}), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82{\%}. We then explore three shielding methods{---}visual character embeddings, adversarial training, and rule-based recovery{---}which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks."
D19-1053,{M}over{S}core: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance,2019,0,11,6,1,994,wei zhao,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we investigate strategies to encode system and reference texts to devise a metric that shows a high correlation with human judgment of text quality. We validate our new metric, namely MoverScore, on a number of text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, where the outputs are produced by a variety of neural and non-neural systems. Our findings suggest that metrics combining contextualized representations with a distance measure perform the best. Such metrics also demonstrate strong generalization capability across tasks. For ease-of-use we make our metrics available as web service."
W18-5216,{PD}3: Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection,2018,0,0,1,1,995,steffen eger,Proceedings of the 5th Workshop on Argument Mining,0,"We consider unsupervised cross-lingual transfer on two tasks, viz., sentence-level argumentation mining and standard POS tagging. We combine direct transfer using bilingual embeddings with annotation projection, which projects labels across unlabeled parallel data. We do so by either merging respective source and target language datasets or alternatively by using multi-task learning. Our combination strategy considerably improves upon both direct transfer and projection with few available parallel sentences, the most realistic scenario for many low-resource target languages."
W18-4508,One Size Fits All? A simple {LSTM} for non-literal token and construction-level classification,2018,0,0,2,0,25887,eriklan dinh,"Proceedings of the Second Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"In this paper, we tackle four different tasks of non-literal language classification: token and construction level metaphor detection, classification of idiomatic use of infinitive-verb compounds, and classification of non-literal particle verbs. One of the tasks operates on the token level, while the three other tasks classify constructions such as {``}hot topic{''} or {``}stehen lassen{''} ({``}to allow sth. to stand{''} vs. {``}to abandon so.{''}). The two metaphor detection tasks are in English, while the two non-literal language detection tasks are in German. We propose a simple context-encoding LSTM model and show that it outperforms the state-of-the-art on two tasks. Additionally, we experiment with different embeddings for the token level metaphor detection task and find that 1) their performance varies according to the genre, and 2) word2vec embeddings perform best on 3 out of 4 genres, despite being one of the simplest tested model. In summary, we present a large-scale analysis of a neural model for non-literal language classification (i) at different granularities, (ii) in different languages, (iii) over different non-literal language phenomena."
N18-5005,{A}rgumen{T}ext: Searching for Arguments in Heterogeneous Sources,2018,0,16,7,0,25574,christian stab,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"Argument mining is a core technology for enabling argument search in large corpora. However, most current approaches fall short when applied to heterogeneous texts. In this paper, we present an argument retrieval system capable of retrieving sentential arguments for any given controversial topic. By analyzing the highest-ranked results extracted from Web sources, we found that our system covers 89{\%} of arguments found in expert-curated lists of arguments from an online debate portal, and also identifies additional valid arguments."
N18-2006,Multi-Task Learning for Argumentation Mining in Low-Resource Settings,2018,23,3,2,0,21698,claudia schulz,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We investigate whether and where multi-task learning (MTL) can improve performance on NLP problems related to argumentation mining (AM), in particular argument component identification. Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks."
D18-1472,Is it Time to Swish? Comparing Deep Learning Activation Functions Across {NLP} tasks,2018,0,12,1,1,995,steffen eger,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Activation functions play a crucial role in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. One of the currently most popular activation functions is ReLU, but several competitors have recently been proposed or {`}discovered{'}, including LReLU functions and swish. While most works compare newly proposed activation functions on few tasks (usually from image classification) and against few competitors (usually ReLU), we perform the first largescale comparison of 21 activation functions across eight different NLP tasks. We find that a largely unknown activation function performs most stably across all tasks, the so-called penalized tanh function. We also show that it can successfully replace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage point (pp) improvement over the standard choices on a challenging NLP task."
C18-1071,Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!,2018,0,3,1,1,995,steffen eger,Proceedings of the 27th International Conference on Computational Linguistics,0,"Argumentation mining (AM) requires the identification of complex discourse structures and has lately been applied with success monolingually. In this work, we show that the existing resources are, however, not adequate for assessing cross-lingual AM, due to their heterogeneity or lack of complexity. We therefore create suitable parallel corpora by (human and machine) translating a popular AM dataset consisting of persuasive student essays into German, French, Spanish, and Chinese. We then compare (i) annotation projection and (ii) bilingual word embeddings based direct transfer strategies for cross-lingual AM, finding that the former performs considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/coling2018-xling{\_}argument{\_}mining."
C18-1132,Killing Four Birds with Two Stones: Multi-Task Learning for Non-Literal Language Detection,2018,0,0,2,0,25887,eriklan dinh,Proceedings of the 27th International Conference on Computational Linguistics,0,"Non-literal language phenomena such as idioms or metaphors are commonly studied in isolation from each other in NLP. However, often similar definitions and features are being used for different phenomena, challenging the distinction. Instead, we propose to view the detection problem as a generalized non-literal language classification problem. In this paper we investigate multi-task learning for related non-literal language phenomena. We show that in contrast to simply joining the data of multiple tasks, multi-task learning consistently improves upon four metaphor and idiom detection tasks in two languages, English and German. Comparing two state-of-the-art multi-task learning architectures, we also investigate when soft parameter sharing and learned information flow can be beneficial for our related tasks. We make our adapted code publicly available."
W17-3402,How Many Stemmata with Root Degree k?,2017,16,0,2,0,14669,armin hoenen,Proceedings of the 15th Meeting on the Mathematics of Language,0,None
S17-2163,{EELECTION} at {S}em{E}val-2017 Task 10: Ensemble of n{E}ural Learners for k{E}yphrase {C}lassifica{TION},2017,2,0,1,1,995,steffen eger,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper describes our approach to the SemEval 2017 Task 10: Extracting Keyphrases and Relations from Scientific Publications, specifically to Subtask (B): Classification of identified keyphrases. We explored three different deep learning approaches: a character-level convolutional neural network (CNN), a stacked learner with an MLP meta-classifier, and an attention based Bi-LSTM. From these approaches, we created an ensemble of differently hyper-parameterized systems, achieving a micro-$F_1$-score of 0.63 on the test data. Our approach ranks 2nd (score of 1st placed system: 0.64) out of four according to this official score. However, we erroneously trained 2 out of 3 neural nets (the stacker and the CNN) on only roughly 15{\%} of the full data, namely, the original development set. When trained on the full data (training+development), our ensemble has a micro-$F_{1}$-score of 0.69. Our code is available from \url{https://github.com/UKPLab/semeval2017-scienceie}."
P17-1002,Neural End-to-End Learning for Computational Argumentation Mining,2017,40,14,1,1,995,steffen eger,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning {`}natural{'} subtasks, in a multi-task learning setup, improves performance."
D17-1218,What is the Essence of a Claim? Cross-Domain Claim Identification,2017,45,0,2,0.789474,1556,johannes daxenberger,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent conceptualization of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps."
P16-2009,On the Linearity of Semantic Change: Investigating Meaning Variation via Dynamic Graph Models,2016,23,20,1,1,995,steffen eger,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We consider two graph models of semantic change. The first is a time-series model that relates embedding vectors from one time period to embedding vectors of previous time periods. In the second, we construct one graph for each word: nodes in this graph correspond to time points and edge weights to the similarity of the wordxe2x80x99s meaning across two time points. We apply our two models to corpora across three different languages. We find that semantic change is linear in two senses. Firstly, todayxe2x80x99s embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both findings as new laws/hypotheses of semantic change."
L16-1239,Lemmatization and Morphological Tagging in {G}erman and {L}atin: A Comparison and a Survey of the State-of-the-art,2016,0,9,1,1,995,steffen eger,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper relates to the challenge of morphological tagging and lemmatization in morphologically rich languages by example of German and Latin. We focus on the question what a practitioner can expect when using state-of-the-art solutions out of the box. Moreover, we contrast these with old(er) methods and implementations for POS tagging. We examine to what degree recent efforts in tagger development are reflected by improved accuracies â and at what cost, in terms of training and processing time. We also conduct in-domain vs. out-domain evaluation. Out-domain evaluations are particularly insightful because the distribution of the data which is being tagged by a user will typically differ from the distribution on which the tagger has been trained. Furthermore, two lemmatization techniques are evaluated. Finally, we compare pipeline tagging vs. a tagging approach that acknowledges dependencies between inflectional categories."
C16-1160,Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks,2016,32,6,2,0,15452,carsten schnober,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We analyze the performance of encoder-decoder neural models and compare them with well-known established methods. The latter represent different classes of traditional approaches that are applied to the monotone sequence-to-sequence tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion, and lemmatization. Such tasks are of practical relevance for various higher-level research fields including digital humanities, automatic text correction, and speech recognition. We investigate how well generic deep-learning approaches adapt to these tasks, and how they perform in comparison with established and more specialized methods, including our own adaptation of pruned CRFs."
C16-1331,Language classification from bilingual word embedding graphs,2016,35,1,1,1,995,steffen eger,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We study the role of the second language in bilingual word embeddings in monolingual semantic evaluation tasks. We find strongly and weakly positive correlations between down-stream task performance and second language similarity to the target language. Additionally, we show how bilingual word embeddings can be employed for the task of semantic language classification and that joint semantic spaces vary in meaningful ways across second languages. Our results support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact."
W15-3716,Lexicon-assisted tagging and lemmatization in {L}atin: A comparison of six taggers and two lemmatization methods,2015,20,5,1,1,995,steffen eger,"Proceedings of the 9th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({L}a{T}e{CH})",0,"We present a survey of tagging accuracies xe2x80x94 concerning part-of-speech and full morphological tagging xe2x80x94 for several taggers based on a corpus for medieval church Latin (see www.comphistsem.org). The best tagger in our sample, Lapos, has a PoS tagging accuracy of close to 96% and an overall tagging accuracy (including full morphological tagging) of about 85%. When we xe2x80x98intersectxe2x80x99 the taggers with our lexicon, the latter score increases to almost 91% for Lapos. A conservative assessment of lemmatization accuracy on our data estimates a score of 93-94% for a lexicon-based lemmatization strategy and a score of 94-95% for lemmatizing via trained lemmatizers."
S15-1014,Towards Semantic Language Classification: Inducing and Clustering Semantic Association Networks from {E}uroparl,2015,31,2,1,1,995,steffen eger,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"We induce semantic association networks from translation relations in parallel corpora. The resulting semantic spaces are encoded in a single reference language, which ensures cross-language comparability. As our main contribution, we cluster the obtained (crosslingually comparable) lexical semantic spaces. We find that, in our sample of languages, lexical semantic spaces largely coincide with genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task."
P15-1088,Multiple Many-to-Many Sequence Alignment for Combining String-Valued Variables: A {G}2{P} Experiment,2015,36,5,1,1,995,steffen eger,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We investigate multiple many-to-many alignments as a primary step in integrating supplemental information strings in string transduction. Besides outlining DP based solutions to the multiple alignment problem, we detail an approximation of the problem in terms of multiple sequence segmentations satisfying a coupling constraint. We apply our approach to boosting baseline G2P systems using homogeneous as well as heterogeneous sources of supplemental information."
D15-1139,Do we need bigram alignment models? On the effect of alignment quality on transduction accuracy in {G}2{P},2015,29,3,1,1,995,steffen eger,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We investigate the need for bigram alignment models and the benefit of supervised alignment techniques in graphemeto-phoneme (G2P) conversion. Moreover, we quantitatively estimate the relationship between alignment quality and overall G2P system performance. We find that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task. Moreover, we find that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so. Finally, we estimate a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature."
S12-1015,Lexical semantic typologies from bilingual corpora {---} A framework,2012,12,1,1,1,995,steffen eger,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We present a framework, based on Sejane and Eger (2012), for inducing lexical semantic typologies for groups of languages. Our framework rests on lexical semantic association networks derived from encoding, via bilingual corpora, each language in a common reference language, the tertium comparationis, so that distances between languages can easily be determined."
C12-1048,"{S}-Restricted Monotone Alignments: Algorithm, Search Space, and Applications",2012,28,5,1,1,995,steffen eger,Proceedings of {COLING} 2012,0,"We present a simple and straightforward alignment algorithm for monotone many-to-many alignments in grapheme-to-phoneme conversion and related fields such as morphology, and discuss a few noteworthy extensions. Moreover, we specify combinatorial formulas for monotone many-to-many alignments and decoding in G2P which indicate that exhaustive enumeration is generally possible, so that some limitations of our approach can easily be overcome. Finally, we present a decoding scheme, within the monotone many-to-many alignment paradigm, that relates the decoding problem to restricted integer compositions and that is, putatively, superior to alternatives suggested in the literature. Title and Abstract in German S-beschrankte monotone Alignierungen: Algorithmus, Suchraum und Anwendungen Wir prasentieren einen einfachen Alignment-Algorithmus fur monotone xe2x80x98many-tomanyxe2x80x99 Alignierungen im Bereich Graphem-zu-Phonem-Konversion und verwandten Gebieten wie z.B. Morphologie, und besprechen sinnvolle Erweiterungen. Daruber hinaus geben wir kombinatorische Formeln im Bereich der monotonen xe2x80x98many-to-manyxe2x80x99 Alignierungen und im Bereich des Decoding in G2P an, die suggerieren, dass vollstandige Enumeration hier im Allgemeinen moglich ist, sodass ein paar Einschrankungen unseres Ansatzes leicht behoben werden konnen. Schlieslich prasentieren wir ein Decoding-Schema, innerhalb des Paradigmas von xe2x80x98many-to-manyxe2x80x99 Alignierungen, dass das Decoding-Problem mit beschrankten Zahlenkompositionen in Beziehung setzt und das in der Literatur vorgeschlagenen Alternativen vermeintlich uberlegen ist."
