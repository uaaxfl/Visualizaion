2020.acl-main.586,N18-2123,0,0.0792663,"of visual grounding and its application in downstream tasks such as image retrieval (Young et al., 2014) and question answering (Antol et al., 2015; Zhu et al., 2016). To track ∗ Region Proposals (r1, r2) progress on this task, various datasets have been proposed, in which real world images are annotated by crowdsourced workers (Kazemzadeh et al., 2014; Mao et al., 2016). Recently, neural models have achieved tremendous progress on these datasets (Yu et al., 2018; Lu et al., 2019). However, multiple studies have suggested that these models could be exploiting strong biases in these datasets (Cirik et al., 2018b; Liu et al., 2019). For example, models could be just selecting a salient object in an image or a referring expression without recourse to linguistic structure (see Figure 1). This defeats the true purpose of the task casting doubts on the actual progress. In this work, we examine RefCOCOg dataset (Mao et al., 2016), a popular testbed for evaluating referring expression models, using crowdsourced workers. We show that a large percentage of samples in the RefCOCOg test set indeed do not rely on linguistic structure (word order) of the expressions. Accordingly, we split RefCOCOg test set into"
2020.acl-main.586,D17-1303,1,0.827126,"of ViL BERT, [x]+ is the hinge loss defined by max 0, x , and τ is the margin parameter. F indicates a function over all batch samples. We define F to be either sum of hinges (Sum-H) or max of hinges (Max-H). While Sum-H takes sum over all nega tive samples, If batch size is n, foreach i, e, b , 0 0 0 there will be  n−1 triplets of i , e, 0b and  i, e , b . 0 For i,e, b , there will be one i , e, b and one i, e0 , b . Similar proposals are known to increase the robustness of vision and language problems like visual-semantic embeddings and image description ranking (Kiros et al., 2014; Gella et al., 2017; Faghri et al., 2018). Multi-task Learning (MTL) with GQA In order to increase the sensitivity to linguistic structure, we rely on tasks that require reasoning on linguistic structure and learn to perform them alongside our task. We employ MTL with GQA (Hudson and Manning, 2019), a compositional visual question answering dataset. Specifically, we use the GQA-Rel split which contains questions that require reasoning on both linguistic structure and spatial relations (e.g., Is there a boy wearing a red hat standing next to yellow bus? as opposed to Is there a boy wearing hat?). Figure 3 depicts"
2020.acl-main.586,D14-1086,0,\N,Missing
2020.acl-main.586,P18-1238,0,\N,Missing
2020.clinicalnlp-1.15,Q17-1010,0,0.0656662,"Missing"
2020.clinicalnlp-1.15,W19-5010,0,0.079659,"Second, we believe that understanding natural language in a knowledge-rich domain such as medicine requires understanding of domain knowledge at some level, similar to how humans can understand medical text only after receiving medical training. The abbreviation disambiguation task enables models to use domain knowledge to understand the global and local context, as well as the possible meanings of the abbreviation in the medical domain. Medical abbreviation disambiguation has long been studied (Skreta et al., 2019; Li et al., 2019; Finley et al., 2016; Liu et al., 2018; Joopudi et al., 2018; Jin et al., 2019) and our work builds upon many of them. In particular, our data generation process is inspired by the reverse substitution tech2 For example, ‘MR’ is a commonly used abbreviation which has a number of possible meanings, including ‘morphinone reductase’, ‘magnetoresistance’ and ‘menstrual regulation’, depending on the context. 130 Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 130–135 c November 19, 2020. 2020 Association for Computational Linguistics Task-speciﬁc data Pre-training MeDAL Initialized model Pre-training: abbreviation disambiguation Fine-tuning Diagnos"
2020.findings-emnlp.221,P18-1090,0,0.0206227,"ranking agent. This is a strong result as HITS@1/20 is a tough metric to improve upon (Hancock et al., 2019). Our work joins the class of models that use natural language feedback to improve different tasks, e.g., image captioning (Ling and Fidler, 2017), classification (Srivastava et al., 2017; Hancock et al., 2018; Murty et al., 2020). While these methods use feedback for reward shaping or feature extraction, we use feedback to produce correct response using adversarial learning. We pose this problem as a style transfer problem inspired from the style transfer literature (Shen et al., 2017; Xu et al., 2018; Li et al., 2018; Conneau and Lample, 2019; Dai et al., 2019). While these focus on studying the stylistic attributes of sentences, e.g, sentiment, we explore this problem in the context of improving chatbots. Acknowledgements We thank Yue Dong for her helpful discussions during the course of this project. We also thank Sandeep Subramanian for his insightful guidance at a crucial stage of this work. This research was enabled in part by computations support provided by Compute Canada (www.computecanada.ca). The last author is supported by the NSERC Discovery Grant on Robust conversational mode"
2021.acl-long.416,D14-1162,0,0.0856692,"language modeling ability of language models. pora using variants of language modeling objective (i.e., predicting a word given its surrounding context). In the recent years, these representations empowered neural models to attain unprecedented levels of performance gains on multiple language tasks. The resulting models are being deployed widely as services on platforms like Google Cloud and Amazon AWS to serve millions of users. Introduction A key idea behind the current success of neural network models for language is pretrained representations such as word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019). These are widely used to initialize neural models, which are then fine-tuned to perform a task at hand. Typically, these are learned from massive text corWhile this growth is commendable, there are concerns about the fairness of these models. Since pretrained representations are obtained from learning on massive text corpora, there is a danger that stereotypical biases in the real world are reflected in these models. For example, GPT2 (Radford et al., 2019"
2021.acl-long.416,N18-1202,0,0.0555007,"variants of language modeling objective (i.e., predicting a word given its surrounding context). In the recent years, these representations empowered neural models to attain unprecedented levels of performance gains on multiple language tasks. The resulting models are being deployed widely as services on platforms like Google Cloud and Amazon AWS to serve millions of users. Introduction A key idea behind the current success of neural network models for language is pretrained representations such as word embeddings (Mikolov et al., 2013; Pennington et al., 2014) and pretrained language models (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019). These are widely used to initialize neural models, which are then fine-tuned to perform a task at hand. Typically, these are learned from massive text corWhile this growth is commendable, there are concerns about the fairness of these models. Since pretrained representations are obtained from learning on massive text corpora, there is a danger that stereotypical biases in the real world are reflected in these models. For example, GPT2 (Radford et al., 2019), a pretrained language model, has shown to generat"
2021.acl-long.416,D19-1339,0,0.0452073,"o initialize neural models, which are then fine-tuned to perform a task at hand. Typically, these are learned from massive text corWhile this growth is commendable, there are concerns about the fairness of these models. Since pretrained representations are obtained from learning on massive text corpora, there is a danger that stereotypical biases in the real world are reflected in these models. For example, GPT2 (Radford et al., 2019), a pretrained language model, has shown to generate unpleasant stereotypical text when prompted with context containing certain races such as African-Americans (Sheng et al., 2019). In this work, we assess the stereotypical biases of popular pretrained language models. 5356 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5356–5371 August 1–6, 2021. ©2021 Association for Computational Linguistics The seminal works of Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) contain stereotypical biases using diagnostic methods like word analogies and associa"
2021.acl-long.416,N18-2003,0,0.0671341,"and ignores discourse-level (inter5358 sentence) measurements. Furthermore, StereoSet contains an order of magnitude of data that contains greater variety, and hence, has the potential to detect a wider range of biases that may be otherwise overlooked. Lastly, StereoSet measures bias across both masked and autoregressive language models, while CrowS-Pairs only measures bias in masked language models. 3.3 Measuring bias through extrinsic tasks Another method to evaluate bias in pretrained representations is to measure bias on extrinsic tasks like coreference resolution (Rudinger et al., 2018; Zhao et al., 2018) and sentiment analysis (Kiritchenko and Mohammad, 2018). This method fine-tunes pretrained representations on the target task. The bias in pretrained representations is estimated by the target task’s performance. However, it is hard to segregate the bias of task-specific training data from the pretrained representations. Our CATs are an intrinsic way to evaluate bias in pretrained models. 4 Dataset Creation In StereoSet, we select four domains as the target domains of interest for measuring bias: gender, profession, race and religion. For each domain, we select terms (e.g., Asian) that repres"
2021.emnlp-main.516,2020.acl-main.586,1,0.895285,"Missing"
2021.emnlp-main.516,N16-1181,0,0.528855,"xample from the CLEVR-Ref+ dataset. In addition to passing textual inputs (arguments) cubical, large and metallic to neural modules, we also provide them with the relevant neighborhood of arguments as context (highlighted in blue). language-to-vision matching problem and has several downstream applications such as question answering, robot navigation, and image retrieval (Zhu et al., 2016; Qi et al., 2020; Young et al., 2014; Yang et al., 2016; Tu et al., 2014; Qi et al., 2015; Liu et al., 2016; Akula and Zhu, 2019; Akula, 2015; Palakurthi et al., 2015). Recently, neural module networks (NMN; Andreas et al. 2016b; Hu et al. 2017b; Liu et al. 2019) have been gaining popularity as a promising approach for solving this task. Briefly, NMN models use an explicit modular reasoning process where a program generator first analyzes the input referring expression and predicts a sequence of learnable neural modules (e.g. count, filter, compare). Next, an execution engine dynamically assembles these modules to predict the target object in the image. Such a 1 Introduction module based hierarchical reasoning process helps NMNs in providing high model interpretability and Visual referring expression recognition is"
2021.emnlp-main.516,D14-1086,0,0.256438,"alyzes the input referring expression and predicts a sequence of learnable neural modules (e.g. count, filter, compare). Next, an execution engine dynamically assembles these modules to predict the target object in the image. Such a 1 Introduction module based hierarchical reasoning process helps NMNs in providing high model interpretability and Visual referring expression recognition is the task therefore facilitates in improving overall trust in the of identifying the object in an image that is referred model (Andreas et al., 2016b; Akula et al., 2020b). to by a natural language expression (Kazemzadeh et al., 2014; Mao et al., 2016). It is a fundamental Although achieving promising results, exist6398 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6398–6416 c November 7–11, 2021. 2021 Association for Computational Linguistics ing NMN models primarily focused on designing module architectures with textual inputs directly hard-coded in the module instantiation (Johnson et al., 2017b; Liu et al., 2019). For example, processing the textual inputs ‘red’ and ‘blue’ require the instantiation of two different modules filter_color[red] and filter_color[blue]. Howeve"
2021.emnlp-main.516,D16-1155,1,0.877792,"terial Argument [cube] [large] [metallic] Context [cubical] [big cubical] [big cubical metallic thing] Figure 1: An example from the CLEVR-Ref+ dataset. In addition to passing textual inputs (arguments) cubical, large and metallic to neural modules, we also provide them with the relevant neighborhood of arguments as context (highlighted in blue). language-to-vision matching problem and has several downstream applications such as question answering, robot navigation, and image retrieval (Zhu et al., 2016; Qi et al., 2020; Young et al., 2014; Yang et al., 2016; Tu et al., 2014; Qi et al., 2015; Liu et al., 2016; Akula and Zhu, 2019; Akula, 2015; Palakurthi et al., 2015). Recently, neural module networks (NMN; Andreas et al. 2016b; Hu et al. 2017b; Liu et al. 2019) have been gaining popularity as a promising approach for solving this task. Briefly, NMN models use an explicit modular reasoning process where a program generator first analyzes the input referring expression and predicts a sequence of learnable neural modules (e.g. count, filter, compare). Next, an execution engine dynamically assembles these modules to predict the target object in the image. Such a 1 Introduction module based hierarchic"
2021.emnlp-main.516,N19-1119,0,0.0606828,"Missing"
2021.emnlp-main.516,N16-1019,1,0.893112,"Missing"
2021.emnlp-main.516,Q14-1006,0,0.0221829,"t of [ big cubical metallic thing ]e 1 filter shape filter size filter material Argument [cube] [large] [metallic] Context [cubical] [big cubical] [big cubical metallic thing] Figure 1: An example from the CLEVR-Ref+ dataset. In addition to passing textual inputs (arguments) cubical, large and metallic to neural modules, we also provide them with the relevant neighborhood of arguments as context (highlighted in blue). language-to-vision matching problem and has several downstream applications such as question answering, robot navigation, and image retrieval (Zhu et al., 2016; Qi et al., 2020; Young et al., 2014; Yang et al., 2016; Tu et al., 2014; Qi et al., 2015; Liu et al., 2016; Akula and Zhu, 2019; Akula, 2015; Palakurthi et al., 2015). Recently, neural module networks (NMN; Andreas et al. 2016b; Hu et al. 2017b; Liu et al. 2019) have been gaining popularity as a promising approach for solving this task. Briefly, NMN models use an explicit modular reasoning process where a program generator first analyzes the input referring expression and predicts a sequence of learnable neural modules (e.g. count, filter, compare). Next, an execution engine dynamically assembles these modules to predict the ta"
2021.emnlp-main.516,P19-1644,0,0.019414,"dividual row j in M :  K ht , Mt (j) = ht · Mt (j) . kht k Mt (j) (3) A read weight vector wt is computed using a softmax over the cosine similarity and then a memory row mt is retrieved. The vectors mt , ht are concatenated with the textual input (em ) and then an element-wise multiplication is performed with the output of the convolution layer before passing to the ReLU function (see Appendix A). 5 5.1 Experiments Datasets We evaluate our approach on F-Ref and S-Ref splits of CLEVR-Ref+ (Liu et al., 2019). In addition, we also test our approach on CLOSURE (Bahdanau et al., 2019) and NLVR2 (Suhr et al., 2019) benchmarks. CLOSURE is a VQA benchmark, consisting of synthetically generated image and question pairs with emphasis on grounding simple and complex referring expressions. NLVR2 is a language grounding task where the goal is to determine whether an expression is true based on two paired real images. While reporting results on CLOSURE, we train our NMN model using CLEVR (Johnson et al., 2017a) train and val splits. 5.2 Baselines 2019) is a direct extension to FiLM that uses vectorvalued inputs and outputs for the modules instead of high-capacity 3D tensors; (5) NS-VQA (Yi et al., 2018) uses st"
2021.emnlp-main.516,D19-1514,0,0.0344574,"Missing"
2021.emnlp-main.516,R15-1065,1,0.723961,"ubical] [big cubical] [big cubical metallic thing] Figure 1: An example from the CLEVR-Ref+ dataset. In addition to passing textual inputs (arguments) cubical, large and metallic to neural modules, we also provide them with the relevant neighborhood of arguments as context (highlighted in blue). language-to-vision matching problem and has several downstream applications such as question answering, robot navigation, and image retrieval (Zhu et al., 2016; Qi et al., 2020; Young et al., 2014; Yang et al., 2016; Tu et al., 2014; Qi et al., 2015; Liu et al., 2016; Akula and Zhu, 2019; Akula, 2015; Palakurthi et al., 2015). Recently, neural module networks (NMN; Andreas et al. 2016b; Hu et al. 2017b; Liu et al. 2019) have been gaining popularity as a promising approach for solving this task. Briefly, NMN models use an explicit modular reasoning process where a program generator first analyzes the input referring expression and predicts a sequence of learnable neural modules (e.g. count, filter, compare). Next, an execution engine dynamically assembles these modules to predict the target object in the image. Such a 1 Introduction module based hierarchical reasoning process helps NMNs in providing high model inte"
2021.emnlp-main.516,P13-1043,0,0.0184299,"first convolution layer in the RCB block to produce joint representation cm of module’s textual input (em ) and visual input (vm ), which is then passed to ReLU function (see Appendix Figure 1b): Neural Module Networks. Neural module networks (NMNs) learn to parse textual expressions as executable programs composed of learnable neural modules (Andreas et al., 2016b; Johnson et al., 2017a,b; Hu et al., 2017a). Each of these modules are specialized to compute basic reasoning tasks and can be assembled to perform complex and compositional reasoning. (Andreas et al., 2016b) used dependency trees (Zhu et al., 2013) to generate the execution layouts. (Andreas et al., 2016a)  h = LSTM e , h , t m,t t−1 proposed dynamic NMNs that learns and adapts (1) cm = conv (vm ) ht . the structure of the execution layouts to the question. (Johnson et al., 2017b) proposed homogeTable 2 shows the count of distinct modules and neous (IEP) and generic neural modules, unlike the model performance before and after paramefixed and hand-crafted neural module, in which the terizing the RCB modules (i.e. IEP-Ref vs P-Ref). semantics of each neural module is learnt during As we can see, there are total 60 distinct modules train"
2021.emnlp-main.566,Q19-1026,0,0.0382084,"Missing"
2021.emnlp-main.566,W19-8624,0,0.0166545,"ow). ST and BT refer to Self-training and Back-training models respectively. and corresponding generated questions are sampled from the MLQuestions test set and annotated by a domain expert. We find that the model generates few Explanation questions and even fewer Preference questions while over-generating Description questions. Comparison and Method questions show good F1-score overall, hence these classes benefit the most from domain adaptation. 6 Related Work Question Generation methods have focused on training neural Seq2Seq models (Du et al., 2017; Mishra et al., 2020; Zhao et al., 2018; Chan and Fan, 2019; Klein and Nabi, 2019) on supervised QA datasets such as SQuAD (Rajpurkar et al., 2016). Many recent works such as (Tang et al., 2017; Wang et al., 2017) recognize the duality between QG and QA and propose joint training for the two. Duan et al. (2017) generate QA pairs from YahooAnswers, and improve QA by adding a question-consistency loss in addition to QA loss. Our work instead establishes strong duality between QG and IR task. Ours is also the first work towards unsupervised domain adaptation for QG to the best of our knowledge. Data Augmentation methods like self-training have been appli"
2021.emnlp-main.566,N18-1143,0,0.0229939,"s SQuAD (Rajpurkar et al., 2016). Many recent works such as (Tang et al., 2017; Wang et al., 2017) recognize the duality between QG and QA and propose joint training for the two. Duan et al. (2017) generate QA pairs from YahooAnswers, and improve QA by adding a question-consistency loss in addition to QA loss. Our work instead establishes strong duality between QG and IR task. Ours is also the first work towards unsupervised domain adaptation for QG to the best of our knowledge. Data Augmentation methods like self-training have been applied in numerous NLP problems such as question answering (Chung et al., 2018), machine translation (Ueffing, 2006), and sentiment analysis (He and Zhou, 2011). Sachan and Xing (2018) apply self-training to generate synthetic data for question generation and question answering (QA) in the same domain, and filter data using QA model confidence on answer generated by question. Back-translation’s idea of aligning real outputs with noisy inputs is shared with back-training and has been successful in improving Unsupervised NMT (Artetxe et al., 2018; Edunov et al., 2018). Zhang et al. (2018) use back-translation to generate synthetic data for the task of automatic style trans"
2021.emnlp-main.566,P17-1123,0,0.0216609,"tions from MLQuestions (first row) and PubMedQA (second row). ST and BT refer to Self-training and Back-training models respectively. and corresponding generated questions are sampled from the MLQuestions test set and annotated by a domain expert. We find that the model generates few Explanation questions and even fewer Preference questions while over-generating Description questions. Comparison and Method questions show good F1-score overall, hence these classes benefit the most from domain adaptation. 6 Related Work Question Generation methods have focused on training neural Seq2Seq models (Du et al., 2017; Mishra et al., 2020; Zhao et al., 2018; Chan and Fan, 2019; Klein and Nabi, 2019) on supervised QA datasets such as SQuAD (Rajpurkar et al., 2016). Many recent works such as (Tang et al., 2017; Wang et al., 2017) recognize the duality between QG and QA and propose joint training for the two. Duan et al. (2017) generate QA pairs from YahooAnswers, and improve QA by adding a question-consistency loss in addition to QA loss. Our work instead establishes strong duality between QG and IR task. Ours is also the first work towards unsupervised domain adaptation for QG to the best of our knowledge."
2021.emnlp-main.566,2020.acl-main.703,0,0.0591603,"Missing"
2021.emnlp-main.566,P14-1043,0,0.0222005,"-training to generate synthetic data for question generation and question answering (QA) in the same domain, and filter data using QA model confidence on answer generated by question. Back-translation’s idea of aligning real outputs with noisy inputs is shared with back-training and has been successful in improving Unsupervised NMT (Artetxe et al., 2018; Edunov et al., 2018). Zhang et al. (2018) use back-translation to generate synthetic data for the task of automatic style transfer. Back-training also shares similarities with co-training (Blum and Mitchell, 1998; Wan, 2009) and tri-training (Li et al., 2014; Weiss et al., 2015) where multiple models of same task generate synthetic data for each other. 7 Conclusion and Future Work We introduce back-training as an unsupervised domain adaptation method focusing on Question Generation and Passage Retrieval. Our algorithm generates synthetic data pairing high-quality outputs Passage Retrieval has previously been per- with noisy inputs in contrast to self-training producformed using classical Lucene-BM25 systems ing noisy outputs aligned with quality inputs. We (Robertson and Zaragoza, 2009) based on sparse find that back-training outperforms self-tra"
2021.emnlp-main.566,P02-1040,0,0.113029,"_learning training details, see A.1. 7066 1 Introduction MLQuestions NaturalQuestions Question Generation Passage Retrieval 50 100 40 80 30 60 20 40 10 20 0 B1 B2 B3 B4 M R 0 Table 3: Notations used throughout the paper. 4 Unsupervised Domain Adaptation In this section, we describe self-training and backtraining methods to generate synthetic training data for unsupervised domain adaptation (UDA). We also introduce consistency filters to further improve the quality of the synthetic data. Evaluation Metrics 4.1 We evaluate question generation using standard language generation metrics: BLEU1-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGEL (Lin, 2004). They are abbreviated as B1, B2, B3, B4, M, and R respectively throughout the paper. We also perform human evaluation on the model generated questions. For passage retrieval, we report top-k retrieval accuracy for k = 1, 10, 20, 40, 100 following Karpukhin et al. (2020) by measuring the fraction of cases where the correct passage lies in the top k retrieved passages. We consider 11K passages in all datasets for retrieval during test time. 3 Definition Source, Target Domain Source, Target data distribution Source labeled corpus Target u"
2021.emnlp-main.566,P15-1032,0,0.0279443,"rate synthetic data for question generation and question answering (QA) in the same domain, and filter data using QA model confidence on answer generated by question. Back-translation’s idea of aligning real outputs with noisy inputs is shared with back-training and has been successful in improving Unsupervised NMT (Artetxe et al., 2018; Edunov et al., 2018). Zhang et al. (2018) use back-translation to generate synthetic data for the task of automatic style transfer. Back-training also shares similarities with co-training (Blum and Mitchell, 1998; Wan, 2009) and tri-training (Li et al., 2014; Weiss et al., 2015) where multiple models of same task generate synthetic data for each other. 7 Conclusion and Future Work We introduce back-training as an unsupervised domain adaptation method focusing on Question Generation and Passage Retrieval. Our algorithm generates synthetic data pairing high-quality outputs Passage Retrieval has previously been per- with noisy inputs in contrast to self-training producformed using classical Lucene-BM25 systems ing noisy outputs aligned with quality inputs. We (Robertson and Zaragoza, 2009) based on sparse find that back-training outperforms self-training vector represen"
2021.emnlp-main.566,D16-1264,0,0.0499052,"rresponding generated questions are sampled from the MLQuestions test set and annotated by a domain expert. We find that the model generates few Explanation questions and even fewer Preference questions while over-generating Description questions. Comparison and Method questions show good F1-score overall, hence these classes benefit the most from domain adaptation. 6 Related Work Question Generation methods have focused on training neural Seq2Seq models (Du et al., 2017; Mishra et al., 2020; Zhao et al., 2018; Chan and Fan, 2019; Klein and Nabi, 2019) on supervised QA datasets such as SQuAD (Rajpurkar et al., 2016). Many recent works such as (Tang et al., 2017; Wang et al., 2017) recognize the duality between QG and QA and propose joint training for the two. Duan et al. (2017) generate QA pairs from YahooAnswers, and improve QA by adding a question-consistency loss in addition to QA loss. Our work instead establishes strong duality between QG and IR task. Ours is also the first work towards unsupervised domain adaptation for QG to the best of our knowledge. Data Augmentation methods like self-training have been applied in numerous NLP problems such as question answering (Chung et al., 2018), machine tra"
2021.emnlp-main.566,N18-1101,0,0.0865349,"Missing"
2021.emnlp-main.566,2020.coling-main.603,0,0.0736055,"Missing"
2021.emnlp-main.566,P95-1026,0,0.838098,"et al., 2019). To leads to overfitting (Yu et al., 2020). This means address this issue, these models are further trained that the distributional gap between the target doon cheap synthetically generated labeled data by main’s true output distribution and the learned outexploiting unlabeled data from target domain (Ram- put distribution could grow wider as training proponi and Plank, 2020). One such popular data aug- ceeds. In this paper, we propose a new training mentation method for unsupervised domain adap- protocol called back-training which closes this gap tation (UDA) is self-training (Yarowsky, 1995). (the name is inspired from back-translation for ma7064 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7064–7078 c November 7–11, 2021. 2021 Association for Computational Linguistics chine translation). While self-training generates synthetic data where noisy outputs are aligned with quality inputs, back-training generates quality outputs aligned with noisy inputs. The model finetuned to predict real target domain outputs from noisy inputs reduces overfitting to the source domain (Vincent et al., 2008), and matches the target domain distribution"
2021.emnlp-main.566,D18-1424,0,0.0181025,"PubMedQA (second row). ST and BT refer to Self-training and Back-training models respectively. and corresponding generated questions are sampled from the MLQuestions test set and annotated by a domain expert. We find that the model generates few Explanation questions and even fewer Preference questions while over-generating Description questions. Comparison and Method questions show good F1-score overall, hence these classes benefit the most from domain adaptation. 6 Related Work Question Generation methods have focused on training neural Seq2Seq models (Du et al., 2017; Mishra et al., 2020; Zhao et al., 2018; Chan and Fan, 2019; Klein and Nabi, 2019) on supervised QA datasets such as SQuAD (Rajpurkar et al., 2016). Many recent works such as (Tang et al., 2017; Wang et al., 2017) recognize the duality between QG and QA and propose joint training for the two. Duan et al. (2017) generate QA pairs from YahooAnswers, and improve QA by adding a question-consistency loss in addition to QA loss. Our work instead establishes strong duality between QG and IR task. Ours is also the first work towards unsupervised domain adaptation for QG to the best of our knowledge. Data Augmentation methods like self-trai"
2021.naacl-main.102,P19-1470,0,0.0183169,"ency parse of the sentences, parts of speech Pre-trained language models have shown impres- tags, lemma, and morphological features of the sive results across many tasks, such as question an- words. We also filter out sentences with more than 20 words. swering (Alberti et al., 2019) and natural language inference (Liu et al., 2019). These models are also To test the coverage of our Semgrex patterns, we known to encode factual and common-sense knowl- randomly sampled 930 sentences from Wikipedia. edge (Radford et al., 2019; Petroni et al., 2019; Only 31 of them did not match any of our Semgrex Bosselut et al., 2019). Despite these abilities, Kass- patterns (See table 8 in Appendix B for the number 1302 Model BERT BERT + KL BERTNOT SQuAD 13.53 13.64 13.97 ConceptNet 15.65 15.64 15.49 T-REx 29.10 29.28 29.25 Google-RE 10.24 10.27 10.31 Table 1: Mean precision at k = 1 (p @ 1) for original LAMA queries (higher is better) of pre-trained BERT, BERT trained with distillation objective, and BERT with unlikelihood and distillation objectives (BERTNOT, sec 4.2). The scores are averaged across 3 runs. Model BERT BERT + KL BERTNOT SQuAD 8.61 4.97 2.10 ConceptNet 2.24 1.19 0.73 T-REx 21.42 21.77 11.86 Google-RE 3.76"
2021.naacl-main.102,D15-1075,0,0.10172,"Missing"
2021.naacl-main.102,W09-1105,0,0.121321,"Missing"
2021.naacl-main.102,C18-1198,0,0.0657979,"Missing"
2021.naacl-main.102,2020.acl-main.309,0,0.14547,"018; Tenney et al., 2019; Warstadt and Bowman, 2019; Talmor et al., 2019). Recent work has also studied the shortcomings in negation scope detection (Jumelet and Hupkes, 2018; Fancellu et al., 2016, 2017; Morante and Daelemans, 2009; Li and Lu, 2018; Zhao and Bethard, 2020; Chen, 2019) and focus detection (Shen et al., 2019; Zou et al., 2014, 2015; Hossain et al., 2020a). Naik et al. (2018) and McCoy et al. (2019) systematically study the linguistic abilities of these models using NLI, and show that these models rely on erroneous syntactic heuristics. Our work is in this spirit for negations. Noji and Takamura (2020) propose taking advantage of negative examples and unlikelihood in the training of language models to increase their syntactic abilities. Similarly, Min et al. (2020) show the effectiveness of syntactic data augmentation in the case of robustness in NLI. Neither of these works focus on negations. 3 Syntactic Negation Augmentation We generate the negated versions of sentences using a syntactic augmentation method. The method gets as input the dependency parse of the sentence, POS tags and morphological information of each word and negates the sentence using a set of rules. Each rule has a depen"
2021.naacl-main.102,D19-1250,0,0.374171,"ation method and an unlikelihood token is chosen and replaced with [MASK]. This new sentence is concatenated with the original sentence and fed into the model. The unlikelihood loss is computed using p(improvements) from the language modeling head of BERT. Negation is an important property in many language understanding tasks, such as sentiment analysis, question answering, knowledge base completion and natural language inference (Kassner and Schütze, 2019; Naik et al., 2018). While Pretrained Language Models (PLMs) such as BERT pushed the state-of-the-art on these tasks (Devlin et al., 2019; Petroni et al., 2019), they fail dra- contain not or no as contradiction when the true label is neutral or entailment (Naik et al., 2018). Rematically on instances that require understanding cently, Hossain et al. (2020b) proposed new natural negation. language inference test sets to specifically target Kassner and Schütze (2019) show that current the model’s understanding of negation and show PLMs cannot correctly distinguish between the that current state-of-the-art models perform poorly negated and non-negated forms of fill-in-the-blank on these test sets. tests. For instance, when asked to predict the [MASK] t"
ambati-etal-2012-word,I11-1079,1,\N,Missing
ambati-etal-2012-word,N06-1042,0,\N,Missing
ambati-etal-2012-word,P07-2011,1,\N,Missing
ambati-etal-2012-word,J08-4010,0,\N,Missing
ambati-etal-2012-word,J08-3003,0,\N,Missing
ambati-etal-2012-word,N10-1012,0,\N,Missing
ambati-etal-2012-word,P98-2127,0,\N,Missing
ambati-etal-2012-word,C98-2122,0,\N,Missing
ambati-etal-2012-word,kilgarriff-etal-2010-corpus,1,\N,Missing
D16-1214,D13-1160,0,0.126124,"realization is potentially a correct derivation. For example, the Freebase logical form for the example sentence in Figure 1 is shown below, and none of its derivations are penalized if they could result in this logical form. λe. business.acquisition(e) ∧ acquiring company(e, G OOGLE) ∧ company acquired(e, N EST) (1) ∧ date(e, 2014) Since grammar induction systems are traditionally trained on declarative sentences, we would ideally require declarative sentences paired with Freebase logical forms. But such datasets do not exist in the Freebase semantic parsing literature (Cai and Yates, 2013; Berant et al., 2013). To alleviate this prob1 Please see Bisk and Hockenmaier (2013) for more details. 2023 lem, and yet perform Freebase semantic parsing, we propose an entity slot-filling task. Entity Slot-Filling Task. Given a declarative sentence containing mentions of Freebase entities, we randomly remove one of the mentions to create a blank slot. The task is to fill this slot by translating the declarative sentence into a Freebase query. Consider the following sentence where the entity Nest has been removed: Google acquired which was founded in Palo Alto To correctly fill in the blank, one has to query Fre"
D16-1214,P15-1135,1,0.941398,"nois.edu, steedman@inf.ed.ac.uk, Abstract We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 1 Introduction The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations – attachment scores that depend strongly on the particular annotation styles of the gold treebank – we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that will shed light on the extent to which these systems recover semantic informat"
D16-1214,C04-1180,1,0.682913,"gold answer is the same as the predicted answer. 4 Sentences to Freebase Logical Forms CCG provides a clean interface between syntax and semantics, i.e. each argument of a words syntactic category corresponds to an argument of the lambda expression that defines its semantic interpretation (e.g., the lambda expression corresponding to the category (SNP)/NP of the verb acquired is λf.λg.λe.∃x.∃y.acquired(e) ∧ f (x) ∧ g(y) ∧ arg1 (e, y) ∧ arg2 (e, x)), and the logical form for the complete sentence can be constructed by composing word level lambda expressions following the syntactic derivation (Bos et al., 2004). In Figure 2 we show two syntactic derivations for the same sentence, and the corresponding logical forms and equivalent graph representations derived by G RAPH PARSER (Reddy et al., 2014). The graph representations are possible because G RAPH PARSER assumes access to coindexations of input CCG categories. We provide acquired NP (SNP)/NP hblanki NP which was founded in PA (NPNP)/(SNP) SNP founded. in.arg2 > NPNP < NP > SNP x target < S e2 fo u in n d .a e d rg . 1 Google acquired. arg2 e1 Palo Alto acquired. arg1 Google λe1 .∃xe2 . TARGET(x) ∧ acquired(e1 ) ∧ arg1 (e1 , Google) ∧ arg2 ("
D16-1214,S13-1045,0,0.0184229,"at could lead to this realization is potentially a correct derivation. For example, the Freebase logical form for the example sentence in Figure 1 is shown below, and none of its derivations are penalized if they could result in this logical form. λe. business.acquisition(e) ∧ acquiring company(e, G OOGLE) ∧ company acquired(e, N EST) (1) ∧ date(e, 2014) Since grammar induction systems are traditionally trained on declarative sentences, we would ideally require declarative sentences paired with Freebase logical forms. But such datasets do not exist in the Freebase semantic parsing literature (Cai and Yates, 2013; Berant et al., 2013). To alleviate this prob1 Please see Bisk and Hockenmaier (2013) for more details. 2023 lem, and yet perform Freebase semantic parsing, we propose an entity slot-filling task. Entity Slot-Filling Task. Given a declarative sentence containing mentions of Freebase entities, we randomly remove one of the mentions to create a blank slot. The task is to fill this slot by translating the declarative sentence into a Freebase query. Consider the following sentence where the entity Nest has been removed: Google acquired which was founded in Palo Alto To correctly fill in the blank"
D16-1214,P04-1061,0,0.063815,"ign ybisk@isi.edu, siva.reddy@ed.ac.uk, blitzer@google.com, juliahmr@illinois.edu, steedman@inf.ed.ac.uk, Abstract We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 1 Introduction The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations – attachment scores that depend strongly on the particular annotation styles of the gold treebank – we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that w"
D16-1214,D14-1107,1,0.890059,"Missing"
D16-1214,N10-1116,0,0.0189303,"eddy@ed.ac.uk, blitzer@google.com, juliahmr@illinois.edu, steedman@inf.ed.ac.uk, Abstract We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. 1 Introduction The past several years have seen significant progress in unsupervised grammar induction (Carroll and Charniak, 1992; Yuret, 1998; Klein and Manning, 2004; Spitkovsky et al., 2010; Garrette et al., 2015; Bisk and Hockenmaier, 2015). But how useful are unsupervised syntactic parsers for downstream NLP tasks? What phenomena are they able to capture, and where would additional annotation be required? Instead of standard intrinsic evaluations – attachment scores that depend strongly on the particular annotation styles of the gold treebank – we examine the utility of unsupervised and weakly supervised parsers for semantics. We perform an extrinsic evaluation of unsupervised and weakly supervised CCG parsers on a grounded semantic parsing task that will shed light on the ext"
D16-1214,P12-1063,0,0.0558732,"Missing"
D17-1009,E17-2039,0,0.0131179,"Missing"
D17-1009,P15-1039,0,0.0304249,"ttempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded repres"
D17-1009,P14-1133,0,0.21499,"Missing"
D17-1009,Q15-1039,0,0.154417,"Missing"
D17-1009,W13-3520,0,0.0208057,"ty linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1 -scores for each language and dataset. Features We use features similar to Reddy et al. (2016): basic features of words and Freebase relations, and graph features crossing ungrounded events wi"
D17-1009,W09-1206,0,0.0126084,"e English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UD EP L AMBDA consistently outperforms the S INGLE E VENT and D EP T REE representations in all languages. For English, performance is on par with CCGG RAPH, which suggests that UD EP L AMBDA does not sacrifice too much specificity for universality. With both datasets, results"
D17-1009,P02-1041,0,0.341625,"Missing"
D17-1009,C04-1180,1,0.821524,"Missing"
D17-1009,W13-2322,0,0.0191232,", some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD EP L AMBDA as a first step towards learning rules for converting UD to richer semantic representations such as PropBank, AMR, or the Parallel Meaning Bank (Palmer et al., 2005; Banarescu et al., 2013; Abzianidze et al., 2017).. 4 Semantic Parsing as Graph Matching Cross-lingual Semantic Parsing To study the multilingual nature of UD EP L AMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed. 4.2 Datasets We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English questions and their ans"
D17-1009,W02-1001,0,0.0458946,"problem with the goal of finding the Freebase graphs that are structurally isomorphic to an ungrounded graph and rank them according to a model. To account for structural mismatches, G RAPH PARSER uses two graph transformations: CONTRACT and EXPAND . In Figure 3(a) there are two edges between x and Ghana. CONTRACT collapses one of these edges to create a graph isomorphic to Freebase. EXPAND, in contrast, adds edges to connect the graph in the case of disconnected components. The search space is explored by beam search and model parameters are estimated with the averaged structured perceptron (Collins, 2002) from training data consisting of question-answer pairs, using answer F1 -score as the objective. Other constructions that require lexical information are quantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD"
D17-1009,W15-0128,0,0.0171085,"2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in"
D17-1009,D15-1127,0,0.0138165,"Missing"
D17-1009,C16-1056,0,0.096081,"ave mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurth"
D17-1009,jakob-etal-2010-mapping,0,0.0578418,"Missing"
D17-1009,D16-1086,0,0.0201594,"emantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Instead, UD EP L AMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) l"
D17-1009,C14-1122,0,0.0632483,"Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit ap"
D17-1009,P14-1134,0,0.024364,"for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to li"
D17-1009,P12-1051,0,0.0794212,"and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this frame"
D17-1009,E03-1030,0,0.105881,"Missing"
D17-1009,P15-1143,0,0.0127531,"tly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common"
D17-1009,N16-1088,0,0.0302269,"motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched"
D17-1009,Q16-1023,0,0.0120801,"stions de es 82.8 91.2 86.7 94.0 GraphQuestions en de es 47.2 56.9 39.9 48.4 39.5 51.6 Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing prob"
D17-1009,Q15-1019,0,0.0329059,"d Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012"
D17-1009,D10-1119,1,0.865489,"al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to ex"
D17-1009,D13-1161,0,0.0259018,"nde et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less)"
D17-1009,levy-andrew-2006-tregex,0,0.0175027,"the in English have different semantics, despite being both determiners) and are not encoded in the UD schema. Furthermore, some cross-linguistic phenomena, such as long-distance dependencies, are not part of the core UD representation. To circumvent this limitation, a simple enhancement step enriches the original UD representation before binarization takes place (Section 3.1). This step adds to the dependency tree missing syntactic information and long-distance dependencies, thereby creating a graph. Whereas D EP L AMBDA is not able to handle graph-structured input, UD EP - 2 We use Tregex (Levy and Andrew, 2006) for substitution mappings and Cornell SPF (Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx. ∃y. f (x) ∧ g(y) ∧ amodi (ye , xa ). 3 In what follows, all references to UD are to UD v1.3. 91 L AMBDA is designed to work with dependency graphs as well (Section 3.2). Finally, several constructions differ in structure between UD and SD, which requires different handling in the semantic interface (Section 3.3). 3.1 xcomp nsubj Anna wants to dobj marry Kristoff nsubj (a) With l"
D17-1009,N15-1114,0,0.00825081,"P L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit applications such as summarization (Liu et al., 2015) and machine reading (Sachan and Xing, 2016). Acknowledgements This work greatly benefited from discussions with Michael Collins, Dipanjan Das, Federico Fancellu, Julia Hockenmaier, Tom Kwiatkowski, Adam Lopez, Valeria de Paiva, Martha Palmer, Fernando Pereira, Emily Pitler, Vijay Saraswat, Nathan Schneider, Bonnie Webber, Luke Zettlemoyer, and the members of ILCC Edinburgh University, the Microsoft Research Redmond NLP group, the Stanford NLP group, and the UW NLP and Linguistics group. We thank Reviewer 2 for useful feedback. The authors would also like to thank the Universal Dependencies co"
D17-1009,Q14-1030,1,0.830946,"ikipedia, or SimpleQuestions (Bordes et al., 2015) are shown in parentheses. On GraphQuestions, we achieve a new state-of-the-art result with a gain of 4.8 F1 points over the previously reported best result. On WebQuestions we are 2.1 points below the best model using comparable resources, and 3.8 points below the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score o"
D17-1009,J93-2004,0,0.0647121,"ng-distance dependencies in relative clauses and control constructions. We follow Schuster and Manning (2016) and find these using the labels acl (relative) and xcomp (control). Figure 2(a) shows the long-distance dependency in the sentence Anna wants to marry Kristoff. Here, marry is provided with its missing nsubj (dashed arc). Second, UD conflates all coordinating constructions to a single dependency label, conj. To obtain the correct coordination scope, we refine conj to conj:verb, conj:vp, conj:sentence, conj:np, and conj:adj, similar to Reddy et al. (2016). Finally, unlike the PTB tags (Marcus et al., 1993) used by SD, the UD part-of-speech tags do not distinguish question words. Since these are crucial to question-answering, we use a small lexicon to refine the tags for determiners (DET), adverbs (ADV) and pronouns (PRON) to DET: WH, ADV: WH and PRON : WH, respectively. Specifically, we use a list of 12 (English), 14 (Spanish) and 35 (German) words, respectively. This is the only part of UD EP L AMBDA that relies on language-specific information. We hope that, as the coverage of morphological features in UD improves, this refinement can be replaced by relying on morphological features, such as"
D17-1009,D14-1045,0,0.0131176,"the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UD EP L AMBDA consistently outperforms the S INGLE E VENT and D EP T REE representations in all languages. For English, performance is on par with CCGG RAPH, which suggests that UD EP L AMBDA does not sacrifice too much specifi"
D17-1009,P16-2079,0,0.0223299,"versal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit applications such as summarization (Liu et al., 2015) and machine reading (Sachan and Xing, 2016). Acknowledgements This work greatly benefited from discussions with Michael Collins, Dipanjan Das, Federico Fancellu, Julia Hockenmaier, Tom Kwiatkowski, Adam Lopez, Valeria de Paiva, Martha Palmer, Fernando Pereira, Emily Pitler, Vijay Saraswat, Nathan Schneider, Bonnie Webber, Luke Zettlemoyer, and the members of ILCC Edinburgh University, the Microsoft Research Redmond NLP group, the Stanford NLP group, and the UW NLP and Linguistics group. We thank Reviewer 2 for useful feedback. The authors would also like to thank the Universal Dependencies community for the treebanks and documentation."
D17-1009,L16-1376,0,0.089339,"(Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx. ∃y. f (x) ∧ g(y) ∧ amodi (ye , xa ). 3 In what follows, all references to UD are to UD v1.3. 91 L AMBDA is designed to work with dependency graphs as well (Section 3.2). Finally, several constructions differ in structure between UD and SD, which requires different handling in the semantic interface (Section 3.3). 3.1 xcomp nsubj Anna wants to dobj marry Kristoff nsubj (a) With long-distance dependency. Enhancement Both Schuster and Manning (2016) and Nivre et al. (2016) note the necessity of an enhanced UD representation to enable semantic applications. However, such enhancements are currently only available for a subset of languages in UD. Instead, we rely on a small number of enhancements for our main application—semantic parsing for questionanswering—with the hope that this step can be replaced by an enhanced UD representation in the future. Specifically, we define three kinds of enhancements: (1) long-distance dependencies; (2) types of coordination; and (3) refined question word tags. These correspond to line 2 in Algorithm 1. Fi"
D17-1009,J05-1004,0,0.0829545,"uantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD EP L AMBDA as a first step towards learning rules for converting UD to richer semantic representations such as PropBank, AMR, or the Parallel Meaning Bank (Palmer et al., 2005; Banarescu et al., 2013; Abzianidze et al., 2017).. 4 Semantic Parsing as Graph Matching Cross-lingual Semantic Parsing To study the multilingual nature of UD EP L AMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed. 4.2 Datasets We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English"
D17-1009,P15-1142,0,0.0101447,"in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multi"
D17-1009,D14-1162,0,0.116047,"tic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1 -scores for each language and dataset. Features We"
D17-1009,P16-2067,0,0.0225596,"ir translations. k en 1 10 Implementation Details 89.6 95.7 WebQuestions de es 82.8 91.2 86.7 94.0 GraphQuestions en de es 47.2 56.9 39.9 48.4 39.5 51.6 Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input t"
D17-1009,R11-1065,0,0.0249782,"ntic interfaces is the reliance on rich typed feature structures or semantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Instead, UD EP L AMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base"
D17-1009,D16-1054,0,0.127651,"uctions such as control. The different treatments of various linguistic constructions in UD compared to SD also require different handling in UD EP L AMBDA (Section 3.3). Our experiments focus on Freebase semantic parsing as a testbed for evaluating the framework’s multilingual appeal. We convert natural language to logical forms which in turn are converted to machine interpretable formal meaning representations for retrieving answers to questions from Freebase. To facilitate multilingual evaluation, we provide translations of the English WebQuestions (Berant et al., 2013) and GraphQuestions (Su et al., 2016) datasets to German and Spanish. We demonstrate that UD EP L AMBDA can be used to derive logical forms for these languages using a minimal amount of language-specific knowledge. Aside from developing the first multilingual semantic parsing tool for Freebase, we also experimentally show that UD EP L AMBDA outperforms strong baselines across Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. Howe"
D17-1009,N15-3006,0,0.0206547,"g semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et a"
D17-1009,D16-1177,0,0.0268016,"Missing"
D17-1009,P16-1220,1,0.172318,"Missing"
D17-1009,N15-3014,0,0.0204028,"Missing"
D17-1009,P14-1090,0,0.131516,"Missing"
D17-1009,D16-1015,0,0.0546486,"Missing"
D17-1009,P15-1128,0,0.0788276,"Missing"
D17-1091,D13-1160,0,0.104457,"ialized from a uniform distribution U (−0.08, 0.08). The learning rate and decay rate of RMSProp were 0.01 and 0.95, respectively. The batch size was set to 150. To alleviate the exploding gradient problem (Pascanu et al., 2013), the gradient norm was clipped to 5. Early stopping was used to determine the number of epochs. 3.2 3.3 Our model was trained on three datasets, representative of different types of QA tasks. The first two datasets focus on question answering over a structured knowledge base, whereas the third one is specific to answer sentence selection. W EB Q UESTIONS This dataset (Berant et al., 2013) contains 3, 778 training instances and 2, 032 test instances. Questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted and the answers were crowd-sourced using Freebase as the backend knowledge base. G RAPH Q UESTIONS The dataset (Su et al., 2016) contains 5, 166 question-answer pairs (evenly split into a training and a test set). It was created by asking crowd workers to paraphrase 500 Freebase graph queries in natural language. Implementation Details Table 3 presents descriptive statistics on the paraphrases generated by the variou"
D17-1091,P14-1133,0,0.107546,"Missing"
D17-1091,D14-1067,0,0.0459875,".edu, mlap@inf.ed.ac.uk Abstract the use of paraphrases in relation to question answering. There have been three main strands of research. The first one applies paraphrasing to match natural language and logical forms in the context of semantic parsing. Berant and Liang (2014) use a template-based method to heuristically generate canonical text descriptions for candidate logical forms, and then compute paraphrase scores between the generated texts and input questions in order to rank the logical forms. Another strand of work uses paraphrases in the context of neural question answering models (Bordes et al., 2014a,b; Dong et al., 2015). These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Da"
D17-1091,D15-1181,0,0.0144941,"tead, we translate from multiple pivot sentences (Mallinson et al., 2016). As shown in Figure 2, question q is translated to K-best German pivots, Gq = {g1 , . . . , gK }. The probability of generating paraphrase q 0 = y1 . . . y|q0 |is decomposed as: 0 0  p q |Gq = |q | Y p (yt |y<t , Gq ) t=1 (2) 0 = |q |K Y X 2.2 p (gk |q) p (yt |y<t , gk ) Recall from Equation (1) that pθ (q 0 |q) scores the generated paraphrases q 0 ∈ Hq ∪ {q}. We estimate pθ (q 0 |q) using neural networks given their successful application to paraphrase identification tasks (Socher et al., 2011; Yin and Sch¨utze, 2015; He et al., 2015). Specifically, the input question and its paraphrases are encoded as vectors. Then, we employ a neural network to obtain the score s (q 0 |q) which after normalization becomes the probability pθ (q 0 |q). t=1 k=1 where y<t = y1 , . . . , yt−1 , and |q 0 |is the length of q 0 . Probabilities p (gk |q) and p (yt |y<t , gk ) are computed by the E N -D E and D E -E N models, respectively. We use beam search to decode tokens by conditioning on multiple pivoting sentences. The results with the best decoding scores are considered candidate paraphrases. Examples of NMT paraphrases are shown in Table"
D17-1091,D08-1021,0,0.0997306,"Missing"
D17-1091,P82-1020,0,0.849456,"Missing"
D17-1091,P16-1073,0,0.0916487,"Missing"
D17-1091,N03-1017,0,0.0842091,"ch as the ability to learn continuous representations and to consider wider context while paraphrasing. In our work, we select German as our pivot following Mallinson et al. (2016) who show that it outperforms other languages in a wide range of paraphrasing experiments, and pretrain two NMT systems, English-to-German (E N -D E) and PPDB-based Generation Bilingual pivoting (Bannard and Callison-Burch, 2005) is one of the most well-known approaches to paraphrasing; it uses bilingual parallel corpora to learn paraphrases based on techniques from phrase-based statistical machine translation (SMT, Koehn et al. 2003). The intuition is that two English strings that translate to the same foreign string can be assumed to have the same meaning. The method first extracts a bilingual phrase table and then obtains English paraphrases by pivoting through foreign language phrases. Drawing inspiration from syntax-based SMT, Callison-Burch (2008) and Ganitkevitch et al. (2011) extended this idea to syntactic paraphrases, 877 Source the average size of be locate on which continent language speak in what be the money in WikiAnswers1 users. This corpus is a large resource (the average cluster size is 25), but is relati"
D17-1091,P15-1026,1,0.292885,"Abstract the use of paraphrases in relation to question answering. There have been three main strands of research. The first one applies paraphrasing to match natural language and logical forms in the context of semantic parsing. Berant and Liang (2014) use a template-based method to heuristically generate canonical text descriptions for candidate logical forms, and then compute paraphrase scores between the generated texts and input questions in order to rank the logical forms. Another strand of work uses paraphrases in the context of neural question answering models (Bordes et al., 2014a,b; Dong et al., 2015). These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al.,"
D17-1091,N06-2009,0,0.0954207,"such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al., 2016), as well as rules mined from Wiktionary (Chen et al., 2016) and large-scale paraphrase corpora (Fader et al., 2013). A common problem with the generated paraphrases is that they often contain inappropriate candidates. Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer (Duboue and Chu-Carroll, 2006; Narayan et al., 2016). Problematically, the separate paraphrase models used in previous work do not fully utilize the supervision signal from the training data, and as such cannot be properly tuned Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-toend using question-answer pairs as a supervision signal. A question and its par"
D17-1091,P13-1158,0,0.123542,"ti-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al., 2016), as well as rules mined from Wiktionary (Chen et al., 2016) and large-scale paraphrase corpora (Fader et al., 2013). A common problem with the generated paraphrases is that they often contain inappropriate candidates. Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer (Duboue and Chu-Carroll, 2006; Narayan et al., 2016). Problematically, the separate paraphrase models used in previous work do not fully utilize the supervision signal from the training data, and as such cannot be properly tuned Question answering (QA) systems are"
D17-1091,D16-1147,0,0.00856936,"Missing"
D17-1091,D11-1108,0,0.0149075,"Missing"
D17-1091,W16-6625,1,0.865193,"g et al., 2015). These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al., 2016), as well as rules mined from Wiktionary (Chen et al., 2016) and large-scale paraphrase corpora (Fader et al., 2013). A common problem with the generated paraphrases is that they often contain inappropriate candidates. Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer (Duboue and Chu-Carroll, 2006; Narayan et al., 2016). Problematically, the separate paraphrase models used in previous work do not fully utilize the"
D17-1091,P02-1040,0,0.106423,"a test set). It was created by asking crowd workers to paraphrase 500 Freebase graph queries in natural language. Implementation Details Table 3 presents descriptive statistics on the paraphrases generated by the various systems across datasets (training set). As can be seen, the average paraphrase length is similar to the average length of the original questions. The NMT method generates more paraphrases and has wider coverage, while the average number and coverage of the other two methods varies per dataset. As a way of quantifying the extent to which rewriting takes place, we report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores between the original questions and their paraphrases. The NMT Paraphrase Generation Candidate paraphrases were stemmed (Minnen et al., 2001) and lowercased. We discarded duplicate or trivial paraphrases which only rewrite stop words or punctuation. For the NMT model, we followed the implementation2 and settings described in Mallinson et al. (2016), and used English↔German as the language pair. The system was trained on data released as part of the WMT15 shared translation task (4.2 million sentence pairs). We also had access to back-translated monolingual"
D17-1091,2006.amta-papers.25,0,0.0466375,"asking crowd workers to paraphrase 500 Freebase graph queries in natural language. Implementation Details Table 3 presents descriptive statistics on the paraphrases generated by the various systems across datasets (training set). As can be seen, the average paraphrase length is similar to the average length of the original questions. The NMT method generates more paraphrases and has wider coverage, while the average number and coverage of the other two methods varies per dataset. As a way of quantifying the extent to which rewriting takes place, we report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores between the original questions and their paraphrases. The NMT Paraphrase Generation Candidate paraphrases were stemmed (Minnen et al., 2001) and lowercased. We discarded duplicate or trivial paraphrases which only rewrite stop words or punctuation. For the NMT model, we followed the implementation2 and settings described in Mallinson et al. (2016), and used English↔German as the language pair. The system was trained on data released as part of the WMT15 shared translation task (4.2 million sentence pairs). We also had access to back-translated monolingual training data (Sennrich et al."
D17-1091,D16-1244,0,0.0259126,"Missing"
D17-1091,D16-1054,0,0.0434992,"Missing"
D17-1091,P15-2070,0,0.0669942,"Missing"
D17-1091,D14-1162,0,0.0941715,"del which we call PARA 4QA (as shorthand for learning to paraphrase for question answering) against multiple previous systems on three datasets. In the following we introduce these datasets, provide implementation details for our model, describe the systems used for comparison, and present our results. 3.1 Datasets W IKI QA This dataset (Yang et al., 2015) has 3, 047 questions sampled from Bing query logs. The questions are associated with 29, 258 candidate answer sentences, 1, 473 of which contain the correct answers to the questions. Training For the paraphrase scoring model, we used GloVe (Pennington et al., 2014) vectors3 pretrained on Wikipedia 2014 and Gigaword 5 to initialize the word embedding matrix. We kept this matrix fixed across datasets. Out-of-vocabulary words were replaced with a special unknown symbol. We also augmented questions with start-ofand end-of-sequence symbols. Word vectors for these special symbols were updated during training. Model hyperparameters were validated on the development set. The dimensions of hidden vectors and word embeddings were selected from {50, 100, 200} and {100, 200}, respectively. The dropout rate was selected from {0.2, 0.3, 0.4}. The B I LSTM for the ans"
D17-1091,N16-1170,0,0.0417696,"Missing"
D17-1091,Q16-1010,1,0.849541,"Missing"
D17-1091,P16-1220,1,0.501367,"Missing"
D17-1091,D17-1009,1,0.05169,"Missing"
D17-1091,D15-1237,0,0.0652633,"Missing"
D17-1091,P16-1009,0,0.0474696,"Query graphs for the questions typically contain more than one predicate. For example, to answer the question “who is the ceo of microsoft in 2008”, we need to use one relation to query “ceo of microsoft” and another relation for the constraint “in 2008”. For this task, we employ the S IMPLE G RAPH model described in Reddy et al. (2016, 2017), and follow their training protocol and feature design. In brief, their method uses rules to a ˆ = arg max p a0 |q a0 ∈Cq 879  (8) where Cq is the set of candidate answers, and p (a0 |q) is computed as shown in Equation (1). 3 split into subword units (Sennrich et al., 2016b) to handle out-of-vocabulary words in questions. We used the top 15 decoding results as candidate paraphrases. We used the S size package of PPDB 2.0 (Pavlick et al., 2015) for high precision. At most 10 candidate paraphrases were considered. We mined paraphrase rules from WikiAnswers (Fader et al., 2014) as described in Section 2.1.3. The extracted rules were ranked using the pointwise mutual information between template pairs in the WikiAnswers corpus. The top 10 candidate paraphrases were used. Experiments We compared our model which we call PARA 4QA (as shorthand for learning to paraphra"
D17-1091,N15-3014,0,0.0292453,"Missing"
D17-1091,P16-1162,0,0.0159355,"Query graphs for the questions typically contain more than one predicate. For example, to answer the question “who is the ceo of microsoft in 2008”, we need to use one relation to query “ceo of microsoft” and another relation for the constraint “in 2008”. For this task, we employ the S IMPLE G RAPH model described in Reddy et al. (2016, 2017), and follow their training protocol and feature design. In brief, their method uses rules to a ˆ = arg max p a0 |q a0 ∈Cq 879  (8) where Cq is the set of candidate answers, and p (a0 |q) is computed as shown in Equation (1). 3 split into subword units (Sennrich et al., 2016b) to handle out-of-vocabulary words in questions. We used the top 15 decoding results as candidate paraphrases. We used the S size package of PPDB 2.0 (Pavlick et al., 2015) for high precision. At most 10 candidate paraphrases were considered. We mined paraphrase rules from WikiAnswers (Fader et al., 2014) as described in Section 2.1.3. The extracted rules were ranked using the pointwise mutual information between template pairs in the WikiAnswers corpus. The top 10 candidate paraphrases were used. Experiments We compared our model which we call PARA 4QA (as shorthand for learning to paraphra"
D17-1091,D16-1015,0,0.0326675,"Missing"
D17-1091,P15-1128,0,0.0349307,"Missing"
D17-1091,N15-1091,0,0.0236111,"Missing"
D17-1091,P14-1090,0,\N,Missing
D17-1091,Q15-1039,0,\N,Missing
D17-1091,E17-1083,1,\N,Missing
D17-1091,J13-3001,0,\N,Missing
I11-1024,W03-1812,0,0.900245,"their multiplicative counterparts. 1 Introduction Compositionality is a language phenomenon where the meaning of an expression can be expressed in terms of the meaning of its constituents. Multiword expressions (Sag et al., 2002, MWEs) are known to display a continuum of compositionality (McCarthy et al., 2003) where some of them are compositional e.g. “swimming pool”, some are non-compositional e.g. “cloud nine”, and some in between e.g. “zebra crossing”. The past decade has seen interest in developing computational methods for compositionality in MWEs (Lin, 1999; Schone and Jurafsky, 2001; Baldwin et al., 2003; Bannard et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Sporleder and Li, 2 Compositionality in Compound Nouns In this section, we describe the experimental setup for the collecting compositionality judgments of English compound nouns. All the existing datasets focused either on verb-particle, verbnoun or adjective-noun phrases. Instead, we focus on compound nouns for which resources are rel210 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 210–218, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 A"
I11-1024,D07-1039,1,0.914925,"Missing"
I11-1024,W03-1809,0,0.0884781,"(Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the contribution of the parts with the compositionality of the whole (section 3). We propose various constituent based models (section 4.3) which are intuitive and related to existing models of compositionality detection (section 4.1) and we evaluate these models in comparison to composition"
I11-1024,P08-1028,0,0.812426,"the second in determining compositionality. Results (both ρ and R2 ) clearly show that a relation exists between the constituent literality scores and the phrase compositionality. Existing compositionality approaches on noun-noun compounds such as (Baldwin et al., 2003; Korkontzelos and Manandhar, 2009) use the semantics of only one of the constituent words (generally the head word) Overall, this study suggests that it is possible to estimate the phrase level compositionality scores given the constituent word level literality scores. This motivates us to present constituent 214 vector models (Mitchell and Lapata, 2008; Widdows, 2008) which make use of the semantics of the constituents in a different manner. These models are described in section 4.4 and are evaluated in comparison with the constituent-based models. The vector space model used in all our experiments is described as follows. based models (section 4.3) for compositionality score estimation of a compound. We begin the next section on computational models with a discussion of related work. 4 4.1 Computational Models Related work 4.2 Most methods in compositionality detection can be classified into two types - those which make use of lexical fixe"
I11-1024,W11-1304,0,0.221301,"thy.co.uk suresh@cs.york.ac.uk Abstract 2009). Recent developments in vector-based semantic composition functions (Mitchell and Lapata, 2008; Widdows, 2008) have also been applied to compositionality detection (Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the contribution of the parts with the compositionality of the whole (section 3). We propos"
I11-1024,W07-1106,0,0.0288709,"e lexical fixedness in which the component words have high statistical association. Some of the methods which exploit this feature are (Lin, 1999; Pedersen, 2011). This property does not hold always because institutionalized MWEs (Sag et al., 2002) are known to have high association even though they are compositional, especially in the case of compound nouns. Another property of non-compositional MWEs is that they show syntactic rigidness which do not allow internal modifiers or morphological variations of the components, or variations that break typical selectional preferences. Methods like (Cook et al., 2007; McCarthy et al., 2007; Fazly et al., 2009) exploit this property. This holds mostly for verbal idioms but not for compound nouns since the variations of any compound noun are highly limited. Other methods like (Baldwin et al., 2003; Sporleder and Li, 2009) are based on semantic similarities between the constituents and the MWE. Baldwin et al. (2003) use only the information of the semantic similarity between one of the constituents and the compound to determine the compositionality. Sporleder and Li (2009) determine the compositionality of verbal phrases in a given context (token-based disam"
I11-1024,W11-1306,0,0.0320891,"ty score estimation of a compound. We begin the next section on computational models with a discussion of related work. 4 4.1 Computational Models Related work 4.2 Most methods in compositionality detection can be classified into two types - those which make use of lexical fixedness and syntactic properties of the MWEs, and those which make use of the semantic similarities between the constituents and the MWE. Non compositional MWEs are known to have lexical fixedness in which the component words have high statistical association. Some of the methods which exploit this feature are (Lin, 1999; Pedersen, 2011). This property does not hold always because institutionalized MWEs (Sag et al., 2002) are known to have high association even though they are compositional, especially in the case of compound nouns. Another property of non-compositional MWEs is that they show syntactic rigidness which do not allow internal modifiers or morphological variations of the components, or variations that break typical selectional preferences. Methods like (Cook et al., 2007; McCarthy et al., 2007; Fazly et al., 2009) exploit this property. This holds mostly for verbal idioms but not for compound nouns since the vari"
I11-1024,W11-1310,1,0.854619,"Missing"
I11-1024,J09-1005,0,0.18527,"and reduce the impact of ambiguity. The second is that distributional models are greatly influenced by frequency and since we aim to work with distributional models for compositionality detection we base our findings on the most frequent sense of the compound noun. In this work we consider the compositionality of the noun-noun compound type without token based disambiguation which we leave for future work. atively scarce. In this paper, we only deal with compound nouns made up of two words separated by space. 2.1 Annotation setup In the literature (Nunberg et al., 1994; Baldwin et al., 2003; Fazly et al., 2009), compositionality is discussed in many terms including simple decomposable, semantically analyzable, idiosyncratically decomposable and non-decomposable. For practical NLP purposes, Bannard et al. (2003) adopt a straightforward definition of a compound being compositional if “the overall semantics of the multi-word expression (here compound) can be composed from the simplex semantics of its parts, as described (explicitly or implicitly) in a finite lexicon”. We adopt this definition and pose compositionality as a literality issue. A compound is compositional if its meaning can be understood f"
I11-1024,W01-0513,0,0.0462878,"Missing"
I11-1024,W11-0115,0,0.0607213,"2005; Biemann and Giesbrecht, 2011): higher correlation scores indicate better compositionality predictions. s3 = f (s1, s2) Composition function based models In these models (Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Giesbrecht, 2009) of compositionality detection, firstly a vector for the compound is composed from its constituents using a compositionality function ⊕. Then the similarity between the composed vector and true cooccurrence vector of the compound is measured to determine the compositionality: the higher the similarity, the higher the compositionality of the compound. Guevara (2011) observed that additive models performed well for building composition vectors of phrases from their parts whereas Mitchell and Lapata (2008) found in favor of multiplicative models. We experiment using both the compositionality functions simple addition5 and simple multiplication, which are the most widely used composition functions, known for their simplicity and good performance. Vector v1 ⊕ v2 for a compound w3 is composed from its constituent word vectors v1 and v2 using the vector addition av1 + bv2 and simple multiplication v1v2 where the ith element of v1 ⊕ v2 is defined as (av1 + bv2)"
I11-1024,D08-1027,0,0.0267317,"Missing"
I11-1024,W06-1203,0,0.263496,"york.ac.uk diana@dianamccarthy.co.uk suresh@cs.york.ac.uk Abstract 2009). Recent developments in vector-based semantic composition functions (Mitchell and Lapata, 2008; Widdows, 2008) have also been applied to compositionality detection (Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the contribution of the parts with the compositionality of t"
I11-1024,E09-1086,0,0.00987205,"known to have high association even though they are compositional, especially in the case of compound nouns. Another property of non-compositional MWEs is that they show syntactic rigidness which do not allow internal modifiers or morphological variations of the components, or variations that break typical selectional preferences. Methods like (Cook et al., 2007; McCarthy et al., 2007; Fazly et al., 2009) exploit this property. This holds mostly for verbal idioms but not for compound nouns since the variations of any compound noun are highly limited. Other methods like (Baldwin et al., 2003; Sporleder and Li, 2009) are based on semantic similarities between the constituents and the MWE. Baldwin et al. (2003) use only the information of the semantic similarity between one of the constituents and the compound to determine the compositionality. Sporleder and Li (2009) determine the compositionality of verbal phrases in a given context (token-based disambiguation) based on the lexical chain similarities of the constituents and the context of the MWE. Bannard et al. (2003) and McCarthy et al. (2003) study the compositionality in verb particles and they found that methods based on the similarity between simpl"
I11-1024,P09-2017,1,0.915764,"Missing"
I11-1024,H05-1113,0,0.257826,"Missing"
I11-1024,P99-1041,0,0.0571438,"s, additive models perform better than their multiplicative counterparts. 1 Introduction Compositionality is a language phenomenon where the meaning of an expression can be expressed in terms of the meaning of its constituents. Multiword expressions (Sag et al., 2002, MWEs) are known to display a continuum of compositionality (McCarthy et al., 2003) where some of them are compositional e.g. “swimming pool”, some are non-compositional e.g. “cloud nine”, and some in between e.g. “zebra crossing”. The past decade has seen interest in developing computational methods for compositionality in MWEs (Lin, 1999; Schone and Jurafsky, 2001; Baldwin et al., 2003; Bannard et al., 2003; McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Sporleder and Li, 2 Compositionality in Compound Nouns In this section, we describe the experimental setup for the collecting compositionality judgments of English compound nouns. All the existing datasets focused either on verb-particle, verbnoun or adjective-noun phrases. Instead, we focus on compound nouns for which resources are rel210 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 210–218, c Ch"
I11-1024,W03-1810,1,0.568929,", UK Suresh Manandhar University of York, UK siva@cs.york.ac.uk diana@dianamccarthy.co.uk suresh@cs.york.ac.uk Abstract 2009). Recent developments in vector-based semantic composition functions (Mitchell and Lapata, 2008; Widdows, 2008) have also been applied to compositionality detection (Giesbrecht, 2009). While the existing methods of compositionality detection use constituent word level semantics to compose the semantics of the phrase, the evaluation datasets are not particularly suitable to study the contribution of each constituent word to the semantics of the phrase. Existing datasets (McCarthy et al., 2003; Venkatapathy and Joshi, 2005; Katz and Giesbrecht, 2006; Biemann and Giesbrecht, 2011) only have the compositionality judgment of the whole expression without constituent word level judgment, or they have judgments on the constituents without judgments on the whole (Bannard et al., 2003). Our dataset allows us to examine the relationship between the two rather than assume the nature of it. In this paper we collect judgments of the contribution of constituent nouns within noun-noun compounds (section 2) alongside judgments of compositionality of the compound. We study the relation between the"
I11-1079,J93-1003,0,0.133967,"ty. The selected clusters are then combined using a composition function, to produce a single vector representing the semantics of the target compound noun N . 3.1.1 Figure 2: Running example of WSI The aim of this stage is to capture words contextually related to tw. In the first step, the target word is removed from bc and part-of-speech tagging is applied to each context. Only nouns and verbs are kept and lemmatised. In the next step, the distribution of each word in the base corpus is compared to the distribution of the same noun in a reference corpus using the log-likelihood ratio (G2 ) (Dunning, 1993). Words that have a G2 below a pre-specified threshold (parameter p1 ) are removed from each context of the base corpus. The result of this stage is shown in the upper left part of Figure 2. Graph creation & clustering: Each context ci ∈ bc is represented as a vertex in a graph G. Edges between the vertices of the graph are drawn based on their similarity, defined in Equation 2, where smcl (ci , cj ) is the collocational weight of contexts ci , cj and smwd (ci , cj ) is their bag-of-words weight. If the edge weight W (ci , cj ) is above a prespecified threshold (parameter p3 ), then an edge is"
I11-1079,D08-1094,0,0.15084,"Missing"
I11-1079,P10-2017,0,0.0445281,"Missing"
I11-1079,D10-1073,1,0.813692,"Missing"
I11-1079,N10-1010,1,0.912273,"y is a problem in vector space models. Our approach differs to theirs in the way we represent meaning - we experiment with static multi prototypes and dynamic prototypes. Our vector space model is based on simple bag-of-words which does not require selectional preferences for sense disambiguation and can be applied to resource-poor languages. There are several other researchers who tried to address polysemy for improving the performance of different tasks but not particularly to the task of semantic composition. Some of them are Navigli and Crisafulli (2010) for web search results clustering, Klapaftis and Manandhar (2010b) for taxonomy learning, Reisinger and Mooney (2010) for word similarity and Korkontzelos and Manandhar (2009) for compositionality detection. In all cases, the reported results demonstrate that handling polysemy lead to improved performance of the corresponding tasks. This motivates our research for handling polysemy for the task of semantic composition using two different methods described in the next section. Related work Any distributional model that aims to describe language adequately needs to address the issue of compositionality. Many distributional composition functions have been pro"
I11-1079,P09-2017,1,0.880724,"Missing"
I11-1079,S07-1002,0,0.00838561,"the set of vertices which share a direct connection with vertex i. During the update step for a vertex i: each class Ck receives a score equal to the sum of the weights of edges (i, j), where j has been assigned class Ck . The maximum score determines the strongest class. In case of multiple strongest classes, one is chosen randomly. Classes are updated immediately, which means that a node can inherit classes from its LN that were introduced in the same iteration. Experimental setting The parameters of the WSI method were fine-tuned on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task, i.e. supervised (WSD) evaluation. We tried various parameter combinations shown in Table 1. Specifically, we selected the parameter combination p1 =15, p2 =10, p3 = 0.05 that maximized the performance in this evaluation. We use ukWaC (Ferraresi et al., 2008) corpus to retrieve all the instances of the target words. 3.2 Dynamic Prototype Based Sense Selection Kilgarriff (1997) argues that representing a word with a fixed set of senses is not a good way of modelling word senses. Instead word senses should be defined according to a given context."
I11-1079,P08-1028,0,0.849714,"ounds and evaluate on a compositionality-based similarity task. Our results show that: (1) selecting relevant senses of the constituent words leads to a better semantic composition of the compound, and (2) dynamic prototypes perform better than static prototypes. 1 animal house h 30 hunting h 90 buy 60 15 vector dimensions apartment price 90 55 12 20 rent 45 33 kill 10 i 90 i Figure 1: A hypothetical vector space model. Compositional Distributional Semantic methods formalise the meaning of a phrase by applying a vector composition function on the vectors associated with its constituent words (Mitchell and Lapata, 2008; Widdows, 2008). For example, the result of vector addition to compose the semantics of house hunting from the vectors house and hunting is the vector h120, 75, 102, 75, 78, 100i. As can be observed the resulting vector does not reflect the correct meaning of the compound house hunting due to the presence of irrelevant co-occurrences such as animal or kill. These cooccurrences are relevant to one sense of hunting, i.e. (the activity of hunting animals), but not to the sense of hunting meant in house hunting, i.e. the activity of looking thoroughly. Given that hunting has been associated with"
I11-1079,D10-1012,0,0.0071372,"e compound noun. The results are encouraging showing that polysemy is a problem in vector space models. Our approach differs to theirs in the way we represent meaning - we experiment with static multi prototypes and dynamic prototypes. Our vector space model is based on simple bag-of-words which does not require selectional preferences for sense disambiguation and can be applied to resource-poor languages. There are several other researchers who tried to address polysemy for improving the performance of different tasks but not particularly to the task of semantic composition. Some of them are Navigli and Crisafulli (2010) for web search results clustering, Klapaftis and Manandhar (2010b) for taxonomy learning, Reisinger and Mooney (2010) for word similarity and Korkontzelos and Manandhar (2009) for compositionality detection. In all cases, the reported results demonstrate that handling polysemy lead to improved performance of the corresponding tasks. This motivates our research for handling polysemy for the task of semantic composition using two different methods described in the next section. Related work Any distributional model that aims to describe language adequately needs to address the issue of composit"
I11-1079,J07-2002,0,0.0285319,"of hunting, i.e. (the activity of hunting animals), but not to the sense of hunting meant in house hunting, i.e. the activity of looking thoroughly. Given that hunting has been associated with a single prototype (vector) by conflating all of its senses, the application of a composition function ⊕ is likely to include irrelevant co-occurrences in house ⊕ hunting. A potential solution to this problem would involve the following steps: Introduction Vector Space Models of lexical semantics have become a standard framework for representing a word’s meaning. Typically these methods (Sch¨utze, 1998; Pado and Lapata, 2007; Erk and Pad´o, 2008) utilize a bag-of-words model or 705 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 705–713, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP following Mitchell and Lapata (2008) are defined as follows: 1. build separate prototype vectors for each of the senses of house and hunting 2. select the relevant prototype vectors of house and hunting and then perform the semantic composition. In this paper we present two methods (section 3) for carrying out the above steps on noun-noun compounds. The first one (section 3.1) ap"
I11-1079,N10-1013,0,0.284791,"fers to theirs in the way we represent meaning - we experiment with static multi prototypes and dynamic prototypes. Our vector space model is based on simple bag-of-words which does not require selectional preferences for sense disambiguation and can be applied to resource-poor languages. There are several other researchers who tried to address polysemy for improving the performance of different tasks but not particularly to the task of semantic composition. Some of them are Navigli and Crisafulli (2010) for web search results clustering, Klapaftis and Manandhar (2010b) for taxonomy learning, Reisinger and Mooney (2010) for word similarity and Korkontzelos and Manandhar (2009) for compositionality detection. In all cases, the reported results demonstrate that handling polysemy lead to improved performance of the corresponding tasks. This motivates our research for handling polysemy for the task of semantic composition using two different methods described in the next section. Related work Any distributional model that aims to describe language adequately needs to address the issue of compositionality. Many distributional composition functions have been proposed in order to estimate the semantics of compound"
I11-1079,P07-2011,0,0.0890041,"Missing"
I11-1079,J98-1004,0,0.706399,"Missing"
I11-1079,W11-1301,0,0.0217646,"xt section. Related work Any distributional model that aims to describe language adequately needs to address the issue of compositionality. Many distributional composition functions have been proposed in order to estimate the semantics of compound words from the semantics of the constituent words. Mitchell and Lapata (2008) discussed and evaluated various composition functions for phrases consisting of two words. Among these, the simple additive (ADD) and simple multiplicative (MULT) functions are easy to implement and competitive with respect to existing sophisticated methods (Widdows, 2008; Vecchi et al., 2011). Let us assume a target compound noun N that consists of two nouns n and n0 . Bold letters represent their corresponding distributional vectors obtained from corpora. ⊕(N) denotes the vector of N obtained by applying the composition function ⊕ on n and n0 . Real number vi denote the ith cooccurrence in v. The functions ADD and MULT 3 Sense Prototype Vectors for Semantic Composition In this section we describe two approaches for building sense specific prototype vectors of constituent words in a noun-noun compound. The first approach performs WSI to build static multi prototype vectors. The ot"
I11-1079,W06-3812,0,\N,Missing
I11-1079,W11-0115,0,\N,Missing
J19-1002,D11-1039,0,0.0605649,"Missing"
J19-1002,Q13-1005,0,0.152077,"ck Obama)) corresponds to the query How many daughters does Obama have? and is executed against the Freebase knowledge base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures"
J19-1002,P98-1013,0,0.385819,"Missing"
J19-1002,W13-2322,0,0.0304597,"nd NashWebber 1972). The literature is rife with semantic formalisms that can be used to define logical forms. Examples include lambda calculus (Montague 1973), which has been used by many semantic parsers (Zettlemoyer and Collins 2005; Kwiatkowksi et al. 2010; Reddy, Lapata, and Steedman 2014) because of its expressiveness and flexibility to construct logical forms of great complexity; Combinatory Categorial Grammar (Steedman 2000); dependency-based compositional semantics (Liang, Jordan, and Klein 2011); frame semantics (Baker, Fillmore, and Lowe 1998); and abstract meaning representations (Banarescu et al. 2013). In this work, we adopt a database querying language as the semantic formalism, namely, the functional query language (FunQL; Zelle 1995). FunQL maps first-order logical forms into function-argument structures, resulting in recursive, tree-structured, 61 Computational Linguistics Volume 45, Number 1 program representations. Although it lacks expressive power, FunQL has a modeling advantage for downstream tasks, because it is more natural to describe the manipulation of a simple world as procedural programs. This modeling advantage has been revealed in recent advances of neural programmings: R"
J19-1002,D13-1160,0,0.148103,"vel features because the dynamic programming algorithm requires features defined over substructures. In comparison, our linear-time parser allows us to generate parse structures incrementally conditioned on the entire sentence. We perform several experiments in downstream question-answering tasks and demonstrate the effectiveness of our approach across different training scenarios. These include full supervision with questions paired with annotated logical forms using the G EO Q UERY (Zettlemoyer and Collins 2005) data set, weak supervision with questionanswer pairs using the W EB Q UESTIONS (Berant et al. 2013a) and G RAPH Q UESTIONS (Su et al. 2016) data sets, and distant supervision without question-answer pairs, using the SPADES (Bisk et al. 2016) data set. Experimental results show that our neural semantic parser is able to generate high-quality logical forms and answer real-world questions on a wide range of domains. The remainder of this article is structured as follows. Section 2 provides an overview of related work. Section 3 introduces our neural semantic parsing framework and discusses the various training scenarios to which it can be applied. Our experiments are described in Section 4, t"
J19-1002,P14-1133,0,0.0824315,"Missing"
J19-1002,Q15-1039,0,0.0140481,"ed with an attention mechanism (Dong and Lapata 2016). On W EB Q UESTIONS, the best performing TNSP system generates logical forms based on top–down pre-order while using soft attention. The same top–down system with structured attention performs closely. Again we observe that bottom–up preorder lags behind. In general, our semantic parser obtains performance on par with the best symbolic systems (see the first block in Table 7a). It is important to note that Bast and Haussmann (2015) develop a question-answering system, which, contrary to ours, cannot produce meaning representations, whereas Berant and Liang (2015) propose a sophisticated agenda-based parser that is trained borrowing ideas from imitation learning. Reddy et al. (2016) learn a semantic parser via intermediate representations that they generate based on the output of a dependency parser. TNSP performs competitively despite not having access to linguistically informed syntactic structure. Regarding neural systems (see the second block in Table 7a), our model outperforms the sequenceto-sequence baseline and other related neural architectures using similar resources. Xu et al. (2016) represent the state of the art on WEBQUESTIONS. Their syste"
J19-1002,D16-1214,1,0.899725,"Missing"
J19-1002,D14-1067,0,0.0290169,"Missing"
J19-1002,P13-1042,0,0.188191,"t al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata 2016;"
J19-1002,D16-1053,1,0.844363,"Missing"
J19-1002,P17-2019,1,0.92724,"rules in a grammar to obtain well-typed logical forms. Rabinovich, Stern, and Klein (2017) propose abstract syntax networks with a modular decoder, whose multiple submodels (one per grammar construct) are composed to generate abstract syntax trees in a top– down manner. Our work shares similar motivation: We generate tree-structured, syntactically valid logical forms, but following a transition-based generation approach (Dyer et al. 2015, 62 Cheng, Reddy, Saraswat, and Lapata Learning an Executable Neural Semantic Parser 2016). Our semantic parser is a generalization of the model presented in Cheng et al. (2017). Whereas they focus solely on top–down generation using hard attention, the parser presented in this work generates logical forms following either a top–down or bottom–up generation order and introduces additional attention mechanisms (i.e., soft and structured attention) for handling mismatches between natural language and logical form tokens. We empirically compare generation orders and attention variants, elaborate on model details, and formalize how the neural parser can be effectively trained under different types of supervision. Training Regimes. Various types of supervision have been e"
J19-1002,P17-1005,1,0.730352,"rules in a grammar to obtain well-typed logical forms. Rabinovich, Stern, and Klein (2017) propose abstract syntax networks with a modular decoder, whose multiple submodels (one per grammar construct) are composed to generate abstract syntax trees in a top– down manner. Our work shares similar motivation: We generate tree-structured, syntactically valid logical forms, but following a transition-based generation approach (Dyer et al. 2015, 62 Cheng, Reddy, Saraswat, and Lapata Learning an Executable Neural Semantic Parser 2016). Our semantic parser is a generalization of the model presented in Cheng et al. (2017). Whereas they focus solely on top–down generation using hard attention, the parser presented in this work generates logical forms following either a top–down or bottom–up generation order and introduces additional attention mechanisms (i.e., soft and structured attention) for handling mismatches between natural language and logical form tokens. We empirically compare generation orders and attention variants, elaborate on model details, and formalize how the neural parser can be effectively trained under different types of supervision. Training Regimes. Various types of supervision have been e"
J19-1002,W10-2903,0,0.162083,"12), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence tra"
J19-1002,P16-1004,1,0.856912,"2; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata 2016; Jia and Liang 2016; Koˇcisky´ et al. 2016). Neural semantic parsers generate a sentence in linear time, while reducing the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost because it is no longer possible to interpret how meaning composition is performed, given that logical forms are structured objects like trees or graphs. Such knowledge plays a critical role in understanding modeling limitations so as to build better semantic parsers. Moreover, without any task-specific knowledge, the lea"
J19-1002,D17-1091,1,0.851895,"neous candidate answers extracted from Freebase. Our model would also benefit from a similar post-processing. With respect to G RAPH Q UESTIONS, we report F1 for various TNSP models (third block in Table 7), and conventional statistical semantic parsers (first block in Table 7b). The first three systems are presented in Su et al. (2016). Again, we observe that a top– down variant of TNSP with soft attention performs best. It is superior to the sequence-tosequence baseline and obtains performance comparable to Reddy et al. (2017) without making use of an external syntactic parser. The model of Dong et al. (2017) is state of the art on G RAPH Q UESTIONS. Their method is trained end-to-end using question-answer pairs as a supervision signal together with question paraphrases as a means of capturing different ways of expressing the same content. Importantly, their system is optimized with question answering in mind, and does not produce logical forms. When learning from denotations, a challenge concerns the handling of an exponentially large set of logical forms. In our approach, we rely on the neural semantic parser to generate a list of candidate logical forms by beam search. Ideally, we hope the beam"
J19-1002,P15-1026,0,0.0415523,"Missing"
J19-1002,P15-1033,0,0.111119,"bstract syntax trees for source code with a grammar-constrained neural decoder. Krishnamurthy, Dasigi, and Gardner (2017) also introduce a neural semantic parser that decodes rules in a grammar to obtain well-typed logical forms. Rabinovich, Stern, and Klein (2017) propose abstract syntax networks with a modular decoder, whose multiple submodels (one per grammar construct) are composed to generate abstract syntax trees in a top– down manner. Our work shares similar motivation: We generate tree-structured, syntactically valid logical forms, but following a transition-based generation approach (Dyer et al. 2015, 62 Cheng, Reddy, Saraswat, and Lapata Learning an Executable Neural Semantic Parser 2016). Our semantic parser is a generalization of the model presented in Cheng et al. (2017). Whereas they focus solely on top–down generation using hard attention, the parser presented in this work generates logical forms following either a top–down or bottom–up generation order and introduces additional attention mechanisms (i.e., soft and structured attention) for handling mismatches between natural language and logical form tokens. We empirically compare generation orders and attention variants, elaborate"
J19-1002,N16-1024,0,0.0623422,"Missing"
J19-1002,W17-2607,0,0.0164948,"rks have been shown to be extremely useful in context modeling and sequence generation (Bahdanau, Cho, and Bengio 2015). Following this direction, Dong and Lapata (2016) and Jia and Liang (2016) have developed neural semantic parsers that treat semantic parsing as a sequence to a sequence learning problem. Jia and Liang (2016) also introduce a data augmentation approach that bootstraps a synchronous grammar from existing data and generates artificial examples as extra training data. Other related work extends the vanilla sequence-to-sequence model in various ways, such as multi-task learning (Fan et al. 2017), parsing cross-domain queries (Herzig and Berant 2017) and context-dependent queries (Suhr, Iyer, and Artzi 2018), and applying the model to other formalisms such as AMR (Konstas et al. 2017) and SQL (Zhong, Xiong, and Socher 2017). The fact that logical forms have a syntactic structure has motivated some of the recent work on exploring structured neural decoders to generate tree or graph structures and grammar-constrained decoders to ensure the outputs are meaningful and executable. Related work includes Yin and Neubig (2017), who generate abstract syntax trees for source code with a grammar"
J19-1002,P17-2098,0,0.0239303,"ntext modeling and sequence generation (Bahdanau, Cho, and Bengio 2015). Following this direction, Dong and Lapata (2016) and Jia and Liang (2016) have developed neural semantic parsers that treat semantic parsing as a sequence to a sequence learning problem. Jia and Liang (2016) also introduce a data augmentation approach that bootstraps a synchronous grammar from existing data and generates artificial examples as extra training data. Other related work extends the vanilla sequence-to-sequence model in various ways, such as multi-task learning (Fan et al. 2017), parsing cross-domain queries (Herzig and Berant 2017) and context-dependent queries (Suhr, Iyer, and Artzi 2018), and applying the model to other formalisms such as AMR (Konstas et al. 2017) and SQL (Zhong, Xiong, and Socher 2017). The fact that logical forms have a syntactic structure has motivated some of the recent work on exploring structured neural decoders to generate tree or graph structures and grammar-constrained decoders to ensure the outputs are meaningful and executable. Related work includes Yin and Neubig (2017), who generate abstract syntax trees for source code with a grammar-constrained neural decoder. Krishnamurthy, Dasigi, and"
J19-1002,P17-1089,0,0.15664,"s. Although it lacks expressive power, FunQL has a modeling advantage for downstream tasks, because it is more natural to describe the manipulation of a simple world as procedural programs. This modeling advantage has been revealed in recent advances of neural programmings: Recurrent neural networks have demonstrated great capability in inducing compositional programs (Neelakantan, Le, and Sutskever 2016; Reed and De Freitas 2016; Cai, Shin, and Song 2017). For example, they learn to perform grade-school additions, bubble sort, and table comprehension in procedures. Finally, some recent work (Iyer et al. 2017; Yin and Neubig 2017; Zhong, Xiong, and Socher 2017) uses other programming languages, such as the SQL, as the semantic formalism. Semantic Parsing Model. The problem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, th"
J19-1002,P16-1002,0,0.512797,". Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata 2016; Jia and Liang 2016; Koˇcisky´ et al. 2016). Neural semantic parsers generate a sentence in linear time, while reducing the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost because it is no longer possible to interpret how meaning composition is performed, given that logical forms are structured objects like trees or graphs. Such knowledge plays a critical role in understanding modeling limitations so as to build better semantic parsers. Moreover, without any task-specific knowledge, the learning problem is fai"
J19-1002,P06-1115,0,0.609097,"and Socher 2017) uses other programming languages, such as the SQL, as the semantic formalism. Semantic Parsing Model. The problem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, the form of a support vector machine (Kate and Mooney 2006), a structured perceptron (Zettlemoyer and Collins 2007; Lu et al. 2008; Reddy, Lapata, and Steedman 2014; Reddy et al. 2016), or a log-linear model (Zettlemoyer and Collins 2005; Berant et al. 2013a)—scores the set of candidate derivations generated from the grammar. During inference, a chart-based parsing algorithm is commonly used to predict the most likely semantic parse for a sentence. With recent advances in neural networks and deep learning, there is a trend of reformulating semantic parsing as a machine translation problem. The idea is not novel, because semantic parsing has been previ"
J19-1002,P17-1014,0,0.0295639,"(2016) have developed neural semantic parsers that treat semantic parsing as a sequence to a sequence learning problem. Jia and Liang (2016) also introduce a data augmentation approach that bootstraps a synchronous grammar from existing data and generates artificial examples as extra training data. Other related work extends the vanilla sequence-to-sequence model in various ways, such as multi-task learning (Fan et al. 2017), parsing cross-domain queries (Herzig and Berant 2017) and context-dependent queries (Suhr, Iyer, and Artzi 2018), and applying the model to other formalisms such as AMR (Konstas et al. 2017) and SQL (Zhong, Xiong, and Socher 2017). The fact that logical forms have a syntactic structure has motivated some of the recent work on exploring structured neural decoders to generate tree or graph structures and grammar-constrained decoders to ensure the outputs are meaningful and executable. Related work includes Yin and Neubig (2017), who generate abstract syntax trees for source code with a grammar-constrained neural decoder. Krishnamurthy, Dasigi, and Gardner (2017) also introduce a neural semantic parser that decodes rules in a grammar to obtain well-typed logical forms. Rabinovich, S"
J19-1002,D16-1116,0,0.0892223,"Missing"
J19-1002,D17-1160,0,0.181076,"Missing"
J19-1002,D12-1069,0,0.173391,"sented by the logical form longest(and(type.river, location(Ohio))), which when executed against a database of US geography returns the answer Ohio River. In the second example, the logical form count(daughterOf(Barack Obama)) corresponds to the query How many daughters does Obama have? and is executed against the Freebase knowledge base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Cla"
J19-1002,P14-1112,0,0.0333867,"Missing"
J19-1002,E17-1117,0,0.0253808,"ively pops the stack-LSTM states as well as corresponding tree tokens on the output stack. The popping stops when a non-terminal state is reached and popped, after which the stack-LSTM reaches an intermediate state st−1:t .1 The representation of the completed subtree u is then computed as u = Wu · [pu : cu ] (15) where pu denotes the parent (non-terminal) embedding of the subtree, cu denotes the average of the children (terminal or completed subtree) embeddings, and Wu denotes the weight matrix. Note that cu can also be computed with more advanced methods, such as a recurrent neural network (Kuncoro et al. 2017). Finally, the subtree embedding u serves as the input to the LSTM and updates st−1:t to st as st = LSTM(u, st−1:t ) (16) Figure 1 provides a graphical view on how the three operations change the configuration of a stack-LSTM. In comparison, the bottom–up transition system uses the same TER operation to update the stack-LSTM representation st when a terminal yt is newly generated: st = LSTM(yt , st−1 ) (17) Differently, the effects of NT and RED are merged into a NT-RED(X) operation. When NT-RED(X) is invoked, a non-terminal yt is first predicted and then the stack-LSTM starts popping its stat"
J19-1002,D10-1119,0,0.605108,"swer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is common"
J19-1002,D13-1161,0,0.343444,"ialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where a"
J19-1002,D11-1140,0,0.0314212,"forms, and denotations. The query What is the longest river in Ohio? is represented by the logical form longest(and(type.river, location(Ohio))), which when executed against a database of US geography returns the answer Ohio River. In the second example, the logical form count(daughterOf(Barack Obama)) corresponds to the query How many daughters does Obama have? and is executed against the Freebase knowledge base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to al"
J19-1002,P17-1003,0,0.0597375,"eak supervision from utterance-denotation pairs (Clarke et al. 2010; Liang, Jordan, and Klein 2011; Berant et al. 2013a; Kwiatkowski et al. 2013; Pasupat and Liang 2015). The approach enables more efficient data collection, since denotations (such as answers to a question, responses to a system) are much easier to obtain via crowdsourcing. For this reason, semantic parsing can be scaled to handle large, complex, and open domain problems. Examples include learning semantic parsers from question-answer pairs on Freebase (Liang, Jordan, and Klein 2011; Berant et al. 2013a; Berant and Liang 2014; Liang et al. 2017; Cheng et al. 2017), from system feedbacks (Clarke et al. 2010; Chen and Mooney 2011; Artzi and Zettlemoyer 2013), from abstract examples (Goldman et al. 2018), and from human feedbacks (Iyer et al. 2017) or statements (Artzi and Zettlemoyer 2011). Some work seeks more clever ways of gathering data and trains semantic parsers with even weaker supervision. In a class of distant supervision methods, the input is solely a knowledge base and a corpus of unlabeled sentences. Artificial training data are generated from the given resources. For example, Cai and Yates (2013) generate utterances paire"
J19-1002,J13-2005,0,0.018296,"rance is labeled with annotated logical forms, a weakly supervised setting where utterance-denotation pairs are available, and a distant-supervision setting where only a collection of unlabeled sentences and a knowledge base is given. 3.1 FunQL Semantic Representation As mentioned earlier, we adopt FunQL as our semantic formalism. FunQL is a variablefree recursive meaning representation language that maps simple first-order logical forms to function-argument structures that abstract away from variables and quantifiers (Kate and Mooney 2006). The language is also closely related to lambda DCS (Liang 2013), which makes existential quantifiers implicit. Lambda DCS is more compact in the sense that it can use variables in rare cases to handle anaphora and build composite binary predicates. The FunQL logical forms we define contain the following primitive functional operators. They overlap with simple lambda DCS (Berant et al. 2013a) but differ slightly in syntax to ease recursive generation of logical forms. Let l denote a logical form, JlK represent its denotation, and K refer to a knowledge base. • Unary base case: An entity e (e.g., Barack Obama) is a unary logical form whose denotation is a s"
J19-1002,P11-1060,0,0.303228,"Missing"
J19-1002,Q18-1005,1,0.836676,"attention for predicting InfluentialTeensByYear. Darker shading indicates higher values. which outputs the parameters of the multinomial distribution over logical form tokens (either predicates or entities). When dealing with extremely large knowledge bases, the output space can be pruned and restricted with an entity linking procedure. This method requires us to identity potential entity candidates in the sentence, and then generate only entities belonging to this subset and the relations linking them. Structured Soft Attention. We also explored a structured attention layer (Kim et al. 2017; Liu and Lapata 2018) to encourage the model to attend to contiguous natural language phrases when generating a logical token, while still being differentiable. The structured attention layer we adopt is a linear-chain conditional random field (Lafferty, Mccallum, and Pereira 2001). Assume that at time step t each token in the buffer (e.g., the ith token) is assigned an attention label Ait ∈ {0, 1}. The conditional random field defines p(At ), the probability of the sequence of attention labels at time step t as p(At ) = P where P · ψ(Ati−1 , Ait , bi , st ) P i −1 i i Wf · ψ (At , At , bi , st ) A1t ,··· ,Ant exp"
J19-1002,D08-1082,0,0.188897,"mantic formalism. Semantic Parsing Model. The problem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, the form of a support vector machine (Kate and Mooney 2006), a structured perceptron (Zettlemoyer and Collins 2007; Lu et al. 2008; Reddy, Lapata, and Steedman 2014; Reddy et al. 2016), or a log-linear model (Zettlemoyer and Collins 2005; Berant et al. 2013a)—scores the set of candidate derivations generated from the grammar. During inference, a chart-based parsing algorithm is commonly used to predict the most likely semantic parse for a sentence. With recent advances in neural networks and deep learning, there is a trend of reformulating semantic parsing as a machine translation problem. The idea is not novel, because semantic parsing has been previously studied with statistical machine translation approaches in both W"
J19-1002,P15-1142,0,0.0771755,"Missing"
J19-1002,D14-1162,0,0.0823538,"Missing"
J19-1002,C04-1021,0,0.0678008,"base of US geography returns the answer Ohio River. In the second example, the logical form count(daughterOf(Barack Obama)) corresponds to the query How many daughters does Obama have? and is executed against the Freebase knowledge base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishn"
J19-1002,P17-1105,0,0.361335,"Missing"
J19-1002,Q14-1030,1,0.87593,"Missing"
J19-1002,Q16-1010,1,0.595148,"lem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, the form of a support vector machine (Kate and Mooney 2006), a structured perceptron (Zettlemoyer and Collins 2007; Lu et al. 2008; Reddy, Lapata, and Steedman 2014; Reddy et al. 2016), or a log-linear model (Zettlemoyer and Collins 2005; Berant et al. 2013a)—scores the set of candidate derivations generated from the grammar. During inference, a chart-based parsing algorithm is commonly used to predict the most likely semantic parse for a sentence. With recent advances in neural networks and deep learning, there is a trend of reformulating semantic parsing as a machine translation problem. The idea is not novel, because semantic parsing has been previously studied with statistical machine translation approaches in both Wong and Mooney (2006) and Andreas, Vlachos, and Clark"
J19-1002,D17-1009,1,0.889912,"Missing"
J19-1002,D16-1054,0,0.122317,"algorithm requires features defined over substructures. In comparison, our linear-time parser allows us to generate parse structures incrementally conditioned on the entire sentence. We perform several experiments in downstream question-answering tasks and demonstrate the effectiveness of our approach across different training scenarios. These include full supervision with questions paired with annotated logical forms using the G EO Q UERY (Zettlemoyer and Collins 2005) data set, weak supervision with questionanswer pairs using the W EB Q UESTIONS (Berant et al. 2013a) and G RAPH Q UESTIONS (Su et al. 2016) data sets, and distant supervision without question-answer pairs, using the SPADES (Bisk et al. 2016) data set. Experimental results show that our neural semantic parser is able to generate high-quality logical forms and answer real-world questions on a wide range of domains. The remainder of this article is structured as follows. Section 2 provides an overview of related work. Section 3 introduces our neural semantic parsing framework and discusses the various training scenarios to which it can be applied. Our experiments are described in Section 4, together with detailed analysis of system"
J19-1002,D15-1199,0,0.0626104,"Missing"
J19-1002,H89-1033,0,0.522124,"usses the various training scenarios to which it can be applied. Our experiments are described in Section 4, together with detailed analysis of system output. Discussion of future work concludes in Section 5. 2. Related Work The proposed framework has connections to several lines of research, including various formalisms for representing natural language meaning, semantic parsing models, and the training regimes they adopt. We review related work in these areas here. Semantic Formalism. Logical forms have played an important role in semantic parsing systems since their inception in the 1970s (Winograd 1972; Woods, Kaplan, and NashWebber 1972). The literature is rife with semantic formalisms that can be used to define logical forms. Examples include lambda calculus (Montague 1973), which has been used by many semantic parsers (Zettlemoyer and Collins 2005; Kwiatkowksi et al. 2010; Reddy, Lapata, and Steedman 2014) because of its expressiveness and flexibility to construct logical forms of great complexity; Combinatory Categorial Grammar (Steedman 2000); dependency-based compositional semantics (Liang, Jordan, and Klein 2011); frame semantics (Baker, Fillmore, and Lowe 1998); and abstract meaning"
J19-1002,N06-1056,0,0.686446,"base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart p"
J19-1002,P16-1220,1,0.906697,"tterance (excluding stop-words) and the logical form, the token overlap count between the two, and also similar features between the lemmatized utterance and the logical form. In addition, we include as features the embedding cosine similarity 81 Computational Linguistics Volume 45, Number 1 between the question words and the logical form, the similarity between the question words (e.g., what, who, where, whose, date, which, how many, count ) and relations in the logical form, and the similarity between the question words and answer type as indicated by the last word in the Freebase relation (Xu et al. 2016). Finally, we add as a feature the length of the denotation given by the logical form (Berant et al. 2013a). 4.3 Results In this section, we present the experimental results of our Transition-based Neural Semantic Parser (TNSP). We present various instantiations of our own model as well as comparisons against semantic parsers proposed in the literature. Experimental results on G EO Q UERY are shown in Table 6. The first block contains conventional statistical semantic parsers, previously proposed neural models are presented in the second block, and variants of TNSP are shown in the third block"
J19-1002,P14-1090,0,0.038961,"Missing"
J19-1002,P15-1128,0,0.058584,"Missing"
J19-1002,P17-1041,0,0.226018,"ks expressive power, FunQL has a modeling advantage for downstream tasks, because it is more natural to describe the manipulation of a simple world as procedural programs. This modeling advantage has been revealed in recent advances of neural programmings: Recurrent neural networks have demonstrated great capability in inducing compositional programs (Neelakantan, Le, and Sutskever 2016; Reed and De Freitas 2016; Cai, Shin, and Song 2017). For example, they learn to perform grade-school additions, bubble sort, and table comprehension in procedures. Finally, some recent work (Iyer et al. 2017; Yin and Neubig 2017; Zhong, Xiong, and Socher 2017) uses other programming languages, such as the SQL, as the semantic formalism. Semantic Parsing Model. The problem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, the form of a support v"
J19-1002,D07-1071,0,0.283917,"Missing"
J19-1002,N15-1162,0,0.096033,"Missing"
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
kilgarriff-etal-2010-corpus,E06-2001,1,\N,Missing
kilgarriff-etal-2010-corpus,J03-3005,0,\N,Missing
kilgarriff-etal-2010-corpus,P99-1068,0,\N,Missing
kilgarriff-etal-2010-corpus,ide-etal-2002-american,0,\N,Missing
kilgarriff-etal-2010-corpus,baroni-bernardini-2004-bootcat,0,\N,Missing
kilgarriff-etal-2010-corpus,pomikalek-rychly-2008-detecting,1,\N,Missing
N16-1120,N15-1006,1,0.900649,"Missing"
N16-1120,P11-2117,0,0.158616,"Missing"
N16-1120,C12-1065,0,0.0447967,"Missing"
N16-1120,N15-1022,0,0.31539,"Missing"
N16-1120,P03-1054,0,0.0326684,"Missing"
N16-1120,Q14-1026,1,0.87849,"Missing"
N16-1120,J01-2004,0,0.281102,"Missing"
N16-1120,P11-1063,0,0.0692792,"Missing"
N16-1120,E14-1076,0,0.0634645,"Missing"
N16-1120,E14-1031,0,0.267204,"Missing"
N16-1120,P12-1107,0,0.142533,"Missing"
N16-1120,P11-1069,0,0.0594387,"Missing"
N16-1120,N01-1021,0,\N,Missing
N16-1120,W04-0304,0,\N,Missing
N16-1120,C10-1152,0,\N,Missing
P10-3003,J01-3001,0,0.0326529,"Missing"
P10-3003,E09-1005,0,0.520842,"in than to the other senses. 4.4 Sense Relatedness Sense relatedness between senses of two words wi , wj is captured by a function f : Dwi × Dwj → ℜ where f returns sense relatedness (utility) between senses based on sense taxonomy and gloss overlaps. 5 Experiment: DCOP based All Words WSD We carried out a simple experiment to test the effectiveness of DCOP algorithm. We conducted our experiment in an all words setting and used only WordNet (Fellbaum, 1998) based relatedness measures as knowledge source so that results can be compared with earlier state-of-art knowledgebased WSD systems like (Agirre and Soroa, 2009; Sinha and Mihalcea, 2007) which used similar knowledge sources as ours. 4.5 Discourse Discourse constraints can be modelled using a n-ary function. For instance, to the extent one sense per discourse (Gale et al., 1992) holds true, higher utility can be returned to the solutions which favour same sense to all the occurrences of a word in a given discourse. This information can be modeled as follows: If wi , wj , . . . wm are 15 Our method performs disambiguation on sentence by sentence basis. A utility function based on semantic relatedness is defined for every pair of words falling in a par"
P10-3003,H92-1045,0,0.334457,"xonomy and gloss overlaps. 5 Experiment: DCOP based All Words WSD We carried out a simple experiment to test the effectiveness of DCOP algorithm. We conducted our experiment in an all words setting and used only WordNet (Fellbaum, 1998) based relatedness measures as knowledge source so that results can be compared with earlier state-of-art knowledgebased WSD systems like (Agirre and Soroa, 2009; Sinha and Mihalcea, 2007) which used similar knowledge sources as ours. 4.5 Discourse Discourse constraints can be modelled using a n-ary function. For instance, to the extent one sense per discourse (Gale et al., 1992) holds true, higher utility can be returned to the solutions which favour same sense to all the occurrences of a word in a given discourse. This information can be modeled as follows: If wi , wj , . . . wm are 15 Our method performs disambiguation on sentence by sentence basis. A utility function based on semantic relatedness is defined for every pair of words falling in a particular window size. Restricting utility functions to a window size reduces the number of constraints. An objective function is defined as sum of these restricted utility functions over the entire sentence and thus allowi"
P10-3003,W02-1006,0,0.0261673,"knowledgebased WSD, DCOP framework is a potential alternative to graph based models. Table 1 also shows the system (Agirre and Soroa, 2009), which obtained best results for knowledge based WSD. A direct comparison between this and our system is not quantitative since they used additional knowledge such as extended WordNet relations (Mihalcea and 1 6 Related work Earlier approaches to WSD which encoded information from variety of knowledge sources can be classified as follows: • Supervised approaches: Most of the supervised systems (Yarowsky and Florian, 2002; http://liawww.epfl.ch/frodo/ 16 7 Lee and Ng, 2002; Mart´ınez et al., 2002; Stevenson and Wilks, 2001) rely on the sense tagged data. These are mainly discriminative or aggregative models which essentially pose WSD a classification problem. Discriminative models aim to identify the most informative feature and aggregative models make their decisions by combining all features. They disambiguate word by word and do not collectively disambiguate whole context and thereby do not capture all the relationships (e.g sense relatedness) among all the words. Further, they lack the ability to directly represent constraints like one sense per discourse."
P10-3003,C02-1112,0,0.0736435,"Missing"
P10-3003,J92-1001,0,0.218469,"s. In the sentence, He took all his money from the bank, bank refers to a financial institution sense instead of other possibilities like the edge of river sense. Given a word and its possible senses, as defined by a dictionary, the problem of Word Sense Disambiguation (WSD) can be defined as the task of assigning the most appropriate sense to the word within a given context. WSD is one of the oldest problems in computational linguistics which dates back to early 1950’s. A range of knowledge sources have been found to be useful for WSD. (Agirre and Stevenson, 2006; Agirre and Mart´ınez, 2001; McRoy, 1992; Hirst, 1987) highlight the importance of various knowledge sources like part of speech, morphology, collocations, lexical knowledge base (sense taxonomy, gloss), sub-categorization, semantic word associations, selectional preferences, 13 Proceedings of the ACL 2010 Student Research Workshop, pages 13–18, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics 3 WSD as a DCOP DCOP. Section 3 describes modelling WSD as a DCOP. Utility functions for various knowledge sources are described in section 4. In section 5, we conduct a simple experiment by modelling allwords WS"
P16-1220,N07-4013,0,0.14223,"Missing"
P16-1220,P14-1091,0,0.527771,"efine these candidate answers by applying an answer refinement model which takes the Wikipedia page of the topic entity into consideration to filter out the wrong answers and pick the correct ones. While the overview in Figure 1 works for questions containing single Freebase relation, it also works for questions involving multiple Freebase relations. Consider the question who plays anakin skywalker in star wars 1. The actors who are the answers to this question should satisfy the following constraints: (1) the actor played anakin skywalker; and (2) the actor played in star wars 1. Inspired by Bao et al. (2014), we design a dependency treebased method to handle such multi-relational questions. We first decompose the original question into a set of sub-questions using syntactic patterns which are listed in Appendix. The final answer set of the original question is obtained by intersecting the answer sets of all its sub-questions. For the 2327 example question, the sub-questions are who plays anakin skywalker and who plays in star wars 1. These sub-questions are answered separately over Freebase and Wikipedia, and the intersection of their answers to these sub-questions is treated as the final answer."
P16-1220,P14-1133,0,0.149467,"The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compo"
P16-1220,Q15-1039,0,0.599273,"Missing"
P16-1220,P11-1055,0,0.0472067,"Missing"
P16-1220,D13-1160,0,0.510828,"the MCCNN, and the target prediction during testing time is over these relations. 5.2 Experimental Settings We have 6 dependency tree patterns based on Bao et al. (2014) to decompose the question into subquestions (See Appendix). We initialize the word embeddings with Turian et al. (2010)’s word representations with dimensions set to 50. The hyper parameters in our model are tuned using the development set. The window size of MCCNN is set to 3. The sizes of the hidden layer 1 and the hidden layer 2 of the two MCCNN channels are set to 200 and 100, respectively. We use the Freebase version of Berant et al. (2013), containing 4M entities and 5,323 relations. 5.3 Results and Discussion We use the average question-wise F1 as our evaluation metric.4 To give an idea of the impact of different configurations of our method, we compare the following with existing methods. Structured. This method involves inference on Freebase only. First the entity linking (EL) system is run to predict the topic entity. Then we run the relation extraction (RE) system and select the best relation that can occur with the topic entity. We choose this entity-relation pair to predict the answer. 4 We use the evaluation script avai"
P16-1220,D14-1067,0,0.330735,"t of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function"
P16-1220,P13-1042,0,0.372956,"Missing"
P16-1220,D14-1117,0,0.0839941,"Missing"
P16-1220,D12-1069,0,0.143,"Missing"
P16-1220,D13-1161,0,0.250474,"mprovement over the state-of-the-art. 1 Introduction Since the advent of large structured knowledge bases (KBs) like Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and DBpedia (Auer et al., 2007), answering natural language questions using those structured KBs, also known as KBbased question answering (or KB-QA), is attracting increasing research efforts from both natural language processing and information retrieval communities. The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate"
P16-1220,P15-2047,0,0.00839767,"ntually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering (Yih et al., 2015; Dong et al., 2015), and the success of syntactic dependencies for relation extraction (Liu et al., 2015; Xu et al., 2015), we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction. 1 who, when, what, where, how, which, why, whom, whose. S-MART demo can be accessed at http://msre2edemo.azurewebsites.net/ 2 dobj Feature Extraction Word Representation Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation r with an entity e found in the question, and predict a single KB triple (e, r, ?) for each subquestion (here ? stands for the answer entities). The QA pr"
P16-1220,P14-5010,0,0.00510178,"use the shortest path between an entity mention and the question word in the dependency tree3 as input to the first channel. Similar to Xu et al. (2015), we treat the path as a concatenation of vectors of words, dependency edge directions and dependency labels, and feed it to the convolution layer. Note that, the entity mention and the question word are excluded from the dependency path so as to learn a more general relation representation in syntactic level. As shown in Figure 2, the dependency path between who and shaq is ← dobj – play – nsubj →. 3 We use Stanford CoreNLP dependency parser (Manning et al., 2014). 2328 Sentential Features This channel takes the words in the sentence as input excluding the question word and the entity mention. As illustrated in Figure 2, the vectors for did, first, play and for are fed into this channel. 3.2.2 Objective Function and Learning The model is learned using pairs of question and its corresponding gold relation from the training data. Given an input question x with an annotated entity mention, the network outputs a vector o(x), where the entry ok (x) is the probability that there exists the k-th relation between the entity and the expected answer. We denote t"
P16-1220,P09-1113,0,0.125939,"Missing"
P16-1220,D13-1184,0,0.0223252,"s to Wikipedia, that person might first determine that the question is about Shaquille O’Neal, then go to O’Neal ’s Wikipedia page, and search for the sentences that contain the candidate answers as evidence. By analyzing these sentences, one can figure out whether a candidate answer is correct or not. 4.1 Finding Evidence from Wikipedia As mentioned above, we should first find the Wikipedia page corresponding to the topic entity in the given question. We use Freebase API to convert Freebase entity to Wikipedia page. We extract the content from the Wikipedia page and process it with Wikifier (Cheng and Roth, 2013) which recognizes Wikipedia entities, which can further be linked to Freebase entities using Freebase API. Additionally we use Stanford CoreNLP (Manning et al., 2014) for tokenization and entity co-reference resolution. We search for the sentences containing the candidate answer entities retrieved from Freebase. For example, the Wikipedia page of O’Neal contains a sentence “O’Neal was drafted by the Orlando Magic with the first overall pick in the 1992 NBA draft”, which is taken into account by the refinement model (our inference model on Wikipedia) to discriminate whether Orlando Magic is the"
P16-1220,N15-1077,0,0.0182235,"Missing"
P16-1220,P15-1026,0,0.785384,"ng examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select t"
P16-1220,Q14-1030,1,0.879493,"thods for this task can be roughly categorized into two streams. The first is based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013), which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions th"
P16-1220,Q16-1010,1,0.244478,"Missing"
P16-1220,P13-1158,0,0.290087,"Missing"
P16-1220,N10-1145,0,0.0208895,"Missing"
P16-1220,N13-1008,0,0.0574113,"Missing"
P16-1220,P16-1056,0,0.00682235,"Missing"
P16-1220,P10-1040,0,0.0233341,"ct to a mediator node, and the second from the mediator to the object node. For each relation candidate r, we issue the query (e, r, ?) to the KB, and label the relation that produces the answer with minimal F1 -loss against the gold answer, as the surrogate gold relation. From the training set, we collect 461 relations to train the MCCNN, and the target prediction during testing time is over these relations. 5.2 Experimental Settings We have 6 dependency tree patterns based on Bao et al. (2014) to decompose the question into subquestions (See Appendix). We initialize the word embeddings with Turian et al. (2010)’s word representations with dimensions set to 50. The hyper parameters in our model are tuned using the development set. The window size of MCCNN is set to 3. The sizes of the hidden layer 1 and the hidden layer 2 of the two MCCNN channels are set to 200 and 100, respectively. We use the Freebase version of Berant et al. (2013), containing 4M entities and 5,323 relations. 5.3 Results and Discussion We use the average question-wise F1 as our evaluation metric.4 To give an idea of the impact of different configurations of our method, we compare the following with existing methods. Structured. T"
P16-1220,D07-1003,0,0.0587728,"Missing"
P16-1220,P15-1129,0,0.0159012,"gates representational issues in relation extraction, but also alleviates the data scarcity problem to some extent. Consider the question, who was queen isabella’s mother. Answering this question involves predicting two constraints hidden in the word mother. One constraint is that the answer should be the parent of Isabella, and the other is that the answer’s gender is female. Such words with multiple latent constraints have been a pain-in-the-neck for both semantic parsing and relation extraction, and requires larger training data (this phenomenon is coined as sub-lexical compositionality by Wang et al. (2015)). Most systems are good at triggering the parent constraint, but fail on the other, i.e., the answer entity should be female. Whereas the textual evidence from Wikipedia, . . . her mother was Isabella of Barcelos . . . , can act as a further constraint to answer the question correctly. We present a novel method for question answering which infers on both structured and unstructured resources. Our method consists of two main steps as outlined in §2. In the first step we extract answers for a given question using a structured KB (here Freebase) by jointly performing entity linking and relation"
P16-1220,P14-1090,0,0.525003,"Missing"
P16-1220,N13-1106,0,0.0263742,"Missing"
P16-1220,N15-3014,0,0.252342,"eaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America bec"
P16-1220,P13-1171,0,0.0610357,"Missing"
P16-1220,P14-2105,0,0.102811,"a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem (Kwiatkowski et al., 2013; Berant and Liang, 2014; Reddy et al., 2014). On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015). Designing large training datasets for these methods is relatively easy (Yao and Van Durme, 2014; Bordes et al., 2015; Serban et al., 2016). These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North"
P16-1220,P15-1128,0,0.571937,"retrieve the top 5 entities from Freebase. These entities are treated as candidate entities that will eventually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering (Yih et al., 2015; Dong et al., 2015), and the success of syntactic dependencies for relation extraction (Liu et al., 2015; Xu et al., 2015), we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction. 1 who, when, what, where, how, which, why, whom, whose. S-MART demo can be accessed at http://msre2edemo.azurewebsites.net/ 2 dobj Feature Extraction Word Representation Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation r with an entity e found in the question, and pre"
P16-1220,D15-1062,1,0.178255,"guated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering (Yih et al., 2015; Dong et al., 2015), and the success of syntactic dependencies for relation extraction (Liu et al., 2015; Xu et al., 2015), we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction. 1 who, when, what, where, how, which, why, whom, whose. S-MART demo can be accessed at http://msre2edemo.azurewebsites.net/ 2 dobj Feature Extraction Word Representation Given a sub-question, we assume the question word1 that represents the answer has a distinct KB relation r with an entity e found in the question, and predict a single KB triple (e, r, ?) for each subquestion (here ? stands for the answer entities). The QA problem is thus form"
P16-1220,D12-1035,0,0.0586932,"Missing"
P16-1220,P15-1049,0,0.0342182,"the 2327 example question, the sub-questions are who plays anakin skywalker and who plays in star wars 1. These sub-questions are answered separately over Freebase and Wikipedia, and the intersection of their answers to these sub-questions is treated as the final answer. 3 dobj [Who] did [shaq] first Inference on Freebase Convolution 3.1 Entity Linking For each question, we use hand-built sequences of part-of-speech categories to identify all possible named entity mention spans, e.g., the sequence NN (shaq) may indicate an entity. For each mention span, we use the entity linking tool S-MART2 (Yang and Chang, 2015) to retrieve the top 5 entities from Freebase. These entities are treated as candidate entities that will eventually be disambiguated in the joint inference step. For a given mention span, S-MART first retrieves all possible entities of Freebase by surface matching, and then ranks them using a statistical model, which is trained on the frequency counts with which the surface form occurs with the entity. 3.2 Relation Extraction We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answeri"
P16-1220,D15-1237,0,0.0132239,"Missing"
P17-1005,W15-0128,0,0.0277833,"tural Language Representations for Semantic Parsing Jianpeng Cheng† Siva Reddy† Vijay Saraswat‡ and Mirella Lapata† † School of Informatics, University of Edinburgh ‡ IBM T.J. Watson Research {jianpeng.cheng,siva.reddy}@ed.ac.uk, vsaraswa@us.ibm.com, mlap@inf.ed.ac.uk Abstract representation (Kwiatkowski et al., 2013; Reddy et al., 2016, 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially enables the handling of unseen words and knowledge transfer across domains (Bender et al., 2015). The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) they reduce the need for domain-specific assumptions, grammar learni"
P17-1005,P15-1033,0,0.0194296,"bility p(U |x) is factorized over time steps as: p(U |x) = p(a, u|x) = T Y t=1 (1) p(at |a&lt;t , x)p(ut |a&lt;t , x)I(at 6=RED) p(utGENERAL |a&lt;t , x) ∝ exp(Wp · et ) To choose a natural language term, we directly compute a probability distribution of all natural language terms (in the buffer) conditioned on the stack representation st and select the most relevant term (Jia and Liang, 2016): where I is an indicator function. To predict the actions of the transition system, we encode the input buffer with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and the output stack with a stack-LSTM (Dyer et al., 2015). At each time step, the model uses the representation of the transition system et to predict an action: p(at |a&lt;t , x) ∝ exp(Wa · et ) (3) p(utNL |a&lt;t , x) ∝ exp(st ) (4) When the predicted action is RED, the completed subtree is composed into a single representation on the stack. For the choice of composition function, we use a single-layer neural network as in Dyer et al. (2015), which takes as input the concatenated representation of the predicate and argument of the subtree. (2) where et is the concatenation of the buffer representation bt and the stack representation st . While the stack"
P17-1005,D13-1160,0,0.827823,"erent from linguistically motivated ones.1 1 Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approa"
P17-1005,N16-1024,0,0.176489,"where next to is a domain-specific binary predicate that takes one argument (i.e., the entity texas) and returns a set of entities (e.g., the states bordering Texas) as its denotation. all is a special predicate that returns a collection of entities. exclude is a predicate that returns the difference between two input sets. An advantage of FunQL is that the resulting s-expression encodes semantic compositionality and derivation of the logical forms. This property makes FunQL logical forms convenient to be predicted with recurrent neural networks (Vinyals et al., 2015; Choe and Charniak, 2016; Dyer et al., 2016). However, FunQL is less expressive than lambda calculus, partially due to the elimination of variables. A more compact logical formulation which our method also applies to is λ-DCS (Liang, 2013). In the absence of anaphora and composite binary predicates, conversion algorithms exist between FunQL and λ-DCS. However, we leave this to future work. Preliminaries Problem Formulation Let K denote a knowledge base or more generally a reasoning system, and x an utterance paired with a grounded meaning representation G or its denotation y. Our problem is to learn a semantic parser that maps x to G vi"
P17-1005,P14-1133,0,0.261837,"anner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yields recursive semantic structures, avoiding the problem of generating ill-formed meaning representations. Compared to most existing semantic parsers which employ a CKY style bottom-up parsing strategy (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Berant et al., 2013; Berant and Liang, 2014), the transition-based approach we proposed does not require feature decomposition over structures and thereby enables the exploration of rich, non-local features. The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomorphic.2 As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding. The whole network is trained end-to-end on natural language utterances paired with annotated logical forms or th"
P17-1005,P14-1134,0,0.0313606,"ally motivated ones.1 1 Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yiel"
P17-1005,Q15-1039,0,0.189964,"er three datasets, we treat surrogate meaning representations which lead to the correct answer as gold standard. The surrogates were selected from a subset of candidate Freebase graphs, which were obtained by entity linking. Entity mentions in S PADES have been automatically annotated with Freebase entities (Gabrilovich et al., 2013). For W EB Q UESTIONS and G RAPH Q UESTIONS, we follow the procedure described in Reddy et al. (2016). We identify po4 http://developers.google.com/ freebase/ 49 Models Berant et al. (2013) Yao and Van Durme (2014) Berant and Liang (2014) Bast and Haussmann (2015) Berant and Liang (2015) Reddy et al. (2016) Bordes et al. (2014) Dong et al. (2015) Yih et al. (2015) Xu et al. (2016) Neural Baseline S CANNE R F1 35.7 33.0 39.9 49.4 49.7 50.3 39.2 40.8 52.5 53.3 48.3 49.4 Models Zettlemoyer and Collins (2005) Zettlemoyer and Collins (2007) Kwiatkowksi et al. (2010) Kwiatkowski et al. (2011) Kwiatkowski et al. (2013) Zhao and Huang (2015) Liang et al. (2011) Dong and Lapata (2016) Jia and Liang (2016) Jia and Liang (2016) with extra data S CANNE R Table 5: G EO Q UERY results. Table 3: W EB Q UESTIONS results. Models SEMPRE (Berant et al., 2013) PARASEMPRE (Berant and Liang, 2014)"
P17-1005,D16-1214,1,0.897283,"ich, non-local features. The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomorphic.2 As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding. The whole network is trained end-to-end on natural language utterances paired with annotated logical forms or their denotations. We conduct experiments on four datasets, including G EO Q UERY (which has logical forms; Zelle and Mooney 1996), S PADES (Bisk et al., 2016), W EB Q UESTIONS (Berant et al., 2013), and G RAPH Q UESTIONS (Su et al., 2016) (which have denotations). Our semantic parser achieves the state of the art on S PADES and G RAPH Q UESTIONS, while obtaining competitive results on G EO Q UERY and W EB Q UESTIONS. A side-product of our modeling framework is that the induced intermediate representations can contribute to rationalizing neural predictions (Lei et al., 2016). Specifically, they can shed light on the kinds of representations (especially predicates) useful for semantic parsing. Evaluation of the induced predicate-argument relations ag"
P17-1005,D14-1067,0,0.0665306,"Missing"
P17-1005,P15-1143,0,0.0186659,"rsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yields recursive semantic structures, avoiding the prob"
P17-1005,P13-1042,0,0.0795468,"ilable at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yields recursive semantic structures, avoiding the problem of generating ill-formed meaning representations. Compared to most existing semantic parsers which employ a CKY style bottom-up parsing strategy (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Berant et al., 2013; Berant and Liang, 2014), the transition-based approach we proposed does not require feature decomposition over structures and thereby enables the exploration of rich, non-local features. The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomorphic.2 As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding. The whole network is trained end-to-end on natural language uttera"
P17-1005,D16-1257,0,0.0243005,"e(all), next to(texas))) where next to is a domain-specific binary predicate that takes one argument (i.e., the entity texas) and returns a set of entities (e.g., the states bordering Texas) as its denotation. all is a special predicate that returns a collection of entities. exclude is a predicate that returns the difference between two input sets. An advantage of FunQL is that the resulting s-expression encodes semantic compositionality and derivation of the logical forms. This property makes FunQL logical forms convenient to be predicted with recurrent neural networks (Vinyals et al., 2015; Choe and Charniak, 2016; Dyer et al., 2016). However, FunQL is less expressive than lambda calculus, partially due to the elimination of variables. A more compact logical formulation which our method also applies to is λ-DCS (Liang, 2013). In the absence of anaphora and composite binary predicates, conversion algorithms exist between FunQL and λ-DCS. However, we leave this to future work. Preliminaries Problem Formulation Let K denote a knowledge base or more generally a reasoning system, and x an utterance paired with a grounded meaning representation G or its denotation y. Our problem is to learn a semantic parser"
P17-1005,P16-1004,1,0.825987,"Reddy et al., 2016, 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially enables the handling of unseen words and knowledge transfer across domains (Bender et al., 2015). The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) they reduce the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost since it is no longer possible to interpret how meaning composition is performed. Such knowledge plays a critical role in understand modeling limitations so as to build better semantic parsers. Moreover, without any tasks"
P17-1005,P16-1002,0,0.376516,"4; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially enables the handling of unseen words and knowledge transfer across domains (Bender et al., 2015). The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) they reduce the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost since it is no longer possible to interpret how meaning composition is performed. Such knowledge plays a critical role in understand modeling limitations so as to build better semantic parsers. Moreover, without any taskspecific prior knowled"
P17-1005,J13-2005,0,0.108582,"stic conventions. 2 Predicate answer Usage denotation wrapper type entity type checking all aggregation logical connectors querying for an entire set of entities one-argument meta predicates for sets two-argument meta predicates for sets Sub-categories — stateid, cityid, riverid, etc. — count, largest, smallest, etc. intersect, union, exclude Table 1: List of domain-general predicates. tion y. Grounded Meaning Representation We represent grounded meaning representations in FunQL (Kate et al., 2005) amongst many other alternatives such as lambda calculus (Zettlemoyer and Collins, 2005), λ-DCS (Liang, 2013) or graph queries (Holzschuher and Peinl, 2013; Harris et al., 2013). FunQL is a variable-free query language, where each predicate is treated as a function symbol that modifies an argument list. For example, the FunQL representation for the utterance which states do not border texas is: answer(exclude(state(all), next to(texas))) where next to is a domain-specific binary predicate that takes one argument (i.e., the entity texas) and returns a set of entities (e.g., the states bordering Texas) as its denotation. all is a special predicate that returns a collection of entities. exclude is a pre"
P17-1005,P11-1060,0,0.875123,"the handling of unseen words and knowledge transfer across domains (Bender et al., 2015). The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) they reduce the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost since it is no longer possible to interpret how meaning composition is performed. Such knowledge plays a critical role in understand modeling limitations so as to build better semantic parsers. Moreover, without any taskspecific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missin"
P17-1005,P14-5010,0,0.00234263,"Missing"
P17-1005,D16-1116,0,0.132496,"Missing"
P17-1005,D12-1069,0,0.0921173,"Missing"
P17-1005,P15-1142,0,0.139675,"Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yields recursive semantic str"
P17-1005,Q15-1019,0,0.0691589,"Missing"
P17-1005,D14-1162,0,0.0945929,"set of features include the embedding similarity between the relation and the utterance, as well as the similarity between the relation and the question words. The last set of features includes the answer type as indicated by the last word in the Freebase relation (Xu et al., 2016). We used the Adam optimizer for training with an initial learning rate of 0.001, two momentum parameters [0.99, 0.999], and batch size 1. The dimensions of the word embeddings, LSTM states, entity embeddings and relation embeddings are [50, 100, 100, 100]. The word embeddings were initialized with Glove embeddings (Pennington et al., 2014). All other embeddings were randomly initialized. In this section, we verify empirically that our semantic parser derives useful meaning representations. We give details on the evaluation datasets and baselines used for comparison. We also describe implementation details and the features used in the discriminative ranker. 4.1 Datasets We evaluated our model on the following datasets which cover different domains, and use different types of training data, i.e., pairs of natural language utterances and grounded meanings or question-answer pairs. G EO Q UERY (Zelle and Mooney, 1996) contains 880"
P17-1005,D10-1119,0,0.0264859,"ul for semantic parsing and how these are different from linguistically motivated ones.1 1 Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This i"
P17-1005,Q14-1030,1,0.910705,"ations so as to build better semantic parsers. Moreover, without any taskspecific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missing brackets). In this work, we propose a neural semantic parser that alleviates the aforementioned problems. Our model falls under the second class of approaches where utterances are first mapped to an intermediate representation containing natural language predicates. However, rather than using an external parser (Reddy et al., 2014, 2016) or manually specified CCG grammars (Kwiatkowski et al., 2013), we induce intermediate representations in the form of predicate-argument structures We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art"
P17-1005,D13-1161,0,0.23211,"ut any taskspecific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missing brackets). In this work, we propose a neural semantic parser that alleviates the aforementioned problems. Our model falls under the second class of approaches where utterances are first mapped to an intermediate representation containing natural language predicates. However, rather than using an external parser (Reddy et al., 2014, 2016) or manually specified CCG grammars (Kwiatkowski et al., 2013), we induce intermediate representations in the form of predicate-argument structures We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on S PADES and G RAPH Q UESTIONS and obtain competitive results on G"
P17-1005,Q16-1010,1,0.75919,"Missing"
P17-1005,D11-1140,0,0.571151,"Missing"
P17-1005,D16-1011,0,0.0082742,"Missing"
P17-1005,1998.amta-tutorials.1,0,0.612057,"Missing"
P17-1005,N15-1162,0,0.184412,"Missing"
P17-1005,N06-1056,0,0.268253,"of representations useful for semantic parsing and how these are different from linguistically motivated ones.1 1 Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/"
P17-1005,P16-1220,1,0.67213,"on score as a feature in the discriminative reranker, thus leaving the final disambiguation to the semantic parser. Apart from the entity score, the discriminative ranker uses the following basic features. The first feature is the likelihood score of a grounded representation aggregating all intermediate representations. The second set of features include the embedding similarity between the relation and the utterance, as well as the similarity between the relation and the question words. The last set of features includes the answer type as indicated by the last word in the Freebase relation (Xu et al., 2016). We used the Adam optimizer for training with an initial learning rate of 0.001, two momentum parameters [0.99, 0.999], and batch size 1. The dimensions of the word embeddings, LSTM states, entity embeddings and relation embeddings are [50, 100, 100, 100]. The word embeddings were initialized with Glove embeddings (Pennington et al., 2014). All other embeddings were randomly initialized. In this section, we verify empirically that our semantic parser derives useful meaning representations. We give details on the evaluation datasets and baselines used for comparison. We also describe implement"
P17-1005,P14-1090,0,0.0872756,"Missing"
P17-1005,P15-1128,0,0.0562248,"Missing"
P17-1005,P16-2033,0,0.00557879,"intermediate syntactic representation and then grounded to Freebase. Specifically, Bisk et al. (2016) evaluate the effectiveness of four different CCG parsers on the semantic parsing task when varying the amount of supervision required. As can be seen, S CANNE R outperforms all CCG variants (from unsupervised to fully supervised) without having access to any manually annotated derivations or lexicons. For fair comparison, we also built a neural baseline that encodes an utterance with a recurrent neural network and then predicts a grounded meaning representation directly (Ture and Jojic, 2016; Yih et al., 2016). Again, we observe that S CANNE R outperforms this baseline. Results on W EB Q UESTIONS are summarized in Table 3. S CANNE R obtains performance on par with the best symbolic systems (see the first block in the table). It is important to note that Bast and Haussmann (2015) develop a question answering system, which contrary to ours cannot produce meaning representations whereas Berant and Liang (2015) propose a sophisticated agenda-based parser which is trained borrowing ideas from imitation learning. S CANNE R is conceptually similar to Reddy et al. (2016) who also learn a semantic parser vi"
P17-1005,D07-1071,0,0.789834,"Missing"
P17-1005,D14-1107,0,\N,Missing
P17-1005,D16-1054,0,\N,Missing
P17-2057,N16-1181,0,0.0155087,"uestion answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Yahya et al., 2016; Xu et al., 2016a). Our work differs from them in two ways: 1) we do not need an explicit database query to retrieve the answers (Neelakantan et al., 2015a; Andreas et al., 2016); and 2) our text-based facts retain complete sentential context unlike the OpenIE triples (Banko et al., 2007; Carlson et al., 2010). Related Work A majority of the QA literature that focused on exploiting KB and text either improves the infer362 6 Conclusions In this work, we showed universal schema is a promising knowledge source for QA than using KB or text alone. Our results conclude though KB is preferred over text when the KB contains the fact of interest, a large portion of queries still attend to text indicating the amalgam of both text and KB is superior than KB alone. Acknowledgment"
P17-2057,D13-1160,0,0.0804093,"rsal schema. For evaluation, literature offers two options: 1) datasets for text-based question answering tasks such as answer sentence selection and reading comprehension; and 2) datasets for KB question answering. Although the text-based question answering datasets are large in size, e.g., SQuAD (Rajpurkar et al., 2016) has over 100k questions, answers to these are often not entities but rather sentences which are not the focus of our work. Moreover these texts may not contain Freebase entities at all, making these skewed heavily towards text. Coming to the alternative option, WebQuestions (Berant et al., 2013) is widely used for QA on Freebase. This dataset is curated such that all questions can be answered on Freebase alone. But since our goal is to explore the impact of universal schema, testing on a dataset completely answerable on a KB is not ideal. WikiMovies dataset (Miller et al., 2016) also has similar properties. Gardner and Krishnamurthy (2017) created a dataset with motivations similar to 360 Model Bisk et al. (2016) O NLY KB O NLY T EXT E NSEMBLE . U NI S CHEMA Dev. F1 Test F1 Question Answer 32.7 39.1 25.3 39.4 41.1 31.4 38.5 26.6 38.6 39.9 1. USA have elected blank , our first african"
P17-2057,P07-1073,0,0.0121583,", e.g., BASEBALL (Green Jr et al., 1961). This problem has matured into learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, An important but under-explored QA paradigm is where KB and text are exploited together (Ferrucci et al., 2010). Such combination is attractive because text contains millions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguis"
P17-2057,P15-1127,0,0.0230433,"y sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Yahya et al., 2016; Xu"
P17-2057,D15-1038,0,0.0141344,"ce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with OpenIE triples (Fader"
P17-2057,D14-1117,0,0.0110954,"performs O NLY KB by a wide-margin indicating U NI S CHEMA is robust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logica"
P17-2057,D12-1069,0,0.0101355,"age is less than 16 facts per entity, U NI S CHEMA outperforms O NLY KB by a wide-margin indicating U NI S CHEMA is robust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal withou"
P17-2057,D16-1147,0,0.292628,"r textual relations. 359 Memory Networks MemNNs are neural attention models with external and differentiable memory. MemNNs decouple the memory component from the network thereby allowing it store external information. Previously, these have been successfully applied to question answering on KB where the memory is filled with distributed representation of KB triples (Bordes et al., 2015), or for reading comprehension (Sukhbaatar et al., 2015; Hill et al., 2016), where the memory consists of distributed representation of sentences in the comprehension. Recently, key-value MemNN are introduced (Miller et al., 2016) where each memory slot consists of a key and value. The attention weight is computed only by comparing the question with the key memory, whereas the value is used to compute the contextual representation to predict the answer. We use this variant of MemNN for our model. Miller et al. (2016), in their experiments, store either KB triples or sentences as memories but they do not explicitly model multiple memories containing distinct data sources like we do. 3 Model Our model is a MemNN with universal schema as its memory. Figure 1 shows the model architecture. Memory: Our memory M comprise of b"
P17-2057,P09-1113,0,0.110593,"et al., 1961). This problem has matured into learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, An important but under-explored QA paradigm is where KB and text are exploited together (Ferrucci et al., 2010). Such combination is attractive because text contains millions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org"
P17-2057,P15-1016,1,0.327275,"obust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with Op"
P17-2057,D16-1264,0,0.0206994,"aset show that exploiting universal schema for question answering is better than using either a KB or text alone. This model also outperforms the current state-of-the-art by 8.5 F1 points. 1 The paradigm of exploiting text for questions started in the early 1990s (Kupiec, 1993). With the advent of web, access to text resources became abundant and cheap. Initiatives like TREC QA competitions helped popularizing this paradigm (Voorhees et al., 1999). With the recent advances in deep learning and availability of large public datasets, there has been an explosion of research in a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016). Still, text representation is unstructured and does not allow the compositional reasoning which structured KB supports. Introduction Question Answering (QA) has been a longstanding goal of natural language processing. Two main paradigms evolved in solving this problem: 1) answering questions on a knowledge base; and 2) answering questions using text. Knowledge bases (KB) contains facts expressed in a fixed schema, facilitating compositional reasoning."
P17-2057,Q14-1030,1,0.844524,"ty, U NI S CHEMA outperforms O NLY KB by a wide-margin indicating U NI S CHEMA is robust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the anno"
P17-2057,N13-1008,1,0.92922,"ions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2057 arg hea 2 is in a dquar rg 1 tere d ... of nt_ ide ... res kb: p 1 arg non 2 is t h arg -whit e firs ep t 1 res ide nt o ... f y pan _co m ty has _ci has kb: kb: USA/ Obama k ald rac ry lla USA NYC.. .. .. Ba ama .. .. .. .. .. Don p .. Hi m . . . Ob . . . . . Tru . 1 Affine+Softmax ..."
P17-2057,W14-4504,0,0.0805068,"Missing"
P17-2057,N16-1103,1,0.601109,"question answer pairs are shown in Table 2. Universal Schema Traditionally universal schema is used for relation extraction in the context of knowledge base population. Rows in the schema are formed by entity pairs (e.g. USA, NYC), and columns represent the relation between them. A relation can either be a KB relation, or it could be a pattern of text that exist between these two entities in a large corpus. The embeddings of entities and relation types are learned by low-rank matrix factorization techniques. Riedel et al. (2013) treat textual patterns as static symbols, whereas recent work by Verga et al. (2016) replaces them with distributed representation of sentences obtained by a RNN. Using distributed representation allows reasoning on sentences that are similar in meaning but different on the surface form. We too use this variant to encode our textual relations. 359 Memory Networks MemNNs are neural attention models with external and differentiable memory. MemNNs decouple the memory component from the network thereby allowing it store external information. Previously, these have been successfully applied to question answering on KB where the memory is filled with distributed representation of K"
P17-2057,C16-1226,0,0.0793455,"Missing"
P17-2057,P16-1220,1,0.730734,"as O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Ya"
P17-2057,D10-1099,1,0.857893,"into learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, An important but under-explored QA paradigm is where KB and text are exploited together (Ferrucci et al., 2010). Such combination is attractive because text contains millions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2057 arg hea 2 is in a"
P17-2057,P14-1090,0,0.109472,"Missing"
P17-2057,P15-1128,0,0.0642788,"U NI S CHEMA is robust even in resource-scarce scenario, whereas O NLY KB is very sensitive to the coverage. U NI S CHEMA also outperforms E N SEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them. 5 ence on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms. A few QA methods infer on curated"
P17-2057,D15-1203,0,0.0162448,"ntic parsers from parallel question and logical form pairs (Zelle and Mooney, An important but under-explored QA paradigm is where KB and text are exploited together (Ferrucci et al., 2010). Such combination is attractive because text contains millions of facts not present in KB, and a KB’s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts 358 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–365 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2057 arg hea 2 is in a dquar rg 1 tere d ."
Q14-1030,D11-1039,0,0.0484367,"interpretable formal meaning representations. Traditionally, sentences are converted into logical form grounded in the symbols of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). F"
Q14-1030,Q13-1005,0,0.640509,"ls of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a se"
Q14-1030,W13-2322,0,0.0261855,"hout using annotated question-answer pairs. We have shown how to obtain graph representations from the output of a CCG parser and subsequently learn their correspondence to Freebase using a rich feature set and their denotations as a form of weak supervision. Our parser yields state-of-the art performance on three large Freebase domains and is not limited to question answering. We can create semantic parses for any type of NL sentences. Our work brings together several strands of research. Graph-based representations of sentential meaning have recently gained some attention in the literature (Banarescu et al., 2013), and attempts to map sentences to semantic graphs have met with good inter-annotator agreement. Our work is also closely related to Kwiatkowski et al. (2013) and Berant and Liang (2014) who present open-domain se388 mantic parsers based on Freebase and trained on QA pairs. Despite differences in formulation and model structure, both approaches have explicit mechanisms for handling the mismatch between natural language and the KB (e.g., using logical-type equivalent operators or paraphrases). The mismatch is handled implicitly in our case via our graphical representation which allows for the i"
Q14-1030,D13-1160,0,0.919534,"e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching problem. We convert t"
Q14-1030,P14-1133,0,0.753639,"m conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching problem. We convert the output of an open-dom"
Q14-1030,C04-1180,1,0.417277,"tory Categorial Grammar The graph like structure of Freebase inspires us to create a graph like structure for natural language, and learn a mapping between them. To do this we take advantage of the representational power of Combinatory Categorial Grammar (Steedman, 2000). CCG is a linguistic formalism that tightly couples syntax and semantics, and can be used to model a wide range of language phenomCameron Titanic Cameron λyλx. directed.arg1(e, x) ∧ directed.arg2(e, y) NP Cameron e directed dir e cte d.a r g1 < e dir S directed.arg1(e, Cameron) ∧ directed.arg2(e, Titanic) ect 2 See Bos et al. (2004) for a detailed introduction to semantic representation using CCG. 3 Neo-Davidsonian semantics is a form of first-order logic that uses event identifiers (e) to connect verb predicates and their subcategorized arguments through conjunctions. 380 Cameron directed n 1997 in directed.arg1(e, Cameron) ∧ directed.arg2(e, Titanic) ∧ directed.in(e, 1997) ena. CCG is well known for capturing long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as standard local predicate-argument dependencies (Clark et al., 2002), thus supporting wide-coverag"
Q14-1030,P09-1010,0,0.0253326,"m each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013; Liang et al., 2011; Clarke et al., 2010), feedback-based learning has been previously used for interpreting and following NL instructions (Branavan et al., 2009; Chen and Mooney, 2011), playing computer games (Branavan et al., 2012), and grounding language in the physical world (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012). Lemma * * * * * be the * not, n’t no * * POS VB*, IN, TO, POS NN, NNS NNP*, PRP* RB* JJ* * * CD Semantic Class * NEGATION * WDT, WP*, WRB WDT, WP*, WRB COMPLEMENT EVENT TYPE ENTITY EVENTMOD TYPEMOD COPULA UNIQUE COUNT QUESTION CLOSED Semantic Category directed : (Se NPx <1&gt;)/NPy <2&gt; : λQλPλe.∃x∃y. directed.arg1(e, x) ∧ directed.arg2(e, y) ∧ P(x) ∧ Q(y) movie : NP : λx.movie(x) Obama : NP : λx.equal(x, Obama) annually :"
Q14-1030,P12-1014,0,0.0161775,"eir relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013; Liang et al., 2011; Clarke et al., 2010), feedback-based learning has been previously used for interpreting and following NL instructions (Branavan et al., 2009; Chen and Mooney, 2011), playing computer games (Branavan et al., 2012), and grounding language in the physical world (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012). Lemma * * * * * be the * not, n’t no * * POS VB*, IN, TO, POS NN, NNS NNP*, PRP* RB* JJ* * * CD Semantic Class * NEGATION * WDT, WP*, WRB WDT, WP*, WRB COMPLEMENT EVENT TYPE ENTITY EVENTMOD TYPEMOD COPULA UNIQUE COUNT QUESTION CLOSED Semantic Category directed : (Se NPx <1&gt;)/NPy <2&gt; : λQλPλe.∃x∃y. directed.arg1(e, x) ∧ directed.arg2(e, y) ∧ P(x) ∧ Q(y) movie : NP : λx.movie(x) Obama : NP : λx.equal(x, Obama) annually : Se Se : λPλe.lexe .annually(e) ∧ P(e) state : NPx /NPx : λPλx.lexx .sta"
Q14-1030,P13-1042,0,0.850757,"s have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs"
Q14-1030,P04-1014,0,0.0112942,"nation, extraction, raising and control, as well as standard local predicate-argument dependencies (Clark et al., 2002), thus supporting wide-coverage semantic analysis. Moreover, due to the transparent interface between syntax and semantics, it is relatively straightforward to built a semantic parse for a sentence from its corresponding syntactic derivation tree (Bos et al., 2004). In our case, the choice of syntactic parser is motivated by the scale of our problem; the parser must be broad-coverage and robust enough to handle a web-sized corpus. For these reasons, we rely on the C&C parser (Clark and Curran, 2004), a generalpurpose CCG parser, to obtain syntactic derivations. To our knowledge, we present the first attempt to use a CCG parser trained on treebanks for grounded semantic parsing. Most previous work has induced task-specific CCG grammars (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010). An example CCG derivation is shown in Figure 4. Semantic parses are constructed from syntactic CCG parses, with semantic composition being guided by the CCG syntactic derivation.2 We use a neo-Davidsonian (Parsons, 1990) semantics to represent semantic parses.3 Each word has a semantic categor"
Q14-1030,J07-4004,0,0.0398868,"mple of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching problem. We convert the output of an open-domain combinatory categorial grammar (CCG) parser (Clark and Curran, 2007) into a graphical representation and subsequently map it onto Freebase. The parser’s graphs (also called ungrounded graphs) are mapped to all possible Freebase subgraphs (also called grounded graphs) by replacing edges and nodes with relations and types in Freebase. Each grounded graph corresponds to a unique grounded logical query. During learning, our semantic parser is trained to identify which KB subgraph best corresponds to the NL graph. Problem377 Transactions of the Association for Computational Linguistics, 2 (2014) 377–392. Action Editor: Noah Smith. c Submitted 3/2014; Revised 6/2014"
Q14-1030,P02-1042,1,0.698829,"cted.arg2(e, Titanic) ect 2 See Bos et al. (2004) for a detailed introduction to semantic representation using CCG. 3 Neo-Davidsonian semantics is a form of first-order logic that uses event identifiers (e) to connect verb predicates and their subcategorized arguments through conjunctions. 380 Cameron directed n 1997 in directed.arg1(e, Cameron) ∧ directed.arg2(e, Titanic) ∧ directed.in(e, 1997) ena. CCG is well known for capturing long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as standard local predicate-argument dependencies (Clark et al., 2002), thus supporting wide-coverage semantic analysis. Moreover, due to the transparent interface between syntax and semantics, it is relatively straightforward to built a semantic parse for a sentence from its corresponding syntactic derivation tree (Bos et al., 2004). In our case, the choice of syntactic parser is motivated by the scale of our problem; the parser must be broad-coverage and robust enough to handle a web-sized corpus. For these reasons, we rely on the C&C parser (Clark and Curran, 2004), a generalpurpose CCG parser, to obtain syntactic derivations. To our knowledge, we present the"
Q14-1030,W10-2903,0,0.627118,"gical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large com"
Q14-1030,W02-1001,0,0.0343689,"ven a NL sentence s, we construct from its CCG syntactic derivation all corresponding ungrounded graphs u. Using a beam search procedure (described in Section 4.2), we find the best scoring graphs (g, ˆ u), ˆ maximizing over different graph configurations (g, u) of s: (g, ˆ u) ˆ = arg max Φ(g, u, s, K B ) · θ g,u (1) We define the score of (g, ˆ u) ˆ as the dot product between a high dimensional feature representation Φ = (Φ1 , . . . Φm ) and a weight vector θ (see Section 3.3 for details on the features we employ). We estimate the weights θ using the averaged structured perceptron algorithm (Collins, 2002). As shown in Algorithm 1, the perceptron makes several passes over sentences, and in each iteration it computes the best scoring (g, ˆ u) ˆ among the candidate graphs for a given sentence. In line 6, the algorithm updates θ with the difference (if any) be383 Algorithm 1: Averaged Structured Perceptron 1 2 3 4 Input: Training sentences: {si }Ni=1 θ←0 for t ← 1 . . . T do for i ← 1 . . . N do (gˆi , uˆi ) = arg max Φ(gi , ui , si , K B ) · θ gi ,ui 5 6 7 + if (u+ i , gi ) 6= (uˆi , gˆi ) then + θ ← θ + Φ(g+ i , ui , si , K B )−Φ(gˆi , uˆi , si , K B ) return 1 T T 1 N i ∑t=i N ∑i=1 θ t tween th"
Q14-1030,P13-1158,0,0.286302,"sisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora."
Q14-1030,P11-1149,0,0.0128605,"ed, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge"
Q14-1030,P11-1060,0,0.8618,"d Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowl"
Q14-1030,P11-1055,0,0.0290585,"ails when questions are too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi"
Q14-1030,Q13-1016,0,0.0259582,") formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013; Liang et al., 2011; Clarke et al., 2010), feedback-based learning has been previously used for interpreting and following NL instructions (Branavan et al., 2009; Chen and Mooney, 2011), playing computer games (Branavan et al., 2012), and grounding language in the physical world (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012). Lemma * * * * * be the * not, n’t no * * POS VB*, IN, TO, POS NN, NNS NNP*, PRP* RB* JJ* * * CD Semantic Class * NEGATION * WDT, WP*, WRB WDT, WP*, WRB COMPLEMENT EVENT TYPE ENTITY EVENTMOD TYPEMOD COPULA UNIQUE COUNT QUESTION CLOSED Semantic Category directed : (Se NPx <1&gt;)/NPy <2&gt; : λQλPλe.∃x∃y. directed.arg1(e, x) ∧ directed.arg2(e, y) ∧ P(x) ∧ Q(y) movie : NP : λx.movie(x) Obama : NP : λx.equal(x, Obama) annually : Se Se : λPλe.lexe .annually(e) ∧ P(e) state : NPx /NPx : λPλx.lexx .state(x) ∧ P(x) be: (Sy NPx )/NPy : λQλPλy.∃x.lexy (x) ∧ P(x) ∧ Q(y) the : NPx /"
Q14-1030,D12-1069,0,0.773171,"aches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or"
Q14-1030,D13-1161,0,0.680647,"te the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching"
Q14-1030,D10-1119,1,0.94973,"s requiring communication with machines in a language interpretable by them. Semantic parsing addresses the specific task of learning to map natural language (NL) to machine interpretable formal meaning representations. Traditionally, sentences are converted into logical form grounded in the symbols of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining m"
Q14-1030,Q13-1015,1,0.822387,"s for the incorporation of all manner of powerful features. More generally, our method is based on the assumption that linguistic structure has a correspondence to Freebase structure which does not always hold (e.g., in Who is the grandmother of Prince William?, grandmother is not directly expressed as a relation in Freebase). Additionally, our model fails when questions are too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, a"
Q14-1030,P09-1113,0,0.0420296,"ionally, our model fails when questions are too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond"
Q14-1030,P13-1092,0,0.16245,"ing data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) a"
Q14-1030,N13-1008,0,0.0175128,"too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013"
Q14-1030,P07-1121,0,0.589876,"to play a game are tasks requiring communication with machines in a language interpretable by them. Semantic parsing addresses the specific task of learning to map natural language (NL) to machine interpretable formal meaning representations. Traditionally, sentences are converted into logical form grounded in the symbols of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answe"
Q14-1030,P14-1090,0,0.512733,"Missing"
Q14-1030,D07-1071,0,0.68019,"Missing"
Q16-1010,D15-1138,0,0.00679455,"atkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vecto"
Q16-1010,P02-1041,0,0.0927099,"(Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on te"
Q16-1010,P14-1091,0,0.0366925,"Missing"
Q16-1010,P14-1133,0,0.232833,"Missing"
Q16-1010,Q15-1039,0,0.656552,"Missing"
Q16-1010,D13-1160,0,0.418417,"za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-arg"
Q16-1010,D14-1067,0,0.304718,"in and obtains the best result to date. Interestingly, D EP T REE outperforms S IMPLE G RAPH in this case. We attribute this to the small training set and larger lexical variation of Free917. The structural features of the graph-based representations seem highly beneficial in this case. 6.3 Error Analysis We categorized 100 errors made by D EP L AMBDA (+C +E) on the WebQuestions development set. In 43 cases the correct answer is present in the beam, 136 Method Cai and Yates (2013) Berant et al. (2013) Kwiatkowski et al. (2013) Yao and Van Durme (2014) Berant and Liang (2014) Bao et al. (2014) Bordes et al. (2014) Yao (2015) Yih et al. (2015) (FB API) Bast and Haussmann (2015) Berant and Liang (2015) Yih et al. (2015) (Y&C) Free917 Accuracy WebQuestions Average F1 59.0 62.0 68.0 – 68.5 – – – – 76.4 – – – 35.7 – 33.0 39.9 37.5 39.2 44.3 48.4 49.4 49.7 52.5 This Work D EP T REE S IMPLE G RAPH CCGG RAPH (+ C + E) D EP L AMBDA (+ C + E) 53.2 43.7 73.3 78.0 40.4 48.5 48.6 50.3 Table 3: Question-answering results on the WebQuestions and Free917 test sets. but ranked below an incorrect answer (e.g., for where does volga river start, the annotated gold answer is Valdai Hills, which is ranked second, with Russi"
Q16-1010,C04-1180,1,0.158689,"There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment."
Q16-1010,P13-1042,0,0.356088,"Missing"
Q16-1010,P15-1127,1,0.568655,"was sworn into office when john f kennedy was assassinated ), we do not have a special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from"
Q16-1010,W09-3726,0,0.0740413,"it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus, mimicking the stru"
Q16-1010,W02-1001,1,0.101306,"∈ &lt;n denotes the features for the pair of ungrounded and grounded graphs. Note that for a given query there may be multiple ungrounded graphs, primarily due to the optional use of the CON TRACT operation.3 The feature function has access to the ungrounded and grounded graphs, to the question, as well as to the content of the knowledge base and the denotation |g|K (the denotation of a grounded graph is defined as the set of entities or attributes reachable at its TARGET node). See Section 5.3 for the features employed. The model parameters are estimated with the averaged structured perceptron (Collins, 2002; Fre3 Another source of ambiguity may be a lexical item having multiple lambda-calculus entries; in our rules this only arises when analyzing count expressions such as how many. und and Schapire, 1999). Given a training questionanswer pair (q, A), the update is: θt+1 ← θt + Φ(u+ , g + , q, K) − Φ(ˆ u, gˆ, q, K) , where (u+ , g + ) denotes the pair of gold ungrounded and grounded graphs for q. Since we do not have direct access to these gold graphs, we instead rely on the set of oracle graphs, OK,A (q), as a proxy: (u+ , g + ) = arg max θt · Φ(u, g, q, K) , (u,g)∈OK,A (q) where OK,A (q) is def"
Q16-1010,P01-1019,0,0.126066,"nd Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph"
Q16-1010,C04-1026,0,0.10619,"tics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus,"
Q16-1010,P15-1026,0,0.238247,"et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grounded to Freebase by learning from question-answer pairs. E"
Q16-1010,P14-1134,0,0.0159776,"gh an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting represent"
Q16-1010,E03-1030,0,0.118015,"araphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to"
Q16-1010,P09-1069,0,0.0392655,"entation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to ra"
Q16-1010,P15-1143,0,0.0199683,"ntic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grou"
Q16-1010,N10-1145,0,0.0444525,"ource semantic representation and the target application’s representation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion"
Q16-1010,D12-1069,0,0.104733,"cquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances ove"
Q16-1010,Q15-1019,0,0.0603458,"s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparable method that generates ungrounded logical forms using"
Q16-1010,D10-1119,1,0.364959,"tructures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of"
Q16-1010,D13-1161,1,0.949182,"lambda-calculus expression and the relabeled s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparab"
Q16-1010,D14-1107,1,0.663912,"XPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connected to an edge. After an edge is grounded, the entity type nodes connected to it are grounded in turn, before the next edge is processed. To restrict the search, if two beam items correspond to the same grounded graph, the one with the lower score is discarded. A beam size of 100 was used in all experiments. Features. We use the f"
Q16-1010,P11-1060,0,0.141153,"plication’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally der"
Q16-1010,N15-1114,0,0.0108635,"representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TA"
Q16-1010,P14-5010,0,0.00310002,"presentation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use eight handcrafted part-of-speech patterns to identify entity span candidates. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014). For each candidate mention span, we retrieve the top 10 entities according to the Freebase API.5 We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. Finally, we generate ungrounded graphs for the top 10 paths through the lattice and treat the final entity disambiguation as part of the semantic parsing problem. 4 5 http://github.com/sivareddyg/graph-parser http://developers.google.com/freebase/ Representation -C -E -C +E"
Q16-1010,P13-2109,0,0.018844,"robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since our approach uses dependency trees as input, we hypothesize that it will generalize better to domains that are well covered by dependency parsers than methods that induce semantic grammars from scratch. The system that maps a dependency tree to its logical form (hencefo"
Q16-1010,H05-1066,0,0.0191937,"Missing"
Q16-1010,P14-1041,0,0.019399,"special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al.,"
Q16-1010,N15-1077,0,0.0234129,"ith approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given q"
Q16-1010,P15-1146,0,0.0242314,"Missing"
Q16-1010,P13-1092,0,0.00889818,"nt problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential"
Q16-1010,Q14-1030,1,0.452199,"mbda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of t"
Q16-1010,N15-1118,0,0.0159445,"Missing"
Q16-1010,N15-1040,0,0.0128228,"epresentations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and a"
Q16-1010,N06-1056,0,0.0904852,"trongest result to date on Free917 and competitive results on WebQuestions. 1 Disney acquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et"
Q16-1010,P07-1121,0,0.0158316,"converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provid"
Q16-1010,P15-1049,0,0.0484602,"Missing"
Q16-1010,P14-1090,0,0.408064,"Missing"
Q16-1010,N13-1106,0,0.0699006,"Missing"
Q16-1010,N15-3014,0,0.0687989,"iginal dependency tree. An event is created for each parent and its dependents in the tree. Each dependent is linked to this event with an edge labeled with its dependency relation, while the parent is linked to the event with an edge labeled arg0 . If a word is a question word, an additional TARGET predicate is attached to its entity node. S IMPLE G RAPH. This representation has a single event to which all entities in the question are connected by the predicate arg1 . An additional TARGET node is connected to the event by the predicate arg0 . This is similar to the template representation of Yao (2015) and Bast and Haussmann (2015). Note that this cannot represent any compositional structure. CCGG RAPH. Finally, we compare to the CCGbased semantic representation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use"
Q16-1010,P15-1128,0,0.133944,"into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since ou"
Q16-1010,P14-2107,0,0.0173573,".0 73.4 (c) Accuracy 42.6 48.2 46.5 48.8 42.6 48.2 48.9 50.4 D EP T REE S IMPLE G RAPH CCGG RAPH D EP L AMBDA Table 1: Oracle statistics and accuracies on the Web21.3 40.9 68.3 69.3 21.3 40.9 69.4 71.3 Table 2: Oracle statistics and accuracies on the Free917 Questions development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connect"
Q16-1010,J90-1001,0,\N,Missing
Q16-1010,D15-1198,0,\N,Missing
Q18-1048,P11-1062,0,0.135964,"t and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity constraints to be effective in learning entailment graphs, the Integer Linear Programming (ILP) solution of Berant et al. is not scalable beyond a few hundred nodes. In fact, the problem of finding a maximally weighted transitive subgraph of a graph with arbitrary edge weights is NP-hard (Berant et al., 2011). This paper instead proposes a scalable solution that does not rely on transitivity closure, but 703 Transactions of the Association fo"
Q18-1048,D15-1075,0,0.125403,"Missing"
Q18-1048,D17-1070,0,0.0422865,"Missing"
Q18-1048,P14-1061,1,0.833691,"better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions fro"
Q18-1048,P15-1034,0,0.0372871,"n C1 =3 unique predicates; (2) remove any predicate that is observed with fewer than C2 =3 unique argument-pairs. This leaves us with |P |=101K unique predicates in 346 entailment graphs. The maximum graph size is 53K nodes,8 and the total number of non-zero local scores in all graphs is 66M. In the future, we plan to test our method on an even larger corpus, but preliminary experiments suggest that data sparsity will persist regardless of the corpus size, because of the power law distribution of the terms. We compared our extractions qualitatively with Stanford Open IE (Etzioni et al., 2011; Angeli et al., 2015). Our CCG-based extraction generated noticeably 7 In our experiments, the total number of edges is ≈ .01|V |2 and most of predicate pairs are seen in less than 20 subgraphs, rather than |T |2 . 8 There are 4 graphs with more than 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of B"
Q18-1048,J15-2003,0,0.30255,"ing the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a combinatory categorial gramma"
Q18-1048,D17-1091,1,0.841356,"only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) con"
Q18-1048,P12-1013,0,0.0150022,"ith PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) connections. on assumptions concerning the graph structure. Berant et al. (2012, 2015) propose Tree-Node-Fix (TNF), an approximation method that scales better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensiv"
Q18-1048,P16-2041,0,0.530878,"han 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of Berant’s entailment data set. The types of this data set do not match with FIGER types, but we perform a simple handmapping between their types and FIGER types.10 Evaluation Entailment Data Sets Levy/Holt’s Entailment Data Set Levy and Dagan (2016) proposed a new annotation method (and a new data set) for collecting relational inference data in context. Their method removes a major bias in other inference data sets such as Zeichner’s (Zeichner et al., 2012), where candidate entailments were selected using a directional similarity measure. Levy and Dagan form questions of the type which city (qtype ), is located near (qrel ), mountains (qarg )? and provide possible answers of the form Kyoto (aanswer ), is surrounded by (arel ), mountains (aarg ). Annotators are shown a question with multiple possible answers, where aanswer is masked by q"
Q18-1048,S17-1026,0,0.22754,"Missing"
Q18-1048,N13-1092,0,0.0938356,"Missing"
Q18-1048,D13-1064,1,0.942149,"third argument by concatenating the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a comb"
Q18-1048,W14-2406,1,0.894326,"Missing"
Q18-1048,P05-1014,0,0.315327,"o arguments, where the type of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions signif"
Q18-1048,P98-2127,0,0.422662,"es as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies"
Q18-1048,P13-2078,0,0.0219818,"ype of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imp"
Q18-1048,C16-1268,0,0.106009,"ed by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on"
Q18-1048,P18-1188,1,0.78834,"Missing"
Q18-1048,W04-3250,0,0.0561007,"Missing"
Q18-1048,W17-2623,0,0.121698,"ign workers . . . . . . Barnes & Noble CEO William Lynch said as he unveiled his company’s Nook Tablet on Monday. The report said opium has accounted for more than half of Afghanistan’s gross domestic product in 2007. Who praised Mitt Romney’s credentials? Which gene did the ALS association discover ? How many Americans suffer from food allergies? What law might the deal break? Who launched the Nook Tablet? What makes up half of Afghanistans GDP ? Table 3: Examples where explicit entailment relations improve the rankings. The related words are boldfaced. contains questions about CNN articles (Trischler et al., 2017). Machine reading comprehension is usually evaluated by posing questions about a text passage and then assessing the answers of a system (Trischler et al., 2017). The data sets that are used for this task are often in the form of (document,question,answer) triples, where answer is a short span of the document. Answer selection is an important task, where the goal is to select the sentence(s) that contain the answer. We show improvements by adding knowledge from our learned entailments without changing the graphs or tuning them to this task in any way. Inverse sentence frequency (ISF) is a stro"
Q18-1048,P15-2070,0,0.0778647,"Missing"
Q18-1048,D14-1162,0,0.0911904,"Missing"
Q18-1048,P15-1129,0,0.0166683,"arallelizable and takes only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within"
Q18-1048,Q14-1030,1,0.832844,"(if any). We thus type all entities that can be grounded in Wikipedia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relati"
Q18-1048,W03-1011,0,0.834164,"and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although"
Q18-1048,N13-1008,0,0.245356,"dge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions from text, facts in knowledge bases, or both. Unlike our work, which directly learns entailment relations between predicates, these methods aim at predicting the source data—that is, whether two entities have a particular relationship. The common Related Work Our work is closely related to Berant et al. (2011), where entailment graphs are learned by imposing transitivity constraints on the entailment relations. However, the exact solution to the problem is not scalabl"
Q18-1048,D10-1106,0,0.090144,"Missing"
Q18-1048,P12-2031,0,0.170474,"Missing"
Q18-1048,D13-1183,0,0.540702,"edia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relations for the given sentence: visit1,2 with arguments (Obama, Hawaii),"
Q18-1048,C08-1107,0,0.828734,"as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity"
Q19-1016,D13-1160,0,0.0119652,"performance on our test set is 88.8 F1, theirs is 74.6 F1. Moreover, although CoQA’s answers can be freeform text, their answers are restricted only to extractive text spans. Our dataset contains passages from seven diverse domains, whereas their dataset is built only from Wikipedia articles about people. Related work We organize CoQA’s relation to existing work under the following criteria. Knowledge source We answer questions about text passages—our knowledge source. Another common knowledge source is machine-friendly databases, which organize world facts in the form of a table or a graph (Berant et al., 2013; Pasupat and Liang, 2015; Bordes et al., 2015; Saha et al., 2018; Talmor and Berant, 2018). However, understanding their structure requires expertise, making it challenging to crowd-source large QA datasets without relying on templates. Like passages, other human-friendly sources are 12 Concurrently, Saeidi et al. (2018) created a conversational QA dataset for regulatory text such as tax and visa regulations. Their answers are limited to yes or no along with a positive characteristic of permitting to ask clarification questions when a given question cannot be answered. Elgohary et al. (2018)"
Q19-1016,P18-1060,0,0.0111553,"is Table 8 presents fine-grained results of models and humans on the development set. We observe that humans have the highest disagreement on the unanswerable questions. The human agreement on answers that do not overlap with passage is lower than on answers that do overlap. This is expected because our evaluation metric is based on word overlap rather than on the meaning of words. For the question did Jenny like her new room?, human answers she loved it and yes are both accepted. Finding the perfect evaluation metric for abstractive responses is still a challenging problem (Liu et al., 2016; Chaganty et al., 2018) and beyond the scope of our work. For our models’ performance, seq2seq and PGNet perform well on non-overlapping answers, and DrQA performs well on overlapping answers, thanks to their respective designs. The augmented and combined models improve on both categories. Among the different question types, humans find lexical matches the easiest, followed by paraphrasing, and pragmatics the hardest—this is expected because questions with lexical matches and paraphrasing share some similarity with the passage, thus making them relatively easier to answer 11 We collect children’s stories from MCTest"
Q19-1016,P18-1082,0,0.0455605,"Missing"
Q19-1016,P16-1154,0,0.0100604,"ence-to-sequence with attention model for generating answers (Bahdanau et al., 2015). We append the conversation history and the current question to the passage, as p &lt;q&gt; qi−n &lt;a&gt; ai−n . . . &lt;q&gt; qi−1 &lt;a&gt; ai−1 &lt;q&gt; qi , and feed it into a bidirectional long short-term memory (LSTM) encoder, where n is the size of the history to be used. We generate the answer using an LSTM decoder which attends to the encoder states. Additionally, as the answer words are likely to appear in the original passage, we employ a copy mechanism in the decoder which allows to (optionally) copy a word from the passage (Gu et al., 2016; See et al., 2017). This model is referred to as the Pointer-Generator network, PGNet. A coherent conversation must have smooth transitions between turns. We expect the narrative structure of the passage to influence our conversation flow. We split each passage into 10 uniform chunks, and identify chunks of interest in a given turn and its transition based on rationale spans. Figure 4 shows the conversation flow of the first 10 turns. The starting turns tend to focus on the first few chunks and as the conversation advances, the focus shifts to the later chunks. Moreover, the turn transitions"
Q19-1016,P17-1171,1,0.896185,"rsational Models Models Given a passage p, the conversation history {q1 , a1 , . . . qi−1 , ai−1 }, and a question qi , the task is to predict the answer ai . Gold answers a1 , a2 , . . . , ai−1 are used to predict ai , similar to the setup discussed in Section 3.3. Our task can either be modeled as a conversational response generation problem or a reading comprehension problem. We evaluate 5.2 Reading Comprehension Models The state-of-the-art reading comprehension models for extractive question answering focus on finding a span in the passage that matches the question best (Seo et al., 2016; Chen et al., 2017; Yu et al., 2018). Because their answers are limited to spans, they cannot handle questions 7 We only pick the questions in which none of its answers can be found as a span in the passage. 8 6 whose answers do not overlap with the passage (e.g., Q3 , Q4 , and Q5 in Figure 1). However, this limitation makes them more effective learners than conversational models, which have to generate an answer from a large space of pre-defined vocabulary. We use the Document Reader (DrQA) model of Chen et al. (2017), which has demonstrated strong performance on multiple datasets (Rajpurkar et al., 2016; Labu"
Q19-1016,D18-1241,0,0.379907,"t. We present CoQA as a challenge to the community at https://stanfordnlp.github. io/coqa. 1 Introduction We ask other people a question to either seek or test their knowledge about a subject. Depending on their answer, we follow up with another question and their second answer builds on what has already been discussed. This incremental aspect makes human conversations succinct. An inability to build and maintain common ground in this way is part of why virtual assistants usually don’t seem like competent conversational partners. In this 1 CoQA is pronounced as coca. Concurrent with our work, Choi et al. (2018) also created a conversational dataset with a similar goal, but it differs in many aspects. We discuss the details in Section 7. 2 ∗ The first two authors contributed equally. 1 Transactions of the Association for Computational Linguistics, vol. 7, pp. 1–18, 2019. Action Editor: Scott Wen-tau Yih. Submission batch: 10/2018; Revision batch: 1/2019; Published 5/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. science. The last two are used for out-of-domain evaluation. To summarize, CoQA has the following key characteristics: • It consists of 127k co"
Q19-1016,P17-1167,0,0.0306674,"cts a rationale current affairs, politics, and culture and generates an answer three; for a question With who?, it predicts a rationale Mary and her husband, Rick, and then compresses it into Mary and Rick for improving the fluency; and for a multiple choice question Does this help or hurt their memory of the event? it predicts a rationale this obsession may prevent their brains from remembering and answers hurt. We think there is still great room for improving the combined model and we leave it to future work. 7 Conversational Modeling Our focus is on questions that appear in a conversation. Iyyer et al. (2017) and Talmor and Berant (2018) break down a complex question into a series of simple questions mimicking conversational QA. Our work is closest to Das et al. (2017) and Saha et al. (2018), who perform conversational QA on images and a knowledge graph, respectively, with the latter focusing on questions obtained by paraphrasing templates. In parallel to our work, Choi et al. (2018) also created a dataset of conversations in the form of questions and answers on text passages. In our interface, we show a passage to both the questioner and the answerer, whereas their interface only shows a title to"
Q19-1016,D18-1134,0,0.0285084,"ph (Berant et al., 2013; Pasupat and Liang, 2015; Bordes et al., 2015; Saha et al., 2018; Talmor and Berant, 2018). However, understanding their structure requires expertise, making it challenging to crowd-source large QA datasets without relying on templates. Like passages, other human-friendly sources are 12 Concurrently, Saeidi et al. (2018) created a conversational QA dataset for regulatory text such as tax and visa regulations. Their answers are limited to yes or no along with a positive characteristic of permitting to ask clarification questions when a given question cannot be answered. Elgohary et al. (2018) proposed a sequential question answering dataset collected from Quiz Bowl tournaments, where a sequence contains multiple related questions. These questions are related to the same concept while not focusing on the dialogue aspects (e.g., coreference). Zhou et al. (2018) is another dialogue dataset based on a single movie-related Wikipedia article, in which two workers are asked to chat about the content. Their dataset is more like chit-chat style conversations whereas our dataset focuses on multi-turn question answering. text spans as rationales, and text passages from seven diverse domains."
Q19-1016,N18-1023,0,0.0499559,"Missing"
Q19-1016,D17-2014,0,0.0335616,"Missing"
Q19-1016,P17-4012,0,0.0171088,"ct the next answer. In SQuAD, for computing a model’s performance, each individual prediction is compared against n human answers resulting in n F1 scores, the maximum of which is chosen as the prediction’s F1.10 For each question, we average out F1 across these n sets, both for humans and models. In our final evaluation, we use n = 4 human answers for every question (the original answer and 3 additionally collected answers). The articles a, an, and the and punctuations are excluded in evaluation. 6.2 Experimental Setup For all the experiments of seq2seq and PGNet, we use the OpenNMT toolkit (Klein et al., 2017) and its default settings: 2-layers of LSTMs with 500 hidden units for both the encoder and the decoder. The models are optimized using SGD, with an initial learning rate of 1.0 and a decay rate of 0.5. A dropout rate of 0.3 is applied to all layers. For the DrQA experiments, we use the implementation from the original paper (Chen et al., 2017). We tune the hyperparameters on the development data: the number of turns to use from the conversation history, the number of layers, number of each hidden units per layer, and dropout rate. The best configuration we find is 3 layers of LSTMs with 300 h"
Q19-1016,S18-1119,0,0.0489101,"Missing"
Q19-1016,P18-1077,0,0.041239,"Missing"
Q19-1016,P15-1142,0,0.0159657,"est set is 88.8 F1, theirs is 74.6 F1. Moreover, although CoQA’s answers can be freeform text, their answers are restricted only to extractive text spans. Our dataset contains passages from seven diverse domains, whereas their dataset is built only from Wikipedia articles about people. Related work We organize CoQA’s relation to existing work under the following criteria. Knowledge source We answer questions about text passages—our knowledge source. Another common knowledge source is machine-friendly databases, which organize world facts in the form of a table or a graph (Berant et al., 2013; Pasupat and Liang, 2015; Bordes et al., 2015; Saha et al., 2018; Talmor and Berant, 2018). However, understanding their structure requires expertise, making it challenging to crowd-source large QA datasets without relying on templates. Like passages, other human-friendly sources are 12 Concurrently, Saeidi et al. (2018) created a conversational QA dataset for regulatory text such as tax and visa regulations. Their answers are limited to yes or no along with a positive characteristic of permitting to ask clarification questions when a given question cannot be answered. Elgohary et al. (2018) proposed a sequential que"
Q19-1016,D17-1082,0,0.14884,"many questions. We use conversation history Q1 and A1 to answer Q2 with A2 based on the evidence R2 . Formally, to answer Qn , it depends on the conversation 3 In contrast, in NarrativeQA, the annotators were encouraged to use their own words and copying was not allowed in their interface. 2 Dataset Conversational Answer Type MCTest (Richardson et al., 2013) CNN/Daily Mail (Hermann et al., 2015) Children’s book test (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) NewsQA (Trischler et al., 2017) SearchQA (Dunn et al., 2017) TriviaQA (Joshi et al., 2017) RACE (Lai et al., 2017) Narrative QA (Koˇcisk`y et al., 2018) SQuAD 2.0 (Rajpurkar et al., 2018) CoQA (this work) 7 7 7 7 7 7 7 7 7 7 7 3 Multiple choice Spans Multiple choice Spans Free-form text, Unanswerable Spans Spans Spans Multiple choice Free-form text Spans, Unanswerable Free-form text, Unanswerable; Each answer comes with a text span rationale Domain Children’s stories News Children’s stories Wikipedia Web Search News Jeopardy Trivia Mid/High School Exams Movie Scripts, Literature Wikipedia Children’s Stories, Literature, Mid/High School Exams, News, Wikipedia, Reddit, Science Table 1: Comparison of CoQA wi"
Q19-1016,D14-1162,1,0.109684,"pplied to all layers. For the DrQA experiments, we use the implementation from the original paper (Chen et al., 2017). We tune the hyperparameters on the development data: the number of turns to use from the conversation history, the number of layers, number of each hidden units per layer, and dropout rate. The best configuration we find is 3 layers of LSTMs with 300 hidden units for each layer. A dropout rate of 0.4 is applied to all LSTM layers and a dropout rate of 0.5 is applied to word embeddings. We used Adam to optimize DrQA models. We initialized the word projection matrix with GloVe (Pennington et al., 2014) for conversational models and fastText (Bojanowski et al., 2017) for reading comprehension models, based on empirical performance. We update the projection matrix during training in order to learn embeddings for delimiters such as &lt;q&gt;. A Combined Model Finally, we propose a model that combines the advantages from both conversational models and extractive reading comprehension models. We use DrQA with PGNet in a combined model, in which DrQA first points to the answer evidence in the text, and PGNet naturalizes the evidence into an answer. For example, for Q5 in Figure 1, we expect that DrQA f"
Q19-1016,N16-1014,0,0.0304353,"he passage (see row No span found in Table 8). The augmented DrQA circumvents this problem with additional yes/no tokens, giving it a boost of 12.8 points. When DrQA is fed into PGNet, we empower both DrQA and PGNet—DrQA in producing free-form answers, PGNet in focusing on the rationale Table 7 presents the results of the models on the development and test data. Considering the results on the test set, the seq2seq model performs the worst, generating frequently occurring answers irrespective of whether these answers appear in the passage or not, a well known behavior of conversational models (Li et al., 2016). PGNet alleviates the frequent response problem by focusing on the vocabulary in the passage and it outperforms seq2seq by 17.8 points. However, 10 instead of the passage. This combination outperforms vanilla PGNet and DrQA models by 21.0 and 12.5 points, respectively, and is competitive with the augmented DrQA (65.1 vs. 65.4). History Augmt. DrQA+ size Seq2seq PGNet DrQA DrQA PGNet 0 1 2 all Models vs. Humans The human performance on the test data is 88.8 F1, a strong agreement indicating that the CoQA’s questions have concrete answers. Our best model is 23.4 points behind humans. 24.0 27.5"
Q19-1016,P18-2124,0,0.440792,"with A2 based on the evidence R2 . Formally, to answer Qn , it depends on the conversation 3 In contrast, in NarrativeQA, the annotators were encouraged to use their own words and copying was not allowed in their interface. 2 Dataset Conversational Answer Type MCTest (Richardson et al., 2013) CNN/Daily Mail (Hermann et al., 2015) Children’s book test (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) NewsQA (Trischler et al., 2017) SearchQA (Dunn et al., 2017) TriviaQA (Joshi et al., 2017) RACE (Lai et al., 2017) Narrative QA (Koˇcisk`y et al., 2018) SQuAD 2.0 (Rajpurkar et al., 2018) CoQA (this work) 7 7 7 7 7 7 7 7 7 7 7 3 Multiple choice Spans Multiple choice Spans Free-form text, Unanswerable Spans Spans Spans Multiple choice Free-form text Spans, Unanswerable Free-form text, Unanswerable; Each answer comes with a text span rationale Domain Children’s stories News Children’s stories Wikipedia Web Search News Jeopardy Trivia Mid/High School Exams Movie Scripts, Literature Wikipedia Children’s Stories, Literature, Mid/High School Exams, News, Wikipedia, Reddit, Science Table 1: Comparison of CoQA with existing reading comprehension datasets. nature of questions requires"
Q19-1016,D16-1264,0,0.344946,"ersation history, otherwise its answer could be Virginia or Richmond or something else. In our task, conversation history is indispensable for answering many questions. We use conversation history Q1 and A1 to answer Q2 with A2 based on the evidence R2 . Formally, to answer Qn , it depends on the conversation 3 In contrast, in NarrativeQA, the annotators were encouraged to use their own words and copying was not allowed in their interface. 2 Dataset Conversational Answer Type MCTest (Richardson et al., 2013) CNN/Daily Mail (Hermann et al., 2015) Children’s book test (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) NewsQA (Trischler et al., 2017) SearchQA (Dunn et al., 2017) TriviaQA (Joshi et al., 2017) RACE (Lai et al., 2017) Narrative QA (Koˇcisk`y et al., 2018) SQuAD 2.0 (Rajpurkar et al., 2018) CoQA (this work) 7 7 7 7 7 7 7 7 7 7 7 3 Multiple choice Spans Multiple choice Spans Free-form text, Unanswerable Spans Spans Spans Multiple choice Free-form text Spans, Unanswerable Free-form text, Unanswerable; Each answer comes with a text span rationale Domain Children’s stories News Children’s stories Wikipedia Web Search News Jeopardy Trivia Mid/High School Exams Movie Sc"
Q19-1016,D16-1230,0,0.011367,"urns. Error Analysis Table 8 presents fine-grained results of models and humans on the development set. We observe that humans have the highest disagreement on the unanswerable questions. The human agreement on answers that do not overlap with passage is lower than on answers that do overlap. This is expected because our evaluation metric is based on word overlap rather than on the meaning of words. For the question did Jenny like her new room?, human answers she loved it and yes are both accepted. Finding the perfect evaluation metric for abstractive responses is still a challenging problem (Liu et al., 2016; Chaganty et al., 2018) and beyond the scope of our work. For our models’ performance, seq2seq and PGNet perform well on non-overlapping answers, and DrQA performs well on overlapping answers, thanks to their respective designs. The augmented and combined models improve on both categories. Among the different question types, humans find lexical matches the easiest, followed by paraphrasing, and pragmatics the hardest—this is expected because questions with lexical matches and paraphrasing share some similarity with the passage, thus making them relatively easier to answer 11 We collect childr"
Q19-1016,K17-1028,0,0.0244175,"Figure 1 shows a conversation between two humans who are reading a passage, one acting as a questioner and the other as an answerer. In this conversation, every question after the first is dependent on the conversation history. For instance, Q5 (Who?) is only a single word and is impossible to answer without knowing what has already been said. Posing short questions is an effective human conversation strategy, but such questions are difficult for machines to parse. As is well known, state-of-the-art models rely heavily on lexical similarity between a question and a passage (Chen et al., 2016; Weissenborn et al., 2017). At present, there are no largescale reading comprehension datasets that contain questions that depend on a conversation history (see Table 1) and this is what CoQA is mainly developed for.2 The second goal of CoQA is to ensure the naturalness of answers in a conversation. Many existing QA datasets restrict answers to contiguous text spans in a given passage (Table 1). Such answers are not always natural—for example, there is no span-based answer to Q4 (How many?) in Figure 1. In CoQA, we propose that the answers can be free-form text, while for each answer, we also provide a text span from t"
Q19-1016,D18-1233,0,0.205047,"Missing"
Q19-1016,W17-4413,0,0.0277,"Missing"
Q19-1016,Q18-1021,0,0.0380035,"e use 200 stories for the development and the test sets. Augmented DrQA vs. Combined Model Although the performance of the augmented 11 Yes No Fluency Counting Multiple choice Augmt. DrQA DrQA+ PGNet Human 76.2 64.0 37.6 8.8 0.0 72.5 57.5 32.3 24.8 46.4 97.7 96.8 77.2 88.3 94.3 images and videos (Antol et al., 2015; Das et al., 2017; Hori et al., 2018). Naturalness There are various ways to curate questions: removing words from a declarative sentence to create a fill-in-the-blank question (Hermann et al., 2015), using a hand-written grammar to create artificial questions (Weston et al., 2016; Welbl et al., 2018), paraphrasing artificial questions to natural questions (Saha et al., 2018; Talmor and Berant, 2018), or, in our case, letting humans ask natural questions (Rajpurkar et al., 2016; Nguyen et al., 2016). While the former enable collecting large and cheap datasets, the latter enable collecting natural questions. Recent efforts emphasize collecting questions without seeing the knowledge source in order to encourage the independence of question and documents (Joshi et al., 2017; Dunn et al., 2017; Koˇcisk`y et al., 2018). Because we allow a questioner to see the passage, we incorporate measures t"
Q19-1016,P17-1099,1,0.0571326,"Missing"
Q19-1016,P18-1205,0,0.0266707,"ation progresses. Each chunk is one tenth of a passage. The x-axis indicates the turn number and the y -axis indicates the chunk containing the rationale. The height of a chunk indicates the concentration of conversation in that chunk. The width of the bands is proportional to the frequency of transition between chunks from one turn to the next. strong baselines from each modeling type and a combination of the two on CoQA. 5.1 Conversation Flow Sequence-to-sequence (seq2seq) models have shown promising results for generating conversational responses (Vinyals and Le, 2015; Serban et al., 2016; Zhang et al., 2018). Motivated by their success, we use a sequence-to-sequence with attention model for generating answers (Bahdanau et al., 2015). We append the conversation history and the current question to the passage, as p &lt;q&gt; qi−n &lt;a&gt; ai−n . . . &lt;q&gt; qi−1 &lt;a&gt; ai−1 &lt;q&gt; qi , and feed it into a bidirectional long short-term memory (LSTM) encoder, where n is the size of the history to be used. We generate the answer using an LSTM decoder which attends to the encoder states. Additionally, as the answer words are likely to appear in the original passage, we employ a copy mechanism in the decoder which allows to"
Q19-1016,N18-1059,0,0.2696,"ugh the performance of the augmented 11 Yes No Fluency Counting Multiple choice Augmt. DrQA DrQA+ PGNet Human 76.2 64.0 37.6 8.8 0.0 72.5 57.5 32.3 24.8 46.4 97.7 96.8 77.2 88.3 94.3 images and videos (Antol et al., 2015; Das et al., 2017; Hori et al., 2018). Naturalness There are various ways to curate questions: removing words from a declarative sentence to create a fill-in-the-blank question (Hermann et al., 2015), using a hand-written grammar to create artificial questions (Weston et al., 2016; Welbl et al., 2018), paraphrasing artificial questions to natural questions (Saha et al., 2018; Talmor and Berant, 2018), or, in our case, letting humans ask natural questions (Rajpurkar et al., 2016; Nguyen et al., 2016). While the former enable collecting large and cheap datasets, the latter enable collecting natural questions. Recent efforts emphasize collecting questions without seeing the knowledge source in order to encourage the independence of question and documents (Joshi et al., 2017; Dunn et al., 2017; Koˇcisk`y et al., 2018). Because we allow a questioner to see the passage, we incorporate measures to increase independence, although complete independence is not attainable in our setup (Section 3.1)."
Q19-1016,D18-1076,0,0.0231911,"other human-friendly sources are 12 Concurrently, Saeidi et al. (2018) created a conversational QA dataset for regulatory text such as tax and visa regulations. Their answers are limited to yes or no along with a positive characteristic of permitting to ask clarification questions when a given question cannot be answered. Elgohary et al. (2018) proposed a sequential question answering dataset collected from Quiz Bowl tournaments, where a sequence contains multiple related questions. These questions are related to the same concept while not focusing on the dialogue aspects (e.g., coreference). Zhou et al. (2018) is another dialogue dataset based on a single movie-related Wikipedia article, in which two workers are asked to chat about the content. Their dataset is more like chit-chat style conversations whereas our dataset focuses on multi-turn question answering. text spans as rationales, and text passages from seven diverse domains. We hope this work will stir more research in conversational modeling, a key ingredient for enabling natural human–machine communication. Acknowledgments We would like to thank MTurk workers, especially the Master Chatters and the MTC forum members, for contributing to th"
Q19-1016,W17-2623,0,0.339286,"Missing"
Q19-1016,Q17-1010,0,\N,Missing
Q19-1016,Q18-1023,0,\N,Missing
R09-1066,S01-1004,0,0.0306779,"Missing"
R09-1066,P97-1009,0,0.0426881,"use eat is a first order collocational feature of all the three words orange, banana and apple Definition 3.3. Semantic Category Tree (SCT): As already said, hindi wordnet has an ontological hierarchy and each sense of a word is mapped to some place in this hierarchy. The SCT of a word is a sub tree of this hierarchy which is shared with all the senses of this word. For example the SCT of word billA is shown in figure 1. If the pos tag of the word is known beforehand, only the subtree corresponding to this pos-tag is considered as SCT. 4 Our Approach Our approach is inspired from the work Lin [9]. He uses syntactic dependency as local context to do word sense disambiguation. His work is based on the intuition that Two different words are likely to have similar meanings if they occur in identical local contexts. Our assumption similar to Lin [9] is Two different words are likely to have similar semantic category if they have identical first order collocational features i.e. if they are second order collocates to each other. In this section, we present the methods Flat Semantic Category Labeler (FSCL) and Hierarchical Semantic Category Labeler (HSCL). FSCL treats semantic categories as"
R09-1066,S01-1005,0,0.088714,"Missing"
R09-1066,C92-2070,0,0.143232,"ng WordNet-based measures of semantic relatedness to find the sense of the word that is semantically most strongly related to the senses of the words in the context of the target word. Sinha and Mihalcea [13] present Graph based unsupervised word sense disambiguation. Their work combines the word semantic similarity measures and graph centrality measures for sense disambiguation. Semantic relatedness measures between ontological categories of hindi wordnet have yet to be studied and expored. In this scenario, we present approaches which do not need such semantic relatedness measures. Yarowsky [14] unsupervised WSD uses Bayesian theoretical framework where words that are indicative to each category are identified and weighed and these words are used in selection of a category. We use a probabilistic model slightly similar to the one used by Yarowsky. 3 Definitions We first introduce the task formally and define some terms which are used in further discussion. The task of semantic category labeling can be formally defined as follows. Given a sequence of words W = {w1 , w2 , . . . , wn } with each word wi having semantic categories SCwi = 366 If we define a feature of a word as (sw) withi"
R09-1066,W04-0807,0,\N,Missing
S10-1087,H93-1061,0,0.352349,"echniques using the background documents released in the task were used to assign ranking scores to the words and their senses. The test data was disambiguated using the Personalized PageRank algorithm which was applied to a graph constructed from the whole of WordNet in which nodes are initialized with ranking scores of words and their senses. In the competition, our systems achieved comparable accuracy of 53.4 and 52.2, which outperforms the most frequent sense baseline (50.5). 1 Introduction The senses in WordNet are ordered according to their frequency in a manually tagged corpus, SemCor (Miller et al., 1993). Senses that do not occur in SemCor are ordered arbitrarily after those senses of the word that have occurred. It is known from the results of SENSEVAL2 (Cotton et al., 2001) and SENSEVAL3 (Mihalcea and Edmonds, 2004) that first sense heuristic outperforms many WSD systems (see McCarthy et al. (2007)). The first sense baseline’s strong performance is due to the skewed frequency distribution of word senses. WordNet sense distributions based on SemCor are clearly useful, however in a given domain these distributions may not hold true. For example, the first sense for “bank” in WordNet refers to"
S10-1087,E09-1005,0,0.435374,", our approach aims to use these sense distributions collected from domain specific corpora as a knowledge source and combine this with information from the context. Our approach focuses on the strong influence of domain for WSD (Buitelaar et al., 2006) and the benefits of focusing on words salient to the domain (Koeling et al., 2005). Words are assigned a ranking score based on its keyness (salience) in the given domain. We use these word scores as another knowledge source. Graph based methods have been shown to produce state-of-the-art performance for unsupervised word sense disambiguation (Agirre and Soroa, 2009; Sinha and Mihalcea, 2007). These approaches use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular lexical knowledge base (LKB), such as WordNet. These graphbased algorithms are appealing because they take into account information drawn from the entire graph as well as from the given context, making them superior to other approaches that rely only on local information individually derived for each word. Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph We describe two systems that part"
S10-1087,N04-3012,0,0.0579884,"e of sense wsi is Domain Sense Ranking McCarthy et al. (2004) propose a method for finding predominant senses from raw text. The method uses a thesaurus acquired from automatically parsed text based on the method described by Lin (1998). This provides the top k nearest neighbours for each target word w, along with the distributional similarity score between the target word and each neighbour. The senses of a word w are each assigned a score by summing over the distributional similarity scores of its neighbours. These are weighted by a semantic similarity score (using WordNet Similarity score (Pedersen et al., 2004) between the sense of w and the sense of the neighbour that maximizes the semantic similarity score. More formally, let Nw = {n1 , n2 , . . . nk } be the ordered set of the top k scoring neighbours of w from the thesaurus with associated distributional similarity scores {dss(w, n1 ), dss(w, n2 ), . . . dss(w, nk )}. Let senses(w) be the set of senses of w. For each sense of w (wsi ∈ senses(w)) a ranking score is obtained by summing over the dss(w, nj ) of each neighbour (nj ∈ Nw ) multiplied by a weight. This weight is the WordNet similarity score (wnss) between the target sense (wsi ) and the"
S10-1087,W00-0901,0,0.0576778,"the domain specific corpus. In the next section we describe the way in which we compute krs(nj ). WordNet::Similarity::lesk (Pedersen et al., 2004) was used to compute word similarity wnss. IIITH1 and IIITH2 systems differ in the way senses are ranked. IIITH1 uses srs(wsj ) whereas IIITH2 system uses msrs(wsj ) for computing sense ranking scores in the given domain. 3 Domain Keyword Ranking We extracted keywords in the domain by comparing the frequency lists of domain corpora (background documents) and a very large general corpus, ukWaC (Ferraresi et al., 2008), using the method described by Rayson and Garside (2000). For each word in the frequency list of the domain corpora, words(domain), we calculated the loglikelihood (LL) statistic as described in Rayson and Garside (2000). We then normalized LL to compute keyword ranking score krs(w) of word w words(domain) using wnss(wsi , nj ) X wnss(wsi , nj ) X wsi senses(w) srs(wsi ) = X dss(w, nj )× wnss(wsi , nj ) wsi senses(w) 388 krs(w) = XLL(w) Keyword Ranking scores with PPR (KRS + PPR): This is same as PPR except that context words are initialized with krs. Sense Ranking scores with PPR (SRS + PPR): Edges connecting words and their synsets are assigned"
S10-1087,kilgarriff-etal-2010-corpus,1,0.850105,"Missing"
S10-1087,P03-1054,0,0.00449573,"ver the SemEval data are provided in Section 5. 2 where wnss(wsi , nj ) = maxnsx ∈senses(nj ) (wnss(wsi , nsx )) Since this approach requires only raw text, sense rankings for a particular domain can be generated by simply training the algorithm using a corpus representing that domain. We used the background documents provided to the participants in this task as a domain specific corpus. In general, a domain specific corpus can be obtained using domain-specific keywords (Kilgarriff et al., 2010). A thesaurus is acquired from automatically parsed background documents using the Stanford Parser (Klein and Manning, 2003). We used k = 5 to built the thesaurus. As we increased k we found the number of non-domain specific words occurring in the thesaurus increased and negatively affected the sense distributions. To counter this, one of our systems IIITH2 used a slightly modified ranking score by multiplying the effect of each neighbour with its domain keyword ranking score. The modified sense ranking msrs(wsj ) score of sense wsi is Domain Sense Ranking McCarthy et al. (2004) propose a method for finding predominant senses from raw text. The method uses a thesaurus acquired from automatically parsed text based o"
S10-1087,H05-1053,1,\N,Missing
S10-1087,J07-4005,1,\N,Missing
S10-1087,P04-1036,1,\N,Missing
S10-1087,P98-2127,0,\N,Missing
S10-1087,C98-2122,0,\N,Missing
S12-1081,S12-1051,0,0.0425111,"systems. For this reason we only provide a brief description of that. The results are promising, with Pearson’s coefficients on each individual dataset ranging from .3765 to .7761 for our relatively simple heuristics based systems that do not require training on different datasets. We provide some analysis of the results and also provide results for our data using Spearman’s, which as a nonparametric measure which we argue is better able to reflect the merits of the different systems (average is ranked between the others). 1 Introduction Our motivation for the systems entered in the STS task (Agirre et al., 2012) was to model the contribution of each linguistic component of a sentence to the similarity of a candidate match and vice versa. Siva Reddy Lexical Computing Ltd, UK siva@sivareddy.in Ultimately such a system could be exploited for ranking candidate paraphrases of a chunk of text of any length. We envisage a system as outlined in the future work section. The systems reported are simple baselines to such a system. We have two main systems (alignheuristic and wordsim) and also a system which simply uses the average score for each item from the two main systems (average). In our systems we: • onl"
S12-1081,W03-1004,0,0.0313257,"e similarity, where similarity is cast as meaning equivalence. 1 Textual entailment the relation under question is the more specific relation of entailment, where the meaning of one sentence is entailed by another and a system needs to determine the direction of the entailment. Lexical substitution relates to semantic textual similarity though the task involves a lemma in the context of a sentence, candidate substitutes are not provided, and the relation at question in the task is one of substitutability. 2 Paraphrase recognition is a highly related task, for example using comparable corpora (Barzilay and Elhadad, 2003) and it is likely that semantic textual similarity measures might be useful for ranking candidates in paraphrase acquisition. In addition to various works related to textual entailment, lexical substitution and paraphrasing, there has been some prior work explicitly on semantic text similarity. Semantic textual similarity has been explored in various works. Mihalcea et al. (2006) extend earlier work on word similarity using various WordNet similarity measures (Patwardhan et al., 2003) and a couple of corpus-based distributional measure PMI-IR (Turney, 2002) and LSA (Berry, 1992) using a measur"
S12-1081,de-marneffe-etal-2006-generating,0,0.0195891,"Missing"
S12-1081,P08-1028,0,0.0411168,"ystems for each item, is ranked between the other two systems. This gives a similar ranking of our three systems as the mean score. We also show average ρ. This is a macro average of the Spearman’s value for the 5 datasets without weighting by the number of sentence pairs. 10 6 Conclusions The systems were developed in less than a week including the time with the test data. There are many trivial fixes that may improve the basic algorithm, such as decapitalising proper nouns. There are many things we would like to try, such as val9 Note that Spearman’s ρ is often a little lower than Pearson’s Mitchell and Lapata (2008) 10 We do recognise the difficulty in determining metrics on a new pilot study. The task organisers are making every effort to make it clear that this enterprise is a pilot, not a competition and that they welcome feedback. idating the dependency matching process with the thesaurus matching. We would like to match larger units rather than tokens, with preferences towards the longer matching blocks. In parallel to the development of alignheuristic, we developed a system which measures the similarity between a node in the dependency tree of s1 and a node in the dependency tree of s2 as the sum o"
S12-1081,P11-1076,0,0.0197745,"distributional) weighted by the inverse document frequency of that word. The distributional similarity measures perform at a similar level to the knowledge-based 1 See the guidelines given to the annotators at http://www.cs.columbia.edu/ weiwei/workshop/instructions.pdf 2 This is more or less semantic equivalence since the annotators were instructed to focus on meaning http://www.dianamccarthy.co.uk/files/instructions.pdf. measures that use WordNet. Mohler and Mihalcea (2009) adapt this work for automatic short answer grading, that is matching a candidate answer to one supplied by the tutor. Mohler et al. (2011) take this application forward, combining lexical semantic similarity measures with a graph-alignment which considers dependency graphs using the Stanford dependency parser (de Marneffe et al., 2006) in terms of lexical, semantic and syntactic features. A score is then provided for each node in the graph. The features are combined using machine learning. The systems we propose likewise use lexical similarity and dependency relations, but in a simple heuristic formulation without a man-made thesaurus such as WordNet and without machine learning. 3 Systems We lemmatize and part-of-speech tag the"
S12-1081,E09-1065,0,0.0331374,"d LSA (Berry, 1992) using a measure which takes a summation over all tokens in both sentences for each finding the maximum similarity (WordNet or distributional) weighted by the inverse document frequency of that word. The distributional similarity measures perform at a similar level to the knowledge-based 1 See the guidelines given to the annotators at http://www.cs.columbia.edu/ weiwei/workshop/instructions.pdf 2 This is more or less semantic equivalence since the annotators were instructed to focus on meaning http://www.dianamccarthy.co.uk/files/instructions.pdf. measures that use WordNet. Mohler and Mihalcea (2009) adapt this work for automatic short answer grading, that is matching a candidate answer to one supplied by the tutor. Mohler et al. (2011) take this application forward, combining lexical semantic similarity measures with a graph-alignment which considers dependency graphs using the Stanford dependency parser (de Marneffe et al., 2006) in terms of lexical, semantic and syntactic features. A score is then provided for each node in the graph. The features are combined using machine learning. The systems we propose likewise use lexical similarity and dependency relations, but in a simple heurist"
S12-1081,W07-1401,0,\N,Missing
W11-1310,W03-1812,0,0.465554,"im(Vw1 ⊕w2 , Vw1 w2 ) &gt; γ, the compound is classified as compositional, where γ is a threshold for deciding compositionality. Global values of a and b were chosen by optimizing the performance on the development set. It was found that no single threshold value γ held for all compounds. Changing the threshold alters performance arbitrarily. This might be due to the polysemous nature of the constituent words which makes the composed vector Vw1 ⊕w2 filled with noisy contexts and thus making the judgement unpredictable. In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al. (2003). They also observe similar behaviour of the threshold γ. We try to address this problem by addressing the polysemy in WSMs using exemplar-based modelling. The above models use a simple addition based compositionality function. Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition. Contrary to that, Guevara (2011) observed additive models worked well for building compositional vectors. In our work, we try using evidence from both compositionality functions, simple addition and simple multiplication. Bannard et al. (2003); McCar"
W11-1310,W03-1809,0,0.532911,"Missing"
W11-1310,W11-1304,0,0.520526,"s.york.ac.uk diana@dianamccarthy.co.uk Suresh Manandhar University of York, UK Spandana Gella University of York, UK suresh@cs.york.ac.uk spandana@cs.york.ac.uk Abstract In this paper, we highlight the problems of polysemy in word space models of compositionality detection. Most models represent each word as a single prototype-based vector without addressing polysemy. We propose an exemplar-based model which is designed to handle polysemy. This model is tested for compositionality detection and it is found to outperform existing prototype-based models. We have participated in the shared task (Biemann and Giesbrecht, 2011) and our best performing exemplar-model is ranked first in two types of evaluations and second in two other evaluations. 1 Introduction In the field of computational semantics, to represent the meaning of a compound word, two mechanisms are commonly used. One is based on the distributional hypothesis (Harris, 1954) and the other is on the principle of semantic compositionality (Partee, 1995, p. 313). The distributional hypothesis (DH) states that words that occur in similar contexts tend to have similar meanings. Using this hypothesis, distributional models like the Word-space model (WSM, Sahl"
W11-1310,P10-2017,0,0.0416216,"Missing"
W11-1310,W06-1203,0,0.555699,"s as a PSC-based vector. So a PSC-based is composed of component DH-based vectors. Both of these two mechanisms are capable of determining the meaning vector of a compound word. For a given compound, if a DH-based vector and a PSC-based vector of the compound are projected into an identical space, one would expect the vectors to occupy the same location i.e. both the vectors should be nearly the same. However the principle of semantic compositionality does not hold for noncompositional compounds, which is actually what the existing WSMs of compositionality detection exploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006; Schone and Jurafsky, 2001). The DH-based and PSC -based vectors are expected to have high similarity when a compound is compositional and low similarity for non-compositional compounds. Most methods in WSM (Turney and Pantel, 2010) represent a word as a single context vector built from merging all its corpus instances. Such a representation is called the prototype-based modelling (Murphy, 2002). These prototype-based vectors do not 54 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 54–60, c Portland, Oregon, 24 June 2011. 2011 Association for"
W11-1310,W03-1810,1,0.953329,"Missing"
W11-1310,P08-1028,0,0.553899,"attern using corpus query language. Let w1 w2 be a compound word with constituent words w1 and w2 . Ew denotes the set of exemplars of w. Vw is the prototype vector of the word w, which is built by merging all the exemplars in Ew 1 Sketch Engine http://www.sketchengine.co.uk 55 For the purposes of producing a PSC-based vector for a compound, a vector of a constituent word is built using only the exemplars which do not contain the compound. Note that the vectors are sensitive to a compound’s word-order since the exemplars of w1 w2 are not the same as w2 w1 . We use other WSM settings following Mitchell and Lapata (2008). The dimensions of the WSM are the top 2000 content words in the given corpus (along with their coarse-grained part-of-speech information). Cosine similarity (sim) is used to measure the similarity between two vectors. Values at the specific positions in the vector representing context words are set to the ratio of the probability of the context word given the target word to the overall probability of the context word. The context window of a target word’s exemplar is the whole sentence of the target word excluding the target word. Our language of interest is English. We use the ukWaC corpus"
W11-1310,N10-1013,0,0.0369443,"ionality behaviour of phrases. We therefore also use evidence from the similarities between each constituent word and the compound. 4 Our Approach: Exemplar-based Model Our approach works as follows. Firstly, given a compound w1 w2 , we build its DH-based prototype vector Vw1 w2 from all its exemplars Ew1 w2 . Secondly, we remove irrelevant exemplars in Ew1 and Ew2 of constituent words and build the refined prototype vectors Vw1r and Vw2r of the constituent words w1 and w2 respectively. These refined vectors are used to compose the PSC-based vectors 2 of the compound. Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. This work does not relate to compositionality but to measuring semantic similarity of single words. As such, their clusters are not influenced by other words whereas in our approach for detecting compositionality, the other constituent word plays a major role. We use the compositionality functions, simple addition and simple multiplication to build Vw1r +w2r and Vw1r ×w2r respectively. Based on the similarities sim(Vw1 w2 , Vw1r ), sim(Vw1 w2 , Vw2r ), sim(Vw1 w2 , Vw1r +w2r ) and sim(Vw1 w2 , Vw1r ×w2r ), we"
W11-1310,W11-0115,0,0.0621038,"he constituent words which makes the composed vector Vw1 ⊕w2 filled with noisy contexts and thus making the judgement unpredictable. In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al. (2003). They also observe similar behaviour of the threshold γ. We try to address this problem by addressing the polysemy in WSMs using exemplar-based modelling. The above models use a simple addition based compositionality function. Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition. Contrary to that, Guevara (2011) observed additive models worked well for building compositional vectors. In our work, we try using evidence from both compositionality functions, simple addition and simple multiplication. Bannard et al. (2003); McCarthy et al. (2003) observed that methods based on distributional similarities between a phrase and its constituent words help when determining the compositionality behaviour of phrases. We therefore also use evidence from the similarities between each constituent word and the compound. 4 Our Approach: Exemplar-based Model Our approach works as follows. Firstly, given a compound w1"
W11-1310,P07-2011,0,0.060728,"Missing"
W11-1310,W01-0513,0,0.791299,"a PSC-based is composed of component DH-based vectors. Both of these two mechanisms are capable of determining the meaning vector of a compound word. For a given compound, if a DH-based vector and a PSC-based vector of the compound are projected into an identical space, one would expect the vectors to occupy the same location i.e. both the vectors should be nearly the same. However the principle of semantic compositionality does not hold for noncompositional compounds, which is actually what the existing WSMs of compositionality detection exploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006; Schone and Jurafsky, 2001). The DH-based and PSC -based vectors are expected to have high similarity when a compound is compositional and low similarity for non-compositional compounds. Most methods in WSM (Turney and Pantel, 2010) represent a word as a single context vector built from merging all its corpus instances. Such a representation is called the prototype-based modelling (Murphy, 2002). These prototype-based vectors do not 54 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 54–60, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics di"
W11-3603,P11-1061,0,0.0232462,"rt-of-speech tagger for Kannada exploiting the resources of Telugu. We also build large corpora and a morphological analyser (including lemmatisation) for Kannada. Our experiments reveal that a cross-language taggers are as efficient as mono-lingual taggers. We aim to extend our work to other Indian languages. Our tools are efficient and significantly faster than the existing monolingual tools. 1 2 Introduction Related Work There are several methods for building POS taggers for a target language using source language resources. Some researchers (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Das and Petrov, 2011) built POS taggers for a target language using parallel corpus. The source (cross) language is expected to have a POS tagger. First, the source language tools annotate the source side of the parallel corpora. Later these annotations are projected to the target language side using the alignments in the parallel corpora, creating virtual annotated corpora for the target language. A POS tagger for the target is then built from the virtual annotated corpora. Other methods which make use of parallel corpora are (Snyder et al., 2008; Naseem et al., 2009). These approaches are based on hierarchical B"
W11-3603,feldman-etal-2006-cross,0,0.0617409,"s and Rundell, 2008), building lexicons and morphological analysers is also possible to considerable extent. The other reason for the lack of POS taggers is partly due the lack of researchers working on a 11 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 11–19, Chiang Mai, Thailand, November 8-12, 2011. 2.2 sampling techniques. They aim to gain from information shared across languages. The main disadvantage of all such methods is that they rely on parallel corpora which itself is a costly resource for resource-poor languages. Hana et al. (2004) and Feldman et al. (2006) propose a method for developing a POS tagger for a target language using the resources of another typologically related language. Our method is motivated from them, but with the focus on resources available for Indian languages. 2.1 There exists literature on Kannada morphological analysers (Vikram and Urs, 2007; Antony et al., 2010; Shambhavi et al., 2011) and POS taggers (Antony and Soman, 2010) but none of them have any publicly downloadable resources. Murthy (2000) gives an overview of existing resources for Kannada and points out that most of these exist without public access. We are int"
W11-3603,W04-3229,0,0.760204,"in lexicography (Atkins and Rundell, 2008), building lexicons and morphological analysers is also possible to considerable extent. The other reason for the lack of POS taggers is partly due the lack of researchers working on a 11 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 11–19, Chiang Mai, Thailand, November 8-12, 2011. 2.2 sampling techniques. They aim to gain from information shared across languages. The main disadvantage of all such methods is that they rely on parallel corpora which itself is a costly resource for resource-poor languages. Hana et al. (2004) and Feldman et al. (2006) propose a method for developing a POS tagger for a target language using the resources of another typologically related language. Our method is motivated from them, but with the focus on resources available for Indian languages. 2.1 There exists literature on Kannada morphological analysers (Vikram and Urs, 2007; Antony et al., 2010; Shambhavi et al., 2011) and POS taggers (Antony and Soman, 2010) but none of them have any publicly downloadable resources. Murthy (2000) gives an overview of existing resources for Kannada and points out that most of these exist without"
W11-3603,J03-3001,0,0.0188693,"tion probabilities of Telugu by training TnT on the machine annotated corpora of Telugu. Since Telugu and Kannada are typologically related, we assume the transition probabilities of Kannada to be the same as of Telugu 3. Estimate the emission probabilities of Kannada from machine annotated Telugu corpus or machine annotate Kannada corpus 4. Use the probabilities from the step 2 and 3 to build a POS tagger for Kannada 4.1 Step1: Kannada and Telugu Corpus Creation Corpus collection once used to be long, slow and expensive. But with the advent of the Web and the success of Web-as-Corpus notion (Kilgarriff and Grefenstette, 2003), corpus collection can be highly automated, and thereby fast and inexpensive. We have used Corpus Factory method (Kilgarriff et al., 2010) to collect Kannada and Telugu corpora from the Web. The method is described in the following steps. Frequency List: Corpus Factory method requires a frequency list of the language of interest to start corpus collection. The frequency list of 3 Wikipedia Dumps: http://dumps.wikimedia. org 4 Bing: http://bing.com 14 which the ratio of non-frequent words to the highfrequent words is maintained. If a page doesn’t meet this criteria, we discard it. Near-Duplica"
W11-3603,kilgarriff-etal-2010-corpus,1,0.442272,"Missing"
W11-3603,A00-1031,0,0.047623,"d Karthik, 2007) was ranked best among all the existing taggers. Indian languages are morphologically rich with Dravidian languages posing extra challenge because of their agglutinative nature. Avinesh and Karthik (2007) noted that morphological information play an important role in Indian language POS tagging. Their CRF model is trained on all the important morphological features to predict the output tag for a word in a given context. The pipeline of (Avinesh and Karthik, 2007) can be described as below Hana et al. (2004) Hana et al. aim to develop a tagger for Russian from Czech using TnT (Brants, 2000), a second-order Markov model. Though the languages Czech and Russian are free-word order, they argue that TnT is as efficient as other models. TnT tagger is based on two probabilities - the transition and emission probabilities. The tag sequence of a given word sequence is selected by calculating argmax t1 ...tn "" n Y i=1 # P (ti |ti−1 , ti−2 )P (wi |ti ) Existing Tools for Kannada (1) where wi . . . wn is the word sequence and t1 . . . tn are their corresponding POS tags. Transition probabilities, P (ti |ti−1 , ti−2 ), describe the conditional probability of a tag given the tags of previous"
W11-3603,C08-1098,0,0.0117851,"agged corpus captures an approximation of the true transition probabilities in the manually annotated corpora. The tagged corpus is converted to the format in Figure 1 and then using TnT we estimate transition probabilities. 15 4.3.2 4.4 Source tags and target morphology We experimented with various TnT tagging models by selecting transition and emission probabilities from the Steps 2 and 3. Though one may question the performance of TnT for free-word order languages like Kannada, Hana et al. (2004) found that TnT models are as good as other models for free-word order languages. Additionally, Schmid and Laws (2008) observed that TnT models are also good at learning fined-grained transition probabilities. In our evaluation, we also found that our TnT models are competitive to the existing CRF model of (Avinesh and Karthik, 2007). Apart from building POS tagging models, we also learned the associations of each word with its lemma and suffix given a POS tag, from the machine annotated Kannada corpus. For example, Kannada word aramaneVgalYannu is associated with lemma aramaneV and suffix annu when occurred with the tag NN.n.n.pl..o and similarly word aramaneVgeV is associated with lemma aramaneV and suffix"
W11-3603,D08-1109,0,0.0144097,"Some researchers (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Das and Petrov, 2011) built POS taggers for a target language using parallel corpus. The source (cross) language is expected to have a POS tagger. First, the source language tools annotate the source side of the parallel corpora. Later these annotations are projected to the target language side using the alignments in the parallel corpora, creating virtual annotated corpora for the target language. A POS tagger for the target is then built from the virtual annotated corpora. Other methods which make use of parallel corpora are (Snyder et al., 2008; Naseem et al., 2009). These approaches are based on hierarchical Bayesian models and Markov Chain Monte Carlo Part-of-speech (POS) taggers are some of the basic tools for natural language processing in any language. For example, they are needed for terminology extraction using linguistic patterns or for selecting word lists in language teaching and lexicography. At the same time, many languages lack POS taggers. One reasons for this is the lack of other basic resources like corpora, lexicons or morphological analysers. With the advent of Web, collecting corpora is no longer a major problem ("
W11-3603,N01-1026,0,0.0125042,"build a cross-language part-of-speech tagger for Kannada exploiting the resources of Telugu. We also build large corpora and a morphological analyser (including lemmatisation) for Kannada. Our experiments reveal that a cross-language taggers are as efficient as mono-lingual taggers. We aim to extend our work to other Indian languages. Our tools are efficient and significantly faster than the existing monolingual tools. 1 2 Introduction Related Work There are several methods for building POS taggers for a target language using source language resources. Some researchers (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Das and Petrov, 2011) built POS taggers for a target language using parallel corpus. The source (cross) language is expected to have a POS tagger. First, the source language tools annotate the source side of the parallel corpora. Later these annotations are projected to the target language side using the alignments in the parallel corpora, creating virtual annotated corpora for the target language. A POS tagger for the target is then built from the virtual annotated corpora. Other methods which make use of parallel corpora are (Snyder et al., 2008; Naseem et al., 2009). These approaches are"
W11-3603,H01-1035,0,0.0315839,"paper, we show how to build a cross-language part-of-speech tagger for Kannada exploiting the resources of Telugu. We also build large corpora and a morphological analyser (including lemmatisation) for Kannada. Our experiments reveal that a cross-language taggers are as efficient as mono-lingual taggers. We aim to extend our work to other Indian languages. Our tools are efficient and significantly faster than the existing monolingual tools. 1 2 Introduction Related Work There are several methods for building POS taggers for a target language using source language resources. Some researchers (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Das and Petrov, 2011) built POS taggers for a target language using parallel corpus. The source (cross) language is expected to have a POS tagger. First, the source language tools annotate the source side of the parallel corpora. Later these annotations are projected to the target language side using the alignments in the parallel corpora, creating virtual annotated corpora for the target language. A POS tagger for the target is then built from the virtual annotated corpora. Other methods which make use of parallel corpora are (Snyder et al., 2008; Naseem et al., 200"
W14-5146,I08-2099,1,0.802491,"Missing"
W16-6625,N03-1003,0,0.171655,"l corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An example word lattice for t"
W16-6625,P14-1133,0,0.181583,"robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approach"
W16-6625,Q15-1039,0,0.0619899,"Missing"
W16-6625,D13-1160,0,0.0680962,"k dataset show that the performance of the semantic parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-ba"
W16-6625,C04-1180,0,0.0518088,"st find the set of oracle grounded graphs—Freebase subgraphs which when executed yield the correct answer—derivable from the question’s ungrounded graphs. These oracle graphs are then used to train a structured perceptron model. These steps are discussed in detail below. 157 3.1 Ungrounded Graphs from Paraphrases We use G RAPH PARSER (Reddy et al., 2014) to convert paraphrases to ungrounded graphs. This conversion involves three steps: 1) parsing the paraphrase using a CCG parser to extract syntactic derivations (Lewis and Steedman, 2014), 2) extracting logical forms from the CCG derivations (Bos et al., 2004), and 3) converting the logical forms to an ungrounded graph.8 The ungrounded graph for the example question and its paraphrases are shown in Figure 3(a), Figure 3(b) and Figure 3(c), respectively. 3.2 Grounded Graphs from Ungrounded Graphs The ungrounded graphs are grounded to Freebase subgraphs by mapping entity nodes, entity-entity edges and entity type nodes in the ungrounded graph to Freebase entities, relations and types, respectively. For example, the graph in Figure 3(b) can be converted to a Freebase graph in Figure 3(d) by replacing the entity node Czech Republic with the Freebase en"
W16-6625,P05-1022,0,0.0827711,"araphrases of the input question q. These paraphrases are further filtered with a classifier to improve the precision of the generated paraphrases. L-PCFG Estimation We train the L-PCFG Gsyn on the Paralex corpus (Fader et al., 2013). Paralex is a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what ex"
W16-6625,N13-1015,1,0.837008,"ase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clusters them into latent states. Then it foll"
W16-6625,W02-1001,0,0.0392051,"gˆ) under the model θ ∈ Rn : (ˆ p, u ˆ, gˆ) = arg max θ · Φ(p, u, g, q, K) , (p,u,g) where Φ(p, u, g, q, K) ∈ Rn denotes the features for the tuple of paraphrase, ungrounded and grounded graphs. The feature function has access to the paraphrase, ungrounded and grounded graphs, the original question, as well as to the content of the knowledge base and the denotation |g|K (the denotation of a grounded graph is defined as the set of entities or attributes reachable at its TARGET node). See §4.3 for the features employed. The model parameters are estimated with the averaged structured perceptron (Collins, 2002). Given a training questionanswer pair (q, A), the update is: θt+1 ← θt +Φ(p+ , u+ , g + , q, K)−Φ(ˆ p, u ˆ, gˆ, q, K) , where (p+ , u+ , g + ) denotes the tuple of gold paraphrase, gold ungrounded and grounded graphs for 158 q. Since we do not have direct access to the gold paraphrase and graphs, we instead rely on the set of oracle tuples, OK,A (q), as a proxy: (p+ , u+ , g + ) = arg max (p,u,g)∈OK,A (q) θ · Φ(p, u, g, q, K) , where OK,A (q) is defined as the set of tuples (p, u, g) derivable from the question q, whose denotation |g|K has minimal F1 -loss against the gold answer A. We find t"
W16-6625,N06-2009,0,0.358515,"ebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. 15"
W16-6625,P13-1158,0,0.615591,"ney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases."
W16-6625,N13-1092,0,0.134832,"Missing"
W16-6625,P96-1027,0,0.368333,"First, the sampling from an L-PCFG grammar lessens the lexical ambiguity problem evident in lexicalized grammars such as tree adjoining grammars (Narayan and Gardent, 2012) and combinatory categorial grammars (White, 2004). Our grammar is not lexicalized, only unary context-free rules are lexicalized. Second, the top-down sampling restricts the combinatorics inherent to bottom-up search (Shieber et al., 1990). Third, we do not restrict the generation by the order information in the input. The lack of order information in the input often raises the high combinatorics in lexicalist approaches (Kay, 1996). In our case, however, we use sampling to reduce this problem, and it allows us to produce syntactically diverse questions. And fourth, we impose no constraints on the grammar thereby making it easier to maintain bi-directional (recursive) grammars that can be used both for parsing and for generation (Shieber, 1988). 2.2 Bi-Layered L-PCFGs As mentioned earlier, one of our lattice types is based on bi-layered PCFGs introduced here. In their traditional use, the latent states in LPCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states:"
W16-6625,P07-2045,0,0.00417424,"Missing"
W16-6625,P02-1003,0,0.0603275,"e and outside trees as they use, capturing contextual syntactic information about nonterminals. We refer the reader to Narayan and Cohen (2015) for more detailed description of these features. In our experiments, we set the number of latent states to 24. Once we estimate Gsyn from the Paralex corpus, we restrict it for each question to a grammar G0syn by keeping only the rules that could lead to a derivation over the lattice. This step is similar to lexical pruning in standard grammar-based generation process to avoid an intermediate derivation which can never lead to a successful derivation (Koller and Striegnitz, 2002; Narayan and Gardent, 2012). Paraphrase Sampling Sampling a question from the grammar G0syn is done by recursively sampling nodes in the derivation tree, together with their latent states, in a top-down breadth-first fashion. Sampling from the pruned grammar G0syn raises an issue of oversampling words that are more frequent in the training data. To lessen this problem, we follow a controlled sampling approach where sampling is guided by the word lattice Wq . Once a word w from a path e in Wq is sampled, all other parallel or conflicting paths to e are removed from Wq . For example, generating"
W16-6625,D12-1069,0,0.0277247,"y idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasi"
W16-6625,D13-1161,0,0.0181592,"parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mai"
W16-6625,P98-1116,0,0.119099,"a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An ex"
W16-6625,D14-1107,0,0.0240241,"ly on manually assembled questionanswer pairs. For each training question, we first find the set of oracle grounded graphs—Freebase subgraphs which when executed yield the correct answer—derivable from the question’s ungrounded graphs. These oracle graphs are then used to train a structured perceptron model. These steps are discussed in detail below. 157 3.1 Ungrounded Graphs from Paraphrases We use G RAPH PARSER (Reddy et al., 2014) to convert paraphrases to ungrounded graphs. This conversion involves three steps: 1) parsing the paraphrase using a CCG parser to extract syntactic derivations (Lewis and Steedman, 2014), 2) extracting logical forms from the CCG derivations (Bos et al., 2004), and 3) converting the logical forms to an ungrounded graph.8 The ungrounded graph for the example question and its paraphrases are shown in Figure 3(a), Figure 3(b) and Figure 3(c), respectively. 3.2 Grounded Graphs from Ungrounded Graphs The ungrounded graphs are grounded to Freebase subgraphs by mapping entity nodes, entity-entity edges and entity type nodes in the ungrounded graph to Freebase entities, relations and types, respectively. For example, the graph in Figure 3(b) can be converted to a Freebase graph in Fig"
W16-6625,N12-1019,0,0.0463653,"ar is not fine-grained enough and often merges different paraphrase information into the same latent state. This problem is often severe for nonterminals at the top level of the bilayered tree. Hence, we rely only on unary lexical rules (the rules that produce terminal nodes) to extract paraphrase patterns in our experiments. 6 We have 154 positive and 846 negative paraphrase pairs. 7 We do not use the paraphrase pairs from the Paralex corpus to train our classifier, as they do not represent the distribution of our sampled paraphrases and the classifier trained on them performs poorly. follow Madnani et al. (2012), who used MT metrics for paraphrase identification, and experiment with 8 MT metrics as features for our binary classifier. In addition, we experiment with a binary feature which checks if the sampled paraphrase preserves named entities from the input sentence. We use WEKA (Hall et al., 2009) to replicate the classifier of Madnani et al. (2012) with our new feature. We tune the feature set for our classifier on the development data. 3 Semantic Parsing using Paraphrasing In this section we describe how the paraphrase algorithm is used for converting natural language to Freebase queries. Follow"
W16-6625,P14-5010,0,0.00325599,"r paraphrase generation (Quirk et al., 2004; Wubben et al., 2010). In particular, we use Moses (Koehn et al., 2007) to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions. MT 4.3 Implementation Details Entity Resolution For WebQuestions, we use 8 handcrafted part-of-speech patterns (e.g., the pattern (DT)?(JJ.?|NN.?){0,2}NN.? matches the noun phrase the big lebowski) to identify candidate named entity mention spans. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014). For each candidate mention span, we retrieve the top 10 entities according to the Freebase API.10 We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate n-best paraphrases that contains the entity mention spans. In the end, this process creates a total of 10n paraphrases. We generate ungrounded graphs for thes"
W16-6625,P05-1010,0,0.02558,"rove semantic parsing of questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clust"
W16-6625,P15-1126,0,0.0215356,"latent states for each nonterminals ensures that we retrieve the correct syntactic representation of the sentence. Here, however, we are more interested in the second latent states assigned to each nonterminals which capture the paraphrase information of the sentence at various levels. For example, we have a unary lexical rule (NN-*-142 day) indicating that we observe day with NN of the paraphrase type 142. We could use this information to extract unary rules of the form (NN-*-142 w) in the treebank that will generate 4 For other cases of separating syntax from semantics in a similar way, see Mitchell and Steedman (2015). 156 words w which are paraphrases to day. Similarly, any node WHNP-*-291 in the treebank will generate paraphrases for what day, SBARQ-*-403, for what day is nochebuena. This way we will be able to generate paraphrases when is nochebuena and when is nochebuena celebrated as they both have SBARQ-*-403 as their roots.5 To generate a word lattice Wq for a given question q, we parse q with the bi-layered grammar Glayered . For each rule of the form X-m1 -m2 → w in the bilayered tree with X ∈ P, m1 ∈ {1, . . . , 24}, m2 ∈ {1, . . . , 1000} and w a word in q, we extract rules of the form X-∗-m2 →"
W16-6625,D15-1214,1,0.808172,". Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. 153 Proceedings of The 9th International Natural Language Generation conference, pages 153–162, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics The main contributions of this paper are two fold. First, we present an algorithm (§2) to generate paraphrases using latent-variable PCFGs. We use the spectral method of Narayan and Cohen (2015) to estimate L-PCFGs on a large scale question treebank. Our grammar model leads to a robust and an efficient system for paraphrase generation in opendomain question answering. While CFGs have been explored for paraphrasing using bilingual parallel corpus (Ganitkevitch et al., 2013), ours is the first implementation of CFG that uses only monolingual data. Second, we show that generated paraphrases can be used to improve semantic parsing of questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline e"
W16-6625,P16-1146,1,0.817238,"baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clusters them into latent states. Then it follows with a maximum likelihood estimation step, that"
W16-6625,C12-1124,1,0.916187,"se, capturing contextual syntactic information about nonterminals. We refer the reader to Narayan and Cohen (2015) for more detailed description of these features. In our experiments, we set the number of latent states to 24. Once we estimate Gsyn from the Paralex corpus, we restrict it for each question to a grammar G0syn by keeping only the rules that could lead to a derivation over the lattice. This step is similar to lexical pruning in standard grammar-based generation process to avoid an intermediate derivation which can never lead to a successful derivation (Koller and Striegnitz, 2002; Narayan and Gardent, 2012). Paraphrase Sampling Sampling a question from the grammar G0syn is done by recursively sampling nodes in the derivation tree, together with their latent states, in a top-down breadth-first fashion. Sampling from the pruned grammar G0syn raises an issue of oversampling words that are more frequent in the training data. To lessen this problem, we follow a controlled sampling approach where sampling is guided by the word lattice Wq . Once a word w from a path e in Wq is sampled, all other parallel or conflicting paths to e are removed from Wq . For example, generating for the word lattice in Fig"
W16-6625,N03-1024,0,0.0794827,"million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An example word lattice for the question What la"
W16-6625,P06-1055,0,0.0144064,"questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clusters them into latent"
W16-6625,W05-1512,0,0.00962666,"n be used to improve semantic parsing of questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outsid"
W16-6625,W04-3219,0,0.273262,"estion paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An example word lattice for the question What language do people in C"
W16-6625,Q14-1030,1,0.410455,"ng baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical p"
W16-6625,P07-1059,0,0.0439532,"al of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. 153 Proceedings of The 9t"
W16-6625,C88-2128,0,0.670481,"e top-down sampling restricts the combinatorics inherent to bottom-up search (Shieber et al., 1990). Third, we do not restrict the generation by the order information in the input. The lack of order information in the input often raises the high combinatorics in lexicalist approaches (Kay, 1996). In our case, however, we use sampling to reduce this problem, and it allows us to produce syntactically diverse questions. And fourth, we impose no constraints on the grammar thereby making it easier to maintain bi-directional (recursive) grammars that can be used both for parsing and for generation (Shieber, 1988). 2.2 Bi-Layered L-PCFGs As mentioned earlier, one of our lattice types is based on bi-layered PCFGs introduced here. In their traditional use, the latent states in LPCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a SBARQ-33-403 SBARQ-30-403 SQ-8-925 WHNP-7-291 WP-7-254 NN-45-142 AUX-22-300 NN-41-854 what day is nochebuena SBARQ-24-403 WRB-42-707 SQ-8-709 WRB-42-707 when AUX-12-300 NN-41-85"
W16-6625,P15-1129,0,0.0272823,", 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does"
W16-6625,N06-1056,0,0.0309204,"phrase generation that does not require any sentence-aligned paraphrase corpus. Our key idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et a"
W16-6625,W10-4223,0,0.165589,"Missing"
W16-6625,P16-1220,1,0.837668,"lts on the test data. We get similar results on the test data as we reported on the development data. Again, the PPDB model performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang’s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use G RAPH PARSER as our semantic parsing 160 avg oracle F1 # oracle graphs avg F1 65.1 71.5 71.2 71.8 71.6 11.0 77.2 53.6 59.8 55.0 44.7 47.0 47.5 47.9 47.1 ORIGINAL MT NAIVE PPDB BILAYERED Table 1: Oracle statistics and results on the WebQuestions development set. Method avg P. Berant and Liang ’14 40.5 Bast and Haussmann ’15 49.8 Berant and Liang ’15 50.4 49.0 Reddy et al. ’16 Yih et al. ’1"
W16-6625,P15-1049,0,0.0148143,"odel performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang’s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use G RAPH PARSER as our semantic parsing 160 avg oracle F1 # oracle graphs avg F1 65.1 71.5 71.2 71.8 71.6 11.0 77.2 53.6 59.8 55.0 44.7 47.0 47.5 47.9 47.1 ORIGINAL MT NAIVE PPDB BILAYERED Table 1: Oracle statistics and results on the WebQuestions development set. Method avg P. Berant and Liang ’14 40.5 Bast and Haussmann ’15 49.8 Berant and Liang ’15 50.4 49.0 Reddy et al. ’16 Yih et al. ’15 52.8 Xu et al. ’16 53.1 This paper ORIGINAL 53.2 48.0 MT NAIVE 48.1 PPDB 48.4 BILAYERED 47.0 avg R. 46.6 60.4 55.7 61.1 60"
W16-6625,P15-1128,0,0.0431321,"2 shows our final results on the test data. We get similar results on the test data as we reported on the development data. Again, the PPDB model performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang’s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use G RAPH PARSER as our semantic parsing 160 avg oracle F1 # oracle graphs avg F1 65.1 71.5 71.2 71.8 71.6 11.0 77.2 53.6 59.8 55.0 44.7 47.0 47.5 47.9 47.1 ORIGINAL MT NAIVE PPDB BILAYERED Table 1: Oracle statistics and results on the WebQuestions development set. Method avg P. Berant and Liang ’14 40.5 Bast and Haussmann ’15 49.8 Berant and Liang ’15 50.4 49.0 Reddy et"
W16-6625,C98-1112,0,\N,Missing
W16-6625,P14-1091,0,\N,Missing
W16-6625,N15-3014,0,\N,Missing
W16-6625,P15-1026,0,\N,Missing
W17-1804,basile-etal-2012-developing,0,0.423653,"Malta borders no country’ has the UD graph shown in Figure 1(a). When compared to the correct representation given in Figure 1(c), the UDepLambda output shown in Figure 1(b) shows the absence of universal quantification, which in turn leads negation scope to be misrepresented. For this reason, we set the foundation of UDepLambda¬ (UDepLambda-not), an enhanced version of the original framework, whose type theory allows us to jointly handle negation and universal quantification. Moreover, unlike its predecessor, the logical forms are based on the one used in the ‘Groeningen Meaning Bank’ (GMB; (Basile et al., 2012a)), so to allow future comparison to a manually annotated semantic bank. Although the present work shows the conversion process for English, given that the edge labels are universal, our framework could be used to explore the problem of representing the scope of negation in the other 40+ languages universal dependencies are available in. This could also address the problem that all existing resources to represent negation scope as a logical form are limited to English (e.g. GMB and ‘DeepBank’ (Flickinger et al., 2012)) or only to a few other languages (e.g. ‘The Spanish Resource Grammar’ (Mar"
W17-1804,S12-1040,0,0.0947935,"Malta borders no country’ has the UD graph shown in Figure 1(a). When compared to the correct representation given in Figure 1(c), the UDepLambda output shown in Figure 1(b) shows the absence of universal quantification, which in turn leads negation scope to be misrepresented. For this reason, we set the foundation of UDepLambda¬ (UDepLambda-not), an enhanced version of the original framework, whose type theory allows us to jointly handle negation and universal quantification. Moreover, unlike its predecessor, the logical forms are based on the one used in the ‘Groeningen Meaning Bank’ (GMB; (Basile et al., 2012a)), so to allow future comparison to a manually annotated semantic bank. Although the present work shows the conversion process for English, given that the edge labels are universal, our framework could be used to explore the problem of representing the scope of negation in the other 40+ languages universal dependencies are available in. This could also address the problem that all existing resources to represent negation scope as a logical form are limited to English (e.g. GMB and ‘DeepBank’ (Flickinger et al., 2012)) or only to a few other languages (e.g. ‘The Spanish Resource Grammar’ (Mar"
W17-1804,P13-2017,0,0.0583409,"Missing"
W17-1804,morante-daelemans-2012-conandoyle,0,0.230024,"phenomena and be learned automatically, given the link to a manually annotated semantic bank. Related work Available resources that contain a representation of negation scope can be divided in two types: 1) those that represent negation as a FOL (or FOL-translatable) representation (e.g. GMB, ‘DeepBank’), where systems built using these resources are concerned with correctly representing FOL variables and predicates in the scope of negation; and 2) those that try to ground negation at a string-level, where both the negation operator and scope are defined as spans of text (Vincze et al., 2008; Morante and Daelemans, 2012). Systems trained on these resources are then concerned with detecting these spans of text. Resources in 1) are limited in that they are only available in English or for a small number of languages. Moreover no attempt has been made to connect them to more widely-used, cross-linguistic frameworks. On the other hand, grounding a semantic phenomenon to a string-level leads to inevitable simplification. For instance, the interaction between the negation operator and the universal quantifier (e.g. ‘Not every staff member is British’ vs. ‘None of the staff members are British’), along a formal repr"
W17-1804,P14-1007,0,0.0154036,"ator and the universal quantifier (e.g. ‘Not every staff member is British’ vs. ‘None of the staff members are British’), along a formal representation that would allow for inference operations is lost. Furthermore, each corpus is tailored to different applications, making annotation styles across corpora incompatible. Nonetheless these resources have been widely used in the field of Information Extraction and in particular in the Bio-Medical domain. Finally, it is also worth mentioning that there has been some attempts to use formal semantic representations to detect scope at a string level. Packard et al. (2014) used hand-crafted heuristics to traverse the MRS (Minimal Recursion Semantics) structures of negative sentences to then detect which words were in the scope of negation and which were not. Basile et al. (2012b) tried instead to first transform a DRS (Discourse Representation Structure) into graph form and then align this to strings. Whilst the MRS-based system outperformed previous work, mainly due to the fact that MRS structures are closely related to the surface realization, the DRT-based approach performed worse than most systems, mostly given to the fact that the formalism is not easily t"
W17-1804,Q16-1010,1,0.837752,"Missing"
W17-1804,D17-1009,1,0.884082,"Missing"
W17-1804,L16-1376,0,0.0174284,"ues. A FOL representation of the entire input graph can be then obtained by traversing the edges in a given order and combining their semantics. However, in its original formulation, UDeIntroduction Amongst the different challenges around the topic of negation, detecting and representing its scope is one that has been extensively researched in different sub-fields of NLP (e.g. Information Extraction (Velldal et al., 2012; Fancellu et al., 2016)). In particular, recent work have acknowledged the value of representing the scope of negation on top of existing linguistic resources (e.g. AMR – Bos (2016)). Manually annotating the scope of negation is however a time-consuming process, requiring annotators to have some expertise of formal semantics. Our solution to this problem is to automatically convert an available representation that captures negation into a framework that allows a rich variety of semantic phenomena to be represented, in22 Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR), pages 22–32, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics 2 pLambda does not handle either universal quantifiers or other scope phen"
W17-1804,J12-2005,0,0.122327,"edited minimally so to yield a more fine-grained description on the phenomena they describe, while lexical information is used only for a very restricted class of lexical items, such as negation cues. A FOL representation of the entire input graph can be then obtained by traversing the edges in a given order and combining their semantics. However, in its original formulation, UDeIntroduction Amongst the different challenges around the topic of negation, detecting and representing its scope is one that has been extensively researched in different sub-fields of NLP (e.g. Information Extraction (Velldal et al., 2012; Fancellu et al., 2016)). In particular, recent work have acknowledged the value of representing the scope of negation on top of existing linguistic resources (e.g. AMR – Bos (2016)). Manually annotating the scope of negation is however a time-consuming process, requiring annotators to have some expertise of formal semantics. Our solution to this problem is to automatically convert an available representation that captures negation into a framework that allows a rich variety of semantic phenomena to be represented, in22 Proceedings of the Workshop Computational Semantics Beyond Events and Rol"
W17-1804,W08-0606,0,0.229036,"Missing"
W17-1804,marimon-2010-spanish,0,\N,Missing
W17-4707,W07-0702,1,0.671821,"oposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurrent neural networks to learn representations that generate both words and CCG supertags, conditioned on the entire lexical and syntactic target history. ments, and also tense and morphological aspects of the word in a given context. Consider the sentence in Figure 1. This sentence contains two PP attachments and could lead to several disambiguation possibilities (“in” can attach to “Netanyahu” or “receives”, and “of” can attach to “capit"
W17-4707,J07-2003,0,0.0156397,"ining. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. Th"
W17-4707,W14-4012,0,0.028354,"Missing"
W17-4707,P17-2021,0,0.0761778,"ework to show source and target syntax provide complementary information. Applying more tightly coupled linguistic factors on the target for NMT has been previously investigated. Niehues et al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based model"
W17-4707,D14-1179,0,0.00956885,"Missing"
W17-4707,N16-1024,0,0.0146616,"al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurr"
W17-4707,P16-1231,0,0.0172413,"butions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselines, table 2 compares the scores obtained by ou"
W17-4707,P16-1078,0,0.0552534,"ntence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: Obama IOB: O CCG: NP Target-side receives O ((S[dcl]NP)/PP)/NP Net+ B NP an+ I NP yahu E NP in O PP/NP the O NP/N capital O N of O (NPNP)/NP USA"
W17-4707,D16-1025,0,0.0250595,"Missing"
W17-4707,N04-1035,0,0.0228682,"ht coupling of target words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned wi"
W17-4707,W16-2208,0,0.0269734,"Missing"
W17-4707,P02-1040,0,0.120603,"of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_885"
W17-4707,D13-1176,0,0.032968,"urther improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: O"
W17-4707,W05-0908,0,0.0147936,"lish and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_8859 5 72 guage pairs and use bootstrap resampling (Riezler and Maxwell, 2005) to test statistical significance. We compute BLEU with multi-bleu.perl over tokenized sentences both on the development sets, for early stopping, and on the test sets for evaluating our systems. Words are segmented into sub-units that are learned jointly for source and target using BPE (Sennrich et al., 2016b), resulting in a vocabulary size of 85,000. The vocabulary size for CCG supertags was 500. For the experiments with source-side features we use the BPE sub-units and the IOB tags as baseline features. We keep the total word embedding size fixed to 500 dimensions. We allocate 10 dimension"
W17-4707,Q15-1013,1,0.864408,"n the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntacti"
W17-4707,D15-1169,0,0.0163034,"DE→EN EN→DE RO→EN EN→RO1 Experimental Setup and Evaluation 4.1 dev 2,986 1,984 Table 1: Number of sentences in the training, development and test sets. We use EasySRL to label the English side of the parallel corpus with CCG supertags1 instead of using a corpus with gold annotations as in Luong et al. (2016). 4 train 4,468,314 605,885 Data and methods We train the neural MT systems on all the parallel data available at WMT16 (Bojar et al., 2016) for the German↔English and Romanian↔English language pairs. The English side of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016"
W17-4707,E17-3017,1,0.894752,"Missing"
W17-4707,W16-2209,1,0.896448,") show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of how best to incorporate this information. For language pairs where syntactic resources are available on both the source and target-side, we show that approaches to incorporate source syntax and target syntax are complementary. We propose a method for tightly coupling words and syntax by interleaving the target syntactic representation"
W17-4707,W07-0701,0,0.202057,"words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories,"
W17-4707,W16-2323,1,0.729608,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,W16-2204,1,0.813749,"signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word"
W17-4707,P16-1162,1,0.46286,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,R13-1079,1,0.793726,"1 and T2 . This results in two probability distributions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselin"
W17-4707,D16-1159,0,0.0475338,"Alexandra Birch1 1 School of Informatics, University of Edinburgh 2 Adam Mickiewicz University 3 Dep. of Computer Science, Johns Hopkins University {m.nadejde,siva.reddy, rico.sennrich, a.birch}@ed.ac.uk {t.dwojak,junczys}@amu.edu.pl, phi@jhu.edu Abstract tured by these models. In a detailed analysis, Bentivogli et al. (2016) show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of h"
W17-4707,W12-3150,1,0.846453,"erleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indi"
