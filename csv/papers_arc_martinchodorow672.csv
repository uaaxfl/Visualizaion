2021.scil-1.2,A Network Science Approach to Bilingual Code-switching,2021,-1,-1,3,0,2195,qihui xu,Proceedings of the Society for Computation in Linguistics 2021,0,None
2020.cmcl-1.9,Learning Pronoun Case from Distributional Cues: Flexible Frames for Case Acquisition,2020,-1,-1,2,0,21831,xiaomeng ma,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,0,"Case is an abstract grammatical feature that indicates argument relationship in a sentence. In English, cases are expressed on pronouns, as nominative case (e.g. I, he), accusative case (e.g. me, him) and genitive case (e.g. my, his). Children correctly use cased pronouns at a very young age. How do they acquire abstract case in the first place, when different cases are not associated with different meanings? This paper proposes that the distributional patterns in parents{'} input could be used to distinguish grammatical cases in English."
Q18-1007,Towards Evaluating Narrative Quality In Student Writing,2018,20,1,3,0,12223,swapna somasundaran,Transactions of the Association for Computational Linguistics,0,"This work lays the foundation for automated assessments of narrative quality in student writing. We first manually score essays for narrative-relevant traits and sub-traits, and measure inter-annotator agreement. We then explore linguistic features that are indicative of good narrative writing and use them to build an automated scoring system. Experiments show that our features are more effective in scoring specific aspects of narrative quality than a state-of-the-art feature set."
W15-0605,Automated Scoring of Picture-based Story Narration,2015,19,9,3,0.864542,12223,swapna somasundaran,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This work investigates linguistically motivated features for automatically scoring a spoken picture-based narration task. Specifically, we build scoring models with features for story development, language use and task relevance of the response. Results show that combinations of these features outperform a baseline system that uses state of the art speechbased features, and that best results are obtained by combining the linguistic and speech features."
W15-0619,Preliminary Experiments on Crowdsourced Evaluation of Feedback Granularity,2015,18,1,2,0.546529,16057,nitin madnani,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Providing writing feedback to English language learners (ELLs) helps them learn to write better, but it is not clear what type or how much information should be provided. There have been few experiments directly comparing the effects of different types of automatically generated feedback on ELL writing. Such studies are difficult to conduct because they require participation and commitment from actual students and their teachers, over extended periods of time, and in real classroom settings. In order to avoid such difficulties, we instead conduct a crowdsourced study on Amazon Mechanical Turk to answer questions concerning the effects of type and amount of writing feedback. We find that our experiment has several serious limitations but still yields some interesting results."
Y14-1063,Automatic Detection of Comma Splices,2014,31,2,3,0.199239,2821,john lee,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"In English text, independent clauses should be demarcated with full-stops (periods), or linked together with conjunctions. Non-native speakers are often prone to linking them improperly with commas instead of conjunctions, producing comma splices. This paper describes a method to detect comma splices using Conditional Random Fields (CRF), with features derived from parse tree patterns. In experiments, our model achieved an average of 0.91 precision and 0.28 recall in detecting comma splices, significantly outperforming both a baseline model using only local features and a widely used commercial grammar checker."
W14-4906,Finding your {``}Inner-Annotator{''}: An Experiment in Annotator Independence for Rating Discourse Coherence Quality in Essays,2014,10,0,3,0,30726,jill burstein,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"An experimental annotation method is described, showing promise for a subjective labeling task xe2x80x93 discourse coherence quality of essays. Annotators developed personal protocols, reducing front-end resources: protocol development and annotator training. Substantial inter-annotator agreement was achieved for a 4-point scale. Correlational analyses revealed how unique linguistic phenomena were considered in annotation. Systems trained with the annotator data demonstrated utility of the data."
W14-1801,Automated Measures of Specific Vocabulary Knowledge from Constructed Responses ({`}Use These Words to Write a Sentence Based on this Picture{'}),2014,38,4,2,0.864542,12223,swapna somasundaran,Proceedings of the Ninth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We describe a system for automatically scoring a vocabulary item type that asks test-takers to use two specific words in writing a sentence based on a picture. The system consists of a rule-based component and a machine learned statistical model which uses a variety of construct-relevant features. Specifically, in constructing the statistical model, we investigate if grammar, usage, and mechanics features developed for scoring essays can be applied to short answers, as in our task. We also explore new features reflecting the quality of the collocations in the response, as well as features measuring the consistency of the response to the picture. System accuracy in scoring is 15 percentage points greater than the majority class baseline and 10 percentage points less than human performance."
C14-1090,Lexical Chaining for Measuring Discourse Coherence Quality in Test-taker Essays,2014,41,25,3,0.864542,12223,swapna somasundaran,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper presents an investigation of lexical chaining (Morris and Hirst, 1991) for measuring discourse coherence quality in test-taker essays. We hypothesize that attributes of lexical chains, as well as interactions between lexical chains and explicit discourse elements, can be harnessed for representing coherence. Our experiments reveal that performance achieved by our new lexical chain features is better than that of previous discourse features used for this task, and that the best system performance is achieved when combining lexical chaining features with complementary discourse features, such as those provided by a discourse parser based on rhetorical structure theory, and features that reflect errors in grammar, word usage, and mechanics."
W13-1739,Detecting Missing Hyphens in Learner Text,2013,6,3,2,0.13977,4873,aoife cahill,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We present a method for automatically detecting missing hyphens in English text. Our method goes beyond a purely dictionary-based approach and also takes context into account. We evaluate our model on artificially generated data as well as naturally occurring learner text. Our best-performing model achieves high precision and reasonable recall, making it suitable for inclusion in a system that gives feedback to language learners."
W12-2005,Exploring Grammatical Error Correction with Not-So-Crummy Machine Translation,2012,14,15,3,1,16057,nitin madnani,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"To date, most work in grammatical error correction has focused on targeting specific error types. We present a probe study into whether we can use round-trip translations obtained from Google Translate via 8 different pivot languages for whole-sentence grammatical error correction. We develop a novel alignment algorithm for combining multiple round-trip translations into a lattice using the TERp machine translation metric. We further implement six different methods for extracting whole-sentence corrections from the lattice. Our preliminary experiments yield fairly satisfactory results but leave significant room for improvement. Most importantly, though, they make it clear the methods we propose have strong potential and require further study."
N12-1003,Identifying High-Level Organizational Elements in Argumentative Discourse,2012,10,34,4,1,16057,nitin madnani,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Argumentative discourse contains not only language expressing claims and evidence, but also language used to organize these claims and pieces of evidence. Differentiating between the two may be useful for many applications, such as those that focus on the content (e.g., relation extraction) of arguments and those that focus on the structure of arguments (e.g., automated essay scoring). We propose an automated approach to detecting high-level organizational elements in argumentative discourse that combines a rule-based system and a probabilistic sequence model in a principled manner. We present quantitative results on a dataset of human-annotated persuasive essays, and qualitative analyses of performance on essays and on political debates."
N12-1019,Re-examining Machine Translation Metrics for Paraphrase Identification,2012,22,111,3,1,16057,nitin madnani,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose to re-examine the hypothesis that automated metrics developed for MT evaluation can prove useful for paraphrase identification in light of the significant work on the development of new MT metrics over the last 4 years. We show that a meta-classifier trained using nothing but recent MT metrics outperforms all previous paraphrase identification approaches on the Microsoft Research Paraphrase corpus. In addition, we apply our system to a second corpus developed for the task of plagiarism detection and obtain extremely positive results. Finally, we conduct extensive error analysis and uncover the top systematic sources of error for a paraphrase identification approach relying solely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community."
N12-1029,"Correcting Comma Errors in Learner Essays, and Restoring Commas in Newswire Text",2012,29,7,3,0,38683,ross israel,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"While the field of grammatical error detection has progressed over the past few years, one area of particular difficulty for both native and non-native learners of English, comma placement, has been largely ignored. We present a system for comma error correction in English that achieves an average of 89% precision and 25% recall on two corpora of unedited student essays. This system also achieves state-of-the-art performance in the sister task of restoring commas in well-formed text. For both tasks, we show that the use of novel features which encode long-distance information improves upon the more lexically-driven features used in prior work."
C12-1038,Problems in Evaluating Grammatical Error Detection Systems,2012,45,31,1,1,2196,martin chodorow,Proceedings of {COLING} 2012,0,"Many evaluation issues for grammatical error detection have previously been overlooked, making it hard to draw meaningful comparisons between different approaches, even when they are evaluated on the same corpus. To begin with, the three-way contingency between a writerxe2x80x99s sentence, the annotatorxe2x80x99s correction, and the systemxe2x80x99s output makes evaluation more complex than in some other NLP tasks, which we address by presenting an intuitive evaluation scheme. Of particular importance to error detection is the skew of the data xe2x80x90 the low frequency of errors as compared to non-errors xe2x80x90 which distorts some traditional measures of performance and limits their usefulness, leading us to recommend the reporting of raw measurements (true positives, false negatives, false positives, true negatives). Other issues that are particularly vexing for error detection focus on defining these raw measurements: specifying the size or scope of an error, properly treating errors as graded rather than discrete phenomena, and counting non-errors. We discuss recommendations for best practices with regard to reporting the results of system evaluation for these cases, recommendations which depend upon making clear onexe2x80x99s assumptions and applications for error detection. By highlighting the problems with current error detection evaluation, the field will be better able to move forward."
C12-1158,"Native Tongues, Lost and Found: Resources and Empirical Evaluations in Native Language Identification",2012,23,0,4,0.838283,191,joel tetreault,Proceedings of {COLING} 2012,0,"In this paper we present work on the task of Native Language Identification (NLI). We present an alternative corpus to the ICLE which has been used in most work up until now. We believe that our corpus, TOEFL11, is more suitable for the task of NLI and will allow researchers to better compare systems and results. We show that many of the features that have been commonly used in this task generalize to new and larger corpora. In addition, we examine possible ways of increasing current system performance (e.g., additional features and feature combination methods), and achieve overall state-of-the-art results (accuracy of 90.1%) on the ICLE corpus using an ensemble classifier that includes previously examined features and a novel feature (n-gram language models). We also show that training on a large corpus and testing on a smaller one works well, but not vice versa. Finally, we show that system performance varies across proficiency scores."
W11-2111,{E}-rating Machine Translation,2011,26,17,4,0,43862,kristen parton,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We describe our submissions to the WMT11 shared MT evaluation task: MTeRater and MTeRater-Plus. Both are machine-learned metrics that use features from e-raterxc2xae, an automated essay scoring engine designed to assess writing proficiency. Despite using only features from e-rater and without comparing to translations, MTeRater achieves a sentence-level correlation with human rankings equivalent to BLEU. Since MTeRater only assesses fluency, we build a meta-metric, MTeRater-Plus, that incorporates adequacy by combining MTeRater with other MT evaluation metrics and heuristics. This meta-metric has a higher correlation with human rankings than either MTeRater or individual MT metrics alone. However, we also find that e-rater features may not have significant impact on correlation in every case."
P11-2089,They Can Help: Using Crowdsourcing to Improve the Evaluation of Grammatical Error Detection Systems,2011,24,24,2,1,16057,nitin madnani,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"Despite the rising interest in developing grammatical error detection systems for non-native speakers of English, progress in the field has been hampered by a lack of informative metrics and an inability to directly compare the performance of systems developed by different researchers. In this paper we address these problems by presenting two evaluation methodologies, both based on a novel use of crowdsourcing."
D11-1119,Exploiting Syntactic and Distributional Information for Spelling Correction with Web-Scale N-gram Models,2011,32,12,3,0,4068,wei xu,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,We propose a novel way of incorporating dependency parse and word co-occurrence information into a state-of-the-art web-scale n-gram model for spelling correction. The syntactic and distributional information provides extra evidence in addition to that provided by a web-scale n-gram corpus and especially helps with data sparsity problems. Experimental results show that introducing syntactic features into n-gram based models significantly reduces errors by up to 12.4% over the current state-of-the-art. The word co-occurrence information shows potential but only improves overall accuracy slightly.
W10-1006,Rethinking Grammatical Error Annotation and Evaluation with the {A}mazon {M}echanical {T}urk,2010,8,22,3,1,191,joel tetreault,Proceedings of the {NAACL} {HLT} 2010 Fifth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper we present results from two pilot studies which show that using the Amazon Mechanical Turk for preposition error annotation is as effective as using trained raters, but at a fraction of the time and cost. Based on these results, we propose a new evaluation method which makes it feasible to compare two error detection systems tested on different learner data sets."
P10-2065,Using Parse Features for Preposition Selection and Error Detection,2010,18,69,3,1,191,joel tetreault,Proceedings of the {ACL} 2010 Conference Short Papers,0,We evaluate the effect of adding parse features to a leading model of preposition usage. Results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an ESL error detection task. Analysis of the parser output indicates that it is robust enough in the face of noisy non-native writing to extract useful information.
W09-3010,Human Evaluation of Article and Noun Number Usage: Influences of Context and Construction Variability,2009,11,9,3,0.199239,2821,john lee,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,Evaluating systems that correct errors in non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number.
W08-1205,Native Judgments of Non-Native Usage: Experiments in Preposition Error Detection,2008,14,56,2,1,191,joel tetreault,Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics,0,"Evaluation and annotation are two of the greatest challenges in developing NLP instructional or diagnostic tools to mark grammar and usage errors in the writing of non-native speakers. Past approaches have commonly used only one rater to annotate a corpus of learner errors to compare to system output. In this paper, we show how using only one rater can skew system evaluation and then we present a sampling approach that makes it possible to evaluate a system more efficiently."
C08-1109,The Ups and Downs of Preposition Error Detection in {ESL} Writing,2008,19,137,2,1,191,joel tetreault,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper we describe a methodology for detecting preposition errors in the writing of non-native English speakers. Our system performs at 84% precision and close to 19% recall on a large set of student essays. In addition, we address the problem of annotation and evaluation in this domain by showing how current approaches of using only one rater can skew system evaluation. We present a sampling approach to circumvent some of the issues that complicate evaluation of error detection systems."
W07-1604,Detection of Grammatical Errors Involving Prepositions,2007,8,113,1,1,2196,martin chodorow,Proceedings of the Fourth {ACL}-{SIGSEM} Workshop on Prepositions,0,"This paper presents ongoing work on the detection of preposition errors of non-native speakers of English. Since prepositions account for a substantial proportion of all grammatical errors by ESL (English as a Second Language) learners, developing an NLP application that can reliably detect these types of errors will provide an invaluable learning resource to ESL students. To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays. Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3."
han-etal-2004-detecting,"Detecting Errors in {E}nglish Article Usage with a Maximum Entropy Classifier Trained on a Large, Diverse Corpus",2004,7,32,2,0,18861,narae han,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"One of the most difficult challenges faced by non-native speakers of English is mastering the system of English articles. We trained a maximum entropy classifier to select among a/an, the, or zero article for noun phrases, based on a set of features extracted from the local context of each. When the classifier was trained on 6 million noun phrases, its performance was correct about 88% of the time. We also used the classifier to detect article errors in the TOEFL essays of native speakers of Chinese, Japanese, and Russian. Agreement with human annotators was about 88% (kappa = 0.36). Many of the disagreements were due to the classifier s lack of discourse information. Performance rose to 94% agreement (kappa = 0.47) when the system accepted noun phrases as correct in cases where its own confidence was low."
P01-1014,Towards Automatic Classification of Discourse Elements in Essays,2001,17,52,4,1,30726,jill burstein,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"Educators are interested in essay evaluation systems that include feedback about writing features that can facilitate the essay revision process. For instance, if the thesis statement of a student's essay could be automatically identified, the student could then use this information to reflect on the thesis statement with regard to its quality, and its relationship to other discourse elements in the essay. Using a relatively small corpus of manually annotated data, we use Bayesian classification to identify thesis statements. This method yields results that are much closer to human performance than the results produced by two baseline systems."
A00-2019,An Unsupervised Method for Detecting Grammatical Errors,2000,13,121,1,1,2196,martin chodorow,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora. The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL). The errorrecognition system, ALEK, performs with about 80% precision and 20% recall."
W99-0411,Automated Essay Scoring for Nonnative {E}nglish Speakers,1999,12,70,2,1,30726,jill burstein,Computer Mediated Language Assessment and Evaluation in Natural Language Processing,0,"The e-rater systemxe2x84xa2 is an operational automated essay scoring system, developed at Educational Testing Service (ETS). The average agreement between human readers, and between independent human readers and e-rater is approximately 92%. There is much interest in the larger writing community in examining the system's performance on nonnative speaker essays. This paper focuses on results of a study that show e-rater's performance on Test of Written English (TWE) essay responses written by nonnative English speakers whose native language is Chinese, Arabic, or Spanish. In addition, one small sample of the data is from US-born English speakers, and another is from non-US-born candidates who report that their native language is English. As expected, significant differences were found among the scores of the English groups and the nonnative speakers. While there were also differences between e-rater and the human readers for the various language groups, the average agreement rate was as high as operational agreement. At least four of the five features that are included in e-rater's current operational models (including discourse, topical, and syntactic features) also appear in the TWE models. This suggests that the features generalize well over a wide range of linguistic variation, as e-rater was not confounded by non-standard English syntactic structures or stylistic discourse structures which one might expect to be a problem for a system designed to evaluate native speaker writing."
W98-0303,Enriching Automated Essay Scoring Using Discourse Marking,1998,-1,-1,5,1,30726,jill burstein,Discourse Relations and Discourse Markers,0,None
P98-1032,Automated Scoring Using A Hybrid Feature Identification Technique,1998,11,127,5,1,30726,jill burstein,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"This study exploits statistical redundancy inherent in natural language to automatically predict scores for essays. We use a hybrid feature identification method, including syntactic structure analysis, rhetorical structure analysis, and topical analysis, to score essay responses from test-takers of the Graduate Management Admissions Test (GMAT) and the Test of Written English (TWE). For each essay question, a stepwise linear regression analysis is run on a training set (sample of human scored essay responses) to extract a weighted set of predictive features for each test question. Score prediction for cross-validation sets is calculated from the set of predictive features. Exact or adjacent agreement between the Electronic Essay Rater (e-rater) score predictions and human rater scores ranged from 87% to 94% across the 15 test questions."
J98-1006,Using Corpus Statistics and {W}ord{N}et Relations for Sense Identification,1998,23,392,2,0,40176,claudia leacock,Computational Linguistics,0,"Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck. We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora. We describe a statistical classifier that combines topical context with local cues to identify a word sense. The classifier is used to disambiguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples."
C98-1032,Automated Scoring Using A Hybrid Feature Identification Technique,1998,11,127,5,1,30726,jill burstein,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"This study exploits statistical redundancy inherent in natural language to automatically predict scores for essays. We use a hybrid feature identification method, including syntactic structure analysis, rhetorical structure analysis, and topical analysis, to score essay responses from test-takers of the Graduate Management Admissions Test (GMAT) and the Test of Written English (TWE). For each essay question, a stepwise linear regression analysis is run on a training set (sample of human scored essay responses) to extract a weighted set of predictive features for each test question. Score prediction for cross-validation sets is calculated from the set of predictive features. Exact or adjacent agreement between the Electronic Essay Rater (e-rater) score predictions and human rater scores ranged from 87% to 94% across the 15 test questions."
H94-1046,Using a Semantic Concordance for Sense Identification,1994,7,153,2,0,50595,george miller,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"This paper proposes benchmarks for systems of automatic sense identification. A textual corpus in which open-class words had been tagged both syntactically and semantically was used to explore three statistical strategies for sense identification: a guessing heuristic, a most-frequent heuristic, and a co-occurrence heuristic. When no information about sense-frequencies was available, the guessing heuristic using the numbers of alternative senses in WordNet was correct 45% of the time. When statistics for sense-frequencies were derived from a semantic concordance, the assumption that each word is used in its most frequently occurring sense was correct 69% of the time; when that figure was calculated for polysemous words alone, it dropped to 58%. And when a co-occurrence heuristic took advantage of prior occurrences of words together in the same sentences, little improvement was observed. The semantic concordance is still too small to estimate the potential limits of a co-occurrence heuristic."
C92-4177,Degrees of Stativity: The Lexical Representation of Verb Aspect,1992,13,21,2,0,8328,judith klavans,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"L'acquisition automatique de connaissance lexicale a partir de larges corpus s'est essentiellement occupee des phenomenes de co-occurrence, aux depens des traits lexicaux inherents. Nous presentons ici une methodologie qui permet d'obtenir l'information semantique sur l'aspect du verbe en analysant automatiquement un corpus et en appliquant des tests linguistiques a l'aide d'une serie d'outils d'analyse structurale. Lorsque ces deux txc3xa2ches sont accomplies, nous proposons une representation de l'aspect du verbe qui associe une valeur de mesure pour les differents types d'evenements. Les mesures refletent l'usage typique du verbe, et par consequent une mesure de resistance ou de non-resistance a la coercion dans le contexte de la phrase. Les resultats que nous rapportons ici ont ete obtenus de deux manieres: en extrayant l'information necessaire a partir du corpus etiquete de Francis and Kucera (1982), et en faisant tourner un analyseur syntaxique (McCord 1980, 1990) sur le corpus du Reader's Digest afin d'extraire une information plus precise sur l'usage du verbe dans le texte."
A88-1020,A Tool for Investigating the Synonymy Relation in a Sense Disambiguated Thesaurus,1988,9,18,1,1,2196,martin chodorow,Second Conference on Applied Natural Language Processing,0,This paper describes an exploration of the implicit synonymy relationship expressed by synonym lists in an on-line thesaurus. A series of automatic steps was taken to properly constrain this relationship. The resulting groupings of semantically related word senses are believed to constitute a useful tool for natural language processing and for work in lexicography.
J87-3003,Tools and Methods for Computational Linguistics,1987,21,98,3,0.952381,52731,roy byrd,Computational Linguistics,0,"This paper presents a set of tools and methods for acquiring, manipulating, and analyzing machine-readable dictionaries. We give several detailed examples of the use of these tools and methods for particular analyses. A novel aspect of our work is that it allows the combined processing of multiple machine-readable dictionaries. Our examples describe analyses of data from Webster's Seventh Collegiate Dictionary, the Longman Dictionary of Contemporary English, the Collins bilingual dictionaries, the Collins Thesaurus, and the Zingarelli Italian dictionary. We describe existing facilities and results they have produced as well as planned enhancements to those facilities, particularly in the area of managing associations involving the senses of polysemous words. We show how these enhancements expand the ways in which we can exploit machine-readable dictionaries in the construction of large lexicons for natural language processing systems."
P85-1034,Using an On-Line Dictionary to Find Rhyming Words and Pronunciations for Unknown Words,1985,3,11,2,0,52731,roy byrd,23rd Annual Meeting of the Association for Computational Linguistics,1,"Humans know a great deal about relationships among words. This paper discusses relationships among word pronunciations. We describe a computer system which models human judgement of rhyme by assigning specific roles to the location of primary stress, the similarity of phonetic segments, and other factors. By using the model as an experimental tool, we expect to improve our understanding of rhyme. A related computer model will attempt to generate pronunciations for unknown words by analogy with those for known words. The analogical processes involve techniques for segmenting and matching word spellings, and for mapping spelling to sound in known words. As in the case of rhyme, the computer model will be an important tool for improving our understanding of these processes. Both models serve as the basis for functions in the WordSmith automated dictionary system."
P85-1037,Extracting Semantic Hierarchies From a Large On-Line Dictionary,1985,4,202,1,1,2196,martin chodorow,23rd Annual Meeting of the Association for Computational Linguistics,1,"Dictionaries are rich sources of detailed semantic information, but in order to use the information for natural language processing, it must be organized systematically. This paper describes automatic and semi-automatic procedures for extracting and organizing semantic feature information implicit in dictionary definitions. Two head-finding heuristics are described for locating the genus terms in noun and verb definitions. The assumption is that the genus term represents inherent features of the word it defines. The two heuristics have been used to process definitions of 40,000 nouns and 8,000 verbs, producing indexes in which each genus term is associated with the words it defined. The Sprout program interactively grows a taxonomic tree from any specified root feature by consulting the genus index. Its output is a tree in which all of the nodes have the root feature for at least one of their senses. The Filter program uses an inverted form of the genus index. Filtering begins with an initial filter file consisting of words that have a given feature (e.g. [human]) in all of their senses. The program then locates, in the index, words whose genus terms all appear in the filter file. The output is a list of new words that have the given feature in all of their senses."
