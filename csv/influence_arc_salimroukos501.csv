2004.iwslt-evaluation.6,P96-1019,1,0.726053,"Missing"
2004.iwslt-evaluation.6,P03-1051,1,0.881182,"Missing"
2004.iwslt-evaluation.6,N04-1033,0,0.0525923,"Missing"
2004.iwslt-evaluation.6,J00-3003,0,0.162201,"Missing"
2004.iwslt-evaluation.6,C04-1073,0,0.030973,"Missing"
2004.iwslt-evaluation.6,J93-2003,0,0.00865176,"Missing"
2004.iwslt-evaluation.6,W99-0604,0,0.283371,"Missing"
2004.iwslt-evaluation.6,P01-1067,0,0.158769,"Missing"
2004.iwslt-evaluation.6,W02-1018,0,0.0302735,"Missing"
2004.iwslt-evaluation.6,W03-1001,0,0.324213,"efines a block alignment link a. The block consisting of the target and source words at the link positions is denoted as b. Target and source words in a block are subject to the contiguity condition. Extension Algorithm: We expand the alignment links to include alignment points in the neighborhood of the high precision alignment AP and lie within the high recall alignment AR. The extensions are carried out iteratively until no new alignment links from AR are added. Among the candidate blocks obtained according to 2. Baseline System Overview Our baseline phrase translation system described in [Tillmann 2003] consists of three major components: word alignment, block selection, and decoding. 39 the projection and extension algorithm, blocks satisfying the following three conditions are kept for use in translation: i. Source phrase ( f ) length ≤ 10 morphemes1 ii. Target phrase ( e ) length ≤ 10 morphemes iii. Block (b) frequency &gt; 1 3. Performance Enhancing Techniques Performance evaluations are carried out on the C-STAR 2003 development test data consisting of 506 segments for both Japanese-to-English (J2E hereafter) and Chinese-to-English (C2E hereafter) translations. BLEU [9] has been used for"
2004.iwslt-evaluation.6,N04-4015,1,0.849653,"Missing"
2004.iwslt-evaluation.6,C96-2141,0,0.349351,"Missing"
2004.iwslt-evaluation.6,P02-1040,1,0.101383,"Missing"
2004.iwslt-evaluation.6,N03-2021,0,\N,Missing
2004.iwslt-evaluation.6,N03-1017,0,\N,Missing
2004.iwslt-evaluation.6,J03-1005,0,\N,Missing
2020.acl-main.117,N19-1423,0,0.136108,"), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Koˇcisk´y et al., 2018) and H OTPOT QA. A common problem of the earlier MRC datasets is observation bias: annotators first read a paragraph and then wrote appropriate questions and answers, which, as a result, have substantial lexical overlap with the paragraph. Also, systems trained on SQuAD 1.1 could be easily fooled by the insertion of distractor sentences that should not change the answer, as shown in (Jia and Liang, 2017). Based on these considerations, SQuAD 2.0 added “unanswerable” questions. However, large pretrained language models (Devlin et al., 2019; Liu et al., 2019) were able to achieve super-human performance in less than a year on SQuAD 2.0 as well; this suggests that the evidence needed to correctly identify unanswerable questions also are present as specific patterns in the paragraphs. Recently, the NQ dataset has been introduced which overcomes the above problems and constitutes a much harder and realistic benchmark. The questions came from a commercial search engine and were asked by humans who had actual information needs. The answers were manually extracted from a Wikipedia page which the user may have selected among the search"
2020.acl-main.117,D17-1215,0,0.0214012,"on 6. 2 Related Work Recent notable datasets for Machine Reading Comprehension (henceforth, MRC) include SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Koˇcisk´y et al., 2018) and H OTPOT QA. A common problem of the earlier MRC datasets is observation bias: annotators first read a paragraph and then wrote appropriate questions and answers, which, as a result, have substantial lexical overlap with the paragraph. Also, systems trained on SQuAD 1.1 could be easily fooled by the insertion of distractor sentences that should not change the answer, as shown in (Jia and Liang, 2017). Based on these considerations, SQuAD 2.0 added “unanswerable” questions. However, large pretrained language models (Devlin et al., 2019; Liu et al., 2019) were able to achieve super-human performance in less than a year on SQuAD 2.0 as well; this suggests that the evidence needed to correctly identify unanswerable questions also are present as specific patterns in the paragraphs. Recently, the NQ dataset has been introduced which overcomes the above problems and constitutes a much harder and realistic benchmark. The questions came from a commercial search engine and were asked by humans who"
2020.acl-main.117,Q18-1023,0,0.0458961,"Missing"
2020.acl-main.117,Q19-1026,0,0.0194103,"uninstall all products including Install Manager (IM) then reinstall IM and Data Studio 4.1.2. Figure 1: Examples of questions in the TechQA dataset. We briefly review related work in Section 2; we then describe the process of collecting the data for T ECH QA in Section 3, where we detail the automatic filtering, human filtering, annotation guidelines, and annotation procedure. We present statistics of the dataset in Section 4, introduce the associated leaderboard task in Section 5 and present baseline results obtained by fine-tuning MRC systems built for Natural Questions (hence-forth, NQ) (Kwiatkowski et al., 2019) and H OTPOT QA (Yang et al., 2018) in Section 6. 2 Related Work Recent notable datasets for Machine Reading Comprehension (henceforth, MRC) include SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Koˇcisk´y et al., 2018) and H OTPOT QA. A common problem of the earlier MRC datasets is observation bias: annotators first read a paragraph and then wrote appropriate questions and answers, which, as a result, have substantial lexical overlap with the paragraph. Also, systems trained on SQuAD 1.1 could be easily fooled by the insertion of distractor sentences that"
2020.acl-main.117,2021.ccl-1.108,0,0.135289,"Missing"
2020.acl-main.117,P18-2124,0,0.105149,"Missing"
2020.acl-main.117,D16-1264,0,0.100497,"Section 2; we then describe the process of collecting the data for T ECH QA in Section 3, where we detail the automatic filtering, human filtering, annotation guidelines, and annotation procedure. We present statistics of the dataset in Section 4, introduce the associated leaderboard task in Section 5 and present baseline results obtained by fine-tuning MRC systems built for Natural Questions (hence-forth, NQ) (Kwiatkowski et al., 2019) and H OTPOT QA (Yang et al., 2018) in Section 6. 2 Related Work Recent notable datasets for Machine Reading Comprehension (henceforth, MRC) include SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Koˇcisk´y et al., 2018) and H OTPOT QA. A common problem of the earlier MRC datasets is observation bias: annotators first read a paragraph and then wrote appropriate questions and answers, which, as a result, have substantial lexical overlap with the paragraph. Also, systems trained on SQuAD 1.1 could be easily fooled by the insertion of distractor sentences that should not change the answer, as shown in (Jia and Liang, 2017). Based on these considerations, SQuAD 2.0 added “unanswerable” questions. However, large pretrained language models (D"
2020.acl-main.117,K17-1029,0,0.0179664,"rum by technical users who 1270 Questions Total retrieved With Accepted Answers With link to Technote in Accepted Answer Count 276,968 57,990 15,918 Table 1: Statistics of questions from the forums. The questions with a Technote link in the Accepted Answer were manually annotated by our annotators. had a specific information need, and answers from technical documents mentioned in the ”Accepted Answer” to the post. In Section 4 we will contrast structural properties of T ECH QA to those of some of the datasets mentioned here. Datasets for specialized domain require effective domain adaptation (Wiese et al., 2017), because they contain a much smaller number of labeled examples than open-domain datasets like (Bajaj et al., 2016). Having a limited number of quality labeled examples is a real-world situation: domain experts are much more expensive than crowd-sourcing participants. 3 T ECH QA Dataset Collection The questions for the T ECH QA dataset were posed by real users on public forums maintained and hosted by IBM at the developer.ibm.com answers2 and IBM developerworks3 sites. The questions are related to products running in environments supported by IBM and mostly fall into three categories: i) gene"
2020.acl-main.117,D18-1259,0,0.0667405,"Missing"
2020.acl-main.167,P19-1620,0,0.0402586,"time, we provide the AMR as context as in conventional conditional text generation: w ˆj = arg max{pGPT-2 (wj |w1:j−1 , a1:M )} wj 3 Re-scoring via Cycle Consistency The general idea of cycle consistency is to assess the quality of a system’s output based on how well an external ‘reverse’ system can reconstruct the input from it. In previous works, cycle-consistency based losses have been used as part of the training objective in machine translation (He et al., 2016) and speech recognition (Hori et al., 2019). It has also been used for filtering synthetic training data for question answering (Alberti et al., 2019). Here we propose the use of a cycle consistency measure to re-score the system outputs. In particular, we take the top k sentences generated by our system from each gold AMR graph and parse them using an off-the-shelf parser to obtain a second AMR graph. We then re-score each sentence using the standard AMR parsing metric Smatch (Cai and Knight, 2013) by comparing the gold and parsed AMRs. 4 Experimental setup Following Previous works on AMR-to-text, we Use the standard LDC2017T10 AMR corpus for evaluation of the proposed model. This Corpus contains 36,521 training instances of AMR graphs in"
2020.acl-main.167,W13-2322,0,0.0845262,"graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Grap"
2020.acl-main.167,W05-0909,0,0.0296777,"ent with greedy decoding, beam search, and nucleus sampling (Holtzman et al., 2019). For beam search, we explore beam sizes of 5, 10 and 15. As the system, in some cases, produces repetitive output at the end of the text, we additionally perform a post-processing step to remove these occurrences. Metrics. We considered the three automatic evaluation metrics commonly used in previous works. We compute BLEU (Papineni et al., 2002) using SacreBLEU (Ma et al., 2019). We compute chrF++ (Popovi´c, 2017) using both SacreBLEU and the scripts used by authors of the baseline systems. We compute METEOR (Banerjee and Lavie, 2005) with the default values for English of the CMU implementation.2 In addition to the standard automatic metrics, we also carry out human evaluation experiments and use the semantic similarity metric BERTScore (Zhang et al., 2020). Both metrics arguably have less dependency on the surface symbols of the reference text used for evaluation. This is particularly relevant for the AMR-to-text task, since one single AMR graph corresponds to multiple sentences with the same semantic meaning. Conventional metrics for AMR-to-text are are strongly influenced by surface symbols and thus do not capture well"
2020.acl-main.167,P18-1026,0,0.134477,"our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying"
2020.acl-main.167,2020.acl-main.640,0,0.0938116,"Missing"
2020.acl-main.167,P13-2131,0,0.0643467,"le-consistency based losses have been used as part of the training objective in machine translation (He et al., 2016) and speech recognition (Hori et al., 2019). It has also been used for filtering synthetic training data for question answering (Alberti et al., 2019). Here we propose the use of a cycle consistency measure to re-score the system outputs. In particular, we take the top k sentences generated by our system from each gold AMR graph and parse them using an off-the-shelf parser to obtain a second AMR graph. We then re-score each sentence using the standard AMR parsing metric Smatch (Cai and Knight, 2013) by comparing the gold and parsed AMRs. 4 Experimental setup Following Previous works on AMR-to-text, we Use the standard LDC2017T10 AMR corpus for evaluation of the proposed model. This Corpus contains 36,521 training instances of AMR graphs in PENMAN notation and the corresponding texts. It also includes 1368 and 1371 development and test instances, respectively. We tokenize each input text using The JAMR toolkit (Flanigan et al., 2014). The concatenation of an AMR graph and the corresponding text is split into words, special symbols and sub-word units using the GPT-2 tokenizer. We add all a"
2020.acl-main.167,N19-1223,0,0.148268,"n evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (20"
2020.acl-main.167,N19-1366,0,0.129568,"ted, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al."
2020.acl-main.167,N19-1423,0,0.0327542,"se graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfer learning to yield powerful language models that considerably outperform the prior art. They have also shown great success when fine-tuned to particular text generation tasks (See et al., 2019; Zhang et al., 2019; Keskar et al., 2019). Given their success, it would be desirable to apply pre-trained transformer models to a graph-to-text task like AMR-to-text, but the need for graph encoding precludes in principle that option. Feeding the network with some sequential representation of the graph, such as a topological sorting, looses some of the graphs represen"
2020.acl-main.167,N16-1087,0,0.0518772,"orm all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research"
2020.acl-main.167,P14-1134,0,0.165504,"graph and parse them using an off-the-shelf parser to obtain a second AMR graph. We then re-score each sentence using the standard AMR parsing metric Smatch (Cai and Knight, 2013) by comparing the gold and parsed AMRs. 4 Experimental setup Following Previous works on AMR-to-text, we Use the standard LDC2017T10 AMR corpus for evaluation of the proposed model. This Corpus contains 36,521 training instances of AMR graphs in PENMAN notation and the corresponding texts. It also includes 1368 and 1371 development and test instances, respectively. We tokenize each input text using The JAMR toolkit (Flanigan et al., 2014). The concatenation of an AMR graph and the corresponding text is split into words, special symbols and sub-word units using the GPT-2 tokenizer. We add all arc labels seen in the training set and the root node :root to the vocabulary of the GPT-2model, but we freeze the embedding layer for training. We use the Hugging Face implementation of (Wolf et al., 2019) for GPT-2 small (GPT-2S), medium (GPT-2M) and large (GPT-2L). Fine-tuning converges after 6 epochs, which takes just a few hours on a V100 GPU1 . For cycle-consistency re-scoring we use an implementation of Naseem et al. (2019) in PyTor"
2020.acl-main.167,Q19-1019,0,0.103533,"at to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer represe"
2020.acl-main.167,P17-1014,0,0.156544,"etrics, we provide human evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh,"
2020.acl-main.167,W19-5302,0,0.0214036,":ARG1 it :manner vigorous Penman (r / recommend-01 :ARG1 (a / advocate-01 :ARG1 (i / it) :manner (v / vigorous))) Decoding. For generation, we experiment with greedy decoding, beam search, and nucleus sampling (Holtzman et al., 2019). For beam search, we explore beam sizes of 5, 10 and 15. As the system, in some cases, produces repetitive output at the end of the text, we additionally perform a post-processing step to remove these occurrences. Metrics. We considered the three automatic evaluation metrics commonly used in previous works. We compute BLEU (Papineni et al., 2002) using SacreBLEU (Ma et al., 2019). We compute chrF++ (Popovi´c, 2017) using both SacreBLEU and the scripts used by authors of the baseline systems. We compute METEOR (Banerjee and Lavie, 2005) with the default values for English of the CMU implementation.2 In addition to the standard automatic metrics, we also carry out human evaluation experiments and use the semantic similarity metric BERTScore (Zhang et al., 2020). Both metrics arguably have less dependency on the surface symbols of the reference text used for evaluation. This is particularly relevant for the AMR-to-text task, since one single AMR graph corresponds to mult"
2020.acl-main.167,P19-1451,1,0.829397,"olkit (Flanigan et al., 2014). The concatenation of an AMR graph and the corresponding text is split into words, special symbols and sub-word units using the GPT-2 tokenizer. We add all arc labels seen in the training set and the root node :root to the vocabulary of the GPT-2model, but we freeze the embedding layer for training. We use the Hugging Face implementation of (Wolf et al., 2019) for GPT-2 small (GPT-2S), medium (GPT-2M) and large (GPT-2L). Fine-tuning converges after 6 epochs, which takes just a few hours on a V100 GPU1 . For cycle-consistency re-scoring we use an implementation of Naseem et al. (2019) in PyTorch. For re-scoring experiments, we use a beam size of 15. AMR input representation. we test three variants of AMR representation. First, a depth-first search (DFS) through the graph following Konstas et al. (2017), where the input sequence is the path followed in the graph. Second, to see if GPT-2 is in fact learning from the graph structure, we remove all the edges from the DFS, keeping only the concept nodes. This has the effect of removing the relation information between concepts, such as subject/object relations. As a third option, we use the PENMAN representation without any mod"
2020.acl-main.167,P02-1040,1,0.124304,"igorous DFS recommend :ARG1 advocate-01 :ARG1 it :manner vigorous Penman (r / recommend-01 :ARG1 (a / advocate-01 :ARG1 (i / it) :manner (v / vigorous))) Decoding. For generation, we experiment with greedy decoding, beam search, and nucleus sampling (Holtzman et al., 2019). For beam search, we explore beam sizes of 5, 10 and 15. As the system, in some cases, produces repetitive output at the end of the text, we additionally perform a post-processing step to remove these occurrences. Metrics. We considered the three automatic evaluation metrics commonly used in previous works. We compute BLEU (Papineni et al., 2002) using SacreBLEU (Ma et al., 2019). We compute chrF++ (Popovi´c, 2017) using both SacreBLEU and the scripts used by authors of the baseline systems. We compute METEOR (Banerjee and Lavie, 2005) with the default values for English of the CMU implementation.2 In addition to the standard automatic metrics, we also carry out human evaluation experiments and use the semantic similarity metric BERTScore (Zhang et al., 2020). Both metrics arguably have less dependency on the surface symbols of the reference text used for evaluation. This is particularly relevant for the AMR-to-text task, since one si"
2020.acl-main.167,W17-4770,0,0.0426759,"Missing"
2020.acl-main.167,W16-6603,0,0.201065,"set, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transfor"
2020.acl-main.167,D19-1548,0,0.705729,"d Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfer learning to yield powerful language models that considerably outperform the prior art. They have also shown great success when fine-tuned to particular text generation tasks (See et al., 2019; Zhang et al., 2019; Keskar et al., 2019). Given th"
2020.acl-main.167,D19-1314,0,0.229439,"an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfe"
2020.acl-main.167,K19-1079,0,0.131093,"pplied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfer learning to yield powerful language models that considerably outperform the prior art. They have also shown great success when fine-tuned to particular text generation tasks (See et al., 2019; Zhang et al., 2019; Keskar et al., 2019). Given their success, it would be desirable to apply pre-trained transformer models to a graph-to-text task like AMR-to-text, but the need for graph encoding precludes in principle that option. Feeding the network with some sequential representation of the graph, such as a topological sorting, looses some of the graphs representational power. Complex graph annotations, such as AMR, also contain many special symbols and special constructs that departure from natural language and may by not interpretable by a pretrained language model. In this paper we"
2020.acl-main.167,P18-1150,0,0.0585758,"on (AMR) (Banarescu et al., 2013) is a rooted, directed, acyclic graph with labeled edges (relations) and nodes (concepts) expressing “who is doing what to whom”. AMR-to-text generates sentences representing the semantics underlying an AMR graph. Initial works in AMR-to-text used transducers (Flanigan et al., 2016), phrase-based machine translation (Pourdamghani et al., 2016) and neural sequence-to-sequence (seq2seq) models with linearized graphs (Konstas et al., 2017). Cao and Clark (2019) leverage constituency parsing for generation. Beck et al. (2018) improve upon prior RNN graph encoding (Song et al., 2018) with Levi Graph Transformations. Damonte and Cohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using t"
2020.acl-main.167,2020.tacl-1.2,0,0.103704,"ohen (2019) compare multiple representations and find graph encoders to be the best. Guo et al. (2019) use RNN graph encoders with dense graph convolutional encoding. Ribeiro et al. (2019) ∗ This research was done during an internship at IBM Research AI. use RNN encoders with dual graph representations. Transformer-based seq2seq (Vaswani et al., 2017) was first applied to AMR-to-text in (Sinh and Le Minh, 2019). Zhu et al. (2019) greatly improve over the prior state-of-the-art by modifying self-attention to account for AMR graph structure. Using transformers has also been recently explored by Wang et al. (2020) who propose a mutli-head graph attention mechanism. Pre-trained transformer representations (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019) use transfer learning to yield powerful language models that considerably outperform the prior art. They have also shown great success when fine-tuned to particular text generation tasks (See et al., 2019; Zhang et al., 2019; Keskar et al., 2019). Given their success, it would be desirable to apply pre-trained transformer models to a graph-to-text task like AMR-to-text, but the need for graph encoding precludes in principle that option."
2020.coling-demos.8,D19-3006,1,0.588997,"C) demo which is able to answer questions in over 100 languages. M-GAAMA answers questions from a given passage in the same or a different language. It incorporates several existing multilingual models that can be used interchangeably in the demo such as M-BERT and XLM-R. The M-GAAMA demo also improves language accessibility by incorporating the IBM Watson machine translation widget to provide additional capabilities to the user to see an answer in their desired language. We also show how M-GAAMA can be used in downstream tasks by incorporating it into an E ND - TO -E ND -QA system using CFO (Chakravarti et al., 2019). We experiment with our system architecture on the Multi-Lingual Question Answering (MLQA) and the CORD-19 COVID (Wang et al., 2020; Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Model"
2020.coling-demos.8,2020.tacl-1.30,0,0.0177366,"ncluding question answering (Lewis et al., 2019) (Conneau et al., 2019). We train our underlying MRC system with these pre-trained language models and achieve results that are consistently as strong as prior work. Many datasets for English MRC have been introduced with annotated Wikipedia documents including (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Yang et al., 2018; Kwiatkowski et al., 2019). Fewer resources are available for the cross-lingual setting. The MLQA (Lewis et al., 2019) dataset contains parallel instances in 7 languages where the context is found in Wikipedia. The TyDiQA (Clark et al., 2020) dataset containes instances in 11 languages. However, TyDiQA is not parallel and it only has instances where the question and context are in the same language. 42 Quelle est la période d’incubation du virus? What is the incubation period of the virus? For an individual case with exposure lying between 1 E and 2 E , the likelihood function for an incubation observation was 12 (
Commission of China, reporting an incubation time -------------------------------. Statistical estimation of the distribution of ??????? 1 ??? 14 ???? s incubation periods has been done in two other studies. ENGLISH Pa"
2020.coling-demos.8,P19-4007,0,0.0279245,"Missing"
2020.coling-demos.8,N19-1423,0,0.491563,"ur system architecture on the Multi-Lingual Question Answering (MLQA) and the CORD-19 COVID (Wang et al., 2020; Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). We achieve the 2nd rank1 on the Google Natural Questions (Kwiatkowski et al., 2019) leaderboard2 . In this paper, we expand our approach by introducing new multilingual capabilities using models such as Multilingual-BERT (M-BERT) (Devlin et al., 2019) and XLM-R (Conneau et al., 2019). This addition has the capability of transcending language boundaries to 104 languages. Figure 1 shows examples of QA pairs from the MLQA dataset (Lewis et al., 2019). To the best of our knowledge, this is the first published demo of a Multi-Lingual QA system. We achieve this by in"
2020.coling-demos.8,Q19-1026,0,0.0507249,"Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). We achieve the 2nd rank1 on the Google Natural Questions (Kwiatkowski et al., 2019) leaderboard2 . In this paper, we expand our approach by introducing new multilingual capabilities using models such as Multilingual-BERT (M-BERT) (Devlin et al., 2019) and XLM-R (Conneau et al., 2019). This addition has the capability of transcending language boundaries to 104 languages. Figure 1 shows examples of QA pairs from the MLQA dataset (Lewis et al., 2019). To the best of our knowledge, this is the first published demo of a Multi-Lingual QA system. We achieve this by introducing a novel multilingual component to our QA GAAMA (Go Ahead, Ask Me Anything) (Chakravarti et al., 2019) pipe"
2020.coling-demos.8,P12-3005,0,0.0336218,"omponents together using the ReactJS framework5 . Providing M-GAAMA as a gRPC server allows it to be quite flexible. This enables it to seamlessly transition between being a standalone system and integrating with larger systems. We show this via the downstream E ND - TO -E ND -QA task described below. E ND - TO -E ND -QA builds upon M-GAAMA, with a full IR-MRC pipeline. Information Retrieval is obtained using an Elasticsearch index6 for each language7 . The user can ask a question in any language for which an index exists. The language of the question is identified using the ‘langid’ toolkit (Lui and Baldwin, 2012) to determine the appropriate index. The appropriate index is then searched for documents in the target language. These documents are then evaluated together with the user’s question 4 https://www.ibm.com/watson/services/language-translator/ https://reactjs.org/ 6 https://hub.docker.com/_/elasticsearch/ 7 In our implementation we built an index in English and Spanish as a proof of concept. 5 43 F1 ROBERTAL M-BERT XLM-RB XLM-RL en 84.4 80.4 80.1 83.9 es 66.7 67.6 74.0 de 61.3 63.0 69.9 ar 51.9 56.3 66.3 MLQA hi vi 50.7 61.6 61.1 66.2 71.2 74.0 zh 60.2 61.6 69.9 XLT 61.8 65.1 72.7 G-XLT 52.1 41."
2020.coling-demos.8,D16-1264,0,0.501214,"er (Vaswani et al., 2017) based masked language model on text in multiple languages. The use of pretrained multilingual language models such as M-BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLM-R (Conneau et al., 2019) achieve the previous SOTA on cross-lingual tasks including question answering (Lewis et al., 2019) (Conneau et al., 2019). We train our underlying MRC system with these pre-trained language models and achieve results that are consistently as strong as prior work. Many datasets for English MRC have been introduced with annotated Wikipedia documents including (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Yang et al., 2018; Kwiatkowski et al., 2019). Fewer resources are available for the cross-lingual setting. The MLQA (Lewis et al., 2019) dataset contains parallel instances in 7 languages where the context is found in Wikipedia. The TyDiQA (Clark et al., 2020) dataset containes instances in 11 languages. However, TyDiQA is not parallel and it only has instances where the question and context are in the same language. 42 Quelle est la période d’incubation du virus? What is the incubation period of the virus? For an individual case with exposure lying between 1 E and 2"
2020.coling-demos.8,P18-2124,0,0.173969,"widget to provide additional capabilities to the user to see an answer in their desired language. We also show how M-GAAMA can be used in downstream tasks by incorporating it into an E ND - TO -E ND -QA system using CFO (Chakravarti et al., 2019). We experiment with our system architecture on the Multi-Lingual Question Answering (MLQA) and the CORD-19 COVID (Wang et al., 2020; Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). We achieve the 2nd rank1 on the Google Natural Questions (Kwiatkowski et al., 2019) leaderboard2 . In this paper, we expand our approach by introducing new multilingual capabilities using models such as Multilingual-BERT (M-BERT) (Devlin et al., 2019) and XLM-R (Conneau et al., 2019). This addition"
2020.coling-demos.8,2020.nlpcovid19-acl.1,0,0.0254579,"Missing"
2020.coling-demos.8,D18-1259,0,0.135037,"ional capabilities to the user to see an answer in their desired language. We also show how M-GAAMA can be used in downstream tasks by incorporating it into an E ND - TO -E ND -QA system using CFO (Chakravarti et al., 2019). We experiment with our system architecture on the Multi-Lingual Question Answering (MLQA) and the CORD-19 COVID (Wang et al., 2020; Tang et al., 2020) datasets to provide insights into the performance of the system. 1 Introduction Recent advances in open domain question answering (QA) have mostly revolved around machine reading comprehension (MRC) (Rajpurkar et al., 2018; Yang et al., 2018). The MRC task is to read and comprehend a given text and then answer questions based on it. Our monolingual MRC approach (Pan et al., 2019) has the capability of being applied to train many Language Models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). We achieve the 2nd rank1 on the Google Natural Questions (Kwiatkowski et al., 2019) leaderboard2 . In this paper, we expand our approach by introducing new multilingual capabilities using models such as Multilingual-BERT (M-BERT) (Devlin et al., 2019) and XLM-R (Conneau et al., 2019). This addition has the capability o"
2020.coling-demos.8,N19-4013,0,0.0210743,"he question was originally asked. It incorporates several multilingual components including multilingual LMs, machine translation, and indexed corpora in multiple languages. The rest of the paper is organized as follows: We first discuss related work, then talk about the data used in our experiments and models. Sections 4 and 5 discuss the demo system and model architecture. Finally, we discuss the Model and Runtime Experiments on MLQA (Lewis et al., 2019) and the COVID-19 CORD-19 dataset (Wang et al., 2020; Tang et al., 2020) in Section 6. 2 Related Work Few other QA demos exist; BERTSerini (Yang et al., 2019), leverages the Anserini IR toolkit (Yang et al., 2017) to extract relevant documents given a question, then uses BERT-based techniques (Devlin et al., 2019) to extract the correct answer. However, their demo is designed to perform only mono-lingual English QA. The GAAMA and CFO (Chakravarti et al., 2019) demos also only performs English QA. In contrast, M-GAAMA and our downstream E ND - TO -E ND -QA task perform cross-lingual QA. Several cross lingual large scale representations have been created by training a large scale transformer (Vaswani et al., 2017) based masked language model on text"
2020.coling-industry.9,P19-1620,0,0.163254,"Wikipedia Page: Total Quality Management Gold Long Answer: The exact origin of the term “total quality management” is uncertain. It is almost certainly inspired by Armand V. Feigenbaum’s multi-edition book Total Quality Control... Gold Short Answer: NULL BERTQA : Armand V. Feigenbaum Figure 1: Examples of questions in the NQ dataset. Example 1 contains the short answer in the long answer whereas Example 2 has none. We propose G AA M A that possesses several MRC technologies that are necessary to perform well on NQ and achieve significant boosts over another industry setting competitor system (Alberti et al., 2019a) pre-trained on a large language model (LM) and then over millions of synthetic examples. Specifically, G AA M A builds on top of a large pre-trained LM and focusses on two broad dimensions: 1. Improved Attention: With the reduction of observation bias in NQ, we find a distinct lack of lexical and grammatical alignment between answer contexts and the questions. For example, here is a question to identify the date of an event from the SQuAD 2.0 dataset: According to business journalist Kimberly Amadeo, when did the first signs of decline in real estate occur? This question can be aligned almo"
2020.coling-industry.9,P17-1171,0,0.0458488,"for supervised machine learning. CovidQA evaluates the zero-shot transfer capabilities of existing models on topics specifically related to COVID-19. One difference of CovidQA from the other QA datasets we evaluate is that it requires systems to predict the correct sentence that answers the question. Hence we intuitively report the P@1, R@3, and MRR based on the official evaluation metric. 5.2 Competitors We compare G AA M A against three strong competitors from the industry research: 1) A hybrid of a decomposable attention model for Natural Language Inference (Parikh et al., 2016) and DrQA (Chen et al., 2017), a retrieve and rank QA model, which obtains commendable results on SQuAD. 2) The NQ baseline system (Alberti et al., 2019b) and 3) The current industry SOTA on NQ (Alberti et al., 2019a) which utilizes 4 million synthetic examples as pre-training. Architecturally, the latter is similar to us but we propose more technical novelty in terms of both improved attention and data augmentation. We note there is very recent academic work (Zheng et al., 2020)which we omit as G AA M A outperforms them on short answers and more importantly we compare against large scale industry SOTA for the scope of th"
2020.coling-industry.9,P17-1055,0,0.105412,"are one-hot vectors for the ground-truth beginning 0.4 and end positions, and 1(a) for the ground-truth answer type. During decoding, the span over argmax of `b and argmax of `e is picked Similarity as the predicted short answer. 0.2 3.2 Attention Strategies 1995 2000 In this section, we outline our investigation of the attention mechanisms on top of the above BERTQA model. Our main question: BERT already computes self-attention over the question and the passage in several layers—can we improve on top that? 3.2.1 Attention-over-Attention (AoA) 1 Our first approach is AoA: originally designed (Cui et al., 2017) for cloze-style question answering, where a phrase in a short passage of text is removed in forming a question. We seek to explore whether AoA helps in a more traditional MRC setting. Let Q be a sequence of question tokens [q1 , . . . , qm ], and C a sequence of context tokens [c1 , . . . , cn ]. AoA first computes an attention matrix: M = CQT , (1) n×h m×h where C ∈ R ,Q ∈ R , and M ∈ Rn×m . In our case, the hidden dimension is h = 1024. Next, it separately performs on M a column-wise softmax α = sof tmax(MT ) and a row-wise softmax β = sof tmax(M). Each row i of matrix α represents the docu"
2020.coling-industry.9,N19-1423,0,0.134633,"performance when compared to baselines either trained on the target domain or zero-shot transferred to the target. Overall, our contributions can be summarized as follows: 1. We propose a novel system that investigates several improved attention and enhanced data augmentation strategies, 2. Outperforms the previous industry-scale QA system on NQ, 3. Provides ZSTL capabilities on two unseen domains and 4. Achieves competitive performance compared to the respective corresponding baselines. 2 Related Work Most recent MRC systems either achieve SOTA by adding additional components on top of BERT (Devlin et al., 2019) such as syntax (Zhang et al., 2019) or perform attention fusion (Wang et al., 2018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generatin"
2020.coling-industry.9,N18-2092,0,0.0233721,"018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generating more questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhingra et al., 2018; Zhou et al., 2019). (Alberti et al., 2019a; Alberti et al., 2019b) combine models of question generation with answer extraction and filter results to ensure round-trip consistency to get the SOTA on NQ. Contrary to this, we explore several strategies for DA that either involve diverse question generation from a dynamic nucleus (Holtzman et al., 2019) of the probability distribution over question tokens or shuffling the existing dataset to produce adversarial examples. Recently (Lee et al., 2019; Min et al., 2019) focus on “open” NQ, a modified version of the full NQ dataset for document retr"
2020.coling-industry.9,Q18-1023,0,0.0285251,"Missing"
2020.coling-industry.9,D19-1445,0,0.0218723,"C Hi = H Wi , (5) M ×1024 ; HC , HC ∈ RN ×1024 ; and WQ , WC ∈ R1024×1024 . Therefore, the AoA where HQ , HQ ∈ R i i i i layer adds about 2.1 million parameters on top of BERT which already has 340 million. Next, we feed Q HC 1 and H1 into the AoA calculation specified in Equations (1) - (3) to get the attention vector s1 for C head 1. The same procedure is applied to HQ 2 and H2 to get s2 for head 2. Lastly, s1 and s2 are combined with `b and `e respectively via two weighted sum operations for answer extraction. 3.2.2 Attention Diversity (AD) layer It has been shown through ablation studies (Kovaleva et al., 2019; Michel et al., 2019) that removing BERT attention heads can achieve comparable or better performance on some tasks. Our objective is to find out if we can diversify the information captured and train a better BERT model by enforcing diversity among the attention heads. In a Transformer model, (Li et al., 2018) examine a few methods to enforce such diversity and see an improvement on machine translation tasks. Contrary to that we start with a pre-trained BERT model, take the attention output from scaled dot-product attention and compute the cosine similarity between all pairs of heads: D= Hea"
2020.coling-industry.9,Q19-1026,0,0.147651,"leaderboard submission). For each question, crowd sourced annotators also provide start and end offsets for short answer spans5 within the Wikipedia article, if available, as well as long answer spans (which is generally the most immediate HTML paragraph, table, or list span containing the short answer), if available. The dataset also forces models to make an attempt at “knowing what they don’t know” (Rajpurkar et al., 2018) by requiring a confidence score with each prediction. For evaluation, we report the offset-based F1 overlap score. For additional details on the data and evaluation see (Kwiatkowski et al., 2019). Target Domain To test G AA M A’s ZSTL transfer capability, we choose two academic6 benchmark datasets on a related domain: Bio-medical. The first one uses a subset of the questions and annotations from task 8b of the BioASQ competition (Tsatsaronis et al., 2015). Specifically, we extract 1,266 factoid biomedical questions for which exact answers can be extracted from one of the PubMED abstracts marked as relevant by the annotators. We report the Factoid Mean Reciprocal Rank (MRR) as the evaluation metric. Secondly, we choose the very recent CovidQA (Tang et al., 2020) benchmark to illustrate"
2020.coling-industry.9,P19-1612,0,0.0354479,"ially generating more questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhingra et al., 2018; Zhou et al., 2019). (Alberti et al., 2019a; Alberti et al., 2019b) combine models of question generation with answer extraction and filter results to ensure round-trip consistency to get the SOTA on NQ. Contrary to this, we explore several strategies for DA that either involve diverse question generation from a dynamic nucleus (Holtzman et al., 2019) of the probability distribution over question tokens or shuffling the existing dataset to produce adversarial examples. Recently (Lee et al., 2019; Min et al., 2019) focus on “open” NQ, a modified version of the full NQ dataset for document retrieval QA that discards unanswerable questions. Contrary to that, we specifically focus on the full NQ dataset and believe there is room for improvement from a MRC research standpoint. 3 Model Architecture In this section, we first describe BERTQA , G AA M A’s underlying QA model, and two additional attention layers on top of it. Figure 2 shows our overall model architecture with details explained below. 3.1 Underlying QA model: BERTQA Given a token sequence X = [x1 , x2 , . . . , xT ]: BERT, a de"
2020.coling-industry.9,D18-1317,0,0.0247664,"or s1 for C head 1. The same procedure is applied to HQ 2 and H2 to get s2 for head 2. Lastly, s1 and s2 are combined with `b and `e respectively via two weighted sum operations for answer extraction. 3.2.2 Attention Diversity (AD) layer It has been shown through ablation studies (Kovaleva et al., 2019; Michel et al., 2019) that removing BERT attention heads can achieve comparable or better performance on some tasks. Our objective is to find out if we can diversify the information captured and train a better BERT model by enforcing diversity among the attention heads. In a Transformer model, (Li et al., 2018) examine a few methods to enforce such diversity and see an improvement on machine translation tasks. Contrary to that we start with a pre-trained BERT model, take the attention output from scaled dot-product attention and compute the cosine similarity between all pairs of heads: D= Head X Head X i=1 j=1 Oi · Oj . ||Oi |Oj || We then average D for the per-token similarity and add it as an additional loss term. For each token, there are 16 + 15 + ... + 2 total similarity calculations, 16 being the number of heads in BERTQA . Figure 3 shows the modified structure of Multi-head Attention in the T"
2020.coling-industry.9,D19-1284,0,0.0137809,"ore questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhingra et al., 2018; Zhou et al., 2019). (Alberti et al., 2019a; Alberti et al., 2019b) combine models of question generation with answer extraction and filter results to ensure round-trip consistency to get the SOTA on NQ. Contrary to this, we explore several strategies for DA that either involve diverse question generation from a dynamic nucleus (Holtzman et al., 2019) of the probability distribution over question tokens or shuffling the existing dataset to produce adversarial examples. Recently (Lee et al., 2019; Min et al., 2019) focus on “open” NQ, a modified version of the full NQ dataset for document retrieval QA that discards unanswerable questions. Contrary to that, we specifically focus on the full NQ dataset and believe there is room for improvement from a MRC research standpoint. 3 Model Architecture In this section, we first describe BERTQA , G AA M A’s underlying QA model, and two additional attention layers on top of it. Figure 2 shows our overall model architecture with details explained below. 3.1 Underlying QA model: BERTQA Given a token sequence X = [x1 , x2 , . . . , xT ]: BERT, a deep Transformer (Vas"
2020.coling-industry.9,D16-1244,0,0.106908,"Missing"
2020.coling-industry.9,D16-1264,0,0.136795,"Missing"
2020.coling-industry.9,P18-2124,0,0.218824,"n NQ. Further, we show that G AA M A transfers zero-shot to unseen real life and important domains as it yields respectable performance on two benchmarks: the BioASQ and the newly introduced CovidQA datasets. 1 Introduction A relatively new task in open domain question answering (QA) is machine reading comprehension (MRC), which aims to read and comprehend a given text and then answer questions based on it. Recent work on transfer learning, from large pre-trained language models like BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) has practically solved SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), the most widely used MRC benchmark. This necessitates harder QA benchmarks for the field to advance. Additionally, SQuAD and other existing datasets like NarrativeQA (Koˇcisk`y et al., 2018) and HotpotQA (Yang et al., 2018) suffer from observation bias: annotators had read the passages before creating their questions. In industry research, there is an urgent demand to build a usable MRC QA system that not only provides very good performance on academic benchmarks but also real life industry applications (Tang et al., 2020) in a ZSTL environment. In this paper, to build such a system, we firs"
2020.coling-industry.9,N19-5004,0,0.0377738,"Missing"
2020.coling-industry.9,2020.acl-main.500,0,0.0540436,"Missing"
2020.coling-industry.9,P18-1158,0,0.0118638,"t transferred to the target. Overall, our contributions can be summarized as follows: 1. We propose a novel system that investigates several improved attention and enhanced data augmentation strategies, 2. Outperforms the previous industry-scale QA system on NQ, 3. Provides ZSTL capabilities on two unseen domains and 4. Achieves competitive performance compared to the respective corresponding baselines. 2 Related Work Most recent MRC systems either achieve SOTA by adding additional components on top of BERT (Devlin et al., 2019) such as syntax (Zhang et al., 2019) or perform attention fusion (Wang et al., 2018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generating more questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhin"
2020.coling-industry.9,2020.nlpcovid19-acl.1,0,0.0256188,"Missing"
2020.coling-industry.9,D18-1259,0,0.0913112,"Missing"
2020.coling-industry.9,D19-1253,0,0.0218363,"l components on top of BERT (Devlin et al., 2019) such as syntax (Zhang et al., 2019) or perform attention fusion (Wang et al., 2018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generating more questions to enhance the training data or in a MTL setup (Yatskar, 2018; Dhingra et al., 2018; Zhou et al., 2019). (Alberti et al., 2019a; Alberti et al., 2019b) combine models of question generation with answer extraction and filter results to ensure round-trip consistency to get the SOTA on NQ. Contrary to this, we explore several strategies for DA that either involve diverse question generation from a dynamic nucleus (Holtzman et al., 2019) of the probability distribution over question tokens or shuffling the existing dataset to produce adversarial examples."
2020.coling-industry.9,D19-1169,0,0.0121634,"s either trained on the target domain or zero-shot transferred to the target. Overall, our contributions can be summarized as follows: 1. We propose a novel system that investigates several improved attention and enhanced data augmentation strategies, 2. Outperforms the previous industry-scale QA system on NQ, 3. Provides ZSTL capabilities on two unseen domains and 4. Achieves competitive performance compared to the respective corresponding baselines. 2 Related Work Most recent MRC systems either achieve SOTA by adding additional components on top of BERT (Devlin et al., 2019) such as syntax (Zhang et al., 2019) or perform attention fusion (Wang et al., 2018) without using BERT. However, we argue that additional attention mechanisms should be explored on top of BERT such as computing additional cross-attention between the question and the passage and maximizing the diversity among different attention heads in BERT. Our work is also generic enough to be applied on recently introduced transformer based language models such as ALBERT (Lan et al., 2019) and REFORMER (Kitaev et al., 2020). Another common technique is DA (Zhang and Bansal, 2019) by artificially generating more questions to enhance the trai"
2020.coling-industry.9,2020.acl-main.599,0,0.60604,"Missing"
2020.emnlp-demos.5,D19-3006,1,0.826158,"m.com Abstract and explore how to ensemble multiple MRC models from GAAMA1 . We evaluate these techniques on the NQ short answer task. Using our ensemble of models, for each example (question, passage pair), we take the top predictions per system, group by span (answer extracted from the passage), normalize and aggregate the scores, take the mean score across systems for each span, and then take the highest scoring short and long answer spans as our final prediction. These improved ensembling techniques are applied to our MRC systems to produce stronger answers. Whereas other systems such as (Chakravarti et al., 2019; Yang et al., 2019a) and Allen NLP’s2 make use of a single model, we are able to use multiple models to produce a stronger result. We further take advantage of the fact that both the individual model predictions and the ensembed predictions are returned to help increase explainability for the user. For the graphical interface we use a heatmap to show the level of (dis)agreement between the underlying models along with the “best ensemble” answer. An example of this can be seen in Figure 1. More completely, our contributions include: We introduce ARES (A Reading Comprehension Ensembling Service"
2020.emnlp-demos.5,Q19-1026,0,0.0155355,"ARES leverages the CFO (Chakravarti et al., 2019) and ReactJS distributed frameworks to provide a scalable interactive Question Answering experience that capitalizes on the agreement (or lack thereof) between models to improve the answer visualization experience. 1 Introduction Machine Reading Comprension (MRC) involves computer systems that can take a question and some text and produce an answer to that question using the content in that text. This field has recently received considerable attention, yielding popular leaderboard challenges such as SQuAD (Rajpurkar et al., 2016, 2018) and NQ (Kwiatkowski et al., 2019). Currently, the top submissions on both the SQuAD and NQ leaderboards combine multiple system outputs. These ensembled systems traditionally outperform single models by 1-4 Fmeasure. Unfortunately, many of the papers for these systems provide little to no information about the ensembling techniques they use. In this work, we use GAAMA, a prototype question-answering system using the MRC techniques of (Pan et al., 2019), as our starting point ∗ • A novel MRC demonstration system, which leverages multiple underlying MRC model predictions and ensembles them for the user. • A system architecture"
2020.emnlp-demos.5,N19-4013,0,0.0971586,"how to ensemble multiple MRC models from GAAMA1 . We evaluate these techniques on the NQ short answer task. Using our ensemble of models, for each example (question, passage pair), we take the top predictions per system, group by span (answer extracted from the passage), normalize and aggregate the scores, take the mean score across systems for each span, and then take the highest scoring short and long answer spans as our final prediction. These improved ensembling techniques are applied to our MRC systems to produce stronger answers. Whereas other systems such as (Chakravarti et al., 2019; Yang et al., 2019a) and Allen NLP’s2 make use of a single model, we are able to use multiple models to produce a stronger result. We further take advantage of the fact that both the individual model predictions and the ensembed predictions are returned to help increase explainability for the user. For the graphical interface we use a heatmap to show the level of (dis)agreement between the underlying models along with the “best ensemble” answer. An example of this can be seen in Figure 1. More completely, our contributions include: We introduce ARES (A Reading Comprehension Ensembling Service): a novel Machine"
2020.emnlp-demos.5,2020.acl-main.604,0,0.0495456,"s the most important regions of the passage from the perspective of different models in addition to boxing in the ensembled answer as seen in Figure 1. 2.2 Many of the top recent MRC systems publish few details on their ensembling strategies. Systems such as (Devlin et al., 2019; Alberti et al., 2019; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019; Group, 2017; Seo et al., 2016) report using ensembles of 5 to 18 models to gain 1.3 - 4 F1 points on tasks such as GLUE, SQuAD 1.0, and SQuAD 2.0; unfortunately most of these systems report little information on their ensembling techniques. (Liu et al., 2020) reports slightly more information: gaining 1.8 and 0.6 F1 points short answer (SA) and long answer (LA) respectively on the NQ dev set with an ensemble of three models with different hyperparameters. We also consider work in the field of information retrieval (IR) as a way to aggregate multiple scores for the same span. Similar to the popular CombSUM and CombMNZ (Kurland and Culpepper, 2018; Wu, 2012) methods, considering the spans as the “documents”, we use span-score weighted aggregation in our noisy-or aggregator. Futher, we additionally incorporate the use of rank-based scoring from Borda"
2020.emnlp-demos.5,2021.ccl-1.108,0,0.0475564,"Missing"
2020.emnlp-demos.5,P18-2124,0,0.0636688,"Missing"
2020.emnlp-demos.5,D16-1264,0,0.0423104,"bling strategies using the NQ dataset. ARES leverages the CFO (Chakravarti et al., 2019) and ReactJS distributed frameworks to provide a scalable interactive Question Answering experience that capitalizes on the agreement (or lack thereof) between models to improve the answer visualization experience. 1 Introduction Machine Reading Comprension (MRC) involves computer systems that can take a question and some text and produce an answer to that question using the content in that text. This field has recently received considerable attention, yielding popular leaderboard challenges such as SQuAD (Rajpurkar et al., 2016, 2018) and NQ (Kwiatkowski et al., 2019). Currently, the top submissions on both the SQuAD and NQ leaderboards combine multiple system outputs. These ensembled systems traditionally outperform single models by 1-4 Fmeasure. Unfortunately, many of the papers for these systems provide little to no information about the ensembling techniques they use. In this work, we use GAAMA, a prototype question-answering system using the MRC techniques of (Pan et al., 2019), as our starting point ∗ • A novel MRC demonstration system, which leverages multiple underlying MRC model predictions and ensembles th"
2020.emnlp-main.440,N16-1153,0,0.0652316,"Missing"
2020.emnlp-main.440,P19-1484,0,0.0166446,"ta in many domains can have implicit structure which can be taken advantage of. For example, in the IT domain, technical documents are often created using predefined templates, and support forums have data in the form of questions and accepted answers. In this work, we propose to make use of the structure in such unlabeled domain data to create synthetic data that can provide additional domain knowledge to the model. Augmenting training data with generated synthetic examples has been found to be effective in improving performance on low-resource tasks. Golub et al. (2017), Yang et al. (2017), Lewis et al. (2019) and Dhingra et al. (2018) develop approaches to generate natural questions that can aid downstream question answering tasks. However, when it is not possible to obtain synthetic data that exactly fits the target task description, we show that creating auxiliary tasks from such unlabeled data can be 5461 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5461–5468, c November 16–20, 2020. 2020 Association for Computational Linguistics Dataset TechQA AskUbuntu useful to the downstream task in a transfer learning setting. For preliminary experiments in"
2020.emnlp-main.440,2020.tacl-1.47,0,0.0222883,"in content and terminology from the pre-training corpora. To address this language mismatch problem, recent work (Alsentzer et al., 2019; Lee et al., 2019; ∗ Both authors contributed equally. Work done during AI Residency at IBM Research. ‡ Corresponding author. † Beltagy et al., 2019; Gururangan et al., 2020) has adapted pre-trained LMs to specific domains by continuing to train the same LM on target domain text. Similar approaches are also used in multilingual adaptation, where the representations learned from multilingual pre-training are further optimized for a particular target language (Liu et al., 2020; Bapna and Firat, 2019). However, many specialized domains contain their own specific terms that are not part of the pre-trained LM vocabulary. Furthermore, in many such domains, large enough corpora may not be available to support LM training from scratch. To resolve this out-of-vocabulary issue, in this work, we extend the open-domain vocabulary with in-domain terms while adapting the LM, and show that it helps improve performance on downstream tasks. While language modeling can help the model better encode the domain language, it might not be sufficient to gain the domain knowledge necessa"
2020.emnlp-main.440,2021.ccl-1.108,0,0.114872,"Missing"
2020.emnlp-main.440,P17-2081,0,0.0204121,"ion Detection. 1 Introduction Pre-trained language models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019) have pushed performance in many natural language processing tasks to new heights. The process of model construction has effectively been reduced to extending the pre-trained LM architecture with simpler taskspecific layers, while fine-tuning on labeled target data. In cases where the target task has limited labeled data, prior work has also employed transfer learning by pre-training on a source dataset with abundant labeled data before fine-tuning on the target task dataset (Min et al., 2017; Chung et al., 2018; Wiese et al., 2017). However, directly fine-tuning to a task in a new domain may not be optimal when the domain is distant in content and terminology from the pre-training corpora. To address this language mismatch problem, recent work (Alsentzer et al., 2019; Lee et al., 2019; ∗ Both authors contributed equally. Work done during AI Residency at IBM Research. ‡ Corresponding author. † Beltagy et al., 2019; Gururangan et al., 2020) has adapted pre-trained LMs to specific domains by continuing to train the same LM on target domain text. Similar approaches are also used in m"
2020.emnlp-main.440,N19-4009,0,0.059706,"Missing"
2020.emnlp-main.440,D16-1264,0,0.0406929,"se to use the inherent structure in unlabeled data to formulate synthetic tasks that can transfer to downstream tasks in a lowresource setting. (3) In our experiments, we show considerable improvements in performance over directly fine-tuning an underlying RoBERTa-large LM (Liu et al., 2019) on multiple tasks in the IT domain: extractive reading comprehension (RC), document ranking (DR) and duplicate question detection (DQD).1 2 Datasets We use two publicly available IT domain datasets. Table 1 shows their size statistics. TechQA (Castelli et al., 2019) is an extractive reading comprehension (Rajpurkar et al., 2016) dataset developed from real user questions in the customer support domain. Each question is accompanied by 50 documents, at most one of which has the answer. A companion collection of 801K unlabeled Technotes is provided to support LM training. In addition to the primary reading comprehension task (TechQA-RC), we also evaluate on a new document ranking task (TechQA-DR). Given the question, the task is to find the document that contains the answer. AskUbuntu2 (Lei et al., 2016) is a dataset containing user-marked pairs of similar questions from Stack Exchange3 , which was developed for a dupli"
2020.emnlp-main.440,D19-1171,0,0.0427042,"Missing"
2020.emnlp-main.440,K17-1029,0,0.0192711,"ned language models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019) have pushed performance in many natural language processing tasks to new heights. The process of model construction has effectively been reduced to extending the pre-trained LM architecture with simpler taskspecific layers, while fine-tuning on labeled target data. In cases where the target task has limited labeled data, prior work has also employed transfer learning by pre-training on a source dataset with abundant labeled data before fine-tuning on the target task dataset (Min et al., 2017; Chung et al., 2018; Wiese et al., 2017). However, directly fine-tuning to a task in a new domain may not be optimal when the domain is distant in content and terminology from the pre-training corpora. To address this language mismatch problem, recent work (Alsentzer et al., 2019; Lee et al., 2019; ∗ Both authors contributed equally. Work done during AI Residency at IBM Research. ‡ Corresponding author. † Beltagy et al., 2019; Gururangan et al., 2020) has adapted pre-trained LMs to specific domains by continuing to train the same LM on target domain text. Similar approaches are also used in multilingual adaptation, where the represe"
2020.emnlp-main.440,P17-1096,0,0.02229,"at such unlabeled data in many domains can have implicit structure which can be taken advantage of. For example, in the IT domain, technical documents are often created using predefined templates, and support forums have data in the form of questions and accepted answers. In this work, we propose to make use of the structure in such unlabeled domain data to create synthetic data that can provide additional domain knowledge to the model. Augmenting training data with generated synthetic examples has been found to be effective in improving performance on low-resource tasks. Golub et al. (2017), Yang et al. (2017), Lewis et al. (2019) and Dhingra et al. (2018) develop approaches to generate natural questions that can aid downstream question answering tasks. However, when it is not possible to obtain synthetic data that exactly fits the target task description, we show that creating auxiliary tasks from such unlabeled data can be 5461 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5461–5468, c November 16–20, 2020. 2020 Association for Computational Linguistics Dataset TechQA AskUbuntu useful to the downstream task in a transfer learning setting. For prelim"
2020.findings-emnlp.288,P19-1620,0,0.0215709,"ess to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that multiple sentences correspond to a single AMR and thus needs no external data. This is closer to recent work on question generation for question answering systems (Alberti et al., 2019), which also uses cycle consistency filtering. Finally, regarding synthetic AMR, discussed in Section 5, with respect to prior work (Konstas et al., 2017b; van Noord and Bos, 2017) we show that synthetic AMR parsing still can yield improvements for high performance baselines, and introduce the cycle-consistency filtering. 8 Conclusions In this work6 , we explored different ways in which trained models can be applied to improve AMR parsing performance via self-learning. Despite the recent strong improvements in performance through novel architectures, we show that the proposed techniques improv"
2020.findings-emnlp.288,D16-1211,0,0.015395,"need for graph post-processing using Core-NLP. Compared to this, transition-based approaches provide a more uniform performance across categories, and in this context the presented self-learning methods are able to improve in all categories. One aspect that merits further study, is the increase in the Negation category when using synTxt, which improves 5.4 points, probably due to generation of additional negation examples. 7 Related Works Mining for gold, introduced in Section 3, can be related to previous works addressing oracle limitations such as dynamic oracles (Goldberg and Nivre, 2012; Ballesteros et al., 2016), imitation learning (Goodman et al., 2016) and minimum risk training (Naseem et al., 2019). All these approaches increase parser robustness to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that multiple sentences correspo"
2020.findings-emnlp.288,2020.acl-main.119,0,0.463735,"e rise of pre-trained transformer models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019), but also due to AMR-specific architecture improvements. A non-exhaustive list includes latent node-word alignments through learned permutations (Lyu and Titov, 2018a), minimum risk training via REINFORCE (Naseem et al., 2019), a sequence-to-graph modeling of linearized trees with copy mechanisms and re-entrance features ∗ † Equal contribution. Work done during AI Residency at IBM Research. (Zhang et al., 2019a) and more recently a highly performant graph-sequence iterative refinement model (Cai and Lam, 2020) and a hard-attention transition-based parser (F. A. et al., 2020), both based on the Transformer architecture. Given the strong improvements in architectures for AMR, it becomes interesting to explore alternative avenues to push performance even further. AMR annotations are relatively expensive to produce and thus typical corpora have on the order of tens of thousands of sentences. In this work we explore the use self-learning techniques as a means to escape this limitation. We explore the use of a trained parser to iteratively refine a rule-based AMR oracle (Ballesteros and Al-Onaizan, 2017;"
2020.findings-emnlp.288,P13-2131,0,0.203945,"ousands of sentences. In this work we explore the use self-learning techniques as a means to escape this limitation. We explore the use of a trained parser to iteratively refine a rule-based AMR oracle (Ballesteros and Al-Onaizan, 2017; F. A. et al., 2020) to yield better action sequences. We also exploit the fact that a single AMR graph maps to multiple sentences in combination with AMR-to-text (Mager et al., 2020), to generate additional training samples without using external data. Finally we revisit silver data training (Konstas et al., 2017a). These techniques reach 77.3 and 80.7 Smatch (Cai and Knight, 2013) on AMR1.0 and AMR2.0 respectively using only gold data as well as 78.2 and 81.3 with silver data. 2 Baseline Parser and Setup To test the proposed ideas, we used the AMR setup and parser from (F. A. et al., 2020) with improved embedding representations. This is a transitionbased parsing approach, following the original AMR oracle in (Ballesteros and Al-Onaizan, 2017) and further improvements in (Naseem et al., 2019). Briefly, rather than predicting a graph g from a sentence s directly, transition-based parsers predict instead an action sequence a. This action sequence, when applied to a state"
2020.findings-emnlp.288,E17-1051,0,0.0332596,"+synAMRU 80.9 84.9 81.4 88.4 88.0 66.0 79.3 70.9 78.9 synTxt+synAMRU 81.0 84.9 81.5 88.6 88.3 67.4 78.9 71.5 79.1 mining+synTxt+synAMRU 81.3 85.3 81.8 88.7 88.7 66.3 79.2 71.9 79.4 Table 5: Detailed scoring of the final system on AMR2.0 test sets which is the best result obtained at the time of submission for AMR2.0, improving 1.1 over (Cai and Lam, 2020). It also obtains 78.2 for AMR1.0, which is 2.8 points above best previous results. Excluding silver data training, synTxt achieves 80.7 (+0.5) in AMR2.0 and 77.5 (+2.1) with minining in AMR1.0. We also provide the detailed AMR analysis from (Damonte et al., 2017) for the best previously published system, baseline and the proposed methods in Table 5. This analysis computes Smatch for sub-sets of AMR to loosely reflect particular subtasks, such as Word Sense Disambiguation (WSD), Named Entity recognition or Semantic Role Labeling (SRL). The proposed approaches and the baseline consistently outperform prior art in a majority of categories and the main observable differences seems due to differences between the transitionbased and graph recategorization approaches. Wikification and negation, the only categories where the proposed methods do not outperform"
2020.findings-emnlp.288,N19-1423,0,0.104904,"mprove an already performant parser and achieve state-ofthe-art results on AMR 1.0 and AMR 2.0. 1 Introduction Abstract Meaning Representation (AMR) are broad-coverage sentence-level semantic representations expressing who does what to whom. Nodes in an AMR graph correspond to concepts such as entities or predicates and are not always directly related to words. Edges in AMR represent relations between concepts such as subject/object. AMR has experienced unprecedented performance improvements in the last two years, partly due to the rise of pre-trained transformer models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019), but also due to AMR-specific architecture improvements. A non-exhaustive list includes latent node-word alignments through learned permutations (Lyu and Titov, 2018a), minimum risk training via REINFORCE (Naseem et al., 2019), a sequence-to-graph modeling of linearized trees with copy mechanisms and re-entrance features ∗ † Equal contribution. Work done during AI Residency at IBM Research. (Zhang et al., 2019a) and more recently a highly performant graph-sequence iterative refinement model (Cai and Lam, 2020) and a hard-attention transition-based parser (F. A. et al., 2020"
2020.findings-emnlp.288,2020.findings-emnlp.89,1,0.79522,"Missing"
2020.findings-emnlp.288,P14-1134,0,0.10189,"onal Linguistics Figure 1: Role of sentence s, AMR graph g and oracle actions a in the different self-learning strategies. Left: Replacing rule-based actions by machine generated ones. Middle: synthetic text generation for existing graph annotations. Right: synthetic AMR generation for external data. Generated data ( ). External data ( ). predicting the graph into a sequence to sequence problem, but introduces the need for an oracle to determine the action sequence a = O(g, s). As in previous works, the oracle in (F. A. et al., 2020) is rule-based, relying on external word-to-node alignments (Flanigan et al., 2014; Pourdamghani et al., 2016) to determine action sequences. It however force-aligns unaligned nodes to suitable words, notably improving oracle performance. As parser, (F. A. et al., 2020) introduces the stack-Transformer model. This is a modification of the sequence to sequence Transformer (Vaswani et al., 2017) to account for the parser state. It modifies the cross-attention mechanism dedicating two heads to attend the stack and buffer of the state machine M (a, s). This parser is highly performant achieving the best results for a transition-based parser as of date and second overall for AMR"
2020.findings-emnlp.288,C12-1059,0,0.0151358,"category, probably due to need for graph post-processing using Core-NLP. Compared to this, transition-based approaches provide a more uniform performance across categories, and in this context the presented self-learning methods are able to improve in all categories. One aspect that merits further study, is the increase in the Negation category when using synTxt, which improves 5.4 points, probably due to generation of additional negation examples. 7 Related Works Mining for gold, introduced in Section 3, can be related to previous works addressing oracle limitations such as dynamic oracles (Goldberg and Nivre, 2012; Ballesteros et al., 2016), imitation learning (Goodman et al., 2016) and minimum risk training (Naseem et al., 2019). All these approaches increase parser robustness to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that"
2020.findings-emnlp.288,P16-1001,0,0.0159831,"Compared to this, transition-based approaches provide a more uniform performance across categories, and in this context the presented self-learning methods are able to improve in all categories. One aspect that merits further study, is the increase in the Negation category when using synTxt, which improves 5.4 points, probably due to generation of additional negation examples. 7 Related Works Mining for gold, introduced in Section 3, can be related to previous works addressing oracle limitations such as dynamic oracles (Goldberg and Nivre, 2012; Ballesteros et al., 2016), imitation learning (Goodman et al., 2016) and minimum risk training (Naseem et al., 2019). All these approaches increase parser robustness to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that multiple sentences correspond to a single AMR and thus needs no extern"
2020.findings-emnlp.288,W16-2316,0,0.0222026,"Missing"
2020.findings-emnlp.288,P17-1014,0,0.14619,"e to produce and thus typical corpora have on the order of tens of thousands of sentences. In this work we explore the use self-learning techniques as a means to escape this limitation. We explore the use of a trained parser to iteratively refine a rule-based AMR oracle (Ballesteros and Al-Onaizan, 2017; F. A. et al., 2020) to yield better action sequences. We also exploit the fact that a single AMR graph maps to multiple sentences in combination with AMR-to-text (Mager et al., 2020), to generate additional training samples without using external data. Finally we revisit silver data training (Konstas et al., 2017a). These techniques reach 77.3 and 80.7 Smatch (Cai and Knight, 2013) on AMR1.0 and AMR2.0 respectively using only gold data as well as 78.2 and 81.3 with silver data. 2 Baseline Parser and Setup To test the proposed ideas, we used the AMR setup and parser from (F. A. et al., 2020) with improved embedding representations. This is a transitionbased parsing approach, following the original AMR oracle in (Ballesteros and Al-Onaizan, 2017) and further improvements in (Naseem et al., 2019). Briefly, rather than predicting a graph g from a sentence s directly, transition-based parsers predict inste"
2020.findings-emnlp.288,2021.ccl-1.108,0,0.0417005,"Missing"
2020.findings-emnlp.288,P18-1037,0,0.371916,"mantic representations expressing who does what to whom. Nodes in an AMR graph correspond to concepts such as entities or predicates and are not always directly related to words. Edges in AMR represent relations between concepts such as subject/object. AMR has experienced unprecedented performance improvements in the last two years, partly due to the rise of pre-trained transformer models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019), but also due to AMR-specific architecture improvements. A non-exhaustive list includes latent node-word alignments through learned permutations (Lyu and Titov, 2018a), minimum risk training via REINFORCE (Naseem et al., 2019), a sequence-to-graph modeling of linearized trees with copy mechanisms and re-entrance features ∗ † Equal contribution. Work done during AI Residency at IBM Research. (Zhang et al., 2019a) and more recently a highly performant graph-sequence iterative refinement model (Cai and Lam, 2020) and a hard-attention transition-based parser (F. A. et al., 2020), both based on the Transformer architecture. Given the strong improvements in architectures for AMR, it becomes interesting to explore alternative avenues to push performance even fur"
2020.findings-emnlp.288,2020.acl-main.167,1,0.915499,"MR, it becomes interesting to explore alternative avenues to push performance even further. AMR annotations are relatively expensive to produce and thus typical corpora have on the order of tens of thousands of sentences. In this work we explore the use self-learning techniques as a means to escape this limitation. We explore the use of a trained parser to iteratively refine a rule-based AMR oracle (Ballesteros and Al-Onaizan, 2017; F. A. et al., 2020) to yield better action sequences. We also exploit the fact that a single AMR graph maps to multiple sentences in combination with AMR-to-text (Mager et al., 2020), to generate additional training samples without using external data. Finally we revisit silver data training (Konstas et al., 2017a). These techniques reach 77.3 and 80.7 Smatch (Cai and Knight, 2013) on AMR1.0 and AMR2.0 respectively using only gold data as well as 78.2 and 81.3 with silver data. 2 Baseline Parser and Setup To test the proposed ideas, we used the AMR setup and parser from (F. A. et al., 2020) with improved embedding representations. This is a transitionbased parsing approach, following the original AMR oracle in (Ballesteros and Al-Onaizan, 2017) and further improvements in"
2020.findings-emnlp.288,P19-1451,1,0.306011,"es in an AMR graph correspond to concepts such as entities or predicates and are not always directly related to words. Edges in AMR represent relations between concepts such as subject/object. AMR has experienced unprecedented performance improvements in the last two years, partly due to the rise of pre-trained transformer models (Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019), but also due to AMR-specific architecture improvements. A non-exhaustive list includes latent node-word alignments through learned permutations (Lyu and Titov, 2018a), minimum risk training via REINFORCE (Naseem et al., 2019), a sequence-to-graph modeling of linearized trees with copy mechanisms and re-entrance features ∗ † Equal contribution. Work done during AI Residency at IBM Research. (Zhang et al., 2019a) and more recently a highly performant graph-sequence iterative refinement model (Cai and Lam, 2020) and a hard-attention transition-based parser (F. A. et al., 2020), both based on the Transformer architecture. Given the strong improvements in architectures for AMR, it becomes interesting to explore alternative avenues to push performance even further. AMR annotations are relatively expensive to produce and"
2020.findings-emnlp.288,P02-1040,1,0.124689,"synAMR corpus. This set is optionally filtered to reduce the training corpus size for AMR 2.0 experiments and is left unfiltered for AMR 1.0, due to its smaller size. The filtering combines two criteria. First, it is easy to detect when the transition-based system produces disconnected AMR graphs. Outputs with disconnected graphs are therefore filtered out. Second, we use a cycle-consistency criteria as in Section 4 whereby synthetic text is generated for each synthetic AMR with (Mager et al., 2020). For each pair of original text and generated text, the synAMR is filtered out if BLEU score (Papineni et al., 2002) is lower than a pre-specified 4 synTxt training takes 17h for AMR 2.0 and 5h hours for AMR 1.0 on a Tesla V100. AMR-to-text training for 15 epochs takes 4.5h on AMR 1.0 and 15h on AMR 2.0. 3210 threshold, 5 in our experiments. Because the AMRto-text generation system is trained on the humanannotated AMR only, generation performance may be worse on synthetic AMR and out of domain data. Consequently we apply BLEU-based filtering only to the input texts with no out of vocabulary (OOV) tokens with respect to the original humanannotated corpus. After filtering, the synAMR data is reduced to 58k se"
2020.findings-emnlp.288,W16-6603,0,0.0267822,"1: Role of sentence s, AMR graph g and oracle actions a in the different self-learning strategies. Left: Replacing rule-based actions by machine generated ones. Middle: synthetic text generation for existing graph annotations. Right: synthetic AMR generation for external data. Generated data ( ). External data ( ). predicting the graph into a sequence to sequence problem, but introduces the need for an oracle to determine the action sequence a = O(g, s). As in previous works, the oracle in (F. A. et al., 2020) is rule-based, relying on external word-to-node alignments (Flanigan et al., 2014; Pourdamghani et al., 2016) to determine action sequences. It however force-aligns unaligned nodes to suitable words, notably improving oracle performance. As parser, (F. A. et al., 2020) introduces the stack-Transformer model. This is a modification of the sequence to sequence Transformer (Vaswani et al., 2017) to account for the parser state. It modifies the cross-attention mechanism dedicating two heads to attend the stack and buffer of the state machine M (a, s). This parser is highly performant achieving the best results for a transition-based parser as of date and second overall for AMR2.0 and tied with the best f"
2020.findings-emnlp.288,P16-1009,0,0.0358422,"rks addressing oracle limitations such as dynamic oracles (Goldberg and Nivre, 2012; Ballesteros et al., 2016), imitation learning (Goodman et al., 2016) and minimum risk training (Naseem et al., 2019). All these approaches increase parser robustness to its own errors by exposing it to actions that are often inferior to the oracle sequence in score. The approach presented here seeks only the small set of sequences improving over the oracle and uses them for conventional maximum likelihood training. Synthetic text, introduced in Section 4, is related to Back-translation in Machine Translation (Sennrich et al., 2016). The approach presented here exploits however the fact that multiple sentences correspond to a single AMR and thus needs no external data. This is closer to recent work on question generation for question answering systems (Alberti et al., 2019), which also uses cycle consistency filtering. Finally, regarding synthetic AMR, discussed in Section 5, with respect to prior work (Konstas et al., 2017b; van Noord and Bos, 2017) we show that synthetic AMR parsing still can yield improvements for high performance baselines, and introduce the cycle-consistency filtering. 8 Conclusions In this work6 ,"
2020.findings-emnlp.288,P19-1009,0,0.273105,"Missing"
2020.findings-emnlp.288,D19-1392,0,0.429197,"Missing"
2021.acl-short.34,2020.findings-emnlp.89,1,0.890895,"nowledge base entities using BLINK entity linker. The entity nodes in graph are also used to predict the number and locations of relation slots. A slot is defined as a pair of nodes in the AMR graph, where the corresponding entities have a relation in knowledge base in the context of the question. For instance, in Figure 3, nodes city and person are involved in a KB relation death place relevant for this question. Slot prediction is done using a deterministic rule-based transformation described in (Kapanipathi et al., 2021). In particular, we use their 1 We use the stack transformer parser of Astudillo et al. (2020); Lee et al. (2020) for generating AMR graphs and the BLINK system of Wu et al. (2019) for entity linking. 2.2 Neural Relation Linking Model SemReL employs a Siamese network, where the input question and target relations are embedded in the same vector space. The most likely relation is the one whose representation is closest to that of the input question. Figure 3 shows the overall architecture of our model. We use a Transformer model (Vaswani et al., 2017) as a shared encoder for both the input questions and candidate relations. In particular, we use the pre-trained BERT model (Devlin et al."
2021.acl-short.34,2020.findings-emnlp.288,1,0.722696,"ing BLINK entity linker. The entity nodes in graph are also used to predict the number and locations of relation slots. A slot is defined as a pair of nodes in the AMR graph, where the corresponding entities have a relation in knowledge base in the context of the question. For instance, in Figure 3, nodes city and person are involved in a KB relation death place relevant for this question. Slot prediction is done using a deterministic rule-based transformation described in (Kapanipathi et al., 2021). In particular, we use their 1 We use the stack transformer parser of Astudillo et al. (2020); Lee et al. (2020) for generating AMR graphs and the BLINK system of Wu et al. (2019) for entity linking. 2.2 Neural Relation Linking Model SemReL employs a Siamese network, where the input question and target relations are embedded in the same vector space. The most likely relation is the one whose representation is closest to that of the input question. Figure 3 shows the overall architecture of our model. We use a Transformer model (Vaswani et al., 2017) as a shared encoder for both the input questions and candidate relations. In particular, we use the pre-trained BERT model (Devlin et al., 2018) to initiali"
2021.acl-short.34,2020.coling-main.222,0,0.0372914,"ved significant interest due to its real-world applications. KBQA is a task where a natural language question is transformed into a precise structured query, using Entity Linking and Relation Linking as necessary sub-tasks to retrieve an answer. For example, the question “Who founded the city where Pat Vincent died?” requires mapping (a) founded and died to relations dbo:founder and dbo:deathPlace, and (b) entity Pat Vincent to dbr:Pat Vincent, given DBpedia as the knowledge base. Semantic parses such as Abstract Meaning Representation (AMR) have recently shown to be useful for the KBQA task (Lim et al., 2020). However, critical tasks for KBQA such as Relation Linking continue to be addressed primarily using the question text (Mulang’ et al., 2020; Sakor et al., 2019b; Lin et al., 2020), ignoring the AMR parses of the question which can introduce additional semantics. In the literature, some systems such as SLING (Mihindukulasooriya et al., 2020) have used AMR for relation linking. However, similar to other rulebased approaches (Sakor et al., 2019b), SLING depends heavily on the specific target KG (DBpedia) and it is based on a complex ensemble of different approaches, making portability to new kno"
2021.acl-short.34,2020.starsem-1.13,0,0.0352846,"Missing"
2021.acl-short.34,N19-1243,0,0.374389,"sing Entity Linking and Relation Linking as necessary sub-tasks to retrieve an answer. For example, the question “Who founded the city where Pat Vincent died?” requires mapping (a) founded and died to relations dbo:founder and dbo:deathPlace, and (b) entity Pat Vincent to dbr:Pat Vincent, given DBpedia as the knowledge base. Semantic parses such as Abstract Meaning Representation (AMR) have recently shown to be useful for the KBQA task (Lim et al., 2020). However, critical tasks for KBQA such as Relation Linking continue to be addressed primarily using the question text (Mulang’ et al., 2020; Sakor et al., 2019b; Lin et al., 2020), ignoring the AMR parses of the question which can introduce additional semantics. In the literature, some systems such as SLING (Mihindukulasooriya et al., 2020) have used AMR for relation linking. However, similar to other rulebased approaches (Sakor et al., 2019b), SLING depends heavily on the specific target KG (DBpedia) and it is based on a complex ensemble of different approaches, making portability to new knowledge bases a non-trivial task. In this work, we propose SemReL; a single Semantics-aware neural model for Relation linking. SemReL takes as input the question"
2021.eacl-main.30,N19-1388,0,0.0277026,"mance over the base models. For concept alignment, we combine the proposed contextual word alignments with previously established alignment techniques utilizing matching rules tailored to AMR as well as machine translation aligners (Flanigan et al., 2014; Pourdamghani et al., 2014). For AMR parser training, we pre-train an AMR parser on the treebanks of different languages simultaneously and subsequently finetune on each language. This is analogous to the techniques used for silver data pre-training (Konstas et al., 2017; van Noord and Bos, 2017) in AMR parsing and multi-lingual pre-training (Aharoni et al., 2019) in machine translation. Finally, we conduct a detailed error analysis of the multilingual AMR parsing. One of the major errors we have found involves synonymous concepts, which share the same meaning as the original concepts in English, but differ in spellings. While this error is mainly caused by the fact that the multilingual word embeddings bridge non-English input tokens to English concepts, it also highlights the highly lexical nature of Smatch scoring (Cai and Knight, 2013) which does not take synonymous concepts into consideration. We also elaborate upon error analysis of the direct co"
2021.eacl-main.30,L18-1157,0,0.222047,"Missing"
2021.eacl-main.30,P17-1042,0,0.0187446,"2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent body of work (Conneau et al., 2018; Schuster et al., 2019; Artetxe et al., 2018). The work most similar to ours is Cao et al. (2020) where the authors obtain contextual embedding alignments from multilingual BERT (Devlin et al., 2018; Pires et al., 2019) and subsequently improve the alignments via finetuning using supervised parallel corpora. Our contextual word alignment between two parallel sentences may be thoug"
2021.eacl-main.30,P18-1073,0,0.023089,"nt of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent body of work (Conneau et al., 2018; Schuster et al., 2019; Artetxe et al., 2018). The work most similar to ours is Cao et al. (2020) where the authors obtain contextual embedding alignments from multilingual BERT (Devlin et al., 2018; Pires et al., 2019) and subsequently improve the alignments via finetuning using supervised parallel corpora. Our contextual word alignment between two parallel sentences may be thought of as an adaptation of their contextual word retrieval task. However, we refrain from any finetuning of the contextual embeddings and show that the contextual word alignments from the off-the-shelf XLM-R model achieves results competitive to the word alignmen"
2021.eacl-main.30,D17-1130,0,0.0529413,"Missing"
2021.eacl-main.30,W13-2322,0,0.275064,"upervised and relies on the contextualized XLM-R word embeddings. We achieve a highly competitive performance that surpasses the best published results for German, Italian, Spanish and Chinese. 1 Figure 1: AMR graph for The boy wants to go and its German translation Der Junge will gehen. Implicit alignments between the English text and AMR concepts are denoted by dotted arrows. Explicit alignments between English and German texts are denoted by solid arrows. Introduction Abstract Meaning Representation graphs are rooted, labeled, directed, acyclic graphs representing sentence-level semantics (Banarescu et al., 2013). In the example shown in Figure 1, the sentence The boy wants to go is parsed into an AMR graph. The nodes of the AMR graph represent the AMR concepts, which may include normalized surface symbols e.g. boy, Propbank frames (Kingsbury and Palmer, 2002) e.g. want-01, go-02 as well as other AMR-specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignmen"
2021.eacl-main.30,P19-4007,0,0.0263166,"Missing"
2021.eacl-main.30,N18-1104,0,0.15062,"018; Chen and Palmer, 2017). A significant emphasis of this paper is on deriving these alignments for multilingual AMR parsers. Even though by nature AMR is biased towards English, recent work has evaluated the potential of AMR to work as an interlingua. Hajiˇc et al. (2014) and Xue et al. (2014) categorize and propose refinements for divergences in the annotation between English and Chinese as well as Czech AMRs. Anchiˆeta and Pardo (2018) import the corresponding AMR annotation for each sentence from the English annotated corpus and revisit the annotation to adapt it to Portuguese. However, Damonte and Cohen (2018) show that it may be possible to use the original AMR annotations devised for English as representation for equivalent sentences in other languages without any modification despite the translation divergence. This defines the problem of multilingual AMR parsing that we seek to address in this paper - given a sentence in a foreign language, recover the AMR graph originally designed for its English translation. We implement 394 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 394–404 April 19 - 23, 2021. ©2021 Association for Comp"
2021.eacl-main.30,2020.emnlp-main.195,0,0.440068,"lingual AMR parsers by projecting English AMR annotation to target foreign languages (German, Spanish, Italian and Chinese), but we depart from their approach in the specifics of the annotation projection by exploring contextual word alignments directly derived from multilingual contextualized word embeddings. While both procedures utilize parallel corpora, the annotation projection of Damonte and Cohen (2018) requires additional supervised training of their statistical word aligner. Our proposed contextualized word alignment is however unsupervised in nature. Alternatively, a recent study by Blloshmi et al. (2020) showed that one may in fact not need alignmentbased parsers for cross-lingual AMR, rather modelling concept identification as a seq2seq problem. In this paper, we will compare our results to both 395 Damonte and Cohen (2018) and Blloshmi et al. (2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervisi"
2021.eacl-main.30,N13-1073,0,0.239243,"l word aligner. Our proposed contextualized word alignment is however unsupervised in nature. Alternatively, a recent study by Blloshmi et al. (2020) showed that one may in fact not need alignmentbased parsers for cross-lingual AMR, rather modelling concept identification as a seq2seq problem. In this paper, we will compare our results to both 395 Damonte and Cohen (2018) and Blloshmi et al. (2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent body of work (Conne"
2021.eacl-main.30,J90-2002,0,0.741955,"nd Cohen (2018) requires additional supervised training of their statistical word aligner. Our proposed contextualized word alignment is however unsupervised in nature. Alternatively, a recent study by Blloshmi et al. (2020) showed that one may in fact not need alignmentbased parsers for cross-lingual AMR, rather modelling concept identification as a seq2seq problem. In this paper, we will compare our results to both 395 Damonte and Cohen (2018) and Blloshmi et al. (2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternatin"
2021.eacl-main.30,2020.findings-emnlp.89,1,0.874783,"thought of as an adaptation of their contextual word retrieval task. However, we refrain from any finetuning of the contextual embeddings and show that the contextual word alignments from the off-the-shelf XLM-R model achieves results competitive to the word alignments by fast-align (see Damonte and Cohen (2018)). This suggests the potential for inexpensive, massive scaling of AMR parsing up to 100 languages on which XLM-R is trained. 3 Annotation projection We adopt a transition-based parsing approach for AMR parsing following (Ballesteros and AlOnaizan, 2017; Naseem et al., 2019; Fernandez Astudillo et al., 2020). These produce an AMR graph g from an input sentence s by predicting instead an action sequence a from s as a sequence to sequence problem. This action sequence applied to a state machine M produces then the desired target graph as g = M (a, s). Transition-based parsers require the action sequence for each graph in the training data. This is determined by a rule-based oracle a = O(g, s) which relies on external word-to-node alignments. In all the subsequent experiments we will use the oracle and action set from (Fernandez Astudillo et al., 2020). 3.1 Projection method In order to train AMR pa"
2021.eacl-main.30,J93-2003,0,0.122903,"Missing"
2021.eacl-main.30,P14-1134,0,0.595198,"h may include normalized surface symbols e.g. boy, Propbank frames (Kingsbury and Palmer, 2002) e.g. want-01, go-02 as well as other AMR-specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignments are essential for training many of presentday AMR parsers, there have been various efforts to link the AMR concepts to their corresponding span of words (Flanigan et al., 2014; Pourdamghani ∗ This research was done during an internship at IBM Research AI. et al., 2014; Lyu and Titov, 2018; Chen and Palmer, 2017). A significant emphasis of this paper is on deriving these alignments for multilingual AMR parsers. Even though by nature AMR is biased towards English, recent work has evaluated the potential of AMR to work as an interlingua. Hajiˇc et al. (2014) and Xue et al. (2014) categorize and propose refinements for divergences in the annotation between English and Chinese as well as Czech AMRs. Anchiˆeta and Pardo (2018) import the corresponding AMR annotation for"
2021.eacl-main.30,P13-2131,0,0.642017,"r data pre-training (Konstas et al., 2017; van Noord and Bos, 2017) in AMR parsing and multi-lingual pre-training (Aharoni et al., 2019) in machine translation. Finally, we conduct a detailed error analysis of the multilingual AMR parsing. One of the major errors we have found involves synonymous concepts, which share the same meaning as the original concepts in English, but differ in spellings. While this error is mainly caused by the fact that the multilingual word embeddings bridge non-English input tokens to English concepts, it also highlights the highly lexical nature of Smatch scoring (Cai and Knight, 2013) which does not take synonymous concepts into consideration. We also elaborate upon error analysis of the direct comparison between our proposed annotation projection method using contextual word alignment and a previous baseline, using fast align. The rest of the paper is organized as follows: In Section 2, we discuss related work. In Section 3, we present our main proposal on annotation projection based on contextual word alignments. In Section 4, we describe various combination approaches that improve the multilingual parser performances significantly. These include combining word-toconcept"
2021.eacl-main.30,W14-5808,0,0.678758,"Missing"
2021.eacl-main.30,kingsbury-palmer-2002-treebank,0,0.516216,"German translation Der Junge will gehen. Implicit alignments between the English text and AMR concepts are denoted by dotted arrows. Explicit alignments between English and German texts are denoted by solid arrows. Introduction Abstract Meaning Representation graphs are rooted, labeled, directed, acyclic graphs representing sentence-level semantics (Banarescu et al., 2013). In the example shown in Figure 1, the sentence The boy wants to go is parsed into an AMR graph. The nodes of the AMR graph represent the AMR concepts, which may include normalized surface symbols e.g. boy, Propbank frames (Kingsbury and Palmer, 2002) e.g. want-01, go-02 as well as other AMR-specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignments are essential for training many of presentday AMR parsers, there have been various efforts to link the AMR concepts to their corresponding span of words (Flanigan et al., 2014; Pourdamghani ∗ This research was done during an internship at IBM Resear"
2021.eacl-main.30,E17-1053,0,0.0168677,"specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignments are essential for training many of presentday AMR parsers, there have been various efforts to link the AMR concepts to their corresponding span of words (Flanigan et al., 2014; Pourdamghani ∗ This research was done during an internship at IBM Research AI. et al., 2014; Lyu and Titov, 2018; Chen and Palmer, 2017). A significant emphasis of this paper is on deriving these alignments for multilingual AMR parsers. Even though by nature AMR is biased towards English, recent work has evaluated the potential of AMR to work as an interlingua. Hajiˇc et al. (2014) and Xue et al. (2014) categorize and propose refinements for divergences in the annotation between English and Chinese as well as Czech AMRs. Anchiˆeta and Pardo (2018) import the corresponding AMR annotation for each sentence from the English annotated corpus and revisit the annotation to adapt it to Portuguese. However, Damonte and Cohen (2018) sh"
2021.eacl-main.30,2020.findings-emnlp.288,1,0.775662,"ed from English treebank via annotation projection - we also experiment with combining all the target language treebanks to create a single multilingual treebank. We notice that pre-training an AMR parser on this multilingual treebank with subsequent finetuning on the treebank of each language, improves performance over the parser trained only on each individual treebank. 4.3 Human and synthetic treebank combination We create a synthetic AMR corpus by parsing 85k unlabeled sentences from the context portion of SQuAD-2.0. The resulting synthetic AMR graphs are filtered as per the procedure in (Lee et al., 2020) and combined with the AMR-2.0 training set (LDC2017T10), to produce an expanded AMR2.0 + SQuAD training dataset of 94k sentences. We then project annotations of this expanded English treebank onto each of the target languages, and train the corresponding target language parser. We observe that despite the lower quality of the synthetic AMRs as compared to their human-annotated counterparts, their inclusion in the training set significantly improves parser performance. 5 5.1 Experimental Results AMR Parser and Data For our experiments, we use the stack-Transformer model (Fernandez Astudillo et"
2021.eacl-main.30,W16-1702,0,0.0156732,"d work Multilingual AMR. There have been significant advances in AMR parsing for languages other than English. Previous studies (Hajiˇc et al., 2014; Xue et al., 2014; Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019) investigated AMR annotations for a variety of different languages such as Chinese, Czech, Spanish and Brazilian Portuguese. Vanderwende et al. (2015) automatically parse the logical representation for sentences in Spanish, Italian, German and Japanese, which is then converted to AMR using a small set of rules. While much of this work, along with studies such as Li et al. (2016); Anchiˆeta and Pardo (2018), produces AMR graphs whose nodes were labeled with words from the target language, Damonte and Cohen (2018) developed AMR parsers for English and used parallel corpora for annotation projection to train Italian, Spanish, German, and Chinese parsers that recover the AMR graph originally designed for the English translation. Their main results showed that the new parsers can overcome certain structural differences between languages. Similar to Damonte and Cohen (2018), we also train multilingual AMR parsers by projecting English AMR annotation to target foreign langu"
2021.eacl-main.30,P18-1037,0,0.183397,"as well as other AMR-specific constructs. Edges in an AMR graph represent the relations between concepts. In this example :arg0, :arg1 correspond to standard roles of Propbank. One distinctive aspect of AMR annotation is the lack of explicit alignments between nodes in the graph and words in the sentences. Since such alignments are essential for training many of presentday AMR parsers, there have been various efforts to link the AMR concepts to their corresponding span of words (Flanigan et al., 2014; Pourdamghani ∗ This research was done during an internship at IBM Research AI. et al., 2014; Lyu and Titov, 2018; Chen and Palmer, 2017). A significant emphasis of this paper is on deriving these alignments for multilingual AMR parsers. Even though by nature AMR is biased towards English, recent work has evaluated the potential of AMR to work as an interlingua. Hajiˇc et al. (2014) and Xue et al. (2014) categorize and propose refinements for divergences in the annotation between English and Chinese as well as Czech AMRs. Anchiˆeta and Pardo (2018) import the corresponding AMR annotation for each sentence from the English annotated corpus and revisit the annotation to adapt it to Portuguese. However, Dam"
2021.eacl-main.30,2020.acl-main.167,1,0.895687,"Missing"
2021.eacl-main.30,L18-1486,0,0.0239282,"Missing"
2021.eacl-main.30,P19-1451,1,0.851111,"n two parallel sentences may be thought of as an adaptation of their contextual word retrieval task. However, we refrain from any finetuning of the contextual embeddings and show that the contextual word alignments from the off-the-shelf XLM-R model achieves results competitive to the word alignments by fast-align (see Damonte and Cohen (2018)). This suggests the potential for inexpensive, massive scaling of AMR parsing up to 100 languages on which XLM-R is trained. 3 Annotation projection We adopt a transition-based parsing approach for AMR parsing following (Ballesteros and AlOnaizan, 2017; Naseem et al., 2019; Fernandez Astudillo et al., 2020). These produce an AMR graph g from an input sentence s by predicting instead an action sequence a from s as a sequence to sequence problem. This action sequence applied to a state machine M produces then the desired target graph as g = M (a, s). Transition-based parsers require the action sequence for each graph in the training data. This is determined by a rule-based oracle a = O(g, s) which relies on external word-to-node alignments. In all the subsequent experiments we will use the oracle and action set from (Fernandez Astudillo et al., 2020). 3.1 Project"
2021.eacl-main.30,J03-1002,0,0.0327283,"of their statistical word aligner. Our proposed contextualized word alignment is however unsupervised in nature. Alternatively, a recent study by Blloshmi et al. (2020) showed that one may in fact not need alignmentbased parsers for cross-lingual AMR, rather modelling concept identification as a seq2seq problem. In this paper, we will compare our results to both 395 Damonte and Cohen (2018) and Blloshmi et al. (2020). Word vector alignment techniques. Traditional word alignment methods often use parallel corpora and IBM alignment models (Brown et al., 1990, 1993) as well as improved versions (Och and Ney, 2003; Dyer et al., 2013). More recently, there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent"
2021.eacl-main.30,P19-1493,0,0.0665407,"Missing"
2021.eacl-main.30,D14-1048,0,0.234354,"del. We show that our proposed procedure achieves competitive results as some of the classical methods for text-to-AMR alignment. Furthermore, such a procedure is easily scalable to the 100 languages that XLM-R is trained on. We also combine different techniques for concept alignments and AMR parser training which significantly improve performance over the base models. For concept alignment, we combine the proposed contextual word alignments with previously established alignment techniques utilizing matching rules tailored to AMR as well as machine translation aligners (Flanigan et al., 2014; Pourdamghani et al., 2014). For AMR parser training, we pre-train an AMR parser on the treebanks of different languages simultaneously and subsequently finetune on each language. This is analogous to the techniques used for silver data pre-training (Konstas et al., 2017; van Noord and Bos, 2017) in AMR parsing and multi-lingual pre-training (Aharoni et al., 2019) in machine translation. Finally, we conduct a detailed error analysis of the multilingual AMR parsing. One of the major errors we have found involves synonymous concepts, which share the same meaning as the original concepts in English, but differ in spellings"
2021.eacl-main.30,N19-1162,0,0.0225805,"there have been an advent of techniques that align vector representation of words from varying levels of supervision (Ruder et al., 2019). Often word vectors are learned independently for each language and then a mapping from source language vectors to target language vectors with a bilingual dictionary is developed (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). To reduce the need for bilingual supervision, the iterative method of starting from a minimal seed dictionary and alternating with learning the linear map was employed by a recent body of work (Conneau et al., 2018; Schuster et al., 2019; Artetxe et al., 2018). The work most similar to ours is Cao et al. (2020) where the authors obtain contextual embedding alignments from multilingual BERT (Devlin et al., 2018; Pires et al., 2019) and subsequently improve the alignments via finetuning using supervised parallel corpora. Our contextual word alignment between two parallel sentences may be thought of as an adaptation of their contextual word retrieval task. However, we refrain from any finetuning of the contextual embeddings and show that the contextual word alignments from the off-the-shelf XLM-R model achieves results competiti"
2021.eacl-main.30,W19-4028,0,0.470697,"describe various combination approaches that improve the multilingual parser performances significantly. These include combining word-toconcept alignments, using multi-lingual treebanks and combining human-annotated and synthetic treebanks. In Section 5, we discuss experimental results. In Sections 6 and 7, we present detailed error analyses. We conclude the paper in Section 8. 2 Related work Multilingual AMR. There have been significant advances in AMR parsing for languages other than English. Previous studies (Hajiˇc et al., 2014; Xue et al., 2014; Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019) investigated AMR annotations for a variety of different languages such as Chinese, Czech, Spanish and Brazilian Portuguese. Vanderwende et al. (2015) automatically parse the logical representation for sentences in Spanish, Italian, German and Japanese, which is then converted to AMR using a small set of rules. While much of this work, along with studies such as Li et al. (2016); Anchiˆeta and Pardo (2018), produces AMR graphs whose nodes were labeled with words from the target language, Damonte and Cohen (2018) developed AMR parsers for English and used parallel corpora for annotation project"
2021.eacl-main.30,N15-3006,0,0.0166891,"ments, using multi-lingual treebanks and combining human-annotated and synthetic treebanks. In Section 5, we discuss experimental results. In Sections 6 and 7, we present detailed error analyses. We conclude the paper in Section 8. 2 Related work Multilingual AMR. There have been significant advances in AMR parsing for languages other than English. Previous studies (Hajiˇc et al., 2014; Xue et al., 2014; Migueles-Abraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019) investigated AMR annotations for a variety of different languages such as Chinese, Czech, Spanish and Brazilian Portuguese. Vanderwende et al. (2015) automatically parse the logical representation for sentences in Spanish, Italian, German and Japanese, which is then converted to AMR using a small set of rules. While much of this work, along with studies such as Li et al. (2016); Anchiˆeta and Pardo (2018), produces AMR graphs whose nodes were labeled with words from the target language, Damonte and Cohen (2018) developed AMR parsers for English and used parallel corpora for annotation projection to train Italian, Spanish, German, and Chinese parsers that recover the AMR graph originally designed for the English translation. Their main resu"
2021.eacl-main.30,xue-etal-2014-interlingua,0,0.600325,"Missing"
2021.emnlp-main.507,N19-1253,0,0.0173912,"structured approach that guarantees well-formed graphs and yields other desirable sub-products such as alignments. We show that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unattachable nodes and full recovery of all graphs. We are the first to explore transition-based p"
2021.emnlp-main.507,2020.findings-emnlp.89,1,0.411053,"desirable properties. There are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, predicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapanipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, generating the graph incrementally. They implicitly model graph structural constraints through transitions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequenceto-sequence (seq-to-seq) language models with the transition-based approach for AMR parsin"
2021.emnlp-main.507,D17-1130,0,0.0512146,"Missing"
2021.emnlp-main.507,W13-2322,0,0.0474374,"ion system with a small set of basic actions – a generalization of the action-pointer transition system of Zhou et al. (2021). We use BART (Lewis et al., 2019) as our pre-trained language model, since it has shown significant improvements in linearized AMR generation (Bevilacqua et al., 2021). Unlike previous approaches that directly fine-tune the model with The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the semantics of the sentence, with nodes representing concepts and edges representing their relations (Banarescu et al., 2013). Recent works utilizing pretrained encoder-decoder language models show great improvements in AMR parsing results (Xu et al., 2020; Bevilacqua et al., 2021). These approaches avoid explicit modeling of the graph structure. Instead, they directly predict the linearized AMR graph treated as free text. While the use 1 of pre-trained Transformer encoders is widely exWith the only exception being disconnected graphs, tended in AMR parsing, the use of pre-trained which happen infrequently in practice. 6279 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages"
2021.emnlp-main.507,P17-1112,0,0.0179208,"#8) works the best. 8 Related Work providing a more structured approach that guarantees well-formed graphs and yields other desirable sub-products such as alignments. We show that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unattachable nodes and full recovery of all graphs"
2021.emnlp-main.507,2020.acl-main.119,0,0.0183415,"termined cases. tions, but most high performing approaches (Cai Subgraph actions have been used in all transitionand Lam, 2020; Bevilacqua et al., 2021) utilize based AMR systems (Naseem et al., 2019; Asthe re-categorization described in Appendix A.1 of tudillo et al., 2020; Zhou et al., 2021). Zhang et al. (2019a). This version requires of an external Named Entity Recognition (NER) system Aside from NER, past AMR parsers have other to anonymize named entities, both at train time external dependencies such as POS taggers (Zhang and test time. It also makes use of look-up tables et al., 2019a; Cai and Lam, 2020) and lemmatizers for nominalizations (e.g. English to England) and (Cai and Lam, 2020; Naseem et al., 2019; Astudillo other hand-crafted rules. Graph re-categorization et al., 2020). 6280 like-01 ARG 1 ARG 0 ARG 0 person ARG 1-of trip-03 ARG 1 city name name employ-01 op1 Employees Employees Employees liked 1 person 2 employ-01 RA (1, ARG 1-of) liked liked their 5 SHIFT like-01 ROOT LA (1, ARG 0) SHIFT SHIFT Boston Boston 10 city 11 name Boston Boston 13 COPY SHIFT RA (10,name) RA (11,op1) trip trip 16 trip-03 SHIFT LA (10, ARG 1) RA (5, ARG 1) LA (1, ARG 0) Figure 2: From top to bottom: graph"
2021.emnlp-main.507,P13-2131,0,0.0241183,"ess is always guaranteed and no post-processing is needed to return valid graphs, unlike Xu et al. (2020); Bevilacqua et al. (2021). The only post-processing we use is to add wikification nodes as used in all previous parsers.5 5 Experimental Setup Datasets We evaluate our models on 3 AMR benchmark datasets, namely AMR 1.0 (LDC2014T12), AMR 2.0 (LDC2017T10), and AMR 3.0 (LDC2020T02). They have around 10K, 37K, and 56K sentence-AMR pairs for training, respectively.6 Both AMR 2.0 and AMR 3.0 have wikification nodes but AMR 1.0 does not. Evaluation We assess our models with S MATCH (F1) scores7 (Cai and Knight, 2013). We also report the fine-grained evaluation metrics (Damonte et al., 2016) to further investigate different aspects of parsing results, such as concept identification, entity recognition, re-entrancies, etc. Model Configuration We follow the original BART configuration (Lewis et al., 2019) and code. 8 We use the large model configuration as default, and also the base model for ablation studies. The pointer network is always tied with one head of the decoder top layer, and the pointer loss is added to the model cross-entropy loss with 1:1 ratio for training. Transition alignments are used to m"
2021.emnlp-main.507,2020.emnlp-main.413,0,0.0324443,"iques have become dominant in seman- the new state of the art for AMR 2.0. Our results tic parsing. Xu et al. (2020) proposed custom indicate that instead of simply converting the strucmulti-task pre-training and fine-tuning approach tured data into unstructured sequences to fit the for conventional Transformer models (Vaswani need of the pre-trained model, it is possible to efet al., 2017). The massively pre-trained transformer fectively re-purpose a generic pre-trained model BART (Lewis et al., 2019) was used for executable to a structure-aware one achieving strong perforsemantic parsing in Chen et al. (2020) and AMR mance. Similar principles can be applied to adapt parsing in Bevilacqua et al. (2021). The impor- other powerful pre-trained models such as T5 (Raftance of strongly pre-trained decoders seems also fel et al., 2019) and GPT-2 (Radford et al., 2019) for justified as BART gains popularity in various se- structured data predictions. It is worth exploring mantic generation tasks (Chen et al., 2020; Shi thoroughly the pros and cons of introducing strucet al., 2020). Our work aims at capitalizing on the ture to the model compared to removing structure outstanding performance shown by BART, w"
2021.emnlp-main.507,2020.findings-emnlp.288,1,0.836348,"Missing"
2021.emnlp-main.507,2020.acl-main.703,0,0.0789857,"Missing"
2021.emnlp-main.507,W17-6315,0,0.0247298,"m BART with structure-aware fine-tuning (#8) works the best. 8 Related Work providing a more structured approach that guarantees well-formed graphs and yields other desirable sub-products such as alignments. We show that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unatt"
2021.emnlp-main.507,2021.emnlp-main.714,0,0.0506102,"Missing"
2021.emnlp-main.507,P18-1037,0,0.0206507,"hown to pipelines that are hard to analyze and generalize hurt performance notably on the AMR 2.0 corpus. poorly. This situation has notably improved in the Subgraph actions (Ballesteros and Al-Onaizan, past few years but there are still two main sources 2017b) are used in transition-based systems and of complexity present in almost all recent parsers: play a role similar to re-categorization. Instead of graph re-categorization and subgraph actions. normalizing and reverting, transition-based parsers Graph re-categorization (Wang and Xue, 2017; apply a subgraph action that generates an entire Lyu and Titov, 2018; Zhang et al., 2019a) normal- subgraph at once. This subgraph action coincides with many of the subgraphs collapsed in izes the graph prior to learning. This includes re-categorization. Subgraph actions bring howjoining certain subgraphs such as entities, dates ever fewer external dependencies, since the parser and other constructs into single nodes, removing special types of nodes like polarity and normal- learns to segment and identify subgraphs during training. They still suffer however from data sparizing propbank names. An example of common sity since some subgraphs appear very few times"
2021.emnlp-main.507,P19-1451,1,0.907229,"bles for nominalization and similar pens in this re-categorized space. Re-categorized constructs that hinder generalization. Furthermore, graphs are expanded to normal valid AMR graphs they create the problem of unattachable nodes. This in a post-processing stage. The type and number of subgraphs normalized vary across implementa- was addressed in Zhou et al. (2021) by ignoring subgraphs for a set of heuristically determined cases. tions, but most high performing approaches (Cai Subgraph actions have been used in all transitionand Lam, 2020; Bevilacqua et al., 2021) utilize based AMR systems (Naseem et al., 2019; Asthe re-categorization described in Appendix A.1 of tudillo et al., 2020; Zhou et al., 2021). Zhang et al. (2019a). This version requires of an external Named Entity Recognition (NER) system Aside from NER, past AMR parsers have other to anonymize named entities, both at train time external dependencies such as POS taggers (Zhang and test time. It also makes use of look-up tables et al., 2019a; Cai and Lam, 2020) and lemmatizers for nominalizations (e.g. English to England) and (Cai and Lam, 2020; Naseem et al., 2019; Astudillo other hand-crafted rules. Graph re-categorization et al., 2020)"
2021.emnlp-main.507,N19-4009,0,0.0242225,"n of 4 steps. Learning rate is 1e−4 with 4000 warm-up steps using the inversesqrt scheduling scheme (Vaswani et al., 2017). The hyper-parameters are fixed and not tuned for different models and datasets, as we found results are not sensitive within small ranges. We train sep-voc models for 100 epochs and joint-voc models for 40 epochs as the latter is found to converge faster. The best 5 checkpoints based on development set S MATCH from greedy decoding are averaged, and default beam size of 10 is used for decoding for our final parsing scores. We implement our model9 with the FAIRSEQ toolkit (Ott et al., 2019). More details can be found in the Appendix. 6 Results Main Results We present parsing performances of our model (StructBART) in comparison with previous approaches in Table 1. For each model, we also list its features such as utilization of pre9 Code and model available at https://github.com/ IBM/transition-amr-parser. 6284 Features Transition System Astudillo et al. (2020) Zhou et al. (2021) Ours Model Results on AMR 2.0 APT∗ Model Results on AMR 3.0 #Base Actions Distant Edges Special Subgraph (Zhou et al., 2021) StructBART sep-voc APT∗ (Zhou et al., 2021) StructBART sep-voc 12 10 6 S WAP p"
2021.emnlp-main.507,2021.acl-long.289,1,0.692068,"that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unattachable nodes and full recovery of all graphs. We are the first to explore transition-based parsing applied on fine-tuning strongly pre-trained seq-toseq models, and we demonstrate that parser state encoding is sti"
2021.emnlp-main.507,D18-1548,0,0.0542695,"Missing"
2021.emnlp-main.507,D18-1086,0,0.0524736,"Missing"
2021.emnlp-main.507,D17-1129,0,0.0203499,", resulting in complex sults without re-categorization, but this is shown to pipelines that are hard to analyze and generalize hurt performance notably on the AMR 2.0 corpus. poorly. This situation has notably improved in the Subgraph actions (Ballesteros and Al-Onaizan, past few years but there are still two main sources 2017b) are used in transition-based systems and of complexity present in almost all recent parsers: play a role similar to re-categorization. Instead of graph re-categorization and subgraph actions. normalizing and reverting, transition-based parsers Graph re-categorization (Wang and Xue, 2017; apply a subgraph action that generates an entire Lyu and Titov, 2018; Zhang et al., 2019a) normal- subgraph at once. This subgraph action coincides with many of the subgraphs collapsed in izes the graph prior to learning. This includes re-categorization. Subgraph actions bring howjoining certain subgraphs such as entities, dates ever fewer external dependencies, since the parser and other constructs into single nodes, removing special types of nodes like polarity and normal- learns to segment and identify subgraphs during training. They still suffer however from data sparizing propbank names"
2021.emnlp-main.507,N15-1040,0,0.0245526,"et al., 2021). These approaches however lack certain desirable properties. There are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, predicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapanipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, generating the graph incrementally. They implicitly model graph structural constraints through transitions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequenceto-sequence (seq-to-seq) langua"
2021.emnlp-main.507,2020.emnlp-main.196,0,0.319656,"(Lewis et al., 2019) as our pre-trained language model, since it has shown significant improvements in linearized AMR generation (Bevilacqua et al., 2021). Unlike previous approaches that directly fine-tune the model with The task of Abstract Meaning Representation (AMR) parsing translates a natural sentence into a rooted directed acyclic graph capturing the semantics of the sentence, with nodes representing concepts and edges representing their relations (Banarescu et al., 2013). Recent works utilizing pretrained encoder-decoder language models show great improvements in AMR parsing results (Xu et al., 2020; Bevilacqua et al., 2021). These approaches avoid explicit modeling of the graph structure. Instead, they directly predict the linearized AMR graph treated as free text. While the use 1 of pre-trained Transformer encoders is widely exWith the only exception being disconnected graphs, tended in AMR parsing, the use of pre-trained which happen infrequently in practice. 6279 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6279–6290 c November 7–11, 2021. 2021 Association for Computational Linguistics have-03 linearized graphs, we modify the model str"
2021.emnlp-main.507,P19-1009,0,0.0356805,"Missing"
2021.emnlp-main.507,D19-1392,0,0.032939,"Missing"
2021.emnlp-main.507,D17-1175,0,0.0254667,"-aware fine-tuning (#8) works the best. 8 Related Work providing a more structured approach that guarantees well-formed graphs and yields other desirable sub-products such as alignments. We show that this is not only possible but also attains state-of-the art parsing results without graph re-categorization. Our analysis also shows that contrary to Xu et al. (2020), vocabulary sharing is not necessary for strong performance for our structural fine-tuning. Encoding of the parser state into neural parsers has been undertaken in various works, including seq-to-seq RNN models (Liu and Zhang, 2017; Zhang et al., 2017; Buys and Blunsom, 2017), encoder-only Transformers (Ahmad et al., 2019), seq-to-seq Transformers (Astudillo et al., 2020; Zhou et al., 2021) and pre-trained language models (Qian et al., 2021). Here we explore the application of these approaches to pre-trained seq-to-seq Transformers. Borrowing ideas from Zhou et al. (2021), we encode alignment states into the pre-trained BART attention mechanism, and re-purpose its selfattention as a pointer network. We also rely on a minimal set of actions targeted to utilize BART’s generation with desirable guarantees, such as no unattachable nodes and fu"
2021.emnlp-main.507,2021.naacl-main.443,1,0.0878221,"ere are no structural guarantees of graph well-formedness, i.e. the model may predict strings that can not be decoded into valid graphs, and post-processing is required. Furthermore, predicting AMR linearizations ignores the implicit alignments between graph nodes and words, which provide a strong inductive bias and are useful for downstream AMR applications (Mitra and Baral, 2016; Liu et al., 2018; Vlachos et al., 2018; Kapanipathi et al., 2021; Naseem et al., 2021). On the other hand, transition-based AMR parsers (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017a; Astudillo et al., 2020; Zhou et al., 2021) operate over the tokens of the input sentence, generating the graph incrementally. They implicitly model graph structural constraints through transitions and yield alignments by construction, thus guaranteeing graph well-formedness.1 However, it remains unclear whether explicit modeling of structure is still beneficial for AMR parsing in the presence of powerful pre-trained language models and their strong free text generation abilities. In this work, we integrate pre-trained sequenceto-sequence (seq-to-seq) language models with the transition-based approach for AMR parsing, and explore to wh"
2021.findings-acl.339,2020.findings-emnlp.89,1,0.729463,"d OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate variables 3885 that behave as secondary unknowns. In this case, a movie produced by Benicio del Toro in Spain. NSQA utilizes a stack-Transformer transitionbased model (Naseem et al., 2019; Astudillo et al., 2020) for AMR parsing. An advantage of transition-based systems is that they provide explicit question text to AMR node alignments. This allows encoding closely integrated text and AMR input to multiple modules (Entity Linking and Relation Linking) that can benefit from this joint input. 2.2 AMR to KG Logic The core contribution of this work is our next step where the AMR of the question is transformed to a query graph aligned with the underlying knowledge graph. We formalize the two graphs as follows: AMR graph G is a rooted edge-labeled directed acyclic graph hVG , EG i. The edge set EG consists"
2021.findings-acl.339,W13-2322,0,0.0612546,"Missing"
2021.findings-acl.339,D13-1160,0,0.0607099,"f the football world cup 2018? Table 4: Question types supported by NSQA , with examples from QALD Falcon NMD+BLINK Dataset QALD-9 QALD-9 P 0.81 0.82 R 0.83 0.90 F1 0.82 0.85 Falcon NMD+BLINK LC-QuAD 1.0 LC-QuAD 1.0 0.56 0.87 0.69 0.86 0.62 0.86 Table 5: Performance of Entity Linking modules compared to SOTA Falcon on our dev sets we intend to explore more uses of such reasoners for KBQA in the future. 4 Related Work Early work in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts"
2021.findings-acl.339,P13-2131,0,0.0801353,"Missing"
2021.findings-acl.339,dorr-etal-1998-thematic,0,0.326306,"Missing"
2021.findings-acl.339,kingsbury-palmer-2002-treebank,0,0.0480491,"(AMR) graph; (ii) transforms the AMR graph to a set of candidate KB-aligned logical queries, via a novel but simple graph transformation approach; (iii) uses a Logical Neural Network (LNN) (Riegel et al., 2020) to reason over KB facts and produce answers to KB-aligned logical queries. We describe each of these modules in the following sections. 2.1 AMR Parsing NSQA utilizes AMR parsing to reduce the complexity and noise of natural language questions. An AMR parse is a rooted, directed, acyclic graph. AMR nodes represent concepts, which may include normalized surface symbols, Propbank frames (Kingsbury and Palmer, 2002) as well as other AMR-specific constructs to handle named entities, quantities, dates and other phenomena. Edges in an AMR graph represent the relations between concepts such as standard OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate vari"
2021.findings-acl.339,2020.findings-emnlp.288,1,0.824748,"Missing"
2021.findings-acl.339,P19-1451,1,0.842549,"cepts such as standard OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate variables 3885 that behave as secondary unknowns. In this case, a movie produced by Benicio del Toro in Spain. NSQA utilizes a stack-Transformer transitionbased model (Naseem et al., 2019; Astudillo et al., 2020) for AMR parsing. An advantage of transition-based systems is that they provide explicit question text to AMR node alignments. This allows encoding closely integrated text and AMR input to multiple modules (Entity Linking and Relation Linking) that can benefit from this joint input. 2.2 AMR to KG Logic The core contribution of this work is our next step where the AMR of the question is transformed to a query graph aligned with the underlying knowledge graph. We formalize the two graphs as follows: AMR graph G is a rooted edge-labeled directed acyclic graph hVG , EG i."
2021.findings-acl.339,N19-1243,0,0.0615569,"Missing"
2021.findings-acl.339,D07-1071,0,0.057132,"l start [sic] the final match of the football world cup 2018? Table 4: Question types supported by NSQA , with examples from QALD Falcon NMD+BLINK Dataset QALD-9 QALD-9 P 0.81 0.82 R 0.83 0.90 F1 0.82 0.85 Falcon NMD+BLINK LC-QuAD 1.0 LC-QuAD 1.0 0.56 0.87 0.69 0.86 0.62 0.86 Table 5: Performance of Entity Linking modules compared to SOTA Falcon on our dev sets we intend to explore more uses of such reasoners for KBQA in the future. 4 Related Work Early work in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to re"
2021.findings-acl.339,N19-1301,0,0.0169546,"proaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions have also been developed. Some have primarily focused on generating SPARQL sketches (Maheshwari et al., 2019; Chen et al., 2020) where they evaluate these sketches (2-hop) by providing gold entities and ignoring the evaluation of selecting target variables or other aggregation functions like sorting and counting. (Zheng and Zhang, 2019) generates the question subgrap"
2021.findings-acl.339,D18-1110,1,0.837164,"ually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions have also been developed. Some have primarily focused on generating SPARQL sketches (Maheshwari et al., 2019; Chen et al., 2020) where they evaluate these sketches (2-hop) by providing gold entities and ignoring the evaluation of selecting target variables or other aggregation functions like sorting and counting. (Zheng and Zhang, 2019) generates the question subgraph via filling the entity and relationship slots of 12 predefined question template. Their performance on th"
2021.findings-acl.339,P14-1090,0,0.0858204,"Missing"
2021.findings-acl.339,P17-1053,1,0.839108,"rk in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions"
C10-1062,A97-1024,0,0.0490301,"o predict readability: syntactic features, language-model features, and lexical features, as described below. 5.1 We want to answer the question whether a machine can accurately estimate readability as judged by a human. Therefore, we built a machine-learning system that predicts the readFeatures Based on Syntax Many times, a document is found to be unreadable due to unusual linguistic constructs or ungram548 1 http://www.cs.waikato.ac.nz/ml/weka/ matical language that tend to manifest themselves in the syntactic properties of the text. Therefore, syntactic features have been previously used (Bernth, 1997) to gauge the “clarity” of written text, with the goal of helping writers improve their writing skills. Here too, we use several features based on syntactic analyses. Syntactic analyses are obtained from the Sundance shallow parser (Riloff and Phillips, 2004) and from the English Slot Grammar (ESG) (McCord, 1989). Sundance features: The Sundance system is a rule-based system that performs a shallow syntactic analysis of text. We expect that this analysis over readable text would be “well-formed”, adhering to grammatical rules of the English language. Deviations from these rules can be indicati"
C10-1062,N04-1025,0,0.76587,"ability. The evaluation was then designed to compare how well machine and naive human judges predict expert human judgements. In order to make the machine’s predicted score comparable to a human judge’s score (details about our evaluation metrics are in Section 6.1), we also restricted the machine scores to integers. Hence, the task is to predict an integer score from 1 to 5 that measures the readability of the document. This task could be modeled as a multi-class classification problem treating each integer score as a separate class, as done in some of the previous work (Si and Callan, 2001; Collins-Thompson and Callan, 2004). However, since the classes are numerical and not unrelated (for example, the score 2 is in between scores 1 and 3), we decided to model the task as a regression problem and then round the predicted score to obtain the closest integer value. Preliminary results verified that regression performed better than classification. Heilman et al. (2008) also found that it is better to treat the readability scores as ordinal than as nominal. We take the average of the expert judge scores for each document as its goldstandard score. Regression was also used by Kanungo and Orr (2009), although their eval"
C10-1062,N07-1058,0,0.796875,"he documents. Some later methods use pre-determined lists of words to determine the grade level of a document, for example the Lexile measure (Stenner et al., 1988), the Fry Short Passage measure (Fry, 1990) and the Revised Dale-Chall formula (Chall and Dale, 1995). The word lists these methods use may be thought of as very simple language models. More recently, language models have been used for predicting the grade level of documents. Si and Callan (2001) and CollinsThompson and Callan (2004) train unigram language models to predict grade levels of documents. In addition to language models, Heilman et al. (2007) and Schwarm and Ostendorf (2005) also use some syntactic features to estimate the grade level of texts. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Their system predicts readability of texts from Wall Street Journal using lexical, syntactic and discourse features. Kanungo and Orr (2009) consider the task of predicting readability of web summary snippets produced by search engines. Using simple surface level features like the number of characters and syllables per word, capitalization, punctuation, ellipses etc. they train a re"
C10-1062,P05-1065,0,0.720084,"ethods use pre-determined lists of words to determine the grade level of a document, for example the Lexile measure (Stenner et al., 1988), the Fry Short Passage measure (Fry, 1990) and the Revised Dale-Chall formula (Chall and Dale, 1995). The word lists these methods use may be thought of as very simple language models. More recently, language models have been used for predicting the grade level of documents. Si and Callan (2001) and CollinsThompson and Callan (2004) train unigram language models to predict grade levels of documents. In addition to language models, Heilman et al. (2007) and Schwarm and Ostendorf (2005) also use some syntactic features to estimate the grade level of texts. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Their system predicts readability of texts from Wall Street Journal using lexical, syntactic and discourse features. Kanungo and Orr (2009) consider the task of predicting readability of web summary snippets produced by search engines. Using simple surface level features like the number of characters and syllables per word, capitalization, punctuation, ellipses etc. they train a regression model to predict readabi"
C10-1062,P96-1041,0,0.0157412,"ed features in categorizing text (McCallum and Nigam, 1998; Yang and Liu, 1999) and evaluating readability (Collins-Thompson and Callan, 2004; Heilman et al., 2007) has been investigated in previous work. In our experiments, however, since documents were acquired through several different channels, such as machine translation or web logs, 549 we also build models that try to predict the genre of a document. Since the genre information for many English documents is readily available, we trained a series of genre-specific 5-gram LMs using the modified Kneser-Ney smoothing (Kneser and Ney, 1995; Stanley and Goodman, 1996). Table 1 contains a list of a base LM and genrespecific LMs. Given a document D consisting of tokenized word sequence {wi : i = 1, 2, · · · , |D|}, its perplexity L(D|Mj ) with respect to a LM Mj is computed as: ¢ ¡ 1 P|D| − |D| i=1 log P (wi |hi ;Mj ) , (2) L(D|Mj ) = e where |D |is the number of words in D and hi are the history words for wi , and P (wi |hi ; Mj ) is the probability Mj assigns to wi , when it follows the history words hi . Posterior perplexities from genre-specific language models: While perplexities computed from genre-specific LMs reflect the absolute probability that a d"
C10-1062,W08-0909,0,0.545949,"score from 1 to 5 that measures the readability of the document. This task could be modeled as a multi-class classification problem treating each integer score as a separate class, as done in some of the previous work (Si and Callan, 2001; Collins-Thompson and Callan, 2004). However, since the classes are numerical and not unrelated (for example, the score 2 is in between scores 1 and 3), we decided to model the task as a regression problem and then round the predicted score to obtain the closest integer value. Preliminary results verified that regression performed better than classification. Heilman et al. (2008) also found that it is better to treat the readability scores as ordinal than as nominal. We take the average of the expert judge scores for each document as its goldstandard score. Regression was also used by Kanungo and Orr (2009), although their evaluation did not constrain machine scores to be integers. We tested several regression algorithms available in the Weka1 machine learning package, and in Section 6.2 we report results for several which performed best. The next section describes the numerically-valued features that we used as input for regression. 5 Features for Predicting Readabil"
C10-1062,W02-1028,0,0.00647861,"retations. Sometimes ESG’s grammar rules fail to produce a single complete interpretation of a sentence, in which case it generates partial parses. This typically happens in cases when sentences are ungrammatical, and possibly, less readable. Thus, we use the proportion of such incomplete parses within a document as a readability feature. In case of extremely short documents, this proportion of incomplete parses can be misleading. To account for such short documents, we introduce a variation of the above incomplete parse feature, by weighting it with a log factor as was done in (Riloff, 1996; Thelen and Riloff, 2002). We also experimented with some other syntactic features such as average sentence parse scores from Stanford parser and an in-house maximum entropy statistical parer, average constituent scores etc., however, they slightly degraded the performance in combination with the rest of the features and hence we did not include them in the final set. One possible explanation could be that averaging diminishes the effect of low scores caused by ungrammaticality. 5.2 Features Based on Language Models A probabilistic language model provides a prediction of how likely a given sentence was generated by th"
C10-1062,D08-1020,0,0.778707,"enner et al., 1988), the Fry Short Passage measure (Fry, 1990) and the Revised Dale-Chall formula (Chall and Dale, 1995). The word lists these methods use may be thought of as very simple language models. More recently, language models have been used for predicting the grade level of documents. Si and Callan (2001) and CollinsThompson and Callan (2004) train unigram language models to predict grade levels of documents. In addition to language models, Heilman et al. (2007) and Schwarm and Ostendorf (2005) also use some syntactic features to estimate the grade level of texts. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Their system predicts readability of texts from Wall Street Journal using lexical, syntactic and discourse features. Kanungo and Orr (2009) consider the task of predicting readability of web summary snippets produced by search engines. Using simple surface level features like the number of characters and syllables per word, capitalization, punctuation, ellipses etc. they train a regression model to predict readability values. Our work differs from this previous research in several ways. Firstly, the task we h"
D10-1033,A97-1029,0,0.0257025,"rror-trade-off (DET) (Martin et al., 1997) analysis, in addition to traditional precision/recall/F measure. This paper is organized as follows. Section 2 discusses previous work. Section 3 describes the baseline maximum-entropy-based MD system. Section 4 introduces enhancements to the system to achieve robustness. Section 5 describes databases used for experiments, which are discussed in Section 6, and Section 7 draws conclusions and plots future work. 2 Previous work on mention detection The MD task has close ties to named-entity recognition, which has been the focus of much recent research (Bikel et al., 1997; Borthwick et al., 1998; Tjong Kim Sang, 2002; Florian et al., 2003; Benajiba et al., 2009), and has been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computationallinguistics literature, a named entity represents an instance of either a location, a person, an organization, and the named-entity-recognition task consists of identifying each individual occurrence of names of such an entity appearing in the text. As stated earlier, in this paper we are interested in identification and classification of textual references to object/abstractio"
D10-1033,W98-1118,0,0.0589296,"(Martin et al., 1997) analysis, in addition to traditional precision/recall/F measure. This paper is organized as follows. Section 2 discusses previous work. Section 3 describes the baseline maximum-entropy-based MD system. Section 4 introduces enhancements to the system to achieve robustness. Section 5 describes databases used for experiments, which are discussed in Section 6, and Section 7 draws conclusions and plots future work. 2 Previous work on mention detection The MD task has close ties to named-entity recognition, which has been the focus of much recent research (Bikel et al., 1997; Borthwick et al., 1998; Tjong Kim Sang, 2002; Florian et al., 2003; Benajiba et al., 2009), and has been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computationallinguistics literature, a named entity represents an instance of either a location, a person, an organization, and the named-entity-recognition task consists of identifying each individual occurrence of names of such an entity appearing in the text. As stated earlier, in this paper we are interested in identification and classification of textual references to object/abstraction mentions, which can be"
D10-1033,J92-1002,0,0.049749,"genres of data, including machine-translation output, informal communications, mixed-language material, varied forms of non-standard database mark-up, etc. We somewhatarbitrarily choose to employ three classifiers as described below. We select a classifier based on a sentence-level determination of the material’s fit to the target language. First, we build an n-gram language model on clean target-language training text. This language model is used to compute the perplexity (P P ) of each sentence during decoding. The P P indicates the quality of the text in the targetlanguage (i.e. English) (Brown et al., 1992); the lower the P P , the cleaner the text. A sentence with a P P lower than a threshold θ1 is considered “clean” and hence the “clean” baseline MD model described in Section 3 is used to detect mentions of this sentence. The clean MD model has access to standard features described in Section 3.1. In the case where a sentence looks particularly badly matched to the target language, defined as P P &gt; θ2 , we use a “gazetteer-based” model based on a dictionary look-up to detect mentions; we retreat to seeking known mentions in a context-independent manner reflecting that most of the context consi"
D10-1033,W03-0425,1,0.82552,"Missing"
D10-1033,N04-1001,0,0.135199,"t a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. We also assign to every nonoutside label a class to specify entity type e.g. person, organization, location, etc. We are interested in a statistical approach that can easily be adapted for several languages and that has the ability to integrate easily and make effective use of diverse sources of information to achieve high system performance. This is because, similar to many NLP tasks, good performance has been shown to depend heavily on integrating many sources of information (Florian et al., 2004). We choose a Maximum Entropy Markov Model (MEMM) as described previously (Florian et al., 2004; Zitouni and Florian, 2009). The maximum-entropy model is trained using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002), and it uses a Gaussian prior for regularization (Chen and Rosenfeld, 2000)1 . 3.1 Mention detection: standard features The featues used by our mention detection systems can be divided into the following categories: 1. Lexical Features Lexical features are implemented as token n-grams spanning the current token, both preceding and followin"
D10-1033,P06-1060,1,0.70183,"t rather is a maximum-a-posteriori model. 338 The gazetteers consist of several class of dictionaries: including person names, country names, company names, etc. Dictionaries contain single names such as John or Boston, and also phrases such as Barack Obama, New York City, or The United States. During both training and decoding, when we encounter in the text a token or a sequence of tokens that completely matches an entry in a dictionary, we fire its corresponding class. The use of this framework to build MD systems for clean English text has given very competitive results at ACE evaluations (Florian et al., 2006). Trying other classifiers is always a good experiment, which we didn’t pursue here for two reasons: first, the MEMM system used here is state-of-the-art, as proven in evaluations and competitions – while it is entirely possible that another system might get better results, we don’t think the difference would be large. Second, we are interested in ways of improving performance on noisy data, and we expect any system to observe similar degradation in performance when presented with unexpected input – showing results for multiple classifier types might very well dilute the message, so we stuck t"
D10-1033,P02-1002,0,0.0149425,"approach that can easily be adapted for several languages and that has the ability to integrate easily and make effective use of diverse sources of information to achieve high system performance. This is because, similar to many NLP tasks, good performance has been shown to depend heavily on integrating many sources of information (Florian et al., 2004). We choose a Maximum Entropy Markov Model (MEMM) as described previously (Florian et al., 2004; Zitouni and Florian, 2009). The maximum-entropy model is trained using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002), and it uses a Gaussian prior for regularization (Chen and Rosenfeld, 2000)1 . 3.1 Mention detection: standard features The featues used by our mention detection systems can be divided into the following categories: 1. Lexical Features Lexical features are implemented as token n-grams spanning the current token, both preceding and following it. For a token xi , token n-gram features will contain the previous n−1 tokens (xi−n+1 , . . . xi−1 ) and the following n − 1 tokens (xi+1 , . . . xi+n−1 ). Setting n equal to 3 turned out to be a good choice. 2. Gazetteer-based Features The gazetteerbase"
D10-1033,A00-1044,0,0.0348788,"ch can be either named, nominal or pronominal. This task has been a focus of interest in ACE since 2003. The recent ACE evaluation campaign was in 2008. Effort to handle noisy data is still limited, especially for scenarios in which the system at decoding time does not have prior knowledge of the input data source. Previous work dealing with unstructured data assumes the knowledge of the input data source. As an example, E. Minkov et al. (Minkov et al., 2005) assume that the input data is text from e-mails, and define special features to enhance the detection of named entities. Miller et al. (Miller et al., 2000) assume that the input data is the output of a speech or optical character recognition system, and hence extract new features for better named-entity recognition. In a different research problem, L. Yi et al. eliminate the noisy text from the document before performing data mining (Yi et al., 2003). Hence, they do not try to process noisy data; instead, they remove it. The approach we propose in this paper does not assume prior knowledge of the data source. Also we do not want to eliminate the noisy data, but rather attempt to detect the appropriate mentions, if any, that appear in that portio"
D10-1033,H05-1056,0,0.0173144,"n the text. As stated earlier, in this paper we are interested in identification and classification of textual references to object/abstraction mentions, which can be either named, nominal or pronominal. This task has been a focus of interest in ACE since 2003. The recent ACE evaluation campaign was in 2008. Effort to handle noisy data is still limited, especially for scenarios in which the system at decoding time does not have prior knowledge of the input data source. Previous work dealing with unstructured data assumes the knowledge of the input data source. As an example, E. Minkov et al. (Minkov et al., 2005) assume that the input data is text from e-mails, and define special features to enhance the detection of named entities. Miller et al. (Miller et al., 2000) assume that the input data is the output of a speech or optical character recognition system, and hence extract new features for better named-entity recognition. In a different research problem, L. Yi et al. eliminate the noisy text from the document before performing data mining (Yi et al., 2003). Hence, they do not try to process noisy data; instead, they remove it. The approach we propose in this paper does not assume prior knowledge o"
D10-1033,W02-2024,0,0.195402,"n addition to traditional precision/recall/F measure. This paper is organized as follows. Section 2 discusses previous work. Section 3 describes the baseline maximum-entropy-based MD system. Section 4 introduces enhancements to the system to achieve robustness. Section 5 describes databases used for experiments, which are discussed in Section 6, and Section 7 draws conclusions and plots future work. 2 Previous work on mention detection The MD task has close ties to named-entity recognition, which has been the focus of much recent research (Bikel et al., 1997; Borthwick et al., 1998; Tjong Kim Sang, 2002; Florian et al., 2003; Benajiba et al., 2009), and has been at the center of several evaluations: MUC-6, MUC-7, CoNLL’02 and CoNLL’03 shared tasks. Usually, in computationallinguistics literature, a named entity represents an instance of either a location, a person, an organization, and the named-entity-recognition task consists of identifying each individual occurrence of names of such an entity appearing in the text. As stated earlier, in this paper we are interested in identification and classification of textual references to object/abstraction mentions, which can be either named, nominal"
D10-1033,D08-1063,1,0.868288,"Section 3.1. The classifier for “mixed”-quality data and the “gazetteer” model were each trained on that set plus the “Latin” training set and the supplemental set. In addition, “mixed” training included the additional features described in Section 4.5. The framework used to build the baseline MD system is similar to the one we used in the ACE evaluation2 . This system has achieved competitive results with an F -measure of 82.7 when trained on the seven main types of ACE data with access to wordnet and part-of-speech-tag information as well as output of other MD and named-entity recognizers (Zitouni and Florian, 2008). It is instructive to evaluate on the individual component systems as well as the combination, despite the fact that the individual components are not wellsuited to all the data sets, for example, the mixed and gazetteer systems being a poorer fit to the English task than the baseline, and vice versa for the 2 NIST’s ACE evaluation http://www.nist.gov/speech/tests/ace/index.htm plan: age animal award cardinal disease event event-award event-communication event-crime event-custody event-demonstration event-disaster event-legal event-meeting event-performance event-personnel event-sports event-"
D10-1033,W03-0419,0,\N,Missing
D11-1082,H05-1024,0,0.0195863,"al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and a move selector. The move generator perturbs an existing alignment A in order to create a set of candidate alig"
D11-1082,D10-1097,0,0.0144158,"s training data. Word alignments appear as hidden variables in IBM Models 15 (Brown et al., 1993) in order to bridge a gap between the sentence-level granularity that is explicit in the training data, and the implicit word-level correspondence that is needed to statistically model lexical ambiguity and word order rearrangements that are inherent in the translation process. Other notable applications of word alignments include crosslanguage projection of linguistic analyzers (such as POS taggers and named entity detectors,) a subject which continues to be of interest. (Yarowsky et al., 2001), (Benajiba and Zitouni, 2010) The structure of the alignment model is tightly linked to the task of finding the optimal alignment. Many alignment models are factorized in order to use dynamic programming and beam search for efficient marginalization and search. Such a factorization encourages - but does not require - a sequential (often left-to-right) decoding order. If left-to-right decoding is adopted (and exact dynamic programming is intractable) important right context may exist beyond the search window. For example, the linkage of an English determiner may be considered before the linkage of a distant head noun. An a"
D11-1082,P06-1009,0,0.0862178,"target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). 3 Joint Models An alternate parameterization of alignment is the alignment matrix (Niehues and Vogel, 2008). For a source sentence F consisting of words f1 ...fm , and a target sentence E = e1 ...el , the alignment matrix A = {σij } is an l × m matrix of binary variables. If σij = 1, then ei is said to be linked to fj . If ei is unlinked then σij = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matrix allows some alignments that cannot be modeled by the sequence parameterization. All 2lm binary matrices are po"
D11-1082,J93-2003,0,0.037353,"e an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian. 1 Introduction Word-level alignments of parallel text are crucial for enabling machine learning algorithms to fully utilize parallel corpora as training data. Word alignments appear as hidden variables in IBM Models 15 (Brown et al., 1993) in order to bridge a gap between the sentence-level granularity that is explicit in the training data, and the implicit word-level correspondence that is needed to statistically model lexical ambiguity and word order rearrangements that are inherent in the translation process. Other notable applications of word alignments include crosslanguage projection of linguistic analyzers (such as POS taggers and named entity detectors,) a subject which continues to be of interest. (Yarowsky et al., 2001), (Benajiba and Zitouni, 2010) The structure of the alignment model is tightly linked to the task of"
D11-1082,P09-2058,0,0.0149371,"ction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These mod"
D11-1082,W08-0306,0,0.018839,"we restrict ourselves to a very simple move generator that changes the linkage of exactly one source word at a time, or exactly one target word at a time. Many of our corrections are similar to those of (Setiawan et al., 2010), although our motivation is perhaps closer to (Brown et al., 1993), who used similar perturbations to approximate intractable sums that arise when estimating the parameters of the generative models Models 3-5, and approach refined in (Och and Ney, 2003). We note that our corrections are designed to improve even a highquality starting alignment; in contrast the model of (Fossum et al., 2008) considers deletion of links from an initial alignment (union of aligners) that is likely to overproduce links. From the point of view of the alignment matrix, we consider changes to one row or one column (generically, one slice) of the alignment matrix. At each step t, the move set Mt (At ) is formed by choosing a slice of the current alignment matrix At , and generating all possible alignments from a few families of moves. Then the move generator picks another slice and repeats. The m + l slices are cycled in a fixed order: the first m slices correspond to source words (ordered according to"
D11-1082,J07-3002,0,0.0456387,"ks are more numerous than 2−1 links, in both language pairs. This is consequence of the choice of directionality and tokenization style. 6.5 Translation Impact We tested the impact of improved alignments on the performance of a phrase-based translation system (Ittycheriah and Roukos, 2007) for three language pairs. Our alignment did not improve the performance of a mature Arabic to English translation system, but two notable successes were obtained: Chinese to English, and English to Italian. It is well known that improved alignment performance does not always improve translation performance (Fraser and Marcu, 2007). A mature machine translation system may incorporate alignments obtained from multiple aligners, or from both directions of an asymmetric aligner. Furthermore, with large amounts of training data (the Gale Phase 4 Arabic English corpus consisting of 8 × 106 sentences,) a machine translation system is subject to a saturation effect: correcting an alignment may not yield a significant improvement because the the phrases learned from the correct alignment have already been acquired in other contexts. For the Chinese to English translation system (table 6) the training corpus consisted of 11 × 10"
D11-1082,H05-1012,1,0.965578,"as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). 3 Joint Models An alternate parameterization of alignment is the alignment matrix (Niehues and Vogel, 2008). For a source sentence F consisting of words f1 ...fm , and a target sentence E = e1 ...el , the alignment matrix A = {σij } is an l × m matrix of binary variables. If σij = 1, then ei is said to be linked to fj . If ei is unlinked then σij = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary"
D11-1082,P07-2045,0,0.0075034,"get languages results in a different aligner. This parameterization does not allow a target word to be linked to more than one source word, so some phrasal alignments are simply not considered. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically traine"
D11-1082,N04-4015,0,0.0181409,"nglish and a 1.26 BLEU improvement on English to Italian translation. 2 Alignment sequence models Sequence models are the traditional workhorse for word alignment, appearing, for instance, in IBM Models 1-5. This type of alignment model is not symmetric; interchanging source and target languages results in a different aligner. This parameterization does not allow a target word to be linked to more than one source word, so some phrasal alignments are simply not considered. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of"
D11-1082,N06-1014,0,0.0354576,"a target word to be linked to more than one source word, so some phrasal alignments are simply not considered. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignm"
D11-1082,P05-1057,0,0.137591,"j = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matrix allows some alignments that cannot be modeled by the sequence parameterization. All 2lm binary matrices are potentially allowed in alignment matrix models. For typical l and m, 2lm  (m + 1)l , the number of alignments described by a comparable sequence model. This parameterization is symmetric if source and target are interchanged, then the alignment matrix is transposed. A straightforward approach to the alignment matrix is to build a log linear model (Liu et al., 2005) for the probability of the alignment A. (We continue to refer to “source” and “target” words only for consistency of notation - alignment models such as this are indifferent to the actual direction of translation.) The log linear model for the alignment (Liu et al., 2005) is P exp ( i λi φi (A, E, F )) (1) p(A|E, F ) = Z(E, F ) where the partition function (normalization) is given by ! X X Z(E, F ) = exp λi φi (A, E, F ) . (2) A i Here the φi (A, E, F ) are feature functions. The model is parameterized by a set of weights λi , one for each feature function. Feature functions are often binary,"
D11-1082,P06-1065,0,0.0192965,"a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and a move selector. The move generator perturbs an existing"
D11-1082,H05-1011,0,0.0242944,"e restricted to a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and a move selector. The move generato"
D11-1082,W08-0303,0,0.113468,"typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). 3 Joint Models An alternate parameterization of alignment is the alignment matrix (Niehues and Vogel, 2008). For a source sentence F consisting of words f1 ...fm , and a target sentence E = e1 ...el , the alignment matrix A = {σij } is an l × m matrix of binary variables. If σij = 1, then ei is said to be linked to fj . If ei is unlinked then σij = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matrix allows some alignments that cannot be modeled by the sequence parameterization. All 2lm binary matrices are potentially allowed in alignment matrix models. For typical l and m, 2lm  (m + 1)l , the number of alignments"
D11-1082,J03-1002,0,0.163895,"or alignment correction target word is sufficient. 4.1 Move generation Many different types of alignment perturbations are possible. Here we restrict ourselves to a very simple move generator that changes the linkage of exactly one source word at a time, or exactly one target word at a time. Many of our corrections are similar to those of (Setiawan et al., 2010), although our motivation is perhaps closer to (Brown et al., 1993), who used similar perturbations to approximate intractable sums that arise when estimating the parameters of the generative models Models 3-5, and approach refined in (Och and Ney, 2003). We note that our corrections are designed to improve even a highquality starting alignment; in contrast the model of (Fossum et al., 2008) considers deletion of links from an initial alignment (union of aligners) that is likely to overproduce links. From the point of view of the alignment matrix, we consider changes to one row or one column (generically, one slice) of the alignment matrix. At each step t, the move set Mt (At ) is formed by choosing a slice of the current alignment matrix At , and generating all possible alignments from a few families of moves. Then the move generator picks a"
D11-1082,D10-1052,0,0.0974221,"Missing"
D11-1082,2006.amta-papers.25,0,0.0203284,"ng an alignment may not yield a significant improvement because the the phrases learned from the correct alignment have already been acquired in other contexts. For the Chinese to English translation system (table 6) the training corpus consisted of 11 × 106 sentence pairs, subsampled to 106 . The test set was NIST MT08 Newswire, consisting of 691 sentences and 4 reference translations. Corpus-level performance (columns 2 and 3) improved when measured by BLEU, but not by TER. Performance on the most difficult sentences (near the 90th percentile, columns 4 and 5) improved on both BLEU and TER (Snover et al., 2006), and the improvement in BLEU was larger for the more difficult sentences than it was overall. Translation performance further improved, by a smaller amount, using both ME-seq and corr(ME-seq) alignments during the training. The improved alignments impacted the translation performance of the English to Italian translation system (table 7) even more strongly. Here the training corpus consisted of 9.4×106 sentence pairs, subsampled to 387000 pairs. The test set consisted of 7899 sentences. Overall performance improved as measured by both TER and BLEU (1.26 points.) 7 Conclusions A log linear mod"
D11-1082,H05-1010,0,0.0264195,"example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and"
D11-1082,C96-2141,0,0.400464,"internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional rando"
D11-1082,H01-1035,0,0.0575331,"tilize parallel corpora as training data. Word alignments appear as hidden variables in IBM Models 15 (Brown et al., 1993) in order to bridge a gap between the sentence-level granularity that is explicit in the training data, and the implicit word-level correspondence that is needed to statistically model lexical ambiguity and word order rearrangements that are inherent in the translation process. Other notable applications of word alignments include crosslanguage projection of linguistic analyzers (such as POS taggers and named entity detectors,) a subject which continues to be of interest. (Yarowsky et al., 2001), (Benajiba and Zitouni, 2010) The structure of the alignment model is tightly linked to the task of finding the optimal alignment. Many alignment models are factorized in order to use dynamic programming and beam search for efficient marginalization and search. Such a factorization encourages - but does not require - a sequential (often left-to-right) decoding order. If left-to-right decoding is adopted (and exact dynamic programming is intractable) important right context may exist beyond the search window. For example, the linkage of an English determiner may be considered before the linkag"
D11-1082,N07-1008,1,\N,Missing
D19-3006,D16-1244,0,0.0975469,"Missing"
D19-3006,N18-1202,0,0.115417,"span from the document along with a prediction score as its output. Note that CFO’s orchestrator automatically realizes that the IR node produces a list of documents, while the MRC node accepts a single document at a time. So CFO will take care of calling the MRC node for each of the k retrieved documents (using asynchronous calls to parallelize requests if a configuration flag is set). The underlying BERT-for-QA model is based on (Alberti et al., 2019). BERT (Devlin et al., 2018) is one of a series of pre-trained neural models that can be fine tuned to provide state-of-theart results in NLP (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2019) including on the SQuAD (Rajpurkar et al., 2018) and NQ (Kwiatkowski et al., 2019) tasks that align with our MRC based QA. We use the Huggingface PyTorch implementation of BERT 9 which supports starting from a Base (a 12 layer, 768 hidden dimension, 12 atten4 Experiments Prior to integration of the MRC node into GAAMA, we first experiment with data preparation and training of the BERT MRC model as a standalone component using dev sets provided with the NQ and SQuAD data sets (see 5 for more on these data sets). NQ is preferred for evaluat7 10 http"
D19-3006,K18-2016,0,0.012446,"Anserini IR toolkit (Yang et al., 2017) to look for relevant documents for a question, then uses BERT-based techniques (Devlin et al., 2018) to extract the correct answer. However, their reliance on a Lucene based IR toolkit means that constructing a NLP pipeline would either require pipeline components to be written as Lucene based plugins (which comes with a variety of constraints on programming language and structure) or writing custom orchestration code to connect components outside of the toolkit. Similar constraints are imposed by other popular NLP pipeline toolkits such as StanfordNLP (Qi et al., 2018) and Spacy15 (both of which require development in Python with limited flexibility in training neural models with other frameworks such as Tensorflow16 ). In contrast, CFO’s inherent programming lan14 Used 500 random examples from dev set for experiments https://spacy.io/ 16 https://www.tensorflow.org/ 15 35 ity across type definitions. In contrast, CFO only requires consistency in the data model for components that directly connect to each other, and the names of corresponding types and fields do not need to match. These differences make CFO easier to use in cases where components were develo"
D19-3006,P18-2124,0,0.10194,"Missing"
D19-3006,D16-1264,0,0.07123,"en the median latency is greater than one and a half seconds, effectively cementing GPUs as a requirement for deploying to production environments. In future work we intend to explore network pruning or knowledge distillation techniques for potential speedups with the large model. 5 This flexibility is important in a domain such as Machine Reading Comprehension (MRC) where recent advances in language-modeling based pretrained embeddings like ELMO (Peters et al., 2018) and BERT (Devlin et al., 2018) along with large scale open data sets like the Stanford Question Answering Dataset (SQuAD) 1.1 (Rajpurkar et al., 2016) and its successor SQuAD 2.0 (Rajpurkar et al., 2018) have spurred a diverse array of model architecture improvements in a short time span. Recent work has even produced systems that surpass human-level exact match accuracy on the SQuAD datasets, causing us to focus on the challenging new Natural Questions (NQ) dataset (Kwiatkowski et al., 2019) where the questions do not have any observational bias as they were not artificially created. To the best of our knowledge, there is no current software framework paper that shows its analysis on the NQ dataset and displays strong empirical performance"
D19-3006,N19-4013,0,0.442889,"rporates state-of-the-art BERT based MRC (Machine Reading Comprehension) with IR components to enable end-to-end answer retrieval. Results from the demo system are shown to be high quality in both academic and industry domain specific settings. Finally, we discuss best practices when (pre-)training BERT based MRC models for production systems. 1 Introduction Production NLP (Natural Language Processing) and IR (information retrieval) applications often rely on a system flow consisting of multiple components that need to be woven together to build an end-to-end system (A. Ferrucci et al., 2010; Yang et al., 2019). This paper presents a novel approach for defining flow graphs and a toolkit for compiling those definitions into deploy-able production grade systems1 . Though the framework, which we refer to as CFO (C OMPUTATION F LOW O RCHESTRATOR), is well suited to a variety of use cases, we demonstrate it by creating an interactive QA (Question Answering) system that can be used both for academic benchmarking as well as industry specific use cases. The interactive system integrates SOTA (state-of-the-art) BERT-based MRC (Machine Reading Comprehension), an Elasticsearch 2 CFO Architecture The CFO framew"
D19-3006,P18-1031,0,\N,Missing
D19-3006,Q19-1026,0,\N,Missing
D19-3006,N19-1423,0,\N,Missing
H05-1012,J96-1002,0,0.021156,"distant words from aligning to the state. In terms of alignment F-measure to be described below, the state visit penalty, if removed makes the performance degrade from F=87.8 to F=84.0 compared to removing the frontier notion which only degrades performance to F=86.9. 3.2 Observation Model The observation model measures the linkage of the source and target using a set of feature functions defined on the words and their context. In Figure 1, an event is a single link from an English word to an Arabic state and the event space is the sentence pair. We use the maximum entropy formulation (e.g. (Berger et al., 1996)), 1 We are overloading the word ‘state’ to mean Arabic word position. 91 f = ψ(li )   h = t1i−1 , sK 1 X 1 λi φi (h, f ), p(f |h) = exp Z(h) i where Z(h) is the normalizing constant, Z(h) = X f exp X λi φi (h, f ). i and φi (h, f ) are binary valued feature functions. The function ψ selects the Arabic word at the position being linked or in the case of segmentation features, one of the segmentations of that position. We restrict the history context to select from the current English word and words to the left as well as the current word’s WordNet (Miller, 1990) synset as required by the fea"
H05-1012,J93-2003,0,0.0163179,"nt on several machine translation tests. Performance of the algorithm is contrasted with human annotation performance. 1 Although there are a number of other applications for word alignment, for example in creating bilingual dictionaries, the primary application continues to be as a component in a machine translation system. We test our aligner on several machine translation tests and show encouraging improvements. 2 Related Work Most of the prior work on word alignments has been done on parallel corpora where the alignment at the sentence level is also done automatically. The IBM models 1-5 (Brown et al., 1993) produce word alignments with increasing algorithmic complexity and performance. These IBM models and more recent refinements (Moore, 2004) as well as algorithms that bootstrap from these models like the HMM algorithm described in (Vogel et al., 1996) are unsupervised algorithms. Introduction Machine translation takes a source sequence, S = [s1 s2 . . . sK ] and generates a target sequence, T = [t1 t2 . . . tM ] that renders the meaning of the source sequence into the target sequence. Typically, algorithms operate on sentences. In the most general setup, one or more source words can generate 0"
H05-1012,P03-1012,0,0.44088,"rce words can generate 0, 1 or more target words. Current state of the art machine translation systems (Och, 2003) use phrasal (n-gram) features extracted automatically from parallel corpora. These phrases are extracted using word alignment algorithms that are trained on parallel corpora. Phrases, or phrasal features, represent a mapping of source sequences into a target sequences which are typically a few words long. The relative success of these automatic techniques together with the human annotation cost has delayed the collection of supervised word-aligned corpora for more than a decade. (Cherry and Lin, 2003) recently proposed a direct alignment formulation and state that it would be straightforward to estimate the parameters given a supervised alignment corpus. In this paper, we extend their work and show that with a small amount of annotated data, together with a modeling strategy and search algorithm yield significant gains in alignment F-measure. 89 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 89–96, Vancouver, October 2005. 2005 Association for Computational Linguistics wA$Art Words Source Segm. w#"
H05-1012,P03-1051,1,0.354828,"Features The lexical features are similar to the translation matrix of the IBM Model 1. However, there is a signficant out of vocabulary (OOV) issue in the model since training data is limited. All words that have a corpus frequency of 1 are left out of the model and classed into an unknown word class in order to explicitly model connecting unknown words. From the training data we obtain 50K lexical features, and applying the Arabic segmenter obtain another 17K lexical features of the form φ(English content word, Arabic stem). 3.2.2 Arabic Segmentation Features An Arabic segmenter similar to (Lee et al., 2003) provides the segmentation features. A small dictionary is used (with 71 rules) to restrict the set of Arabic segments that can align to English stopwords, for example that ‘the’ aligns to ‘Al#’ and that ‘for’, ‘in’ and ‘to’ align to ‘b#’ and ‘her’ aligns with the suffix ‘+hA’. Segmentation features also help align unknown words, as stems might be seen in the training corpus with other prefixes or suffixes. Additionally, the ability to align the prefix and suffix accurately, tends to ‘drag’ the unknown stem to its English target. 3.2.3 WordNet Features WordNet features provide normalization on"
H05-1012,P04-1066,0,0.0307488,"er of other applications for word alignment, for example in creating bilingual dictionaries, the primary application continues to be as a component in a machine translation system. We test our aligner on several machine translation tests and show encouraging improvements. 2 Related Work Most of the prior work on word alignments has been done on parallel corpora where the alignment at the sentence level is also done automatically. The IBM models 1-5 (Brown et al., 1993) produce word alignments with increasing algorithmic complexity and performance. These IBM models and more recent refinements (Moore, 2004) as well as algorithms that bootstrap from these models like the HMM algorithm described in (Vogel et al., 1996) are unsupervised algorithms. Introduction Machine translation takes a source sequence, S = [s1 s2 . . . sK ] and generates a target sequence, T = [t1 t2 . . . tM ] that renders the meaning of the source sequence into the target sequence. Typically, algorithms operate on sentences. In the most general setup, one or more source words can generate 0, 1 or more target words. Current state of the art machine translation systems (Och, 2003) use phrasal (n-gram) features extracted automati"
H05-1012,P00-1056,0,0.336591,"Missing"
H05-1012,J03-1002,0,0.0377592,"Missing"
H05-1012,W99-0604,0,0.0721662,"r the HMM alignment, whereas the MaxEnt algorithm should be rejected with a probability of 1.7e-6 over the HMM algorithm and similarly MaxEnt should be rejected with a probability of 0.9e6 over the GIZA++ algorithm. These significance tests indicate that the MaxEnt algorithm presented above is significantly better than either GIZA++ or HMM. 94 Figure 2: An alignment showing a split link from an Arabic word. 8 Phrase Extraction Once an alignment is obtained, phrases which satisfy the inverse projection constraint are extracted (although earlier this constraint was called consistent alignments (Och et al., 1999)). This constraint enforces that a sequence of source words align to a sequence of target words as defined by the lowest and highest target index, and when the target words are projected back to the source language through the alignment, the original source sequence is retrieved. Examination of the hand alignment training data showed that this criteria is often violated for Arabic and English. Prepositional phrases with adjectives often require a split– for example, the alignment shown in Figure 2 has ‘of its relations’ aligned to a word in Arabic and ‘tense’ aligned to the next word. The inve"
H05-1012,P03-1021,0,0.0572811,"These IBM models and more recent refinements (Moore, 2004) as well as algorithms that bootstrap from these models like the HMM algorithm described in (Vogel et al., 1996) are unsupervised algorithms. Introduction Machine translation takes a source sequence, S = [s1 s2 . . . sK ] and generates a target sequence, T = [t1 t2 . . . tM ] that renders the meaning of the source sequence into the target sequence. Typically, algorithms operate on sentences. In the most general setup, one or more source words can generate 0, 1 or more target words. Current state of the art machine translation systems (Och, 2003) use phrasal (n-gram) features extracted automatically from parallel corpora. These phrases are extracted using word alignment algorithms that are trained on parallel corpora. Phrases, or phrasal features, represent a mapping of source sequences into a target sequences which are typically a few words long. The relative success of these automatic techniques together with the human annotation cost has delayed the collection of supervised word-aligned corpora for more than a decade. (Cherry and Lin, 2003) recently proposed a direct alignment formulation and state that it would be straightforward"
H05-1012,J03-1005,0,0.0412658,"ures for single source words as long as the target phrase has a gap less than 2 English words. This relaxation allows a pair of adjectives to modify the head noun. In future work we explore the use of features with variables to be filled at decode time. 9 Translation Experiments The experiments in machine translation are carried out on a phrase based decoder similar to the one deGIZA++ HMM MaxEnt Combined Significance MT03 0.454 0.459 0.468 0.479 0.017 MT04 — 0.419 0.433 0.437 0.020 MT05 — 0.456 0.451 0.465 — Table 5: Machine Translation Performance using the NIST 2005 Bleu scorer scribed in (Tillmann and Ney, 2003). In order to contrast the performance of the extracted features, we compare the translation performance to (a) a system built from alignments proposed by an HMM Max Posterior Aligner, and (b) a system built from GIZA alignments. All other parameters of the decoder remain constant and only the feature set is changed for these experiments. As training data, we use the UN parallel corpus and the LDC news corpora released in 2005. Comparison should therefore be only made across systems reported here and not to earlier evaluations or other systems. The results are shown in Table 5. Combination of"
H05-1012,C96-2141,0,\N,Missing
H89-1012,P89-1029,1,0.721288,"e predetermined threshold, and eliminate all others. In practice, we found it nearly impossible to find a single threshold that works for all words and have adopted a strategy that uses dual ...) :NPTYPE ...) . ..) where the variable :AGR enforces agreement between the VP (ultimately, its head V) and the subject NP; :NPT'Z'PE, agreement between the syntactic type of the subject NP and that selected by the head V of the VP; and :MOOD, agreement between the mood of the S and that of the VP. 107 ACFGs, depth-boundedACFGs, for which the parsing algorithm is guaranteed to find all parses and halt (Haas (1989)). Depth-bounded ACFGs are characterized by the property that the depth of a parse tree cannot grow unboundedly large unless the length of the string also increases. In effect, such grammars do not permit rules in which a category derives only itself and no other children; such rules do not seem to be needed for the analysis of natural languages, so computational tractability is maintained without sacrificing linguistic coverage. The fact that the parsing algorithm for this class of ACFGs halts is a useful result, since parsers for complex feature based grammars cannot be guaranteed to halt, i"
H92-1023,A88-1019,0,0.0334987,"Missing"
H92-1023,H91-1065,0,0.0333321,"Missing"
H92-1023,J92-4003,1,\N,Missing
H92-1026,P91-1027,0,0.016161,"t have a mechanism for estimating the coherence of an interpretation, both in isolation and in context. Probabilistic language models provide such a mechanism. Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and their tags, introduced in [1] and [11] and popularized in the natural language community by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities. Brent [2] acquires likely verb subcategorization patterns using the A probabilistic language model a t t e m p t s to estimate the probability of a sequence of sentences and their respective interpretations (parse trees) occurring in the language, ""P(S1 T1 $2 T2 ... S~ T~). *Thanks to Philip Resnik and Stanley Chen for their valued input. 134 The difficulty in applying probabilistic models to natural language is deciding what aspects of the sentence and the discourse are relevant to the model. Most previous probabilistic models of parsing assume the probabilities of sentences in a discourse are indepen"
H92-1026,H90-1055,1,0.865814,"few million words of sentences that are completely covered by this vocabulary from 40,000,000 words of computer manuals. A randomly chosen sentence from a sample of 5000 sentences from this corpus is: 5. T h e The g r a m m a r used in this experiment is a broadcoverage, feature-based unification g r a m m a r . The gramm a r is context-free but uses unification to express rule templates for the the context-free productions. For example, the rule template: : n 396. Grammar It indicates whether a call completed successfully or if some error was detected that caused the call to fail. unspec : n (3) corresponds to three C F G productions where the second feature : n is either s, p, or : n. This rule t e m p l a t e m a y elicit up to 7 non-terminals. The g r a m m a r has 21 features whose range of values m a y b e from 2 to a b o u t 100 with a median of 8. There are 672 rule templates of which 400 are actually exercised when we parse a corpus of 15,000 sentences. The number of productions t h a t are realized in this training corpus is several hundred thousand. To define what we mean by a correct parse, we use a corpus of manually bracketed sentences at the University of Lancaster call"
H92-1026,A88-1019,0,0.0212882,"d Grammars One goal of a parser is to produce a g r a m m a t i c a l interpretation of a sentence which represents the syntactic and semantic intent of the sentence. To achieve this goal, the parser must have a mechanism for estimating the coherence of an interpretation, both in isolation and in context. Probabilistic language models provide such a mechanism. Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and their tags, introduced in [1] and [11] and popularized in the natural language community by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities. Brent [2] acquires likely verb subcategorization patterns using the A probabilistic language model a t t e m p t s to estimate the probability of a sequence of sentences and their respective interpretations (parse trees) occurring in the language, ""P(S1 T1 $2 T2 ... S~ T~). *Thanks to Philip Resnik and Stanley Chen for their valued input. 134 The difficulty in applying probabilistic models to natural la"
H92-1026,H90-1056,0,0.0158234,"Missing"
H92-1026,H90-1052,0,0.0173264,"resents the syntactic and semantic intent of the sentence. To achieve this goal, the parser must have a mechanism for estimating the coherence of an interpretation, both in isolation and in context. Probabilistic language models provide such a mechanism. Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and their tags, introduced in [1] and [11] and popularized in the natural language community by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities. Brent [2] acquires likely verb subcategorization patterns using the A probabilistic language model a t t e m p t s to estimate the probability of a sequence of sentences and their respective interpretations (parse trees) occurring in the language, ""P(S1 T1 $2 T2 ... S~ T~). *Thanks to Philip Resnik and Stanley Chen for their valued input. 134 The difficulty in applying probabilistic models to natural language is deciding what aspects of the sentence and the discourse are relevant to the model. Most previous p"
H92-1026,E91-1004,0,0.104846,"guage, ""P(S1 T1 $2 T2 ... S~ T~). *Thanks to Philip Resnik and Stanley Chen for their valued input. 134 The difficulty in applying probabilistic models to natural language is deciding what aspects of the sentence and the discourse are relevant to the model. Most previous probabilistic models of parsing assume the probabilities of sentences in a discourse are independent of other sentences. I n fact, previous works have made much stronger independence assumptions. The P-CFG model considers the probability of each constituent rule independent of all other constituents in the sentence. The Pearl [10] model includes a slightly richer model of context, allowing the probability of a constituent rule to depend upon the immediate parent of the rule and a part-of-speech trigram from the input sentence. But none of these models come close to incorporating enough context to disambiguate many cases of ambiguity. A significant reason researchers have limited the contextual information used by their models is because of the difficulty in estimating very rich probabilistic models of context. In this work, we present a model, the historybased grammar model, which incorporates a very rich model of cont"
H92-1026,H90-1054,0,0.0711553,"Missing"
H92-1026,H91-1067,0,\N,Missing
H92-1026,J92-4003,1,\N,Missing
H92-1026,1991.iwpt-1.22,1,\N,Missing
H93-1021,H91-1057,0,0.205971,"r performance after seeing some of the data. This is particularly useful in two situations. First, when a large heterogeneous language source is composed of smaller, more homogeneous segments, such as newspaper articles. An adaptive model trained on the heterogeneous source will be able to hone in on the particular &quot;sublanguage&quot; used in each of the articles. Secondly, when a model trained on data from one domain is used in another domain. Again, an adaptive model will be able to adjust to the new language, thus improving its performance. The most successful adaptive LM to date is described in [3]. A cache of the last few hundred words is maintained, and is used *This work is now continued by Ron Rosenfeld at Carnegie Mellon University. 108 the idea of a trigger pair as the basic information bearing element. If a word sequence A is significantly correlated with another word sequence B, then (A---, B) is considered a &quot;trigger pair&quot;, with A being the trigger and B the triggered sequence. When A occurs in the document, it triggers B, causing its probability estimate to change. Before attempting to design a trigger-based model, one should study what long distance factors have significant e"
H93-1021,H92-1020,0,0.022632,"Missing"
H93-1021,H92-1021,1,\N,Missing
H93-1092,P93-1005,1,\N,Missing
H94-1048,H90-1052,0,0.0303172,"Missing"
H94-1048,H92-1020,0,0.0145242,"Missing"
H94-1048,P79-1022,0,0.0755402,"ions in fi. 0.7 The newly created features are then added to P , and compete for selection in the next Delta-Likelihood ranking process. This method allows the introduction of complex features on word classes while keeping the search space manageable; &quot;P grows linearly with .M. 0.6 Training 0.5 0.4 20 A dO dO . . 120. 100 .140 160 180 Figure 2: Entropy of Maximum Entropy Model on Wall St. Journal Data a zero or negligible 6L, and will therefore be outranked by genuinely informative features. The chosen feature is added to M and used in the ME Model. 3.2. Growth of Putative Feature Set At step (5) in the search we expand the space 7~ of putative features based on the feature last selected from 72 for addition to M . Given an n-gram feature f~ (i.e., of type &quot;word&quot;, &quot;class&quot; or&quot;mixed&quot;) that was last added to M , we create 2 m . 4 new n + 1-gram features which ask questions about class bits in addition to the questions asked in fi. E.g., let fi(h, d) constrain d = 0 and constrain h with the questions v == &apos; &apos; imposed&apos; &apos; , P == &apos; &apos;on&apos; &apos; Then, given fi(h,d), the 2m new features generated for just the Head Noun are the following: V == Bit V == Bit V Noun P == for Noun for Noun &apos;&apos;imposed&apos;&apos;, m"
H94-1048,J82-2005,0,0.0671417,"rase, i.e., the verb phrase without the attachment decision, as a history h, and the conditional probability of an attachment as p(dlh), where d 6 .[0, 1} and corresponds to a noun or verb attachment (respectively). The probability model depends on certain features of the whole event (h, d) denoted by fi(h, d). An example of a binary-valued feature function is the indicator function that a particular (V, P ) bigram occured along with the attachment decision being V, i.e. fprint,on(h, d) is one if and only if the main verb of h is &quot;print&quot;, the preposition is &quot;on&quot;, and d is &quot;V&quot;. As discussed in [6], the ME principle leads to a model for p(dlh ) which maximizes the training data log-likelihood, a) log p(dlh), h,d where ~(h, w) is the empirical distribution of the training set, and where p(dlh ) itself is an exponential model: 250 4. Head Noun of the Object of the Preposition (N2) k 11 eX&apos;Y&apos;(h&apos;d) i=0 1 k p(dlh) = YI e~&apos;f&apos;(h&quot;0 For example, questions on the history &quot;imposed a gradual ban on virtually all uses of asbestos&quot;, can only ask about the following four words: d=0 i=0 At the maximum of the training data log-likelihood, the model has the property that its k parameters, namely the At&apos;s"
H94-1048,P93-1005,1,0.767263,"Missing"
H94-1048,H92-1026,1,\N,Missing
H94-1048,J92-4003,0,\N,Missing
H94-1099,P93-1005,1,0.841127,"Missing"
mccarley-roukos-1998-fast,J93-2003,0,\N,Missing
mccarley-roukos-1998-fast,J96-1002,0,\N,Missing
N03-2014,W97-0319,0,\N,Missing
N03-2014,P02-1060,0,\N,Missing
N03-2029,N01-1005,1,0.887433,"estigated the use of text patterns for QA (Soubbotin and Soubbotin, 2001), (Soubbotin and Soubbotin, 2002), (Ravichandran and Hovy, 2002). For example, for questions like “When was Gandhi born?”, typical answers are “Gandhi was born in 1869” and “Gandhi (1869-1948)”. These examples suggest that the text patterns such as “ NAME was born in BIRTHDATE ” and “ NAME ( BIRTHDATE DEATHYEAR )” when formulated as regular expressions, can be used to select the answer phrase to questions. Another approach to a QA system is learning correspondences between question and answer pairs. IBM’s Statistical QA (Ittycheriah et al., 2001a) system uses a probabilistic model trainable from Question-Answer sentence pairs. The training is performed under a Maximum Entropy model, using bag of words, syntactic and name entity features. This QA system does not employ the use of patterns. In this paper, we explore the inclusion of surface text patterns into the framework of a statistical question answering system.         1. Which country was invaded by the Libyan troops in 1983? - Chad 2. Who led the 1930 Salt March in India? - Mohandas Gandhi 1 Introduction   KM database. Each of the pairs in KM represents a trivia questi"
N03-2029,P02-1006,1,0.523186,"ce text patterns for a Maximum Entropy based Question Answering (QA) system. These text patterns are collected automatically in an unsupervised fashion using a collection of trivia question and answer pairs as seeds. These patterns are used to generate features for a statistical question answering system. We report our results on the TREC-10 question set.  Abraham Ittycheriah and Salim Roukos IBM TJ Watson Research Center Yorktown Heights, NY, 10598 abei,roukos @us.ibm.com  We use an unsupervised technique that uses the QA in KM as seeds to learn patterns. This method was first described in Ravichandran and Hovy (2002). However, in this work we have enriched the pattern format by inducing specific semantic types of QTerms, and have learned many more patterns using the KM. 3.1 Algorithm for sentence construction 1. For every question, we run a Named Entity Tagger HMMNE 3 and identify chunks of words, that signify entities. Each such entity obtained from the Question is defined as a Question term (QTerm). The Answer Term (ATerm) is the Answer given by the KM corpus. 2. Each of the question-answer pairs is submitted as query to a popular Internet search engine4 . We use the top 50 relevant documents after stri"
N03-4001,C96-1017,0,0.0944467,"Missing"
N03-4001,J93-2003,0,0.00628603,"Missing"
N03-4001,P03-1051,1,0.838841,"Missing"
N03-4001,J03-1005,1,0.874545,"Missing"
N07-1008,P06-1067,0,0.107533,"Missing"
N07-1008,W05-0823,0,0.0225928,"Missing"
N07-1008,J93-2003,0,0.0230413,"ased systems, including the baseline decoder used in this work use feature functions: • a target word n-gram model (e.g., n = 5), direct modeling for machine translation, we call our current approach DTM2 (Direct Translation Model 2). 1.2 • a target part-of-speech n-gram model (n ≥ 5), • various translation models such as a block inventory with the following three varieties: 1) the unigram block count, 2) a model 1 score p(si |ti ) on the phrase-pair, and 3)a model 1 score for the other direction p(ti |si ), Statistical modeling for translation Earlier work in statistical machine translation (Brown et al., 1993) is based on the “noisy-channel” formulation where T ∗ = arg max p(T |S) = arg max p(T )p(S|T ) T • a target word count penalty feature |T |, (1) • a phrase count feature, T • a distortion model (Al-Onaizan and Papineni, 2006). where the target language model p(T ) is further decomposed as p(T ) ∝ Y p(ti |ti−1 , . . . , ti−k+1 ) i where k is the order of the language model and the translation model p(S|T ) has been modeled by a sequence of five models with increasing complexity (Brown et al., 1993). The parameters of each of the two components are estimated using Maximum Likelihood Estimation"
N07-1008,P05-1033,0,0.031883,"tersection, inverse projection constraint, etc. As discussed earlier, these approaches result in a large overlap between the extracted blocks (longer blocks overlap with all the shorter subcomponents blocks). Also, slightly restating the advantages of phrase-pairs identified in (Quirk and Menezes, 2006), these blocks are effective at capturing context including the encoding of non-compositional phrase pairs, and capturing local reordering, but they lack variables (e.g. embedding between ne . . . pas in French), have sparsity problems, and lack a strategy for global reordering. More recently, (Chiang, 2005) extended phrase-pairs (or blocks) to hierarchical phrase-pairs where a grammar with a single non-terminal allows the embedding of phrases-pairs, to allow for arbitrary embedding and capture global reordering though this approach still has the high overlap problem. However, in (Quirk and Menezes, 2006), the authors investigate minimum translation units (MTU) which is a refinement over a similar approach by (Banchs et al., 2005) to eliminate the overlap issue. The MTU approach picks all the minimal blocks subject to the condition that no word alignment link crosses distinct blocks. They do not"
N07-1008,P00-1006,0,0.0477797,"t output. So in Table 1 only the blocks corresponding to a single Arabic word are in the block inventory. To differentiate this work from previous approaches in 57 Proceedings of NAACL HLT 2007, pages 57–64, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics This is the simplest MaxEnt1 model that uses two feature functions. The parameter α is tuned on a development set (usually to improve an error metric instead of MLE). This model is a special case of the Direct Translation Model proposed in (Papineni et al., 1997; Papineni et al., 1998) for language understanding; (Foster, 2000) demostrated perplexity reductions by using direct models; and (Och and Ney, 2002) employed it very successfully for language translation by using about ten feature functions: lljnp Almrkzyp committee central of the commission the central commission of the central of the committee of central the committee and the central of the commission on and central the commission , central committee of ’s central ... ... of the central committee(11) of the central committee of (11) the central committee of (8) central committee(7) committee central (2) central committee , (2) ... p(T |S) = Table 1: Exampl"
N07-1008,H05-1012,1,0.325764,"translation model which is built from a smaller parallel corpus. In the rest of this paper, we are concerned only with the translation model. The minimum requirements for the algorithm are (a) parallel corpus of source and target languages and (b) word-alignments. While one can use the EM algorithm to train this hidden alignment model (the jump step), we use Viterbi training, i.e. we use the most likely alignment between target and source words in the training corpus to estimate this model. We assume that each sentence pair in the training corpus is word-aligned (e.g. using a MaxEnt aligner (Ittycheriah and Roukos, 2005) or an HMM aligner (Ge, 2004)). The algorithm performs the following steps in order to train the maximum entropy model: (a) block extraction, (b) feature extraction, and (c) parameter estimation. Each of the first two steps requires a pass over the training data and parameter estimation requires typically 5-10 passes over the data. (Della Pietra et al., 1995) documents the Improved Iterative Scaling (IIS) algorithm for training maximum entropy models. When the system is restricted to 1-N type blocks, the future space includes all the source word positions that are within the skip window and al"
N07-1008,P03-1051,1,0.767778,"‘lljnp’ and ‘Almrkzyp’. Feature Name SRC LEFT SRC RIGHT SRC TGT LEFT SRC TGT LEFT 2 Feature variables source left, source word, target word source right, source word, target word source left, target left, source word, target word source left, target left, target left 2, source word, target word when the target sequence has a variable. Table 2: Context Feature Types 4.3.3 Arabic Segmentation Features An Arabic segmenter produces morphemes; in Arabic, prefixes and suffixes are used as prepositions, pronouns, gender and case markers. This produces a segmentation view of the arabic source words (Lee et al., 2003). The features used in the model are formed from the Cartesian product of all segmentation tokens with the English target sequence produced by this source word or words. However, prefixes and suffixes which are specific in translation are limited to their English translations. For example the prefix ‘Al#’ is only allowed to participate in a feature with the English word ‘the’ and similarly ‘the’ is not allowed to participate in a feature with the stem of the Arabic word. These restrictions limit the number of features and also reduce the over fitting by the model. 4.3.4 Part-of-speech Features"
N07-1008,P06-1096,0,0.0570059,"Missing"
N07-1008,2000.eamt-1.5,0,0.044463,"abic-English translation. 1 AlsyAsy lljnp Almrkzyp llHzb Al$ywEy AlSyny Figure 1: Example of Arabic snipet and alignment to its English translation. Introduction Statistical machine translation takes a source sequence, S = [s1 s2 . . . sK ], and generates a target sequence, T ∗ = [t1 t2 . . . tL ], by finding the most likely translation given by: T ∗ = arg max p(T |S). T 1.1 the Politburo of the Central Committee of the Chinese Communist Party Block selection Recent statistical machine translation (SMT) algorithms generate such a translation by incorporating an inventory of bilingual phrases (Och and Ney, 2000). A m-n phrase-pair, or block, is a sequence of m source words paired with a sequence of n target words. The inventory of blocks in current systems is highly redundant. We illustrate the redundancy using the example in Table 1 which shows a set of phrases that cover the two-word Arabic fragment “lljnp Almrkzyp” whose alignment and translation is shown in Figure 1. One notices the significant overlap between the various blocks including the fact the output target sequence “of the central committee” can be produced in at least two different ways: 1) as 2-4 block “lljnp Almrkzyp |of the central c"
N07-1008,P02-1038,0,0.0559253,"are in the block inventory. To differentiate this work from previous approaches in 57 Proceedings of NAACL HLT 2007, pages 57–64, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics This is the simplest MaxEnt1 model that uses two feature functions. The parameter α is tuned on a development set (usually to improve an error metric instead of MLE). This model is a special case of the Direct Translation Model proposed in (Papineni et al., 1997; Papineni et al., 1998) for language understanding; (Foster, 2000) demostrated perplexity reductions by using direct models; and (Och and Ney, 2002) employed it very successfully for language translation by using about ten feature functions: lljnp Almrkzyp committee central of the commission the central commission of the central of the committee of central the committee and the central of the commission on and central the commission , central committee of ’s central ... ... of the central committee(11) of the central committee of (11) the central committee of (8) central committee(7) committee central (2) central committee , (2) ... p(T |S) = Table 1: Example Arabic-English blocks showing possible 1-n and 2-n blocks ranked by frequency. B"
N07-1008,P03-1021,0,0.0600719,"lation model is estimated via the EM algorithm or approximations that are bootstrapped from the previous model in the sequence as introduced in (Brown et al., 1993). As is well known, improved results are achieved by modifying the Bayes factorization in Equation 1 above by weighing each distribution differently as in: p(T |S) ∝ pα (T )p1−α (S|T ) (2) 58 The weight vector λ is estimated by tuning on a rather small (as compared to the training set used to define the feature functions) development set using the BLEU metric (or other translation error metrics). Unlike MaxEnt training, the method (Och, 2003) used for estimating the weight vector for BLEU maximization are not computationally scalable for a large number of feature functions. 2 Related Work Most recent state-of-the-art machine translation decoders have the following aspects that we improve upon in this work: 1) block style, and 2) model parameterization and parameter estimation. We discuss each item next. 1 The subfields of log-linear models, exponential family, and MaxEnt describe the equivalent techniques from different perspectives. 2.1 Block style In order to extract phrases from alignments available in one or both directions, m"
N07-1008,N06-1002,0,0.0096306,"timation. We discuss each item next. 1 The subfields of log-linear models, exponential family, and MaxEnt describe the equivalent techniques from different perspectives. 2.1 Block style In order to extract phrases from alignments available in one or both directions, most SMT approaches use a heuristic such as union, intersection, inverse projection constraint, etc. As discussed earlier, these approaches result in a large overlap between the extracted blocks (longer blocks overlap with all the shorter subcomponents blocks). Also, slightly restating the advantages of phrase-pairs identified in (Quirk and Menezes, 2006), these blocks are effective at capturing context including the encoding of non-compositional phrase pairs, and capturing local reordering, but they lack variables (e.g. embedding between ne . . . pas in French), have sparsity problems, and lack a strategy for global reordering. More recently, (Chiang, 2005) extended phrase-pairs (or blocks) to hierarchical phrase-pairs where a grammar with a single non-terminal allows the embedding of phrases-pairs, to allow for arbitrary embedding and capture global reordering though this approach still has the high overlap problem. However, in (Quirk and Me"
N07-1008,J03-1005,0,0.0285424,"the source word to the left and the source word to the right. During training, the coverage is determined by examining the alignments; the source word to the left is uncovered if its target sequence is to the right of the current target sequence. Since the model employs binary questions and predominantly the source word to the left is already covered and the right source word is uncovered, these features fire only if the left is open or if the right is closed in order to minimize the number of features in the model. 5 Translation Decoder A beam search decoder similar to phrase-based systems (Tillmann and Ney, 2003) is used to translate the Arabic sentence into English. These decoders have two parameters that control their search strategy: (a) the skip length (how many positions are allowed to be untranslated) and (b) the window width, which controls how many words are allowed to be considered for translation. Since the majority of the blocks employed in this work do not encode local reordering explicitly, the current DTM2 decoder uses a large skip (4 source words for Arabic) and tries all possible reorderings. The primary difference between a DTM2 decoder and standard phrase based decoders is that the m"
N07-1008,P06-1091,0,0.0566022,"wed as an indicator of how probable the current translation is. As discussed in Section 1.2, these features are typically MLE models (e.g. block translation, Model 1, language model, etc.) whose scores are log-linearly combined using a weight vector, λf where f is a particular feature. The λf are trained using a held-out corpus using maximum BLEU training (Och, 2003). This method is only practical for a small number of features; typically, the number of features is on the order of 10 to 20. Recently, there have been several discriminative approaches at training large parameter sets including (Tillmann and Zhang, 2006) and (Liang et al., 2006). In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. (Liang et al., 2006) demonstrates a dis59 criminatively trained system for machine translation that has the following characteristics: 1) requires a varying update strategy (local vs. bold) depending on whether the reference sentence is “reachable” or not, 2) uses sentence level BLEU as a criterion for selecting which output to update towards, and 3) only trains on limited length (5-15 words) sentences. So both"
P02-1016,H94-1052,0,\N,Missing
P02-1016,W00-1306,0,\N,Missing
P02-1016,W01-0710,0,\N,Missing
P02-1016,P96-1025,0,\N,Missing
P02-1016,J92-4003,0,\N,Missing
P02-1016,N01-1023,0,\N,Missing
P02-1040,P02-1040,1,0.148116,"Missing"
P03-1020,A97-1029,0,0.0238141,"Missing"
P03-1020,P00-1037,0,0.131485,"Missing"
P03-1020,P02-1061,0,0.0426806,"Missing"
P03-1020,A97-1025,0,0.0226379,"Missing"
P03-1020,P99-1021,0,0.123015,"Missing"
P03-1020,2001.mtsummit-papers.68,0,0.0212049,"Chinese and English, or Japanese and English, there is no transfer of case information. In these situations the translation output has no case information and it is beneficial to apply truecasing as a post-processing step. This makes the output more legible and the system performance increases if case information is required. We have applied truecasing to Chinese-to-English translation output. The data source consists of news stories (2500 sentences) from the Xinhua News Agency. The news stories are first translated, then subjected to truecasing. The translation output is evaluated with BLEU (Papineni et al., 2001), which is a robust, language independent automatic machine translation evaluation method. BLEU scores are highly correlated to human judges scores, providing a way to perform frequent and accurate automated evaluations. BLEU uses a modified n-gram precision metric and a weighting scheme that places more emphasis on longer n-grams. In table 1, both truecasing methods are applied to machine translation output with and without uppercasing the first letter in each sentence. The truecasing methods are compared against the all letters lowercased version of the articles as well as against an existin"
P03-1020,P94-1013,0,0.0708965,"Missing"
P03-1020,P02-1040,1,\N,Missing
P03-1051,C96-1017,0,\N,Missing
P03-1051,N01-1024,0,\N,Missing
P03-1051,J93-2003,0,\N,Missing
P03-1051,H01-1035,0,\N,Missing
P03-1051,P00-1027,0,\N,Missing
P03-1051,J01-2001,0,\N,Missing
P03-1051,P96-1019,1,\N,Missing
P03-1051,W02-0506,0,\N,Missing
P04-1018,N01-1008,0,0.267428,"Missing"
P04-1018,N03-2014,1,0.734513,"mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded encouraging results, the way mentions are linked is arguably suboptimal in that an instant decision is made when"
P04-1018,W97-0319,0,0.22214,"ntions other than the two in question is ignored. However, models in (McCallum and Wellner, 2003) compute directly the probability of an entity configuration conditioned on mentions, and it is not clear how the models can be factored to do the incremental search, as it is impractical to enumerate all possible entities even for documents with a moderate number of mentions. The Bell tree representation proposed in this paper, however, provides us with a naturally incremental framework for coreference resolution. Maximum entropy method has been used in coreference resolution before. For example, Kehler (1997) uses a mention-pair maximum entropy model, and two methods are proposed to compute entity scores based on the mention-pair model: 1) a distribution over entity space is deduced; 2) the most recent mention of an entity, together with the candidate mention, is used to compute the entity-mention score. In contrast, in our mention pair model, an entity-mention pair is scored by taking the maximum score among possible mention pairs. Our entity-mention model eliminates the need to synthesize an entity-mention score from mention-pair scores. Morton (2000) also uses a maximum entropy mention-pair mod"
P04-1018,P98-2143,0,0.0165512,"ferring to the same object in a document form an entity. For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded enco"
P04-1018,P00-1023,0,0.0519623,"coreference resolution before. For example, Kehler (1997) uses a mention-pair maximum entropy model, and two methods are proposed to compute entity scores based on the mention-pair model: 1) a distribution over entity space is deduced; 2) the most recent mention of an entity, together with the candidate mention, is used to compute the entity-mention score. In contrast, in our mention pair model, an entity-mention pair is scored by taking the maximum score among possible mention pairs. Our entity-mention model eliminates the need to synthesize an entity-mention score from mention-pair scores. Morton (2000) also uses a maximum entropy mention-pair model, and a special “dummy” mention is used to model the event of starting a new entity. Features involving the dummy mention are essentially computed with the single (normal) mention, and therefore the starting model is weak. In our model, the starting model is obtained by “complementing” the linking scores. The advantage is that we do not need to train a starting model. To compensate the model inaccuracy, we introduce a “starting penalty” to balance the linking and starting scores. To our knowledge, the paper is the first time the Bell tree is used"
P04-1018,P02-1014,0,0.675827,"following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded encouraging results, the way mentions are linked is arguably suboptimal in that an ins"
P04-1018,W97-0301,0,0.0108198,"is a feature and 5 is its weight; is a normalizing factor to ensure that (10) or (11) is a probability. Effective training algorithm !exists (Berger  6 et al., 1996) once the set of features is selected. The basic features used in the models are tabulated in Table 1. Features in the lexical category are applicable to non-pronominal mentions only. Distance features characterize how far the two mentions are, either by the number of tokens, by the number of sentences, or by the number of mentions in-between. Syntactic features are derived from parse trees output from a maximum entropy parser (Ratnaparkhi, 1997). The “Count” feature calculates how many times a mention string is seen. For pronominal mentions, attributes such as gender, number, possessiveness and reflexiveness are also used. Apart from basic features in Table 1, composite features can be generated by taking conjunction of basic features. For example, a distance feature together with reflexiveness of a pronoun mention can help to capture that the antecedent of a reflexive pronoun is often closer than that of a non-reflexive pronoun. The same set of basic features in Table 1 is used in the entity-mention model, but feature definitions ar"
P04-1018,J96-1002,0,0.0167168,"Missing"
P04-1018,J01-4004,0,0.862961,"nt form an entity. For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded encouraging results, the way mentions are link"
P04-1018,N04-1001,0,0.0591729,"Missing"
P04-1018,M95-1005,0,0.954619,"Each system entity is constrained to align with at most one reference entity, and vice versa. For example, suppose that a reference document contains three entities: while a system out   , then puts four entities: the best alignment (from reference to system) would be   , and other entities are not aligned. The number of common mentions of and ), which leads to the best alignment is (i.e., a mention recall  and precision  . The ECM-F measures the percentage of mentions that are in the “right” entities. For tests on the MUC data, we report both F-measure using the official MUC score (Vilain et al., 1995) and ECM-F. The MUC score counts the common links between the reference and the system output. [+ ] *[ + ] G [ +  G + ( ]G G [ + ^] G ]5 ]G ] G ]5 * [+ +  [+ ( [+ [+ [ + G + ] [ +  G + ( ] [ + ( ]  + + (  5.2 Results on the ACE data & The system is first developed and tested using the ACE data. The ACE coreference system is trained with   documents (about words) of  ACE 2002 training words) is used as data. A separate documents ( the development-test (Devtest) set. In 2002, NIST released two test sets in February (Feb02) and September (Sep02), respectively. Statistics of"
P04-1018,P03-1023,0,0.778615,"For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded encouraging results, the way mentions are linked is arguably subo"
P04-1018,W98-1119,0,\N,Missing
P04-1018,C98-2138,0,\N,Missing
P07-1131,N04-1001,0,0.0250948,"nly partial interviews is due to the high cost of transcription and annotation. Since those partial interviews had already been transcribed for speech recognition purpose, we decided to reuse them in our annotation. In addition, we transcribed and annotated 20 complete interviews (each interview is about 2 hours) for building the development and test sets, in order to give a more accurate assessment of extraction performance. 4.3 Implementation We developed the initial entity detection, relation detection, and event detection systems using the same techniques as our submission systems to ACE (Florian et al., 2004). Our submission systems use statistical approaches, and have ranked in the top tier in ACE evaluations. We easily built the models for our application by retraining existing systems with our training set. The entity detection task is accomplished in two steps: mention detection and co-reference resolution. The mention detection is formulated as a la1044 Figure 2: Social network extracted by the system. beling problem, and a maximum-entropy classifier is trained to identify all the mentions. Similarly, relation detection is also cast as a classification problem — for each pair of mentions, the"
P07-1131,H05-1003,0,0.0504105,"Missing"
P07-1131,P04-1018,1,0.812882,"rotherOf (Mark, Tim). However, if the co-reference system mistakenly links “John” to “his” in the second clause and links “Tim” to “his” in the first clause, then we will still have a network with four people, but the ties will be: motherOf (Mary, Tim), and brotherOf (Mark, John), which are completely wrong. This example shows that co-reference errors involving mentions that are relation arguments can lead to very bad performance in social network extraction. Our existing co-reference module is a state-ofthe-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). It traverses the document from left to right and uses a mention-synchronous approach to decide whether a mention should be merged with an existing entity or start a new entity. However, our existing system has shortcomings for this data: the system lacks features for handling conversational speech, and the system often makes mistakes in pronoun resolution. Resolving pronominal references is very important for extracting social networks from conversational speech, as illustrated in the previous example. 3.3 Improving Co-reference for Conversational Speech We developed a new co-reference resol"
P07-1131,P05-1020,0,0.0120733,"ther a mention should be merged with an existing entity or start a new entity. However, our existing system has shortcomings for this data: the system lacks features for handling conversational speech, and the system often makes mistakes in pronoun resolution. Resolving pronominal references is very important for extracting social networks from conversational speech, as illustrated in the previous example. 3.3 Improving Co-reference for Conversational Speech We developed a new co-reference resolution system for conversational speech transcripts. Similar to many previous works on co-reference (Ng, 2005), we cast the problem as a classification task and solve it in two steps: (1) train a classifier to determine whether two mentions are co-referent or not, and (2) use a clustering algorithm to partition the mentions into clusters, based on the pairwise predictions. We added many features to our model specifi1042 cally designed for conversational speech, and significantly improved the agglomerative clustering used for co-reference, including integrating relations as constraints, and designing better cluster linkage methods and clustering stopping criteria. 3.3.1 Adding Features for Conversation"
P14-1081,C04-1046,0,0.317297,"y measurement for MT post editing since the reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whe"
P14-1081,W12-3102,0,0.110492,"Missing"
P14-1081,W12-3110,0,0.0124939,"e ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output unless they spend time to analyze ∗ This work was done when the author was with IBM Research. 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics source side of the MT training corpus, which, combined with features from translation output, achieved significantly superior performance in the MT QE evaluation. Felice and Specia (2012) investigates the impact of a large set of linguistically inspired features on quality estimation accuracy, which are not able to outperform the shallower features based on word statistics. Gonz´alezRubio et al. (2013) proposed a principled method for performing regression for quality estimation using dimensionality reduction techniques based on partial least squares regression. Given the feature redundancy in MT QE, their approach is able to improve prediction accuracy while significantly reducing the size of the feature sets. it can not be adapted to provide accurate estimates on the outputs"
P14-1081,D10-1044,0,0.0622853,"Missing"
P14-1081,D11-1125,0,0.0213707,"e pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with weights estimated by PRO (Hopkins and May, 2011) in the decoding process, and achieves state-of-the-art translation performance in various Arabic-English translation evaluations (NIST MT2008, GALE and BOLT projects). 4 4.1 Features for MT QE The features for quality estimation should reflect the complexity of the source sentence and the decoding process. Therefore we conduct syntactic analysis on the source sentences, extract features from the decoding process and select the following 26 features: • 17 decoding features, including phrase translation probabilities (source-to-target and target-to-source), word translation probabilities (also"
P14-1081,H05-1012,1,0.697822,"62 Figure 1: Adaptive QE for document-specific MT system. score or translation error rate of the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER interchangably. In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with we"
P14-1081,P11-1022,1,0.890138,"-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output unless they spend time to analyze ∗ This work was done when the author"
P14-1081,N07-1008,1,0.754885,"f the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER interchangably. In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with weights estimated by PRO (Hopkins and May, 2011) in the decoding process, and achieves state-"
P14-1081,C96-2141,0,0.0501762,"of the source sentence to the 862 Figure 1: Adaptive QE for document-specific MT system. score or translation error rate of the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER interchangably. In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether th"
P14-1081,D09-1074,0,0.0224513,"utomatically segmented into sentences, which are also called segments. Thus in the rest of the paper we will use sentences and segments interchangeably. Our parallel corpora includes tens of millions of sentence pairs covering a wide range of topics. Building a general MT system using all the parallel data not only produces a huge translation model (unless with very aggressive pruning), the performance on the given input document is suboptimal due to the unwanted dominance of out-of-domain data. Past research suggests using weighted sentences or corpora for domain adaptation (Lu et al., 2007; Matsoukas et al., 2009; Foster et al., 2010). Here we adopt the same strategy, building a documentspecific translation model for each input document. The document-specific system is built based on sub-sampling: from the parallel corpora we select sentence pairs that are the most similar to the sentences from the input document, then build the MT system with the sub-sampled sentence pairs. The similarity is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the sta"
P14-1081,P10-1062,0,0.155701,"from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output unless they spend time"
P14-1081,quirk-2004-training,0,0.0256409,"is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the standard pipeline in SMT system building: word alignRelated Work There has been a long history of study in confidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Calliso"
P14-1081,E12-3001,0,0.0259277,"Missing"
P14-1081,2006.amta-papers.25,0,0.0852742,"nt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops with the “Quality Estimation” shared task. Bic¸ici et al. (2013) proposes a number of features measuring the similarity of the source sentence to the 862 Figure 1: Adaptive QE for document-specific MT system. score or translation error rate of the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER interchangably. In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall"
P14-1081,P10-1063,0,0.25206,"on of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output unless they spend time to analyze ∗ This work was"
P14-1081,2009.mtsummit-papers.16,0,0.171072,"reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output u"
P14-1081,D07-1036,0,\N,Missing
P14-1081,W13-2201,0,\N,Missing
P19-1451,D17-1130,1,0.927908,"beled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and"
P19-1451,D15-1041,1,0.831875,"nd concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without 4586 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586–4592 c Florence, Italy, July 28 - August 2, 2019. 2019 Association"
P19-1451,D16-1211,1,0.880434,"oracle action sequences.(As mentioned above, the oracle upper bound is only 93.3 F1. With the enhanced alignments, BO reported 89.5 F1 in the LDC2014 development set). Second, even for the perfectly aligned sentences, the oracle action sequence is not the only or the best action sequence that can lead to the gold graph; there could be shorter sequences that are easier to learn. Therefore, strictly binding the training objective to the oracle action sequences can lead to suboptimal performance, as evidenced in (Daumé III and Marcu, 2005; Daumé III et al., 2009; Goldberg and Nivre, 2012, 2013; Ballesteros et al., 2016) among others. To circumvent these issues, we resort to a Reinforcement Learning (RL) objective where the Smatch score of the predicted graph for a given sentence is used as reward. This alleviates the strong dependency on hard alignment and leaves room to training with exploration of the action space. This line of work is also motivated by Goodman et al. (2016), who used imitation learning to build AMR parses from dependency trees. We use the self-critical policy gradient training algorithm by Rennie et al. (2017) which is a special case of the REINFORCE algorithm of Williams (1992) with a ba"
P19-1451,W13-2322,0,0.430589,"ck-LSTM transition-based AMR parser (Ballesteros and Al-Onaizan, 2017) by augmenting training with Policy Learning and rewarding the Smatch score of sampled graphs. In addition, we also combined several AMR-to-text alignments with an attention mechanism and we supplemented the parser with pre-processed concept identification, named entities and contextualized embeddings. We achieve a highly competitive performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-"
P19-1451,P13-2131,0,0.656993,"tural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also c"
P19-1451,P15-3007,0,0.0187689,"d Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros a"
P19-1451,E17-1053,0,0.0949736,"TMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it"
P19-1451,P15-1033,1,0.866449,"sentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017;"
P19-1451,S16-1186,0,0.0728509,"ents AMR annotations do not provide alignments between the nodes of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser. In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables. In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism. Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et"
P19-1451,P14-1134,0,0.723706,"ild upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM trans"
P19-1451,P17-1043,0,0.0327329,"of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser. In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables. In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism. Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et al. (2014) with those of the rule-based algorithm (JAMR) of Flanigan"
P19-1451,C12-1059,0,0.0596808,"graph nodes introduce noise into oracle action sequences.(As mentioned above, the oracle upper bound is only 93.3 F1. With the enhanced alignments, BO reported 89.5 F1 in the LDC2014 development set). Second, even for the perfectly aligned sentences, the oracle action sequence is not the only or the best action sequence that can lead to the gold graph; there could be shorter sequences that are easier to learn. Therefore, strictly binding the training objective to the oracle action sequences can lead to suboptimal performance, as evidenced in (Daumé III and Marcu, 2005; Daumé III et al., 2009; Goldberg and Nivre, 2012, 2013; Ballesteros et al., 2016) among others. To circumvent these issues, we resort to a Reinforcement Learning (RL) objective where the Smatch score of the predicted graph for a given sentence is used as reward. This alleviates the strong dependency on hard alignment and leaves room to training with exploration of the action space. This line of work is also motivated by Goodman et al. (2016), who used imitation learning to build AMR parses from dependency trees. We use the self-critical policy gradient training algorithm by Rennie et al. (2017) which is a special case of the REINFORCE algor"
P19-1451,Q13-1033,0,0.0548568,"Missing"
P19-1451,P16-1001,0,0.127283,"lts. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contr"
P19-1451,D18-1198,0,0.620886,"h study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1)"
P19-1451,N15-1142,0,0.0320197,"tities from the AMR dataset (there are more than 100 entity types in the AMR language) and we trained a neural network NER model (Ni et al., 2017) to predict NER labels for the AMR parser. In the NER model, the target word and its surrounding words and tags are used as features. We jackknifed (90/10) the training data, to train the AMR parser. The ten jackknifed models got an average NER F1 score of 79.48 on the NER dev set. 2.4 Contextualized Vectors Recent work has shown that the use of pre-trained networks improves the performance of downstream tasks. BO uses pre-trained word embeddings by Ling et al. (2015) along with learned character embeddings. In this work, we explore the effect of using contextualized word vectors as pre-trained word embeddings. We experiment with recent context based embedding obtained with BERT (Devlin et al., 2018). We use average of last 4 layers of BERT Large model with hidden representations of size 1024. We produce the word representation by mean pooling the representations of word piece tokens obtained using BERT. We only use the contextualized word vectors as input to our model, we do not back-propagate through the BERT layers. 2.5 Wikification Given that BO does n"
P19-1451,D18-1264,0,0.347593,"with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM depend"
P19-1451,D15-1166,0,0.0127201,"ed and fill in intermediate nodes again. Soft Alignments via Attention: The parser state is represented by the STACK, BUFFER and a list with the history of actions (which are encoded as LSTMs, the first two being Stack-LSTMs (Dyer et al., 2015)). This forms the vector st that represents the state: st = max {0, W[stt ; bt ; at ] + d} . (1) This vector st is used to predict the best action (and concept to add, if applicable) to take, given the state with a softmax. We complement the state with an attention over the input sentence (Bahdanau et al., 2014). In particular, we use general attention (Luong et al., 2015). In order to do so, we add a bidirectional LSTM encoder to the BO parsing model and we run attention over it in each time step. More formally, the attention weights αi (for position i) are calculated based on the actions predicted so far (represented as aj ), the encoder representation of the sentence (hi ) and a projection weight matrix Wa : ei = a> j Wa hi (2) exp(ei ) . αi = P k exp(ek ) (3) A vector representation (cj ) is computed by a weighted sum of the encoded sentence word representations and the α values. X cj = αi · hi . (4) i 2 https://isi.edu/~damghani/papers/ Aligner.zip 3 When"
P19-1451,P18-1037,0,0.310823,"ach of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of diffe"
P19-1451,S16-1166,0,0.0434133,"trics represent the structure and semantic parsing task. For all the remaining metrics, our parser consistently achieves the second best results. Also, our best single model (16) achieves more than 9 Smatch points on top of BO (0). Guo and Lu (2018)’s parser is a reimplementation of BO with a refined search space (which we did not attempt) and we beat their performance by 5 points. 5 BO reported results on the 2014 dataset. LDC2016E25 and LDC2017T10 contain the same AMR annotations as of March 2016. LDC2017T10 is the general release while LDC2016E25 was released for Semeval 2016 participants (May, 2016). 4589 6 Id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Experiment BO (JAMR) BO + Label (JAMR) BO + Label 2 + POS 3 + DEP 4 + NER 5 + Concepts 6 + BERT 1 + Attention 8 + POS 9 + DEP 10 + NER 11 + Concepts 12 + BERT11 13 + Smatch 8 + BERT 14 + RL Zhang et al. (2019) Lyu and Titov (2018) van Noord and Bos (2017) Guo and Lu (2018) Smatch 65.9 67.0 68.3 69.0 69.4 69.8 70.9 72.9 69.8 70.4 70.7 70.8 71.8 73.1 73.6 73.4 75.5 76.3 74.4 71.0 69.8 Unlabeled 71 72 73 74 75 75 76 78 75 75 75 76 77 78 78 78 80 79 77 74 74 No WSD 66 68 69 70 70 70 71 73 70 71 71 71 72 74 74 74 76 77 76 72 72 Named Entities 80"
P19-1451,P17-1135,1,0.902876,"Missing"
P19-1451,W03-3017,0,0.289676,"Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without 4586 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586–4592 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics any preprocessing required.1 We use Dynet (Neubig et al., 2017) to implement the parser. In what follows, we present several additions to the original BO model that improved the resul"
P19-1451,J08-4003,0,0.114297,"Missing"
P19-1451,N18-1202,0,0.0509141,"Missing"
P19-1451,D14-1048,0,0.535358,"ased parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has been significant research in that direction (Flanigan et al., 2014; Pourdamghani et al., 2014; Chen, 2015; Chu and Kurohashi, 2016; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). In this paper, we show that combination of different methods makes a positive impact. We also combine hard alignments with an attention mechanism (Bahdanau et al., 2014). (2) Preprocessing of named entities and concepts. (3) Incorporating contextualized vectors (with BERT) and compare their effectiveness with detailed ablation experiments. (4) Employing policy gradient training algorithm that uses Smatch as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of B"
P19-1451,D17-1129,0,0.197202,"s between the nodes of an AMR graph and the tokens in the corresponding sentence. We need such alignments to generate action sequences with an oracle for training. The parser is then trained to generate these action sequences. The quality of word-to-graph alignments has a direct impact in the accuracy of the parser. In previous work, both rule-based (Flanigan et al., 2014) and machine learning (Pourdamghani et al., 2014) methods have been used to produce word-to-graph alignments. Once generated, the alignments are often not updated during training (Flanigan et al., 2016; Damonte et al., 2016; Wang and Xue, 2017; Foland and Martin, 2017). More recently, Lyu and Titov (2018) learn these alignments as latent variables. In this work, we combine pre-learned (hard) alignments with an attention mechanism. As shown in section 4, the combination has a synergistic effect. In the following, we first explain our method for producing hard alignments and then we elaborate on the attention mechanism. Hard Alignments Generation: In order to produce word-to-graph alignments, we combine the outputs of the symmetrized Expectation Maximization approach (SEM) of Pourdamghani et al. (2014) with those of the rule-based al"
P19-1451,P15-2141,0,0.142384,"e performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition,"
P19-1451,N15-1040,0,0.231316,"e performance that is comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition,"
P19-1451,W03-3023,0,0.169656,"as reward. 2 Stack-LSTM AMR Parser We use the Stack-LSTM transition based AMR parser of Ballesteros and Al-Onaizan (2017) (henceforth, we refer to it as BO). BO follows the Stack-LSTM dependency parser by Dyer et al. (2015). This approach allows unbounded lookahead and makes use of greedy inference. BO also learns character-level word representations to capitalize on morphosyntactic regularities (Ballesteros et al., 2015). BO uses recurrent neural networks to represent the stack data structures that underlie many linear-time parsing algorithms. It follows transition-based parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2003, 2008); words are read from a buffer and they are incrementally combined, in a stack, with a set of actions towards producing the final parse. The input is a sentence and the output is a complete AMR graph without 4586 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586–4592 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics any preprocessing required.1 We use Dynet (Neubig et al., 2017) to implement the parser. In what follows, we present several additions to the original BO model that impro"
P19-1451,P19-1009,0,0.337492,"Missing"
P19-1451,D16-1065,0,0.058624,"s comparable to the best published results. We show an indepth study ablating each of the new components of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several"
P19-1451,N18-1106,0,0.0653911,"Missing"
P19-1451,N18-2023,0,0.0336166,"ents of the parser. 1 Introduction Abstract meaning representations (AMRs) (Banarescu et al., 2013) are rooted labeled directed acyclic graphs that represent a non intersentential abstraction of natural language with broad-coverage semantic representations. AMR parsing thus requires solving several natural language processing tasks; named entity recognition, word sense disambiguation and joint syntactic and semantic role labeling. AMR parsing has acquired a lot of attention (Wang et al., 2015a; Zhou et al., 2016; Wang et al., 2015b; Goodman et al., 2016; Guo and Lu, 2018; Lyu and Titov, 2018; Vilares and Gómez-Rodríguez, 2018; Zhang et al., 2019) in recent years. We build upon a transition-based parser (Ballesteros and Al-Onaizan, 2017) that uses StackLSTMs (Dyer et al., 2015). We augment training with self-critical policy learning (Rennie et al., 2017) using sentence-level Smatch scores (Cai and Knight, 2013) as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of tokenlevel AMR-to-text alignments. In addition, we perform several modifications which are inspired Our contributions are: (1) combinations of different alignment methods: There has b"
P92-1024,H92-1026,1,0.761021,"tences. In the second task, that of selecting a parse from the many parses proposed by a grammar, can best be done by maximum likelihood estimation constrained by a large treebank. The use of a large treebank allows the development of sophisticated statistical models that should outperform the traditional approach of using human intuition to develop parse preference strategies. We describe in this paper a model based on probabilistic context-free grammars estimated with a constrained version of the Inside-Outside algorithm (see Section 4)that can be used for picking a parse for a sentence. In [2], we desrcibe a more sophisticated stochastic g r a m m a r that achieves even higher parsing accuracy. 3. G r a m m a r Our g r a m m a r is a feature-based context-free phrase structure g r a m m a r employing traditional syntactic categories. Each of its roughly 700 ""rules"" is actually a rule template, compressing a family of related productions via unification. 7 Boolean conditions on values of variables occurring within these rule templates serve to limit their ambit where necessary. To illustrate, the rule template below s f 2 : V1 f 3 : V2 ~ f 2 : V1 f 3 : V3 f 2 : V1 f 3 : V2 While a n"
P92-1024,H91-1060,0,0.0580436,"Missing"
P92-1024,P93-1005,1,\N,Missing
P93-1005,P91-1027,0,0.0106549,"Missing"
P93-1005,H90-1055,1,0.820077,"Missing"
P93-1005,A88-1019,0,0.0913946,"Missing"
P93-1005,H90-1056,0,0.0309426,"Missing"
P93-1005,H90-1052,0,0.0403796,"Missing"
P93-1005,E91-1004,0,0.436285,"Missing"
P93-1005,H90-1054,0,0.105119,"Missing"
P93-1005,H91-1067,0,\N,Missing
P93-1005,J92-4003,1,\N,Missing
P93-1005,1991.iwpt-1.22,1,\N,Missing
P96-1019,P94-1010,0,0.038478,"straightforward for English, it is not trivial to collect statistics for Chinese words since word boundaries are not marked in written Chinese text. Chinese is a morphosyllabic language (DeFrancis, 1984) in that almost all Chinese characters represent a single syllable and most Chinese characters are also morphemes. Since a word can be multi-syllabic, it is generally non-trivial to segment a Chinese sentence into words(Wu and Tseng, 1993). Since segmentation is a fundamental problem in Chinese information processing, there is a large literature to deal with the problem. Recent work includes (Sproat et al., 1994) and (Wang et al., 1992). In this paper, we adopt a statistical approach to segment Chinese text based on an LM because of its autonomous nature and its capability to handle unseen words. As far as speech recognition is concerned, what is needed is a model to assign a probability to a string of characters. One may argue that we could bypass the segmentation problem by building a characterbased LM. However, we have a strong belief that a word-based LM would be better than a characterbased 1 one. In addition to speech recognition, the use of word based models would have value in information retr"
P96-1019,C92-4199,0,\N,Missing
P97-1022,J93-2003,1,0.0147769,". O . B o x 218 Y o r k t o w n H e i g h t s , N Y 10598, U S A (*Now W i t h R e n a i s s a n c e T e c h n o l o g i e s , S t o n y b r o o k , NY, U S A ) sdella@rentec, tom [meps/roukos/tward] ©watson. ibm. com Abstract model p, = are maxp(F[E) --- are m a x p ( F , E). Several recent efforts in statistical natural language understanding (NLU) have focused on generating clumps of English words from semantic meaning concepts (Miller et al., 1995; Levin and Pieraccini, 1995; Epstein et al., 1996; Epstein, 1996). This paper extends the IBM Machine Translation Group's concept of fertility (Brown et al., 1993) to the generation of clumps for natural language understanding. The basic underlying intuition is that a single concept may be expressed in English as many disjoint clump of words. We present two fertility models which attempt to capture this phenomenon. The first is a Poisson model which leads to appealing computational simplicity. The second is a general nonparametric fertility model. The general model's parameters are bootstrapped from the Poisson model and updated by the EM algorithm. These fertility models can be used to impose clump fertility structure on top of preexisting clump genera"
P97-1022,H90-1021,0,0.0158113,"vel Information Service (ATIS] domain. 1 over all F over all F We have previously built a fully automatic statistical NLU system (Epstein et al., 1996) based on the source-channel factorization of the joint distribution p ( f , E) p ( f , E) = p(f)p(ZlF ). This factorization, which has proven effective in speech recognition (Bahl, Jelinek, and Mercer, 1983), partitions the joint probability into an a priori intention model p(F), and a translation model p(E[F) which models how a user might phrase a request F in English. For the ATIS task, our formal language is a minor variant of the NL-Parse (Hemphill, Godfrey, and Doddington, 1990) used by ARPA to annotate the ATIS corpus. An example of a formal and natural language pair is: • F : List flights from New Orleans to Memphis flying on Monday departing early_morning • E: do you have any flights going to Memphis leaving New Orleans early Monday morning Here, the evidence for the formal language concept 'early_morning' resides in the two disjoint clumps of English 'early' and 'morning'. In this paper, we introduce the notion of concept fertility into our translation models p(EIF ) to capture this effect and the more general linguistic phenomenon of embedded clauses. Basically"
P97-1022,P96-1008,0,0.0249978,"ral language understanding (NLU) system is to interpret a user's request and respond with an appropriate action. We view this interpretation as translation from a natural language expression, E, into an equivalent expression, F, in an unambigous formal language. Typically, this formal language will be hand-crafted to enhance performance on some task-specific domain. A statistical NLU system translates a request E as the most likely formal expression ~' according to a probability 168 used to generate the SQL query for ATIS. More recently, BBN has replaced handwritten rules with decision trees (Miller et al., 1996). Moreover, both systems were trained using English annotated by hand with segmentation and labeling, and both systems produce a semantic representation which is forced to preserve the time order expressed in the English. Interestingly, both the AT&T and BBN systems generate words within a clump according to bigram models. Other statistical approachs to NLU include decision trees (Kuhn and Mori, 1995) and neural nets (Gorin et al., 1991). In earlier IBM translation systems (Brown et al., 1993) each English word would be generated by, or ""aligned to"", exactly one formal language word. This mapp"
W02-0209,W00-0305,0,0.061368,"Missing"
W02-0209,A00-1014,0,0.0519332,"Missing"
W02-0209,W00-0303,0,0.247667,"Missing"
W02-0209,A00-1010,0,0.0476826,"Missing"
W02-0209,P90-1010,0,0.152908,"Missing"
W02-0209,W00-0309,0,0.10284,"Missing"
W02-0209,H01-1073,0,\N,Missing
