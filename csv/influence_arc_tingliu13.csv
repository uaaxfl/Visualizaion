2020.acl-demos.2,P19-1595,0,0.104085,"f transferring knowledge from a teacher model to a student model, which is usually smaller than the teacher. The student model is trained to mimic the outputs of the teacher model. Before the birth of BERT, KD had been applied to several specific tasks like machine translation (Kim and Rush, 2016; Tan et al., 2019) in NLP. While the recent studies of distilling large pre-trained models focus on finding general distillation methods that work on various tasks and are receiving more and more attention (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Tang et al., 2019; Liu et al., 2019a; Clark et al., 2019; Zhao et al., 2019). Though various distillation methods have been proposed, they usually share a common workflow: firstly, train a teacher model, then optimize the student model by minimizing some losses that are calculated between the outputs of the teacher and the student. Therefore it is desirable to have a reusable distillation workflow framework and treat different distillation strategies and tricks as plugins so that they could be easily and arbitrarily added to the framework. In this way, we could also achieve great flexibility in experimenting with different combinations of distillat"
2020.acl-demos.2,C18-1166,0,0.0258997,"Missing"
2020.acl-demos.2,D18-1269,0,0.0161826,"nd CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) for named entity recognition (NER) task. For Chinese datasets, we use the Chinese part of XNLI 4 More results are presented in the online documentation: https://textbrewer.readthedocs.io 12 Dataset Task Metrics #Train #Dev MNLI SQuAD CoNLL-2003 Classification MRC NER Acc EM/F1 F1 393K 88K 23K 20K 11K 6K XNLI LCQMC CMRC 2018 DRCD Classification Classification MRC MRC Acc Acc EM/F1 EM/F1 393K 293K 10K 27K 2.5K 8.8K 3.4K 3.5K Model Table 1: A summary of the datasets used in experiments. The size of CoNLL-2003 is measured in number of entities. (Conneau et al., 2018), LCQMC (Liu et al., 2018), CMRC 2018 (Cui et al., 2019b) and DRCD (Shao et al., 2018). XNLI is the multilingual version of MNLI. LCQMC is a large-scale Chinese question matching corpus. CMRC 2018 and DRCD are two span-extraction machine reading comprehension datasets similar to SQuAD. The statistics of the datasets are listed in Table 1. Models. All the teachers are BERTBASE -based models. For English tasks, teachers are initialized with the weights released by Google5 and converted into PyTorch format via Transformers6 . For Chinese tasks, teacher is initialized with the pre-trained RoBERTa-"
2020.acl-demos.2,2021.ccl-1.108,0,0.0842688,"Missing"
2020.acl-demos.2,D16-1264,0,0.0298942,"rocedures for all NLP tasks, we encourage users to implement their own evaluation functions as the callbacks for the best practice. 4 Experiments In this section, we conduct several experiments to show TextBrewer’s ability to distill large pretrained models on different NLP tasks and achieve results are comparable with or even higher than the public distilled BERT models with similar numbers of parameters. 4 4.1 Settings Datasets and tasks. We conduct experiments on both English and Chinese datasets. For English datasets, We use MNLI (Wang et al., 2019) for text classification task, SQuAD1.1 (Rajpurkar et al., 2016) for span-extraction machine reading comprehension (MRC) task and CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) for named entity recognition (NER) task. For Chinese datasets, we use the Chinese part of XNLI 4 More results are presented in the online documentation: https://textbrewer.readthedocs.io 12 Dataset Task Metrics #Train #Dev MNLI SQuAD CoNLL-2003 Classification MRC NER Acc EM/F1 F1 393K 88K 23K 20K 11K 6K XNLI LCQMC CMRC 2018 DRCD Classification Classification MRC MRC Acc Acc EM/F1 EM/F1 393K 293K 10K 27K 2.5K 8.8K 3.4K 3.5K Model Table 1: A summary of the datasets used in experimen"
2020.acl-demos.2,D19-1600,1,0.830779,"d entity recognition (NER) task. For Chinese datasets, we use the Chinese part of XNLI 4 More results are presented in the online documentation: https://textbrewer.readthedocs.io 12 Dataset Task Metrics #Train #Dev MNLI SQuAD CoNLL-2003 Classification MRC NER Acc EM/F1 F1 393K 88K 23K 20K 11K 6K XNLI LCQMC CMRC 2018 DRCD Classification Classification MRC MRC Acc Acc EM/F1 EM/F1 393K 293K 10K 27K 2.5K 8.8K 3.4K 3.5K Model Table 1: A summary of the datasets used in experiments. The size of CoNLL-2003 is measured in number of entities. (Conneau et al., 2018), LCQMC (Liu et al., 2018), CMRC 2018 (Cui et al., 2019b) and DRCD (Shao et al., 2018). XNLI is the multilingual version of MNLI. LCQMC is a large-scale Chinese question matching corpus. CMRC 2018 and DRCD are two span-extraction machine reading comprehension datasets similar to SQuAD. The statistics of the datasets are listed in Table 1. Models. All the teachers are BERTBASE -based models. For English tasks, teachers are initialized with the weights released by Google5 and converted into PyTorch format via Transformers6 . For Chinese tasks, teacher is initialized with the pre-trained RoBERTa-wwm-ext 7 (Cui et al., 2019a). We test the performance"
2020.acl-demos.2,N19-1423,0,0.0382516,"ain a teacher model, then optimize the student model by minimizing some losses that are calculated between the outputs of the teacher and the student. Therefore it is desirable to have a reusable distillation workflow framework and treat different distillation strategies and tricks as plugins so that they could be easily and arbitrarily added to the framework. In this way, we could also achieve great flexibility in experimenting with different combinations of distillation strategies and comparing their effects. Introduction Large pre-trained language models, such as GPT (Radford, 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) and XLNet (Yang et al., 2019) have achieved great success in many NLP tasks and greatly contributed to the progress of NLP research. However, one big issue of these models is the high demand for computing resources — they usually have hundreds of millions of parameters, and take several gigabytes of memory to train and inference — which makes it impractical to deploy them on mobile devices or online systems. From a research point of view, we are tempted to ask: is it necessary to have such a big model that contains hundreds of millions of parameters to achieve a h"
2020.acl-demos.2,D19-1441,0,0.0537409,"Missing"
2020.acl-demos.2,D16-1139,0,0.0315073,"tillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters. 1 1 KD is a technique of transferring knowledge from a teacher model to a student model, which is usually smaller than the teacher. The student model is trained to mimic the outputs of the teacher model. Before the birth of BERT, KD had been applied to several specific tasks like machine translation (Kim and Rush, 2016; Tan et al., 2019) in NLP. While the recent studies of distilling large pre-trained models focus on finding general distillation methods that work on various tasks and are receiving more and more attention (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Tang et al., 2019; Liu et al., 2019a; Clark et al., 2019; Zhao et al., 2019). Though various distillation methods have been proposed, they usually share a common workflow: firstly, train a teacher model, then optimize the student model by minimizing some losses that are calculated between the outputs of the teacher and the student. T"
2020.acl-demos.2,D19-6122,0,0.0225655,"of parameters. 1 1 KD is a technique of transferring knowledge from a teacher model to a student model, which is usually smaller than the teacher. The student model is trained to mimic the outputs of the teacher model. Before the birth of BERT, KD had been applied to several specific tasks like machine translation (Kim and Rush, 2016; Tan et al., 2019) in NLP. While the recent studies of distilling large pre-trained models focus on finding general distillation methods that work on various tasks and are receiving more and more attention (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Tang et al., 2019; Liu et al., 2019a; Clark et al., 2019; Zhao et al., 2019). Though various distillation methods have been proposed, they usually share a common workflow: firstly, train a teacher model, then optimize the student model by minimizing some losses that are calculated between the outputs of the teacher and the student. Therefore it is desirable to have a reusable distillation workflow framework and treat different distillation strategies and tricks as plugins so that they could be easily and arbitrarily added to the framework. In this way, we could also achieve great flexibility in experimenting w"
2020.acl-demos.2,W17-2623,0,0.0218015,"he teacher and various students on Chinese tasks. Table 4: Results of multi-teacher distillation. All the models are BERTBASE . Different teachers are trained with different random seeds. For each task, the ensemble is the average of three teachers’ results. CMRC 2018 has a relatively small training set, DA has a much more significant effect. cases. It may be a hint that narrow-and-deep models are better than wide-and-shallow models. Finally, data augmentation (DA) is critical. For the experiments in the last line in Table 2, we use additional datasets during distillation: a subset of NewsQA (Trischler et al., 2017) training set is used in SQuAD; passages from the HotpotQA (Yang et al., 2018) training set is used in CoNLL-2003. The augmentation datasets significantly improve the performance, especially when the size of the training set is small, like CoNLL-2003. We next show the effectiveness of MultiTeacherDistiller, which distills an ensemble of teachers to a single student model. For each task, we train three BERTBASE teacher models with different seeds. The student is also a BERTBASE model. The temperature is set to 8, and intermediate losses are not used. As Table 4 shows, for each task, the student"
2020.acl-demos.2,D18-1259,0,0.0256078,"distillation. All the models are BERTBASE . Different teachers are trained with different random seeds. For each task, the ensemble is the average of three teachers’ results. CMRC 2018 has a relatively small training set, DA has a much more significant effect. cases. It may be a hint that narrow-and-deep models are better than wide-and-shallow models. Finally, data augmentation (DA) is critical. For the experiments in the last line in Table 2, we use additional datasets during distillation: a subset of NewsQA (Trischler et al., 2017) training set is used in SQuAD; passages from the HotpotQA (Yang et al., 2018) training set is used in CoNLL-2003. The augmentation datasets significantly improve the performance, especially when the size of the training set is small, like CoNLL-2003. We next show the effectiveness of MultiTeacherDistiller, which distills an ensemble of teachers to a single student model. For each task, we train three BERTBASE teacher models with different seeds. The student is also a BERTBASE model. The temperature is set to 8, and intermediate losses are not used. As Table 4 shows, for each task, the student achieves the best performance, even higher than the ensemble result. 5 XNLI A"
2020.acl-demos.2,W03-0419,0,\N,Missing
2020.acl-main.10,N18-1014,0,0.0126173,"act type control the style of sentence. To improve generalization capability of DA, delexicalization technique (Wen et al., 2015a,b; Duˇsek and Jurˇc´ıcˇ ek, 2016; Tran and Nguyen, 2017a) is widely used to replace all values in reference sentence by their corresponding slot in DA, creating pairs of delexicalized input DAs and output templates. Hence the most important step in NLG is to generate templates correctly given an input DA. However, this step can introduce missing and misplaced slots, because of modeling errors or unaligned training data (Balakrishnan et al., 2019; Nie et al., 2019; Juraska et al., 2018). Lexicalization is followed after a template is generated, replacing slots in template with corresponding values in DA. 2.2 (1) ρ(x, yi ) = #({s |s = t; t ∈ yi ; s ∈ x}), (3) where function # computes the size of a set. During evaluation, system f KNN first ranks the templates in set Y by distant function ρ and then selects the top k (beam size) templates. 3 Architeture Figure 1 shows the architecture of Iterative Rectification Network. It consists of two components: a pointer rewriter to produce templates with improved performance metrics and an experience replay buffer to gather and sample"
2020.acl-main.10,J14-4003,0,0.0247447,"ency. The third iteration achieves slot consistency, after which a natural language, though slightly different from the reference text, is generated via lexicalization. 7 Related Work Conventional approaches for solving NLG task are mostly pipeline-based, dividing it into sentence planning and surface realisation (Dethlefs et al., 2013; Stent et al., 2004; Walker et al., 2002). Oh and Rudnicky (2000) introduce a class-based ngram language model and a rule-based reranker. Ratnaparkhi (2002) address the limitations of ngram language models by using more sophisticated syntactic dependency trees. Mairesse and Young (2014) employ a phrase-based generator that learn from a semantically aligned corpus. Despite their robustness, these models are costly to create and maintain as they heavily rely on handcrafted rules. Recent works (Wen et al., 2015b; Duˇsek and Jurˇc´ıcˇ ek, 2016; Tran and Nguyen, 2017a) build data-driven models based on end-to-end learning. Wen et al. (2015a) combine two recurrent neural network (RNN) based models with a CNN reranker to generate required utterances. Wen et al. (2015b) introduce a novel SC-LSTM with an additional reading cell to jointly learn gating mechanism and language model. Du"
2020.acl-main.10,P19-1256,0,0.030871,"Missing"
2020.acl-main.10,W00-0306,0,0.299465,"slot $AUDIO$ but inserts slot $PRICE$. The output template from the first iteration of IRN has a removal of the inserted $PRICE$ slot. The second iteration has improved language fluency but no progress in slot-inconsistency. The third iteration achieves slot consistency, after which a natural language, though slightly different from the reference text, is generated via lexicalization. 7 Related Work Conventional approaches for solving NLG task are mostly pipeline-based, dividing it into sentence planning and surface realisation (Dethlefs et al., 2013; Stent et al., 2004; Walker et al., 2002). Oh and Rudnicky (2000) introduce a class-based ngram language model and a rule-based reranker. Ratnaparkhi (2002) address the limitations of ngram language models by using more sophisticated syntactic dependency trees. Mairesse and Young (2014) employ a phrase-based generator that learn from a semantically aligned corpus. Despite their robustness, these models are costly to create and maintain as they heavily rely on handcrafted rules. Recent works (Wen et al., 2015b; Duˇsek and Jurˇc´ıcˇ ek, 2016; Tran and Nguyen, 2017a) build data-driven models based on end-to-end learning. Wen et al. (2015a) combine two recurren"
2020.acl-main.10,P19-1080,0,0.0469294,"eness. 1 Reference Missing Misplace Table 1: An exmaple (including mistaken generations) extracted from SF Hotel (Wen et al., 2015b) dataset. Errors are marked in colors (missing, misplaced). Tran and Nguyen, 2017a) have attracted much attention. They implicitly learn sentence planning and surface realisation end-to-end with cross entropy objectives. For example, Duˇsek and Jurˇc´ıcˇ ek (2016) employ an attentive encoder-decoder model, which applies attention mechanism over input slot value pairs. Although neural generators can be trained end-to-end, they suffer from hallucination phenomenon (Balakrishnan et al., 2019). Examples in Table 1 show a misplacement error of an unseen slot AREA and a missing error of slot NAME by an end-to-end trained model, when compared against its input DA. Motivated by this observation, in this paper, we define slot consistency of NLG systems as all slot values of input DAs shall appear in output sentences without misplacement. We also observe that, for task-oriented dialogue systems, input DAs are mostly with simple logic forms, therefore enabling retrieval-based methods e.g. K-Nearest Neighbour (KNN) to handle the majority of test cases. Furthermore, there exists a discrepan"
2020.acl-main.10,P13-1123,0,0.0577967,"Missing"
2020.acl-main.10,P16-2008,0,0.0325552,"Missing"
2020.acl-main.10,P04-1011,0,0.435422,"r (KNN) to handle the majority of test cases. Furthermore, there exists a discrepancy between the training criterion of cross entropy loss and evaluation metric of slot error rate (ERR), similarly to that observed in neural machine translation (Ranzato et al., 2015). Therefore, it is beneficial to use training methods that integrate the evaluation metrics in their objectives. Introduction Natural Language Generation (NLG), as a critical component of task-oriented dialogue systems, converts a meaning representation, i.e., dialogue act (DA), into natural language sentences. Traditional methods (Stent et al., 2004; Konstas and Lapata, 2013; Wong and Mooney, 2007) are mostly pipeline-based, dividing the generation process into sentence planing and surface realization. Despite their robustness, they heavily rely on handcrafted rules and domain-specific knowledge. In addition, the generated sentences of rule-based approaches are rather rigid, without the variance of human language. More recently, neural network based models (Wen et al., 2015a,b; Duˇsek and Jurˇc´ıcˇ ek, 2016; ∗ † inform(NAME = pickwick hotel, PRICERANGE = moderate) the hotel named pickwick hotel is in a moderate price range this is a mode"
2020.acl-main.10,K17-1044,0,0.35106,"pplies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness. 1 Reference Missing Misplace Table 1: An exmaple (including mistaken generations) extracted from SF Hotel (Wen et al., 2015b) dataset. Errors are marked in colors (missing, misplaced). Tran and Nguyen, 2017a) have attracted much attention. They implicitly learn sentence planning and surface realisation end-to-end with cross entropy objectives. For example, Duˇsek and Jurˇc´ıcˇ ek (2016) employ an attentive encoder-decoder model, which applies attention mechanism over input slot value pairs. Although neural generators can be trained end-to-end, they suffer from hallucination phenomenon (Balakrishnan et al., 2019). Examples in Table 1 show a misplacement error of an unseen slot AREA and a missing error of slot NAME by an end-to-end trained model, when compared against its input DA. Motivated by th"
2020.acl-main.10,W17-5528,0,0.186184,"pplies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness. 1 Reference Missing Misplace Table 1: An exmaple (including mistaken generations) extracted from SF Hotel (Wen et al., 2015b) dataset. Errors are marked in colors (missing, misplaced). Tran and Nguyen, 2017a) have attracted much attention. They implicitly learn sentence planning and surface realisation end-to-end with cross entropy objectives. For example, Duˇsek and Jurˇc´ıcˇ ek (2016) employ an attentive encoder-decoder model, which applies attention mechanism over input slot value pairs. Although neural generators can be trained end-to-end, they suffer from hallucination phenomenon (Balakrishnan et al., 2019). Examples in Table 1 show a misplacement error of an unseen slot AREA and a missing error of slot NAME by an end-to-end trained model, when compared against its input DA. Motivated by th"
2020.acl-main.10,W15-4639,0,0.039746,"Missing"
2020.acl-main.10,N16-1015,0,0.0929326,"ved after the last token of the utterance is generated. 5.2 N X ∇ log π(aj |hj ), j=1 (14) where θ denotes model parameters. r(a) − b is the advantage function per REINFORCE. b is a baseline. Through experiments, we find that b = BLEU(y, z) performs better (Weaver and Tao, 2001) than tricks such as simple averaging of the 1 PN likelihood N j=1 log π(aj |hj ). 6 6.1 Experiments Experiment Setup We assess the model performances on four NLG datasets of different domains. The SF Hotel and SF Restaurant benchmarks are collected in (Wen et al., 2015a) while Laptop and TV benchmarks are released by (Wen et al., 2016). Each dataset is evaluated with five strong baseline methods, including HLSTM (Wen et al., 2015a), SC-LSTM (Wen et al., 2015b), TGen (Duˇsek and Jurˇc´ıcˇ ek, 2016), ARoA (Tran and Nguyen, 2017b) and RALSTM (Tran and Nguyen, 2017a). Following these prior works, the evaluation metrics consist of BLEU and slot error rate (ERR), which is computed as (12) j=1 r(a) = γ SC rSC + γ LM rLM + γ DS rDS ∇J RL (θ) = (r(a) − b) ∗ Policy Gradient We utilize supervised learning in Eq. (9) to initialize our model with the labels extracted from distant supervision. After its convergence, we continuously tune"
2020.acl-main.10,D15-1199,0,0.0996522,"proving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness. 1 Reference Missing Misplace Table 1: An exmaple (including mistaken generations) extracted from SF Hotel (Wen et al., 2015b) dataset. Errors are marked in colors (missing, misplaced). Tran and Nguyen, 2017a) have attracted much attention. They implicitly learn sentence planning and surface realisation end-to-end with cross entropy objectives. For example, Duˇsek and Jurˇc´ıcˇ ek (2016) employ an attentive encoder-decoder model, which applies attention mechanism over input slot value pairs. Although neural generators can be trained end-to-end, they suffer from hallucination phenomenon (Balakrishnan et al., 2019). Examples in Table 1 show a misplacement error of an unseen slot AREA and a missing error of slot NAME"
2020.acl-main.10,N07-1022,0,0.0373466,"Furthermore, there exists a discrepancy between the training criterion of cross entropy loss and evaluation metric of slot error rate (ERR), similarly to that observed in neural machine translation (Ranzato et al., 2015). Therefore, it is beneficial to use training methods that integrate the evaluation metrics in their objectives. Introduction Natural Language Generation (NLG), as a critical component of task-oriented dialogue systems, converts a meaning representation, i.e., dialogue act (DA), into natural language sentences. Traditional methods (Stent et al., 2004; Konstas and Lapata, 2013; Wong and Mooney, 2007) are mostly pipeline-based, dividing the generation process into sentence planing and surface realization. Despite their robustness, they heavily rely on handcrafted rules and domain-specific knowledge. In addition, the generated sentences of rule-based approaches are rather rigid, without the variance of human language. More recently, neural network based models (Wen et al., 2015a,b; Duˇsek and Jurˇc´ıcˇ ek, 2016; ∗ † inform(NAME = pickwick hotel, PRICERANGE = moderate) the hotel named pickwick hotel is in a moderate price range this is a moderate hotel [NAME] the pickwick hotel in fort mason"
2020.acl-main.127,N19-1423,0,0.00879457,"imilarity of context and candidate responses, which can be divided into single-turn models (Wang et al., 2015) and multi-turn models (Wu et al., 2017; Zhou et al., 2018; Ma et al., 2019) based on the number of turns in context. Those methods construct the representations of the context and response with a single vector space. Consequently, the models tend to select the response with the same words . On the other hand, as those static embeddings can not cope with the phenomenon of polysemy, researchers pay more attention to contextual representations recently. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and XLNet (Yang et al., 2019) have achieved great success in many NLP tasks. However, it is difficult to apply them in the industrial dialog system due to their low computational efficiency. In this paper, we focus on the static embedding, for it is flexible and efficient. The previous works learn the embedding from intra-sentence within a single space, which is not enough for dialog systems. Specifically, the semantic correlation beyond a single sentence in the conversation pair is missing. For example, the words ‘why’ and ‘because’ usually come from different speakers, and we can not catch"
2020.acl-main.127,E17-2068,0,0.03253,"Missing"
2020.acl-main.127,D14-1181,0,0.017077,"n train the embedding on word-level and sentence-level. We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems. The experiment results show that PR-Embedding can improve the quality of the selected response. 2 1 Introduction Word embedding is one of the most fundamental work in the NLP tasks, where low-dimensional word representations are learned from unlabeled corpora. The pre-trained embeddings can reflect the semantic and syntactic information of words and help various downstream tasks get better performance (Collobert et al., 2011; Kim, 2014). The traditional word embedding methods train the models based on the co-occurrence statistics, such as Word2vec (Mikolov et al., 2013a,b), GloVe (Pennington et al., 2014). Those methods are widely used in dialog systems, not only in retrievalbased methods (Wang et al., 2015; Yan et al., 2016) but also the generation-based models (Serban et al., 1 In this paper, we name the first utterance in the conversation pair as ‘post,’ and the latter is ‘reply’ 2 PR-Embedding source code is available at https:// github.com/wtma/PR-Embedding . 2016; Zhang et al., 2018b). The retrieval-based methods predi"
2020.acl-main.127,K19-1069,1,0.830285,"used in dialog systems, not only in retrievalbased methods (Wang et al., 2015; Yan et al., 2016) but also the generation-based models (Serban et al., 1 In this paper, we name the first utterance in the conversation pair as ‘post,’ and the latter is ‘reply’ 2 PR-Embedding source code is available at https:// github.com/wtma/PR-Embedding . 2016; Zhang et al., 2018b). The retrieval-based methods predict the answer based on the similarity of context and candidate responses, which can be divided into single-turn models (Wang et al., 2015) and multi-turn models (Wu et al., 2017; Zhou et al., 2018; Ma et al., 2019) based on the number of turns in context. Those methods construct the representations of the context and response with a single vector space. Consequently, the models tend to select the response with the same words . On the other hand, as those static embeddings can not cope with the phenomenon of polysemy, researchers pay more attention to contextual representations recently. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and XLNet (Yang et al., 2019) have achieved great success in many NLP tasks. However, it is difficult to apply them in the industrial dialog system due to their low"
2020.acl-main.127,D16-1147,0,0.0265702,"e focus on the response selection tasks for retrieval-based dialogue systems both in the single-turn and multi-turn conversations. For the Personchat dataset, we use the current query for response selection in the single-turn task and conduct the experiments in no-persona track because we focus on the relationship between post and reply. Models. For the single-turn task, we compare the embeddings based on BOW (bag-of-words, the average of all word embedding vectors), and select replies by cosine similarity; For the multi-turn task, we use a neural model called key-value (KV) memory network 4 (Miller et al., 2016), which has been proved to be a strong baseline in the ConvAI2 competition (Dinan et al., 2020). Metrics. We use the recall at position k from 20 candidates (hits@k, only one candidate reply is true) as the metrics in the PersonaChat dataset following the previous work (Zhang et al., 2018a). For the Chinese dataset, we use NDCG and P@1 to evaluate the sorted quality of the candidate replies. Setup. We train the model by Adagrad (Duchi et al., 2011) and implement it by Keras (Chollet et al., 2015) with Tensorflow backend. For the PersonaChat dataset, we train the embeddings by the training set"
2020.acl-main.127,J03-1002,0,0.014605,"sk. The main contributions of our work are: (1) we propose a new method to learn the conversational word embedding from human dialogue in two different vector spaces; (2) The experimental results show that PR-Embedding can help the model select better responses and catch the semantic correlation among the conversation pair. 2 need to find the most related word from the other sequence for each word in the pair. In other words, we need to build conversational word alignment between the post and the reply. In this paper, we solve it by the word alignment model in statistical machine translation (Och and Ney, 2003). We treat the post as the source language and the reply as the target language. Then we align the words in the pair with the word alignment model and generate a cross-sentence window centered on the alignment word. Methods 2.3 2.1 Notation We consider two vocabularies for the post and the reply V p := {v1p , v2p , ..., vsp }, V r := {v1r , v2r , ..., vsr } together with two embedding matrices Ep , Er ∈ Rs×d , where s is the size of the vocabularity and d is the embedding dimension. We need to learn the embedding from the conversation pair hpost, replyi. They can be formulated as P = (p1 , ..."
2020.acl-main.127,D14-1162,0,0.0830375,"og systems. The experiment results show that PR-Embedding can improve the quality of the selected response. 2 1 Introduction Word embedding is one of the most fundamental work in the NLP tasks, where low-dimensional word representations are learned from unlabeled corpora. The pre-trained embeddings can reflect the semantic and syntactic information of words and help various downstream tasks get better performance (Collobert et al., 2011; Kim, 2014). The traditional word embedding methods train the models based on the co-occurrence statistics, such as Word2vec (Mikolov et al., 2013a,b), GloVe (Pennington et al., 2014). Those methods are widely used in dialog systems, not only in retrievalbased methods (Wang et al., 2015; Yan et al., 2016) but also the generation-based models (Serban et al., 1 In this paper, we name the first utterance in the conversation pair as ‘post,’ and the latter is ‘reply’ 2 PR-Embedding source code is available at https:// github.com/wtma/PR-Embedding . 2016; Zhang et al., 2018b). The retrieval-based methods predict the answer based on the similarity of context and candidate responses, which can be divided into single-turn models (Wang et al., 2015) and multi-turn models (Wu et al.,"
2020.acl-main.127,N18-1202,0,0.0175564,"ct the answer based on the similarity of context and candidate responses, which can be divided into single-turn models (Wang et al., 2015) and multi-turn models (Wu et al., 2017; Zhou et al., 2018; Ma et al., 2019) based on the number of turns in context. Those methods construct the representations of the context and response with a single vector space. Consequently, the models tend to select the response with the same words . On the other hand, as those static embeddings can not cope with the phenomenon of polysemy, researchers pay more attention to contextual representations recently. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and XLNet (Yang et al., 2019) have achieved great success in many NLP tasks. However, it is difficult to apply them in the industrial dialog system due to their low computational efficiency. In this paper, we focus on the static embedding, for it is flexible and efficient. The previous works learn the embedding from intra-sentence within a single space, which is not enough for dialog systems. Specifically, the semantic correlation beyond a single sentence in the conversation pair is missing. For example, the words ‘why’ and ‘because’ usually come from different sp"
2020.acl-main.127,N18-2028,0,0.0683903,"Missing"
2020.acl-main.127,P17-1046,0,0.0338898,"al., 2014). Those methods are widely used in dialog systems, not only in retrievalbased methods (Wang et al., 2015; Yan et al., 2016) but also the generation-based models (Serban et al., 1 In this paper, we name the first utterance in the conversation pair as ‘post,’ and the latter is ‘reply’ 2 PR-Embedding source code is available at https:// github.com/wtma/PR-Embedding . 2016; Zhang et al., 2018b). The retrieval-based methods predict the answer based on the similarity of context and candidate responses, which can be divided into single-turn models (Wang et al., 2015) and multi-turn models (Wu et al., 2017; Zhou et al., 2018; Ma et al., 2019) based on the number of turns in context. Those methods construct the representations of the context and response with a single vector space. Consequently, the models tend to select the response with the same words . On the other hand, as those static embeddings can not cope with the phenomenon of polysemy, researchers pay more attention to contextual representations recently. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and XLNet (Yang et al., 2019) have achieved great success in many NLP tasks. However, it is difficult to apply them in the indu"
2020.acl-main.127,P18-1205,0,0.281365,"better performance (Collobert et al., 2011; Kim, 2014). The traditional word embedding methods train the models based on the co-occurrence statistics, such as Word2vec (Mikolov et al., 2013a,b), GloVe (Pennington et al., 2014). Those methods are widely used in dialog systems, not only in retrievalbased methods (Wang et al., 2015; Yan et al., 2016) but also the generation-based models (Serban et al., 1 In this paper, we name the first utterance in the conversation pair as ‘post,’ and the latter is ‘reply’ 2 PR-Embedding source code is available at https:// github.com/wtma/PR-Embedding . 2016; Zhang et al., 2018b). The retrieval-based methods predict the answer based on the similarity of context and candidate responses, which can be divided into single-turn models (Wang et al., 2015) and multi-turn models (Wu et al., 2017; Zhou et al., 2018; Ma et al., 2019) based on the number of turns in context. Those methods construct the representations of the context and response with a single vector space. Consequently, the models tend to select the response with the same words . On the other hand, as those static embeddings can not cope with the phenomenon of polysemy, researchers pay more attention to contex"
2020.acl-main.127,C18-1206,1,0.899219,"Missing"
2020.acl-main.127,P18-1103,0,0.0235405,"methods are widely used in dialog systems, not only in retrievalbased methods (Wang et al., 2015; Yan et al., 2016) but also the generation-based models (Serban et al., 1 In this paper, we name the first utterance in the conversation pair as ‘post,’ and the latter is ‘reply’ 2 PR-Embedding source code is available at https:// github.com/wtma/PR-Embedding . 2016; Zhang et al., 2018b). The retrieval-based methods predict the answer based on the similarity of context and candidate responses, which can be divided into single-turn models (Wang et al., 2015) and multi-turn models (Wu et al., 2017; Zhou et al., 2018; Ma et al., 2019) based on the number of turns in context. Those methods construct the representations of the context and response with a single vector space. Consequently, the models tend to select the response with the same words . On the other hand, as those static embeddings can not cope with the phenomenon of polysemy, researchers pay more attention to contextual representations recently. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and XLNet (Yang et al., 2019) have achieved great success in many NLP tasks. However, it is difficult to apply them in the industrial dialog syste"
2020.acl-main.166,P19-1535,1,0.762994,"2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from dialog datasets, which introduces very low cost for train"
2020.acl-main.166,D18-1256,0,0.0169801,", 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al., 2018; Zhao et al., 2019). However, these utterance-level methods tend to produce less coherent multi-turn dialogs since it is quite challenging to learn semantic transitions in a dialog flow merely from dialog data without the help of prior information. In this paper, we propose to represent prior information about dialog transition (between a message and its response) as a graph, and optimize dialog policy based on the graph, to foster a more coherent dialog. To this end, we propose a novel conversational graph (CG) grounded policy learning frame1835 Proceedings of the 58th Annu"
2020.acl-main.166,D17-1259,0,0.0192369,"onse “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al., 2018; Zhao et al., 2019). However, these utterance-level methods tend to produce less coherent multi-turn dialogs since it is quite challenging to l"
2020.acl-main.166,N16-1014,0,0.66715,"t response with two sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et"
2020.acl-main.166,D16-1127,0,0.489962,"t response with two sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et"
2020.acl-main.166,D16-1230,0,0.114845,"Missing"
2020.acl-main.166,D19-1187,1,0.845143,"models (Li et al., 2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from dialog datasets, which introduces very"
2020.acl-main.166,W15-4640,0,0.0973246,"Missing"
2020.acl-main.166,N19-1123,0,0.0334427,"Missing"
2020.acl-main.166,D18-1255,0,0.0342656,"egeneration issue of word-level policy models (Li et al., 2016b; Zhang et al., 2018b), previous works decouple policy learning from response generation, and then use utterance-level latent variables (Zhao et al., 2019) or keywords (Yao et al., 2018) as RL actions to guide response generation. In this work, we investigate how to use prior dialog-transition information to facilitate dialog policy learning. Knowledge aware conversation generation There are growing interests in leveraging knowledge bases for generation of more informative responses (Dinan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Zhou et al., 2018; Liu et al., 2019; Bao et al., 2019; Xu et al., 2020). In this work, we employ a dialog-modeling oriented graph built from dialog corpora, instead of a external knowledge base, in order to facilitate multi-turn policy learning, instead of dialog informativeness improvement. Specifically, we are motivated by (Xu et al., 2020). The method in (Xu et al., 2020) has the issue of cross-domain transfer since it relies on labor-intensive knowledge graph grounded multiturn dialog datasets for model training. Compared with them, our conversational graph is automatically built from di"
2020.acl-main.166,C16-1316,0,0.253008,"rocedure, they randomly select a mechanism for response generation. As shown in Figure 3, the generator consists of a RNN based message encoder, a set of responding mechanisms, and a decoder. First, given a dialog message, the message-encoder represents it as a vector x. Second, the generator uses a responding mechanism (selected by policy) to convert x into a response representation r¯. Finally, r¯ and a keyword (selected by policy) are fed into the decoder for response generation. To ensure that the given keyword will appear in generated responses, we introduce another Seq2BF based decoder (Mou et al., 2016) to replace the original RNN decoder. Moreover, this generator is trained on a dataset with pairs of [the message, a keyword extracted from a response]-the response.3 3.2 CG Construction Given a dialog corpus D, we construct the CG with three steps: what-vertex construction, how-vertex construction, and edge construction. 3 If multiple keywords are extracted from the response, we randomly choose one; and if no keyword exists in the response, we randomly sample a word from the response to serve as “keyword”. 1837 What-vertex construction To extract content words from D as what-vertices, we use"
2020.acl-main.166,D14-1162,0,0.0831011,"Missing"
2020.acl-main.166,P15-1152,0,0.0691534,"t by grid search. The weights of the third/sixth factors are set as 0 by default because they are proposed for target-guided conversation. 1839 rameters, and the parameters of other modules stay intact during RL training. 3.8 NLG As described in Section 3.1, we use the mechanism selected by how-policy to convert x into a response representation r¯. Then we feed the keyword in the selected what-vertex and r¯ into a Seq2BF decoder (Mou et al., 2016) for response generation. Experiments and Results9 4 4.1 Datasets We conduct experiments on two widely used opendomain dialog corpora. Weibo corpus (Shang et al., 2015). This is a large micro-blogging corpora. After data cleaning, we obtain 2.6 million pairs for training, 10k pairs for validation and 10k pairs for testing. We use publicly-available lexical analysis tools10 to obtain POS tag features for this dataset and then we further use this feature to extract keywords from utterances. We use Tencent AI Lab Embedding11 for embedding initialization in models. Persona dialog corpus (Zhang et al., 2018a). This ia a crowd-sourced dialog corpora where each participant plays the part of an assigned persona. To evaluate policy controllability brought by CGPolicy"
2020.acl-main.166,P18-1205,0,0.27686,"o sub-steps: firstly, obtains a response representation r¯ using both M3 and a message representation (from a message-encoder); Next, produces a response “It’s so ...” with “sleepy” and r¯ as input. Notice all the howvertices are from the same set rather than completely independent of each other. How to effectively learn dialog strategies is an enduring challenge for open-domain multi-turn conversation generation. To address this challenge, previous works investigate word-level policy models that simultaneously learn dialog policy and language generation from dialog corpora (Li et al., 2016b; Zhang et al., 2018b). But these word-level policy models often lead to a degeneration issue where the utterances become ungrammatical or repetitive (Lewis et al., 2017). To alleviate this issue, utterance-level policy models have been proposed to decouple policy learning from response generation, and they focus on how to incorporate † 我以为你会 犯困 的， 这么晚了 I thought you’d be sleepy, as it's late. Take care of yourself when doing a very hard work. Introduction ∗ 犯困/sleepy + 辛苦了，好辛苦，注意身体 high-level utterance representations, e.g., latent variables or keywords, to facilitate policy learning (He et al., 2018; Yao et al."
2020.acl-main.269,D15-1075,0,0.0207773,"h the probability 1 − π. = sigmoid((Es + G0 − G00 )/τ ) NLP Benchmarks (5) where G0 and G00 are two independent Gumbel noises (Gumbel, 1954), and τ ∈ (0, ∞) is a temperature parameter. As τ diminishes to zero, a sample from the Gumbel-Sigmoid distribution becomes cold and resembles the one-hot samples. At training time, we can use Gumbel-Sigmoid to obtain Experimental Setup Natural Language Inference aims to classify semantic relationship between a pair of sentences, i.e., a premise and corresponding hypothesis. We conduct experiments on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which has three classes: Entailment, Contradiction and Neutral. We followed Shen et al. (2018b) to use a token2token SAN layer followed by a source2token SAN layer to generate a compressed vector representation of input sentence. The selector is integrated into the token2token SAN layer. Taking the premise representation sp and the hypothesis vector sh as input, their semantic relationship is represented by the concatenation of sp , sh , sp −sh and sp · sh , which is passed to a classification module to generate a categorical distribution over the three classes. We initialize the word embedd"
2020.acl-main.269,P18-1008,0,0.115114,"word order encoding (Yang et al., 2019a) and syntactic structure modeling (Tang et al., 2018). In this work, we concentrate on these two commonly-cited issues. Word Order Encoding SANs merely rely on attention mechanism with neither recurrence nor convolution structures. In order to incorporate sequence order information, Vaswani et al. (2017) proposed to inject position information into the input word embedding with additional position embedding. Nevertheless, SANs are still weak at learning word order information (Yang et al., 2019a). Recent studies have shown that incorporating recurrence (Chen et al., 2018; Hao et al., 2019b,c), convolution (Song et al., 2018; Yang et al., 2019b), or advanced position encoding (Shaw et al., 2018; Wang et al., 2019a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representa"
2020.acl-main.269,P18-1198,0,0.0224993,"Missing"
2020.acl-main.269,N19-1423,0,0.0125418,"word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN. 1 Introduction Self-attention networks (SANs) (Lin et al., 2017) have achieved promising progress in various natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018b), semantic role labeling (Tan et al., 2018; Strubell et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) p"
2020.acl-main.269,C16-1276,1,0.893209,"Missing"
2020.acl-main.269,D19-1082,1,0.814363,"Missing"
2020.acl-main.269,D19-1135,1,0.882131,"urther refinement. In this study, we bridge this gap by assessing the strengths of selective mechanism on capturing essentially linguistic properties via well-designed experiments. The starting point for our approach is recent findings: the standard SANs suffer from two representation limitation on modeling word order encoding (Shaw et al., 2018; Yang et al., 2019a) 2986 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2986–2995 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and syntactic structure modeling (Tang et al., 2018; Hao et al., 2019a), which are essential for natural language understanding and generation. Experimental results on targeted linguistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntactic representations (§5.1) with a better modeling of structure by selective attention (§5.2). • The selective mechanism improves SANs by paying more attention to content words that posses semantic content and contribute to the meaning of the sentence (§5.3). 2 2.1 Metho"
2020.acl-main.269,N19-1122,1,0.877134,"Missing"
2020.acl-main.269,D19-1088,1,0.908786,"Missing"
2020.acl-main.269,D18-1317,1,0.881654,"are pretrained on Wikipedia and Gigaword, to initialize our networks, but they are not fixed during training. We choose the better feed-forward networks (FFN) variants of DEEPATT as our standard settings. Machine Translation is a conditional generation task, which aims to translate a sentence from a source language to its counterpart in a target language. We carry out experiments on several widelyused datasets, including small English⇒Japanese (En⇒Ja) and English⇒Romanian (En⇒Ro) corpora, as well as a relatively large English⇒German (En⇒De) corpus. For En⇒De and En⇒Ro, we respectively follow Li et al. (2018) and He et al. (2018) to prepare WMT20142 and IWSLT20143 corpora. For En⇒Ja, we use KFTT4 dataset provided by Neubig (2011). All the data are tokenized and then segmented into subword symbols using BPE (Sennrich et al., 2016) with 32K operations. We implemented the approach on top of advanced T RANSFORMER model (Vaswani et al., 2017). On the large-scale En⇒De dataset, we followed the base configurations to train the NMT model, which consists of 6 stacked encoder and decoder layers with the layer size being 512 and the number of attention heads being 8. On the small-scale En⇒Ro and En⇒Ja datase"
2020.acl-main.269,N19-1359,1,0.870621,"Missing"
2020.acl-main.269,D15-1166,0,0.0438377,"uistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntactic representations (§5.1) with a better modeling of structure by selective attention (§5.2). • The selective mechanism improves SANs by paying more attention to content words that posses semantic content and contribute to the meaning of the sentence (§5.3). 2 2.1 Methodology Self-Attention Networks SANs (Lin et al., 2017), as a variant of attention model (Bahdanau et al., 2015; Luong et al., 2015), compute attention weights between each pair of elements in a single sequence. Given the input layer H = {h1 , · · · , hN } ∈ N ×d , SANs first transform the layer H into the queries Q ∈ N ×d , the keys K ∈ N ×d , and the values V ∈ N ×d with three separate weight matrices. The output layer O is calculated as: R R O = ATT(Q, K)V R R (1) where the alternatives to ATT(·) can be additive attention (Bahdanau et al., 2015) or dot-product attention (Luong et al., 2015). Due to time and space efficiency, we used the dot-product attention in this study, which is computed as: QKT ATT(Q, K) = sof tmax("
2020.acl-main.269,D18-1458,0,0.0466347,"Missing"
2020.acl-main.269,W18-5444,0,0.0310899,"Missing"
2020.acl-main.269,D14-1162,0,0.0828181,"Missing"
2020.acl-main.269,D19-1145,1,0.865883,"Missing"
2020.acl-main.269,N18-1202,0,0.0976303,"Missing"
2020.acl-main.269,D19-1098,0,0.0165419,"y-cited issues. Word Order Encoding SANs merely rely on attention mechanism with neither recurrence nor convolution structures. In order to incorporate sequence order information, Vaswani et al. (2017) proposed to inject position information into the input word embedding with additional position embedding. Nevertheless, SANs are still weak at learning word order information (Yang et al., 2019a). Recent studies have shown that incorporating recurrence (Chen et al., 2018; Hao et al., 2019b,c), convolution (Song et al., 2018; Yang et al., 2019b), or advanced position encoding (Shaw et al., 2018; Wang et al., 2019a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representations, several researchers propose phrase-level SANs by performing self-attention across words inside a ngram phrase or syntactic constituent (W"
2020.acl-main.269,P16-1162,0,0.016109,"lation is a conditional generation task, which aims to translate a sentence from a source language to its counterpart in a target language. We carry out experiments on several widelyused datasets, including small English⇒Japanese (En⇒Ja) and English⇒Romanian (En⇒Ro) corpora, as well as a relatively large English⇒German (En⇒De) corpus. For En⇒De and En⇒Ro, we respectively follow Li et al. (2018) and He et al. (2018) to prepare WMT20142 and IWSLT20143 corpora. For En⇒Ja, we use KFTT4 dataset provided by Neubig (2011). All the data are tokenized and then segmented into subword symbols using BPE (Sennrich et al., 2016) with 32K operations. We implemented the approach on top of advanced T RANSFORMER model (Vaswani et al., 2017). On the large-scale En⇒De dataset, we followed the base configurations to train the NMT model, which consists of 6 stacked encoder and decoder layers with the layer size being 512 and the number of attention heads being 8. On the small-scale En⇒Ro and En⇒Ja datasets, we followed He et al. (2018) to decrease the layer size to 256 and the number of attention heads to 4. For all the tasks, we applied the selector to the first layer of encoder to better capture lexical and syntactic infor"
2020.acl-main.269,N18-2074,0,0.0355278,"ion (i.e., sequence generation), demonstrate that SSANs consistently outperform the standard SANs (§3). Despite demonstrating the effectiveness of SSANs, the underlying reasons for their strong performance have not been well explained, which poses great challenges for further refinement. In this study, we bridge this gap by assessing the strengths of selective mechanism on capturing essentially linguistic properties via well-designed experiments. The starting point for our approach is recent findings: the standard SANs suffer from two representation limitation on modeling word order encoding (Shaw et al., 2018; Yang et al., 2019a) 2986 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2986–2995 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and syntactic structure modeling (Tang et al., 2018; Hao et al., 2019a), which are essential for natural language understanding and generation. Experimental results on targeted linguistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntact"
2020.acl-main.269,D18-1408,0,0.0225505,"9a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representations, several researchers propose phrase-level SANs by performing self-attention across words inside a ngram phrase or syntactic constituent (Wu et al., 2018; Hao et al., 2019a; Wang et al., 2019b). These studies show that the introduction of syntactic information can achieve further improvement over SANs, demonstrating its potential weakness on structure modeling. 2.3 Selective Self-Attention Networks In this study, we implement the selective mechanism on SANs by introducing an additional selector, namely SSANs, as illustrated in Figure 1. The selector aims to select a subset of elements from the input sequence, on top of which the standard self-attention (Equation 1) is conducted. We implement the selector with Gumbel-Softmax, which has proven e"
2020.acl-main.269,D18-1475,1,0.809343,"guage representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018, 2019b). This poses a"
2020.acl-main.269,P19-1354,1,0.897977,"et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018,"
2020.acl-main.269,N19-1407,1,0.818121,"et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018,"
2020.acl-main.269,C18-1259,0,0.0489127,"Missing"
2020.acl-main.269,D18-1548,0,0.12373,"to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN. 1 Introduction Self-attention networks (SANs) (Lin et al., 2017) have achieved promising progress in various natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018b), semantic role labeling (Tan et al., 2018; Strubell et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al.,"
2020.acl-main.269,W13-3516,0,\N,Missing
2020.acl-main.269,D16-1159,0,\N,Missing
2020.acl-main.516,P17-1152,0,0.0210641,"wolayer tuple-interaction mechanism in the decoder, as shown in the first part of Figure 2. There are three attentions in two layers: query attention (Q-Attn) and persona attention (P-Attn) in the first layer, and persona-query attention (PQ-Attn) in the second layer. NG such identical layers compose of the decoder. For the first layer: (14) (15) ¯ is a hidden size × n matrix. A ¯ = where A ¯ ¯ ¯ ¯ [¯ a1 , a ¯2 , ..., a ¯n ] and B = [b1 , b2 , ..., bm ]. The n and m are the number of words in persona P and response prototype Yˆ (1) . Here we applied average pooling stragety (Liu et al., 2016; Chen et al., 2017) 5824 to get the summary representations: ¯a0 = n X i=1 a ¯i , n (16) and we can get the response attention weights and attentive response representations by: ¯ Wb = ¯a&gt; 0 B, (17) ¯&gt; (18) e = Wb B , B e is response where Wb is attention weights and B e representations. Similarly, we can get Wa and A. e e Once A and B are generated, three matching methods (Chen et al., 2017) are applied to extract relations: concatenation, element-wise product, element-wise difference. The results of these matching methods are concatenated to feed into a multi-layer perceptron, which has three layers and tanh a"
2020.acl-main.516,N19-1423,0,0.0226268,"Missing"
2020.acl-main.516,W19-3646,0,0.221071,"Missing"
2020.acl-main.516,D15-1075,0,0.0678241,"s dataset. Following this line, Yavuz et al. (2019) designed the DeepCopy model, which leverages copy mechanism to incorporate persona texts. Song et al. (2019a) integrated persona texts into the Per-CVAE model for generating diverse responses. However, the persona-based models still face the inconsistency issue (Welleck et al., 2019). To model the persona consistency, Welleck et al. (2019) annotated the Persona-Chat dataset and introduced the Dialogue Natural Language Inference (DNLI) dataset. This dataset converts the detection of dialogue consistency into a natural language inference task (Bowman et al., 2015). Personalized dialogue generation is an active research field (Li et al., 2016b; Qian et al., 2017; Zhang et al., 2018; Zheng et al., 2019a,b; Zhang et al., 2019). In parallel with this work, Song et al. (2019b) leveraged adversarial training to enhance the quality of personalized responses. Liu et al. (2020) incorporated mutual persona perception to build a more explainable (Liu et al., 2019) dialogue agent. Other relevant work lies in the area of multi-stage dialogue models (Lei et al., 2020). Some retrieval-guided dialogue models (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b;"
2020.acl-main.516,N19-1124,1,0.91058,"Missing"
2020.acl-main.516,D19-1195,1,0.861168,"(Bowman et al., 2015). Personalized dialogue generation is an active research field (Li et al., 2016b; Qian et al., 2017; Zhang et al., 2018; Zheng et al., 2019a,b; Zhang et al., 2019). In parallel with this work, Song et al. (2019b) leveraged adversarial training to enhance the quality of personalized responses. Liu et al. (2020) incorporated mutual persona perception to build a more explainable (Liu et al., 2019) dialogue agent. Other relevant work lies in the area of multi-stage dialogue models (Lei et al., 2020). Some retrieval-guided dialogue models (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b; Su et al., 2020) also adopted a multi-stage framework, but the difference from our work is obvious: we generate the prototype rather than retrieve one. A high-quality retrieved response is not always available, especially under the persona-based setting. 3 End-to-end dialogue generation approaches are a class of models for building open-domain dialogue systems, which have seen growing interests in recent years (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Li et al., 2016c; Zhao et al., 2017; Li et al., 2017). These dialogue models adopted recurrent units in a sequence to"
2020.acl-main.516,P17-4012,0,0.0141798,"f both encoder and decoder are 3. The number of heads in multi-head attention is 8, and the inner-layer size of the feedforward network is 2048. The word embeddings are randomly initialized, and the embedding dimension of all models is set to 512. Our model applies the same parameter settings as the transformer. The number of layers NG = ND = NR = 3. G and R share the word embeddings, but the matching model D uses independent embeddings. We use token-level batching with a size 4096. Adam is used for optimization, and the warm-up steps are set to 10,000. We implemented the model in OpenNMT-py (Klein et al., 2017). 4.4 Evaluation Metrics In the evaluation, there are two essential factors to consider: persona consistency and response quality. We apply both human evaluations and automatic metrics on these two aspects to compare different models. Human Evaluation We recruit five professional annotators from a third-party company. These annotators have high-level language skills but know nothing about the models. We sampled 200 persona-query-response tuples per model for evaluation. Duplicated queries (such as greetings which appear more than once) will not be sampled twice. First, we evaluate the persona"
2020.acl-main.516,N16-1014,0,0.281675,"the initial greetings and usually come to focus on their characteristics, such as hobbies, pets, and occupations, etc., in the course of the conversation. For humans, they can easily carry out conversations according to their personalities (Song et al., 2019a), but fulfilling this task is still a challenge for recent neural dialogue models (Welleck et al., 2019). One main issue is that these models are typically trained over millions of dialogues from different speakers, and the neural dialogue models have a propensity to mimic the response with the maximum likelihood in the training corpus (Li et al., 2016b), which results in the frequent inconsistency in responses (Zhang et al., 2018). Another issue ∗ This work was done when the first author was an intern at Tencent AI Lab. Wei-Nan Zhang is the corresponding author. is the user-sparsity problem (Qian et al., 2017) in conventional dialogue corpora (Serban et al., 2015). Some users have very few dialogue data, which makes it difficult for neural models to learn meaningful user representations (Li et al., 2016b). To alleviate the above issues, Zhang et al. (2018) introduced the Persona-Chat dataset to build more consistent dialogue models. Differ"
2020.acl-main.516,P16-1094,0,0.305115,"the initial greetings and usually come to focus on their characteristics, such as hobbies, pets, and occupations, etc., in the course of the conversation. For humans, they can easily carry out conversations according to their personalities (Song et al., 2019a), but fulfilling this task is still a challenge for recent neural dialogue models (Welleck et al., 2019). One main issue is that these models are typically trained over millions of dialogues from different speakers, and the neural dialogue models have a propensity to mimic the response with the maximum likelihood in the training corpus (Li et al., 2016b), which results in the frequent inconsistency in responses (Zhang et al., 2018). Another issue ∗ This work was done when the first author was an intern at Tencent AI Lab. Wei-Nan Zhang is the corresponding author. is the user-sparsity problem (Qian et al., 2017) in conventional dialogue corpora (Serban et al., 2015). Some users have very few dialogue data, which makes it difficult for neural models to learn meaningful user representations (Li et al., 2016b). To alleviate the above issues, Zhang et al. (2018) introduced the Persona-Chat dataset to build more consistent dialogue models. Differ"
2020.acl-main.516,D16-1127,0,0.315054,"the initial greetings and usually come to focus on their characteristics, such as hobbies, pets, and occupations, etc., in the course of the conversation. For humans, they can easily carry out conversations according to their personalities (Song et al., 2019a), but fulfilling this task is still a challenge for recent neural dialogue models (Welleck et al., 2019). One main issue is that these models are typically trained over millions of dialogues from different speakers, and the neural dialogue models have a propensity to mimic the response with the maximum likelihood in the training corpus (Li et al., 2016b), which results in the frequent inconsistency in responses (Zhang et al., 2018). Another issue ∗ This work was done when the first author was an intern at Tencent AI Lab. Wei-Nan Zhang is the corresponding author. is the user-sparsity problem (Qian et al., 2017) in conventional dialogue corpora (Serban et al., 2015). Some users have very few dialogue data, which makes it difficult for neural models to learn meaningful user representations (Li et al., 2016b). To alleviate the above issues, Zhang et al. (2018) introduced the Persona-Chat dataset to build more consistent dialogue models. Differ"
2020.acl-main.516,D17-1230,0,0.0353852,"l-guided dialogue models (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b; Su et al., 2020) also adopted a multi-stage framework, but the difference from our work is obvious: we generate the prototype rather than retrieve one. A high-quality retrieved response is not always available, especially under the persona-based setting. 3 End-to-end dialogue generation approaches are a class of models for building open-domain dialogue systems, which have seen growing interests in recent years (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Li et al., 2016c; Zhao et al., 2017; Li et al., 2017). These dialogue models adopted recurrent units in a sequence to sequence (seq2seq) fashion (Sutskever et al., 2014). Since the transformer has been shown to be on par with or superior to the recurrent units (Vaswani et al., 2017), some dialogue models began to take advantage of 3.1 Model Overview In this work, we consider learning a generative dialogue model to ground the response with explicit persona. We focus on the persona consistency of single-turn responses, and we leave the modeling of multi-turn persona consistency as future work. Formally, we use uppercase letters to represent senten"
2020.acl-main.516,P19-1560,0,0.0314284,"ted the Persona-Chat dataset and introduced the Dialogue Natural Language Inference (DNLI) dataset. This dataset converts the detection of dialogue consistency into a natural language inference task (Bowman et al., 2015). Personalized dialogue generation is an active research field (Li et al., 2016b; Qian et al., 2017; Zhang et al., 2018; Zheng et al., 2019a,b; Zhang et al., 2019). In parallel with this work, Song et al. (2019b) leveraged adversarial training to enhance the quality of personalized responses. Liu et al. (2020) incorporated mutual persona perception to build a more explainable (Liu et al., 2019) dialogue agent. Other relevant work lies in the area of multi-stage dialogue models (Lei et al., 2020). Some retrieval-guided dialogue models (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b; Su et al., 2020) also adopted a multi-stage framework, but the difference from our work is obvious: we generate the prototype rather than retrieve one. A high-quality retrieved response is not always available, especially under the persona-based setting. 3 End-to-end dialogue generation approaches are a class of models for building open-domain dialogue systems, which have seen growing interests"
2020.acl-main.516,2020.acl-main.131,0,0.0978424,"Missing"
2020.acl-main.516,P15-1152,0,0.0539994,"s in the area of multi-stage dialogue models (Lei et al., 2020). Some retrieval-guided dialogue models (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b; Su et al., 2020) also adopted a multi-stage framework, but the difference from our work is obvious: we generate the prototype rather than retrieve one. A high-quality retrieved response is not always available, especially under the persona-based setting. 3 End-to-end dialogue generation approaches are a class of models for building open-domain dialogue systems, which have seen growing interests in recent years (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Li et al., 2016c; Zhao et al., 2017; Li et al., 2017). These dialogue models adopted recurrent units in a sequence to sequence (seq2seq) fashion (Sutskever et al., 2014). Since the transformer has been shown to be on par with or superior to the recurrent units (Vaswani et al., 2017), some dialogue models began to take advantage of 3.1 Model Overview In this work, we consider learning a generative dialogue model to ground the response with explicit persona. We focus on the persona consistency of single-turn responses, and we leave the modeling of multi-turn persona consis"
2020.acl-main.516,P19-1003,0,0.0206235,"model, we carried out experiments on the public available Persona-Chat dataset (Zhang et al., 2018). We summarize the main contributions as follows: • A three-stage end-to-end generative framework, GDR, was proposed for the generation of persona consistent dialogues. • A matching model was integrated into the generation framework to detect and delete inconsistent words in response prototype. • Experimental results show the proposed approach outperforms competitive baselines on both human and automatic metrics. 2 Related Work this architecture for better dialogue modeling (Dinan et al., 2018; Su et al., 2019). Besides the advancements in dialogue models, the emergence of new dialogue corpus has also contributed to the research field. Zhang et al. (2018) introduced the Persona-Chat dataset, with explicit persona texts to each dialogue. Based on seq2seq model and memory network, they further proposed a model named Generative Profile Memory Network for this dataset. Following this line, Yavuz et al. (2019) designed the DeepCopy model, which leverages copy mechanism to incorporate persona texts. Song et al. (2019a) integrated persona texts into the Per-CVAE model for generating diverse responses. Howe"
2020.acl-main.516,P19-1363,0,0.555454,"model generates a response which looks good, it is an inconsistent one. With further rewriting, the model can focus more on improving persona consistency. Introduction In an open-domain conversation scenario, two speakers conduct open-ended chit-chat from the initial greetings and usually come to focus on their characteristics, such as hobbies, pets, and occupations, etc., in the course of the conversation. For humans, they can easily carry out conversations according to their personalities (Song et al., 2019a), but fulfilling this task is still a challenge for recent neural dialogue models (Welleck et al., 2019). One main issue is that these models are typically trained over millions of dialogues from different speakers, and the neural dialogue models have a propensity to mimic the response with the maximum likelihood in the training corpus (Li et al., 2016b), which results in the frequent inconsistency in responses (Zhang et al., 2018). Another issue ∗ This work was done when the first author was an intern at Tencent AI Lab. Wei-Nan Zhang is the corresponding author. is the user-sparsity problem (Qian et al., 2017) in conventional dialogue corpora (Serban et al., 2015). Some users have very few dial"
2020.acl-main.516,W18-5713,0,0.0620038,"nto a natural language inference task (Bowman et al., 2015). Personalized dialogue generation is an active research field (Li et al., 2016b; Qian et al., 2017; Zhang et al., 2018; Zheng et al., 2019a,b; Zhang et al., 2019). In parallel with this work, Song et al. (2019b) leveraged adversarial training to enhance the quality of personalized responses. Liu et al. (2020) incorporated mutual persona perception to build a more explainable (Liu et al., 2019) dialogue agent. Other relevant work lies in the area of multi-stage dialogue models (Lei et al., 2020). Some retrieval-guided dialogue models (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b; Su et al., 2020) also adopted a multi-stage framework, but the difference from our work is obvious: we generate the prototype rather than retrieve one. A high-quality retrieved response is not always available, especially under the persona-based setting. 3 End-to-end dialogue generation approaches are a class of models for building open-domain dialogue systems, which have seen growing interests in recent years (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Li et al., 2016c; Zhao et al., 2017; Li et al., 2017). These dialogue models adopt"
2020.acl-main.516,W19-5917,0,0.0748546,"xperimental results show the proposed approach outperforms competitive baselines on both human and automatic metrics. 2 Related Work this architecture for better dialogue modeling (Dinan et al., 2018; Su et al., 2019). Besides the advancements in dialogue models, the emergence of new dialogue corpus has also contributed to the research field. Zhang et al. (2018) introduced the Persona-Chat dataset, with explicit persona texts to each dialogue. Based on seq2seq model and memory network, they further proposed a model named Generative Profile Memory Network for this dataset. Following this line, Yavuz et al. (2019) designed the DeepCopy model, which leverages copy mechanism to incorporate persona texts. Song et al. (2019a) integrated persona texts into the Per-CVAE model for generating diverse responses. However, the persona-based models still face the inconsistency issue (Welleck et al., 2019). To model the persona consistency, Welleck et al. (2019) annotated the Persona-Chat dataset and introduced the Dialogue Natural Language Inference (DNLI) dataset. This dataset converts the detection of dialogue consistency into a natural language inference task (Bowman et al., 2015). Personalized dialogue generat"
2020.acl-main.516,P18-1205,0,0.515638,"ch as hobbies, pets, and occupations, etc., in the course of the conversation. For humans, they can easily carry out conversations according to their personalities (Song et al., 2019a), but fulfilling this task is still a challenge for recent neural dialogue models (Welleck et al., 2019). One main issue is that these models are typically trained over millions of dialogues from different speakers, and the neural dialogue models have a propensity to mimic the response with the maximum likelihood in the training corpus (Li et al., 2016b), which results in the frequent inconsistency in responses (Zhang et al., 2018). Another issue ∗ This work was done when the first author was an intern at Tencent AI Lab. Wei-Nan Zhang is the corresponding author. is the user-sparsity problem (Qian et al., 2017) in conventional dialogue corpora (Serban et al., 2015). Some users have very few dialogue data, which makes it difficult for neural models to learn meaningful user representations (Li et al., 2016b). To alleviate the above issues, Zhang et al. (2018) introduced the Persona-Chat dataset to build more consistent dialogue models. Different from conventional dialogue corpora, this dataset endows dialogue models with"
2020.acl-main.516,P17-1061,0,0.0230966,"020). Some retrieval-guided dialogue models (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b; Su et al., 2020) also adopted a multi-stage framework, but the difference from our work is obvious: we generate the prototype rather than retrieve one. A high-quality retrieved response is not always available, especially under the persona-based setting. 3 End-to-end dialogue generation approaches are a class of models for building open-domain dialogue systems, which have seen growing interests in recent years (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Li et al., 2016c; Zhao et al., 2017; Li et al., 2017). These dialogue models adopted recurrent units in a sequence to sequence (seq2seq) fashion (Sutskever et al., 2014). Since the transformer has been shown to be on par with or superior to the recurrent units (Vaswani et al., 2017), some dialogue models began to take advantage of 3.1 Model Overview In this work, we consider learning a generative dialogue model to ground the response with explicit persona. We focus on the persona consistency of single-turn responses, and we leave the modeling of multi-turn persona consistency as future work. Formally, we use uppercase letters t"
2020.acl-main.565,D18-1547,0,0.188613,"Missing"
2020.acl-main.565,W17-5506,0,0.364581,"ringer street no traffic 200 Alester Ave 2 miles gas station Valero road block nearby 899 Ames Ct 5 miles hospital Stanford Childrens Health moderate traffic 481 Amaranta Ave 1 miles parking garage Palo Alto Garage R moderate traffic Driver Address to the gas station. Car Valero is located at 200 Alester Ave. Driver OK , please give me directions via a route that avoids all heavy traffic. Car Since there is a road block nearby, I found another route for you and I sent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver"
2020.acl-main.565,E17-2075,0,0.249266,"ere is a road block nearby, I found another route for you and I sent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended dom"
2020.acl-main.565,N19-1375,0,0.421861,"igure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each new domain. Hence, it is"
2020.acl-main.565,D18-1498,0,0.0614214,"Missing"
2020.acl-main.565,P82-1020,0,0.80062,"Missing"
2020.acl-main.565,P18-1133,0,0.108761,"et al. (2019b), We hired human experts and asked them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. Results are illustrated in Table 4. We can see that our framework outperforms GLMP on all metrics, which is consistent with the automatic evaluation. 4 Related Work Existing end-to-end task-oriented systems can be classified into two main classes. A series of work trains a single model on the mixed multi-domain dataset. Eric et al. (2017) augments the vocabulary distribution by concatenating KB attention to generatge entities. Lei et al. (2018) first integrates track dialogue believes in end-to-end task-oriented dialog. Madotto et al. (2018) combines end-toend memory network (Sukhbaatar et al., 2015) into sequence generation. Gangi Reddy et al. (2019) proposes a multi-level memory architecture which first addresses queries, followed by results and finally each key-value pair within a result. Wu et al. (2019a) proposes a global-to-locally pointer mechanism to query the knowledge base. Compared with their models, our framework can not only explicitly utilize domain-specific knowledge but also consider different relevance between each"
2020.acl-main.565,P17-1001,0,0.0269491,"red and final domain-specific feature: d f shprivate : (hsdec,t , hdec,t ) → hfdec,t . (17) Finally, we denote the dynamic fusion function |D| i as dynamic(hsdec,t , {hddec,t }i=1 ). Similar to Sec0 tion 2.2, we replace [hdec,t , hdec,t ] in Eq. 8 with 0 [hfdec,t , hfdec,t ]. The other components are kept the same as the shared-private encoder-decoder framework. Train 2,425 1,839 Dev 302 117 Test 304 141 Table 1: Statistics of datasets. Adversarial Training To encourage the model to learn domain-shared features, we apply adversarial learning on the shared encoder and decoder module. Following Liu et al. (2017), a gradient reversal layer (Ganin and Lempitsky, 2014) is introduced after the domain classifier layer. The adversarial training loss is denoted as Ladv . We follow Qin et al. (2019a) and the final loss function of our Dynamic fusion network is defined as: L = γb Lbasic + γm Lmoe + γa Ladv , (18) where Lbasic keep the same as GLMP (Wu et al., 2019a), γb , γm and γa are hyper-parameters. More details about Lbasic and Ladv can be found in appendix. 3 where θs represents the parameters of encoderm represents the parameters decoder model, θdec of the MoE module (Eq. 15) in the decoder and ei ∈ {0"
2020.acl-main.565,P18-1136,0,0.411253,"route for you and I sent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich"
2020.acl-main.565,D19-1214,1,0.822917,"a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each new domain. Hence, it is important to consi"
2020.acl-main.565,D19-1013,1,0.882709,"Missing"
2020.acl-main.565,C18-1320,1,0.527232,"ent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each"
2020.acl-main.565,N18-1138,0,0.0548659,"Missing"
2020.acl-main.565,P18-1135,0,0.402986,"ynamic Domain-Specific Fusion Shared Module A Module B Module C Module Shared Module Mixed Data Domain A Domain B Domain C Domain A Mixed Data Domain B A Module B Module C Module Domain C Mixed Data (a) (c) (b) (d) Figure 2: Methods for multi-domain dialogue. Previous work either trains a general model on mixed multi-domain mixed datasets (a), or on each domain separately (b). The basic shared-private framework is shown (c). Our proposed extension with dynamic fusion mechanism is shown (d). to incorporate domain-shared and domain-private features is shared-private framework (Liu et al., 2017; Zhong et al., 2018; Wu et al., 2019b). Shown in Figure 2(c), it includes a shared module to capture domain-shared feature and a private module for each domain. The method explicitly differentiates shared and private knowledge. However, this framework still has two issues: (1) given a new domain with extremely little data, the private module can fail to effectively extract the corresponding domain knowledge. (2) the framework neglects the fine-grained relevance across certain subsets of domains. (e.g. schedule domain is more relevant to the navigation than to the weather domain.) To address the above issues, we"
2020.acl-main.599,P19-1620,0,0.156639,"nswer” should be given if there is no suitable short answer. 2.2 • We achieve state-of-the-art performance on both long and short answer leaderboard of NQ at the time of submission (Jun. 25th, 2019), and our model surpasses single human performance on the development dataset at both long and short answer criteria. Preliminary Data Preprocessing Since the average length of the documents in NQ is too long to be considered as one training instance, we first split each document into a list of document fragments with overlapping windows of tokens, like in the original BERT model for the MRC tasks (Alberti et al., 2019b; Devlin et al., 2019). Then we generate an instance from a document fragment by concatenating a “[CLS]” token, tokenized question, a “[SEP]” token, tokens from the content of the doc6709 Output Layer Document Fragment Add & Norm Paragraph Feed-Forward Sentence Add & Norm Concatenate Token Graph Integration Token-Level Self-Attention Sentence-Level Self-Attention N× Paragraph-Level Self-Attention Figure 4: The graph on the left is an illustration of the graph integration layer. The graph on the right shows the incoming information when updating a paragraph node. The solid lines represent the"
2020.acl-main.599,P17-1171,0,0.270549,"al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches, they treat the long and short answer extraction as two individual sub-tasks during training and fail to model this multi-grained characteristic of this benchmark, while we argue that the two sub-tasks of NQ should be considered simultaneously to obtain accurate results. According to Kwiatkowski et al. (2019), a valid long answer must contain all of the information required to answer the question. Besides, an accurate short answer should be helpful to confirm the long answer. For instance,"
2020.acl-main.599,P16-1046,0,0.238693,"agraphs 6708 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained joi"
2020.acl-main.599,P17-1147,0,0.0527221,"dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al"
2020.acl-main.599,N18-2075,0,0.0309702,"normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani e"
2020.acl-main.599,Q19-1026,0,0.165659,"ar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answer. Despite the effectiveness of these approaches,"
2020.acl-main.599,N18-2078,0,0.0307711,"k et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different"
2020.acl-main.599,P18-1078,0,0.295567,"ides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extract the short answer from the selected long answe"
2020.acl-main.599,P17-1055,1,0.862058,"ns that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to select a long answer, and the Document Reader model (Chen et al., 2017) to extrac"
2020.acl-main.599,N18-1158,0,0.0177026,"ion for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote each other. At inference time, we use a pipeline s"
2020.acl-main.599,D16-1244,0,0.153285,"Missing"
2020.acl-main.599,N16-1174,0,0.432339,"of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6708–6718 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (long answer) describing the entity bowling hall of fame, then try to confirm if the location (short answer) of the asked entity exists in the paragraph, which helps to finally decide which paragraph is the long answer. In this way, the two-grained answers can provide evidence for each other. To address the two sub-tasks together, instead of using conventional documents modeling methods like hierarchical RNNs (Cheng and Lapata, 2016; Yang et al., 2016; Nallapati et al., 2017; Narayan et al., 2018), we propose to use graph attention networks (Velickovic et al., 2018) and BERT (Devlin et al., 2019), directly model representations at tokens, sentences, paragraphs, and documents, the four different levels of granularity to capture hierarchical nature of documents. In this way, we directly derive scores of long answers from its paragraph-level representations and obtain scores of short answers from the start and end positions on the token-level representations. Thus the long and short answer selection tasks can be trained jointly to promote eac"
2020.acl-main.599,D16-1264,0,0.0512332,"ong and short answer criteria. 1 Figure 1: An example from NQ dataset. and questions that are from real user queries. Besides, unlike conventional MRC tasks (e.g. Rajpurkar et al.,2016), in NQ, answers are provided in a two-grained format: long answer, which is typically a paragraph, and short answers, which are typically one or more entities inside the long answer. Figure 1 shows an example from NQ dataset. Introduction Machine reading comprehension (MRC), a task that aims to answer questions based on a given document, has been substantially advanced by recently released datasets and models (Rajpurkar et al., 2016; Seo et al., 2017; Xiong et al., 2017; Joshi et al., 2017; Cui et al., 2017; Devlin et al., 2019; Clark and Gardner, 2018). Natural Questions (NQ, Kwiatkowski et al., 2019), a newly released benchmark, makes it more challenging by introducing much longer documents than existing datasets ∗ Work was done while this author was an intern at Microsoft Research Asia. † Email corresponding. Existing approaches on NQ have obtained promising results. For example, Kwiatkowski et al. (2019) builds a pipeline model using two separate models: the Decomposable Attention model (Parikh et al., 2016) to selec"
2020.acl-main.599,K19-1074,0,0.0605159,"Missing"
2020.acl-main.599,D16-1103,0,0.0241031,"Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have s"
2020.acl-main.599,N18-2074,0,0.0339776,"graph integration layer and pass it to the feedforward layer. 6711 3.3.4 Feed-Forward Layer Following the inner structure of the transformer (Vaswani et al., 2017), we also utilize an additional fully connected feed-forward network at the end of our graph encoder. It consists of two linear transformations with a GELU activation in between. GELU is Gaussian Error Linear Unit activation (Hendrycks and Gimpel, 2016), and we use GELU as the non-linear activation, which is consistent with BERT. 3.3.5 Inspired by positional encoding in Vaswani et al. (2017) and relative position representations in Shaw et al. (2018), we introduce a novel relational embedding on our constructed graph, which aims at modeling the relative position information between nodes on the multi-granularity document structure. We make the edges in our document modeling graph to embed relative positional information. We modify equation 1 and 2 for eij and z i to introduce our relational embedding as follows: eij = zi = X αij  h0i = j∈Ni ,oj +1=oi  h0j + aij + boi , Output Layer The objective function is defined as the negative sum of the log probabilities of the predicted distributions, averaged over all the training instances. The"
2020.acl-main.599,D18-1246,0,0.0199332,"evel encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes are represented by different 6715 types of edges in the graph. 6 Jaco"
2020.acl-main.599,P18-1030,0,0.0200399,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D18-1244,0,0.031553,"Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibility in a variant of NLP tasks (Zhang et al., 2018c; Marcheggiani et al., 2018; Zhang et al., 2018b; Song et al., 2018). A recent approach that began with Graph Attention Networks (Velickovic et al., 2018), which applies attention mechanisms to graphs. Wang et al. (2019) proposed knowledge graph attention networks to model the information in the knowledge graph, (Zhang et al., 2018a) proposed gated attention networks, which use a convolutional sub-network to control each attention head’s importance. We model the hierarchical nature of documents by representing them at four different levels of granularity. Besides, the relations between nodes"
2020.acl-main.599,D15-1167,1,0.734635,"Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network model for summarization and this framework allows much complex information flows between nodes, which represents words, sentences, and entities in the graph. Graph neural networks have shown their flexibil"
2020.acl-main.599,P18-1158,0,0.0352041,"l., 2017; Lai et al., 2017; Trischler et al., 2017; Yang et al., 2018). Lots of work has begun to build end-to-end deep learning models and has achieved good results (Seo et al., 2017; Xiong et al., 2017; Cui et al., 2017; Devlin et al., 2019; Lv et al., 2020). They normally treat questions and documents as two simple sequences regardless of their structures and focus on incorporating questions into the documents, where the attention mechanism is most widely used. Clark and Gardner (2018) proposes a model for multiparagraph reading comprehension using TF-IDF as the paragraph selection method. Wang et al. (2018) focuses on modeling a passage at word and sentence level through hierarchical attention. Previous work on document modeling is mainly based on a two-level hierarchy (Ruder et al., 2016; Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Koshorek et al., 2018; Zhang et al., 2019). The first level encodes words or sentences to get the low-level representations. Moreover, a highlevel encoder is applied to obtain document representation from the low-level. In these frameworks, information flows only from low-level to high-level. Fernandes et al. (2018) proposed a graph neural network m"
2020.acl-main.599,W17-2623,0,\N,Missing
2020.acl-main.599,D17-1082,0,\N,Missing
2020.acl-main.599,D18-1259,0,\N,Missing
2020.acl-main.599,N19-1423,0,\N,Missing
2020.acl-main.98,N16-1014,0,0.232101,"nate the goal predicted by our model, all the related knowledge and the dialog context as its input. Turn-level results Dialog-level results Methods↓ Metrics→ Fluency Appro. Infor. Proactivity Goal success rate Coherence S2S +gl. +kg. MGCG R +gl. +kg. MGCG G +gl. +kg. 1.08 1.98 1.94 0.23 0.60 0.75 0.37 1.28 1.68 0.94 1.22 1.34 0.37 0.68 0.82 0.49 0.83 0.91 Table 5: Human evaluation results at the level of turns and dialogs. 5.3 Automatic Evaluations Metrics For automatic evaluation, we use several common metrics such as BLEU (Papineni et al., 2002), F1, perplexity (PPL), and DISTINCT (DIST2) (Li et al., 2016) to measure the relevance, fluency, and diversity of generated responses. Following the setting in previous work (Wu et al., 2019; Zhang et al., 2018a), we also measure the performance of all models using Hits@1 and Hits@3.6 Here we let each model to select the best response from 10 candidates. Those 10 candidate responses consist of the ground-truth response generated by humans and nine randomly sampled ones from the training set. Moreover, we also evaluate the knowledge-selection capability of each model by calculating knowledge precision/recall/F1 scores as done in Wu et al. (2019).7 In add"
2020.acl-main.98,D18-1255,0,0.356097,"The goal-planning module can conduct dialog management to control the dialog 1037 • We identify the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User profile No No Yes No No No No No No No Yes Table 1: Compari"
2020.acl-main.98,P19-1081,0,0.263116,"oal planning module and a goal-guided responding module. The goal-planning module can conduct dialog management to control the dialog 1037 • We identify the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User p"
2020.acl-main.98,P02-1040,0,0.106605,"nts “with(without) knowledge”. For “S2S +gl.+kg.”, we simply concatenate the goal predicted by our model, all the related knowledge and the dialog context as its input. Turn-level results Dialog-level results Methods↓ Metrics→ Fluency Appro. Infor. Proactivity Goal success rate Coherence S2S +gl. +kg. MGCG R +gl. +kg. MGCG G +gl. +kg. 1.08 1.98 1.94 0.23 0.60 0.75 0.37 1.28 1.68 0.94 1.22 1.34 0.37 0.68 0.82 0.49 0.83 0.91 Table 5: Human evaluation results at the level of turns and dialogs. 5.3 Automatic Evaluations Metrics For automatic evaluation, we use several common metrics such as BLEU (Papineni et al., 2002), F1, perplexity (PPL), and DISTINCT (DIST2) (Li et al., 2016) to measure the relevance, fluency, and diversity of generated responses. Following the setting in previous work (Wu et al., 2019; Zhang et al., 2018a), we also measure the performance of all models using Hits@1 and Hits@3.6 Here we let each model to select the best response from 10 candidates. Those 10 candidate responses consist of the ground-truth response generated by humans and nine randomly sampled ones from the training set. Moreover, we also evaluate the knowledge-selection capability of each model by calculating knowledge p"
2020.acl-main.98,P13-2089,0,0.526547,"ender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies.1 1 Introduction In recent years, there has been a significant increase in the work of conversational recommendation due to the rise of voice-based bots (Christakopoulou et al., 2016; Li et al., 2018; Reschke et al., 2013; Warnestal, 2005). They focus on how to provide high-quality recommendations through dialog-based interactions with users. These work fall into two categories: (1) task-oriented dialogmodeling approaches (Christakopoulou et al., 2016; Sun and Zhang, 2018; Warnestal, 2005); (2) nontask dialog-modeling approaches with more freeform interactions (Kang et al., 2019; Li et al., 2018). ∗ This work was done at Baidu. Corresponding author: Wanxiang Che. 1 Dataset and codes are publicly available at https://github.com/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all"
2020.acl-main.98,P19-1565,0,0.111519,"Missing"
2020.acl-main.98,D19-1203,0,0.344356,"on DuRecDial for future studies.1 1 Introduction In recent years, there has been a significant increase in the work of conversational recommendation due to the rise of voice-based bots (Christakopoulou et al., 2016; Li et al., 2018; Reschke et al., 2013; Warnestal, 2005). They focus on how to provide high-quality recommendations through dialog-based interactions with users. These work fall into two categories: (1) task-oriented dialogmodeling approaches (Christakopoulou et al., 2016; Sun and Zhang, 2018; Warnestal, 2005); (2) nontask dialog-modeling approaches with more freeform interactions (Kang et al., 2019; Li et al., 2018). ∗ This work was done at Baidu. Corresponding author: Wanxiang Che. 1 Dataset and codes are publicly available at https://github.com/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all these work focus on a single type of dialogs, either task oriented dialogs for recommendation, or recommendation oriented open-domain conversation. Moreover, they assume that both sides in the dialog (especially the user) are aware of the conversational goal from the beginning. In many real-world applications, there are multiple dialog types in human-bot conver"
2020.acl-main.98,D14-1181,0,0.0103428,"Current goal prediction (a) Goal-planning module Figure 3: The architecture of our multi-goal driven conversation generation framework (denoted as MGCG). K. These goals will be used as answers for training of the goal-planning module, while the tuples of [context, a ground-truth goal, K, response] will be used for training of the responding module. 4.2 Goal-planning Model As shown in Figure 3(a), we divide the task of goal planning into two sub-tasks, goal completion estimation, and current goal prediction. Goal completion estimation For this subtask, we use Convolutional neural network (CNN)(Kim, 2014) to estimate the probability of goal completion by: PGC (l = 1|X, gt−1 ). (1) Current goal prediction If gt−1 is not completed (PGC &lt; 0.5), then gc = gt−1 , where gc is the goal for Y . Otherwise, we use CNN based multi-task classification to predict the current goal by maximizing the following probability: gt = arg max PGP (g ty , g tp |X, G 0 , Pi k , K), (2) gc = gt , (3) s g ty ,g tp where g ty is a candidate dialog type and g tp is a candidate dialog topic. 4.3 Retrieval-based Response Model we modify the original retrieval model to suit our task by emphasizing the use of goals. As shown"
2020.acl-main.98,D14-1007,1,0.863561,"m/PaddlePaddle/models/ tree/develop/PaddleNLP/Research/ACL2020-DuRecDial. † Almost all these work focus on a single type of dialogs, either task oriented dialogs for recommendation, or recommendation oriented open-domain conversation. Moreover, they assume that both sides in the dialog (especially the user) are aware of the conversational goal from the beginning. In many real-world applications, there are multiple dialog types in human-bot conversations (called multi-type dialogs), such as chit-chat, task oriented dialogs, recommendation dialogs, and even question answering (Ram et al., 2018; Wang et al., 2014; Zhou et al., 2018b). Therefore it is crucial to study how to proactively and naturally make conversational recommendation by the bots in the context of multi-type human-bot communication. For example, the bots could proactively make recommendations after question answering or a task dialog to improve user experience, or it could lead a dialog from chitchat to approach a given product as commercial advertisement. However, to our knowledge, there is less previous work on this problem. To address this challenge, we present a novel task, conversational recommendation over multitype dialogs, wher"
2020.acl-main.98,P19-1369,1,0.765759,"the task of conversational recommendation over multi-type dialogs. • To facilitate the study of this task, we create a novel dialog dataset DuRecDial, with rich variability of dialog types and domains as shown in Table 1. • We propose a conversation generation framework with a novel mixed-goal driven dialog policy mechanism. Datasets↓ Metrics→ Facebook Rec (Dodge et al., 2016) REDIAL (Li et al., 2018) GoRecDial (Kang et al., 2019) OpenDialKG (Moon et al., 2019) CMU DoG (Zhou et al., 2018a) IIT DoG (Moghe et al., 2018) Wizard-of-wiki (Dinan et al., 2019) OpenDialKG (Moon et al., 2019) DuConv (Wu et al., 2019) KdConv (Zhou et al., 2020) DuRecDial #Dialogs #Utterances Dialog types Domains 1M 10k 9k 12k 4k 9k 22k 3k 29k 4.5k 10.2k 6M 163k 170k 143k 130k 90k 202k 38k 270k 86k 156k Rec. Rec., chitchat Rec. Rec. Chitchat Chitchat Chitchat Chitchat Chitchat Chitchat Rec., chitchat, QA, task Movie Movie Movie Movie, book Movie Movie 1365 Wikipedia topics Sports, music Movie Movie, music, travel Movie, music, movie star, food, restaurant, news, weather User profile No No Yes No No No No No No No Yes Table 1: Comparison of our dataset DuRecDial to recommendation dialog datasets and knowledge grounded dialog"
2020.acl-main.98,D18-1076,0,\N,Missing
2020.acl-main.98,2020.acl-main.635,0,\N,Missing
2020.coling-main.179,2020.acl-main.708,0,0.195787,"ure and text’s fidelity. In detail, the table structure reconstruction task is proposed for GPT-2 which force it to embed table structure into its representation when modeling structured table. Besides, we utilize content matching task that help model correctly describe important information from table via Optimal-Transport (Chen et al., 2019a) technique, which measures the distance between the information in generated text and information in table and use the distance as penalty for text with incorrect information. We conducted experiments on three data-to-text datasets on different domains (Chen et al., 2020b): Humans, Books, Songs in various settings. Both automatic evaluation and human evaluation results show that our model can achieve new state-of-the-art performance for table-to-text generation in terms of generating fluent and high-fidelity text in most few-shot settings. 2 2.1 Background Task Definition For the table-to-text task discussed in this paper, we can formulate each training instance as pair of table and summary E = (S, T ). Given a table, which can be formulated as sets of records S = {ri }N i=1 , the model is expected to generate descriptive text T = w1 , w2 , ..., wL . N is the"
2020.coling-main.179,2020.acl-main.18,0,0.372609,"ure and text’s fidelity. In detail, the table structure reconstruction task is proposed for GPT-2 which force it to embed table structure into its representation when modeling structured table. Besides, we utilize content matching task that help model correctly describe important information from table via Optimal-Transport (Chen et al., 2019a) technique, which measures the distance between the information in generated text and information in table and use the distance as penalty for text with incorrect information. We conducted experiments on three data-to-text datasets on different domains (Chen et al., 2020b): Humans, Books, Songs in various settings. Both automatic evaluation and human evaluation results show that our model can achieve new state-of-the-art performance for table-to-text generation in terms of generating fluent and high-fidelity text in most few-shot settings. 2 2.1 Background Task Definition For the table-to-text task discussed in this paper, we can formulate each training instance as pair of table and summary E = (S, T ). Given a table, which can be formulated as sets of records S = {ri }N i=1 , the model is expected to generate descriptive text T = w1 , w2 , ..., wL . N is the"
2020.coling-main.179,N19-1423,0,0.179043,"h as WIKIBIO (Lebret et al., 2016) and E2E (Duˇsek et al., 2020). However, it is not always feasible to collect large-scale labeled dataset for various domains in the real world, resulting in unsatisfying performance due to the insufficient training. Such few-shot learning setting for table-to-text generation is not well-explored, and in this paper, we focus on exploring how to efficiently model for few-shot table-to-text generation with limited training pairs. Recently, pre-trained language models have shown promising progress in various natural language processing tasks (Yang et al., 2019b; Devlin et al., 2019; Radford et al., 2019). They can capture linguistic knowledge by pretraining on large-scale unlabeled dataset and generalize to downstream tasks with little labeled data in target domain, effectively modeling for few-shot setting (Peng et al., 2020). However, efforts to benefit table-to-text generation from the powerful pre-trained language model, especially in few-shot setting, are non-trivial due to three challenges. (1) There is a gap between the structured data input for table-to-text generation and natural language input that is used for pretraining GPT-2. (2) Also, it lacks modeling of"
2020.coling-main.179,D19-1310,1,0.799494,"s , England .” and the expression of wrong birth date, which shows the imperfect switch policy on deciding when to copy from table can sometimes hurt model’s ability to generate high-fidelity text. On the contrary, TableGPT, enhanced to generate high-fidelity text with two auxiliary tasks without breaking one unified GPT-2 model for generating text, performs better in terms of fidelity and fluency of text in this example. 5 Related Work In recent years, neural models for generating texts directly from preprocessed data (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Gong et al., 2019; Feng et al., 2020), have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset. Mei et al. (2016) proposes a pre-selector on encoder-aligner-decoder model for generation, which strengthens model’s content selection ability and obtains considerable improvement over standard Seq2Seq model. Sha et al. (2018) proposes a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing on modeling table structure and dual attention mechanism to utilize the"
2020.coling-main.179,D16-1128,0,0.0498271,"le-to-text datasets in different domains, our model outperforms existing systems on most few-shot settings. 1 Introduction Table-to-text generation, aiming at generating descriptive text about important information in structured data, has well application prospect in communicating with human in a comprehensible and natural way, such as financial report (Murakami et al., 2017), medical report (Hasan and Farri, 2019) generation, etc. In recent years, data-driven models have shown impressive capability to produce informative and fluent text with the help of large-scale datasets, such as WIKIBIO (Lebret et al., 2016) and E2E (Duˇsek et al., 2020). However, it is not always feasible to collect large-scale labeled dataset for various domains in the real world, resulting in unsatisfying performance due to the insufficient training. Such few-shot learning setting for table-to-text generation is not well-explored, and in this paper, we focus on exploring how to efficiently model for few-shot table-to-text generation with limited training pairs. Recently, pre-trained language models have shown promising progress in various natural language processing tasks (Yang et al., 2019b; Devlin et al., 2019; Radford et al"
2020.coling-main.179,W04-1013,0,0.015591,"n this paper, we propose TableGPT that exploits GPT-2’s learnt knowledge from pretraining on vast corpus for few-shot learning while enhance it for generating high fidelity text with two auxiliary tasks. Also, we perform ablation studies for evaluating each auxiliary task’s contribution. -sr represents the variant without table structure reconstruction, -cm represents the variant without content matching and -sr&cm shows the performance of GPT-2 without auxilary tasks. 4.3 Automatic Evaluation Following the previous work Chen et al. (2020b), we adopt BLEU-4 (Papineni et al., 2002) and ROUGE4 (Lin, 2004) to conduct automatic evaluations. Table 2 and 3 show corresponding results of comparing methods on different datasets. Although the Base achieves competitive results when training on largescale dataset (Liu et al., 2018), the performance drops drastically in few-shot setting. While utilizing a switch policy to combine copying words from table and generating words from GPT-2 (Base + switch + LM) can achieve impressive performance in all few-shot setting, a standard GPT-2 model (TableGPT - sr&cm) that takes a serialized table as input and generate text afterwards without copying can actually pe"
2020.coling-main.179,N16-1086,0,0.0221814,"y to generate high-fidelity text. On the contrary, TableGPT, enhanced to generate high-fidelity text with two auxiliary tasks without breaking one unified GPT-2 model for generating text, performs better in terms of fidelity and fluency of text in this example. 5 Related Work In recent years, neural models for generating texts directly from preprocessed data (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Gong et al., 2019; Feng et al., 2020), have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset. Mei et al. (2016) proposes a pre-selector on encoder-aligner-decoder model for generation, which strengthens model’s content selection ability and obtains considerable improvement over standard Seq2Seq model. Sha et al. (2018) proposes a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing on modeling table structure and dual attention mechanism to utilize the structure information when decoding. In addition, Bao et al. (2018) develops a table-aware sequence-to-sequence model on this task. However, Chen et al. (2020b) sho"
2020.coling-main.179,P02-1040,0,0.108992,"ase + switch + LM(R). • TableGPT: In this paper, we propose TableGPT that exploits GPT-2’s learnt knowledge from pretraining on vast corpus for few-shot learning while enhance it for generating high fidelity text with two auxiliary tasks. Also, we perform ablation studies for evaluating each auxiliary task’s contribution. -sr represents the variant without table structure reconstruction, -cm represents the variant without content matching and -sr&cm shows the performance of GPT-2 without auxilary tasks. 4.3 Automatic Evaluation Following the previous work Chen et al. (2020b), we adopt BLEU-4 (Papineni et al., 2002) and ROUGE4 (Lin, 2004) to conduct automatic evaluations. Table 2 and 3 show corresponding results of comparing methods on different datasets. Although the Base achieves competitive results when training on largescale dataset (Liu et al., 2018), the performance drops drastically in few-shot setting. While utilizing a switch policy to combine copying words from table and generating words from GPT-2 (Base + switch + LM) can achieve impressive performance in all few-shot setting, a standard GPT-2 model (TableGPT - sr&cm) that takes a serialized table as input and generate text afterwards without"
2020.coling-main.179,2020.findings-emnlp.17,0,0.0500674,"Missing"
2020.coling-main.179,P19-1195,0,0.0841739,"he inconsistent expression “played for , among others , England .” and the expression of wrong birth date, which shows the imperfect switch policy on deciding when to copy from table can sometimes hurt model’s ability to generate high-fidelity text. On the contrary, TableGPT, enhanced to generate high-fidelity text with two auxiliary tasks without breaking one unified GPT-2 model for generating text, performs better in terms of fidelity and fluency of text in this example. 5 Related Work In recent years, neural models for generating texts directly from preprocessed data (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Gong et al., 2019; Feng et al., 2020), have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset. Mei et al. (2016) proposes a pre-selector on encoder-aligner-decoder model for generation, which strengthens model’s content selection ability and obtains considerable improvement over standard Seq2Seq model. Sha et al. (2018) proposes a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing on modeling table structur"
2020.coling-main.179,2020.acl-main.101,0,0.0701278,"able. In order to overcome this 2 2 problem, we use the recently proposed Inexact Proximal point method for Optimal Transport (IPOT) (Chen et al., 2019a) as an approximation. For natural language generation tasks such as neural machine translation, OT distance is often applied by matching source sequence with whole target sequence, since almost every word in both sequences are supposed to be matched. However, when it comes to table-to-text task in a realistic way, there are some redundant information or words in both table and text. In order to apply the OT distance, unlike previous adoption (Wang et al., 2020) based on the assumption that all information in the table should be described in text, we propose to only match the record words which appear in both table and reference text. In this way, the OT distance is able to avoid wrongly penalizing text that doesn’t mention redundant information in table. 3.4 Learning Objective For table structure reconstruction and content matching, both auxiliary tasks are trained with the main GPT-2’s language model loss together, which can be regarded as multi-task learning. The loss function of multi-task learning consists of language model loss LLM , table stru"
2020.coling-main.179,D17-1239,0,0.0673118,"-2 may attributes to the inconsistent expression “played for , among others , England .” and the expression of wrong birth date, which shows the imperfect switch policy on deciding when to copy from table can sometimes hurt model’s ability to generate high-fidelity text. On the contrary, TableGPT, enhanced to generate high-fidelity text with two auxiliary tasks without breaking one unified GPT-2 model for generating text, performs better in terms of fidelity and fluency of text in this example. 5 Related Work In recent years, neural models for generating texts directly from preprocessed data (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Gong et al., 2019; Feng et al., 2020), have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset. Mei et al. (2016) proposes a pre-selector on encoder-aligner-decoder model for generation, which strengthens model’s content selection ability and obtains considerable improvement over standard Seq2Seq model. Sha et al. (2018) proposes a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing o"
2020.coling-main.179,P19-1296,0,0.029996,"Missing"
2020.coling-main.179,2020.acl-demos.30,0,0.0171698,"s a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing on modeling table structure and dual attention mechanism to utilize the structure information when decoding. In addition, Bao et al. (2018) develops a table-aware sequence-to-sequence model on this task. However, Chen et al. (2020b) shows that the well-performed Seq2Seq model trained on large-scale dataset suffer from limited training data in few-shot setting. Recently, GPT-2 has been successfully adapted to dialogue generation in few-shot setting (Zhang et al., 2020; Peng et al., 2020), showing potential to address insufficient training data problem for few-shot learning with the help of learnt knowledge from pretraining on vast corpus. As for table-to-text generation, Chen et al. (2020b) propose a switch model that use GPT-2 to generate template-like functional words while generating factual expressions via copying records’ values from table in few-shot scenario. Different from this work, we model the table and generate text within a GPT-2 model in a unified way and we show that our proposed TableGPT can perform well in the few-shot scenario. In additio"
2020.coling-main.238,D15-1109,0,0.0417366,"Missing"
2020.coling-main.238,L16-1432,0,0.0312328,"Missing"
2020.coling-main.238,D18-1241,0,0.0475096,"Missing"
2020.coling-main.238,N19-1423,0,0.0529108,"prehension for multiparty dialogs Methods. SQuAD 2.0 is an MRC dataset that adopts a passage as the input and the answer is a span from input passage (Rajpurkar et al., 2018). We adopt the following existing methods for SQuAD 2.0 on our dataset. In this paper, we use three different kinds of settings of BERT: BERT-base, BERT-large, and BERT-whole word masking (BERT-wwm). We concatenate all utterances from input dialog as a passage, and each utterance includes speaker and text. We used the open-source code of BERT to perform our experiments3 . BERT is a bidirectional encoder from transformers (Devlin et al., 2019). To learn better representations for text, BERT adopts two objectives: masked language modeling and the next sentence prediction during pretraining. In the BERT-wwm, if a part of a complete word WordPiece is replaced by [mask], the other parts of the same word will also be replaced by mask, which is the whole word mask. 3 https://github.com/google-research/bert 2648 Table 7: Results of machine reading comprehension for multiparty dialogs. Method BERT-base BERT-large BERT-wwm Human performance Human-machine gap EM Squad 2.0 73.1 80.0 86.7 86.8 0.1 Our 45.3 51.8 54.7 64.3 9.6 F1 Squad 2.0 76.2"
2020.coling-main.238,P17-1147,0,0.0218738,"in–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structur"
2020.coling-main.238,Q18-1023,0,0.0494099,"Missing"
2020.coling-main.238,D17-1082,0,0.168593,"y chat dialogs (Asher et al., 2016). The corpus derives from the online version of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et"
2020.coling-main.238,P19-1210,0,0.0607923,"Missing"
2020.coling-main.238,W15-4640,0,0.0772636,"r speakers converse over seven utterances. We additionally employ annotators to read the passage and contribute questions: in the example, the annotators propose three questions: two answerable and one unanswerable. We observe that adjacent utterance pairs can be incoherent, illustrating the key challenge. It is non-trivial to detect discourse relations between non-adjacent utterances; and crucially, difficult to correctly interpret a multiparty dialog without a proper understanding of the input’s complex structure. We derived Molweni from the large-scale multiparty dialog Ubuntu Chat Corpus (Lowe et al., 2015). We chose the name Molweni, as it is the plural form of “Hello” in the Xhosa language, representing multiparty dialog in the same language as Ubuntu. Our dataset contains 10,000 dialogs with 88,303 utterances and 30,066 questions including answerable and unanswerable questions. All answerable questions are extractive questions whose answer is a span in the source dialog. For unanswerable questions, we annotate their plausible answers from dialog. Most questions in Molweni are 5W1H questions – Why, What, Who, Where, When, and How. For each dialog in the corpus, annotators propose three questio"
2020.coling-main.238,N18-1185,0,0.141448,"et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multiparty dialogs and unanswerable questions. 3 The Molweni corpus Our dataset derives from the large scale multiparty dialogs dataset — the Ubuntu Chat Corpus (Lowe et al., 2015). We list our three reasons in choosing the Ubuntu Chat Corpus as the base corpus for annotation. • First, the Ubuntu dataset is a large multiparty dataset. After filtering the dataset by only retaining all utterances wi"
2020.coling-main.238,N16-1013,0,0.0594168,"Missing"
2020.coling-main.238,prasad-etal-2008-penn,0,0.129569,"Missing"
2020.coling-main.238,D16-1264,0,0.0434744,"game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes th"
2020.coling-main.238,P18-2124,0,0.204274,"course relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multipa"
2020.coling-main.238,Q19-1016,0,0.0616554,"Missing"
2020.coling-main.238,D13-1020,0,0.0464644,"urse parsing on multiparty chat dialogs (Asher et al., 2016). The corpus derives from the online version of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2"
2020.coling-main.238,Q19-1014,0,0.222945,"2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multiparty dialogs and unanswerable questions. 3 The Molweni corpus Our dataset derives from the large scale multiparty dialogs dataset — the Ubuntu Chat Corpus (Lowe et al., 2015). We list our three reasons in choosing the Ubuntu Chat Corpus as the base corpus for annotation. • First, the Ubuntu dataset is a large multiparty dataset. After filtering the dataset by only retaining all utterances with response relations, the"
2020.coling-main.238,W17-2623,0,0.022661,"roduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additi"
2020.coling-main.238,D07-1003,0,0.0332586,"from the online version of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Differ"
2020.coling-main.238,W19-5923,0,0.0645038,"datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multiparty dialogs and unanswerable questions. 3 The Molweni corpus Our dataset derives from the large scale multiparty dialogs dataset — the Ubuntu Chat Corpus (Lowe et al., 2015). We list our three reasons in choosing the Ubuntu Chat Corpus as the base corpus for annotation. • First, the Ubuntu dataset is a large multiparty dataset. After filtering the dataset by only retaining all utterances with response relations, there are still over 380K sessions and"
2020.coling-main.238,D15-1237,0,0.0235004,"rsion of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three"
2020.coling-main.238,P19-1374,0,\N,Missing
2020.coling-main.4,C18-1139,0,0.402548,"only one leaf. So we extend the pre-trained language models by integrating character information of words. There are two challenges for character integration: 1) how to model character information for whole words instead of subwords; 2) how to fuse the character representations with the subwords information in the original pre-trained models. We propose a new pre-training method CharBERT (BERT can also be replaced by other pre-trained models like RoBERTa) to solve these problems. Instead of the traditional CNN layer for modeling the character information, we use the context string embedding (Akbik et al., 2018) to model the word’s fine-grained representation. We use a dual-channel architecture for characters and original subwords and fuse them after each transformer block. Furthermore, we propose an unsupervised character learning task, which injects noises into characters and trains the model to denoise and restores the original word. The main advantages of our methods are: 1) character-aware: we construct word representations from characters based on the original subwords, which greatly complements the subword-based modeling. 2) robustness: we improve not only the performance but also the robustne"
2020.coling-main.4,D18-1316,0,0.0166296,"ermore, character representation has also been used to construct word representation; for example, Peters et al. (2018) construct the contextual word representation with character embeddings and achieve significant improvement. Adversarial Attack. PLMs are fragile to adversarial attacks, where human-imperceptible perturbations added to the original examples fool models to make wrong predictions. Jia and Liang (2017) and Si et al. (2020) show that state-of-the-art reading comprehension models can be fooled even with black-box attacks without accessing model parameters. Other white-box attacks (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020; Zang et al., 2020) use gradients or model prediction scores to find adversarial word substitutes as effective attacks. For character-level attacks, Belinkov and Bisk (2017) studied how synthetic noise and noise from natural sources affect character-level machine translations. Ebrahimi et al. (2018) investigated adversarial examples for character-level neural machine translation with a white-box adversary. To defend against character-level attacks, Pruthi et al. (2019) propose to place a word recognition model before the downstream classifier to perform wor"
2020.coling-main.4,P18-1246,0,0.0621178,"Missing"
2020.coling-main.4,N19-1423,0,0.582413,"terogeneous interaction module. We also propose a new pre-training task named NLM (Noisy LM) for unsupervised character representation learning. We evaluate our method on question answering, sequence labeling, and text classification tasks, both on the original datasets and adversarial misspelling test sets. The experimental results show that our method can significantly improve the performance and robustness of PLMs simultaneously. Pretrained models, evaluation sets, and code are available at https://github.com/wtma/CharBERT. 1 Introduction Unsupervised pre-trained language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have achieved surprising results on multiple NLP benchmarks. These models are pre-trained over large-scale open-domain corpora to obtain general language representations and then fine-tuned for specific downstream tasks. To deal with the large vocabulary, these models use Byte-Pair Encoding (BPE) (Sennrich et al., 2016) or its variations as the encoding method. Instead of whole words, BPE performs statistical analysis of the training corpus and split the words into subword units, a hybrid between character- and word-level representation. Even though BPE can enco"
2020.coling-main.4,I05-5002,0,0.0337228,"with ‘Original’ ones by introducing four kinds of character-level noise. The results are reported on Table 1. For comparable experiments, all of the results are reported by a single model without other tricks like data augmentation. We can find that our character-aware models (CharBERT, CharBERTRoBERTa ) outperform the baseline pre-trained models except for RoBERTa in SQuAD 1.1, which indicates the character information probably can not help the remaining questions. 4.3 Results on Text Classification We select four text classification tasks for evaluation: CoLA (Warstadt et al., 2019), MRPC (Dolan and Brockett, 2005), QQP, and QNLI (Wang et al., 2018). CoLA is a single-sentence task annotated with whether it is a grammatical English sentence. MRPC is a similarity task consisted of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in pairs are semantically equivalent. QQP is a paraphrase task with a collection of question pairs from the community question-answering website Quora, annotated with whether a pair of questions are semantically equivalent. QNLI is an inference task consisted of question-paragraph pairs, with human annotations for wh"
2020.coling-main.4,C18-1055,0,0.016148,"o the original examples fool models to make wrong predictions. Jia and Liang (2017) and Si et al. (2020) show that state-of-the-art reading comprehension models can be fooled even with black-box attacks without accessing model parameters. Other white-box attacks (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020; Zang et al., 2020) use gradients or model prediction scores to find adversarial word substitutes as effective attacks. For character-level attacks, Belinkov and Bisk (2017) studied how synthetic noise and noise from natural sources affect character-level machine translations. Ebrahimi et al. (2018) investigated adversarial examples for character-level neural machine translation with a white-box adversary. To defend against character-level attacks, Pruthi et al. (2019) propose to place a word recognition model before the downstream classifier to perform word spelling correction to combat spelling mistakes. Heterogeneous Representation Fusion. In our work, we need to fuse heterogeneous representations from two different sources. Similar modules have been applied before under different settings such as machine reading comprehension (Seo et al., 2017; Yang et al., 2017) and pre-trained lang"
2020.coling-main.4,D17-1215,0,0.0248382,"). CLMs have been shown to perform competitively on various NLP tasks, such as neural machine translation (Lee et al., 2017) and sequence labeling (S¸ahin and Steedman, 2018; Akbik et al., 2018). Furthermore, character representation has also been used to construct word representation; for example, Peters et al. (2018) construct the contextual word representation with character embeddings and achieve significant improvement. Adversarial Attack. PLMs are fragile to adversarial attacks, where human-imperceptible perturbations added to the original examples fool models to make wrong predictions. Jia and Liang (2017) and Si et al. (2020) show that state-of-the-art reading comprehension models can be fooled even with black-box attacks without accessing model parameters. Other white-box attacks (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020; Zang et al., 2020) use gradients or model prediction scores to find adversarial word substitutes as effective attacks. For character-level attacks, Belinkov and Bisk (2017) studied how synthetic noise and noise from natural sources affect character-level machine translations. Ebrahimi et al. (2018) investigated adversarial examples for character-level neural"
2020.coling-main.4,D14-1181,0,0.0107835,"l the input words as sequences of characters to catch the character information within and among subwords, a supplement for WordPiece embedding. The character-level representation is heterogeneous with subword-level representation from the embedding layer of pre-trained models as they come from different sources. However, they capture information at the different granularity and complement each other. In order to enable them to enrich each other effectively, we design a heterogeneous interaction module with two steps: 1) fuse: fuse the information from the dual-channel based on the CNN layer (Kim, 2014); 2) split: build new representations for each channel based on residual connection. 41 MLM NLM Heterogeneous Interaction Heterogeneous Interaction New Token Repr. New Char Repr. Add & Norm Add & Norm Char Channel Token Channel Transformer Divide …. Feed Forward & Act Feed Forward & Act Heterogeneous Interaction Token Channel CNN Char Channel Fusion Transformer Heterogeneous Interaction Token Channel Token Repr. Char Repr. Character Encoder Transformer Wash refer Char Channel It GRU … Character Encoder BERT Embedding Wash Feed Forward Char Channel Token Channel CLS] Feed Forward All Away … may"
2020.coling-main.4,Q17-1026,0,0.0474157,"bytes instead of Unicode characters as the base subword units, allowing BPE to encode any input sequence without OOV words with a modest vocabulary size (50K). 40 Character Representation. Traditional language models employ a pre-defined vocabulary of words, but they cannot handle out-of-vocabulary words well. Character language models (CLMs) can mitigate this problem by using a vocabulary of characters and modeling the character distribution for language modeling (Sutskever et al., 2011). CLMs have been shown to perform competitively on various NLP tasks, such as neural machine translation (Lee et al., 2017) and sequence labeling (S¸ahin and Steedman, 2018; Akbik et al., 2018). Furthermore, character representation has also been used to construct word representation; for example, Peters et al. (2018) construct the contextual word representation with character embeddings and achieve significant improvement. Adversarial Attack. PLMs are fragile to adversarial attacks, where human-imperceptible perturbations added to the original examples fool models to make wrong predictions. Jia and Liang (2017) and Si et al. (2020) show that state-of-the-art reading comprehension models can be fooled even with bl"
2020.coling-main.4,N18-1202,0,0.26853,"-training method CharBERT, which can enrich the word representation in PLMs by incorporating features at different levels of a word; • We evaluate our method on 8 benchmarks, and the results show that our method can significantly improve the performance compared to the strong BERT and RoBERTa baselines; • We construct three character attack test sets on three types of tasks. The experimental results indicate that our method can improve the robustness by a large margin. 2 Related Work Pre-trained Language Model. Early pre-trained language models (PLMs) like CoVe (McCann et al., 2017) and ELMo (Peters et al., 2018) are pre-trained with RNN-based models, which are usually used as a part of the embedding layer in task-specific models. GPT (Radford et al., 2019a) used the transformer decoder for language modeling by generative pre-training and fine-tuned for various downstream tasks. BERT (Devlin et al., 2019) pre-trains the transformer encoder and uses self-supervised pre-training on the larger corpus, achieving surprising results in multiple natural language understanding (NLU) benchmarks. Other PLMs such as RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) and ELECTRA (Cla"
2020.coling-main.4,P19-1561,0,0.496424,"even with black-box attacks without accessing model parameters. Other white-box attacks (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020; Zang et al., 2020) use gradients or model prediction scores to find adversarial word substitutes as effective attacks. For character-level attacks, Belinkov and Bisk (2017) studied how synthetic noise and noise from natural sources affect character-level machine translations. Ebrahimi et al. (2018) investigated adversarial examples for character-level neural machine translation with a white-box adversary. To defend against character-level attacks, Pruthi et al. (2019) propose to place a word recognition model before the downstream classifier to perform word spelling correction to combat spelling mistakes. Heterogeneous Representation Fusion. In our work, we need to fuse heterogeneous representations from two different sources. Similar modules have been applied before under different settings such as machine reading comprehension (Seo et al., 2017; Yang et al., 2017) and pre-trained language models (Zhang et al., 2019; Zhang et al., 2020). Different from these works, we design a two-step fusion module to fuse the character and subword representations by a i"
2020.coling-main.4,D16-1264,0,0.045816,", we use the same setting with the pre-trained model in the token channel like BERT and RoBERTa, both in pre-training and fine-tuning steps. For experimental comparison, we mainly compare CharBERT with previous state-of-the-art pre-trained models in BERTbase setting. We will also pre-train CharBERT with pre-trained models in BERTlarge setting in the future. 4.2 Results on Question Answering (SQuAD) The Stanford Question Answering Dataset (SQuAD) task requires to extract the answer span from a provided passage based on specified questions. We evaluate on two versions of the dataset: SQuAD 1.1 (Rajpurkar et al., 2016) and SQuAD 2.0 (Rajpurkar et al., 2018). For any question in SQuAD 1.1, there is always one or more answers in the corresponding passage. While for some questions in SQuAD 2.0, there is no answer in the passage. In the fine-tuning step for SQuAD, we concatenate the outputs from the character and token channel from CharBERT and use a classification layer to predict whether the token is a start or end position of the answer. For SQuAD 2.0, we use the probability on the token [CLS] as the results of no answer and search the best threshold for it. 44 Models BERT AdvBERT BERT+WordRec CharBERT QNLI"
2020.coling-main.4,P18-2124,0,0.0237081,"trained model in the token channel like BERT and RoBERTa, both in pre-training and fine-tuning steps. For experimental comparison, we mainly compare CharBERT with previous state-of-the-art pre-trained models in BERTbase setting. We will also pre-train CharBERT with pre-trained models in BERTlarge setting in the future. 4.2 Results on Question Answering (SQuAD) The Stanford Question Answering Dataset (SQuAD) task requires to extract the answer span from a provided passage based on specified questions. We evaluate on two versions of the dataset: SQuAD 1.1 (Rajpurkar et al., 2016) and SQuAD 2.0 (Rajpurkar et al., 2018). For any question in SQuAD 1.1, there is always one or more answers in the corresponding passage. While for some questions in SQuAD 2.0, there is no answer in the passage. In the fine-tuning step for SQuAD, we concatenate the outputs from the character and token channel from CharBERT and use a classification layer to predict whether the token is a start or end position of the answer. For SQuAD 2.0, we use the probability on the token [CLS] as the results of no answer and search the best threshold for it. 44 Models BERT AdvBERT BERT+WordRec CharBERT QNLI Original Attack 90.7 90.8 84.0 91.7 CoN"
2020.coling-main.4,P19-1103,0,0.036823,"sentation has also been used to construct word representation; for example, Peters et al. (2018) construct the contextual word representation with character embeddings and achieve significant improvement. Adversarial Attack. PLMs are fragile to adversarial attacks, where human-imperceptible perturbations added to the original examples fool models to make wrong predictions. Jia and Liang (2017) and Si et al. (2020) show that state-of-the-art reading comprehension models can be fooled even with black-box attacks without accessing model parameters. Other white-box attacks (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020; Zang et al., 2020) use gradients or model prediction scores to find adversarial word substitutes as effective attacks. For character-level attacks, Belinkov and Bisk (2017) studied how synthetic noise and noise from natural sources affect character-level machine translations. Ebrahimi et al. (2018) investigated adversarial examples for character-level neural machine translation with a white-box adversary. To defend against character-level attacks, Pruthi et al. (2019) propose to place a word recognition model before the downstream classifier to perform word spelling correct"
2020.coling-main.4,P18-1036,0,0.0525445,"Missing"
2020.coling-main.4,W03-0419,0,0.54789,"Missing"
2020.coling-main.4,P16-1162,0,0.410755,"thod can significantly improve the performance and robustness of PLMs simultaneously. Pretrained models, evaluation sets, and code are available at https://github.com/wtma/CharBERT. 1 Introduction Unsupervised pre-trained language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have achieved surprising results on multiple NLP benchmarks. These models are pre-trained over large-scale open-domain corpora to obtain general language representations and then fine-tuned for specific downstream tasks. To deal with the large vocabulary, these models use Byte-Pair Encoding (BPE) (Sennrich et al., 2016) or its variations as the encoding method. Instead of whole words, BPE performs statistical analysis of the training corpus and split the words into subword units, a hybrid between character- and word-level representation. Even though BPE can encode almost all the words in the vocabulary into WordPiece tokens without OOV words, it has two problems: 1) incomplete modeling: the subword representations may not incorporate the fine-grained character information and the representation of the whole word; 2) fragile representation: minor typos can drastically change the BPE tokens, leading to inaccur"
2020.coling-main.4,W18-5446,0,0.0192559,"kinds of character-level noise. The results are reported on Table 1. For comparable experiments, all of the results are reported by a single model without other tricks like data augmentation. We can find that our character-aware models (CharBERT, CharBERTRoBERTa ) outperform the baseline pre-trained models except for RoBERTa in SQuAD 1.1, which indicates the character information probably can not help the remaining questions. 4.3 Results on Text Classification We select four text classification tasks for evaluation: CoLA (Warstadt et al., 2019), MRPC (Dolan and Brockett, 2005), QQP, and QNLI (Wang et al., 2018). CoLA is a single-sentence task annotated with whether it is a grammatical English sentence. MRPC is a similarity task consisted of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in pairs are semantically equivalent. QQP is a paraphrase task with a collection of question pairs from the community question-answering website Quora, annotated with whether a pair of questions are semantically equivalent. QNLI is an inference task consisted of question-paragraph pairs, with human annotations for whether the paragraph sentence contai"
2020.coling-main.4,Q19-1040,0,0.0135406,"We construct the ‘Attack’ sets with ‘Original’ ones by introducing four kinds of character-level noise. The results are reported on Table 1. For comparable experiments, all of the results are reported by a single model without other tricks like data augmentation. We can find that our character-aware models (CharBERT, CharBERTRoBERTa ) outperform the baseline pre-trained models except for RoBERTa in SQuAD 1.1, which indicates the character information probably can not help the remaining questions. 4.3 Results on Text Classification We select four text classification tasks for evaluation: CoLA (Warstadt et al., 2019), MRPC (Dolan and Brockett, 2005), QQP, and QNLI (Wang et al., 2018). CoLA is a single-sentence task annotated with whether it is a grammatical English sentence. MRPC is a similarity task consisted of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in pairs are semantically equivalent. QQP is a paraphrase task with a collection of question pairs from the community question-answering website Quora, annotated with whether a pair of questions are semantically equivalent. QNLI is an inference task consisted of question-paragraph pai"
2020.coling-main.4,2020.acl-main.540,0,0.0610655,"truct word representation; for example, Peters et al. (2018) construct the contextual word representation with character embeddings and achieve significant improvement. Adversarial Attack. PLMs are fragile to adversarial attacks, where human-imperceptible perturbations added to the original examples fool models to make wrong predictions. Jia and Liang (2017) and Si et al. (2020) show that state-of-the-art reading comprehension models can be fooled even with black-box attacks without accessing model parameters. Other white-box attacks (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020; Zang et al., 2020) use gradients or model prediction scores to find adversarial word substitutes as effective attacks. For character-level attacks, Belinkov and Bisk (2017) studied how synthetic noise and noise from natural sources affect character-level machine translations. Ebrahimi et al. (2018) investigated adversarial examples for character-level neural machine translation with a white-box adversary. To defend against character-level attacks, Pruthi et al. (2019) propose to place a word recognition model before the downstream classifier to perform word spelling correction to combat spelling mistakes. Heter"
2020.coling-main.4,P19-1139,0,0.0217986,"adversarial examples for character-level neural machine translation with a white-box adversary. To defend against character-level attacks, Pruthi et al. (2019) propose to place a word recognition model before the downstream classifier to perform word spelling correction to combat spelling mistakes. Heterogeneous Representation Fusion. In our work, we need to fuse heterogeneous representations from two different sources. Similar modules have been applied before under different settings such as machine reading comprehension (Seo et al., 2017; Yang et al., 2017) and pre-trained language models (Zhang et al., 2019; Zhang et al., 2020). Different from these works, we design a two-step fusion module to fuse the character and subword representations by a interactive way, which can be extended to integrate other information into language model (e.g. diacritics or external knowledge). 3 Methodology In this section, we present the overall framework of CharBERT and its submodules, including the model architecture in Section 3.2, the character encoder in Section 3.3, the heterogeneous interaction module in Section 3.4, the new pre-training task in Section 3.5, and the fine-tuning method in Section 3.6. 3.1 Not"
2020.coling-main.466,D19-1378,0,0.0217347,"graphenhanced linguistic-level representations and program-guided symbolic-level representations together to predict the labels. However, their method focuses on the representation of symbolic information, rather than take advantage of the combination of both types of information. More specifically, we believe that the concatenation operation between two types of representations is not effective enough to leverage the linguistic information and symbolic information to perform reasoning. In recent studies, graph neural networks show their powerful ability in dealing with semi-structured data (Bogin et al., 2019a; Bogin et al., 2019b). Under this consideration, we propose to use graph neural networks that learn to combine linguistic information and symbolic information in a simultaneous fashion. Since the representations of different types of information fall in different embedding spaces, This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 5335 Proceedings of the 28th International Conference on Computational Linguistics, pages 5335–5346 Barcelona, Spain (Online), December 8-13, 2020 Table Date Result Sc"
2020.coling-main.466,P17-1152,0,0.0636035,"Missing"
2020.coling-main.466,P18-1071,0,0.0486588,"Missing"
2020.coling-main.466,N19-1246,0,0.0184922,"rrect by the given semi-structured table evidence. It is well accepted that symbolic information (such as count and only) plays a great role in understanding semi-structured evidence based statements (Wenhu Chen and Wang, 2020). However, most existing approaches for fact verification (Thorne et al., 2018; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b; Soleimani et al., 2020) focus on the understanding of natural language, namely, linguistic reasoning, but fail to consider symbolic information, which plays an important role in complex reasoning (Liang et al., 2017; Dua et al., 2019; Chen et al., 2019). Due to the diversity of natural language expressions, it is difficult to capture symbolic information effectively from natural language directly. Consequently, how to leverage symbolic information effectively becomes a crucial problem. To alleviate this problem, Zhong et al. (2020a) propose a graph module network that concatenates graphenhanced linguistic-level representations and program-guided symbolic-level representations together to predict the labels. However, their method focuses on the representation of symbolic information, rather than take advantage of the combi"
2020.coling-main.466,D18-1192,0,0.0200757,"Missing"
2020.coling-main.466,P17-1003,0,0.0485526,"er a statement is correct by the given semi-structured table evidence. It is well accepted that symbolic information (such as count and only) plays a great role in understanding semi-structured evidence based statements (Wenhu Chen and Wang, 2020). However, most existing approaches for fact verification (Thorne et al., 2018; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b; Soleimani et al., 2020) focus on the understanding of natural language, namely, linguistic reasoning, but fail to consider symbolic information, which plays an important role in complex reasoning (Liang et al., 2017; Dua et al., 2019; Chen et al., 2019). Due to the diversity of natural language expressions, it is difficult to capture symbolic information effectively from natural language directly. Consequently, how to leverage symbolic information effectively becomes a crucial problem. To alleviate this problem, Zhong et al. (2020a) propose a graph module network that concatenates graphenhanced linguistic-level representations and program-guided symbolic-level representations together to predict the labels. However, their method focuses on the representation of symbolic information, rather than take adva"
2020.coling-main.466,D19-1488,0,0.0612609,"Missing"
2020.coling-main.466,2020.acl-main.655,0,0.142399,"he evidence with the structured or semi-structured format. A recently proposed dataset TABFACT (Wenhu Chen and Wang, 2020) fills this gap, which is designed to deal with the table-based fact verification problem, namely, verifying whether a statement is correct by the given semi-structured table evidence. It is well accepted that symbolic information (such as count and only) plays a great role in understanding semi-structured evidence based statements (Wenhu Chen and Wang, 2020). However, most existing approaches for fact verification (Thorne et al., 2018; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b; Soleimani et al., 2020) focus on the understanding of natural language, namely, linguistic reasoning, but fail to consider symbolic information, which plays an important role in complex reasoning (Liang et al., 2017; Dua et al., 2019; Chen et al., 2019). Due to the diversity of natural language expressions, it is difficult to capture symbolic information effectively from natural language directly. Consequently, how to leverage symbolic information effectively becomes a crucial problem. To alleviate this problem, Zhong et al. (2020a) propose a graph module network that co"
2020.coling-main.466,N18-1074,0,0.0790208,"us on the unstructured text as the evidence and ignoring the evidence with the structured or semi-structured format. A recently proposed dataset TABFACT (Wenhu Chen and Wang, 2020) fills this gap, which is designed to deal with the table-based fact verification problem, namely, verifying whether a statement is correct by the given semi-structured table evidence. It is well accepted that symbolic information (such as count and only) plays a great role in understanding semi-structured evidence based statements (Wenhu Chen and Wang, 2020). However, most existing approaches for fact verification (Thorne et al., 2018; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b; Soleimani et al., 2020) focus on the understanding of natural language, namely, linguistic reasoning, but fail to consider symbolic information, which plays an important role in complex reasoning (Liang et al., 2017; Dua et al., 2019; Chen et al., 2019). Due to the diversity of natural language expressions, it is difficult to capture symbolic information effectively from natural language directly. Consequently, how to leverage symbolic information effectively becomes a crucial problem. To alleviate this problem, Zhon"
2020.coling-main.466,P19-1260,0,0.0615356,"Missing"
2020.coling-main.466,2020.acl-main.553,0,0.0432566,"Missing"
2020.coling-main.466,2020.acl-main.539,0,0.331375,"he structured or semi-structured format. A recently proposed dataset TABFACT (Wenhu Chen and Wang, 2020) fills this gap, which is designed to deal with the table-based fact verification problem, namely, verifying whether a statement is correct by the given semi-structured table evidence. It is well accepted that symbolic information (such as count and only) plays a great role in understanding semi-structured evidence based statements (Wenhu Chen and Wang, 2020). However, most existing approaches for fact verification (Thorne et al., 2018; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b; Soleimani et al., 2020) focus on the understanding of natural language, namely, linguistic reasoning, but fail to consider symbolic information, which plays an important role in complex reasoning (Liang et al., 2017; Dua et al., 2019; Chen et al., 2019). Due to the diversity of natural language expressions, it is difficult to capture symbolic information effectively from natural language directly. Consequently, how to leverage symbolic information effectively becomes a crucial problem. To alleviate this problem, Zhong et al. (2020a) propose a graph module network that concatenates graphenha"
2020.coling-main.466,2020.acl-main.549,0,0.0653607,"he structured or semi-structured format. A recently proposed dataset TABFACT (Wenhu Chen and Wang, 2020) fills this gap, which is designed to deal with the table-based fact verification problem, namely, verifying whether a statement is correct by the given semi-structured table evidence. It is well accepted that symbolic information (such as count and only) plays a great role in understanding semi-structured evidence based statements (Wenhu Chen and Wang, 2020). However, most existing approaches for fact verification (Thorne et al., 2018; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b; Soleimani et al., 2020) focus on the understanding of natural language, namely, linguistic reasoning, but fail to consider symbolic information, which plays an important role in complex reasoning (Liang et al., 2017; Dua et al., 2019; Chen et al., 2019). Due to the diversity of natural language expressions, it is difficult to capture symbolic information effectively from natural language directly. Consequently, how to leverage symbolic information effectively becomes a crucial problem. To alleviate this problem, Zhong et al. (2020a) propose a graph module network that concatenates graphenha"
2020.coling-main.466,P19-1085,0,0.0362313,"ence and ignoring the evidence with the structured or semi-structured format. A recently proposed dataset TABFACT (Wenhu Chen and Wang, 2020) fills this gap, which is designed to deal with the table-based fact verification problem, namely, verifying whether a statement is correct by the given semi-structured table evidence. It is well accepted that symbolic information (such as count and only) plays a great role in understanding semi-structured evidence based statements (Wenhu Chen and Wang, 2020). However, most existing approaches for fact verification (Thorne et al., 2018; Nie et al., 2019; Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b; Soleimani et al., 2020) focus on the understanding of natural language, namely, linguistic reasoning, but fail to consider symbolic information, which plays an important role in complex reasoning (Liang et al., 2017; Dua et al., 2019; Chen et al., 2019). Due to the diversity of natural language expressions, it is difficult to capture symbolic information effectively from natural language directly. Consequently, how to leverage symbolic information effectively becomes a crucial problem. To alleviate this problem, Zhong et al. (2020a) propose a graph modu"
2020.coling-main.589,C10-3004,1,0.625896,"rt Chinese pre-trained language models show that there is still much room for them to surpass human performance, indicating the proposed data is challenging. 2 The Proposed Dataset 2.1 Task Definition Generally, the reading comprehension task can be described as a triple 〈P, Q, A〉, where P represents Passage, Q represents Question, and the A represents Answer. Specifically, for sentence cloze-style reading comprehension task, we select several sentences in the passages and replace with special marks (for example, [BLANK]), forming an incomplete passage. The sentences are identified using LTP (Che et al., 2010), and we further split the sentence with comma and period mark, as some of the sentences are too long. The selected sentences form a candidate list, and the machine should fill in the blanks with these candidate sentences to form a complete passage. Note that, to add more difficulties, we could also add the fake candidates, which do not belong to any blanks in the passage. 2.2 Passage Selection The raw material of the proposed dataset is from children’s books, containing fairy tales and narratives, which is the proper genre for testing the sentence-level inference ability, requiring the correc"
2020.coling-main.589,C16-1167,1,0.920479,"ms based on the pre-trained models, and the results show that the stateof-the-art model still underperforms human performance by a large margin. We release the dataset and baseline system to further facilitate our community. Resources available through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Bes"
2020.coling-main.589,P17-1055,1,0.852937,"to further facilitate our community. Resources available through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Dai"
2020.coling-main.589,L18-1431,1,0.890885,"jpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuReader), which consists of 200k queries annotated from the user query logs on the search engine. In span-extraction MRC, Cui et al. (2019b) proposed CMRC 2018 dataset for Simplified Chinese, and Shao et al. (2018) proposed DRCD dataset for Traditional Chinese, similar to the popular dataset SQuAD (Rajpurkar et al., 2016). Zheng et al. (2019) proposed a large-scale Ch"
2020.coling-main.589,D19-1600,1,0.310512,"Missing"
2020.coling-main.589,2020.findings-emnlp.58,1,0.795757,"Missing"
2020.coling-main.589,N19-1423,0,0.0241785,"of 200k queries annotated from the user query logs on the search engine. In span-extraction MRC, Cui et al. (2019b) proposed CMRC 2018 dataset for Simplified Chinese, and Shao et al. (2018) proposed DRCD dataset for Traditional Chinese, similar to the popular dataset SQuAD (Rajpurkar et al., 2016). Zheng et al. (2019) proposed a large-scale Chinese idiom cloze dataset. Though various efforts have been made, most of these datasets stop at token-level or span-level inference, which neglect the importance of long-range reasoning of the context. Moreover, powerful pretrained models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6717 Proceedings of the 28th International Conference on Computational Linguistics, pages 6717–6723 Barcelona, Spain (Online), December 8-13, 2020 [Passage] ”森林里有一棵大树，树上有一个鸟窝。[BLANK1]，还从来没有看到过 鸟宝宝长什么样。小松鼠说：“我爬到树上去看过，鸟宝宝光溜溜的， 身上一根羽毛也没有。” “我不相信，”小白兔说，“所有的鸟都是有羽 毛的。” “鸟宝宝没有羽毛。”小松鼠说，“你不信自己去看。” 小白兔 不会爬树，它没有办法去看。小白兔说：“我请蓝狐狸去看一看，我相 信蓝狐狸的话。” 小松鼠说：“蓝狐狸跟你一样，也不会爬树。” 蓝狐 狸说：“我有魔法树叶，我能变成一只狐狸鸟。” [BLANK2]，一下子飞 到了树顶"
2020.coling-main.589,P17-1168,0,0.0210363,"tate our community. Resources available through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy T"
2020.coling-main.589,W18-2605,0,0.0261255,"massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuReader), which consists of 200k queries annotated from the user query logs on the search engine. In span-extraction MRC, Cui et al. (2019b) proposed CMRC 2018 dataset for Simplified Chinese, and Shao et al. (2018) proposed DRCD dataset for Traditional Chinese, similar to the popular dataset SQuAD (Rajpurkar et al., 2016). Zheng et al. (2019) proposed a large-scale Chinese idiom cloze dataset. Though various efforts have been made, most of these datasets stop at token-level or span-level inference, which neglect the im"
2020.coling-main.589,P16-1086,0,0.0314597,"t and baseline system to further facilitate our community. Resources available through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension d"
2020.coling-main.589,D17-1082,0,0.0203873,": The king married another queen 1: She had a pretty daughter named Snow White 2: The king was also passed away 3: who is most beautiful of all? 4: she was very happy 5: she was very angry [Answers] 4, 3, 2, 1, 0 [Answers] 1, 0, 3, 5 Figure 1: Examples of the proposed CMRC 2019 dataset. The candidate with underline means it is a fake candidate (does not belong to any blank). For clarity, we also provide an English example. 2019) have surpassed human performance on various MRC datasets, such as SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), RACE (Lai et al., 2017), etc. To further test the machine comprehension ability, In this paper, we propose a new task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC). The proposed task preserves the simplicity of cloze-style reading comprehension but requires sentence-level inference when filling the blanks. Figure 1 shows an example of the proposed dataset. We conclude our contributions in three aspects. • We propose a new machine reading comprehension task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC), which aims to test the ability of sentence-level inference. • We release"
2020.coling-main.589,D16-1264,0,0.0906982,"through https://github.com/ymcui/cmrc2019 1 Introduction Machine Reading Comprehension (MRC) is a task to comprehend given articles and answer the questions based on them, which is an important ability for artificial intelligence. The recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) pro"
2020.coling-main.589,P18-2124,0,0.0246221,"4: 小动物们只看到过鸟妈妈和鸟爸爸在鸟窝里飞进飞出 5: 小松鼠变成了一只蓝色的大鸟 [Candidates] 0: The king married another queen 1: She had a pretty daughter named Snow White 2: The king was also passed away 3: who is most beautiful of all? 4: she was very happy 5: she was very angry [Answers] 4, 3, 2, 1, 0 [Answers] 1, 0, 3, 5 Figure 1: Examples of the proposed CMRC 2019 dataset. The candidate with underline means it is a fake candidate (does not belong to any blank). For clarity, we also provide an English example. 2019) have surpassed human performance on various MRC datasets, such as SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), RACE (Lai et al., 2017), etc. To further test the machine comprehension ability, In this paper, we propose a new task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC). The proposed task preserves the simplicity of cloze-style reading comprehension but requires sentence-level inference when filling the blanks. Figure 1 shows an example of the proposed dataset. We conclude our contributions in three aspects. • We propose a new machine reading comprehension task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC), which aims to test th"
2020.coling-main.589,Q19-1016,0,0.0235357,"松鼠变成了一只蓝色的大鸟 [Candidates] 0: The king married another queen 1: She had a pretty daughter named Snow White 2: The king was also passed away 3: who is most beautiful of all? 4: she was very happy 5: she was very angry [Answers] 4, 3, 2, 1, 0 [Answers] 1, 0, 3, 5 Figure 1: Examples of the proposed CMRC 2019 dataset. The candidate with underline means it is a fake candidate (does not belong to any blank). For clarity, we also provide an English example. 2019) have surpassed human performance on various MRC datasets, such as SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), RACE (Lai et al., 2017), etc. To further test the machine comprehension ability, In this paper, we propose a new task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC). The proposed task preserves the simplicity of cloze-style reading comprehension but requires sentence-level inference when filling the blanks. Figure 1 shows an example of the proposed dataset. We conclude our contributions in three aspects. • We propose a new machine reading comprehension task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC), which aims to test the ability of sentence-level"
2020.coling-main.589,P17-1018,0,0.0194371,"recent MRC research was originated from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuR"
2020.coling-main.589,P18-1158,0,0.0154653,"from the cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), which requires to fill in the blank with a word or named entity, and following works on these datasets have laid the foundations of this research (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017). Later on, SQuAD (Rajpurkar et al., 2016) was proposed, and the answer transformed from a single word to a span, which has become a representative span-extraction dataset and massive neural network approaches (Wang and Jiang, 2016; Xiong et al., 2016; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018) have been proposed which further accelerated the MRC research. Besides the MRC in English text, we have also seen rapid progress on Chinese MRC research. Cui et al. (2016) proposed the first Chinese cloze-style reading comprehension dataset: People Daily & Children’s Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuReader), which consists of 200k queri"
2020.coling-main.589,P19-1075,0,0.0222042,"Fairy Tale (PD&CFT). Later, Cui et al. (2018) proposed another dataset for CMRC 2017, which is gathered from children’s reading books, consisting of both cloze and natural questions. He et al. (2018) proposed a large-scale open-domain Chinese reading comprehension dataset (DuReader), which consists of 200k queries annotated from the user query logs on the search engine. In span-extraction MRC, Cui et al. (2019b) proposed CMRC 2018 dataset for Simplified Chinese, and Shao et al. (2018) proposed DRCD dataset for Traditional Chinese, similar to the popular dataset SQuAD (Rajpurkar et al., 2016). Zheng et al. (2019) proposed a large-scale Chinese idiom cloze dataset. Though various efforts have been made, most of these datasets stop at token-level or span-level inference, which neglect the importance of long-range reasoning of the context. Moreover, powerful pretrained models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6717 Proceedings of the 28th International Conference on Computational Linguistics, pages 6717–6723 B"
2020.conll-shared.6,E17-2039,0,0.252146,"Missing"
2020.conll-shared.6,2020.conll-shared.2,0,0.367904,"We didn’t find any existing parser for PTG. Thus we design a list-based arc-eager transitionbased parser for PTG. Abstract Meaning Representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor (2)) used to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts and labeled directed edges are relations between the concepts. We directly employ state-of-the-art parser of Cai and Lam (2020). Discourse Representation Graphs (DRG) are proposed by Abzianidze et al. (2020) (Flavor (2)), which are derived from the DRS annotations in the Parallel Meaning Bank (Bos et al., 2017; Abzianidze et al., 2017). Its concepts are represented by WordNet 3.0 (Fellbaum, 1998) senses and semantic roles by the adapted version of VerbNet (Schuler, 2006) roles. Similar to PTG, we don’t find any existing parser for DRG, thus we modify the AMR parser to process the DRG. 3 3.1 buffer holding unprocessed words. E is a set of labeled dependency arcs. V is a set of graph nodes including concept nodes and surface tokens. The initial state is ([0], [ ], [1, · · · , n], [ ], V ) , where V"
2020.conll-shared.6,W13-2322,0,0.0987643,"ment (or anchoring) information effectively. Prague Tectogrammatical Graphs (PTG) are graph-structured multi-layered semantic representation formalism (Flavor (1)) proposed by Zeman and Hajiˇc (2020). PTG graphs essentially recast core predicate–argument structure in the form of mostly anchored dependency graphs, albeit introducing ‘empty’ (or generated, in FGD terminology) nodes, for which there is no corresponding surface token. We didn’t find any existing parser for PTG. Thus we design a list-based arc-eager transitionbased parser for PTG. Abstract Meaning Representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor (2)) used to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts and labeled directed edges are relations between the concepts. We directly employ state-of-the-art parser of Cai and Lam (2020). Discourse Representation Graphs (DRG) are proposed by Abzianidze et al. (2020) (Flavor (2)), which are derived from the DRS annotations in the Parallel Meaning Bank (Bos et al., 2017; Abzianidze et al., 2017). Its concepts are represented by"
2020.conll-shared.6,P13-1023,0,0.176134,"multiple nodes anchored to overlapping sub-strings, and (b) Flavor (2), including AMR and DRG, not considering the correspondence between nodes and the surface tokens. • Pretrained Language Model Our systems benefit a lot from the pretrained language models, i.e., BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020) and XLM-RoBERTa (Conneau et al., 2020). 2 Background In the following, we will give a brief introduction to these frameworks and our corresponding solutions. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor (1)) firstly proposed by Abend and Rappoport (2013), which treats input words as terminal nodes. The non-terminal node might govern one or more nodes, which may be discontinuous. Moreover, one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships 1 See http://mrp.nlpl.eu/ for further technical details, information on how to obtain the data, and official results. 65 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 65–72 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics between nodes"
2020.conll-shared.6,2020.acl-main.119,0,0.186025,"frameworks (DRG, AMR). In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81/0.69. 1 • Transition-based Parser for Flavor (1) Following Che et al. (2019), the top system in CoNLL 2019 shared task (Oepen et al., 2019), we employ the transition-based parser for Flavor (1) frameworks since it’s very flexible in predicting the anchor information. We directly use their parser for UCCA and EDS. And we design a new parser for PTG. • Iterative Inference Parser for Flavor (2) Recently, Cai and Lam (2020) proposed Graph⇔Sequence Iterative Inference system for AMR parsing, which treats parsing as a series of dual decisions on the input sequence and the incrementally constructed graph, achieving state-of-the-art results. We adopt their model for Flavor (2) frameworks (AMR, DRG). Introduction The goal of the CoNLL 2020 shared task (Oepen et al., 2020) is to develop a unified parsing system to process all five semantic graph banks across different languages. This task combines five frameworks for graph-based meaning representation, each with its specific formal and linguistic assumptions, includin"
2020.conll-shared.6,2020.conll-shared.0,0,0.0819124,"Missing"
2020.conll-shared.6,K19-2007,1,0.791076,"Missing"
2020.conll-shared.6,oepen-lonning-2006-discriminant,0,0.205929,", and official results. 65 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 65–72 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics between nodes are represented by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs). We directly employ the system of Che et al. (2019), which achieves the 1st at CoNLL 2019 shared task. Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor (1)) proposed by Oepen and Lønning (2006). Che et al. (2019) introduce a neural encoder-decoder transition-based parser for the EDS graph, which extracts the node alignment (or anchoring) information effectively. Prague Tectogrammatical Graphs (PTG) are graph-structured multi-layered semantic representation formalism (Flavor (1)) proposed by Zeman and Hajiˇc (2020). PTG graphs essentially recast core predicate–argument structure in the form of mostly anchored dependency graphs, albeit introducing ‘empty’ (or generated, in FGD terminology) nodes, for which there is no corresponding surface token. We didn’t find any existing parser for"
2020.conll-shared.6,W04-2708,0,0.219119,"Missing"
2020.conll-shared.6,2020.conll-shared.3,0,0.395447,"Missing"
2020.conll-shared.6,2020.acl-main.747,0,0.0444384,"Missing"
2020.conll-shared.6,N19-1423,0,0.0948688,"ature of the relationship they assume between the linguistic surface string and the nodes of the graph. They call this relation anchoring. Therefore, the involved five frameworks could be divided into two classes: (a) Flavor (1), including UCCA, EDS, and PTG, allowing arbitrary parts of the sentence as node anchors, as well as multiple nodes anchored to overlapping sub-strings, and (b) Flavor (2), including AMR and DRG, not considering the correspondence between nodes and the surface tokens. • Pretrained Language Model Our systems benefit a lot from the pretrained language models, i.e., BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020) and XLM-RoBERTa (Conneau et al., 2020). 2 Background In the following, we will give a brief introduction to these frameworks and our corresponding solutions. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor (1)) firstly proposed by Abend and Rappoport (2013), which treats input words as terminal nodes. The non-terminal node might govern one or more nodes, which may be discontinuous. Moreover, one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remot"
2020.conll-shared.6,P15-1033,0,0.0236567,"ll be determined by rule, which will be introduced in Section 5. • T ERMINALX creates new non-terminal nodes with label X. • N ODEX creates a new node on the buffer as a parent of the first element on the stack, with X as its label. Transition-based Parser for Flavor (1) Background • N ODE -ROOTX creates a new node on the buffer as a child of the root with X as its label. A tuple (S, L, B, E, V ) is used to represent the parsing state, where S is a stack holding processed words, L is a list holding words popped out of S that will be pushed back in the future, and B is a 2 We recommend reading Dyer et al. (2015) for more details. 66 Before Transition Transition Stack Buffer Nodes Edges S S|x S|x S |y, x S |x, y S |y, x S |x, y S |x, y [root] x|B B B B B B B B ∅ V V V V V V V V V E E E E E E E E E After Transition Buffer Nodes Edges |x B B y|B B B B B x|B ∅ V V V ∪ {y} V V V V V V E E E ∪ {(y, x)X } E ∪ {(x, y)X } E ∪ {(x, y)X } E ∪ {(x, y)∗X } E ∪ {(x, y)∗X } E E S S S S S S S S ∅ S HIFT R EDUCE N ODEX L EFT-E DGEX R IGHT-E DGEX L EFT-R EMOTEX R IGHT-R EMOTEX S WAP F INISH Condition Stack |x |y, x |x, y |y, x |x, y |y x 6= root   x 6∈ w1:n , y 6= root,  y 6 ;G x i(x) &lt; i(y) Table 1: The transition"
2020.conll-shared.6,P17-1104,0,0.0615865,"S S S S ∅ S HIFT R EDUCE N ODEX L EFT-E DGEX R IGHT-E DGEX L EFT-R EMOTEX R IGHT-R EMOTEX S WAP F INISH Condition Stack |x |y, x |x, y |y, x |x, y |y x 6= root   x 6∈ w1:n , y 6= root,  y 6 ;G x i(x) &lt; i(y) Table 1: The transition set of UCCA parser. We write the Stack with its top to the right and the Buffer with its head to the left. (·, ·)X denotes a primary X-labeled edge, and (·, ·)∗X a remote X-labeled edge. i(x) is a running index for the created nodes. In addition to the specified conditions, the prospective child in an E DGE transition must not already have a primary parent. From (Hershcovich et al., 2017). Before Transition Transition Stack List Buffer Nodes Edges S S|x S|x S|y S|x S S S S|y [root] L L L L L L L L L ∅ x|B B y|B x|B B x|B x|B x|B x|B ∅ V V V V V V V V V V E E E E E E E E E E After Transition Stack S HIFT R EDUCE R IGHT-E DGEX L EFT-E DGEX PASS D ROP T OP N ODE -S TARTX N ODE -E ND F INISH S S S S S S S S S ∅ |L|x |x |y |L |y |y Condition List Buffer Nodes Edges ∅ L L L x|L ∅ L L L ∅ B B y|B x|B B B x|B x|B x|B ∅ V V V V V V V ∪ Top(x) V ∪ {ystart=x,label=X } V ∪ {yend=x } V E E E ∪ {(x, y)X } E ∪ {(x, y)X } E E E E E E concept(x) concept(x) ∧ concept(y) concept(x) ∧ concept(y)"
2020.conll-shared.6,2020.conll-shared.1,0,0.205933,"Missing"
2020.conll-shared.6,K19-2001,0,0.0803779,"Missing"
2020.emnlp-main.118,J19-1002,0,0.0138194,"ntic parsing. Recent work has shown that properly designing new MRs often helps improve the performance of neural semantic parsing. Except for the four MRs that we study, Liang et al. (2011); Liang (2013) presented DCS and lambda DCS for querying knowledge bases and demonstrated their advantages over Lambda Calculus. Guo et al. (2019) proposed SemQL, an MR for relational database queries, and they showed improvements in the Spider benchmark. Wolfson et al. (2020) designed an MR, named QDMR, for representing the meaning of questions through question decomposition. Instead of designing new MRs, Cheng et al. (2019) proposed a transition-based semantic parsing approach that supports generating tree-structured logical forms in either top-down or bottom-up manners. Their experimental results on various semantic parsing datasets using FunQL showed that topdown generation outperforms bottom-up in various settings. Our work aims to investigate the characteristics of different MRs and their impact on neural semantic parsing. Ungrounded semantic parsing. Except for the grounded MRs studied in this work, there are also ungrounded MRs that are not tied to any particular applications, such as AMR (Banarescu et al."
2020.emnlp-main.118,W13-2322,0,0.099239,"Missing"
2020.emnlp-main.118,H94-1010,0,0.137841,"ons. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https: //github.com/JasperGuo/Unimer. 1 A remarkable vision of artificial intelligence is to enable human interactions with machines through natural language. Semantic parsing has emerged as a key technology for achieving this goal. In general, semantic parsing aims to transform a natural language utterance into a logic form, i.e., a formal, machine-interpretable meaning representation (MR) (Zelle and Mooney, 1996; Dahl et al., 1994).1 Thanks to the recent development ∗ ATIS 91.3 69.0 79.2 Job 91.4 85.0 92.1 - Table 1: State-of-the-art performance for MRs on Geo, ATIS, and Job. The top table shows exact-match accuracy whereas the bottom table shows execution-match accuracy. Most existing work focuses on evaluating only a small subset of dataset × MR pairs, leaving most of the table unexplored. A finer-grained table is available in the supplementary material (Table 8). Introduction Work done during an internship at Microsoft Research. In this paper, we focus on grounded semantic parsing, where meaning representations are g"
2020.emnlp-main.118,D13-1160,0,0.12455,"rely on an analysis of grammatical structure of natural language. There has not been work on benchmarking MRs for grounded semantic parsing in neural approaches, to the best of our knowledge. Weakly supervised semantic parsing. In this paper, we focus on supervised learning for semantic parsing, where each utterance has its corresponding logical form annotated. But the similar evaluation methodology could be applied to weakly supervised semantic parsing, which receives wide attention because parsers are only supervised with execution results and annotated logical forms are no longer required (Berant et al., 2013; Pasupat and Liang, 2015; Goldman et al., 2018; Liang et al., 2018; Mueller et al., 2019). We also notice that various MRs have been used in weakly supervised semantic parsing, and it would be valuable to explore the impact of MRs in such settings. 7 Conclusion In this work, we propose U NIMER, a unified benchmark on meaning representations, based on established semantic parsing datasets; U NIMER covers three domains and four different meaning representations along with their execution engines. U NIMER allows researchers to comprehensively and fairly evaluate the performance of their approach"
2020.emnlp-main.118,W18-2501,0,0.0481847,"Missing"
2020.emnlp-main.118,P18-1168,0,0.162045,"f natural language. There has not been work on benchmarking MRs for grounded semantic parsing in neural approaches, to the best of our knowledge. Weakly supervised semantic parsing. In this paper, we focus on supervised learning for semantic parsing, where each utterance has its corresponding logical form annotated. But the similar evaluation methodology could be applied to weakly supervised semantic parsing, which receives wide attention because parsers are only supervised with execution results and annotated logical forms are no longer required (Berant et al., 2013; Pasupat and Liang, 2015; Goldman et al., 2018; Liang et al., 2018; Mueller et al., 2019). We also notice that various MRs have been used in weakly supervised semantic parsing, and it would be valuable to explore the impact of MRs in such settings. 7 Conclusion In this work, we propose U NIMER, a unified benchmark on meaning representations, based on established semantic parsing datasets; U NIMER covers three domains and four different meaning representations along with their execution engines. U NIMER allows researchers to comprehensively and fairly evaluate the performance of their approaches. Based on U NIMER, we conduct an empirical s"
2020.emnlp-main.118,P19-1444,1,0.938776,"(Yin and Neubig, 2017)), under the supervised learning setting. In addition to the empirical study above, we further analyze the impact of two operations, i.e., program alias and grammar rules, to understand how they affect different MRs differently. First, Program alias. A semantically equivalent program may have many syntactically different forms. As a result, if the training and testing data have a difference in their syntactic distributions of logic forms, a naive maximum likelihood estimation can suffer from this difference because it fails to 2 In (Kate et al., 2005; Liang et al., 2011; Guo et al., 2019), it was revealed that using an appropriate MR can substantially improve the performance of a semantic parser. capture the semantic equivalence (Bunel et al., 2018). As different MRs have different degrees of syntactic difference, they suffer from this problem differently. Second, Grammar rules. Grammar-based neural models can guarantee that the generated program is syntactically correct (Yin and Neubig, 2017; Wang et al., 2020; Sun et al., 2020). For a given set of logical forms in an MR, there exist multiple sets of grammar rules to model them. We observe that when the grammar-based neural m"
2020.emnlp-main.118,P18-1035,0,0.0263492,"005; Flickinger et al., 2017), and UCCA (Abend and Rappoport, 2013). Abend and Rappoport (2017) conducted a survey on ungrounded MRs to assess their achievements and shortcomings. Hershcovich et al. (2019) evaluated the similarities and divergences in the content encoded by ungrounded MRs and syntactic representation. Lin and Xue (2019) carried out a careful analysis on AMR and MRS to understand the factors contributing to the discrepancy in their parsing accuracy. Partly inspired by this line of work, we conduct a study on grounded MRs and investigate their impact on neural semantic parsing. Hershcovich et al. (2018) proposed to leverage annotated data in different ungrounded MRs to improve parsing performance. With U NIMER, we can explore whether it is feasible in grounded semantic parsing. Extrinsic parser evaluation. Another line of research that is closely related to our work is extrinsic parser evaluation. Miyao et al. (2008) benchmarked different syntactic parsers and their representations, including dependency parsing, phrase structure parsing, and deep parsing, and evaluated their impact on an information extraction system. Oepen et al. (2017) provided a flexible infrastructure, including data and"
2020.emnlp-main.118,N19-1047,0,0.0120104,"ng FunQL showed that topdown generation outperforms bottom-up in various settings. Our work aims to investigate the characteristics of different MRs and their impact on neural semantic parsing. Ungrounded semantic parsing. Except for the grounded MRs studied in this work, there are also ungrounded MRs that are not tied to any particular applications, such as AMR (Banarescu et al., 1527 2013), MRS (Copestake et al., 2005; Flickinger et al., 2017), and UCCA (Abend and Rappoport, 2013). Abend and Rappoport (2017) conducted a survey on ungrounded MRs to assess their achievements and shortcomings. Hershcovich et al. (2019) evaluated the similarities and divergences in the content encoded by ungrounded MRs and syntactic representation. Lin and Xue (2019) carried out a careful analysis on AMR and MRS to understand the factors contributing to the discrepancy in their parsing accuracy. Partly inspired by this line of work, we conduct a study on grounded MRs and investigate their impact on neural semantic parsing. Hershcovich et al. (2018) proposed to leverage annotated data in different ungrounded MRs to improve parsing performance. With U NIMER, we can explore whether it is feasible in grounded semantic parsing. E"
2020.emnlp-main.118,P17-1089,0,0.0195034,"he AllenNLP (Gardner et al., 2018) and PyTorch (Paszke et al., 2019) frameworks. To make a fair comparison, we tune the hyper-parameters of approaches for each MR on the development set or through cross-validation on the training set, with the NNI platform.6 Due to the limited number of test data in each domain, we run each approach five times and take the average number. Section A.2 in the supplementary material provides the search space of hyper-parameters for each approach and the preprocessing procedures of logical forms. Multiple neural semantic parsing approaches (Dong and Lapata, 2016; Iyer et al., 2017; Rabinovich et al., 2017) adopt the data anonymization techniques to replace entities in utterances with placeholders. However, the techniques are usually ad-hoc and specific for domains and MRs, and they sometimes require manual efforts to resolve conflicts (Finegan-Dollak et al., 2018). Hence, we do not apply data anonymization to avoid bias. 5 160 60 150 40 100 20 50 0 0 P L Geo F S 60 40 20 P L ATIS F S 0 P L Job F S Figure 2: Statistics of logical forms in the training set of Prolog (P), Lambda Calculus (L), FunQL (F) and SQL (S). The y-axis indicates the number of production rules in ea"
2020.emnlp-main.118,P16-1002,0,0.601865,"e shows exact-match accuracy whereas the bottom table shows execution-match accuracy. Most existing work focuses on evaluating only a small subset of dataset × MR pairs, leaving most of the table unexplored. A finer-grained table is available in the supplementary material (Table 8). Introduction Work done during an internship at Microsoft Research. In this paper, we focus on grounded semantic parsing, where meaning representations are grounded to specific knowl1 Geo 90.4 92.5 78.0 89.6 82.5 of neural networks techniques, significant improvements have been made in semantic parsing performance (Jia and Liang, 2016; Yin and Neubig, 2017; Dong and Lapata, 2018; Shaw et al., 2019). Despite the advancement in performance, we identify three important biases in existing work’s evaluation methodology. First, although multiple MRs are proposed, most existing work is evaluated on only one or two of them, leading to less comprehensive or even unfair comparisons. Table 1 shows the state-of-the-art performance of semantic parsing on different dataset × MR combinations, where the rows are the MRs and the columns are the datasets. We can observe that while Lambda Calculus is intensively studied, the other MRs have n"
2020.emnlp-main.118,W08-2105,0,0.0486147,"fferent MRs have different degrees of syntactic difference, they suffer from this problem differently. Second, Grammar rules. Grammar-based neural models can guarantee that the generated program is syntactically correct (Yin and Neubig, 2017; Wang et al., 2020; Sun et al., 2020). For a given set of logical forms in an MR, there exist multiple sets of grammar rules to model them. We observe that when the grammar-based neural model is trained with different sets of grammar rules, it exhibits a notable performance discrepancy. This finding alias with the one made in traditional semantic parsers (Kate, 2008) that properly transforming grammar rules can lead to better performance of a traditional semantic parser. In summary, this paper makes the following main contributions: • We propose U NIMER, a new unified benchmark on meaning representations, by integrating and completing semantic parsing datasets in three datasets × four MRs; we also implement six execution engines so that executionmatch accuracy can be evaluated in all cases; • We provide the baseline results for two widely used neural semantic parsing approaches on our benchmark, and we conduct an empirical study to understand the impact t"
2020.emnlp-main.118,D19-1603,0,0.0133929,"k on benchmarking MRs for grounded semantic parsing in neural approaches, to the best of our knowledge. Weakly supervised semantic parsing. In this paper, we focus on supervised learning for semantic parsing, where each utterance has its corresponding logical form annotated. But the similar evaluation methodology could be applied to weakly supervised semantic parsing, which receives wide attention because parsers are only supervised with execution results and annotated logical forms are no longer required (Berant et al., 2013; Pasupat and Liang, 2015; Goldman et al., 2018; Liang et al., 2018; Mueller et al., 2019). We also notice that various MRs have been used in weakly supervised semantic parsing, and it would be valuable to explore the impact of MRs in such settings. 7 Conclusion In this work, we propose U NIMER, a unified benchmark on meaning representations, based on established semantic parsing datasets; U NIMER covers three domains and four different meaning representations along with their execution engines. U NIMER allows researchers to comprehensively and fairly evaluate the performance of their approaches. Based on U NIMER, we conduct an empirical study to understand the characteristics of d"
2020.emnlp-main.118,D10-1119,0,0.323067,"Missing"
2020.emnlp-main.118,P15-1142,0,0.0196701,"f grammatical structure of natural language. There has not been work on benchmarking MRs for grounded semantic parsing in neural approaches, to the best of our knowledge. Weakly supervised semantic parsing. In this paper, we focus on supervised learning for semantic parsing, where each utterance has its corresponding logical form annotated. But the similar evaluation methodology could be applied to weakly supervised semantic parsing, which receives wide attention because parsers are only supervised with execution results and annotated logical forms are no longer required (Berant et al., 2013; Pasupat and Liang, 2015; Goldman et al., 2018; Liang et al., 2018; Mueller et al., 2019). We also notice that various MRs have been used in weakly supervised semantic parsing, and it would be valuable to explore the impact of MRs in such settings. 7 Conclusion In this work, we propose U NIMER, a unified benchmark on meaning representations, based on established semantic parsing datasets; U NIMER covers three domains and four different meaning representations along with their execution engines. U NIMER allows researchers to comprehensively and fairly evaluate the performance of their approaches. Based on U NIMER, we"
2020.emnlp-main.118,P17-1105,0,0.0178751,"r et al., 2018) and PyTorch (Paszke et al., 2019) frameworks. To make a fair comparison, we tune the hyper-parameters of approaches for each MR on the development set or through cross-validation on the training set, with the NNI platform.6 Due to the limited number of test data in each domain, we run each approach five times and take the average number. Section A.2 in the supplementary material provides the search space of hyper-parameters for each approach and the preprocessing procedures of logical forms. Multiple neural semantic parsing approaches (Dong and Lapata, 2016; Iyer et al., 2017; Rabinovich et al., 2017) adopt the data anonymization techniques to replace entities in utterances with placeholders. However, the techniques are usually ad-hoc and specific for domains and MRs, and they sometimes require manual efforts to resolve conflicts (Finegan-Dollak et al., 2018). Hence, we do not apply data anonymization to avoid bias. 5 160 60 150 40 100 20 50 0 0 P L Geo F S 60 40 20 P L ATIS F S 0 P L Job F S Figure 2: Statistics of logical forms in the training set of Prolog (P), Lambda Calculus (L), FunQL (F) and SQL (S). The y-axis indicates the number of production rules in each logical form. Shaw et a"
2020.emnlp-main.118,P19-1010,0,0.0765282,"ion-match accuracy. Most existing work focuses on evaluating only a small subset of dataset × MR pairs, leaving most of the table unexplored. A finer-grained table is available in the supplementary material (Table 8). Introduction Work done during an internship at Microsoft Research. In this paper, we focus on grounded semantic parsing, where meaning representations are grounded to specific knowl1 Geo 90.4 92.5 78.0 89.6 82.5 of neural networks techniques, significant improvements have been made in semantic parsing performance (Jia and Liang, 2016; Yin and Neubig, 2017; Dong and Lapata, 2018; Shaw et al., 2019). Despite the advancement in performance, we identify three important biases in existing work’s evaluation methodology. First, although multiple MRs are proposed, most existing work is evaluated on only one or two of them, leading to less comprehensive or even unfair comparisons. Table 1 shows the state-of-the-art performance of semantic parsing on different dataset × MR combinations, where the rows are the MRs and the columns are the datasets. We can observe that while Lambda Calculus is intensively studied, the other MRs have not been sufficiently studied. This biased evaluation is partly ca"
2020.emnlp-main.118,2020.acl-main.677,0,0.387008,"tic distributions of logic forms, a naive maximum likelihood estimation can suffer from this difference because it fails to 2 In (Kate et al., 2005; Liang et al., 2011; Guo et al., 2019), it was revealed that using an appropriate MR can substantially improve the performance of a semantic parser. capture the semantic equivalence (Bunel et al., 2018). As different MRs have different degrees of syntactic difference, they suffer from this problem differently. Second, Grammar rules. Grammar-based neural models can guarantee that the generated program is syntactically correct (Yin and Neubig, 2017; Wang et al., 2020; Sun et al., 2020). For a given set of logical forms in an MR, there exist multiple sets of grammar rules to model them. We observe that when the grammar-based neural model is trained with different sets of grammar rules, it exhibits a notable performance discrepancy. This finding alias with the one made in traditional semantic parsers (Kate, 2008) that properly transforming grammar rules can lead to better performance of a traditional semantic parser. In summary, this paper makes the following main contributions: • We propose U NIMER, a new unified benchmark on meaning representations, by in"
2020.emnlp-main.118,2020.tacl-1.13,0,0.0153112,"grammar-based models, and it is recommended to mention the used grammar rules in research papers clearly. 6 Related Work Meaning representations for semantic parsing. Recent work has shown that properly designing new MRs often helps improve the performance of neural semantic parsing. Except for the four MRs that we study, Liang et al. (2011); Liang (2013) presented DCS and lambda DCS for querying knowledge bases and demonstrated their advantages over Lambda Calculus. Guo et al. (2019) proposed SemQL, an MR for relational database queries, and they showed improvements in the Spider benchmark. Wolfson et al. (2020) designed an MR, named QDMR, for representing the meaning of questions through question decomposition. Instead of designing new MRs, Cheng et al. (2019) proposed a transition-based semantic parsing approach that supports generating tree-structured logical forms in either top-down or bottom-up manners. Their experimental results on various semantic parsing datasets using FunQL showed that topdown generation outperforms bottom-up in various settings. Our work aims to investigate the characteristics of different MRs and their impact on neural semantic parsing. Ungrounded semantic parsing. Except"
2020.emnlp-main.118,N06-1056,0,0.108517,"randomly sample one. We compare the execution results of the resulting logical forms to ensure their equivalence in semantics. Table 3 presents three transformation rules for SQL. We provide a detailed explanation of transformation rules and examples for each MR in Section A.3 of the supplementary material. Grammar Rules. To understand the grammar rules’ impact on grammar-based models, we provide two sets of grammar rules for each MR. Each set of rules can cover all the logical forms in the three domains. We compare the performance of models trained with different sets of rules. Specifically, Wong and Mooney (2006) and Wong and Mooney (2007) have induced a set of grammar rules for Prolog and FunQL in Geo. We directly use them in Geo and extend them to support logical forms in ATIS and Job. As for SQL, Bogin et al. (2019) have induced a set of rules for SQL in the Spider benchmark, and we adapt it to support the SQL queries in the three domains that we study. 4 Meaning Representation Comparison. To understand the impact of MRs on neural semantic 3 Our benchmark, execution engines and and our implementation can be found on: https://github.com/ JasperGuo/Unimer Description Shuffle expressions in Select, Fr"
2020.emnlp-main.118,D07-1071,0,0.0527482,"ve the problem, Zelle and Mooney (1996) designed a Prologstyle MR and annotated 880 (utterance, logical form) pairs. Popescu et al. (2003) and Kate et al. (2005) proposed to use SQL and FunQL to represent the meanings, respectively. Almost the same time, Zettlemoyer and Collins (2005) proposed to use Lambda Calculus and manually converted the Prolog logical forms to equivalent expressions in Lambda Calculus. Following their work, we adopt the standard 600/280 training/test split. ATIS is a dataset of flight booking questions. It consists of 5,418 questions and their corresponding SQL queries. Zettlemoyer and Collins (2007) proposed to use Lambda Calculus to represent the meanings of natural language and automatically map these SQL queries to its equivalent logical forms in Lambda Calculus. Following the work of Kwiatkowski et al. (2011), we use the standard 4480/480/450 training/dev/test split. Job is a dataset about job announcements posted in the newsgroup austin.jobs (Califf and Mooney, 1999). It consists of 640 utterances and their corresponding Prolog logical forms that query computerrelated job postings. Similarly, in Geo, Zettlemoyer 1523 and Collins (2005) proposed using Lambda Calculus and manually con"
2020.emnlp-main.118,D14-1135,0,0.053918,"Missing"
2020.emnlp-main.118,N15-1162,0,0.0235364,"Missing"
2020.emnlp-main.142,N15-1029,0,0.0367516,"Missing"
2020.emnlp-main.142,N09-2028,0,0.107911,"Missing"
2020.emnlp-main.142,Q14-1011,0,0.0193313,"adapt to the idiosyncrasies of the target disfluency detection data. 3 Experiment 3.1 Settings Dataset. English Switchboard (SWBD) (Godfrey et al., 1992) is the standard and largest (1.73 × 105 sentences for training ) corpus used for disfluency detection. We use English Switchboard as main data. Following the experiment settings in Charniak and Johnson (2001), we split the Switchboard corpus into train, dev and test set as follows: train data consists of all sw[23]∗.dff files, dev data consists of all sw4[5, 6, 7, 8, 9]∗.dff files and test data consists of all sw4[0, 1]∗.dff files. Following Honnibal and Johnson (2014), we lower-case the text and remove all punctuations and partial words.2 We also discard the ‘um’ and ‘uh’ tokens and merge ‘you know’ and ‘i mean’ into single tokens. In addition to Switchboard, we test our models on three out-of-domain publicly available datasets annotated with disfluencies (Zayats et al., 2014; Zayats and Ostendorf, 2018): • CallHome: phone conversations between family members and close friends; • SCOTUS: transcribed Supreme Court oral arguments between justices and advocates; • FCIC: two transcribed hearings from Financial Crisis Inquiry Commission. Corpora SWBD SCOTUS Cal"
2020.emnlp-main.142,N01-1016,0,0.0769255,"d of fine-tuning the ELECTRA model. Although the difference of distribution between our pseudo data and the golden disfluency detection data limits the performance of teacher model, this stage converges faster than fine-tuning the ELECTRA model as it only needs to adapt to the idiosyncrasies of the target disfluency detection data. 3 Experiment 3.1 Settings Dataset. English Switchboard (SWBD) (Godfrey et al., 1992) is the standard and largest (1.73 × 105 sentences for training ) corpus used for disfluency detection. We use English Switchboard as main data. Following the experiment settings in Charniak and Johnson (2001), we split the Switchboard corpus into train, dev and test set as follows: train data consists of all sw[23]∗.dff files, dev data consists of all sw4[5, 6, 7, 8, 9]∗.dff files and test data consists of all sw4[0, 1]∗.dff files. Following Honnibal and Johnson (2014), we lower-case the text and remove all punctuations and partial words.2 We also discard the ‘um’ and ‘uh’ tokens and merge ‘you know’ and ‘i mean’ into single tokens. In addition to Switchboard, we test our models on three out-of-domain publicly available datasets annotated with disfluencies (Zayats et al., 2014; Zayats and Ostendor"
2020.emnlp-main.142,D18-1490,0,0.0142884,"nding. IM Figure 1: A sentence from the English Switchboard corpus with disfluencies annotated. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. corrections. Table 1 gives a few examples. Interregnums are relatively easier to detect as they are often fixed phrases, e.g. “uh”, “you know”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner, relying on nothing but unlabeled text corpora"
2020.emnlp-main.142,N19-1282,0,0.0901314,"Missing"
2020.emnlp-main.142,N19-1423,0,0.0429222,"ing domain, selfsupervised research mainly focus on word embedding (Mikolov et al., 2013a,b) and language model learning (Bengio et al., 2003; Peters et al., 2018; Radford et al., 2018). For word embedding learning, the idea is to train a model that maps each word to a feature vector, such that it is easy to predict the words in the context given the vector. This converts an apparently unsupervised problem into a “self-supervised” one: learning a function from a given word to the words surrounding it. Language model pre-training (Bengio et al., 2003; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019) is another line of self-supervised learning task. A trained language model learns a function to predict the likelihood of occurrence of a word based on the surrounding sequence of words used in the text. There are mainly two existing strategies for applying pre-trained language representations to down-stream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT)"
2020.emnlp-main.142,P17-2087,0,0.128012,"e types: restarts, repetitions and *Email corresponding. IM Figure 1: A sentence from the English Switchboard corpus with disfluencies annotated. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. corrections. Table 1 gives a few examples. Interregnums are relatively easier to detect as they are often fixed phrases, e.g. “uh”, “you know”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner,"
2020.emnlp-main.142,2020.acl-main.346,0,0.0699336,"-based 1819 model requires large annotated tree-banks that contain both disfluencies and syntactic structures. All of the above works heavily rely on humanannotated data. There exist a limited effort to tackle the training data bottleneck. Wang et al. (2018) and Dong et al. (2019) use an autoencoder method to help for disfluency detection by jointly training the autoencoder model and disfluency detection model. Wang et al. (2019) use self-supervised learning to tackle the training data bottleneck. Their selfsupervised method can substantially reduce the need for human-annotated training data. Lou and Johnson (2020) shows that self-training and ensembling are effective methods for improving disfluency detection. These semi-supervised methods achieve higher performance by introducing pseudo training sentences. However, the performance still relies on human-annotated data. We explore unsupervised disfluency detection, taking inspiration from the success of self-supervised learning and self-training on disfluency detection. Self-Supervised Representation Learning Self-supervised learning aims to train a network on an auxiliary task where ground-truth is obtained automatically. Over the last few years, many"
2020.emnlp-main.142,N06-1020,0,0.121635,"ations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018) and BERT (Devlin et al., 2019), introduces minimal task-specific parameters and is trained on the downstream tasks by simply fine-tuning the pre-trained parameters. Motivated by the success of self-supervised learning, we use self-supervised learning method to train a weak disfluency detection model as teacher model. We also train a sentence grammaticality judgment model to help select sentences with highquality pseudo labels. Self-Training Self-training (McClosky et al., 2006) first uses labeled data to train a good teacher model, then use the teacher model to label unlabeled data and finally use the labeled data and unlabeled data to jointly train a student model. Self-training has also been shown to work well for a variety of tasks including leveraging noisy data (Veit et al., 2017), semantic segmentation (Babakhin et al., 2019), text classification (Li et al., 2019). Xie et al. (2019) present Noisy Student Training, which extends the idea of self-training with the use of equal-or-larger student models and noise added to the student during learning. Our model bui"
2020.emnlp-main.142,N18-1202,0,0.194956,"tuning ELECTRA-Small fine-tuning ELECTRA-Base fine-tuning Teacher fine-tuning Unsupervised teacher Our unsupervised P R F1 91.9 92.2 91.6 92.9 92.5 86.8 90.2 85.1 89.8 89.5 91.2 92.1 62.0 89.1 88.4 90.9 90.5 92.0 92.3 72.3 89.6 Method Unsupervised teacher ELECTRA-Base Teacher fine-tuning Pattern-match Our unsupervised Table 3: Experiment results on the Switchboard dev set. “ ∗ fine-tuning” means “ fine-tuning ∗ model” on the Switchboard train set. The first part (from row 1 to row 5) is the supervised method using complicated hand-crafted features or contextualized word embeddings (e.g. ELMo (Peters et al., 2018) and ELECTRA), the second part (row 6 to 7) is the unsupervised methods. Method UBT (Wu et al., 2015) Bi-LSTM (Zayats et al., 2016) NCM (Lou and Johnson, 2017) Transition-based (Wang et al., 2017) Self-supervised(Wang et al., 2019) Self-training(Lou and Johnson, 2020) EGBC(Bach and Huang, 2019) Our Method P R F1 90.3 91.8 91.1 93.4 87.5 95.7 88.2 80.5 80.6 84.1 87.3 93.8 88.3 87.8 85.1 85.9 86.8 87.5 90.2 90.6 91.8 88.0 Table 4: Comparison with previous state-of-the-art methods on the Switchboard test set. The first part (from row 1 to row 4) is the methods without using contextualized word em"
2020.emnlp-main.142,N13-1102,0,0.0591073,"Missing"
2020.emnlp-main.142,D13-1013,0,0.0517005,"Missing"
2020.emnlp-main.142,C18-1299,0,0.0605723,"”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner, relying on nothing but unlabeled text corpora. Our model builds upon the recent work on Noisy Student Training (Xie et al., 2019), a semisupervised learning approach based on the idea of 1813 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1813–1822, c November 16–20, 2020. 2020 Association for Computational Linguistics Algorithm"
2020.emnlp-main.142,C16-1027,1,0.891168,"sing contextualized word embeddings (e.g. BERT and ELECTRA). 1 Type repair repair repetition restart RP Annotation [ the + they ’re ] voice activated [ we want + {well} in our area we want ] to [we got + {uh} we got ] to talking [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Automatic speech recognition (ASR) outputs often contain various disfluencies, which is a characteristic of spontaneous speech and create barriers to subsequent text processing tasks like parsing, machine translation, and summarization. Disfluency detection (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015) focuses on recognizing the disfluencies from ASR outputs. As shown in Figure 1, a standard annotation of the disfluency structure indicates the reparandum (words that the speaker intends to discard), the interruption point (denoted as ‘+’, marking the end of the reparandum), an optional interregnum (filled pauses, discourse cue words, etc.) and the associated repair (Shriberg, 1994). Ignoring the interregnum, disfluencies are categorized into three types: restarts, repetitions and *Email corresponding. IM Figure 1: A sentence from the English Switchboard corpus with disfluen"
2020.emnlp-main.142,D17-1296,1,0.904698,"titions and *Email corresponding. IM Figure 1: A sentence from the English Switchboard corpus with disfluencies annotated. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. corrections. Table 1 gives a few examples. Interregnums are relatively easier to detect as they are often fixed phrases, e.g. “uh”, “you know”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner, relying on nothing"
2020.emnlp-main.142,P15-1048,0,0.248866,"word embeddings (e.g. BERT and ELECTRA). 1 Type repair repair repetition restart RP Annotation [ the + they ’re ] voice activated [ we want + {well} in our area we want ] to [we got + {uh} we got ] to talking [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Automatic speech recognition (ASR) outputs often contain various disfluencies, which is a characteristic of spontaneous speech and create barriers to subsequent text processing tasks like parsing, machine translation, and summarization. Disfluency detection (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015) focuses on recognizing the disfluencies from ASR outputs. As shown in Figure 1, a standard annotation of the disfluency structure indicates the reparandum (words that the speaker intends to discard), the interruption point (denoted as ‘+’, marking the end of the reparandum), an optional interregnum (filled pauses, discourse cue words, etc.) and the associated repair (Shriberg, 1994). Ignoring the interregnum, disfluencies are categorized into three types: restarts, repetitions and *Email corresponding. IM Figure 1: A sentence from the English Switchboard corpus with disfluencies annotated. RM"
2020.emnlp-main.142,D16-1109,0,0.0287976,"Missing"
2020.emnlp-main.142,N19-1008,0,0.0420599,": A sentence from the English Switchboard corpus with disfluencies annotated. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. corrections. Table 1 gives a few examples. Interregnums are relatively easier to detect as they are often fixed phrases, e.g. “uh”, “you know”. On the other hand, reparandums are more difficult to detect in that they are in free form. As a result, most previous disfluency detection work focuses on detecting reparandums. Most work (Zayats and Ostendorf, 2018; Lou and Johnson, 2017; Wang et al., 2017; Jamshid Lou et al., 2018; Zayats and Ostendorf, 2019) on disfluency detection heavily relies on human-annotated corpora, which is scarce and expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning (Wang et al., 2019) and semi-supervised learning techniques (Wang et al., 2018), but they still require human-annotated corpora. In this work, we completely remove the need of humanannotated corpora and propose a novel method to train a disfluency detection system in a completely unsupervised manner, relying on nothing but unlabeled text corpora. Our model builds upon the r"
2020.emnlp-main.142,C10-1154,0,0.0533282,"Missing"
2020.emnlp-main.225,D17-1218,0,0.417693,"stablish relationships among them in an argumentative text. Typical tasks in this line include argumentative zoning (Teufel et al., 1999) , argumentation mining (Mochales and Moens, 2011; Lippi and Torroni, 2016) and analyzing argumentative student essays (Burstein et al., 2003; Stab and Gurevych, 2014). Argumentative zoning identifies arguments in scientific articles (Teufel et al., 1999; Guo et al., 2010). Argumentation mining aims to identify argument components and relations from legal texts (Palau and Moens, 2009; Mochales and Moens, 2011) or argumentative texts (Stab and Gurevych, 2014; Daxenberger et al., 2017). The solutions to these tasks usually adopt similar machine learning methods but use domain related features. The methods could be roughly classified into the following categories. Classification based methods cast DEI as a classification problem. Various classifiers have been tested, such as SVM (Stab and Gurevych, 2014), decision trees (Burstein et al., 2003, 2001) and naive Bayes, maximum entropy model (Moens et al., 2007; Palau and Moens, 2009). Sequence labeling based methods exploit contextual information for DEI with conditional random fields (Hirohata et al., 2008; Song et al., 2015)"
2020.emnlp-main.225,P17-1002,0,0.0201117,"alifying essay organization (Persing et al., 2010), argumentation (Persing and Ng, 2016; Wachsmuth et al., 2016) and general writing (Burstein et al., 2003; Ong et al., 2014; Song et al., 2014). The major feature classes proposed by Burstein et al. (2003) and Stab and Gurevych (2014) are used to build a baseline. The features include: position, cue words, lexical features (main verbs, adverbs and connectives) and structural features (such as number of clauses). Some of these features are based on manually collected lexicons. Deep Learning Methods have achieved great success in many NLP tasks. Eger et al. (2017) proposed neural argumentation mining models based on sequence tagging or dependency parsing. It exploits inter-sentence relations but needs sophisticated language processing. Daxenberger et al. (2017) exploited CNN and LSTM for classifying sentences to identify claims from different domains. It mainly depends on the content of components but does not sufficiently model positions and exploit inter-sentence relatedness. 2821 2.2 Attention Mechanism for Discourse Representation 3.1 Attention mechanism was first introduced by (Bahdanau et al., 2015) in the encoder-decoder framework. Attention has"
2020.emnlp-main.225,P19-1062,0,0.0211671,"classifying sentences to identify claims from different domains. It mainly depends on the content of components but does not sufficiently model positions and exploit inter-sentence relatedness. 2821 2.2 Attention Mechanism for Discourse Representation 3.1 Attention mechanism was first introduced by (Bahdanau et al., 2015) in the encoder-decoder framework. Attention has the ability to learn important regions within a context and has been widely adopted in deep learning. Liu and Lapata (2018) proposed a structured attention mechanism to derive a tree over a text, akin to an RST discourse tree. Ferracane et al. (2019) evaluated the model, however, found multiple negative results. Attention mechanism has also been applied for RST parsing and its applications (Li et al., 2016; Ji and Smith, 2017; Huber and Carenini, 2019) but it is mostly used for capturing local semantic interactions. 2.3 Self-Attention Mechanism Vaswani et al. (2017) proposed the self-attention mechanism and achieved state of the art results in many NLP tasks. Since then, self-attention has drawn increasing interests due to flexibility in modeling long range interactions. Self-attention ignores word order in a sentence. As a result, positi"
2020.emnlp-main.225,W10-1913,0,0.0399295,"e elements as well. 2 2.1 Related Work Discourse Element Identification DEI could be seen as a subtask in discourse structure analysis. It aims to identify discourse elements, determine their functions and establish relationships among them in an argumentative text. Typical tasks in this line include argumentative zoning (Teufel et al., 1999) , argumentation mining (Mochales and Moens, 2011; Lippi and Torroni, 2016) and analyzing argumentative student essays (Burstein et al., 2003; Stab and Gurevych, 2014). Argumentative zoning identifies arguments in scientific articles (Teufel et al., 1999; Guo et al., 2010). Argumentation mining aims to identify argument components and relations from legal texts (Palau and Moens, 2009; Mochales and Moens, 2011) or argumentative texts (Stab and Gurevych, 2014; Daxenberger et al., 2017). The solutions to these tasks usually adopt similar machine learning methods but use domain related features. The methods could be roughly classified into the following categories. Classification based methods cast DEI as a classification problem. Various classifiers have been tested, such as SVM (Stab and Gurevych, 2014), decision trees (Burstein et al., 2003, 2001) and naive Baye"
2020.emnlp-main.225,N04-1024,0,0.146624,"s in Natural Language Processing, pages 2820–2830, c November 16–20, 2020. 2020 Association for Computational Linguistics relative position in its essay, relative position of its paragraph in its essay, and its relative position within its paragraph. On the other hand, relatedness among sentences may also indicate properties of discourse elements. For example, thesis sentences should have close relations to the whole essay; main ideas usually locate in similar positions and have high relatedness. Relatedness between discourse elements has shown to be an important indicator of essay coherence (Higgins et al., 2004). We compute inter-sentence attention vectors to represent either element-wise or content-wise relations to other sentences, which bring in additional information beyond individual sentences and enhance sentence representation without extra information. Experiments show that the proposed approach can get considerable improvements compared with feature-based and neural network based baselines on a Chinese dataset and obtain competitive results compared with the state-of-the-art method on an English dataset. The structural positional encodings of sentences show effectiveness to achieve obvious o"
2020.emnlp-main.225,I08-1050,0,0.0477359,"Gurevych, 2014; Daxenberger et al., 2017). The solutions to these tasks usually adopt similar machine learning methods but use domain related features. The methods could be roughly classified into the following categories. Classification based methods cast DEI as a classification problem. Various classifiers have been tested, such as SVM (Stab and Gurevych, 2014), decision trees (Burstein et al., 2003, 2001) and naive Bayes, maximum entropy model (Moens et al., 2007; Palau and Moens, 2009). Sequence labeling based methods exploit contextual information for DEI with conditional random fields (Hirohata et al., 2008; Song et al., 2015) or recurrent neural networks (Daxenberger et al., 2017). Establishing relations between sentences is often viewed as a classification tasks as well (Stab and Gurevych, 2014). Parsing based methods are also adopted to build more complex structures with techniques like ILP (Stab and Gurevych, 2017) or RST style parsing (Peldszus and Stede, 2015). Feature engineering. Some common features are shared across these tasks, including syntactic, lexical, semantic and discourse relations. There are also domain related features to further boost the performance. Mochales and Moens (20"
2020.emnlp-main.225,D19-1235,0,0.0145362,"Attention Mechanism for Discourse Representation 3.1 Attention mechanism was first introduced by (Bahdanau et al., 2015) in the encoder-decoder framework. Attention has the ability to learn important regions within a context and has been widely adopted in deep learning. Liu and Lapata (2018) proposed a structured attention mechanism to derive a tree over a text, akin to an RST discourse tree. Ferracane et al. (2019) evaluated the model, however, found multiple negative results. Attention mechanism has also been applied for RST parsing and its applications (Li et al., 2016; Ji and Smith, 2017; Huber and Carenini, 2019) but it is mostly used for capturing local semantic interactions. 2.3 Self-Attention Mechanism Vaswani et al. (2017) proposed the self-attention mechanism and achieved state of the art results in many NLP tasks. Since then, self-attention has drawn increasing interests due to flexibility in modeling long range interactions. Self-attention ignores word order in a sentence. As a result, position representations are developed to cooperate with self-attention. In addition to the sinusoidal position representation proposed by Vaswani et al. (2017), there are also other variations to bias the select"
2020.emnlp-main.225,P17-1092,0,0.0227456,"latedness. 2821 2.2 Attention Mechanism for Discourse Representation 3.1 Attention mechanism was first introduced by (Bahdanau et al., 2015) in the encoder-decoder framework. Attention has the ability to learn important regions within a context and has been widely adopted in deep learning. Liu and Lapata (2018) proposed a structured attention mechanism to derive a tree over a text, akin to an RST discourse tree. Ferracane et al. (2019) evaluated the model, however, found multiple negative results. Attention mechanism has also been applied for RST parsing and its applications (Li et al., 2016; Ji and Smith, 2017; Huber and Carenini, 2019) but it is mostly used for capturing local semantic interactions. 2.3 Self-Attention Mechanism Vaswani et al. (2017) proposed the self-attention mechanism and achieved state of the art results in many NLP tasks. Since then, self-attention has drawn increasing interests due to flexibility in modeling long range interactions. Self-attention ignores word order in a sentence. As a result, position representations are developed to cooperate with self-attention. In addition to the sinusoidal position representation proposed by Vaswani et al. (2017), there are also other va"
2020.emnlp-main.225,D16-1035,0,0.0118952,"inter-sentence relatedness. 2821 2.2 Attention Mechanism for Discourse Representation 3.1 Attention mechanism was first introduced by (Bahdanau et al., 2015) in the encoder-decoder framework. Attention has the ability to learn important regions within a context and has been widely adopted in deep learning. Liu and Lapata (2018) proposed a structured attention mechanism to derive a tree over a text, akin to an RST discourse tree. Ferracane et al. (2019) evaluated the model, however, found multiple negative results. Attention mechanism has also been applied for RST parsing and its applications (Li et al., 2016; Ji and Smith, 2017; Huber and Carenini, 2019) but it is mostly used for capturing local semantic interactions. 2.3 Self-Attention Mechanism Vaswani et al. (2017) proposed the self-attention mechanism and achieved state of the art results in many NLP tasks. Since then, self-attention has drawn increasing interests due to flexibility in modeling long range interactions. Self-attention ignores word order in a sentence. As a result, position representations are developed to cooperate with self-attention. In addition to the sinusoidal position representation proposed by Vaswani et al. (2017), the"
2020.emnlp-main.225,D10-1023,0,0.279813,"elements represent the function and contribution of every discourse unit to the discourse. Burstein et al. (2003) formulate discourse elements as 5 categories: introduction, thesis, main idea, supporting and conclusion, while argument components such as major claim, claim and premise are used as discourse elements in argumentation structure parsing in persuasive essays (Stab and Gurevych, 2014). DEI can benefit automated essay scoring in many aspects: modeling organization, inferring topics and opinions or used as features for scoring systems (Attali and Burstein, 2006; Burstein et al., 2001; Persing et al., 2010; Song et al., 2020). Despite its importance, DEI is challenging. First, the ambiguity of sentences makes learning models difficult to distinguish some discourse expressing the central claim of the author and the main ideas support the thesis from specific aspects. However, it is hard to distinguish them from their content and style. Second, the discourse element of a specific sentence depends on context. As a result, considering individual sentences only would have difficulties in identifying discourse elements. The relations and relatedness among multiple sentences should be explored. Third,"
2020.emnlp-main.225,N16-1164,0,0.0155324,"c, lexical, semantic and discourse relations. There are also domain related features to further boost the performance. Mochales and Moens (2011) designed special features for argumentation mining in legal texts. Nguyen and Litman (2015) identified claims based on domain words. Lippi and Torroni (2015) modeled syntactic structures for content independent claim detection based on tree kernels. Our work is mostly related to DEI in argumentative student essays (Burstein et al., 2003; Stab and Gurevych, 2014), which is useful for qualifying essay organization (Persing et al., 2010), argumentation (Persing and Ng, 2016; Wachsmuth et al., 2016) and general writing (Burstein et al., 2003; Ong et al., 2014; Song et al., 2014). The major feature classes proposed by Burstein et al. (2003) and Stab and Gurevych (2014) are used to build a baseline. The features include: position, cue words, lexical features (main verbs, adverbs and connectives) and structural features (such as number of clauses). Some of these features are based on manually collected lexicons. Deep Learning Methods have achieved great success in many NLP tasks. Eger et al. (2017) proposed neural argumentation mining models based on sequence taggin"
2020.emnlp-main.225,Q18-1005,0,0.0224595,"ts inter-sentence relations but needs sophisticated language processing. Daxenberger et al. (2017) exploited CNN and LSTM for classifying sentences to identify claims from different domains. It mainly depends on the content of components but does not sufficiently model positions and exploit inter-sentence relatedness. 2821 2.2 Attention Mechanism for Discourse Representation 3.1 Attention mechanism was first introduced by (Bahdanau et al., 2015) in the encoder-decoder framework. Attention has the ability to learn important regions within a context and has been widely adopted in deep learning. Liu and Lapata (2018) proposed a structured attention mechanism to derive a tree over a text, akin to an RST discourse tree. Ferracane et al. (2019) evaluated the model, however, found multiple negative results. Attention mechanism has also been applied for RST parsing and its applications (Li et al., 2016; Ji and Smith, 2017; Huber and Carenini, 2019) but it is mostly used for capturing local semantic interactions. 2.3 Self-Attention Mechanism Vaswani et al. (2017) proposed the self-attention mechanism and achieved state of the art results in many NLP tasks. Since then, self-attention has drawn increasing interes"
2020.emnlp-main.225,D19-1257,0,0.0211114,"e art results in many NLP tasks. Since then, self-attention has drawn increasing interests due to flexibility in modeling long range interactions. Self-attention ignores word order in a sentence. As a result, position representations are developed to cooperate with self-attention. In addition to the sinusoidal position representation proposed by Vaswani et al. (2017), there are also other variations to bias the selection of attentive regions (Shen et al., 2018; Shaw et al., 2018; Yang et al., 2019). In NLP, self-attention is mostly applied to sequential structures such as a sequence of words. Mihaylov and Frank (2019) proposed a discourse-aware selfattention encoder for reading comprehension on narrative texts, where event chains, discourse relations and coreference relations are used for connecting sentences. Self-attention can be also extended to 2d-dimensions for image processing (Parmar et al., 2018) and lattice inputs (Sperber et al., 2019). 3 Baseline We use Hierarchical BiLSTM (HBiLSTM), which is similar to (Yang et al., 2016), as the base model to model sentence and discourse level representations. The task is to assign discourse element labels y = (y1 , ..., yn ) to sentences (x1 , ..., xn ) in a"
2020.emnlp-main.225,W15-0503,0,0.0251721,"2017). Establishing relations between sentences is often viewed as a classification tasks as well (Stab and Gurevych, 2014). Parsing based methods are also adopted to build more complex structures with techniques like ILP (Stab and Gurevych, 2017) or RST style parsing (Peldszus and Stede, 2015). Feature engineering. Some common features are shared across these tasks, including syntactic, lexical, semantic and discourse relations. There are also domain related features to further boost the performance. Mochales and Moens (2011) designed special features for argumentation mining in legal texts. Nguyen and Litman (2015) identified claims based on domain words. Lippi and Torroni (2015) modeled syntactic structures for content independent claim detection based on tree kernels. Our work is mostly related to DEI in argumentative student essays (Burstein et al., 2003; Stab and Gurevych, 2014), which is useful for qualifying essay organization (Persing et al., 2010), argumentation (Persing and Ng, 2016; Wachsmuth et al., 2016) and general writing (Burstein et al., 2003; Ong et al., 2014; Song et al., 2014). The major feature classes proposed by Burstein et al. (2003) and Stab and Gurevych (2014) are used to build"
2020.emnlp-main.225,W14-2104,0,0.0120183,"rther boost the performance. Mochales and Moens (2011) designed special features for argumentation mining in legal texts. Nguyen and Litman (2015) identified claims based on domain words. Lippi and Torroni (2015) modeled syntactic structures for content independent claim detection based on tree kernels. Our work is mostly related to DEI in argumentative student essays (Burstein et al., 2003; Stab and Gurevych, 2014), which is useful for qualifying essay organization (Persing et al., 2010), argumentation (Persing and Ng, 2016; Wachsmuth et al., 2016) and general writing (Burstein et al., 2003; Ong et al., 2014; Song et al., 2014). The major feature classes proposed by Burstein et al. (2003) and Stab and Gurevych (2014) are used to build a baseline. The features include: position, cue words, lexical features (main verbs, adverbs and connectives) and structural features (such as number of clauses). Some of these features are based on manually collected lexicons. Deep Learning Methods have achieved great success in many NLP tasks. Eger et al. (2017) proposed neural argumentation mining models based on sequence tagging or dependency parsing. It exploits inter-sentence relations but needs sophisticated"
2020.emnlp-main.225,N18-2074,0,0.0239108,"mantic interactions. 2.3 Self-Attention Mechanism Vaswani et al. (2017) proposed the self-attention mechanism and achieved state of the art results in many NLP tasks. Since then, self-attention has drawn increasing interests due to flexibility in modeling long range interactions. Self-attention ignores word order in a sentence. As a result, position representations are developed to cooperate with self-attention. In addition to the sinusoidal position representation proposed by Vaswani et al. (2017), there are also other variations to bias the selection of attentive regions (Shen et al., 2018; Shaw et al., 2018; Yang et al., 2019). In NLP, self-attention is mostly applied to sequential structures such as a sequence of words. Mihaylov and Frank (2019) proposed a discourse-aware selfattention encoder for reading comprehension on narrative texts, where event chains, discourse relations and coreference relations are used for connecting sentences. Self-attention can be also extended to 2d-dimensions for image processing (Parmar et al., 2018) and lattice inputs (Sperber et al., 2019). 3 Baseline We use Hierarchical BiLSTM (HBiLSTM), which is similar to (Yang et al., 2016), as the base model to model sente"
2020.emnlp-main.225,D15-1270,1,0.833002,"Missing"
2020.emnlp-main.225,N18-2028,0,0.0609791,"Missing"
2020.emnlp-main.225,W14-2110,0,0.0257872,"rformance. Mochales and Moens (2011) designed special features for argumentation mining in legal texts. Nguyen and Litman (2015) identified claims based on domain words. Lippi and Torroni (2015) modeled syntactic structures for content independent claim detection based on tree kernels. Our work is mostly related to DEI in argumentative student essays (Burstein et al., 2003; Stab and Gurevych, 2014), which is useful for qualifying essay organization (Persing et al., 2010), argumentation (Persing and Ng, 2016; Wachsmuth et al., 2016) and general writing (Burstein et al., 2003; Ong et al., 2014; Song et al., 2014). The major feature classes proposed by Burstein et al. (2003) and Stab and Gurevych (2014) are used to build a baseline. The features include: position, cue words, lexical features (main verbs, adverbs and connectives) and structural features (such as number of clauses). Some of these features are based on manually collected lexicons. Deep Learning Methods have achieved great success in many NLP tasks. Eger et al. (2017) proposed neural argumentation mining models based on sequence tagging or dependency parsing. It exploits inter-sentence relations but needs sophisticated language processing."
2020.emnlp-main.225,P19-1115,0,0.024193,"oposed by Vaswani et al. (2017), there are also other variations to bias the selection of attentive regions (Shen et al., 2018; Shaw et al., 2018; Yang et al., 2019). In NLP, self-attention is mostly applied to sequential structures such as a sequence of words. Mihaylov and Frank (2019) proposed a discourse-aware selfattention encoder for reading comprehension on narrative texts, where event chains, discourse relations and coreference relations are used for connecting sentences. Self-attention can be also extended to 2d-dimensions for image processing (Parmar et al., 2018) and lattice inputs (Sperber et al., 2019). 3 Baseline We use Hierarchical BiLSTM (HBiLSTM), which is similar to (Yang et al., 2016), as the base model to model sentence and discourse level representations. The task is to assign discourse element labels y = (y1 , ..., yn ) to sentences (x1 , ..., xn ) in a text, where xi , 1 ≤ i ≤ n, is a sentence of a sequence of words and yi ∈ Y, Y is a set of pre-defined discourse elements. Sentence Representation Layer A sequence of words x = {w1 , ..., wN } is modeled with a RNN encoder and is converted into a sequence of hidden states H = {h1 , ..., hN }. The hidden state at the i-th step is hi"
2020.emnlp-main.225,D15-1110,0,0.0635297,"014), decision trees (Burstein et al., 2003, 2001) and naive Bayes, maximum entropy model (Moens et al., 2007; Palau and Moens, 2009). Sequence labeling based methods exploit contextual information for DEI with conditional random fields (Hirohata et al., 2008; Song et al., 2015) or recurrent neural networks (Daxenberger et al., 2017). Establishing relations between sentences is often viewed as a classification tasks as well (Stab and Gurevych, 2014). Parsing based methods are also adopted to build more complex structures with techniques like ILP (Stab and Gurevych, 2017) or RST style parsing (Peldszus and Stede, 2015). Feature engineering. Some common features are shared across these tasks, including syntactic, lexical, semantic and discourse relations. There are also domain related features to further boost the performance. Mochales and Moens (2011) designed special features for argumentation mining in legal texts. Nguyen and Litman (2015) identified claims based on domain words. Lippi and Torroni (2015) modeled syntactic structures for content independent claim detection based on tree kernels. Our work is mostly related to DEI in argumentative student essays (Burstein et al., 2003; Stab and Gurevych, 201"
2020.emnlp-main.225,D14-1006,0,0.578645,"esentation for identifying discourse elements. 1 Introduction Discourse describes how a document is organized. This paper focuses on the task of discourse element identification (DEI) in argumentative student essays. Discourse elements represent the function and contribution of every discourse unit to the discourse. Burstein et al. (2003) formulate discourse elements as 5 categories: introduction, thesis, main idea, supporting and conclusion, while argument components such as major claim, claim and premise are used as discourse elements in argumentation structure parsing in persuasive essays (Stab and Gurevych, 2014). DEI can benefit automated essay scoring in many aspects: modeling organization, inferring topics and opinions or used as features for scoring systems (Attali and Burstein, 2006; Burstein et al., 2001; Persing et al., 2010; Song et al., 2020). Despite its importance, DEI is challenging. First, the ambiguity of sentences makes learning models difficult to distinguish some discourse expressing the central claim of the author and the main ideas support the thesis from specific aspects. However, it is hard to distinguish them from their content and style. Second, the discourse element of a specif"
2020.emnlp-main.225,J17-3005,0,0.266452,"been tested, such as SVM (Stab and Gurevych, 2014), decision trees (Burstein et al., 2003, 2001) and naive Bayes, maximum entropy model (Moens et al., 2007; Palau and Moens, 2009). Sequence labeling based methods exploit contextual information for DEI with conditional random fields (Hirohata et al., 2008; Song et al., 2015) or recurrent neural networks (Daxenberger et al., 2017). Establishing relations between sentences is often viewed as a classification tasks as well (Stab and Gurevych, 2014). Parsing based methods are also adopted to build more complex structures with techniques like ILP (Stab and Gurevych, 2017) or RST style parsing (Peldszus and Stede, 2015). Feature engineering. Some common features are shared across these tasks, including syntactic, lexical, semantic and discourse relations. There are also domain related features to further boost the performance. Mochales and Moens (2011) designed special features for argumentation mining in legal texts. Nguyen and Litman (2015) identified claims based on domain words. Lippi and Torroni (2015) modeled syntactic structures for content independent claim detection based on tree kernels. Our work is mostly related to DEI in argumentative student essay"
2020.emnlp-main.225,N19-1407,0,0.0251712,". 2.3 Self-Attention Mechanism Vaswani et al. (2017) proposed the self-attention mechanism and achieved state of the art results in many NLP tasks. Since then, self-attention has drawn increasing interests due to flexibility in modeling long range interactions. Self-attention ignores word order in a sentence. As a result, position representations are developed to cooperate with self-attention. In addition to the sinusoidal position representation proposed by Vaswani et al. (2017), there are also other variations to bias the selection of attentive regions (Shen et al., 2018; Shaw et al., 2018; Yang et al., 2019). In NLP, self-attention is mostly applied to sequential structures such as a sequence of words. Mihaylov and Frank (2019) proposed a discourse-aware selfattention encoder for reading comprehension on narrative texts, where event chains, discourse relations and coreference relations are used for connecting sentences. Self-attention can be also extended to 2d-dimensions for image processing (Parmar et al., 2018) and lattice inputs (Sperber et al., 2019). 3 Baseline We use Hierarchical BiLSTM (HBiLSTM), which is similar to (Yang et al., 2016), as the base model to model sentence and discourse le"
2020.emnlp-main.225,C16-1158,0,0.0121763,"nd discourse relations. There are also domain related features to further boost the performance. Mochales and Moens (2011) designed special features for argumentation mining in legal texts. Nguyen and Litman (2015) identified claims based on domain words. Lippi and Torroni (2015) modeled syntactic structures for content independent claim detection based on tree kernels. Our work is mostly related to DEI in argumentative student essays (Burstein et al., 2003; Stab and Gurevych, 2014), which is useful for qualifying essay organization (Persing et al., 2010), argumentation (Persing and Ng, 2016; Wachsmuth et al., 2016) and general writing (Burstein et al., 2003; Ong et al., 2014; Song et al., 2014). The major feature classes proposed by Burstein et al. (2003) and Stab and Gurevych (2014) are used to build a baseline. The features include: position, cue words, lexical features (main verbs, adverbs and connectives) and structural features (such as number of clauses). Some of these features are based on manually collected lexicons. Deep Learning Methods have achieved great success in many NLP tasks. Eger et al. (2017) proposed neural argumentation mining models based on sequence tagging or dependency parsing."
2020.emnlp-main.225,N16-1174,0,0.0381741,"entive regions (Shen et al., 2018; Shaw et al., 2018; Yang et al., 2019). In NLP, self-attention is mostly applied to sequential structures such as a sequence of words. Mihaylov and Frank (2019) proposed a discourse-aware selfattention encoder for reading comprehension on narrative texts, where event chains, discourse relations and coreference relations are used for connecting sentences. Self-attention can be also extended to 2d-dimensions for image processing (Parmar et al., 2018) and lattice inputs (Sperber et al., 2019). 3 Baseline We use Hierarchical BiLSTM (HBiLSTM), which is similar to (Yang et al., 2016), as the base model to model sentence and discourse level representations. The task is to assign discourse element labels y = (y1 , ..., yn ) to sentences (x1 , ..., xn ) in a text, where xi , 1 ≤ i ≤ n, is a sentence of a sequence of words and yi ∈ Y, Y is a set of pre-defined discourse elements. Sentence Representation Layer A sequence of words x = {w1 , ..., wN } is modeled with a RNN encoder and is converted into a sequence of hidden states H = {h1 , ..., hN }. The hidden state at the i-th step is hi = f (e (wi ) , hi−1 ) , (1) where f is a RNN unit, e(wi ) ∈ Rd is the embedding of a word,"
2020.emnlp-main.225,N16-1000,0,0.215964,"Missing"
2020.emnlp-main.276,K16-1002,0,0.0181711,"can create new responses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences between our approach and other adversarial learning-based approaches are as follows. First, we cast the dialogue generation model as an SCM to explore potential responses in the environment where observed responses occur. Second, we learn on counterfactual responses that inferred from the SCM. Third, a pre-trained behavior policy is involved during the generation process, ma"
2020.emnlp-main.276,W19-5944,0,0.021886,"Missing"
2020.emnlp-main.276,P18-1139,0,0.0237976,"Missing"
2020.emnlp-main.276,P17-4012,0,0.0285167,"lready been divided into training, validation, and test sets, as shown in Table 1. Given a dialogue that consists of K utterances, we divide it into K-1 instances. Each instance has at most three continuous utterances. The last utterance is the response, and the previous utterances are concatenated as the dialogue history. 4.2 Time (s/epoch) on counterfactual responses. It is model-agnostic and can be applied to any adversarial learningbased dialogue generation model, such as REGS, DPGAN, and StepGAN. 4.3 Training Details We implement REGS, StepGAN, and their variants with COPT using OpenNMT (Klein et al., 2017), an open-source framework for building sequence to sequence models. We manually tune the parameters according to the perplexity on the validation set. The vocabulary consists of the most frequent 10,000 words. Including more words (up to 17,438, the total number of DailyDialog vocabulary) observes no improvement but takes more time for training. We use 300 dimensional GloVe (Pennington et al., 2014) vectors to initialize word embeddings. Both the encoder and the decoder are a two-layer LSTM in G and a single layer LSTM in D. The number of hidden units is 500. During the adversarial learning p"
2020.emnlp-main.276,W06-1303,0,0.0144713,"butions of this paper are summarized as follows: • We connect the concept of counterfactual reasoning with the dialogue generation by casting the dialogue generation model as a structural causal model. • Our counterfactual response is of higher quality than the response synthesized from scratch in standard adversarial learning-based dialogue generation model. • Our approach is model-agnostic and can be applied to any adversarial learning-based dialogue generation model. 2 Related Work Dialogue Generation Data-driven dialogue systems can be roughly divided into two categories: retrieval-based (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016) and generation-based (Shang et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015). Responses of retrieval-based methods come from a fixed candidate response set and thus are incapable of being customized. The generation-based methods can create new responses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et"
2020.emnlp-main.276,N16-1014,0,0.153822,"s the conventional adversarial learning approaches. 1 Dialogue History: What are you up to this Friday? Counterfactual Response: I am going to gym with a friend. Figure 1: An example of a counterfactual response, which is a potential response inferred in hindsight from given observed response. Introduction Open-domain dialogue generation (Shang et al., 2015a; Vinyals and Le, 2015; Sordoni et al., 2015a) intends to produce coherent responses given dialogue history. Nevertheless, it suffers from data insufficiency problem as there may exist many potential responses for a given dialogue history (Li et al., 2016). An ideal way of exploring the potential responses is to train the model by chatting with real users, which is usually time-consuming and labor-intensive in practice. Although replacing a real user with a user simulator could address the issue, the simulator only roughly approximates real user statistics, and its development process is costly (Su et al., 2016). In contrast, humans could independently reason potential responses based on past experiences from the true environment. Having observed a response, one might naturally ask himself or herself: “What ∗ Corresponding author. what if I res"
2020.emnlp-main.276,D17-1230,0,0.299845,"m. After generating the counterfactual response, the generator will receive a reward from a discriminator and optimize itself accordingly. Intuitively, a counterfactual response is synthesized by grounding the model in the scenario where an observed response occurs, rather than the scenario sampled from scratch as standard adversarial learning-based approaches. This improves the quality of the synthesized responses and subsequently benefits the model that learns from the synthesis. To verify the effectiveness of our approach, we conduct experiments on the public available DailyDialog dataset (Li et al., 2017b). Experimental results show that our approach significantly outperforms previous adversarial learning-based approaches in both automatic and human evaluations. The contributions of this paper are summarized as follows: • We connect the concept of counterfactual reasoning with the dialogue generation by casting the dialogue generation model as a structural causal model. • Our counterfactual response is of higher quality than the response synthesized from scratch in standard adversarial learning-based dialogue generation model. • Our approach is model-agnostic and can be applied to any adversa"
2020.emnlp-main.276,I17-1099,0,0.0726707,"m. After generating the counterfactual response, the generator will receive a reward from a discriminator and optimize itself accordingly. Intuitively, a counterfactual response is synthesized by grounding the model in the scenario where an observed response occurs, rather than the scenario sampled from scratch as standard adversarial learning-based approaches. This improves the quality of the synthesized responses and subsequently benefits the model that learns from the synthesis. To verify the effectiveness of our approach, we conduct experiments on the public available DailyDialog dataset (Li et al., 2017b). Experimental results show that our approach significantly outperforms previous adversarial learning-based approaches in both automatic and human evaluations. The contributions of this paper are summarized as follows: • We connect the concept of counterfactual reasoning with the dialogue generation by casting the dialogue generation model as a structural causal model. • Our counterfactual response is of higher quality than the response synthesized from scratch in standard adversarial learning-based dialogue generation model. • Our approach is model-agnostic and can be applied to any adversa"
2020.emnlp-main.276,D16-1230,0,0.0757596,"Missing"
2020.emnlp-main.276,D15-1166,0,0.036369,"ining set as Y and a modelgenerated response as Yˆ . 3.3 Counterfactual Off-Policy Training Our COPT approach is model-agnostic and can be applied to any adversarial learning-based dialogue generation model. Without loss of generality, we take the combination of COPT and the reward for every generation step (REGS) model (Li et al., 2017a) as an example in this section. It consists of two main components: a generator G and a discriminator D. Generator The generator G is a sequence to sequence (Seq2Seq) model (Sutskever et al., 2014) equipped with the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). During the encoding process, G reads the dialogue history into hidden states using an encoder LSTM (Hochreiter and Schmidhuber, 1997): Hi = LSTM(Xi , Hi−1 ), (1) where Xi is the i-th word of the dialogue history, and Hi denotes the corresponding hidden state. 3440 Observed Response: Well, I have the day off from work. Counterfactual Response: I’m going to the gym with a friend. Inferred Scenario Y U SCM X Y=fµ(X, U) u fµ fπ U Y X SCM Y= fπ(X, U) Intervention Discriminator Reward Policy Gradient Message: Hey. What are you up to this Friday? Figure 3: The architecture of our COPT approach. π i"
2020.emnlp-main.276,C16-1316,0,0.0175302,"ialogue systems can be roughly divided into two categories: retrieval-based (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016) and generation-based (Shang et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015). Responses of retrieval-based methods come from a fixed candidate response set and thus are incapable of being customized. The generation-based methods can create new responses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences be"
2020.emnlp-main.276,P02-1040,0,0.106054,"Missing"
2020.emnlp-main.276,N18-1162,0,0.023827,"Missing"
2020.emnlp-main.276,D14-1162,0,0.0835341,"can be applied to any adversarial learningbased dialogue generation model, such as REGS, DPGAN, and StepGAN. 4.3 Training Details We implement REGS, StepGAN, and their variants with COPT using OpenNMT (Klein et al., 2017), an open-source framework for building sequence to sequence models. We manually tune the parameters according to the perplexity on the validation set. The vocabulary consists of the most frequent 10,000 words. Including more words (up to 17,438, the total number of DailyDialog vocabulary) observes no improvement but takes more time for training. We use 300 dimensional GloVe (Pennington et al., 2014) vectors to initialize word embeddings. Both the encoder and the decoder are a two-layer LSTM in G and a single layer LSTM in D. The number of hidden units is 500. During the adversarial learning process, we use the ADAM algorithm to alternately optimize G and D for one batch and five batches. The batch size is 64. We have tested the learning rate from 1e-6 to 1e-3. REGS+COPT and StepGAN+COPT achieve the best performance on 1e-5. The number of parameters for all the baselines is in a range of 21M to 26M. Equipping an adversarial learning baseline with COPT will introduce extra parameters with"
2020.emnlp-main.276,D19-1509,0,0.0376887,"Missing"
2020.emnlp-main.276,P15-1152,0,0.0444287,"Missing"
2020.emnlp-main.276,N15-1020,0,0.0640256,"Missing"
2020.emnlp-main.276,P16-1230,0,0.0489445,"Missing"
2020.emnlp-main.276,D18-1428,0,0.190331,"the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences between our approach and other adversarial learning-based approaches are as follows. First, we cast the dialogue generation model as an SCM to explore potential responses in the environment where observed responses occur. Second, we learn on counterfactual responses that inferred from the SCM. Third, a pre-trained behavior policy is involved during the generation process, making our approach an off-policy algorithm and benefits the exploration of potential responses. Counterfactual Reasoning The counterfactual"
2020.emnlp-main.276,P17-1061,0,0.0173267,"nses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences between our approach and other adversarial learning-based approaches are as follows. First, we cast the dialogue generation model as an SCM to explore potential responses in the environment where observed responses occur. Second, we learn on counterfactual responses that inferred from the SCM. Third, a pre-trained behavior policy is involved during the generation process, making our approach an"
2020.emnlp-main.276,P19-1366,1,0.837886,"n be roughly divided into two categories: retrieval-based (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016) and generation-based (Shang et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015). Responses of retrieval-based methods come from a fixed candidate response set and thus are incapable of being customized. The generation-based methods can create new responses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences between our approach"
2020.emnlp-main.539,D15-1075,0,0.423598,"s the speaker’s profile. On the other hand, although R3 does not contain the attribute word Beijing, we could still infer from the words Tsinghua University that the speaker’s current location entails the profile. Existing studies (Qian et al., 2018; Zheng et al., 2019) train dialogue agents to produce plausible responses that contain attribute information, but still cannot teach agents to understand the differences of consistency relations in these responses. Welleck et al. (2019) made an early step towards reducing the dialogue consistency identification to natural language inference (NLI) (Bowman et al., 2015), where they learn a mapping from two dialogue utterances to an entailment category. All utterances in Welleck et al. (2019) are natural sentences from the PersonaChat dataset (Zhang et al., 2018). However, structured attribute profiles, such as key-value pairs, are ubiquitous in real-world dialogue systems (Shum et al., 2018). Compared with natural sentences, structured profiles have fixed attribute keys from different domains and specific attribute values from limited candidates. The structure information is also essential to a better understanding of the profile. To endow agents with the ab"
2020.emnlp-main.539,P17-1152,0,0.0252982,"etails are in the appendix. 5.2 Identifying Profile Consistency We compare the performance of a variety of baseline models on identifying profile consistency: Feature-based classifier Our goal of setting this baseline was to better understand the difficulty of identifying profile consistency, rather than necessarily a state-of-the-art model. Here we choose SVM as the classifier, with unigram features and bigram features, i.e., SVM+uni+bi. Additionally, the overlaps between profile values and responses are extracted as another feature, which is the SVM+uni+bi+overlap. Rnn-based NLI model ESIM (Chen et al., 2017) is a powerful natural language inference model, which enhanced the interactions in the LSTM. This model was applied in Welleck et al. (2019) and achieved the best results. Therefore, we set ESIM as the rnn baseline for our experiments. Pretrained models Large pre-trained transformers have been shown effective for natural language understanding tasks. We choose the Generative Pre-trained Transformer, i.e. GPT (Radford et al., 2018), and Bidirectional Encoder Representations from Transformers, i.e. BERT (Devlin et al., 2019) as our pre-trained baselines. Chen et al. (2020) proposed a TableBERT"
2020.emnlp-main.539,W19-4828,0,0.0156613,"the natural sentences. For example, from the profile { gender: female, location: Beijing, constellation: Leo}, we can clearly see three dependency relations: female → gender, Beijing → location, and Leo → constellation. Moreover, gender, location, and constellation will define the information in the kv-profile. Here we can see a hierarchical structure of the key-value profiles, as illustrated in Figure 2. More importantly, no matter how the values change, this structure will stay unchanged. Although large pre-trained models such as BERT implicitly capture dependency information more or less (Clark et al., 2019), we argue that such implicit syntactic information may not be enough to support a powerful contextual representation for reasoning on the highly structured key-value profiles, according to the meaningless dependency parsing results generated by BERT on the structured profiles. These observations motivate us to incorporate the explicit structure of profiles directly. To this end, we design the KvBERT, which integrates both language representation from BERT and structure representation from tree-LSTM (Zhu et al., 2015). 6654 [ Entailed, Contradicted, Irrelevant ] Output Layer … … … 12-Layer BER"
2020.emnlp-main.539,N19-1423,0,0.0473779,"Examples in this figure: the key-value profile is { gender: female, location: Beijing, constellation: Leo}, and the dialogue response is “I am glad you could come to Beijing”. 4.3 Model Brief Figure 2 shows the overall framework of the KvBERT model. On the BERT side, we linearize the key-value pairs into a sequence and treating the responses as another sequence2 . The input embedding is the sum of four embeddings, including an additional type embedding (Chen et al., 2020) to inform the model of different key-value pairs, as shown in Figure 2. Here we omit the well-known formulations of BERT (Devlin et al., 2019) for brevity. We can get a contextual representation for the linearized sequence through the BERT model. On the tree-LSTM side, the profiles are parsed to predefined structure, as discussed in Sec 4.2. An example of this structure can be seen in the red part of the Figure 2. In parallel, the responses are passed to a trained parser to fetch the dependency structure. Then the tree-LSTM encodes two structures to corresponding embeddings. Three operations are performed to aggregate information from two embeddings: element-wise multiplication, elementwise difference, and concatenation. The aggrega"
2020.emnlp-main.539,W19-3646,0,0.0106895,"ant) between each conversation and structured profile, and (2) find out the detailed attribute information in each response. With the annotated KvPI dataset, we set up different baseline models, and propose a key-value structure information enriched BERT (KvBERT) model, which leverages dependency structures in profiles to enrich the contextual representations. Experimental results show that KvBERT obtains significant improvements over strong baselines. We further test the KvBERT model on two downstream tasks, including a reranking task (Welleck et al., 2019) and a consistency prediction task (Dziri et al., 2019). Evaluation results show that (1) the KvBERT reranking improves response consistency, and (2) the KvBERT consistency prediction has a good agreement with human annotation. Our contributions are summarized as below: • A KvPI dataset is introduced, which has over 110K fine-grained consistency annotations between responses and their key-value profiles. • A KvBERT model is proposed for consistency identification, which gained significant improvements over strong baselines. • Evaluations on downstream tasks show that the profile consistency identification model could be complementary to dialogue m"
2020.emnlp-main.539,2020.acl-main.516,1,0.829805,"work is the Chen et al. (2020), who proposed to use semi-structured Wikipedia tables as evidence. The difference between our work and Chen et al. (2020) is noticeable: open-domain dialogues have unique language patterns, and the key-value profiles are highly structured, as analyzed in Sec 4.2. To the best of our knowledge, this is the first work that explores the identification of consistency between dialogue responses and structured profiles. Another line of research related to this work is the personalized dialogue generation task (Zhang et al., 2018; Qian et al., 2018; Zheng et al., 2019; Song et al., 2020a,b). This task seeks to improve personality consistency by incorporating persona information in the generated responses. For this purpose, several personalized dialogue datasets have been introduced in recent years, such as PersonaChat (Zhang et al., 2018) and PersonalDialog (Zheng et al., 2019). These datasets successfully inform models of how to incorporate attribute related information in the responses, but still can not teach models how to identify the consistency relations between their response and profile. 7 Conclusion and Discussion In this work, we introduce a large-scale annotated d"
2020.emnlp-main.539,P17-2034,0,0.0305775,"Missing"
2020.emnlp-main.539,P19-1644,0,0.0256384,"Missing"
2020.emnlp-main.539,P19-1363,0,0.564048,"e word Beijing, we could still understand R3 entails the current location. Introduction Despite the recent advancements in assigning attribute profiles to dialogue agents (Qian et al., 2018; Zhang et al., 2019), maintaining a consistent profile is still challenging for an open-domain dialogue agent. Existing works mainly emphasize the incorporation of attribute information in the generated responses (Wolf et al., 2019; Song et al., 2019; Zheng et al., 2020). Although these models have improved the response consistency by explicitly modeling the profiles, they still face the consistency issue (Welleck et al., 2019). One important reason is that they cannot identify the consistency relations between response and profile. As shown in Figure 1, the attribute word Beijing is incorporated in the first two responses, but only R1 is semantically consistent with the speaker’s profile. For example, R2 “I also hope to visit Beijing one day.” implies that the speaker has never been to Beijing, which contradicts the speaker’s profile. On the other hand, although R3 does not contain the attribute word Beijing, we could still infer from the words Tsinghua University that the speaker’s current location entails the pro"
2020.emnlp-main.539,N18-1101,0,0.0476887,"Missing"
2020.emnlp-main.539,P18-1205,0,0.116423,"ils the profile. Existing studies (Qian et al., 2018; Zheng et al., 2019) train dialogue agents to produce plausible responses that contain attribute information, but still cannot teach agents to understand the differences of consistency relations in these responses. Welleck et al. (2019) made an early step towards reducing the dialogue consistency identification to natural language inference (NLI) (Bowman et al., 2015), where they learn a mapping from two dialogue utterances to an entailment category. All utterances in Welleck et al. (2019) are natural sentences from the PersonaChat dataset (Zhang et al., 2018). However, structured attribute profiles, such as key-value pairs, are ubiquitous in real-world dialogue systems (Shum et al., 2018). Compared with natural sentences, structured profiles have fixed attribute keys from different domains and specific attribute values from limited candidates. The structure information is also essential to a better understanding of the profile. To endow agents with the ability to identify structured profile consistency, we 6651 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6651–6662, c November 16–20, 2020. 2020 Asso"
2020.emnlp-main.546,D13-1180,0,0.289781,"e-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations. 1 Introduction Automated essay scoring (AES) is an important educational application of natural language processing (NLP) (Page, 1966). AES aims to automatically judge the quality of student essays, which can reduce teachers’ burden on essay scoring and provide fast feedback to students. AES is usually viewed as a supervised learning problem. Traditionally, AES systems are based on hand-crafted surface-level features (Larkey, 1998; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015). Recently, neural network based representation learning has been applied and achieved superior performance compared with traditional methods (Taghipour and Ng, 2016; Cummins et al., 2016; Alikaniotis et al., 2016; Dong and Zhang, 2016; Dong et al., 2017; Tay et al., 2018). Most of the proposed methods, no matter the feature based or the representation learning based ones, work in an in-domain setting that is to train a scorer for a specific prompt based on a set of example essays for this prompt, and use this scorer to rate more essays from the same prompt. This manner u"
2020.emnlp-main.546,P16-1075,0,0.0433035,"Missing"
2020.emnlp-main.546,N19-1423,0,0.13686,") proposed domain adaptation as a solution to adapt an AES system from one initial prompt to another prompt based on Bayesian linear ridge regression. Dong and Zhang (2016) demonstrated that the hierarchical CNN based model performs better in domain adaptation setting. Pil´an et al. (2016); Xia et al. (2016) also attempted to incorpo6724 rate external knowledge for readability assessment. However, these methods mostly focused on domain adaptation from one domain to another but did not explore external resources. Pre-training for AES Recently, pre-training language models (LM) becomes a trend (Devlin et al., 2019; Yang et al., 2019), which leads to the pretraining then fine-tuning mechanism and achieves great success in many NLP tasks. For AES, Mim et al. (2019) proposed an unsupervised pre-training approach for evaluating the organization and argument strength of argumentative essays, where coherence modeling is used for pre-training. Rodriguez et al. (2019) attempted to apply BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) for AES, but the results on ASAP are similar to the performance of a LSTM based scorer. Howard and Ruder (2018) proposed the universal language model fine-tuning approach"
2020.emnlp-main.546,D16-1115,0,0.135273,"NLP) (Page, 1966). AES aims to automatically judge the quality of student essays, which can reduce teachers’ burden on essay scoring and provide fast feedback to students. AES is usually viewed as a supervised learning problem. Traditionally, AES systems are based on hand-crafted surface-level features (Larkey, 1998; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015). Recently, neural network based representation learning has been applied and achieved superior performance compared with traditional methods (Taghipour and Ng, 2016; Cummins et al., 2016; Alikaniotis et al., 2016; Dong and Zhang, 2016; Dong et al., 2017; Tay et al., 2018). Most of the proposed methods, no matter the feature based or the representation learning based ones, work in an in-domain setting that is to train a scorer for a specific prompt based on a set of example essays for this prompt, and use this scorer to rate more essays from the same prompt. This manner usually requires many rated examples to get acceptable performance. Although cross-domain transferable essay scoring has gained more attention (Phandi et al., 2015; Jin et al., 2018), the progress is still limited. The possible reason may be that the availab"
2020.emnlp-main.546,K17-1017,0,0.220425,"aims to automatically judge the quality of student essays, which can reduce teachers’ burden on essay scoring and provide fast feedback to students. AES is usually viewed as a supervised learning problem. Traditionally, AES systems are based on hand-crafted surface-level features (Larkey, 1998; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015). Recently, neural network based representation learning has been applied and achieved superior performance compared with traditional methods (Taghipour and Ng, 2016; Cummins et al., 2016; Alikaniotis et al., 2016; Dong and Zhang, 2016; Dong et al., 2017; Tay et al., 2018). Most of the proposed methods, no matter the feature based or the representation learning based ones, work in an in-domain setting that is to train a scorer for a specific prompt based on a set of example essays for this prompt, and use this scorer to rate more essays from the same prompt. This manner usually requires many rated examples to get acceptable performance. Although cross-domain transferable essay scoring has gained more attention (Phandi et al., 2015; Jin et al., 2018), the progress is still limited. The possible reason may be that the available corpora for essa"
2020.emnlp-main.546,2020.acl-main.740,0,0.0288729,"Missing"
2020.emnlp-main.546,P18-1031,0,0.0178475,"S Recently, pre-training language models (LM) becomes a trend (Devlin et al., 2019; Yang et al., 2019), which leads to the pretraining then fine-tuning mechanism and achieves great success in many NLP tasks. For AES, Mim et al. (2019) proposed an unsupervised pre-training approach for evaluating the organization and argument strength of argumentative essays, where coherence modeling is used for pre-training. Rodriguez et al. (2019) attempted to apply BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) for AES, but the results on ASAP are similar to the performance of a LSTM based scorer. Howard and Ruder (2018) proposed the universal language model fine-tuning approach for text classification, including components such as general-domain LM pre-training, target domain LM fine-tuning and target task classifier fine-tuning. Gururangan et al. (2020) showed that task-adaptive pretraining can provide a large performance boost for ROBERTA across four domains and eight classification tasks. Motivated by previous works, this paper also adopts a multi-stage pre-training strategy by exploiting weak, distant and target oriented supervision for AES. 3 The Proposed Method 3.1 The ARCNN Model Our base model is the"
2020.emnlp-main.546,P18-1100,0,0.0670785,"Missing"
2020.emnlp-main.546,P19-2053,0,0.0149601,"g and Zhang (2016) demonstrated that the hierarchical CNN based model performs better in domain adaptation setting. Pil´an et al. (2016); Xia et al. (2016) also attempted to incorpo6724 rate external knowledge for readability assessment. However, these methods mostly focused on domain adaptation from one domain to another but did not explore external resources. Pre-training for AES Recently, pre-training language models (LM) becomes a trend (Devlin et al., 2019; Yang et al., 2019), which leads to the pretraining then fine-tuning mechanism and achieves great success in many NLP tasks. For AES, Mim et al. (2019) proposed an unsupervised pre-training approach for evaluating the organization and argument strength of argumentative essays, where coherence modeling is used for pre-training. Rodriguez et al. (2019) attempted to apply BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) for AES, but the results on ASAP are similar to the performance of a LSTM based scorer. Howard and Ruder (2018) proposed the universal language model fine-tuning approach for text classification, including components such as general-domain LM pre-training, target domain LM fine-tuning and target task classifier fine-tuni"
2020.emnlp-main.546,D15-1049,0,0.145307,"scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations. 1 Introduction Automated essay scoring (AES) is an important educational application of natural language processing (NLP) (Page, 1966). AES aims to automatically judge the quality of student essays, which can reduce teachers’ burden on essay scoring and provide fast feedback to students. AES is usually viewed as a supervised learning problem. Traditionally, AES systems are based on hand-crafted surface-level features (Larkey, 1998; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015). Recently, neural network based representation learning has been applied and achieved superior performance compared with traditional methods (Taghipour and Ng, 2016; Cummins et al., 2016; Alikaniotis et al., 2016; Dong and Zhang, 2016; Dong et al., 2017; Tay et al., 2018). Most of the proposed methods, no matter the feature based or the representation learning based ones, work in an in-domain setting that is to train a scorer for a specific prompt based on a set of example essays for this prompt, and use this scorer to rate more essays from the same prompt. This manner usually requires many r"
2020.emnlp-main.546,C16-1198,0,0.027919,"Missing"
2020.emnlp-main.546,P17-1011,1,0.834121,"Missing"
2020.emnlp-main.546,D16-1193,0,0.495048,"is an important educational application of natural language processing (NLP) (Page, 1966). AES aims to automatically judge the quality of student essays, which can reduce teachers’ burden on essay scoring and provide fast feedback to students. AES is usually viewed as a supervised learning problem. Traditionally, AES systems are based on hand-crafted surface-level features (Larkey, 1998; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015). Recently, neural network based representation learning has been applied and achieved superior performance compared with traditional methods (Taghipour and Ng, 2016; Cummins et al., 2016; Alikaniotis et al., 2016; Dong and Zhang, 2016; Dong et al., 2017; Tay et al., 2018). Most of the proposed methods, no matter the feature based or the representation learning based ones, work in an in-domain setting that is to train a scorer for a specific prompt based on a set of example essays for this prompt, and use this scorer to rate more essays from the same prompt. This manner usually requires many rated examples to get acceptable performance. Although cross-domain transferable essay scoring has gained more attention (Phandi et al., 2015; Jin et al., 2018), the"
2020.emnlp-main.546,W16-0502,0,0.0129768,"d Zhang, 2016; Dong et al., 2017; Tay et al., 2018). These models obtained superior performance compared with traditional methods. However, most of these systems are promptspecific. New training data has to be annotated for training a new model for a new prompt. Domain Adaptation for AES Phandi et al. (2015) proposed domain adaptation as a solution to adapt an AES system from one initial prompt to another prompt based on Bayesian linear ridge regression. Dong and Zhang (2016) demonstrated that the hierarchical CNN based model performs better in domain adaptation setting. Pil´an et al. (2016); Xia et al. (2016) also attempted to incorpo6724 rate external knowledge for readability assessment. However, these methods mostly focused on domain adaptation from one domain to another but did not explore external resources. Pre-training for AES Recently, pre-training language models (LM) becomes a trend (Devlin et al., 2019; Yang et al., 2019), which leads to the pretraining then fine-tuning mechanism and achieves great success in many NLP tasks. For AES, Mim et al. (2019) proposed an unsupervised pre-training approach for evaluating the organization and argument strength of argumentative essays, where coher"
2020.emnlp-main.583,N18-2007,0,0.0481101,"e that graph-attention or the entire graph structure can be replaced by self-attention or Transformers. 1 Introduction Different from single-hop question answering, where the answer can be derived from a single sentence in a single paragraph, more and more studies focus on multi-hop question answering across multiple documents or paragraphs (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). To solve this problem, the majority of existing studies constructed a graph structure according to co-occurrence relations of entities that scattered across multiple sentences or paragraphs. Dhingra et al. (2018) and Song et al. (2018) designed a DAG-styled recurrent layer to model the relations between entities. De Cao et al. (2019) first used GCN (Kipf and Welling, 2017) to tackle entity graph. Qiu et al. (2019) proposed a dynamic entity graph for span-based multi-hop QA. Tu et al. (2019b) extended the entity graph to a heterogeneous graph by introducing document nodes and query nodes. Previous works argue that a fancy graph structure is a vital part of their models and demonstrate that • In Section 2, we first describe our baseline model. Then, we show that the graph structure can play an important"
2020.emnlp-main.583,D19-1445,0,0.0291716,"reasoning ability. Only stacking two layers of the Transformer can achieve comparable results as DFGN. 3.3 Training Details For all experiments in this paper, the number of layers of different modules is two, and the hidden dimensions are set to 300. In feature-based setting, all models are trained for 30 epochs with a batch size of 24. In fine-tuning setting, models are trained for 3 epochs with a batch size of 8. The initial learning rate is 2e-4 and 3e-5 in the feature-based setting and fine-tuning setting respectively. 3.4 Entity-centered Attention Pattern in Pre-trained Model Inspired by Kovaleva et al. (2019), we leverage an approximate method to find which attention head contains entity-centered attention patterns. We employ an NER model to identify tokens belong to a certain entity span. Then, for each attention head in the pre-trained model, we sum the absolute attention weights among those tokens belong to an entity and tokens not belong to an entity. The score of an attention head is the difference between the sum of weights from entities and non-entities tokens. We then average the derived scores over all the examples. Finally, the attention head with the maximum score is the desired head th"
2020.emnlp-main.583,2021.ccl-1.108,0,0.0974878,"Missing"
2020.emnlp-main.583,P19-1225,0,0.340452,"Missing"
2020.emnlp-main.583,P19-1617,0,0.663055,"gphu}@iflytek.com ‡ {ymcui,tliu}@ir.hit.edu.cn Abstract by ablation experiments. However, in experiments, we find when we use the pre-trained models in the fine-tuning approach, removing the entire graph structure may not hurt the final results. Therefore, in this paper, we aimed to answer the following question: How much does graph structure contribute to multi-hop question answering? To answer the question above, we choose the widely used multi-hop question answering benchmark, HotpotQA (Yang et al., 2018), as our testbed. We reimplement a graph-based model, Dynamically Fused Graph Network (Qiu et al., 2019), as our baseline model. The remainder of this paper is organized as follows. Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for multi-hop question answering. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, graph structure may not be necessary for multi-hop question answering. We point out that both graph structure and adjacency"
2020.emnlp-main.583,Q18-1021,0,0.0371972,"cessary for multi-hop question answering. We point out that both graph structure and adjacency matrix are task-related prior knowledge, and graphattention can be considered as a special case of self-attention. Experiments and visualized analysis demonstrate that graph-attention or the entire graph structure can be replaced by self-attention or Transformers. 1 Introduction Different from single-hop question answering, where the answer can be derived from a single sentence in a single paragraph, more and more studies focus on multi-hop question answering across multiple documents or paragraphs (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). To solve this problem, the majority of existing studies constructed a graph structure according to co-occurrence relations of entities that scattered across multiple sentences or paragraphs. Dhingra et al. (2018) and Song et al. (2018) designed a DAG-styled recurrent layer to model the relations between entities. De Cao et al. (2019) first used GCN (Kipf and Welling, 2017) to tackle entity graph. Qiu et al. (2019) proposed a dynamic entity graph for span-based multi-hop QA. Tu et al. (2019b) extended the entity graph to a heterogeneous graph by in"
2020.emnlp-main.583,D18-1259,0,0.407064,"te of Technology, Harbin, China § iFLYTEK AI Research (Hebei), Langfang, China †§ {nanshao,ymcui,sjwang3,gphu}@iflytek.com ‡ {ymcui,tliu}@ir.hit.edu.cn Abstract by ablation experiments. However, in experiments, we find when we use the pre-trained models in the fine-tuning approach, removing the entire graph structure may not hurt the final results. Therefore, in this paper, we aimed to answer the following question: How much does graph structure contribute to multi-hop question answering? To answer the question above, we choose the widely used multi-hop question answering benchmark, HotpotQA (Yang et al., 2018), as our testbed. We reimplement a graph-based model, Dynamically Fused Graph Network (Qiu et al., 2019), as our baseline model. The remainder of this paper is organized as follows. Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for multi-hop question answering. Our analysis is centered on HotpotQA. We construct a strong baseline model to establish that, with the proper use of pre-trained models, graph structure may"
2020.emnlp-main.583,N18-1059,0,0.0219182,"p question answering. We point out that both graph structure and adjacency matrix are task-related prior knowledge, and graphattention can be considered as a special case of self-attention. Experiments and visualized analysis demonstrate that graph-attention or the entire graph structure can be replaced by self-attention or Transformers. 1 Introduction Different from single-hop question answering, where the answer can be derived from a single sentence in a single paragraph, more and more studies focus on multi-hop question answering across multiple documents or paragraphs (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). To solve this problem, the majority of existing studies constructed a graph structure according to co-occurrence relations of entities that scattered across multiple sentences or paragraphs. Dhingra et al. (2018) and Song et al. (2018) designed a DAG-styled recurrent layer to model the relations between entities. De Cao et al. (2019) first used GCN (Kipf and Welling, 2017) to tackle entity graph. Qiu et al. (2019) proposed a dynamic entity graph for span-based multi-hop QA. Tu et al. (2019b) extended the entity graph to a heterogeneous graph by introducing document nodes"
2020.emnlp-main.583,P19-1260,0,0.050459,"tion answering across multiple documents or paragraphs (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). To solve this problem, the majority of existing studies constructed a graph structure according to co-occurrence relations of entities that scattered across multiple sentences or paragraphs. Dhingra et al. (2018) and Song et al. (2018) designed a DAG-styled recurrent layer to model the relations between entities. De Cao et al. (2019) first used GCN (Kipf and Welling, 2017) to tackle entity graph. Qiu et al. (2019) proposed a dynamic entity graph for span-based multi-hop QA. Tu et al. (2019b) extended the entity graph to a heterogeneous graph by introducing document nodes and query nodes. Previous works argue that a fancy graph structure is a vital part of their models and demonstrate that • In Section 2, we first describe our baseline model. Then, we show that the graph structure can play an important role only when the pre-trained models are used in a feature-based manner. While the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful. • To explain the results, in Section 3.1, we point out that graph-attention (Veliˇckovi´c et al., 20"
2020.emnlp-main.634,I05-5002,0,\N,Missing
2020.emnlp-main.634,W07-1401,0,\N,Missing
2020.emnlp-main.634,D13-1170,0,\N,Missing
2020.emnlp-main.634,S17-2001,0,\N,Missing
2020.emnlp-main.634,D17-1169,0,\N,Missing
2020.emnlp-main.634,N18-1202,0,\N,Missing
2020.emnlp-main.634,N19-1213,0,\N,Missing
2020.emnlp-main.634,K16-1002,0,\N,Missing
2020.emnlp-main.634,N19-1423,0,\N,Missing
2020.emnlp-main.634,N18-1101,0,\N,Missing
2020.emnlp-main.634,Q19-1040,0,\N,Missing
2020.emnlp-main.634,U19-1011,0,\N,Missing
2020.findings-emnlp.122,P17-4012,0,0.0725721,"9.7/24.9)* 11.0/26.5 (11.1/26.4) QKU-2/3 –/– (–/–) 3.36/2.82 (3.35/2.80) 3.23/2.72* (3.23/2.72)* 2.55/1.94* (2.55/1.95)* 2.45/1.91* (2.45/1.92)* 2.18/1.84* (2.15/1.82)* 2.12/1.77* (2.11/1.76)* 2.08/1.64 (2.05/1.62) Table 2: Automatic evaluations on the CMU DoG Dataset. · (·) means Reduced (Sampled) test data. We take the CAT-DD as the base model to do the significant test,  and * stands p&lt;0.05 and p&lt;0.01, respectively. 5 (1:very bad, 2:bad, 3:acceptable, 4:good, 5:very good). Overall inter-rater agreement measured by Fliess’ Kappa is 0.32 (”fair”). 4.4 Experimental Setup We use OpenNMT-py (Klein et al., 2017) as the code framework. For all models, the pre-trained 300 dimension word embedding (Mikolov et al., 2013) is shared by dialogue, document, and generated responses, the dimension of the hidden size is 300. For the RNN-based models, 3-layer bidirectional GRU and 3-layer GRU are applied for encoder and decoder, respectively. For the Transformer-based models, the layers of both encoder and decoder are set to 3, the number of heads in multi-head attention is 8 and the filter size is 2048. We use Adam (α = 0.001, β1 = 0.9, β2 = 0.999, and  = 10−8 ) (Kingma and Ba, 2015) for optimization. The beam"
2020.findings-emnlp.122,N16-1014,0,0.0892829,"G dataset show that the proposed CAT model outperforms the state-ofthe-art approach and strong baselines. 1 Figure 1: One DGD example in the CMUDoG dataset. S1/S2 means Speaker-1/Speaker-2, respectively. Introduction Dialogue system (DS) attracts great attention from industry and academia because of its wide application prospects. Sequence-to-sequence models (Seq2Seq) (Sutskever et al., 2014; Serban et al., 2016) are verified to be an effective framework for the DS task. However, one problem of Seq2Seq models is that they tended to generate generic responses that provids deficient information Li et al. (2016); Ghazvininejad et al. (2018). Previous researchers proposed different methods to alleviate this issue. One way is to focus on models’ ability to extract information from conversations. Li et al. (2016) introduced Maximum Mutual Information (MMI) as the objective function for generating diverse response. Serban et al. (2017) proposed a latent variable model to capture posterior information of golden response. Zhao et al. (2017) used conditional variational autoencoders to learn discourselevel diversity for neural dialogue models. The other way is introducing external knowledge, either unstruct"
2020.findings-emnlp.122,P19-1002,0,0.113091,"l to capture posterior information of golden response. Zhao et al. (2017) used conditional variational autoencoders to learn discourselevel diversity for neural dialogue models. The other way is introducing external knowledge, either unstructured knowledge texts Ghazvininejad et al. (2018); Ye et al. (2019); Dinan et al. (2019) or structured knowledge triples (Liu et al., 2018; Young et al., 2018; Zhou et al., 2018a) to help open-domain conversation generation by producing responses conditioned on selected knowledge. The Document-grounded Dialogue (DGD) (Zhou et al., 2018b; Zhao et al., 2019; Li et al., 2019) is a new way to use external knowledge. It establishes a conversation mode in which relevant information can be obtained from the given document. One example of DGD is presented in Figure 1. Two interlocutors talk about the given document and freely reference the text segment during the conversation. To address this task, two main challenges need to be considered in a DGD model: 1) Determining which of the historical conversations are related to the current conversation, 2) Using current conversation and the related conversation history to select proper document information and to gener1358 F"
2020.findings-emnlp.122,P18-1138,0,0.049817,"us on models’ ability to extract information from conversations. Li et al. (2016) introduced Maximum Mutual Information (MMI) as the objective function for generating diverse response. Serban et al. (2017) proposed a latent variable model to capture posterior information of golden response. Zhao et al. (2017) used conditional variational autoencoders to learn discourselevel diversity for neural dialogue models. The other way is introducing external knowledge, either unstructured knowledge texts Ghazvininejad et al. (2018); Ye et al. (2019); Dinan et al. (2019) or structured knowledge triples (Liu et al., 2018; Young et al., 2018; Zhou et al., 2018a) to help open-domain conversation generation by producing responses conditioned on selected knowledge. The Document-grounded Dialogue (DGD) (Zhou et al., 2018b; Zhao et al., 2019; Li et al., 2019) is a new way to use external knowledge. It establishes a conversation mode in which relevant information can be obtained from the given document. One example of DGD is presented in Figure 1. Two interlocutors talk about the given document and freely reference the text segment during the conversation. To address this task, two main challenges need to be conside"
2020.findings-emnlp.122,D15-1166,0,0.0917927,"tic concepts. Please refer to Serban et al. (2017) for more details. We use Z to capture the topic transfer in conversations and test three different settings. For the first setting, we do not employ the document knowledge, only use dialogue as input to generate the response. It is recorded as VHRED(-k). For the second one, we use the same encoder RNN with shared parameters to learn the representation of the document and the utterance, then concatenate the final hidden state of them as the input of the context RNN. It is denoted by VHRED(c). For the third one, we use word-level dot-attention (Luong et al., 2015) to get the document-aware utterance representation and use it as the input of context RNN. It is termed as VHRED(a). 4.2.2 Transformer-based models T-DD/T-EDD: They both use the Transformer as the encoder. The inputs are the concatenation of dialogues and the document. These two models parallel encode the dialogue without detecting topic transfer. The T-DD uses a Deliberation Decoder (DD) as the decoder. The T-EDD uses an Enhanced Deliberation Decoder (EDD) as the decoder. ITDD (Li et al., 2019): It uses Incremental Transformer Encoder (ITE) and two-pass Deliberation Decoder (DD). Incremental"
2020.findings-emnlp.122,D18-1255,0,0.0496742,"tions of this paper are: (1) We propose a compare aggregate method to determine the relationship between the historical dialogues and the last utterance. Experiments showed that our model outperformed strong baselines on the CMU DoG dataset. (2) We propose two new metrics to evaluate the document knowledge utilization in the DGD. They are both based on N-gram overlap among generated response, the dialogue, and the document. 2 Related Work The DGD maintains a dialogue pattern where external knowledge can be obtained from the given document. Most recently, some DGD datasets Zhou et al. (2018b); Moghe et al. (2018); Qin et al. (2019); Gopalakrishnan et al. (2019) have been released to exploiting unstructured document information in conversations. Models trying to address the DGD task can be classified into two categories based on their encoding process with dialogues: one is parallel modeling and the other is incremental modeling. For the first category, Moghe et al. (2018) used a generation-based model that learns to copy information from the background knowledge and a span prediction model that predicts the appropriate response span in the background knowledge. Liu et al. (2019) claimed the first to u"
2020.findings-emnlp.122,P02-1040,0,0.110184,"el encode the dialogue without detecting topic transfer. The T-DD uses a Deliberation Decoder (DD) as the decoder. The T-EDD uses an Enhanced Deliberation Decoder (EDD) as the decoder. ITDD (Li et al., 2019): It uses Incremental Transformer Encoder (ITE) and two-pass Deliberation Decoder (DD). Incremental Transformer uses multi-head attention to incorporate document sections and context into each utterance’s encoding process. ITDD incrementally models dialogues without detecting topic transitions. Evaluation Metrics Automatic Evaluation: We employ perplexity (PPL) (Bengio et al., 2000), BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The PPL of the gold response is measured, lower perplexity indicates better performance. BLEU measures the n-gram overlap between a generated response and a gold response. Since there is only one reference for each response, BLEU scores are extremely low. ROUGE measures the n-gram overlap based on the recall rate. Since the conversations are constrained by the background material, ROUGE is reliable. We also introduce two metrics to automatically evaluate the Knowledge Utilization (KU), they are both based on N -grams overlaps. We define one document, conversations and g"
2020.findings-emnlp.122,P19-1539,0,0.023494,"freely reference the text segment during the conversation. To address this task, two main challenges need to be considered in a DGD model: 1) Determining which of the historical conversations are related to the current conversation, 2) Using current conversation and the related conversation history to select proper document information and to gener1358 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1358–1367 c November 16 - 20, 2020. 2020 Association for Computational Linguistics ate an informative response. Previous work Arora et al. (2019); Zhao et al. (2019); Qin et al. (2019); Tian et al. (2020); Ren et al. (2019) generally focused on selecting knowledge with all the conversations. However, the relationship between historical conversations and the current conversation has not been studied enough. For example, in Figure 1, the italics utterance from user1, ”Yes, his style really extended the story.”, is related to dialogue history. While the black fold utterance from user1, ”Sally Hawkins acting was phenomenally expressive. Didn’t feel her character was mentally handicapped.”, has no direct relationship with the historical utterances. when employing this sentence a"
2020.findings-emnlp.122,P19-1004,0,0.0520175,"lly extended the story.”, is related to dialogue history. While the black fold utterance from user1, ”Sally Hawkins acting was phenomenally expressive. Didn’t feel her character was mentally handicapped.”, has no direct relationship with the historical utterances. when employing this sentence as the last utterance, the dialogue history is not conducive to generate a response. In this paper, we propose a novel Transformerbased (Vaswani et al., 2017) model for understanding the dialogues and generate informative responses in the DGD, named Compare Aggregate Transformer (CAT). Previous research (Sankar et al., 2019) has shown that the last utterance is the most important guidance for the response generation in the multi-turn setting. Hence we divide the dialogue into the last utterance and the dialogue history, then measure the effectiveness of the dialogue history. If the last utterance and the dialogue history are related, we need to consider all the conversations to filter the document information. Otherwise, the existence of dialogue history is equal to the introduction of noise, and its impact should be eliminated conditionally. For this purpose, on one side, the CAT filters the document information"
2020.findings-emnlp.122,D18-1076,0,0.143564,"del outperforms the state-ofthe-art approach and strong baselines. 1 Figure 1: One DGD example in the CMUDoG dataset. S1/S2 means Speaker-1/Speaker-2, respectively. Introduction Dialogue system (DS) attracts great attention from industry and academia because of its wide application prospects. Sequence-to-sequence models (Seq2Seq) (Sutskever et al., 2014; Serban et al., 2016) are verified to be an effective framework for the DS task. However, one problem of Seq2Seq models is that they tended to generate generic responses that provids deficient information Li et al. (2016); Ghazvininejad et al. (2018). Previous researchers proposed different methods to alleviate this issue. One way is to focus on models’ ability to extract information from conversations. Li et al. (2016) introduced Maximum Mutual Information (MMI) as the objective function for generating diverse response. Serban et al. (2017) proposed a latent variable model to capture posterior information of golden response. Zhao et al. (2017) used conditional variational autoencoders to learn discourselevel diversity for neural dialogue models. The other way is introducing external knowledge, either unstructured knowledge texts Ghazvini"
2020.findings-emnlp.122,P17-1061,0,0.0573912,"Missing"
2020.findings-emnlp.122,D19-1193,0,0.0473744,"atent variable model to capture posterior information of golden response. Zhao et al. (2017) used conditional variational autoencoders to learn discourselevel diversity for neural dialogue models. The other way is introducing external knowledge, either unstructured knowledge texts Ghazvininejad et al. (2018); Ye et al. (2019); Dinan et al. (2019) or structured knowledge triples (Liu et al., 2018; Young et al., 2018; Zhou et al., 2018a) to help open-domain conversation generation by producing responses conditioned on selected knowledge. The Document-grounded Dialogue (DGD) (Zhou et al., 2018b; Zhao et al., 2019; Li et al., 2019) is a new way to use external knowledge. It establishes a conversation mode in which relevant information can be obtained from the given document. One example of DGD is presented in Figure 1. Two interlocutors talk about the given document and freely reference the text segment during the conversation. To address this task, two main challenges need to be considered in a DGD model: 1) Determining which of the historical conversations are related to the current conversation, 2) Using current conversation and the related conversation history to select proper document information"
2020.findings-emnlp.139,D14-1181,0,0.0124414,"Missing"
2020.findings-emnlp.139,P07-2045,0,0.00976133,"Missing"
2020.findings-emnlp.139,2020.acl-main.703,0,0.111381,"Missing"
2020.findings-emnlp.139,P16-1195,0,0.0204779,"eration, evaluated with smoothed BLEU-4 score. language could improve code-to-NL generation. Besides, results in the Table 4 show that CodeBERT pre-trained with RTD and MLM objectives brings a gain of 1.3 BLEU score over RoBERTa overall and achieve the state-of-the-art performance8 . 4.4 Generalization to Programming Languages NOT in Pre-training We would like to evaluate CodeBERT on the programming language which is never seen in the pretraining step. To this end, we study the task of generating a natural language summary of a C# code snippet. We conduct experiments on the dataset of CodeNN (Iyer et al., 2016)9 , which consists of 66,015 pairs of questions and answers automatically collected from StackOverflow. This dataset is challenging since the scale of dataset is orders of magnitude smaller than CodeSearchNet Corpus. We evaluate models using smoothed BLEU-4 score and use the same evaluation scripts as Iyer et al. (2016). M ODEL BLEU MOSES (KOEHN ET AL ., 2007) IR SUM-NN (RUSH ET AL ., 2015) 2- LAYER B I LSTM T RANSFORMER (VASWANI ET AL ., 2017) T REE LSTM (TAI ET AL ., 2015) C ODE NN (I YER ET AL ., 2016) CODE 2 SEQ (A LON ET AL ., 2019) RO BERTA PRE - TRAIN W / CODE ONLY C ODE BERT (RTD) C OD"
2020.findings-emnlp.139,C04-1072,0,0.0190066,"ly applied. Predicted probabilities of RoBERTa and CodeBERT are given. Code Documentation Generation Although the pre-training objective of CodeBERT does not include generation-based objectives (Lewis et al., 2019), we would like to investigate to what extent does CodeBERT perform on generation tasks. Specifically, we study code-to-NL generation, and report results for the documentation generation task on CodeSearchNet Corpus in six programming languages. Since the generated documentations are short and higher order n-grams may not overlap, we remedy this problem by using smoothed BLEU score (Lin and Och, 2004). 7 The example comes from https:// github.com/peri-source/peri/blob/ 61beed5deaaf978ab31ed716e8470d86ba639867/ peri/comp/psfcalc.py#L994-L1002 Model Comparisons We compare our model with several baselines, including a RNN-based model with attention mechanism (Sutskever et al., 2014), the Transformer (Vaswani et al., 2017), RoBERTa and the model pre-trained on code only. To demonstrate the effectiveness of CodeBERT on code-to-NL generation tasks, we adopt various pre-trained models as encoders and keep the hyperparameters consistent. Detailed hyper-parameters are given in Appendix B.3. Table 4"
2020.findings-emnlp.139,2021.ccl-1.108,0,0.209986,"Missing"
2020.findings-emnlp.139,N18-1202,0,0.370575,"lps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.1 1 Introduction Large pre-trained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019) ∗ Work done while this author was an intern at Microsoft Research Asia. 1 All the codes and data are available at https:// github.com/microsoft/CodeBERT and RoBERTa (Liu et al., 2019) have dramatically improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual representations from massive unlabeled text optimized by self-supervised objectives, such as masked language modeling, which predicts the original masked word from an artifici"
2020.findings-emnlp.139,D19-1250,0,0.0585014,"Missing"
2020.findings-emnlp.139,P19-1493,0,0.0463596,"Missing"
2020.findings-emnlp.139,D15-1044,0,0.118151,"Missing"
2020.findings-emnlp.139,P15-1150,0,0.0976114,"Missing"
2020.findings-emnlp.139,2020.tacl-1.48,0,0.0794978,"Missing"
2020.findings-emnlp.139,P02-1040,0,\N,Missing
2020.findings-emnlp.163,P19-1544,0,0.302918,"erent intents and ensure that the ratio of sentences has 1-3 intents is [0.3, 0.5, 0.2]. a slot-gated joint model to explicitly consider the Finally, we get the 45,000 utterances for training, correlation between slot filling and intent detection. 3) Bi-Model. Wang et al. (2018) proposes the Bi2,500 utterances for validation and 2500 utterances model to consider the cross-impact between the for testing on the MixSNIPS dataset. Similarly, we construct another multi-intent SLU dataset, Mix- intent detection and slot filling. ATIS, from the ATIS dataset (Hemphill et al., 1990). 4) SF-ID Network. Haihong et al. (2019) proposes There are 18,000 utterances for training, 1,000 ut- an SF-ID network to establish direct connections terances for validation and 1,000 utterances for test- for the slot filling and intent detection to help them promote each other mutually. ing. The constructed datasets have been released 5) Stack-Propagation. Qin et al. (2019) adopts a for future research. joint model with Stack-Propagation to capture the 2 The official DSTC4 pilot tasks’ Handbook intent semantic knowledge and perform the tokenhttp://www.colips.org/workshop/dstc4/ DSTC4_pilot_tasks.pdf level intent detection to furth"
2020.findings-emnlp.163,N19-1055,0,0.347591,"of input vectors X ∈ RT ×d (d represents the mapped dimension) to queries Q, keys K and values V matrices by using different linear projections parameters W q , W k , W v . Attention weight is computed by dot product between Q, K and the self-attention output A ∈ RT ×d is a weighted sum of values:   QK > A = softmax √ V , (1) dk where dk denotes the dimension of keys. We concatenate these two representations as the final encoding representation: E = [H ||A] , (2) where E = {e1 , . . . , eT } ∈ RT ×2d and ||is concatenation operation. 2.2 Intent Detection Decoder We follow Gangadharaiah and Narayanaswamy (2019) to perform multiple intent detection as the multi-label classification problem. We compute the utterance context vector over E = {e1 , . . . , eT } ∈ RT ×2d . In our case, we use a self-attention module (Zhong et al., 2018; Goo et al., 2018) to capture relevant context: pt = softmax(we et + b) , X c= pt et , (3) (4) t where we ∈ R1×2d is the trainable parameters, pt is corresponding normalized self-attention score. c is the weighted sum of each element et and utilized for intent detection: y I = σ(W i (LeakyReLU(W c c+bc ))+bi ) , (5) where W i , W c are trainable parameters of the inI } is t"
2020.findings-emnlp.163,N18-2118,0,0.384193,"he model should directly detect its all intents (PlayMusic and GetWeather). Hence, it is important to consider multi-intent SLU. Unlike the prior single intent SLU model which can simply leverage the utterance’s single intent to guide slot prediction (Goo et al., 2018; Qin et al., 2019), multi-intent SLU faces to multiple intents and presents a unique challenge that is worth studying: how to effectively incorporate multiple intents information to lead the slot prediction. To this end, Gangadharaiah and Narayanaswamy (2019) first explored the multi-task framework with the slot-gated mechanism (Goo et al., 2018) for joint multiple intent detection and slot filling. Their model incorporated intent information by simply treating an intent context vector as multiple intents information. While this is a direct method for incorporating multiple intents information, it does not offer fine-grained intent information integration for token-level slot filling in the sense that each token is guided with the same complex intents information, which is shown in Figure 1(a). In addition, providing the same intent information for all tokens may introduce ambiguity, where it’s hard for each token to capture the relat"
2020.findings-emnlp.163,H90-1021,0,0.305227,"trast to prior work simply incorporate multiple intents information statically where the same intents information is used for guiding all tokens, our intent-slot interaction graph is constructed adaptively with graph attention network over each token. This encourages our model to automatically filter the irrelevant information and capture important intent at the token-level. We first conduct experiments on the multi-intent benchmark dataset DSTC4 (Schuster et al., 2019). Then, to verify the generalization of our framework, we empirically construct two large-scale multiintent datasets MixATIS (Hemphill et al., 1990) and MixSNIPS (Coucke et al., 2018). The results of these experiments show the effectiveness of our framework by outperforming the current state-ofthe-art method. To the best of our knowledge, there are no public large-scale multiple intents datasets and we hope the release of it would push forward the research of multi-intent SLU. In addition, our framework achieves state-of-the-art performance on two public single-intent datasets including ATIS (Tur and De Mori, 2011) and SNIPS (Coucke et al., 2018), which further verifies the generalization of the proposed model. To facilitate future resear"
2020.findings-emnlp.163,D18-1417,0,0.0846973,"work, we jointly perform multi-label intent detection and slot prediction, while they only consider the subtask intent detection. Slot Filling Slot filling can be treated as a sequence labeling task. The popular approaches are conditional random fields (CRF) (Raymond and Riccardi, 2007) and recurrent neural networks (RNN) (Xu and Sarikaya, 2013a; Yao et al., 2014). Recently, Shen et al. (2018) and Tan et al. (2018) introduce the self-attention mechanism for CRF-free sequential labeling. Joint Model To consider the high correlation between intent and slots, many joint models (Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019; Qin et al., 2019) are proposed to solve two tasks. Goo et al. (2018); Li et al. (2018); Zhang et al. (2019) propose to utilize the intent information to guide the slot filling. Qin et al. (2019) further utilize a stack-propagation framework for better leveraging intent semantic information to guide the slot filling, which achieves the state-of-the-art performance. Wang et al. (2018) and E et al. (2019) consider the cross-impact between the slot and intents. Our framework follows those state-of-the-art joint model paradigm, and further focus"
2020.findings-emnlp.163,D19-1097,0,0.17481,"on and slot prediction, while they only consider the subtask intent detection. Slot Filling Slot filling can be treated as a sequence labeling task. The popular approaches are conditional random fields (CRF) (Raymond and Riccardi, 2007) and recurrent neural networks (RNN) (Xu and Sarikaya, 2013a; Yao et al., 2014). Recently, Shen et al. (2018) and Tan et al. (2018) introduce the self-attention mechanism for CRF-free sequential labeling. Joint Model To consider the high correlation between intent and slots, many joint models (Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019; Qin et al., 2019) are proposed to solve two tasks. Goo et al. (2018); Li et al. (2018); Zhang et al. (2019) propose to utilize the intent information to guide the slot filling. Qin et al. (2019) further utilize a stack-propagation framework for better leveraging intent semantic information to guide the slot filling, which achieves the state-of-the-art performance. Wang et al. (2018) and E et al. (2019) consider the cross-impact between the slot and intents. Our framework follows those state-of-the-art joint model paradigm, and further focus on the multiple intents scenario while the above jo"
2020.findings-emnlp.163,D19-1214,1,0.85237,"nces model to consider the cross-impact between the for testing on the MixSNIPS dataset. Similarly, we construct another multi-intent SLU dataset, Mix- intent detection and slot filling. ATIS, from the ATIS dataset (Hemphill et al., 1990). 4) SF-ID Network. Haihong et al. (2019) proposes There are 18,000 utterances for training, 1,000 ut- an SF-ID network to establish direct connections terances for validation and 1,000 utterances for test- for the slot filling and intent detection to help them promote each other mutually. ing. The constructed datasets have been released 5) Stack-Propagation. Qin et al. (2019) adopts a for future research. joint model with Stack-Propagation to capture the 2 The official DSTC4 pilot tasks’ Handbook intent semantic knowledge and perform the tokenhttp://www.colips.org/workshop/dstc4/ DSTC4_pilot_tasks.pdf level intent detection to further alleviate the error 1811 Model Attention BiRNN Slot-Gated Slot-gated Intent Bi-Model SF-ID Stack-Propagation (concatenation) Stack-Propagation (sigmoid-decoder) Joint Multiple ID-SF AGIF Slot (F1) 86.6 88.1 86.7 85.5 87.7 86.6 87.4 87.5 88.1 MixATIS Intent (F1) Intent (Acc) 71.6 65.7 66.2 72.3 63.7 76.0 79.0 71.9 80.6 73.1 81.2* 75.8"
2020.findings-emnlp.163,N18-2050,0,0.296264,"ose an alignment-based RNN with the attention MixSNIPS. MixSNIPS dataset is collected from the Snips personal voice assistant (Coucke et al., mechanism, which implicitly learns the relationship between slot and intent. 2018) by using conjunctions, e.g., “and”, to connect 2) Slot-Gated Atten. Goo et al. (2018) proposes sentences with different intents and ensure that the ratio of sentences has 1-3 intents is [0.3, 0.5, 0.2]. a slot-gated joint model to explicitly consider the Finally, we get the 45,000 utterances for training, correlation between slot filling and intent detection. 3) Bi-Model. Wang et al. (2018) proposes the Bi2,500 utterances for validation and 2500 utterances model to consider the cross-impact between the for testing on the MixSNIPS dataset. Similarly, we construct another multi-intent SLU dataset, Mix- intent detection and slot filling. ATIS, from the ATIS dataset (Hemphill et al., 1990). 4) SF-ID Network. Haihong et al. (2019) proposes There are 18,000 utterances for training, 1,000 ut- an SF-ID network to establish direct connections terances for validation and 1,000 utterances for test- for the slot filling and intent detection to help them promote each other mutually. ing. The"
2020.findings-emnlp.163,D18-1348,0,0.0540665,"ble 4 shows the experiment results of the proposed models on the ATIS and SNIPS datasets. From the table, we can see that our model outperforms all the compared baselines and achieves state-of-the-art performance. This demonstrates the generalizability and effectiveness of our framework whether handling multiintent or single-intent SLU. 4 Related Work Intent Detection Intent detection is formulated as an utterance classification problem. Different classification methods, such as support vector machine (SVM) and RNN (Haffner et al., 2003; Sarikaya et al., 2011), have been proposed to solve it. Xia et al. (2018) adopts a capsule-based neural network with self-attention for intent detection. 1814 However, the above models mainly focus on the single intent scenario, which can not handle the complex multiple intent scenario. Xu and Sarikaya (2013b) and Kim et al. (2017a) explore the complex scenario, where multiple intents are assigned to a user’s utterance. Xu and Sarikaya (2013b) use log-linear models to achieve this, while we use neural network models. Compared with their work, we jointly perform multi-label intent detection and slot prediction, while they only consider the subtask intent detection."
2020.findings-emnlp.163,P19-1519,0,0.0568845,"an be treated as a sequence labeling task. The popular approaches are conditional random fields (CRF) (Raymond and Riccardi, 2007) and recurrent neural networks (RNN) (Xu and Sarikaya, 2013a; Yao et al., 2014). Recently, Shen et al. (2018) and Tan et al. (2018) introduce the self-attention mechanism for CRF-free sequential labeling. Joint Model To consider the high correlation between intent and slots, many joint models (Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019; Qin et al., 2019) are proposed to solve two tasks. Goo et al. (2018); Li et al. (2018); Zhang et al. (2019) propose to utilize the intent information to guide the slot filling. Qin et al. (2019) further utilize a stack-propagation framework for better leveraging intent semantic information to guide the slot filling, which achieves the state-of-the-art performance. Wang et al. (2018) and E et al. (2019) consider the cross-impact between the slot and intents. Our framework follows those state-of-the-art joint model paradigm, and further focus on the multiple intents scenario while the above joint models do not consider. Recently, Gangadharaiah and Narayanaswamy (2019) propose a joint model to conside"
2020.findings-emnlp.163,P18-1135,0,0.057693,"Missing"
2020.findings-emnlp.163,N19-1380,0,0.0200537,"ach token has the ability to capture different relevant intent information so that fine-grained multiple intents integration can be achieved. In contrast to prior work simply incorporate multiple intents information statically where the same intents information is used for guiding all tokens, our intent-slot interaction graph is constructed adaptively with graph attention network over each token. This encourages our model to automatically filter the irrelevant information and capture important intent at the token-level. We first conduct experiments on the multi-intent benchmark dataset DSTC4 (Schuster et al., 2019). Then, to verify the generalization of our framework, we empirically construct two large-scale multiintent datasets MixATIS (Hemphill et al., 1990) and MixSNIPS (Coucke et al., 2018). The results of these experiments show the effectiveness of our framework by outperforming the current state-ofthe-art method. To the best of our knowledge, there are no public large-scale multiple intents datasets and we hope the release of it would push forward the research of multi-intent SLU. In addition, our framework achieves state-of-the-art performance on two public single-intent datasets including ATIS ("
2020.findings-emnlp.262,P00-1037,0,0.592852,"Missing"
2020.findings-emnlp.262,D19-1310,1,0.926342,", which entangle aforementioned stages and generate text directly from structured data through a neural encoder-decoder framework (Wiseman et al., 2017; Nie et al., 2018). As in Fig. 1, this task provides tables with redundant records. Each record has three elements: table row header (entity, e.g. Conley), table column header (type, e.g. points) and table cell (value, e.g. 32). Models are expected to generate descriptive text reflecting salient records. Many neural end-to-end models have achieved remarkable progress of generating fluent and natural text on this task (Puduppully et al., 2019b; Gong et al., 2019). However, previous work notices that the content planning stage is the key factor in table-to-text generation (Gkatzia, 2016), but end-to-end models are difficult to explicitly improve their content planning ability. Recently, Puduppully et al. (2019a) proposed Neural Content Planning (NCP), a twostage model that explicitly selects and orders salient records whilst keeping the ability to generate fluent text of end-to-end models. They show that content planning (referring to both “content selection and planning” in Puduppully et al. (2019a)) indeed correlates with the quality of final output."
2020.findings-emnlp.262,P16-1014,0,0.0301339,"ut text, denoted as r∗ = {r1∗ , . . . , rT∗ } (T is the number of records mentioned in y). Here, we follow Puduppully et al. (2019a) to extract content plans using an information extraction (IE) approach as oracles. In each time step, the decoder takes previously selected record’s representation as input and use the attention weights to select the next important one. In Stage 2 (surface realization), a standard encoder-decoder model is applied, taking the output content plan from Stage 1 as input and generating text with attention mechanism (Luong et al., 2015) and conditional copy mechanism (Gulcehre et al., 2016). From results in Puduppully et al. (2019a), it is observed that performance bottleneck lies in Stage 1. That is, if we feed gold content plans into Stage 2, final results are much better, but if inferred content plans are fed instead, performance decreases drastically. Therefore, we focus on improving NCP’s Stage 1 for better final outputs. 2906 Grizzlies Player PTS AST s1 r2cs r3cs … … … … Mean Pooling … r3cs rjcs rjcs cs cs cs y3 rics Attention +Copy p 112 Value Score r3cs r1cs rics … h1 h2 h3 h4 … rs r3cs r1cs rics Entity Importance … r2 r3 rj Reference Record Sequences 1. Contextual Numer"
2020.findings-emnlp.262,P83-1022,0,0.484569,"xample with NCP’s result and gold text. Important/unimportant entities and records are in red/blue. Text that accurately/incorrectly report statistics in table is in bold/italic. Introduction Table-to-text generation refers to the task of generating text from structured data. Models for this task can be mainly categorized into two types: pipeline-style models, which decompose the generation process into sequential stages, including content planning (Stage 1, selecting and ordering salient content from the input) and surface realization (Stage 2, converting the content plan to surface string) (Kukich, 1983; McKeown, 1985); and end-to-end models, which entangle aforementioned stages and generate text directly from structured data through a neural encoder-decoder framework (Wiseman et al., 2017; Nie et al., 2018). As in Fig. 1, this task provides tables with redundant records. Each record has three elements: table row header (entity, e.g. Conley), table column header (type, e.g. points) and table cell (value, e.g. 32). Models are expected to generate descriptive text reflecting salient records. Many neural end-to-end models have achieved remarkable progress of generating fluent and natural text o"
2020.findings-emnlp.262,N16-1014,0,0.031493,"r training contextual numerical value representations, we use the hinge loss (Eq.5). ξ is the margin and T (·) gives +1 if · is true and −1 otherwise. `pre = • Entity Importance (EI) evaluates if a predicted record rt contains an important entity by comparing whether the entity is mentioned in the gold content plan {ri∗ }. R(·) function gives +1 reward when · is true and -1 otherwise. Content Planning Verification The original NCP uses the pointer network to explicitly infer a content plan by optimizing the MLE of gold content plans. As noticed in other generation tasks (Sordoni et al., 2015; Li et al., 2016a; Dai et al., 2017), generation models with the MLE as the objective function tend to generate universal output sequences observed in the training data and it is desirable to integrate developer-defined rewards that better mimic the true goal of an ideal output sequence (Li et al., 2016b), which is the sequence of the content plan in our task. In order to explicitly reflect the quality of content plans, we explore rewards that measure the following five criteria, and optimized the model according to them via policy gradient (Sutton and Barto, 1998). (6) • Entity Recall (ER) measures how many"
2020.findings-emnlp.262,D16-1127,0,0.0183659,"r training contextual numerical value representations, we use the hinge loss (Eq.5). ξ is the margin and T (·) gives +1 if · is true and −1 otherwise. `pre = • Entity Importance (EI) evaluates if a predicted record rt contains an important entity by comparing whether the entity is mentioned in the gold content plan {ri∗ }. R(·) function gives +1 reward when · is true and -1 otherwise. Content Planning Verification The original NCP uses the pointer network to explicitly infer a content plan by optimizing the MLE of gold content plans. As noticed in other generation tasks (Sordoni et al., 2015; Li et al., 2016a; Dai et al., 2017), generation models with the MLE as the objective function tend to generate universal output sequences observed in the training data and it is desirable to integrate developer-defined rewards that better mimic the true goal of an ideal output sequence (Li et al., 2016b), which is the sequence of the content plan in our task. In order to explicitly reflect the quality of content plans, we explore rewards that measure the following five criteria, and optimized the model according to them via policy gradient (Sutton and Barto, 1998). (6) • Entity Recall (ER) measures how many"
2020.findings-emnlp.262,C18-1089,0,0.161244,"Missing"
2020.findings-emnlp.262,D15-1166,0,0.0227895,"equence of important records extracted from the output text, denoted as r∗ = {r1∗ , . . . , rT∗ } (T is the number of records mentioned in y). Here, we follow Puduppully et al. (2019a) to extract content plans using an information extraction (IE) approach as oracles. In each time step, the decoder takes previously selected record’s representation as input and use the attention weights to select the next important one. In Stage 2 (surface realization), a standard encoder-decoder model is applied, taking the output content plan from Stage 1 as input and generating text with attention mechanism (Luong et al., 2015) and conditional copy mechanism (Gulcehre et al., 2016). From results in Puduppully et al. (2019a), it is observed that performance bottleneck lies in Stage 1. That is, if we feed gold content plans into Stage 2, final results are much better, but if inferred content plans are fed instead, performance decreases drastically. Therefore, we focus on improving NCP’s Stage 1 for better final outputs. 2906 Grizzlies Player PTS AST s1 r2cs r3cs … … … … Mean Pooling … r3cs rjcs rjcs cs cs cs y3 rics Attention +Copy p 112 Value Score r3cs r1cs rics … h1 h2 h3 h4 … rs r3cs r1cs rics Entity Importance …"
2020.findings-emnlp.262,P19-1329,0,0.0628395,"es when decoding texts. Different from them, we model numerical values during encoding. Iso et al. (2019) incorporate writers’ information to generate text step-by-step. Our work can also consider such information in surface realization (Stage 2). For a fair comparison of all methods, we do not include the use of this model here. Gong et al. (2019) utilize hierarchical encoders with dual attention to consider both the table structure and history information. In terms of building numerical value representations, Spithourakis and Riedel (2018) explore number prediction for language models while Naik et al. (2019) explore numerical embeddings to capture the numeration and magnitude properties of numbers. In our task, generation models rely heavily on copy mechanism to cover numerical values in text and achieve good results. Thus, how to understand numerical values to select records becomes important and we propose to understand them through their context. 6 Conclusion In order to enhance neural content planning for table-to-text generation, we proposed (1) contextual numerical value representations to help model understand data values and (2) effective rewards 2912 to verify a model’s inferred importan"
2020.findings-emnlp.262,D18-1422,0,0.082854,"t generation refers to the task of generating text from structured data. Models for this task can be mainly categorized into two types: pipeline-style models, which decompose the generation process into sequential stages, including content planning (Stage 1, selecting and ordering salient content from the input) and surface realization (Stage 2, converting the content plan to surface string) (Kukich, 1983; McKeown, 1985); and end-to-end models, which entangle aforementioned stages and generate text directly from structured data through a neural encoder-decoder framework (Wiseman et al., 2017; Nie et al., 2018). As in Fig. 1, this task provides tables with redundant records. Each record has three elements: table row header (entity, e.g. Conley), table column header (type, e.g. points) and table cell (value, e.g. 32). Models are expected to generate descriptive text reflecting salient records. Many neural end-to-end models have achieved remarkable progress of generating fluent and natural text on this task (Puduppully et al., 2019b; Gong et al., 2019). However, previous work notices that the content planning stage is the key factor in table-to-text generation (Gkatzia, 2016), but end-to-end models ar"
2020.findings-emnlp.262,P02-1040,0,0.107318,"valuation Metrics We conducted experiments on both ROTOWIRE1 and MLB (Puduppully et al., 2019b) dataset. The former provides pairs of NBA game statistics and summary. Differently, the latter provides summary and heterogeneous input, consisting of MLB game statistics and event data (including event type, actors, etc.) in chronological order. For ROTOWIRE, we follow official training, development and test splits of 3398/727/728 instances. For MLB, as the contents are not released, we are able to retrieve a split of 22820/1739/1744 instances via official scripts 2 . For evaluations, we use BLEU (Papineni et al., 2002) and three extractive metrics, which evaluate the generated results from the following aspects: (1) Relation Generation (RG), measuring the text fidelity about whether to describe information from table truthfully. (2) Content Selection (CS) to measure whether important information is selected from redundant game statistics. (3) Content Ordering (CO) evaluates a model’s ability to plan and order data records naturally in text. More details can be found in Wiseman et al. (2017). Implementation Details We follow Puduppully et al. (2019a)’s and Puduppully et al. (2019b)’s training configurations"
2020.findings-emnlp.262,P19-1195,0,0.422684,"85); and end-to-end models, which entangle aforementioned stages and generate text directly from structured data through a neural encoder-decoder framework (Wiseman et al., 2017; Nie et al., 2018). As in Fig. 1, this task provides tables with redundant records. Each record has three elements: table row header (entity, e.g. Conley), table column header (type, e.g. points) and table cell (value, e.g. 32). Models are expected to generate descriptive text reflecting salient records. Many neural end-to-end models have achieved remarkable progress of generating fluent and natural text on this task (Puduppully et al., 2019b; Gong et al., 2019). However, previous work notices that the content planning stage is the key factor in table-to-text generation (Gkatzia, 2016), but end-to-end models are difficult to explicitly improve their content planning ability. Recently, Puduppully et al. (2019a) proposed Neural Content Planning (NCP), a twostage model that explicitly selects and orders salient records whilst keeping the ability to generate fluent text of end-to-end models. They show that content planning (referring to both “content selection and planning” in Puduppully et al. (2019a)) indeed correlates with the qua"
2020.findings-emnlp.262,N15-1020,0,0.0233429,"her than f (˜ rj ). For training contextual numerical value representations, we use the hinge loss (Eq.5). ξ is the margin and T (·) gives +1 if · is true and −1 otherwise. `pre = • Entity Importance (EI) evaluates if a predicted record rt contains an important entity by comparing whether the entity is mentioned in the gold content plan {ri∗ }. R(·) function gives +1 reward when · is true and -1 otherwise. Content Planning Verification The original NCP uses the pointer network to explicitly infer a content plan by optimizing the MLE of gold content plans. As noticed in other generation tasks (Sordoni et al., 2015; Li et al., 2016a; Dai et al., 2017), generation models with the MLE as the objective function tend to generate universal output sequences observed in the training data and it is desirable to integrate developer-defined rewards that better mimic the true goal of an ideal output sequence (Li et al., 2016b), which is the sequence of the content plan in our task. In order to explicitly reflect the quality of content plans, we explore rewards that measure the following five criteria, and optimized the model according to them via policy gradient (Sutton and Barto, 1998). (6) • Entity Recall (ER) m"
2020.findings-emnlp.262,P18-1196,0,0.0796316,"he content planning. Puduppully et al. (2019b) propose to specifically model entities when decoding texts. Different from them, we model numerical values during encoding. Iso et al. (2019) incorporate writers’ information to generate text step-by-step. Our work can also consider such information in surface realization (Stage 2). For a fair comparison of all methods, we do not include the use of this model here. Gong et al. (2019) utilize hierarchical encoders with dual attention to consider both the table structure and history information. In terms of building numerical value representations, Spithourakis and Riedel (2018) explore number prediction for language models while Naik et al. (2019) explore numerical embeddings to capture the numeration and magnitude properties of numbers. In our task, generation models rely heavily on copy mechanism to cover numerical values in text and achieve good results. Thus, how to understand numerical values to select records becomes important and we propose to understand them through their context. 6 Conclusion In order to enhance neural content planning for table-to-text generation, we proposed (1) contextual numerical value representations to help model understand data valu"
2020.findings-emnlp.262,D17-1239,0,0.0378658,"Missing"
2020.findings-emnlp.58,C10-3004,1,0.668986,"f the whole word, which is easier for the model to predict. In Chinese condition, WordPiece tokenizer no longer split the word into small fragments, as Chinese characters are not formed by alphabet-like symbols. We use the traditional Chinese Word Segmentation (CWS) tool to split the text into several words. In this way, we could adopt whole word masking in Chinese to mask the word instead of individual Chinese characters. For implementation, we strictly followed the original whole word masking codes and did not change other components, such as the percentage of word masking, etc. We use LTP (Che et al., 2010) for Chinese word segmentation to identify the word boundaries. 659 Chinese English Original Sentence + CWS + BERT Tokenizer 使用语言模型来预测下一个词的概率。 使用 语言 模型 来 预测 下 一个 词 的 概率 。 使用语言模型来预测下一个词的概率。 we use a language model to predict the probability of the next word. we use a language model to pre ##di ##ct the pro ##ba ##bility of the next word . Original Masking + WWM ++ N-gram Masking +++ Mac Masking 使 用 语 言 [M] 型 来 [M] 测 下 一 个 词 的 概 率 。 使 用 语 言 [M] [M] 来 [M] [M] 下 一 个 词 的 概 率 。 使 用 [M] [M] [M] [M] 来 [M] [M] 下 一 个 词 的 概 率 。 使用语法建模来预见下一个词的几率。 we use a language [M] to [M] ##di ##ct the pro [M] ##bility"
2020.findings-emnlp.58,D18-1536,0,0.0401552,"on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of results. The best initial learning rate is determined by selecting the best average development set performance. We report the maximum and average scores to bot"
2020.findings-emnlp.58,D18-1241,0,0.0205389,"could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), SpanBERT (Joshi e"
2020.findings-emnlp.58,D18-1269,0,0.0475474,"rained language models, we carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of results. The best initial learning rate is determined by selecting the best average development set per"
2020.findings-emnlp.58,D19-1600,1,0.93983,"on both left and right context in all Transformer layers. Primarily, BERT consists of two pre-training tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). Later, they further proposed a technique called whole word masking (wwm) for optimizing the original masking in the MLM task. In this setting, instead of randomly selecting WordPiece (Wu et al., 2016) tokens to mask, we always mask all of the tokens corresponding to a whole word at once. This will explicitly force the model to recover the whole word in the MLM pre-training task instead of just recovering WordPiece tokens (Cui et al., 2019a), which is much more challenging. As the whole word masking only affects the masking strategy of the pre-training process, it would not bring additional burdens on down-stream tasks. Moreover, as training pre-trained language models are computationally expensive, they also release all the pretrained models as well as the source codes, which stimulates the community to have great interests in the research of pre-trained language models. 2.2 ERNIE ERNIE (Enhanced Representation through kNowledge IntEgration) (Sun et al., 2019a) is designed to optimize the masking process of BERT, which include"
2020.findings-emnlp.58,P19-1285,0,0.0713452,"Missing"
2020.findings-emnlp.58,N19-1423,0,0.480575,"he community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrat"
2020.findings-emnlp.58,Q19-1026,0,0.0190409,"ances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2019), ALBERT (Lan et al., 2019), ELEC"
2020.findings-emnlp.58,D17-1082,0,0.0475345,"lso ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2019), ALBERT (Lan et al., 2019), ELECTRA (Clark et al., 2020),"
2020.findings-emnlp.58,D07-1081,0,0.024255,"4.2 Setups for Fine-tuning Tasks To thoroughly test these pre-trained language models, we carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of results. The best initial learning r"
2020.findings-emnlp.58,C18-1166,0,0.0996317,"Missing"
2020.findings-emnlp.58,2021.ccl-1.108,0,0.114968,"Missing"
2020.findings-emnlp.58,P18-2124,0,0.0291812,"proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al.,"
2020.findings-emnlp.58,D16-1264,0,0.0704602,"019), we only compare BERT (Devlin et al., 2019), BERT-wwm, BERT-wwm-ext, RoBERTawwm-ext, RoBERTa-wwm-ext-large, ELECTRA, along with our MacBERT to ensure relatively fair comparisons among different models, where all models are trained by ourselves except for the original Chinese BERT by Devlin et al. (2019). We carried out experiments under TensorFlow framework (Abadi et al., 2016) with slight modifications to the fine-tuning scripts6 provided by Devlin et al. (2019) to better adapt to Chinese. 5 • CMRC 2018: A span-extraction machine reading comprehension dataset, which is similar to SQuAD (Rajpurkar et al., 2016) that extract a passage span for the given question. • DRCD: This is also a span-extraction MRC dataset but in Traditional Chinese. • CJRC: Similar to CoQA (Reddy et al., 2019), which has yes/no questions, no-answer questions, and span-extraction questions. The data is collected from Chinese law judgment documents. Note that we only use small-train-data.json for training. F1 EM F1 BERT BERT-wwm BERT-wwm-ext RoBERTa-wwm-ext ELECTRA-base MacBERT-base 83.1 (82.7) 84.3 (83.4) 85.0 (84.5) 86.6 (85.9) 87.5 (87.0) 89.4 (89.2) 89.9 (89.6) 90.5 (90.2) 91.2 (90.9) 92.5 (92.2) 92.5 (92.3) 94.3 (94.1) 82."
2020.findings-emnlp.58,Q19-1016,0,0.0917529,"results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al."
2020.findings-emnlp.58,L18-1431,1,0.872933,"or clarity, we do not list ‘ext’ models, where the other parameters are the same with the one that is not trained on extended data. 4.2 Setups for Fine-tuning Tasks To thoroughly test these pre-trained language models, we carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the"
2020.nlptea-1.5,W13-4414,0,0.0315376,"de gap for our system to solve the Chinese grammatical error diagnosis. Table 5 shows the performances on error correction. We achieve the second-highest correction top1 score. Since we only provide zero or one candidate word, our correction top1 score is the same as our correction top3 score. 5 Related Work The researchers used many different methods to study the English Grammatical Error Correction task and achieved good results (Ng et al., 2014). Compared with English, the research time of Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of e"
2020.nlptea-1.5,W18-3707,1,0.822656,"ility of RoBERTa and n-gram, visual similarity, and phonological similarity (Hong et al., 2019). Afterward, we select the character with the highest score as the correction result. For the multi-character correction, we also select the top 20 characters generated by RoBERTa at each position. We put these characters together to form words and reserved those in the vocabulary as candidates. In addition to the four kinds of features at the single-character correction, we also consider Levenshtein distance between the error words and candidate words. 4 4.1 Experiment Dataset Following the work of Fu et al. (2018), We trained our single models using training units that contain both the erroneous and the corrected sentences from 2016 (HSK Track), 2017 and 2018 training data sets. CGED 2016 HSK track training set consists of 10,071 training units with a total of 24,797 grammatical errors, categorized as redundant (5,538 instances), missing (6,623), word selection (10,949) and word ordering (1,687). CGED 2017 training set consists of 10,449 training units 39 model FPR Detection level Identification Position Precision Recall F1 Precision Recall F1 Precision Recall F1 BERT 0.6333 0.6974 0.8626 0.7713 0.5406"
2020.nlptea-1.5,W04-1213,0,0.14189,"edundant words (R), word selection errors (S) and word ordering errors (W). The input sentence may contain one or more such errors. Given a sentence, the system needs to indicate: (1) If the sentence is correct or not; (2) What kind of errors the sentence contains; (3) The exact error position; (4) Possible corrections for S-type and M-type errors. Some typical examples are shown in Table 1. 3 3.1 Methodology Error Detection We treat the error detection problem as a sequence tagging problem. Specifically, given a sentence x, we generate a corresponding label sequence y using the BIO encoding (Kim et al., 2004). We then combine ResNet and transformer encoder to solve the tagging problem. h0i = We wi + Wp (1) hli = transformer block(hl−1 i ) (2) yiBERT = softmax(Wo hL i + bo ) (3) where wi is a current token, and N denotes the sequence length. Equation 1 thus creates an input embedding. Here, transformer block includes selfattention and fully connected layers, and outputs 37 hli . l is the number of the current layer, l ≥ 1. L is the total number of layers of BERT. Equation 3 denotes the output layer. Wo is an output weight matrix, bo is a bias for the output layer, and yiBERT is a grammatical error"
2020.nlptea-1.5,P17-1080,0,0.0310529,"r of the current layer, l ≥ 1. L is the total number of layers of BERT. Equation 3 denotes the output layer. Wo is an output weight matrix, bo is a bias for the output layer, and yiBERT is a grammatical error detection prediction. and dropout values. The basic ensemble selection procedure is very simple: 1. Start with the empty ensemble. 2. Add to the ensemble the model in the library that maximizes the ensemble’s performance to the Chinese grammatical error detection metric on validation set. Integrating ResNet Deep neural networks learn different representations for each layer. For example, Belinkov et al. (2017) demonstrated that in a machine translation task, the low layers of the network learn to represent the word structure, while higher layers are more focused on word meaning. For tasks that emphasize the grammatical nature such as Chinese grammatical error detection, information from the lower layers is considered to be important. In this work, we use the residual learning framework (He et al., 2016) to combine the information from word embedding with the information from deep layer. Given a sequence S = w0 , ......, wN as input, ResBERT is formulated as follows: h0i = We wi + Wp (4) hli = trans"
2020.nlptea-1.5,W16-4906,0,0.052228,"Missing"
2020.nlptea-1.5,I17-4012,0,0.0280461,"o improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing Results Table 4 shows the performances on error detection. Our system achieves the best F1 scores at the identification level and position level. Although we achieve the highest position-level F1 score of 41 the probability of each characters, and used two strategies to decide whether a character is correct or not. Liao et al. (2017) used the LSTM-CRF model to detect dependencies between outputs to better detect error messages. Yang et al. (2017) added more linguistic information on LSTM-CRF model such as POS, n-gram, PMI score and dependency features. Their s"
2020.nlptea-1.5,W16-4907,1,0.770855,"on task and achieved good results (Ng et al., 2014). Compared with English, the research time of Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing Results Table 4 shows the performances on error detection. Our system achieves the best F1 scores at the identification level and position level. Although we achieve the highest position-level F1 score of 41 the probability of each characters, and used two strategies to decide whether a"
2020.nlptea-1.5,I17-4011,0,0.0273139,"ased on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing Results Table 4 shows the performances on error detection. Our system achieves the best F1 scores at the identification level and position level. Although we achieve the highest position-level F1 score of 41 the probability of each characters, and used two strategies to decide whether a character is correct or not. Liao et al. (2017) used the LSTM-CRF model to detect dependencies between outputs to better detect error messages. Yang et al. (2017) added more linguistic information on LSTM-CRF model such as POS, n-gram, PMI score and dependency features. Their system achieved the best F1-scores in identification level and position level on CGED2017 task. Fu et al. (2018) added richer features on BiLSTM-CRF model such as word segmentation, Gaussian ePMI, combination of POS and PMI. They also adopted a probabilistic ensemble approach to improve system performance. Their system achieved the best F1-score in identification leve"
2020.nlptea-1.5,O15-2003,0,0.0196646,"rection. We achieve the second-highest correction top1 score. Since we only provide zero or one candidate word, our correction top1 score is the same as our correction top3 score. 5 Related Work The researchers used many different methods to study the English Grammatical Error Correction task and achieved good results (Ng et al., 2014). Compared with English, the research time of Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing R"
2020.nlptea-1.5,2021.ccl-1.108,0,0.0988146,"Missing"
2020.nlptea-1.5,W14-1701,0,0.0181731,"ompared with WA ensemble model, the stepwise selection ensemble model also achieves more than 4 point improvements. 4.5 0.4041 among all teams, there still has a wide gap for our system to solve the Chinese grammatical error diagnosis. Table 5 shows the performances on error correction. We achieve the second-highest correction top1 score. Since we only provide zero or one candidate word, our correction top1 score is the same as our correction top3 score. 5 Related Work The researchers used many different methods to study the English Grammatical Error Correction task and achieved good results (Ng et al., 2014). Compared with English, the research time of Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model base"
2020.nlptea-1.5,P17-2064,0,0.0173616,"Chinese grammatical error diagnosis system is short, the data sets and effective methods are lacking. Chen et al. (2013) still used n-gram as the main method, and added Web resources to improve detection performance. Lin and Chu (2015) established a scoring system using n-gram, and get better correction options. In recent years, Chinese grammatical error diagnosis has been cited as a shared task of NLPTEA CGED. Many methods are proposed to solve this task (Yu et al., 2014; Lee et al., 2015, 2016). Zheng et al. (2016) proposed a BiLSTMCRF model based on character embedding on bigram embedding. Shiue et al. (2017) combined machine learning with traditional n-gram methods, using Bi-LSTM to detect the location of errors and adding additional linguistic information, POS, ngram. Li et al. (2017) used Bi-LSTM to generate Testing Results Table 4 shows the performances on error detection. Our system achieves the best F1 scores at the identification level and position level. Although we achieve the highest position-level F1 score of 41 the probability of each characters, and used two strategies to decide whether a character is correct or not. Liao et al. (2017) used the LSTM-CRF model to detect dependencies be"
2020.nlptea-1.5,I17-4006,0,0.0445756,"Missing"
2020.semeval-1.43,D14-1181,0,0.00240485,"pervised fashion. Basically, each pre-trained language model is trained in a supervised fashion with labeled data. For unlabeled data, Pseudo-Labels, created by just picking up the class which has more votes from pre-trained language models, are used as if they were true labels. 2 Related Work Subtask 1 is a text classification task. Traditional machine learning systems, such as Naive Bayes (Domingos and Pazzani, 1997) and Support Vector Machines (Cortes and Vapnik, 1995) perform well in this task. In recent years, text classification has made breakthroughs in deep learning. A few of studies (Kim, 2014; Liu et al., 2016; Lai et al., 2015) made a series of improvements to the structure of deep neural networks. BERT (Devlin et al., 2018), a dynamic word vector based on the language model, has achieved very competitive performance in multiple domains of text classification. In addition, pre-trained language models such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional R"
2020.semeval-1.43,N16-1030,0,0.0301246,"dels such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, including BERT, RoBERTa and XLNet. Then, we enlarge the training set with pseudo-labeled sentences, which are predicted on the test set by voting ensemble of several single pre-trained models. Finally, we use the ensemble model to classify counterfactual statements fo"
2020.semeval-1.43,D15-1104,0,0.0193744,"ts to the structure of deep neural networks. BERT (Devlin et al., 2018), a dynamic word vector based on the language model, has achieved very competitive performance in multiple domains of text classification. In addition, pre-trained language models such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, including BERT, RoBE"
2020.semeval-1.43,P16-1101,0,0.0215048,"ang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, including BERT, RoBERTa and XLNet. Then, we enlarge the training set with pseudo-labeled sentences, which are predicted on the test set by voting ensemble of several single pre-trained models. Finally, we use the ensemble model to classify counterfactual statements for subtask 1 and uti"
2020.semeval-1.43,W14-1609,0,0.0147325,"series of improvements to the structure of deep neural networks. BERT (Devlin et al., 2018), a dynamic word vector based on the language model, has achieved very competitive performance in multiple domains of text classification. In addition, pre-trained language models such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, i"
2020.semeval-1.43,P17-1161,0,0.0132424,"RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, including BERT, RoBERTa and XLNet. Then, we enlarge the training set with pseudo-labeled sentences, which are predicted on the test set by voting ensemble of several single pre-trained models. Finally, we use the ensemble model to classify counterfactual statements for subtask 1 and utilize a CRF model to pe"
2020.semeval-1.43,W09-1119,0,0.0177257,"Lai et al., 2015) made a series of improvements to the structure of deep neural networks. BERT (Devlin et al., 2018), a dynamic word vector based on the language model, has achieved very competitive performance in multiple domains of text classification. In addition, pre-trained language models such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrain"
2020.semeval-1.43,2020.semeval-1.40,0,0.106702,"rning systems’ lack of understanding of causal relations is perhaps the biggest roadblock to giving them human-level intelligence (Pearl, 2019). Causation is a fundamental concept in human thinking and reasoning, which indicates a special semantic relation between the cause with the effect (Stukker et al., 2008). Pearl and Mackenzie (2018) propose that there are three levels of causation, and the top level is the counterfactual analysis. Counterfactual statements describe events that did not actually happen or cannot happen, as well as the possible consequence if the events have had happened (Yang et al., 2020). SemEval 2020 Task 5 consists of two subtasks which aim to detect counterfactual descriptions in sentences, this paper presents solutions to both of two subtasks. Subtask1 — Detecting counterfactual statements refers to determining whether a given statement is counterfactual or not in several domains. For example, given the sentence: “Her post-traumatic stress could have been avoided if a combination of paroxetine and exposure therapy had been prescribed two months earlier.”, the system is required to determine whether it is a counterfactual statement or not. This categorization task is evalu"
2021.acl-demo.29,D18-1183,1,0.885658,"Missing"
2021.acl-demo.29,W19-4410,0,0.0262178,"Missing"
2021.acl-demo.29,D13-1180,0,0.0298588,"s can benefit both teachers and students during the process of writing teaching and learning. 1 Introduction Automated essay assessment (AEA) is an important educational application (Page, 1968; Rudner et al., 2006). It aims to reduce the burden of teachers for scoring student essays and give students direct instructions to improve their writing ability. Automated essay scoring (AES) is one of the most important modules for AEA, which is usually formulated as a supervised learning problem. The early approaches utilized hand-crafted features to predict essay scores (Yannakoudakis et al., 2011; Chen and He, 2013; Phandi et al., 2015). Recently, deep learning has been applied to AES as well (Taghipour and Ng, 2016; Dong et al., 2017; Song et al., 2020c). One issue about AES is that its prediction lacks explainability since a single score gives very limited information. Many efforts have been paid to • IFlyEA has comprehensive multi-level and multi-dimension analytical modules. It provides state-of-the-art Chinese spelling error correction and grammatical error diagnosis at grammar level. More specially, it also provides rich rhetoric and discourse level analysis, which are less studied but important f"
2021.acl-demo.29,N19-1423,0,0.00527643,"xt: We train a 5-gram language model LM on a large-scale corpus. For each character in a sentence, we substitute it with its corresponding F1 0.649 0.664 0.651 confusion characters one by one, and use LM to compute perplexity. If any confusion character leads to a lower perplexity than the original one by a pre-defined threshold, it would be retained as a correction candidate. After state 1, we obtain a small list of correction candidates. This stage can be processed very fast. Stage 2: Correction candidate reranking with global context: We further use the masked language model MLM from BERT (Devlin et al., 2019) to take advantage of the pre-trained transformer based language model and exploit the whole sentence as context to rerank the correction candidates at different positions, respectively. We evaluate our system on the SIGHAN 2015 benchmark. As shown in Table 1, the results demonstrate that our system can obtain competitive results to state-of-the-art methods, although it is unsupervised. Correctly using words is a fundamental requirement for effective writing. Grammar-level analysis would try to detect spelling and grammatical errors in essays, and highlight detected errors as a reminder. 3.1.1"
2021.acl-demo.29,W18-3707,1,0.880601,"Missing"
2021.acl-demo.29,W13-4409,0,0.0222075,"nd also improving the explainability for the predictions of application modules. As illustrated in Figure 2, through web page visualization 241 Figure 2: The visualization and interactive interfaces of IFlyEA. and interfaces, students or teachers can receive rich information and interact with the analytical results. 3 Model Wang et al. (2019) Zhang et al. (2020) Ours Analytical Modules Grammar-level Analysis Spelling Error Correction Given a sentence, our spelling checker would locate spelling errors if there is any, and provide a list of corrected candidates (Tseng et al., 2015). Inspired by Liu et al. (2013); Yu and Li (2014), we establish a confusion-set based unsupervised two-stage method to detect and correct spelling errors. Confusion set: A confusion set is built to group characters with similar pronunciation or graphemic into clusters. We implement it with an inverted indexing structure so that given a target character, we can quickly get a list of confusion characters from the same cluster. Stage 1: Correction candidate detection with local context: We train a 5-gram language model LM on a large-scale corpus. For each character in a sentence, we substitute it with its corresponding F1 0.64"
2021.acl-demo.29,W14-1701,0,0.0113003,"Fu† , Zhichao Sheng¶ , Bo Zhu¶ , Shijin Wang†¶£ , Ting Liu§ State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, Beijing, China ‡ Academy for Multidisciplinary Studies, Capital Normal University, Beijing, China ¶ iFLYTEK AI Research (Hefei), China £ iFLYTEK AI Research (Hebei), LangFang, China § Research Center for SCIR, Harbin Institute of Technology, Harbin, China {jfgong, xiaohu2, rjfu, zcsheng, bozhu, sjwang3}@iflytek.com, wsong@cnu.edu.cn, tliu@ir.hit.edu.cn expand the boundary of AES, and try to analyze Abstract † detailed linguistic properties, such as grammatical errors (Ng et al., 2014), coherence (Somasundaran et al., 2014), organization (Burstein et al., 2003; Persing et al., 2010) and so on. Several AES systems, such as E-Rater (Attali and Burstein, 2006) and Lingglewrite (Tsai et al., 2020), have been successfully applied in the education scenario. However, many of them focus on evaluating second-language learners’ writing ability or evaluating basic language usages depending on shallow features, which may be not sufficient for evaluating essays written by native speakers. Moreover, most existing platforms mainly target on English, while there are significantly fewer sys"
2021.acl-demo.29,D10-1023,0,0.241919,"Intelligence, iFLYTEK Research, Beijing, China ‡ Academy for Multidisciplinary Studies, Capital Normal University, Beijing, China ¶ iFLYTEK AI Research (Hefei), China £ iFLYTEK AI Research (Hebei), LangFang, China § Research Center for SCIR, Harbin Institute of Technology, Harbin, China {jfgong, xiaohu2, rjfu, zcsheng, bozhu, sjwang3}@iflytek.com, wsong@cnu.edu.cn, tliu@ir.hit.edu.cn expand the boundary of AES, and try to analyze Abstract † detailed linguistic properties, such as grammatical errors (Ng et al., 2014), coherence (Somasundaran et al., 2014), organization (Burstein et al., 2003; Persing et al., 2010) and so on. Several AES systems, such as E-Rater (Attali and Burstein, 2006) and Lingglewrite (Tsai et al., 2020), have been successfully applied in the education scenario. However, many of them focus on evaluating second-language learners’ writing ability or evaluating basic language usages depending on shallow features, which may be not sufficient for evaluating essays written by native speakers. Moreover, most existing platforms mainly target on English, while there are significantly fewer systems working on other languages, such as Chinese. In this paper, we introduce the IFlyEssayAssess ("
2021.acl-demo.29,D15-1049,0,0.018818,"teachers and students during the process of writing teaching and learning. 1 Introduction Automated essay assessment (AEA) is an important educational application (Page, 1968; Rudner et al., 2006). It aims to reduce the burden of teachers for scoring student essays and give students direct instructions to improve their writing ability. Automated essay scoring (AES) is one of the most important modules for AEA, which is usually formulated as a supervised learning problem. The early approaches utilized hand-crafted features to predict essay scores (Yannakoudakis et al., 2011; Chen and He, 2013; Phandi et al., 2015). Recently, deep learning has been applied to AES as well (Taghipour and Ng, 2016; Dong et al., 2017; Song et al., 2020c). One issue about AES is that its prediction lacks explainability since a single score gives very limited information. Many efforts have been paid to • IFlyEA has comprehensive multi-level and multi-dimension analytical modules. It provides state-of-the-art Chinese spelling error correction and grammatical error diagnosis at grammar level. More specially, it also provides rich rhetoric and discourse level analysis, which are less studied but important for evaluating native s"
2021.acl-demo.29,W18-3706,0,0.0605047,"Missing"
2021.acl-demo.29,2020.nlptea-1.4,0,0.080277,"Missing"
2021.acl-demo.29,C14-1090,0,0.0309916,"u¶ , Shijin Wang†¶£ , Ting Liu§ State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, Beijing, China ‡ Academy for Multidisciplinary Studies, Capital Normal University, Beijing, China ¶ iFLYTEK AI Research (Hefei), China £ iFLYTEK AI Research (Hebei), LangFang, China § Research Center for SCIR, Harbin Institute of Technology, Harbin, China {jfgong, xiaohu2, rjfu, zcsheng, bozhu, sjwang3}@iflytek.com, wsong@cnu.edu.cn, tliu@ir.hit.edu.cn expand the boundary of AES, and try to analyze Abstract † detailed linguistic properties, such as grammatical errors (Ng et al., 2014), coherence (Somasundaran et al., 2014), organization (Burstein et al., 2003; Persing et al., 2010) and so on. Several AES systems, such as E-Rater (Attali and Burstein, 2006) and Lingglewrite (Tsai et al., 2020), have been successfully applied in the education scenario. However, many of them focus on evaluating second-language learners’ writing ability or evaluating basic language usages depending on shallow features, which may be not sufficient for evaluating essays written by native speakers. Moreover, most existing platforms mainly target on English, while there are significantly fewer systems working on other languages, such a"
2021.acl-demo.29,D15-1270,1,0.886352,"Missing"
2021.acl-demo.29,C16-1076,1,0.895604,"Missing"
2021.acl-demo.29,2020.emnlp-main.225,1,0.661656,"is an important educational application (Page, 1968; Rudner et al., 2006). It aims to reduce the burden of teachers for scoring student essays and give students direct instructions to improve their writing ability. Automated essay scoring (AES) is one of the most important modules for AEA, which is usually formulated as a supervised learning problem. The early approaches utilized hand-crafted features to predict essay scores (Yannakoudakis et al., 2011; Chen and He, 2013; Phandi et al., 2015). Recently, deep learning has been applied to AES as well (Taghipour and Ng, 2016; Dong et al., 2017; Song et al., 2020c). One issue about AES is that its prediction lacks explainability since a single score gives very limited information. Many efforts have been paid to • IFlyEA has comprehensive multi-level and multi-dimension analytical modules. It provides state-of-the-art Chinese spelling error correction and grammatical error diagnosis at grammar level. More specially, it also provides rich rhetoric and discourse level analysis, which are less studied but important for evaluating native speakers’ writing ability. • Based on the information provided by the analytical modules, IFlyEA provides a complete set"
2021.acl-demo.29,P17-1011,1,0.845767,"is even more difficult, since narrative text understanding is still very challenging and open in both theory and practice. IFlyEA uses discourse modes as discourse elements influenced by (Smith, 2003). The main reasons are: (1) discourse modes can represent the essay organization by segmenting an essay into discourse mode zones; (2) discourse modes are closely related to rhetoric (Connors, 1981; Brooks and Warren, 1958) so that discourse modes can reflect writing proficiency in a degree. Discourse modes are categorized into narration, description, exposition, argument and emotion, following (Song et al., 2017). Moreover, we further identify fine-grained description types, such as appearance, facial expression, action, natural scene, psychology, dialogue and so on. How to accurately and vividly describe details of a character, a scene or an object is an important lesson to be learned for writing. Identifying and visualizing fine-grained description types let people quickly find some highlights in writing descriptions. Technically, we adopt a two-stage approach. In the first stage, we use a discourse-level hierarchical encoder to encode an essay and identify 5 discourse modes (Song et al., 2017). The"
2021.acl-demo.29,2020.emnlp-main.546,1,0.788149,"is an important educational application (Page, 1968; Rudner et al., 2006). It aims to reduce the burden of teachers for scoring student essays and give students direct instructions to improve their writing ability. Automated essay scoring (AES) is one of the most important modules for AEA, which is usually formulated as a supervised learning problem. The early approaches utilized hand-crafted features to predict essay scores (Yannakoudakis et al., 2011; Chen and He, 2013; Phandi et al., 2015). Recently, deep learning has been applied to AES as well (Taghipour and Ng, 2016; Dong et al., 2017; Song et al., 2020c). One issue about AES is that its prediction lacks explainability since a single score gives very limited information. Many efforts have been paid to • IFlyEA has comprehensive multi-level and multi-dimension analytical modules. It provides state-of-the-art Chinese spelling error correction and grammatical error diagnosis at grammar level. More specially, it also provides rich rhetoric and discourse level analysis, which are less studied but important for evaluating native speakers’ writing ability. • Based on the information provided by the analytical modules, IFlyEA provides a complete set"
2021.acl-demo.29,D16-1193,0,0.0128552,"troduction Automated essay assessment (AEA) is an important educational application (Page, 1968; Rudner et al., 2006). It aims to reduce the burden of teachers for scoring student essays and give students direct instructions to improve their writing ability. Automated essay scoring (AES) is one of the most important modules for AEA, which is usually formulated as a supervised learning problem. The early approaches utilized hand-crafted features to predict essay scores (Yannakoudakis et al., 2011; Chen and He, 2013; Phandi et al., 2015). Recently, deep learning has been applied to AES as well (Taghipour and Ng, 2016; Dong et al., 2017; Song et al., 2020c). One issue about AES is that its prediction lacks explainability since a single score gives very limited information. Many efforts have been paid to • IFlyEA has comprehensive multi-level and multi-dimension analytical modules. It provides state-of-the-art Chinese spelling error correction and grammatical error diagnosis at grammar level. More specially, it also provides rich rhetoric and discourse level analysis, which are less studied but important for evaluating native speakers’ writing ability. • Based on the information provided by the analytical m"
2021.acl-demo.29,2020.acl-demos.17,0,0.0328507,"Beijing, China ¶ iFLYTEK AI Research (Hefei), China £ iFLYTEK AI Research (Hebei), LangFang, China § Research Center for SCIR, Harbin Institute of Technology, Harbin, China {jfgong, xiaohu2, rjfu, zcsheng, bozhu, sjwang3}@iflytek.com, wsong@cnu.edu.cn, tliu@ir.hit.edu.cn expand the boundary of AES, and try to analyze Abstract † detailed linguistic properties, such as grammatical errors (Ng et al., 2014), coherence (Somasundaran et al., 2014), organization (Burstein et al., 2003; Persing et al., 2010) and so on. Several AES systems, such as E-Rater (Attali and Burstein, 2006) and Lingglewrite (Tsai et al., 2020), have been successfully applied in the education scenario. However, many of them focus on evaluating second-language learners’ writing ability or evaluating basic language usages depending on shallow features, which may be not sufficient for evaluating essays written by native speakers. Moreover, most existing platforms mainly target on English, while there are significantly fewer systems working on other languages, such as Chinese. In this paper, we introduce the IFlyEssayAssess (IFlyEA) system, which is a Chinese automated essay assessment system, focusing on assessing the quality of essays"
2021.acl-demo.29,W15-3106,0,0.0266741,"oviding evidence and diagnosis, and also improving the explainability for the predictions of application modules. As illustrated in Figure 2, through web page visualization 241 Figure 2: The visualization and interactive interfaces of IFlyEA. and interfaces, students or teachers can receive rich information and interact with the analytical results. 3 Model Wang et al. (2019) Zhang et al. (2020) Ours Analytical Modules Grammar-level Analysis Spelling Error Correction Given a sentence, our spelling checker would locate spelling errors if there is any, and provide a list of corrected candidates (Tseng et al., 2015). Inspired by Liu et al. (2013); Yu and Li (2014), we establish a confusion-set based unsupervised two-stage method to detect and correct spelling errors. Confusion set: A confusion set is built to group characters with similar pronunciation or graphemic into clusters. We implement it with an inverted indexing structure so that given a target character, we can quickly get a list of confusion characters from the same cluster. Stage 1: Correction candidate detection with local context: We train a 5-gram language model LM on a large-scale corpus. For each character in a sentence, we substitute it"
2021.acl-demo.29,P19-1578,0,0.0386031,"Missing"
2021.acl-demo.29,P11-1019,0,0.0291572,"visualization. These services can benefit both teachers and students during the process of writing teaching and learning. 1 Introduction Automated essay assessment (AEA) is an important educational application (Page, 1968; Rudner et al., 2006). It aims to reduce the burden of teachers for scoring student essays and give students direct instructions to improve their writing ability. Automated essay scoring (AES) is one of the most important modules for AEA, which is usually formulated as a supervised learning problem. The early approaches utilized hand-crafted features to predict essay scores (Yannakoudakis et al., 2011; Chen and He, 2013; Phandi et al., 2015). Recently, deep learning has been applied to AES as well (Taghipour and Ng, 2016; Dong et al., 2017; Song et al., 2020c). One issue about AES is that its prediction lacks explainability since a single score gives very limited information. Many efforts have been paid to • IFlyEA has comprehensive multi-level and multi-dimension analytical modules. It provides state-of-the-art Chinese spelling error correction and grammatical error diagnosis at grammar level. More specially, it also provides rich rhetoric and discourse level analysis, which are less stud"
2021.acl-demo.29,W14-6835,0,0.0183103,"he explainability for the predictions of application modules. As illustrated in Figure 2, through web page visualization 241 Figure 2: The visualization and interactive interfaces of IFlyEA. and interfaces, students or teachers can receive rich information and interact with the analytical results. 3 Model Wang et al. (2019) Zhang et al. (2020) Ours Analytical Modules Grammar-level Analysis Spelling Error Correction Given a sentence, our spelling checker would locate spelling errors if there is any, and provide a list of corrected candidates (Tseng et al., 2015). Inspired by Liu et al. (2013); Yu and Li (2014), we establish a confusion-set based unsupervised two-stage method to detect and correct spelling errors. Confusion set: A confusion set is built to group characters with similar pronunciation or graphemic into clusters. We implement it with an inverted indexing structure so that given a target character, we can quickly get a list of confusion characters from the same cluster. Stage 1: Correction candidate detection with local context: We train a 5-gram language model LM on a large-scale corpus. For each character in a sentence, we substitute it with its corresponding F1 0.649 0.664 0.651 conf"
2021.acl-demo.29,2020.acl-main.82,0,0.0209667,"Missing"
2021.acl-long.117,2020.tacl-1.28,0,0.0310816,"nowledge (Feng et al., 2020a). In this paper, we combine three types of auxiliary information to help better modeling dialogues, including keywords, redundant utterances and topics. Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have advanced various NLP tasks. On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020). On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku1486 Rob : Hey there , what's up ? Bob : Not much , watching the game . You ? [Topic 1] Rob : Same . Having a few people over . Rob : But the game is boring as fuck lol . That's why I'm writing Bob : Yeah , true that Rob : Any plans for the weekend ? Bob : Most likely the usual run some errands , cook some food , go out for a few beers . Nothing super interesting have appeared yet Rob : I've heard that Jim is planning to celebrate his birthday [Topic 2] Bob : Oh right , his birthday is like next Wednesday ? Rob : Yeah , normally that would make the nex"
2021.acl-long.117,2020.coling-main.499,0,0.10487,"erate a succinct summary while retaining essential information of the dialogue (Gurevych and Strube, 2004; Chen and Yang, 2020). Theoretically, Peyrard (2019) point out that a good summary is intuitively related to three aspects, including Informativeness, Redundancy and Relevance. To this end, previous works have taken the above three aspects into account by incorporating auxiliary annotations into the dialogue. To improve informativeness, some works annotated linguistically specific words (e.g., nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020). To reduce redundancy, some works ∗ Corresponding author. Our codes are available at: https://github.com/ xcfcode/PLM_annotator 1 used sentence similarity-based methods to annotate redundant utterances. (Zechner, 2002; Murray et al., 2005). To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020). However, these annotations are usually obtained via open-domain toolkits, which are not suitable for dialogues, or require manual annotations, which are labor-consuming. To alleviate the above problem, we explore"
2021.acl-long.117,2020.lifelongnlp-1.3,0,0.0604519,"Missing"
2021.acl-long.117,2020.acl-main.703,0,0.336616,"Topic Segmentation aims to divide a dialogue into topically coherent segments (shown in Figure 1(c)). Our DialoGPT annotator inserts a topic segmentation point before one utterance if it is unpredictable. We assume that if an utterance is difficult to be inferred from the dialogue context based on DialoGPT, this utterance may belong to a new topic. We use our DialoGPT annotator to annotate the SAMSum (Gliwa et al., 2019) and AMI (Carletta et al., 2005) datasets. Each annotation is converted into a specific identifier and we insert them into the dialogue text. Then, we employ pre-traind BART (Lewis et al., 2020) and non pre-trained PGN (See et al., 2017) as our summarizers. Extensive experimental results show that our method can obtain consistent and remarkable improvements over strong baselines on both datasets and achieves new stateof-the-art performance on the SAMSum dataset. 2 Preliminaries In this section, we will describe the task definition as well as the background of DialoGPT. 2.1 Task Definition Given an input dialogue D, a dialogue summarizer aims to produce a condensed summary S, where D consists of |D |utterances [u1 , u2 , ...u|D |] and S consists of |S |words [s1 , s2 , ...s|S |]. Each"
2021.acl-long.117,P19-1210,0,0.224025,"nto account by incorporating auxiliary annotations into the dialogue. To improve informativeness, some works annotated linguistically specific words (e.g., nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020). To reduce redundancy, some works ∗ Corresponding author. Our codes are available at: https://github.com/ xcfcode/PLM_annotator 1 used sentence similarity-based methods to annotate redundant utterances. (Zechner, 2002; Murray et al., 2005). To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020). However, these annotations are usually obtained via open-domain toolkits, which are not suitable for dialogues, or require manual annotations, which are labor-consuming. To alleviate the above problem, we explore the pre-trained language model as an unsupervised annotator to automatically provide annotations for the dialogue. Recently, some works have investigated the use of pre-trained language models in an unsupervised manner. For example, Sainz and Rigau (2021) exploited pre-trained models for assigning domain labels to WordNet synsets. The successf"
2021.acl-long.117,2020.emnlp-main.557,0,0.0207306,"paper, we combine three types of auxiliary information to help better modeling dialogues, including keywords, redundant utterances and topics. Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have advanced various NLP tasks. On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020). On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku1486 Rob : Hey there , what's up ? Bob : Not much , watching the game . You ? [Topic 1] Rob : Same . Having a few people over . Rob : But the game is boring as fuck lol . That's why I'm writing Bob : Yeah , true that Rob : Any plans for the weekend ? Bob : Most likely the usual run some errands , cook some food , go out for a few beers . Nothing super interesting have appeared yet Rob : I've heard that Jim is planning to celebrate his birthday [Topic 2] Bob : Oh right , his birthday is like next Wednesday ? Rob : Yeah , normally that would make the next weekend a good time but he is going"
2021.acl-long.117,D19-1387,0,0.018767,"). Other works also explored dialogue act (Goo and Chen, 2018), dialogue discourse (Feng et al., 2020b) and commonsense knowledge (Feng et al., 2020a). In this paper, we combine three types of auxiliary information to help better modeling dialogues, including keywords, redundant utterances and topics. Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have advanced various NLP tasks. On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020). On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku1486 Rob : Hey there , what's up ? Bob : Not much , watching the game . You ? [Topic 1] Rob : Same . Having a few people over . Rob : But the game is boring as fuck lol . That's why I'm writing Bob : Yeah , true that Rob : Any plans for the weekend ? Bob : Most likely the usual run some errands , cook some food , go out for a few beers . Nothing super interesting have appeared yet Rob : I've heard that Jim is planning to celebrate his"
2021.acl-long.117,W04-3252,0,0.0159113,"he first-ranked and second-ranked results respectively. 4.3 R-L 15.70 13.91 13.72 25.47† 24.00 23.49 24.11 24.27 24.05 24.59†† Table 3: Test set results on the AMI dataset. PGN(DK E ), PGN(DR D ) and PGN(DT S ) represent training PGN on the AMI with keywords, redundancy and topic annotation respectively. SAMSum Model BS BART 86.91 MV-BART 88.46 BART(DA LL ) 90.04 AMI Model BS PGN 80.51 HMNet 82.24 PGN(DA LL ) 82.76 Table 4: Test set results on the SAMSum and AMI. “BS” is short for BERTScore. Baselines and Metrics For SAMSum, LONGEST-3 views the first three utterances as the summary. TextRank (Mihalcea and Tarau, 2004) is a traditional graph-based method. Transformer (Vaswani et al., 2017) is a seq2seq method based on full self-attention operations. D-HGN (Feng et al., 2020a) incorporates commonsense knowledge to help understand dialogues. TGDGA (Zhao et al., 2020) uses topic words and models graph structures for dialogues. DialoGPT (Zhang et al., 2020b) means that finetuning DialoGPT on the SAMSum. MV-BART (Chen and Yang, 2020) is a BART-based method that incorporates topic and stage information. For AMI, SummaRunner (Nallapati et al., 2017) is an extractive method based on hierarchical RNN network. UNS (S"
2021.acl-long.117,K16-1028,0,0.0746752,"Missing"
2021.acl-long.117,D18-1206,0,0.0243061,"oGPT emb 53.14 27.25 49.42 Ours DialoGPTK E 53.43 28.03 49.93 Table 5: Human evaluation results. “Info.” is short for informativeness, “Conc.” for conciseness, “Cov.” for coverage. For SAMSum, the inter-annotator agreement (Fleiss’ kappa) scores for each metric are 0.46, 0.37 and 0.43 respectively. For AMI, Fleiss’ kappa scores are 0.48, 0.40 and 0.41 respectively. 4.5 Table 6: Test set results of fine-tuning BART on the SAMSum that is annotated with keywords using various methods. Entities, nouns and verbs are obtained by Qi et al. (2020). Topic words are obtained by a pre-trained LDA model (Narayan et al., 2018). KeyBERT (Grootendorst, 2020) leverages pre-trained language model embeddings to create keywords. Method TextRank Entities DialoGPTK E Human Evaluation We conduct a human evaluation of the dialogue summary to assess its informativeness, conciseness and coverage. Informativeness measures how well the summary includes key information. Conciseness measures how well the summary discards the redundant information. Coverage measures how well the summary covers each part of the dialogue. We randomly sample 100 dialogues (SAMSum) and 10 meetings (AMI) with corresponding generated summaries to conduct"
2021.acl-long.117,P19-1101,0,0.020159,"ge of dialogue background knowledge encoded in DialoGPT. We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizers. Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-theart performance on the SAMSum dataset1 . 1 Introduction Dialogue summarization aims to generate a succinct summary while retaining essential information of the dialogue (Gurevych and Strube, 2004; Chen and Yang, 2020). Theoretically, Peyrard (2019) point out that a good summary is intuitively related to three aspects, including Informativeness, Redundancy and Relevance. To this end, previous works have taken the above three aspects into account by incorporating auxiliary annotations into the dialogue. To improve informativeness, some works annotated linguistically specific words (e.g., nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020). To reduce redundancy, some works ∗ Corresponding author. Our codes are available at: https://github.com/ xcfcode/PLM_an"
2021.acl-long.117,2020.acl-demos.14,0,0.0445304,"guage Model-Based Methods KeyBERT w/ BERT emb 52.39 27.14 48.52 w/ DialoGPT emb 53.14 27.25 49.42 Ours DialoGPTK E 53.43 28.03 49.93 Table 5: Human evaluation results. “Info.” is short for informativeness, “Conc.” for conciseness, “Cov.” for coverage. For SAMSum, the inter-annotator agreement (Fleiss’ kappa) scores for each metric are 0.46, 0.37 and 0.43 respectively. For AMI, Fleiss’ kappa scores are 0.48, 0.40 and 0.41 respectively. 4.5 Table 6: Test set results of fine-tuning BART on the SAMSum that is annotated with keywords using various methods. Entities, nouns and verbs are obtained by Qi et al. (2020). Topic words are obtained by a pre-trained LDA model (Narayan et al., 2018). KeyBERT (Grootendorst, 2020) leverages pre-trained language model embeddings to create keywords. Method TextRank Entities DialoGPTK E Human Evaluation We conduct a human evaluation of the dialogue summary to assess its informativeness, conciseness and coverage. Informativeness measures how well the summary includes key information. Conciseness measures how well the summary discards the redundant information. Coverage measures how well the summary covers each part of the dialogue. We randomly sample 100 dialogues (SAM"
2021.acl-long.117,P17-1099,0,0.43215,"into topically coherent segments (shown in Figure 1(c)). Our DialoGPT annotator inserts a topic segmentation point before one utterance if it is unpredictable. We assume that if an utterance is difficult to be inferred from the dialogue context based on DialoGPT, this utterance may belong to a new topic. We use our DialoGPT annotator to annotate the SAMSum (Gliwa et al., 2019) and AMI (Carletta et al., 2005) datasets. Each annotation is converted into a specific identifier and we insert them into the dialogue text. Then, we employ pre-traind BART (Lewis et al., 2020) and non pre-trained PGN (See et al., 2017) as our summarizers. Extensive experimental results show that our method can obtain consistent and remarkable improvements over strong baselines on both datasets and achieves new stateof-the-art performance on the SAMSum dataset. 2 Preliminaries In this section, we will describe the task definition as well as the background of DialoGPT. 2.1 Task Definition Given an input dialogue D, a dialogue summarizer aims to produce a condensed summary S, where D consists of |D |utterances [u1 , u2 , ...u|D |] and S consists of |S |words [s1 , s2 , ...s|S |]. Each utterance ui is compose of a sequence of w"
2021.acl-long.117,P18-1062,0,0.0509974,"Missing"
2021.acl-long.117,2020.emnlp-main.293,0,0.014839,"., 2020a). In this paper, we combine three types of auxiliary information to help better modeling dialogues, including keywords, redundant utterances and topics. Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have advanced various NLP tasks. On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020). On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku1486 Rob : Hey there , what's up ? Bob : Not much , watching the game . You ? [Topic 1] Rob : Same . Having a few people over . Rob : But the game is boring as fuck lol . That's why I'm writing Bob : Yeah , true that Rob : Any plans for the weekend ? Bob : Most likely the usual run some errands , cook some food , go out for a few beers . Nothing super interesting have appeared yet Rob : I've heard that Jim is planning to celebrate his birthday [Topic 2] Bob : Oh right , his birthday is like next Wednesday ? Rob : Yeah , normally that would make the next weekend a good ti"
2021.acl-long.117,J02-4003,0,0.535386,"formativeness, Redundancy and Relevance. To this end, previous works have taken the above three aspects into account by incorporating auxiliary annotations into the dialogue. To improve informativeness, some works annotated linguistically specific words (e.g., nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020). To reduce redundancy, some works ∗ Corresponding author. Our codes are available at: https://github.com/ xcfcode/PLM_annotator 1 used sentence similarity-based methods to annotate redundant utterances. (Zechner, 2002; Murray et al., 2005). To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020). However, these annotations are usually obtained via open-domain toolkits, which are not suitable for dialogues, or require manual annotations, which are labor-consuming. To alleviate the above problem, we explore the pre-trained language model as an unsupervised annotator to automatically provide annotations for the dialogue. Recently, some works have investigated the use of pre-trained language models in an unsupervised manner. For example, Sainz"
2021.acl-long.117,2020.acl-demos.30,0,0.436249,"oring DialoGPT for Dialogue Summarization Xiachong Feng1 , Xiaocheng Feng1,2∗, Libo Qin1 , Bing Qin1,2 , Ting Liu1,2 1 Harbin Institute of Technology, China 2 Peng Cheng Laboratory, China {xiachongfeng,xcfeng,lbqin,bqin,tliu}@ir.hit.edu.cn Abstract Current dialogue summarization systems usually encode the text with a number of general semantic features (e.g., keywords and topics) to gain more powerful dialogue modeling capabilities. However, these features are obtained via open-domain toolkits that are dialogagnostic or heavily relied on human annotations. In this paper, we show how DialoGPT (Zhang et al., 2020b), a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of dialogue background knowledge encoded in DialoGPT. We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizers. Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-theart performance on the SAMSum dataset1 . 1 Introduction Dialogue summarization aims to generate"
2021.acl-long.117,2020.findings-emnlp.19,0,0.0899951,"ng et al., 2020a) incorporates commonsense knowledge to help understand dialogues. TGDGA (Zhao et al., 2020) uses topic words and models graph structures for dialogues. DialoGPT (Zhang et al., 2020b) means that finetuning DialoGPT on the SAMSum. MV-BART (Chen and Yang, 2020) is a BART-based method that incorporates topic and stage information. For AMI, SummaRunner (Nallapati et al., 2017) is an extractive method based on hierarchical RNN network. UNS (Shang et al., 2018) is a fully unsupervised and graph-based method. TopicSeg (Li et al., 2019) incorporates topics to model the meeting. HMNet (Zhu et al., 2020) is a transformer-based method that incorporates POS and entity information and is pre-trained on news summarization dataset. We adopt ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020a) for evaluating our models. 4.4 Automatic Evaluation The results on SAMSum and AMI are shown in Table 2 and 3 respectively. We can see that using our annotated datasets DK E , DR D and DT S , both BART and PGN can obtain improvements. Furthermore, our BART(DA LL ) achieves SOTA performance. For SAMSum, it’s worth noting that BART(DK E ) performs better compared with BART(DR D ) and BART(DT S ). We attribute t"
2021.acl-long.14,D15-1075,0,0.0557256,"dataset that satisfies both features are difficult to annotate. However, once we disentangle persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation, it is easy to find 167 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 167–177 August 1–6, 2021. ©2021 Association for Computational Linguistics abundant data resources for them. For consistency understanding, we may leverage large-scale nondialogue inference data, such as SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) as the training data. As for dialogue generation, we already have various large-scale persona-sparse datasets. Inspired by the aforementioned motivation, in this work, we explore to learn a consistent personabased dialogue model from limited personalized dialogues, with the assistance of large-scale nondialogue inference data. Specifically, the proposed model consists of an encoder E, an auto-regressive decoder D1 for response generation, and a bidirectional decoder D2 for consistency understanding. Given personas P and dialogue query Q, the E and D1 jointly w"
2021.acl-long.14,N19-1423,0,0.0490199,"response mapping FG (S|Q, P ), and generate a coarse response representation R1 . Then R1 and personas P are fed into the bidirectional decoder D2 to map R1 to final response representations R2 : FU (R2 |S, P ). Since the consistency understanding part FU (R|S, P ) is independent of the dialogue query Q, it can be learned on non-dialogue inference datasets. Here an unlikelihood training objective (Welleck et al., 2019a) is applied to make contradicted cases in the inference data less likely so that D2 could acquire the ability of consistency understanding. We initialize all modules from BERT (Devlin et al., 2019) and name the proposed model BERTover-BERT (BoB). To verify the effectiveness of our model, we experiment on two limited data scenarios: 1) a persona-dense scenario (Zhang et al., 2018) with low-resource settings (Zhao et al., 2019), and 2) a persona-sparse scenario (Zheng et al., 2019). Both automatic and human evaluations indicate that our model generalizes well under different settings and outperforms strong baselines on most metrics, especially on persona consistency. Contributions in this work are three-fold: • We disentangled the task of persona-based dialogue generation into two sub-tas"
2021.acl-long.14,P19-1608,0,0.067331,"s on persona-based dialogue generation focus on a datadriven manner. They learn persona-related features directly from personalized dialogue datasets, either with implicit persona embeddings (Li et al., 2016b) or with explicit profiles (Qian et al., 2018) and personal facts (Mazar´e et al., 2018). Following this research line, more sophisticated neural models are emerging, such as modeling mutual-persona (Liu et al., 2020) and multi-stage persona-based dialogue generation (Song et al., 2020a). Meanwhile, various pre-training methods have also been applied in this field. Wolf et al. (2019) and Golovanov et al. (2019) show that fine-tuning pre-trained GPT on the persona-dense dataset can improve the quality of generated responses. Zheng et al. (2020) propose an attention-routing mechanism in a GPT-based model to control the flow of persona information. Lin et al. (2020) explore how to leverage BERT model for dialogue generation. Different large-scale pretrained chatbots (Roller et al., 2020; Madotto et al., 2020) also show their effectiveness on persona-based dialogues. Disentangled Representation The concept of “disentangling” can be defined as transformations that only change some properties of the under"
2021.acl-long.14,N16-1014,0,0.285928,"ask of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation. • A BERT-based generative framework, BoB, was proposed for training persona-based dialogue models from limited data. • An unlikelihood training method with nondialogue inference data was introduced to enhance persona consistency understanding. 2 Related Work Persona-based Dialogues Recent studies on persona-based dialogue generation focus on a datadriven manner. They learn persona-related features directly from personalized dialogue datasets, either with implicit persona embeddings (Li et al., 2016b) or with explicit profiles (Qian et al., 2018) and personal facts (Mazar´e et al., 2018). Following this research line, more sophisticated neural models are emerging, such as modeling mutual-persona (Liu et al., 2020) and multi-stage persona-based dialogue generation (Song et al., 2020a). Meanwhile, various pre-training methods have also been applied in this field. Wolf et al. (2019) and Golovanov et al. (2019) show that fine-tuning pre-trained GPT on the persona-dense dataset can improve the quality of generated responses. Zheng et al. (2020) propose an attention-routing mechanism in a GPT-"
2021.acl-long.14,P16-1094,0,0.435578,"ask of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation. • A BERT-based generative framework, BoB, was proposed for training persona-based dialogue models from limited data. • An unlikelihood training method with nondialogue inference data was introduced to enhance persona consistency understanding. 2 Related Work Persona-based Dialogues Recent studies on persona-based dialogue generation focus on a datadriven manner. They learn persona-related features directly from personalized dialogue datasets, either with implicit persona embeddings (Li et al., 2016b) or with explicit profiles (Qian et al., 2018) and personal facts (Mazar´e et al., 2018). Following this research line, more sophisticated neural models are emerging, such as modeling mutual-persona (Liu et al., 2020) and multi-stage persona-based dialogue generation (Song et al., 2020a). Meanwhile, various pre-training methods have also been applied in this field. Wolf et al. (2019) and Golovanov et al. (2019) show that fine-tuning pre-trained GPT on the persona-dense dataset can improve the quality of generated responses. Zheng et al. (2020) propose an attention-routing mechanism in a GPT-"
2021.acl-long.14,2020.acl-main.428,0,0.023195,"defined as transformations that only change some properties of the underlying model while leaving all other properties invariant (Higgins et al., 2018). The variational autoencoder (Kingma and Welling, 2013) could be regarded as a disentangled representation learning framework, and various methods are built within it (Kim and Mnih, 2018; Locatello et al., 2019). Unlikelihood Training Likelihood tries to maximize the probability of target sequence, while unlikelihood corrects known biases by minimizing the probability of negative candidates (Welleck et al., 2019a). Closely related to our work, Li et al. (2020) first explored unlikelihood training in addressing dialogue logical contradictions. They get contradicted dialogues from PersonaChat according to DNLI (Welleck et al., 2019b), a PersonaChatoriented dialogue inference dataset. Then unlikelihood training is applied to reduce the probability of contradicted responses. Different from Li et al. (2020), with carefully designed decoders, our model could learn from large-scale non-dialogue inference datasets, making it generalizable to different scenarios, such as persona-dense and personasparse datasets, as will be seen in our experiments. 168 vecto"
2021.acl-long.14,2020.acl-main.131,0,0.359433,"Missing"
2021.acl-long.14,2020.acl-main.516,1,0.892273,"nce data was introduced to enhance persona consistency understanding. 2 Related Work Persona-based Dialogues Recent studies on persona-based dialogue generation focus on a datadriven manner. They learn persona-related features directly from personalized dialogue datasets, either with implicit persona embeddings (Li et al., 2016b) or with explicit profiles (Qian et al., 2018) and personal facts (Mazar´e et al., 2018). Following this research line, more sophisticated neural models are emerging, such as modeling mutual-persona (Liu et al., 2020) and multi-stage persona-based dialogue generation (Song et al., 2020a). Meanwhile, various pre-training methods have also been applied in this field. Wolf et al. (2019) and Golovanov et al. (2019) show that fine-tuning pre-trained GPT on the persona-dense dataset can improve the quality of generated responses. Zheng et al. (2020) propose an attention-routing mechanism in a GPT-based model to control the flow of persona information. Lin et al. (2020) explore how to leverage BERT model for dialogue generation. Different large-scale pretrained chatbots (Roller et al., 2020; Madotto et al., 2020) also show their effectiveness on persona-based dialogues. Disentangl"
2021.acl-long.14,2020.emnlp-main.539,1,0.853547,"nce data was introduced to enhance persona consistency understanding. 2 Related Work Persona-based Dialogues Recent studies on persona-based dialogue generation focus on a datadriven manner. They learn persona-related features directly from personalized dialogue datasets, either with implicit persona embeddings (Li et al., 2016b) or with explicit profiles (Qian et al., 2018) and personal facts (Mazar´e et al., 2018). Following this research line, more sophisticated neural models are emerging, such as modeling mutual-persona (Liu et al., 2020) and multi-stage persona-based dialogue generation (Song et al., 2020a). Meanwhile, various pre-training methods have also been applied in this field. Wolf et al. (2019) and Golovanov et al. (2019) show that fine-tuning pre-trained GPT on the persona-dense dataset can improve the quality of generated responses. Zheng et al. (2020) propose an attention-routing mechanism in a GPT-based model to control the flow of persona information. Lin et al. (2020) explore how to leverage BERT model for dialogue generation. Different large-scale pretrained chatbots (Roller et al., 2020; Madotto et al., 2020) also show their effectiveness on persona-based dialogues. Disentangl"
2021.acl-long.14,P19-1542,0,0.0350048,"Missing"
2021.acl-long.14,D18-1298,0,0.158238,"Missing"
2021.acl-long.14,2020.acl-main.517,0,0.187196,"nce data was introduced to enhance persona consistency understanding. 2 Related Work Persona-based Dialogues Recent studies on persona-based dialogue generation focus on a datadriven manner. They learn persona-related features directly from personalized dialogue datasets, either with implicit persona embeddings (Li et al., 2016b) or with explicit profiles (Qian et al., 2018) and personal facts (Mazar´e et al., 2018). Following this research line, more sophisticated neural models are emerging, such as modeling mutual-persona (Liu et al., 2020) and multi-stage persona-based dialogue generation (Song et al., 2020a). Meanwhile, various pre-training methods have also been applied in this field. Wolf et al. (2019) and Golovanov et al. (2019) show that fine-tuning pre-trained GPT on the persona-dense dataset can improve the quality of generated responses. Zheng et al. (2020) propose an attention-routing mechanism in a GPT-based model to control the flow of persona information. Lin et al. (2020) explore how to leverage BERT model for dialogue generation. Different large-scale pretrained chatbots (Roller et al., 2020; Madotto et al., 2020) also show their effectiveness on persona-based dialogues. Disentangl"
2021.acl-long.14,P19-1363,0,0.184615,"generation, and a bidirectional decoder D2 for consistency understanding. Given personas P and dialogue query Q, the E and D1 jointly work in an encoder-decoder manner to capture a typical query to response mapping FG (S|Q, P ), and generate a coarse response representation R1 . Then R1 and personas P are fed into the bidirectional decoder D2 to map R1 to final response representations R2 : FU (R2 |S, P ). Since the consistency understanding part FU (R|S, P ) is independent of the dialogue query Q, it can be learned on non-dialogue inference datasets. Here an unlikelihood training objective (Welleck et al., 2019a) is applied to make contradicted cases in the inference data less likely so that D2 could acquire the ability of consistency understanding. We initialize all modules from BERT (Devlin et al., 2019) and name the proposed model BERTover-BERT (BoB). To verify the effectiveness of our model, we experiment on two limited data scenarios: 1) a persona-dense scenario (Zhang et al., 2018) with low-resource settings (Zhao et al., 2019), and 2) a persona-sparse scenario (Zheng et al., 2019). Both automatic and human evaluations indicate that our model generalizes well under different settings and outpe"
2021.acl-long.14,N18-1101,0,0.340241,"tures are difficult to annotate. However, once we disentangle persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation, it is easy to find 167 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 167–177 August 1–6, 2021. ©2021 Association for Computational Linguistics abundant data resources for them. For consistency understanding, we may leverage large-scale nondialogue inference data, such as SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) as the training data. As for dialogue generation, we already have various large-scale persona-sparse datasets. Inspired by the aforementioned motivation, in this work, we explore to learn a consistent personabased dialogue model from limited personalized dialogues, with the assistance of large-scale nondialogue inference data. Specifically, the proposed model consists of an encoder E, an auto-regressive decoder D1 for response generation, and a bidirectional decoder D2 for consistency understanding. Given personas P and dialogue query Q, the E and D1 jointly work in an encoder-decoder manner"
2021.acl-long.14,P18-1205,0,0.516184,"ions R2 : FU (R2 |S, P ). Since the consistency understanding part FU (R|S, P ) is independent of the dialogue query Q, it can be learned on non-dialogue inference datasets. Here an unlikelihood training objective (Welleck et al., 2019a) is applied to make contradicted cases in the inference data less likely so that D2 could acquire the ability of consistency understanding. We initialize all modules from BERT (Devlin et al., 2019) and name the proposed model BERTover-BERT (BoB). To verify the effectiveness of our model, we experiment on two limited data scenarios: 1) a persona-dense scenario (Zhang et al., 2018) with low-resource settings (Zhao et al., 2019), and 2) a persona-sparse scenario (Zheng et al., 2019). Both automatic and human evaluations indicate that our model generalizes well under different settings and outperforms strong baselines on most metrics, especially on persona consistency. Contributions in this work are three-fold: • We disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation. • A BERT-based generative framework, BoB, was proposed for training persona-based dialogue models from limited data. • An unlikeli"
2021.acl-long.15,W17-7607,0,0.0673819,"Missing"
2021.acl-long.15,P19-1544,0,0.0346958,"Missing"
2021.acl-long.15,N18-2118,0,0.402918,"essive model generates outputs word by word from left-to-right direction. The gray color denotes the unseen information when model decodes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name. Introduction Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries. It typically includes two sub-tasks: intent detection and slot filling (Tur and De Mori, 2011). Since intents and slots are closely tied, dominant single-intent SLU systems in the literature (Goo et al., 2018; Li et al., 2018; Liu et al., 2019b; E et al., 2019; Qin et al., 2019; Teng et al., 2021; Qin et al., 2021b,c) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot fi"
2021.acl-long.15,H90-1021,0,0.262393,"Missing"
2021.acl-long.15,P82-1020,0,0.788566,"Missing"
2021.acl-long.15,D19-1549,0,0.0957343,"ly, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multiple intent detection. Gangadharaiah and Narayanaswamy (2019) first apply a multi-task framework with a slot-gate mechanism to jointly model the multiple intent detection and slot fill185 Graph Neural Network for NLP Graph neural networks that operate directly on graph structures to model the structural information, which has been applied successfully in various NLP tasks. Linmei et al. (2019) and Huang and Carley (2019) explore graph attention network (GAT) (Veliˇckovi´c et al., 2018) for classification task to incorporate the dependency parser information. Cetoli et al. (2017) and Liu et al. (2019a) apply graph neural network to model the non-local contextual information for sequence labeling tasks. Yasunaga et al. (2017) and Feng et al. (2020a) successfully apply a graph network to model the discourse information for the summarization generation task, which achieved promising performance. Graph structure are successfully applied for dialogue direction (Feng et al., 2020b; Fu et al., 2020; Qin et al., 2020a"
2021.acl-long.15,D18-1417,0,0.105296,"ates outputs word by word from left-to-right direction. The gray color denotes the unseen information when model decodes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name. Introduction Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries. It typically includes two sub-tasks: intent detection and slot filling (Tur and De Mori, 2011). Since intents and slots are closely tied, dominant single-intent SLU systems in the literature (Goo et al., 2018; Li et al., 2018; Liu et al., 2019b; E et al., 2019; Qin et al., 2019; Teng et al., 2021; Qin et al., 2021b,c) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recen"
2021.acl-long.15,2020.acl-main.574,0,0.0142879,"ent detection decoder (§3.2) and a global-local graphinteraction graph decoder for slot filling (§3.3). Both intent detection and slot filling are optimized simultaneously via a joint learning scheme. 3.1 Self-attentive Encoder Following Qin et al. (2019), we utilize a selfattentive encoder with BiLSTM and self-attention mechanism to obtain the shared utterance representation, which can incorporate temporal features within word orders and contextual information. BiLSTM The bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) have been successfully applied to sequence labeling tasks (Li et al., 2020, 2021). We adopt BiLSTM to read the input sequence {x1 , x2 , . . . , xn } forwardly and backwardly to produce contextsensitive hidden states H = {h1 , h2 , . . . , hn }, by repeatedly applying the hi = BiLSTM (φemb (xi ), hi−1 , hi+1 ), where φemb is embedding function. Self-Attention Following Vaswani et al. (2017), we map the matrix of input vectors X ∈ Rn×d (d represents the mapped dimension) to queries Q, keys K and values V matrices by using different linear projections. Then, the self-attention output C ∈ Rn×d is a weighted sum of values: 179  C = softmax QK &gt; √ dk  V. (1) Intent Det"
2021.acl-long.15,D19-1488,0,0.028307,"ent scenario. More recently, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multiple intent detection. Gangadharaiah and Narayanaswamy (2019) first apply a multi-task framework with a slot-gate mechanism to jointly model the multiple intent detection and slot fill185 Graph Neural Network for NLP Graph neural networks that operate directly on graph structures to model the structural information, which has been applied successfully in various NLP tasks. Linmei et al. (2019) and Huang and Carley (2019) explore graph attention network (GAT) (Veliˇckovi´c et al., 2018) for classification task to incorporate the dependency parser information. Cetoli et al. (2017) and Liu et al. (2019a) apply graph neural network to model the non-local contextual information for sequence labeling tasks. Yasunaga et al. (2017) and Feng et al. (2020a) successfully apply a graph network to model the discourse information for the summarization generation task, which achieved promising performance. Graph structure are successfully applied for dialogue direction (Feng et al., 2020b; Fu et"
2021.acl-long.15,2020.findings-emnlp.163,1,0.728742,"which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recently, Gangadharaiah and Narayanaswamy (2019) make the first attempt to propose a multi-task framework to joint model the multiple intent detection and slot filling. Qin et al. (2020b) further propose an adaptive interaction framework (AGIF) to achieve fine-grained multi-intent information integration for slot filling, obtaining state-of-the-art performance. Though achieving the promising performance, the existing multi-intent SLU joint models heavily rely on an autoregressive fashion, as shown in Figure 1(a), leading to two issues: Corresponding author. • Slow inference speed. The autoregressive models make the generation of slot outputs must be done through the left-to-right pass, which cannot achieve parallelizable, leading to slow inference speed. 178 Proceedings of t"
2021.acl-long.15,D19-1097,0,0.273219,"by word from left-to-right direction. The gray color denotes the unseen information when model decodes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name. Introduction Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries. It typically includes two sub-tasks: intent detection and slot filling (Tur and De Mori, 2011). Since intents and slots are closely tied, dominant single-intent SLU systems in the literature (Goo et al., 2018; Li et al., 2018; Liu et al., 2019b; E et al., 2019; Qin et al., 2019; Teng et al., 2021; Qin et al., 2021b,c) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recently, Gangadharaiah"
2021.acl-long.15,2021.ccl-1.108,0,0.0934371,"Missing"
2021.acl-long.15,D19-1214,1,0.533383,"on. The gray color denotes the unseen information when model decodes for the word Denver. (b) Non-autoregressive model can produce outputs in parallel. AN denotes airport name. Introduction Spoken Language Understanding (SLU) (Young et al., 2013) is a critical component in spoken dialog systems, which aims to understand user’s queries. It typically includes two sub-tasks: intent detection and slot filling (Tur and De Mori, 2011). Since intents and slots are closely tied, dominant single-intent SLU systems in the literature (Goo et al., 2018; Li et al., 2018; Liu et al., 2019b; E et al., 2019; Qin et al., 2019; Teng et al., 2021; Qin et al., 2021b,c) adopt joint models to consider the correlation between the two tasks, which have obtained remarkable success. Multi-intent SLU means that the system can handle an utterance containing multiple intents, which is shown to be more practical in the real-world scenario, attracting increasing attention. To this end, ∗ O Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multi-intent SLU. However, their models only consider the multiple intent detection while ignoring slot filling task. Recently, Gangadharaiah and Narayanaswamy (2019) make the"
2021.acl-long.15,N18-2050,0,0.0272234,"milarly, the slot filling task objective is: L2 , − NS n X X (j,S) yˆi   (j,S) log yi , (15) i=1 j=1 where NI is the number of single intent labels and NS is the number of slot labels. The final joint objective is formulated as: L = αL1 + βL2 , Baselines We compare our model with the following best baselines: (1) Attention BiRNN. Liu and Lane (2016) propose an alignment-based RNN for joint slot filling and intent detection; (2) Slot-Gated Atten. Goo et al. (2018) propose a slot-gated joint model, explicitly considering the correlation between slot filling and intent detection; (3) Bi-Model. Wang et al. (2018) propose the Bi-model to model the bi-directional between the intent detection and slot filling; (4) SF-ID Network. E et al. (2019) proposes the SF-ID network to establish a direct connection between the two tasks; (5) Stack-Propagation. Qin et al. (2019) adopt a stack-propagation framework to explicitly incorporate intent detection for guiding slot filling; (6) Joint Multiple ID-SF. Gangadharaiah and Narayanaswamy (2019) propose a multi-task framework with slot-gated mechanism for multiple intent detection and slot filling; (7) AGIF Qin et al. (2020b) proposes an adaptive interaction network"
2021.acl-long.15,2020.emnlp-main.152,0,0.06274,"Missing"
2021.acl-long.15,D18-1348,0,0.0221418,"ed multiple intent information integration for token-level slot filling, achieving the state-of-the-art performance. Their models adopt the autoregressive architecture for joint multiple intent detection and slot filling. In contrast, we propose a non-autoregressive approach, achieving parallel decoding. To the best of our knowledge, we are the first to explore a non-autoregressive architecture for multiple intent detection and slot filling. Related Work Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniT¨ur et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success. Compared with their work, we focus on jointly modeling multiple intent detection and slot filling while they only consider the single-intent scenario. More recently, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, Xu and Sarikaya (2013) and Kim et al. (2017) begin to exp"
2021.acl-long.15,K17-1045,0,0.0281906,"te mechanism to jointly model the multiple intent detection and slot fill185 Graph Neural Network for NLP Graph neural networks that operate directly on graph structures to model the structural information, which has been applied successfully in various NLP tasks. Linmei et al. (2019) and Huang and Carley (2019) explore graph attention network (GAT) (Veliˇckovi´c et al., 2018) for classification task to incorporate the dependency parser information. Cetoli et al. (2017) and Liu et al. (2019a) apply graph neural network to model the non-local contextual information for sequence labeling tasks. Yasunaga et al. (2017) and Feng et al. (2020a) successfully apply a graph network to model the discourse information for the summarization generation task, which achieved promising performance. Graph structure are successfully applied for dialogue direction (Feng et al., 2020b; Fu et al., 2020; Qin et al., 2020a, 2021a). In our work, we apply a global-locally graph interaction network to model the slot dependency and interaction between the multiple intents and slots. 6 Conclusion In this paper, we investigated a non-autoregressive model for joint multiple intent detection and slot filling. To this end, we proposed"
2021.acl-long.15,P19-1519,0,0.0181884,", achieving the state-of-the-art performance. Their models adopt the autoregressive architecture for joint multiple intent detection and slot filling. In contrast, we propose a non-autoregressive approach, achieving parallel decoding. To the best of our knowledge, we are the first to explore a non-autoregressive architecture for multiple intent detection and slot filling. Related Work Slot Filling and Intent Detection Recently, joint models (Zhang and Wang, 2016; HakkaniT¨ur et al., 2016; Goo et al., 2018; Li et al., 2018; Xia et al., 2018; E et al., 2019; Liu et al., 2019b; Qin et al., 2019; Zhang et al., 2019; Wu et al., 2020; Qin et al., 2021b; Ni et al., 2021) are proposed to consider the strong correlation between intent detection and slot filling have obtained remarkable success. Compared with their work, we focus on jointly modeling multiple intent detection and slot filling while they only consider the single-intent scenario. More recently, multiple intent detection can handle utterances with multiple intents, which has attracted increasing attention. To the end, Xu and Sarikaya (2013) and Kim et al. (2017) begin to explore the multiple intent detection. Gangadharaiah and Narayanaswamy (2019"
2021.acl-long.180,H94-1010,0,0.789783,"Missing"
2021.acl-long.180,N19-1423,0,0.0254931,"she was asked to raise interrelated questions towards the goal specified in the pair. Another annotator wrote the SQL query for the question if it was answerable. Benchmark Approach We consider three SOTA approaches as our benchmark approaches to understand the characteristics of existing datasets: EditSQL (Zhang et al., 2019), IGSQL (Cai and Wan, 2020), and RAT-C ON. RAT-C ON is our extension of RAT-SQL (Wang et al., 2020a), which is the SOTA approach for the context-independent Text-to-SQL problem. Appendix A.1 provides the details of our extension. All of the three approaches utilize BERT (Devlin et al., 2019) for encodings. 2.3 Contextual Dependency Prior work (Bertomeu et al., 2006) on database question answering dialogues reveals that questions within a dialogue tend to be context-dependent, i.e., the meaning of a question cannot be understood without its context. The last two questions in Figure 1 are typical context-dependent questions, requiring resolutions of ellipsis. In fact, how to effectively model the context to understand a contextdependent question is one of the major challenges in XDTS (Liu et al., 2020). Hence, we study this characteristic of existing datasets to understand how prag"
2021.acl-long.180,P19-1082,0,0.114093,"e student asks the first question “哪所大学培养 了最多MVP球员？” and writes its corresponding SQL query. The execution results of the SQL query will be shown to the student, helping her raise the follow-up question. After that, she receives the intent category Different Instances of the Same Entity, which is randomly sampled by our annotation tool.3 She chooses the Overlap relation in this category and raises the second question “状元呢？”. This creation session continues until the minimum number of questions is reached. To help study the characteristics of questions and address the schema linking challenge (Guo et al., 2019b; Lei et al., 2020) in Text-to-SQL, we also ask the students to label each question’s contextual dependency as in Section 2.3 and the linking between database schema items (tables and columns in databases) and their mentions in questions. 3.2.3 Data Review To ensure the data quality, we conduct two rounds of data review. First, when a student creates her first 20 question sequences, we carefully review all the annotations to check whether the questions in each sequence are thematically related and whether the semantics of SQL queries match their questions. If not, we run a new round of traini"
2021.acl-long.180,P19-1444,1,0.890733,"e student asks the first question “哪所大学培养 了最多MVP球员？” and writes its corresponding SQL query. The execution results of the SQL query will be shown to the student, helping her raise the follow-up question. After that, she receives the intent category Different Instances of the Same Entity, which is randomly sampled by our annotation tool.3 She chooses the Overlap relation in this category and raises the second question “状元呢？”. This creation session continues until the minimum number of questions is reached. To help study the characteristics of questions and address the schema linking challenge (Guo et al., 2019b; Lei et al., 2020) in Text-to-SQL, we also ask the students to label each question’s contextual dependency as in Section 2.3 and the linking between database schema items (tables and columns in databases) and their mentions in questions. 3.2.3 Data Review To ensure the data quality, we conduct two rounds of data review. First, when a student creates her first 20 question sequences, we carefully review all the annotations to check whether the questions in each sequence are thematically related and whether the semantics of SQL queries match their questions. If not, we run a new round of traini"
2021.acl-long.180,H90-1021,0,0.853536,"Missing"
2021.acl-long.180,P17-1167,0,0.0605491,"Missing"
2021.acl-long.180,P16-1138,0,0.0238555,"ts the object “first pick players” of the verb “have”, but the approach cannot fully resolve it and misses the first pick constraint in the WHERE clause. 6 Related Work Dataset XDTS is a sub-task of contextdependent semantic parsing (CDSP) (Suhr et al., 2018; Guo et al., 2019a; Li et al., 2020). Many datasets have been constructed for CDSP. They can be categorized into two groups according to their annotations. (1) Denotation Utterances in this group of datasets are only labelled with their denotations, i.e., the execution results of logical forms. S EQUEN TIAL QA (Iyyer et al., 2017), SCONE (Long et al., 2016), and CSQA (Saha et al., 2018) are representative datasets in this group. S EQUENTIAL QA was constructed by decomposing some complicated questions from WikiTableQuestions (Pasupat and Liang, 2015) into sequences of simple questions. A question sequence in SCONE was collected by randomly generating a sequence of world states and asking annotators to write an utterance between each pair of successive states. CSQA was constructed by collecting a large number of individual questions and converting them into question sequences via a set of manually crafted templates. (2) Logical Form Utterances in"
2021.acl-long.180,2021.naacl-main.103,0,0.052376,"Missing"
2021.acl-long.180,N18-2074,0,0.025078,"Missing"
2021.acl-long.180,2020.acl-main.742,0,0.0575659,"Missing"
2021.acl-long.180,P14-5010,0,0.005896,"Missing"
2021.acl-long.180,P19-1443,0,0.1904,"a set of question sequences {Qi , Y i , DB i }N i=1 . Two metrics are widely used to evaluate the prediction accuracy for XDTS: Question Match and Interaction Match. Question Match is 1 when the predicted SQL query of qji matches yji .1 Interaction Match is 1 when all predicted SQL queries of Qi match Y i . 1 Following (Yu et al., 2018), we decompose a predicted query into different clauses, such as SELECT, WHERE, and compute scores for each clause using set matching separately. 2317 2.2 Study Setup Dataset There are two datasets for studying XDTS, all of which are English corpora. (1) SParC (Yu et al., 2019b) SParC is the first dataset for XDTS. It is constructed upon the Spider dataset (Yu et al., 2018). Given a pair of question and SQL query chosen from Spider, an annotator was asked to write a sequence of questions to achieve the gold specified in the chosen pair. (2) CoSQL (Yu et al., 2019a) CoSQL is a corpus for task-oriented dialogue. It uses SQL queries for dialogue state tracking. Hence, it is also used to study XDTS. Question sequences in CoSQL were collected under the Wizard-of-Oz setup (Kelley, 1984). An annotator was assigned a pair of question and SQL query chosen from Spider, and s"
2021.acl-long.180,D19-1537,0,0.0580432,"rpus for task-oriented dialogue. It uses SQL queries for dialogue state tracking. Hence, it is also used to study XDTS. Question sequences in CoSQL were collected under the Wizard-of-Oz setup (Kelley, 1984). An annotator was assigned a pair of question and SQL query chosen from Spider, and she was asked to raise interrelated questions towards the goal specified in the pair. Another annotator wrote the SQL query for the question if it was answerable. Benchmark Approach We consider three SOTA approaches as our benchmark approaches to understand the characteristics of existing datasets: EditSQL (Zhang et al., 2019), IGSQL (Cai and Wan, 2020), and RAT-C ON. RAT-C ON is our extension of RAT-SQL (Wang et al., 2020a), which is the SOTA approach for the context-independent Text-to-SQL problem. Appendix A.1 provides the details of our extension. All of the three approaches utilize BERT (Devlin et al., 2019) for encodings. 2.3 Contextual Dependency Prior work (Bertomeu et al., 2006) on database question answering dialogues reveals that questions within a dialogue tend to be context-dependent, i.e., the meaning of a question cannot be understood without its context. The last two questions in Figure 1 are typica"
2021.acl-long.183,D18-1307,0,0.0453628,"Missing"
2021.acl-long.183,N19-1423,0,0.00508925,"ρj = {r1j ∧, · · · , ∧rljj } describes a serial of transitive causal logical rules starting from the cause event C and ending at the effect event E. Considering that each rule rkj ∈ ρj is composed by two events ejk−1 and ejk , a causal logic chain ρj with lj rules contains totally lj + 1 events {ej0 , · · · , ejlj } , where ej0 and ejlj are the cause event C and the effect event E, respectively. Taking C and E as the start and end point respectively, we can enumerate all distinct causal logic chains in the CLG using a Depth First Searching algorithm. 3.2.3 Event Encoding A BERT-based encoder (Devlin et al., 2019) is employed to encode all events within each causal logic chain into chain-specific distributed embeddings. Specifically, for a causal logic chain ρj containing lj+1 events {ej0 , · · · , ejlj }, we first process the event sequence into the form of: [CLS] ej0 · · · [CLS] ejk · · · [CLS] ejlj . After that, the processed event sequence is fed into BERT. We define the final hidden state of the [CLS] token before each event as the representation of the corresponding event. In this way, we obtain an event embedding set H = {hj0 , · · · , hjlj }, where hjk ∈ Rd is the embedding of the kth event wit"
2021.acl-long.183,N18-2017,0,0.021401,"commonly expressed by humans in the text of natural language, and is of great value for various Artificial Intelligence applications, such as question answering (Oh et al., 2013), event prediction (Li et al., 2018), and decision making (Sun et al., 2018). Previous work mainly learns causal knowledge from manually annotated causal event pairs, and achieves promising performances (Luo et al., 2016; ∗ Corresponding author Xie and Mu, 2019a; Li et al., 2019). However, recent works have questioned the seemingly superb performance for some of these studies (McCoy et al., 2019; Poliak et al., 2018; Gururangan et al., 2018). Specifically, training data may contain exploitable superficial cues that are correlative of the expected output. The main concern is that these works have not learned the underlying mechanism of causation so that their inference models are not stable enough and their results are not explainable. While we notice that there is plentiful evidence information outside the given corpus that can provide more clues for understanding the logical law of the causality. Figure 1 (a) exemplifies two clues I1 : Excess Liquidity and I2 : Invest Demand Increase for explaining how a: Quantitative Easing gra"
2021.acl-long.183,P19-1475,0,0.0272946,"Missing"
2021.acl-long.183,P19-1334,0,0.0388484,"Missing"
2021.acl-long.183,D19-1187,0,0.0591499,"Missing"
2021.acl-long.183,P13-1170,0,0.0838456,"Missing"
2021.acl-long.183,N18-1202,0,0.0101493,"and-crafted features; 2) MLN cannot model the influence of antecedents of rules. Different from MLN, in this paper, we propose a Conditional Markov Neural Logic Network, which works on the embedding space of logic rules to model the conditional causal strength of rules. 3 3.1 Given an event pair hC, Ei outside the causal event graph, to obtain the evidences from the CEG, we first locate the cause and effect in the CEG. Intuitively, semantically similar events would have similar causes and effects, and share similar locations in the CEG. To this end, we employ a pretrained language model ELMo (Peters et al., 2018) to derive the semantic representation for events in the CEG, as well as the cause and effect event. Then events in the CEG which are semantically similar to the input cause and effect event can be found using cosine similarity of the semantic representations. These events can serve as anchors for locating the cause and effect event. Then as Figure 2 shows, taking the anchors of the cause event as start points, and taking the anchors of the effect event as end points, the evidence events can be retrieved by a Breadth First Search (BFS) algorithm. After the retrieving process, the cause, effect"
2021.acl-long.183,W18-5441,0,0.0639778,"Missing"
2021.acl-long.183,C18-1069,0,0.0609692,"Missing"
2021.acl-long.183,N18-1089,0,0.055871,"Missing"
2021.acl-long.264,2020.acl-main.421,0,0.0139564,"ge of example consistency R1 . Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, a"
2021.acl-long.264,2020.tacl-1.30,0,0.0152671,"ible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tuned on the concatenation of English t"
2021.acl-long.264,2020.acl-main.747,0,0.0682084,"Missing"
2021.acl-long.264,D18-1269,0,0.0258158,"do not change nword . Thus the three data augmentation strategies will not affect the usage of example consistency R1 . Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on differen"
2021.acl-long.264,2020.acl-main.536,0,0.184978,"arization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method1 significantly improves crosslingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling. 1 Introduction Pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020) have shown great transferability across languages. By fine-tuning on labeled data in a source language, the models can generalize to other target languages, even without any additional training. Such generalization ability reduces the required annotation efforts, which is prohibitively expensive for low-resource languages. Recent work has demonstrated that data augmentation is helpful for cross-lingual transfer, e.g., translating source language training data into target languages (Singh et al., 2019), and generating codeswitch data by randomly replacing input words in the"
2021.acl-long.264,N19-1423,0,0.032175,"Missing"
2021.acl-long.264,E14-1049,0,0.0225658,"ary to the first strand. We focus on the cross-lingual setting, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source"
2021.acl-long.264,2020.acl-main.627,0,0.19623,"s on seven datasets. Experimental results show that our method outperforms conventional fine-tuning with data augmentation. We also demonstrate that X T UNE is flexible to be plugged in various 3403 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3403–3417 August 1–6, 2021. ©2021 Association for Computational Linguistics tasks, such as classification, span extraction, and sequence labeling. We summarize our contributions as follows: the target language. Besides, Qin et al. (2020) finetuned models on multilingual code-switch data, which achieves considerable improvements. • We propose X T UNE, a cross-lingual finetuning method to better utilize data augmentations based on consistency regularization. Consistency Regularization One strand of work in consistency regularization focused on regularizing model predictions to be invariant to small perturbations on image data. The small perturbations can be random noise (Zheng et al., 2016), adversarial noise (Miyato et al., 2019; Carmon et al., 2019) and various data augmentation approaches (Hu et al., 2017; Ye et al., 2019; X"
2021.acl-long.264,P15-1119,1,0.817926,"We focus on the cross-lingual setting, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source language training"
2021.acl-long.264,2020.acl-main.197,0,0.0945311,"Missing"
2021.acl-long.264,P17-1178,0,0.028767,"translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tuned on the concatenation of English training data and its translated data on all target languages"
2021.acl-long.264,2020.acl-main.653,0,0.0247876,". Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tune"
2021.acl-long.264,2020.acl-main.170,0,0.0340809,"(DA , θ, θ∗ ) where λ1 and λ2 are the corresponding weights of two regularization methods. Notice that the data augmentation strategies A, A0 , and A∗ can be either different or the same, which are tuned as hyper-parameters. 3.2 Gaussian Noise Data Augmentation We consider four types of data augmentation strategies in this work, which are shown in Figure 2. We aim to study the impact of different data augmentation strategies on cross-lingual transferability. 3.2.1 Subword Sampling Representing a sentence in different subword sequences can be viewed as a data augmentation strategy (Kudo, 2018; Provilkov et al., 2020). We utilize XLM-R (Conneau et al., 2020a) as our pre-trained Anchor points have been shown useful to improve cross-lingual transferability. Conneau et al. (2020b) analyzed the impact of anchor points in pre-training cross-lingual language models. Following Qin et al. (2020), we generate code-switch data in multiple languages as data augmentation. We randomly select words in the original text in the source language and replace them with target language words in the bilingual dictionaries to obtain code-switch data. Intuitively, this type of data augmentation explicitly helps pre-trained cross-"
2021.acl-long.264,D19-1575,1,0.837666,"ing, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source language training data and translated data in all targ"
2021.acl-long.327,P19-1279,0,0.10335,"L ℎ0 ?? ℎ&apos; Sentence Encoder L L M ℎ$ … ℎ&apos; … ℎ) Sentence Encoder ? = ?$, … , ?&apos; = ?, … , ?) ? = ?$, … , ?&apos; = ?, … , ?) (a) classification (b) sequence labeling Relation Encoder ℎ$ … ℎ&apos; … ℎ) Sentence Encoder ? = ?$, … , ?&apos; = ?, … , ?) (c) relation extraction Figure 1: Formulations of verb metaphor detection: (a) a single word classification model; (b) a sequence labeling model; (c) the proposed relation extraction model, where hs , hi , hc and r(hi , hc ) represent the representations of a sentence, a token, the context and the relation between the target verb v and its context components. dini Soares et al., 2019). Our model has three highlights, as illustrated in Figure 2. First, we explicitly extract and represent context components, such as a verb’s arguments as the local context, the whole sentence as the global context, and its basic meaning as a distant context. So multiple contexts can be modeled interactively and integrated together. Second, MrBERT enables modeling the metaphorical relation between a verb and its context components, and uses the relation representation for determining the metaphoricity of the verb. Third, the model is flexible to incorporate sophisticated relation modeling meth"
2021.acl-long.327,E06-1042,0,0.080028,"73 tokens are used for evaluating All-POS and Verb tracks, respectively. 2019; Stowe et al., 2019; Le et al., 2020). Therefore it is convenient for us to compare the proposed method with previous work. There are two tracks: Verb and All-POS metaphor detection. Some basic statistics of the dataset are shown in Table 1. We focus on the Verb track since we mainly model metaphorical relations for verbs. We use MrBERT’s relation-level predictions for the verb track and use its sequence labeling module to deal with the All-POS track. MOH-X and TroFi datasets MOH-X (Mohammad et al., 2016) and TroFi (Birke and Sarkar, 2006) are two relatively smaller datasets compared with VUA. Only a single target verb is annotated in each sentence. We will report the results on MOHX and TroFi in three settings: zero-shot transfer, re-training and fine-tuning. Metrics The evaluation metrics are accuracy (Acc), precision (P), recall (R) and F1-score (F1), which are most commonly used in previous work. 4.1.2 Baselines We compare with the following approaches. Evaluation Experimental Settings 4.1.1 Datasets and Evaluation Metrics VUA dataset We mainly conduct experiments on the VUA (Steen, 2010) dataset. It is the largest publicly"
2021.acl-long.327,E17-2084,0,0.0131016,"djective-noun relations (Shutova et al., 2016). The key to this category of work is to represent semantics and capture the relation between the arguments. Feature-based methods are based on handcrafted linguistic features. Shutova and Teufel (2010b) proposed to cluster nouns and verbs to construct semantic domains. Turney et al. (2011) and Shutova and Sun (2013) considered the abstractness of concepts and context. Mohler et al. (2013) exploited Wikipedia and WordNet to build domain signatures. Tsvetkov et al. (2014) combined abstractness, imageability, supersenses, and cross-lingual features. Bulat et al. (2017) exploited attribute-based concept representations. The above handcrafted features heavily rely on linguistic resources and expertise. Recently, distributed representations are exploited for grammatical relation-level metaphor detection. Distributed word embeddings were used as features (Tsvetkov et al., 2014) or to measure semantic relatedness (Gutiérrez et al., 2016; Mao et al., 2018). Visual distributed representations were also proven to be useful (Shutova et al., 2016). Rei et al. (2017) designed a supervised similarity network to capture interactions between words. Song et al. (2020) mod"
2021.acl-long.327,D14-1082,0,0.0352238,"ine the form of the relation r for capturing the interactions between v and c. • Linear model We use a parameter vector Vr ∈ Rd+k and a bias br to represent the relation r, and the probability of the relation being metaphorical is computed according to   v p(r|v, c) = σ(Vr> + br ), (1) c Then, we have to extract and represent these contexts. 3.1.2 Context Extraction and Representation We call the target verb’s contexts as context components. To get the contextual or basic meanings of these components. we use the deep transformer models, such as BERT. We first use Stanford dependency parser (Chen and Manning, 2014) to parse each sentence and extract verb-subject and verb-direct object relations with VB head and NN dependent. The nominal subjects and objects are used as the local context components. Motivated by (Baldini Soares et al., 2019), we introduce 6 component marker tokens, [subj], [/subj], [verb], [/verb], [obj] and [/obj], to explicitly label the boundaries of the target verb, its subject and object in each sentence. We also use [CLS] and [SEP ] to mark the whole sentence. For example, the marker inserted token sequence for the sentence He absorbed the costs for the accident is shown in Figure"
2021.acl-long.327,2020.figlang-1.32,0,0.0182436,"). These approaches are mostly based on neural network architectures and learn representations in an end-to-end fashion. These approaches depend on token-level human annotated datasets, such as the widely used VUA dataset (Steen, 2010). BiLSTM plus pre-trained word embeddings is one of the popular architectures for this task (Gao et al., 2018; Mao et al., 2019). Recently, Transformer based pre-trained language models become the most popular architecture in the metaphor detection shared task (Leong et al., 2020). Multitask learning (Dankers et al., 2019; Rohanian et al., 2020; Le et al., 2020; Chen et al., 2020) and discourse context (Dankers et al., 2020) have been exploited as well. Discussion The grammatical relation-level and token-level metaphor detection consider different aspects of information. Grammatical relations incorporate syntactic structures, which are well studied in selectional preferences (Wilks, 1975, 1978) and provide important clues for metaphor detection. However, sentential context is also useful but is ignored. In contrast, token-level metaphor detection explores wider context and gains improvements, but syntactic information is neglected and as discussed in (Zayed et al., 202"
2021.acl-long.327,2021.naacl-main.141,0,0.268618,"ost direct clues, other contexts in running text are mostly ignored. Recently, token-level neural metaphor detection draws more attention. Several approaches discov∗ These authors contributed equally to this work. ered that wider context can lead to better performance. Do Dinh and Gurevych (2016) considered a fixed window surrounding each target token as context. Gao et al. (2018) and Mao et al. (2018) argued that the full sentential context can provide strong clues for more accurate prediction. Some recent work also attempted to design models motivated by metaphor theories (Mao et al., 2019; Choi et al., 2021). Despite the progress of exploiting sentential context, there are still issues to be addressed. First of all, a word’s local context, its sentential context and other contexts should be all important for detecting metaphors; however, they are not well combined in previous work. More importantly, as shown in Figure 1, most token-level metaphor detection methods formulate metaphor detection as either a single-word classification or a sequence labeling problem (Gao et al., 2018). The context information is mainly used for learning contextual representations of tokens, rather than modeling the in"
2021.acl-long.327,J81-4005,0,0.667433,"Missing"
2021.acl-long.327,N19-1423,0,0.0164881,"of metaphoric expressions (Shutova and Teufel, 2010a). As shown in Figure 1, we propose to formulate verb metaphor detection as a relation extraction problem, instead of token classification or sequence labeling formulations. In analogy to identify the relations between entities, our method models the relations between a target verb and its various contexts, and determines the verb’s metaphoricity based on the relation representation rather than only the verb’s (contextual) representation. We present a simple yet effective model — Metaphor-relation BERT (MrBERT), which is adapted from a BERT (Devlin et al., 2019) based state-of-the-art relation learning model (Bal4240 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4240–4251 August 1–6, 2021. ©2021 Association for Computational Linguistics M ?? L ?(ℎ&apos; , ℎ2 ) M ?? L ℎ0 ?? ℎ&apos; Sentence Encoder L L M ℎ$ … ℎ&apos; … ℎ) Sentence Encoder ? = ?$, … , ?&apos; = ?, … , ?) ? = ?$, … , ?&apos; = ?, … , ?) (a) classification (b) sequence labeling Relation Encoder ℎ$ … ℎ&apos; … ℎ) Sentence Encoder ? = ?$, … , ?&apos; = ?, … , ?) (c) relation extraction Figure 1: Formul"
2021.acl-long.327,W16-1104,0,0.305428,"afted features (Shutova and Teufel, 2010b; Tsvetkov et al., 2013; Gutiérrez et al., 2016). Later, word embeddings and neural networks were introduced to alleviate the burden of feature engineering for relation-level metaphor detections (Rei et al., 2017; Mao et al., 2018). However, although grammatical relations provide the most direct clues, other contexts in running text are mostly ignored. Recently, token-level neural metaphor detection draws more attention. Several approaches discov∗ These authors contributed equally to this work. ered that wider context can lead to better performance. Do Dinh and Gurevych (2016) considered a fixed window surrounding each target token as context. Gao et al. (2018) and Mao et al. (2018) argued that the full sentential context can provide strong clues for more accurate prediction. Some recent work also attempted to design models motivated by metaphor theories (Mao et al., 2019; Choi et al., 2021). Despite the progress of exploiting sentential context, there are still issues to be addressed. First of all, a word’s local context, its sentential context and other contexts should be all important for detecting metaphors; however, they are not well combined in previous work."
2021.acl-long.327,J91-1003,0,0.757144,"ts. 2021). The results means MrBERT has good zeroshot transferability, although these datasets have quite different characteristics. In the 10-fold cross-validation setting, the retrained MrBERT can also obtain superior or competitive results compared with previous work. If we continue to fine-tune the pre-trained MrBERT on the target datasets, better performance can be obtained, especially on the MOH-X dataset. 5 Related Work Metaphor detection is a key task in metaphor processing (Veale et al., 2016). It is typically viewed as a classification problem. The early methods were based on rules (Fass, 1991; Narayanan, 1997), 4247 while most recent methods are data-driven. Next, we summarize data-driven methods from the perspective of context types that have been explored. Grammatical relation-level detection This line of work is to determine the metaphoricity of a given grammatical relation, such as verbsubject, verb-direct object or adjective-noun relations (Shutova et al., 2016). The key to this category of work is to represent semantics and capture the relation between the arguments. Feature-based methods are based on handcrafted linguistic features. Shutova and Teufel (2010b) proposed to cl"
2021.acl-long.327,D18-1060,0,0.0710492,"ter, word embeddings and neural networks were introduced to alleviate the burden of feature engineering for relation-level metaphor detections (Rei et al., 2017; Mao et al., 2018). However, although grammatical relations provide the most direct clues, other contexts in running text are mostly ignored. Recently, token-level neural metaphor detection draws more attention. Several approaches discov∗ These authors contributed equally to this work. ered that wider context can lead to better performance. Do Dinh and Gurevych (2016) considered a fixed window surrounding each target token as context. Gao et al. (2018) and Mao et al. (2018) argued that the full sentential context can provide strong clues for more accurate prediction. Some recent work also attempted to design models motivated by metaphor theories (Mao et al., 2019; Choi et al., 2021). Despite the progress of exploiting sentential context, there are still issues to be addressed. First of all, a word’s local context, its sentential context and other contexts should be all important for detecting metaphors; however, they are not well combined in previous work. More importantly, as shown in Figure 1, most token-level metaphor detection methods f"
2021.acl-long.327,P16-1018,0,0.101911,"y life for effective communication (Lakoff and Johnson, 1980). Metaphor processing has become an active research topic in natural language processing due to its importance in understanding implied meanings. This task is challenging, requiring contextual semantic representation and reasoning. Various contexts and linguistic representation techniques have been explored in previous work. Early methods focused on analyzing restricted forms of linguistic context, such as subjectverb-object type grammatical relations, based on hand-crafted features (Shutova and Teufel, 2010b; Tsvetkov et al., 2013; Gutiérrez et al., 2016). Later, word embeddings and neural networks were introduced to alleviate the burden of feature engineering for relation-level metaphor detections (Rei et al., 2017; Mao et al., 2018). However, although grammatical relations provide the most direct clues, other contexts in running text are mostly ignored. Recently, token-level neural metaphor detection draws more attention. Several approaches discov∗ These authors contributed equally to this work. ered that wider context can lead to better performance. Do Dinh and Gurevych (2016) considered a fixed window surrounding each target token as conte"
2021.acl-long.327,2020.figlang-1.31,0,0.042346,"ural network architectures and learn representations in an end-to-end fashion. These approaches depend on token-level human annotated datasets, such as the widely used VUA dataset (Steen, 2010). BiLSTM plus pre-trained word embeddings is one of the popular architectures for this task (Gao et al., 2018; Mao et al., 2019). Recently, Transformer based pre-trained language models become the most popular architecture in the metaphor detection shared task (Leong et al., 2020). Multitask learning (Dankers et al., 2019; Rohanian et al., 2020; Le et al., 2020; Chen et al., 2020) and discourse context (Dankers et al., 2020) have been exploited as well. Discussion The grammatical relation-level and token-level metaphor detection consider different aspects of information. Grammatical relations incorporate syntactic structures, which are well studied in selectional preferences (Wilks, 1975, 1978) and provide important clues for metaphor detection. However, sentential context is also useful but is ignored. In contrast, token-level metaphor detection explores wider context and gains improvements, but syntactic information is neglected and as discussed in (Zayed et al., 2020), interactions between metaphor components"
2021.acl-long.327,2020.figlang-1.3,0,0.0203209,"sification or sequence labeling problem (Do Dinh and Gurevych, 2016; Gao et al., 2018; Mao et al., 2019). These approaches are mostly based on neural network architectures and learn representations in an end-to-end fashion. These approaches depend on token-level human annotated datasets, such as the widely used VUA dataset (Steen, 2010). BiLSTM plus pre-trained word embeddings is one of the popular architectures for this task (Gao et al., 2018; Mao et al., 2019). Recently, Transformer based pre-trained language models become the most popular architecture in the metaphor detection shared task (Leong et al., 2020). Multitask learning (Dankers et al., 2019; Rohanian et al., 2020; Le et al., 2020; Chen et al., 2020) and discourse context (Dankers et al., 2020) have been exploited as well. Discussion The grammatical relation-level and token-level metaphor detection consider different aspects of information. Grammatical relations incorporate syntactic structures, which are well studied in selectional preferences (Wilks, 1975, 1978) and provide important clues for metaphor detection. However, sentential context is also useful but is ignored. In contrast, token-level metaphor detection explores wider context"
2021.acl-long.327,D19-1227,0,0.15267,"quence labeling problem (Do Dinh and Gurevych, 2016; Gao et al., 2018). As shown in Figure 1, the classification paradigm predicts a single binary label to indicate the metaphoricity of the target verb, while the sequence labeling paradigm predicts a sequence of binary labels to all tokens in a sentence. Based on the basic formulations, various approaches have tried to enhance feature representations by using globally trained contextual word embeddings (Gao et al., 2018) or incorporating wider context with powerful encoders such as BiLSTM (Gao et al., 2018; Mao et al., 2019) and Transformers (Dankers et al., 2019; Su et al., 2020). Limitations and recent trends However, the above two paradigms have some limitations. First, contextual information is mostly used to enhance the representation of the target word, but the interactions between the target word and its contexts are not explicitly modeled (Zayed et al., 2020; Su et al., 2020). To alleviate this, Su et al. (2020) proposed a new paradigm by viewing metaphor detection as a reading comprehension problem, which uses the target word as a query and captures its interactions with the sentence and clause. A concurrent work to this work (Choi et al., 20"
2021.acl-long.327,P18-1113,0,0.0286678,"Missing"
2021.acl-long.327,N13-1118,0,0.01472,"ummarize data-driven methods from the perspective of context types that have been explored. Grammatical relation-level detection This line of work is to determine the metaphoricity of a given grammatical relation, such as verbsubject, verb-direct object or adjective-noun relations (Shutova et al., 2016). The key to this category of work is to represent semantics and capture the relation between the arguments. Feature-based methods are based on handcrafted linguistic features. Shutova and Teufel (2010b) proposed to cluster nouns and verbs to construct semantic domains. Turney et al. (2011) and Shutova and Sun (2013) considered the abstractness of concepts and context. Mohler et al. (2013) exploited Wikipedia and WordNet to build domain signatures. Tsvetkov et al. (2014) combined abstractness, imageability, supersenses, and cross-lingual features. Bulat et al. (2017) exploited attribute-based concept representations. The above handcrafted features heavily rely on linguistic resources and expertise. Recently, distributed representations are exploited for grammatical relation-level metaphor detection. Distributed word embeddings were used as features (Tsvetkov et al., 2014) or to measure semantic relatednes"
2021.acl-long.327,P19-1378,0,0.0654021,"ions provide the most direct clues, other contexts in running text are mostly ignored. Recently, token-level neural metaphor detection draws more attention. Several approaches discov∗ These authors contributed equally to this work. ered that wider context can lead to better performance. Do Dinh and Gurevych (2016) considered a fixed window surrounding each target token as context. Gao et al. (2018) and Mao et al. (2018) argued that the full sentential context can provide strong clues for more accurate prediction. Some recent work also attempted to design models motivated by metaphor theories (Mao et al., 2019; Choi et al., 2021). Despite the progress of exploiting sentential context, there are still issues to be addressed. First of all, a word’s local context, its sentential context and other contexts should be all important for detecting metaphors; however, they are not well combined in previous work. More importantly, as shown in Figure 1, most token-level metaphor detection methods formulate metaphor detection as either a single-word classification or a sequence labeling problem (Gao et al., 2018). The context information is mainly used for learning contextual representations of tokens, rather"
2021.acl-long.327,shutova-teufel-2010-metaphor,0,0.105746,"1 Introduction Metaphor is ubiquitous in our daily life for effective communication (Lakoff and Johnson, 1980). Metaphor processing has become an active research topic in natural language processing due to its importance in understanding implied meanings. This task is challenging, requiring contextual semantic representation and reasoning. Various contexts and linguistic representation techniques have been explored in previous work. Early methods focused on analyzing restricted forms of linguistic context, such as subjectverb-object type grammatical relations, based on hand-crafted features (Shutova and Teufel, 2010b; Tsvetkov et al., 2013; Gutiérrez et al., 2016). Later, word embeddings and neural networks were introduced to alleviate the burden of feature engineering for relation-level metaphor detections (Rei et al., 2017; Mao et al., 2018). However, although grammatical relations provide the most direct clues, other contexts in running text are mostly ignored. Recently, token-level neural metaphor detection draws more attention. Several approaches discov∗ These authors contributed equally to this work. ered that wider context can lead to better performance. Do Dinh and Gurevych (2016) considered a fi"
2021.acl-long.327,S16-2003,0,0.0205104,"(Gao et al., 2018). 50,175 and 5,873 tokens are used for evaluating All-POS and Verb tracks, respectively. 2019; Stowe et al., 2019; Le et al., 2020). Therefore it is convenient for us to compare the proposed method with previous work. There are two tracks: Verb and All-POS metaphor detection. Some basic statistics of the dataset are shown in Table 1. We focus on the Verb track since we mainly model metaphorical relations for verbs. We use MrBERT’s relation-level predictions for the verb track and use its sequence labeling module to deal with the All-POS track. MOH-X and TroFi datasets MOH-X (Mohammad et al., 2016) and TroFi (Birke and Sarkar, 2006) are two relatively smaller datasets compared with VUA. Only a single target verb is annotated in each sentence. We will report the results on MOHX and TroFi in three settings: zero-shot transfer, re-training and fine-tuning. Metrics The evaluation metrics are accuracy (Acc), precision (P), recall (R) and F1-score (F1), which are most commonly used in previous work. 4.1.2 Baselines We compare with the following approaches. Evaluation Experimental Settings 4.1.1 Datasets and Evaluation Metrics VUA dataset We mainly conduct experiments on the VUA (Steen, 2010)"
2021.acl-long.327,W13-0904,0,0.0187045,"been explored. Grammatical relation-level detection This line of work is to determine the metaphoricity of a given grammatical relation, such as verbsubject, verb-direct object or adjective-noun relations (Shutova et al., 2016). The key to this category of work is to represent semantics and capture the relation between the arguments. Feature-based methods are based on handcrafted linguistic features. Shutova and Teufel (2010b) proposed to cluster nouns and verbs to construct semantic domains. Turney et al. (2011) and Shutova and Sun (2013) considered the abstractness of concepts and context. Mohler et al. (2013) exploited Wikipedia and WordNet to build domain signatures. Tsvetkov et al. (2014) combined abstractness, imageability, supersenses, and cross-lingual features. Bulat et al. (2017) exploited attribute-based concept representations. The above handcrafted features heavily rely on linguistic resources and expertise. Recently, distributed representations are exploited for grammatical relation-level metaphor detection. Distributed word embeddings were used as features (Tsvetkov et al., 2014) or to measure semantic relatedness (Gutiérrez et al., 2016; Mao et al., 2018). Visual distributed represent"
2021.acl-long.327,2020.coling-main.332,0,0.235641,"Metrics The evaluation metrics are accuracy (Acc), precision (P), recall (R) and F1-score (F1), which are most commonly used in previous work. 4.1.2 Baselines We compare with the following approaches. Evaluation Experimental Settings 4.1.1 Datasets and Evaluation Metrics VUA dataset We mainly conduct experiments on the VUA (Steen, 2010) dataset. It is the largest publicly available metaphor detection dataset and has been used in metaphor detection shared tasks (Leong et al., 2018, 2020). This dataset has a training set and a test set. Previous work utilized the training set in different ways (Neidlein et al., 2020). We use the preprocessed version of the VUA dataset provided by Gao et al. (2018). The first reason is that this dataset has a fixed development set so that different methods can adopt the same model selection strategy. The second reason is that several recent important methods used the same dataset (Mao et al., 2018; Dankers et al., 4244 • Gao et al. (2018) use contextual embeddings ELMo to enhance word representations and use BiLSTM as the encoder. It has two settings: classification (CLS) and sequence labeling (SEQ). • Mao et al. (2019) exploit two linguistic theory motivated intuitions ba"
2021.acl-long.327,D17-1162,0,0.0633568,"in understanding implied meanings. This task is challenging, requiring contextual semantic representation and reasoning. Various contexts and linguistic representation techniques have been explored in previous work. Early methods focused on analyzing restricted forms of linguistic context, such as subjectverb-object type grammatical relations, based on hand-crafted features (Shutova and Teufel, 2010b; Tsvetkov et al., 2013; Gutiérrez et al., 2016). Later, word embeddings and neural networks were introduced to alleviate the burden of feature engineering for relation-level metaphor detections (Rei et al., 2017; Mao et al., 2018). However, although grammatical relations provide the most direct clues, other contexts in running text are mostly ignored. Recently, token-level neural metaphor detection draws more attention. Several approaches discov∗ These authors contributed equally to this work. ered that wider context can lead to better performance. Do Dinh and Gurevych (2016) considered a fixed window surrounding each target token as context. Gao et al. (2018) and Mao et al. (2018) argued that the full sentential context can provide strong clues for more accurate prediction. Some recent work also att"
2021.acl-long.327,2020.acl-main.259,0,0.014819,"2016; Gao et al., 2018; Mao et al., 2019). These approaches are mostly based on neural network architectures and learn representations in an end-to-end fashion. These approaches depend on token-level human annotated datasets, such as the widely used VUA dataset (Steen, 2010). BiLSTM plus pre-trained word embeddings is one of the popular architectures for this task (Gao et al., 2018; Mao et al., 2019). Recently, Transformer based pre-trained language models become the most popular architecture in the metaphor detection shared task (Leong et al., 2020). Multitask learning (Dankers et al., 2019; Rohanian et al., 2020; Le et al., 2020; Chen et al., 2020) and discourse context (Dankers et al., 2020) have been exploited as well. Discussion The grammatical relation-level and token-level metaphor detection consider different aspects of information. Grammatical relations incorporate syntactic structures, which are well studied in selectional preferences (Wilks, 1975, 1978) and provide important clues for metaphor detection. However, sentential context is also useful but is ignored. In contrast, token-level metaphor detection explores wider context and gains improvements, but syntactic information is neglected a"
2021.acl-long.327,N16-1020,0,0.0165973,"e obtained, especially on the MOH-X dataset. 5 Related Work Metaphor detection is a key task in metaphor processing (Veale et al., 2016). It is typically viewed as a classification problem. The early methods were based on rules (Fass, 1991; Narayanan, 1997), 4247 while most recent methods are data-driven. Next, we summarize data-driven methods from the perspective of context types that have been explored. Grammatical relation-level detection This line of work is to determine the metaphoricity of a given grammatical relation, such as verbsubject, verb-direct object or adjective-noun relations (Shutova et al., 2016). The key to this category of work is to represent semantics and capture the relation between the arguments. Feature-based methods are based on handcrafted linguistic features. Shutova and Teufel (2010b) proposed to cluster nouns and verbs to construct semantic domains. Turney et al. (2011) and Shutova and Sun (2013) considered the abstractness of concepts and context. Mohler et al. (2013) exploited Wikipedia and WordNet to build domain signatures. Tsvetkov et al. (2014) combined abstractness, imageability, supersenses, and cross-lingual features. Bulat et al. (2017) exploited attribute-based"
2021.acl-long.327,K19-1034,0,0.189137,"ce and clause. A concurrent work to this work (Choi et al., 2021) adopted a pre-trained contextualized model based late interaction mechanism to compare the basic meaning and the contextual meaning of a word. Second, exploiting wider context will bring in more noise and may lose the focus. Fully depending on data-driven models to discover useful contexts is difficult, given the scale of available datasets for metaphor detection is still limited. The grammar structures, such as verb arguments, are important for metaphor processing (Wilks, 1978), but is not well incorporated into neural models. Stowe et al. (2019) showed that data augmentation based on syntactic patterns can enhance a standard model. Le et al. (2020) adopted graph convolutional networks to incorporate dependency graphs, but did 4241 ?( ⊕ ⊕ ⊕ ?(? = ?) ?(? = ?) ?(? = ?) ， ) ?( a context concatenation + + + ， ) b context average maxout ?( ， ) ?( ， ) ?( ， ) ?( ， ) c context maxout ?????ℎ?????? ? ?? ???????(?)? Relation Representation and Prediction (1) Sequence Prediction (2) (2) Deep Transformer (BERT) (3) [CLS] [subj] He [/subj] [verb] absorbed [/verb] the [obj] costs [/obj] for the accident [SEP] Figure 2: An example shows MrBERT’s main"
2021.acl-long.327,2020.figlang-1.4,0,0.284897,"m (Do Dinh and Gurevych, 2016; Gao et al., 2018). As shown in Figure 1, the classification paradigm predicts a single binary label to indicate the metaphoricity of the target verb, while the sequence labeling paradigm predicts a sequence of binary labels to all tokens in a sentence. Based on the basic formulations, various approaches have tried to enhance feature representations by using globally trained contextual word embeddings (Gao et al., 2018) or incorporating wider context with powerful encoders such as BiLSTM (Gao et al., 2018; Mao et al., 2019) and Transformers (Dankers et al., 2019; Su et al., 2020). Limitations and recent trends However, the above two paradigms have some limitations. First, contextual information is mostly used to enhance the representation of the target word, but the interactions between the target word and its contexts are not explicitly modeled (Zayed et al., 2020; Su et al., 2020). To alleviate this, Su et al. (2020) proposed a new paradigm by viewing metaphor detection as a reading comprehension problem, which uses the target word as a query and captures its interactions with the sentence and clause. A concurrent work to this work (Choi et al., 2021) adopted a pre-"
2021.acl-long.327,P14-1024,0,0.0255953,"rmine the metaphoricity of a given grammatical relation, such as verbsubject, verb-direct object or adjective-noun relations (Shutova et al., 2016). The key to this category of work is to represent semantics and capture the relation between the arguments. Feature-based methods are based on handcrafted linguistic features. Shutova and Teufel (2010b) proposed to cluster nouns and verbs to construct semantic domains. Turney et al. (2011) and Shutova and Sun (2013) considered the abstractness of concepts and context. Mohler et al. (2013) exploited Wikipedia and WordNet to build domain signatures. Tsvetkov et al. (2014) combined abstractness, imageability, supersenses, and cross-lingual features. Bulat et al. (2017) exploited attribute-based concept representations. The above handcrafted features heavily rely on linguistic resources and expertise. Recently, distributed representations are exploited for grammatical relation-level metaphor detection. Distributed word embeddings were used as features (Tsvetkov et al., 2014) or to measure semantic relatedness (Gutiérrez et al., 2016; Mao et al., 2018). Visual distributed representations were also proven to be useful (Shutova et al., 2016). Rei et al. (2017) desi"
2021.acl-long.327,W13-0906,0,0.0197219,"ubiquitous in our daily life for effective communication (Lakoff and Johnson, 1980). Metaphor processing has become an active research topic in natural language processing due to its importance in understanding implied meanings. This task is challenging, requiring contextual semantic representation and reasoning. Various contexts and linguistic representation techniques have been explored in previous work. Early methods focused on analyzing restricted forms of linguistic context, such as subjectverb-object type grammatical relations, based on hand-crafted features (Shutova and Teufel, 2010b; Tsvetkov et al., 2013; Gutiérrez et al., 2016). Later, word embeddings and neural networks were introduced to alleviate the burden of feature engineering for relation-level metaphor detections (Rei et al., 2017; Mao et al., 2018). However, although grammatical relations provide the most direct clues, other contexts in running text are mostly ignored. Recently, token-level neural metaphor detection draws more attention. Several approaches discov∗ These authors contributed equally to this work. ered that wider context can lead to better performance. Do Dinh and Gurevych (2016) considered a fixed window surrounding e"
2021.acl-long.327,D11-1063,0,0.160915,"contexts are entities, and metaphor detection is to determine whether a metaphorical relation holds between the verb and its contexts. We will introduce the proposed model in Section 3. Before diving into details, we argue that viewing metaphor as a relation is reasonable and consistent with existing metaphor theories. According to Wilks (1978), metaphors show a violation of selectional preferences in a given context. The conceptual metaphor theory views metaphors as transferring knowledge from a familiar, or concrete domain to an unfamiliar, or more abstract domain (Lakoff and Johnson, 1980; Turney et al., 2011). The metaphor identification procedure (MIP) theory (Group, 2007) aims to identify metaphorically used words in discourse based on comparing their use in particular context and their basic meanings. All the theories care about a kind of relations between a target word and its contexts, which may help identify metaphors. 3 Metaphor-Relation BERT (MrBERT) We propose the Metaphor-relation BERT (MrBERT) model to realize verb metaphor detection as a relation classification task. Figure 2 shows the architecture of MrBERT. We use the pre-trained language model BERT as the backbone model. There are t"
2021.acl-long.327,2020.findings-emnlp.36,0,0.40755,"xt, there are still issues to be addressed. First of all, a word’s local context, its sentential context and other contexts should be all important for detecting metaphors; however, they are not well combined in previous work. More importantly, as shown in Figure 1, most token-level metaphor detection methods formulate metaphor detection as either a single-word classification or a sequence labeling problem (Gao et al., 2018). The context information is mainly used for learning contextual representations of tokens, rather than modeling the interactions between the target word and its contexts (Zayed et al., 2020). In this paper, we focus on token-level verb metaphor detection, since verb metaphors are of the most frequent type of metaphoric expressions (Shutova and Teufel, 2010a). As shown in Figure 1, we propose to formulate verb metaphor detection as a relation extraction problem, instead of token classification or sequence labeling formulations. In analogy to identify the relations between entities, our method models the relations between a target verb and its various contexts, and determines the verb’s metaphoricity based on the relation representation rather than only the verb’s (contextual) repr"
2021.acl-long.339,P19-1601,0,0.0144492,"with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is content relevance to the dialogue history, rather than content preservation of the input. Second, the data for text style transfer is isomorphic. Data in different styles are in the same free-text format. However, our conversational data are context-response pairs while th"
2021.acl-long.339,N19-1423,0,0.0329375,"Missing"
2021.acl-long.339,N19-1125,0,0.0494538,"Missing"
2021.acl-long.339,D19-1190,0,0.278819,"onversational systems (Gan et al., 2017). Currently, most of the existing parallel datasets are not stylistically consistent. Samples in these datasets are usually contributed by a variety of users, resulting in an averaging effect across style characteristics (Zhang et al., 2018a). Meanwhile, constructing a parallel stylistic dataset for training the open-domain conversational agents is both labor-intensive and time-consuming. Recent studies show the effect of stylizing responses using a monolingual dataset in the desired style and a conventional conversational dataset (Niu and Bansal, 2018; Gao et al., 2019b). However, increasing style intensity often leads to Corresponding author. Style Intensity S2S: I just want to rent a room. Style Fusion: I hope I can share. S2S+LM: My friend had a considerable share in clearing the matter up. Ours: I should prefer having a partner to being alone. Figure 1: An example of responses generated by S2S, S2S+LM (Niu and Bansal, 2018), Style Fusion (Gao et al., 2019b), and our approach, targeting the Holmes style, which is quite formal and polite. Introduction ∗ Content Relevance the expense of decreasing content relevance between dialogue history and response. As"
2021.acl-long.339,N19-1320,0,0.015434,"4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is content relevance to the dialogue history, rather than content preservation of the input. Second, the data for text style transfer is isomorphic. Data in different styles are in the same free-text format. However, our conversational"
2021.acl-long.339,W19-5944,0,0.0241973,"Missing"
2021.acl-long.339,P19-1041,0,0.0187464,"An example of responses generated by baselines and our approach. in our approach. This is because most of the style information is disentangled into Z s in our approach, making its distribution different from sub-sequences of Z c . 4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is"
2021.acl-long.339,W16-6010,0,0.0273768,"and style in latent space by diluting sentence-level information in style representations. Combining the desired style representation and a response content representation will then obtain a stylistic response. Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines. Human evaluation results show that our approach significantly improves style intensity and maintains content relevance. 1 Linguistic style is an essential aspect of natural language interaction and provides particular ways of using language to engage with the audiences (Kabbara and Cheung, 2016). In human-bot conversations, it is crucial to generate stylistic responses for increasing user engagement to conversational systems (Gan et al., 2017). Currently, most of the existing parallel datasets are not stylistically consistent. Samples in these datasets are usually contributed by a variety of users, resulting in an averaging effect across style characteristics (Zhang et al., 2018a). Meanwhile, constructing a parallel stylistic dataset for training the open-domain conversational agents is both labor-intensive and time-consuming. Recent studies show the effect of stylizing responses usi"
2021.acl-long.339,P18-1139,0,0.0493674,"Missing"
2021.acl-long.339,N16-1014,0,0.0876725,"Missing"
2021.acl-long.339,N18-1169,0,0.0593197,"Missing"
2021.acl-long.339,I17-1099,0,0.0634669,"orpus-level feature since sentences within a dataset have the same style. In contrast, the content representation is a sentence-level feature decided by a sentence itself. We thus disentangle the content and style by diluting sentence-level information in the style representation. This encourages the encoding of content information into the content representation. Otherwise, the content information will be corrupted in the style representation, making it hard to reconstruct the original content in the subsequent decoding process. We conduct experiments on DailyDialogue conversational dataset (Li et al., 2017) and Holmes monolingual stylistic dataset (Gao et al., 2019b). Experimental results show that our proposed approach improves style intensity and maintains content relevance. Our contributions are listed below: • We propose a unified framework to simultaneously improve style intensity and maintain content relevance for neural stylistic response generation. • We introduce a scheme of learning latent variables by a diluting strategy to disentangle the style and content. • Experimental results show that our approach achieves higher performance in style intensity without decreasing content relevanc"
2021.acl-long.339,D16-1230,0,0.0685257,"Missing"
2021.acl-long.339,Q18-1027,0,0.0959872,"g user engagement to conversational systems (Gan et al., 2017). Currently, most of the existing parallel datasets are not stylistically consistent. Samples in these datasets are usually contributed by a variety of users, resulting in an averaging effect across style characteristics (Zhang et al., 2018a). Meanwhile, constructing a parallel stylistic dataset for training the open-domain conversational agents is both labor-intensive and time-consuming. Recent studies show the effect of stylizing responses using a monolingual dataset in the desired style and a conventional conversational dataset (Niu and Bansal, 2018; Gao et al., 2019b). However, increasing style intensity often leads to Corresponding author. Style Intensity S2S: I just want to rent a room. Style Fusion: I hope I can share. S2S+LM: My friend had a considerable share in clearing the matter up. Ours: I should prefer having a partner to being alone. Figure 1: An example of responses generated by S2S, S2S+LM (Niu and Bansal, 2018), Style Fusion (Gao et al., 2019b), and our approach, targeting the Holmes style, which is quite formal and polite. Introduction ∗ Content Relevance the expense of decreasing content relevance between dialogue histor"
2021.acl-long.339,P18-1080,0,0.0185419,"forward to getting the information about that. S2S+LM Style Fusion Ours Table 7: An example of responses generated by baselines and our approach. in our approach. This is because most of the style information is disentangled into Z s in our approach, making its distribution different from sub-sequences of Z c . 4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style tran"
2021.acl-long.339,P18-2031,0,0.028426,"out that. S2S+LM Style Fusion Ours Table 7: An example of responses generated by baselines and our approach. in our approach. This is because most of the style information is disentangled into Z s in our approach, making its distribution different from sub-sequences of Z c . 4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to creat"
2021.acl-long.339,P15-1152,0,0.0290416,"y. The maximum length is set to 90 for the dialogue history and 30 for the response. During the training process, we use the ADAM optimizer, whose learning rate is 0.0003. σ 2 for sampling  in Equation 8 is 0.12 . Table 2 shows the average running time on a single TITAN X (Pascal) GPU. During the inference process, the weights γ and η for re-ranking are set to 0.5. The weight (accuracy) of n-gram classifier is 0.93, 0.87, 0.77, and 0.65 for n from 1 to 4. The number of candidate responses, Ny , is set to 10. The radius r is set to 3. • S2S, the sequence-to-sequence response generation model (Shang et al., 2015). 4 • S2S+LM, a S2S trained on C and a stylistic language model trained on S (Niu and Bansal, 2018). During the inference process, it generates a stylistic response by interpolating outputs of the two models. Automatic Evaluation Considering that it is unfair to evaluate a response by the classifiers that are used for selecting the response (Song et al., 2020), we fine-tune a BERT (Devlin et al., 2019) to measure style intensity. Concretely, positive samples are the stylistic sentences. Negative samples are 2 3 Results 4.1 http://yanran.li/dailydialog https://github.com/golsun/StyleFusion 4 43"
2021.acl-long.339,2020.coling-main.203,0,0.0411487,"ement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is content relevance to the dialogue history, rather than content preservation of the input. Second, the data for text style transfer is isomorphic. Data in different styles are in the same free-text format. However, our conversational data are context-response pairs while the stylistic data are free-texts, which"
2021.acl-long.339,P18-1090,0,0.0284223,"our approach. This is because most of the style information is disentangled into Z s in our approach, making its distribution different from sub-sequences of Z c . 4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is content relevance to the dialogue history, rather than content pre"
2021.acl-long.339,2020.findings-emnlp.140,0,0.0647547,"Missing"
2021.acl-long.339,N18-1138,0,0.0591403,"Missing"
2021.acl-long.339,2020.acl-main.294,0,0.253563,"(8) ¯ s remains The batch average style representation Z ¯ s (Sj ) consistent with the target, i.e., being Z AE when the target is Sj . The updated smoothness objective is as follows: c s ¯AE Lsmooth,conv = − log P (Yi |[Zconv :Z (Yi )]), c s ¯ Lsmooth,style = −U log P (Yi |[Zstyle : ZAE (Yi )]) s s s ¯AE Zsoft = ZS2S (Xi ) + α ∗ Z (Sj ), c s ¯AE − (1 − U ) log P (Sj |[Zstyle :Z (Sj )]). (9) The final training loss is the sum of the response generation loss, fusion objective, and smoothness objective: L =LS2S + Lfuse + Lsmooth . (10) Here, we do not employ pre-training models, i.e., DialoGPT (Zhang et al., 2020b) and OpenAI GPT2 (Radford et al., 2019). This is because the disentanglement is usually conducted on a sentence representation. While most of the pre-training models depend on the attention mechanism, and there is no static global sentence representation during the decoding process. 2.4 c (X ) by S2S encoder and subsequently obtain ZS2S i c (X ) sample Z c (Yˆi ) from the hypersphere of ZS2S i with a mannually tuned radius r. After that, we gen¯ s (Sj ), erate Yˆi by concatenating Z c (Yˆi ) and Z AE which is the batch average style representation of randomly sampled stylistic sentences. Con"
2021.acl-long.339,D18-1138,0,0.135572,"s style intensity and maintains content relevance. 1 Linguistic style is an essential aspect of natural language interaction and provides particular ways of using language to engage with the audiences (Kabbara and Cheung, 2016). In human-bot conversations, it is crucial to generate stylistic responses for increasing user engagement to conversational systems (Gan et al., 2017). Currently, most of the existing parallel datasets are not stylistically consistent. Samples in these datasets are usually contributed by a variety of users, resulting in an averaging effect across style characteristics (Zhang et al., 2018a). Meanwhile, constructing a parallel stylistic dataset for training the open-domain conversational agents is both labor-intensive and time-consuming. Recent studies show the effect of stylizing responses using a monolingual dataset in the desired style and a conventional conversational dataset (Niu and Bansal, 2018; Gao et al., 2019b). However, increasing style intensity often leads to Corresponding author. Style Intensity S2S: I just want to rent a room. Style Fusion: I hope I can share. S2S+LM: My friend had a considerable share in clearing the matter up. Ours: I should prefer having a par"
2021.acl-long.339,2020.acl-demos.30,0,0.392173,"(8) ¯ s remains The batch average style representation Z ¯ s (Sj ) consistent with the target, i.e., being Z AE when the target is Sj . The updated smoothness objective is as follows: c s ¯AE Lsmooth,conv = − log P (Yi |[Zconv :Z (Yi )]), c s ¯ Lsmooth,style = −U log P (Yi |[Zstyle : ZAE (Yi )]) s s s ¯AE Zsoft = ZS2S (Xi ) + α ∗ Z (Sj ), c s ¯AE − (1 − U ) log P (Sj |[Zstyle :Z (Sj )]). (9) The final training loss is the sum of the response generation loss, fusion objective, and smoothness objective: L =LS2S + Lfuse + Lsmooth . (10) Here, we do not employ pre-training models, i.e., DialoGPT (Zhang et al., 2020b) and OpenAI GPT2 (Radford et al., 2019). This is because the disentanglement is usually conducted on a sentence representation. While most of the pre-training models depend on the attention mechanism, and there is no static global sentence representation during the decoding process. 2.4 c (X ) by S2S encoder and subsequently obtain ZS2S i c (X ) sample Z c (Yˆi ) from the hypersphere of ZS2S i with a mannually tuned radius r. After that, we gen¯ s (Sj ), erate Yˆi by concatenating Z c (Yˆi ) and Z AE which is the batch average style representation of randomly sampled stylistic sentences. Con"
2021.acl-long.403,D17-1070,0,0.244557,"sample 300,000 positive and 300,000 negative instances from the auxiliary dataset. Then given an event pair (Vi , Vj ), the finetuned RoBERTa-large model would be able to predict the probability that Vj is the subsequent event of Vi . Event Graph Based Pseudo Instance Set for Pretraining ege-RoBERTa To effectively utilize the event graph knowledge, we induce a set of pseudo instances for pretraining the ege-RoBERTa model. Specifically, given a five-sentence-story within the auxiliary dataset, as Table 1 shows, we define the 1st and 5th sentence of the story as two 5185 Methods SVM Infersent (Conneau et al., 2017) GPT (Radford et al., 2018) BERT-base (Devlin et al., 2019) RoBERTa-base (Liu et al., 2019) BERT-large (Devlin et al., 2019) RoBERTa-large (Liu et al., 2019) Concurrent Methods L2 R (Zhu et al., 2020) RoBERTa-GPT-MHKA (Paul et al., 2020) This Work ege-RoBERTa-largeu ege-RoBERTa-largeλ=0 ege-RoBERTa-base ege-RoBERTa-large Human Performance observed events, the 3rd sentence as the hypothesis event, the 2nd and 4th sentence as intermediary events, respectively. In this way, the posterior event sequence X 0 and the event sequence X of a pseudo instance could be obtained. In addition, given X 0 , w"
2021.acl-long.403,N19-1423,0,0.565079,"f natural language in a formal logic system. To facilitate this, Bhagavatula et al. (2019) proposed a natural language based abductive reasoning task αNLI. As shown in Figure 1 (a), given two observed events O1 and O2 , the αNLI task requires the prediction model to choose a more reasonable explanation from two candidate hypothesis events H1 and H2 . Both observed events and hypothesis events are daily-life events, and are described in natural language. Together with the αNLI task, Bhagavatula et al. (2019) also explored conducting such reasoning using pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). However, despite pretrained language models could capture rich linguistic knowledge benefit for understanding the semantics of events, additional commonsense knowledge is still necessary for the abductive reasoning. For example, as illustrated in Figure 1 (b), given observations O1 and O2 , to choose the more likely explanation H1 : A thief 5181 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5181–5190 August 1–6, 2021. ©2021 Association for"
2021.acl-long.403,N16-1147,0,0.0380804,") A Pseudo Instance={X, X’, A}, where 0 X = (O1 , H1 , O2 ); X = (O1 , I1 , H1 , I2 , O2 ) A is initialized from the event graph. Optimizing Experiments 5.1 αNLI Dataset The αNLI dataset (Bhagavatula et al., 2019) consists of 169,654, 1,532 and 4,056 hO1 , O2 , H1 , H2 i Construction of Event Graph The event graph serves as an external knowledge base to provide information about the relationship between observation events and intermediary events. To this end, we build the event graph based on an auxiliary dataset, which are composed of two short story corpora independent to αNLI, i.e., VIST (Huang et al., 2016), and TimeTravel (Qin et al., 2019). Both VIST and TimeTravel are composed of five-sentences short stories. Totally there are 121,326 stories in the auxiliary dataset. To construct the event graph, we define each sentence in the auxiliary dataset as a node in the event graph. To get the edge weight Wij between two nodes Vi and Vj (i.e., the probability that Vj is the subsequent event of Vi ), we finetune a RoBERTa-large model through a next sentence prediction task. Specifically, we define adjacent sentence pairs in the story text (for example, [1st, 2nd] sentence, [4th, 5th] sentence of a sto"
2021.acl-long.403,2021.ccl-1.108,0,0.0667599,"Missing"
2021.acl-long.403,N16-1098,0,0.0213487,"he pre-training stage ege-RoBERTa is trained to predict the masked tokens in the event sequence X rather than the relatedness score. In addition, in order to balance the masked token prediction loss with the KL term, we introduce an additional hyperparameter λ. Hence, the objective function in the pretraining stage is defined as follows: LELBO (θ, φ) =Eq(z|X 0 ,A) logLM LM (X, z; θ) − λKL(qφ (z|X 0 , A)||pθ (z|X)), L(θ) = pθ (Y |z, X) = pθ (Y |z, X)pθ (z|X). quadruples in training, development and test set, respectively. The observation events are collected from a short story corpus ROCstory (Mostafazadeh et al., 2016), while all of hypothesis events are independently generated through crowdsourcing. 5.2 (15) Training Details We implement two different sizes of ege-RoBERTa model (i.e. ege-RoBERTa-base and ege-RoBERTalarge) based on RoBERTa-base framework and RoBERTa-large framework, respectively. For the ege-RoBERTa-base model, in the aggregator, the prior network, the recognition network and the merger, the dimension of the attention mechanism d is set as 768, and all multi-head attention layers contain 12 heads. While for the ege-RoBERTa-large model, d is equal to 1024 and all multi-head attention layers"
2021.acl-long.403,P13-1170,0,0.0684874,"Missing"
2021.acl-long.403,2020.findings-emnlp.267,0,0.0373436,"Missing"
2021.acl-long.403,D19-1509,0,0.0354924,"Missing"
2021.dialdoc-1.7,2020.emnlp-main.652,0,0.623715,"Missing"
2021.dialdoc-1.7,2021.ccl-1.108,0,0.0396085,"Missing"
2021.dialdoc-1.7,D18-1255,0,0.0281243,"ing knowledge in form of document span for the next agent turn. The input is dialogue history, current user utterance, and associated document. The output should be a text span. The evaluation metrics are Exact Match (EM) and F1 (Rajpurkar et al., 2016). Sub-task2 is text generation which requires generating the next agent response in natural language. The input is dialogue history and † Document-grounded Dialogue (DGD) & Conversational QA (CQA) The DGD maintains a dialogue pattern where external knowledge used in dialogues can be obtained from the given document. Recently, some DGD datasets (Moghe et al., 2018; Dinan et al., 2019) have been released to exploiting unstructured document information in open-domain dialogues. The Doc2Dial dataset is also document-grounded dialogue. However, the dialogue in Doc2Dial is goaloriented which guides users to access various forms of information according to their needs. The CQA (such as CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018) and DoQA (Campos et al., 2020)) task is also based on background document, which aims to understand a text passage and answering a series of interconnected questions that appear in a conversation. The difference between DGD a"
2021.dialdoc-1.7,2020.acl-main.652,0,0.145465,"Missing"
2021.dialdoc-1.7,D14-1162,0,0.0838795,"Missing"
2021.dialdoc-1.7,W18-6319,0,0.102333,"Missing"
2021.dialdoc-1.7,N19-1423,0,0.00966837,"hey could not resolve the out-of-vocabulary (OOV) problem and the ambiguity of words in different contexts. To address these problems, Pre-trained These three authors contributed equally. Corresponding author. 52 Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 52–56 August 5–6, 2021. ©2021 Association for Computational Linguistics Data Augmentation Pre-training (Bert, Roberta) Fine-tuning (Bert, Roberta, and Electra) Postprocessing Ensemble Figure 1: The pipeline methods we used in the competition. Language Models (PLMs) such as BERT (Devlin et al., 2019) were introduced. BERT employed a Masked language modeling (MLM) method that first masked out some tokens from the input sentences and then trained the model to predict the masked tokens by the rest of the tokens. Concurrently, there was research proposing different enhanced versions of MLM to further improve on BERT. Instead of static masking, RoBERTa (Liu et al., 2019) improved BERT by dynamic masking and abandoned the Next Sentence Prediction (NSP) loss. Instead of masking the input, ELECTRA (Clark et al., 2020) replaced some input tokens with plausible alternatives sampled from a small gen"
2021.emnlp-demo.6,N19-4010,0,0.0636961,"Missing"
2021.emnlp-demo.6,2020.findings-emnlp.58,1,0.843454,"the need for any knowledge. The following code snippet in Figure 4 shows 45 Chinese Word Segmentation F Part-of-Speech Tagging FLAS Named Entity Recognition F Dependency Parsing F Stanza (Qi et al., 2020) 92.40 98.10 89.50 84.98 - - N-LTP trained separately N-LTP trained jointly with distillation 98.55 99.18 98.35 98.69 95.41 95.97 90.12 90.19 74.47 76.62 79.23 79.49 Model Semantic Dependency Parsing F Semantic Role Labeling F Table 2: Main Results. “-"" represents the absence of tasks in the Stanza toolkit and we cannot report the results. The N-LTP model is based on the Chinese ELECTRA base (Cui et al., 2020). The learning ratio (lr) for teacher models, student model and CRF layer is {1e − 4}, {1e − 4}, {1e − 3}, respectively. The gradient clip value adopted in our experiment is 1.0 and the warmup proportion is 0.02. We use BertAdam (Devlin et al., 2019) to optimize the parameters and adopted the suggested hyper-parameters for optimization. 4.2 Results We compare N-LTP with the state-of-the-art toolkit Stanza. For a fair comparison, we conduct experiments on the same datasets that Stanza adopted. The results are shown in Table 2, we have the following observations: • N-LTP outperforms Stanza on fo"
2021.emnlp-demo.6,N19-1423,0,0.127433,"lated tasks, which has obtained remarkable success on various NLP tasks (Qin et al., 2019; Wang et al., 2020; Zhou et al., 2021). Inspired by this, we adopt the SOTA pre-trained model (ELECTRA) (Clark et al., 2020) as the shared encoder to capture shared knowledge across six Chinese tasks. Given an input utterance s = (s1 , s2 , . . . , sn ), we first construct the input sequence by adding specific tokens s = ([CLS], s1 , s2 , . . . , sn , [SEP]), where [CLS] is the special symbol for representing the whole sequence, and [SEP] is the special symbol to separate non-consecutive token sequences (Devlin et al., 2019). ELECTRA takes the constructed input and output the corresponding hidden representations of sequence H = (h[CLS] , h1 , h2 , . . . , hn , h[SEP] ). 2.2 hˆi = AdaptedTransformer(hi ), (3) ˆ [CLS] , h ˆ 1, h ˆ 2, . . . , h ˆ n, h ˆ [SEP] ) are ˆ = (h where H the updated representations. Finally, similar to CWS and POS, we use a linear decoder to classify label for each word: yi = Softmax(W NER hˆi + bNER ), (4) where yi denotes the NER label probability distribution of each character. Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NL"
2021.emnlp-demo.6,C18-1233,0,0.0601213,"Missing"
2021.emnlp-demo.6,2020.findings-emnlp.425,0,0.0420593,"Missing"
2021.emnlp-demo.6,C10-3004,1,0.702421,"Missing"
2021.emnlp-demo.6,C96-1058,0,0.462606,"bution of each character. Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NLP). In N-LTP, following Xue (2003), CWS is regarded as a character based sequence labeling problem. Specifically, given the hidden representations H = (h[CLS] , h1 , h2 , . . . , hn , h[SEP] ), we adopt a linear decoder to classify each character: 2.5 Dependency Parsing Dependency parsing is the task to analyze the semantic structure of a sentence. In N-LTP, we implement a deep biaffine neural dependency parser (Dozat and Manning, 2017) and einser algorithm (Eisner, 1996) to obtain the parsing result, which is formulated as: (1) where yi denotes the label probability distribution of each character; W CWS and bCWS are trainable parameters. 2.3 Named Entity Recognition The named entity recognition (NER) is the task of finding the start and end of an entity (people, locations, organizations, etc.) in a sentence and assigning a class for this entity. Traditional, NER is regarded as a sequence labeling task. After obtaining the hidden representations H, we follow Yan et al. (2019a) to adopt the Adapted-Transformer to consider directionand distance-aware characteris"
2021.emnlp-demo.6,S12-1050,1,0.760309,"ithm. More specifically, instead of simply shuffling the datasets for our multi-task models, we follow the task sampling procedure from Bowman et al. (2018), where the probability of training on an example for a particular task τ is proportional to |Dτ |0.75 . This ensures that tasks with large datasets don’t overly dominate the training. The above process is also used for scoring a l labeled dependency i↶ j, by extending the 1-dim vector s into L dims, where L is the total number of dependency labels. 2.6 Semantic Dependency Parsing Similar to dependency parsing, semantic dependency parsing (Che et al., 2012, SDP) is a task to capture the semantic structure of a sentence. Specifically, given an input sentence, SDP aims at determining all the word pairs related to each other semantically and assigning specific predefined semantic relations. Following Dozat and Manning (2017), we adopt a biaffine module to perform the task, using pi↶ j = sigmoid(yi↶ j ). Knowledge Distillation (7) If pi↶ j &gt; 0.5, wordi to wordj exists an edge. 2.7 3 Semantic Role Labeling N-LTP is a PyTorch-based Chinese NLP toolkit based on the above model. All the configurations can be initialized from JSON files, and thus it is"
2021.emnlp-demo.6,I17-2014,0,0.0229729,"rocess NLP tasks in Chinese quickly. Recently, Qi et al. (2020) introduce the Python NLP toolkit Stanza for multi-lingual languages, including Chinese language. Though Stanza can be directly applied for processing the Chinese texts, it suffers from several limitations. First, it only supports part of Chinese NLP tasks. For example, it fails to handle semantic parsing analysis, resulting in incomplete analysis in Chinese NLP. Second, it trained each task separately, ignoring the shared knowledge across the related tasks, which has been proven effective for Chinese NLP tasks (Qian et al., 2015; Hsieh et al., 2017; Chang et al., 2018). Third, independent modeling method will occupy more Introduction There is a wide of range of existing natural language processing (NLP) toolkits such as CoreNLP (Manning et al., 2014), UDPipe (Straka and Straková, 2017), FLAIR (Akbik et al., 2019), spaCy,1 and Stanza (Qi et al., 2020) in English, which makes it easier for users to build tools with sophisticated linguistic processing. Recently, the need for Chinese NLP has a dramatic increase in many downstream applications. A Chinese NLP platform usually includes lexical analysis (Chinese word segmentation (CWS), part-of"
2021.emnlp-demo.6,P19-1595,0,0.340222,"ology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks. In addition, a knowledge distillation method (Clark et al., 2019) where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher. Finally, we provide a collection of easy-to-use APIs and a visualization tool to make users to use and view the processing results more easily and directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks. Source code, documentation, and pre-trained models are available at https: //github.com/HIT-SCIR/ltp. 1 preprocess Output Sentence Split lexical analysis Word Segmentation Visualization Tool"
2021.emnlp-demo.6,D19-1399,0,0.04531,"Missing"
2021.emnlp-demo.6,O03-4002,0,0.138111,"tructed input and output the corresponding hidden representations of sequence H = (h[CLS] , h1 , h2 , . . . , hn , h[SEP] ). 2.2 hˆi = AdaptedTransformer(hi ), (3) ˆ [CLS] , h ˆ 1, h ˆ 2, . . . , h ˆ n, h ˆ [SEP] ) are ˆ = (h where H the updated representations. Finally, similar to CWS and POS, we use a linear decoder to classify label for each word: yi = Softmax(W NER hˆi + bNER ), (4) where yi denotes the NER label probability distribution of each character. Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NLP). In N-LTP, following Xue (2003), CWS is regarded as a character based sequence labeling problem. Specifically, given the hidden representations H = (h[CLS] , h1 , h2 , . . . , hn , h[SEP] ), we adopt a linear decoder to classify each character: 2.5 Dependency Parsing Dependency parsing is the task to analyze the semantic structure of a sentence. In N-LTP, we implement a deep biaffine neural dependency parser (Dozat and Manning, 2017) and einser algorithm (Eisner, 1996) to obtain the parsing result, which is formulated as: (1) where yi denotes the label probability distribution of each character; W CWS and bCWS are trainable"
2021.emnlp-demo.6,D18-1191,0,0.0422662,"Missing"
2021.emnlp-demo.6,2020.tacl-1.6,0,0.0494743,"Missing"
2021.emnlp-demo.6,2020.acl-demos.14,0,0.276993,"ing, and semantic parsing. In addition, we provide the visualization tool and easy-to-use API to help users easily use N-LTP. tagging, and named entity recognition (NER)), syntactic parsing (dependency parsing (DEP)), and semantic parsing (semantic dependency parsing (SDP) and semantic role labeling (SRL)). Unfortunately, there are relatively fewer high-performance and high-efficiency toolkits for Chinese NLP tasks. To fill this gap, it’s important to build a Chinese NLP toolkit to support rich Chinese fundamental NLP tasks, and make researchers process NLP tasks in Chinese quickly. Recently, Qi et al. (2020) introduce the Python NLP toolkit Stanza for multi-lingual languages, including Chinese language. Though Stanza can be directly applied for processing the Chinese texts, it suffers from several limitations. First, it only supports part of Chinese NLP tasks. For example, it fails to handle semantic parsing analysis, resulting in incomplete analysis in Chinese NLP. Second, it trained each task separately, ignoring the shared knowledge across the related tasks, which has been proven effective for Chinese NLP tasks (Qian et al., 2015; Hsieh et al., 2017; Chang et al., 2018). Third, independent mod"
2021.emnlp-demo.6,2021.acl-long.485,0,0.0153385,"utput the corresponding POS sequence labels, which is formulated as: SDP SRL CWS ℎ[&'(] ℎ! ℎ"" … ℎ$ ℎ[(*+] s# [SEP] Shared Encoder ELECTRA yi = Softmax(W POS hi + bPOS) , Input Sentences [CLS] s! s"" … where yi denotes the POS label probability distribution of the i-th character; hi is the first sub-token representation of word si . Figure 2: The architecture of the proposed model. 2.1 Shared Encoder 2.4 Multi-task framework uses a shared encoder to extract the shared knowledge across related tasks, which has obtained remarkable success on various NLP tasks (Qin et al., 2019; Wang et al., 2020; Zhou et al., 2021). Inspired by this, we adopt the SOTA pre-trained model (ELECTRA) (Clark et al., 2020) as the shared encoder to capture shared knowledge across six Chinese tasks. Given an input utterance s = (s1 , s2 , . . . , sn ), we first construct the input sequence by adding specific tokens s = ([CLS], s1 , s2 , . . . , sn , [SEP]), where [CLS] is the special symbol for representing the whole sequence, and [SEP] is the special symbol to separate non-consecutive token sequences (Devlin et al., 2019). ELECTRA takes the constructed input and output the corresponding hidden representations of sequence H = (h"
2021.emnlp-demo.6,D15-1211,0,0.01296,"make researchers process NLP tasks in Chinese quickly. Recently, Qi et al. (2020) introduce the Python NLP toolkit Stanza for multi-lingual languages, including Chinese language. Though Stanza can be directly applied for processing the Chinese texts, it suffers from several limitations. First, it only supports part of Chinese NLP tasks. For example, it fails to handle semantic parsing analysis, resulting in incomplete analysis in Chinese NLP. Second, it trained each task separately, ignoring the shared knowledge across the related tasks, which has been proven effective for Chinese NLP tasks (Qian et al., 2015; Hsieh et al., 2017; Chang et al., 2018). Third, independent modeling method will occupy more Introduction There is a wide of range of existing natural language processing (NLP) toolkits such as CoreNLP (Manning et al., 2014), UDPipe (Straka and Straková, 2017), FLAIR (Akbik et al., 2019), spaCy,1 and Stanza (Qi et al., 2020) in English, which makes it easier for users to build tools with sophisticated linguistic processing. Recently, the need for Chinese NLP has a dramatic increase in many downstream applications. A Chinese NLP platform usually includes lexical analysis (Chinese word segment"
2021.emnlp-demo.6,D19-1214,1,0.82673,"dden representations H as input and output the corresponding POS sequence labels, which is formulated as: SDP SRL CWS ℎ[&'(] ℎ! ℎ"" … ℎ$ ℎ[(*+] s# [SEP] Shared Encoder ELECTRA yi = Softmax(W POS hi + bPOS) , Input Sentences [CLS] s! s"" … where yi denotes the POS label probability distribution of the i-th character; hi is the first sub-token representation of word si . Figure 2: The architecture of the proposed model. 2.1 Shared Encoder 2.4 Multi-task framework uses a shared encoder to extract the shared knowledge across related tasks, which has obtained remarkable success on various NLP tasks (Qin et al., 2019; Wang et al., 2020; Zhou et al., 2021). Inspired by this, we adopt the SOTA pre-trained model (ELECTRA) (Clark et al., 2020) as the shared encoder to capture shared knowledge across six Chinese tasks. Given an input utterance s = (s1 , s2 , . . . , sn ), we first construct the input sequence by adding specific tokens s = ([CLS], s1 , s2 , . . . , sn , [SEP]), where [CLS] is the special symbol for representing the whole sequence, and [SEP] is the special symbol to separate non-consecutive token sequences (Devlin et al., 2019). ELECTRA takes the constructed input and output the corresponding hi"
2021.emnlp-demo.6,W96-0213,0,0.881383,"Missing"
2021.emnlp-demo.6,K17-3009,0,0.0303773,"t suffers from several limitations. First, it only supports part of Chinese NLP tasks. For example, it fails to handle semantic parsing analysis, resulting in incomplete analysis in Chinese NLP. Second, it trained each task separately, ignoring the shared knowledge across the related tasks, which has been proven effective for Chinese NLP tasks (Qian et al., 2015; Hsieh et al., 2017; Chang et al., 2018). Third, independent modeling method will occupy more Introduction There is a wide of range of existing natural language processing (NLP) toolkits such as CoreNLP (Manning et al., 2014), UDPipe (Straka and Straková, 2017), FLAIR (Akbik et al., 2019), spaCy,1 and Stanza (Qi et al., 2020) in English, which makes it easier for users to build tools with sophisticated linguistic processing. Recently, the need for Chinese NLP has a dramatic increase in many downstream applications. A Chinese NLP platform usually includes lexical analysis (Chinese word segmentation (CWS), part-of-speech (POS) 1 Processor Toolkit https://spacy.io 42 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 42–49 August 1–6, 2021. ©2021 Association for Computational Linguistics"
2021.emnlp-demo.6,2020.emnlp-main.75,0,0.0856429,"Missing"
2021.emnlp-main.16,2020.findings-emnlp.91,0,0.044654,"Missing"
2021.emnlp-main.16,N19-1423,0,0.005112,"a node pruning 8608 samples for each. Besides, a small test set approach to prune the nodes automatically to filter with 2K samples is provided for human evaluation. such nodes. Figure 4 shows an example. We first Samples in the dataset require symbolic reasoning calculate the relevance score between the node and with logical operations, such as &quot;min&quot;, &quot;argmax&quot;, statement as follows: &quot;count&quot;, etc. Following the existing work, we use p s accuracy as the evaluation metric. si = σ(Ws [hi ||h ] + bs ) (6) Following (Chen et al., 2020b), we use BERT2F ×F F where Ws ∈ R , bs ∈ R are trainable base (Devlin et al., 2019) as the backbone to build parameters. Then, we remove the nodes with the our model. The maximum sequence length is 512, 179 Model BERT classifier w/o Table Table-BERT-Horizontal-F+T-Concatenate Table-BERT-Vertical-F+T-Template Table-BERT-Vertical-T+F-Template Table-BERT-Horizontal-F+T-Template Table-BERT-Horitonzal-T+F-Template LPA-Voting w/o Discriminator LPA-Weighted-Voting LPA-Ranking w/ Transformer LogicalFactChecker HeterTFV SAT ProgVGAT LERGV Human Performance Val 50.9 50.7 56.7 56.7 66.0 66.1 57.7 62.5 65.2 71.8 72.5 73.3 74.9 75.6 - Test 50.5 50.4 56.2 57.0 65.1 65.1 58.2 63.1 65.0 71."
2021.emnlp-main.16,2021.eacl-main.201,0,0.0178879,"erification stage and graph-based reasoning approaches (Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b) are popular in this stage. Studies on fact verification over semi-structured evidence achieve much attention recently due to the proposal of the TABFACT dataset (Chen et al., 2020b). Two official baselines are provided along with this dataset named Table-BERT and LPA, which treat the task in a soft linguistic reasoning manner and hard symbolic reasoning manner respectively. Some approaches on this dataset focus on the representation of the semi-structured data (Zhang et al., 2020; Dong and Smith, 2021). And some approaches focus on the combination of linguistic reasoning and symbolic reasoning (Zhong et al., 2020a; Shi et al., 2020; Yang et al., 2020) mainly by building a semantic parser to select programs to serve the verification process. Different from their work, we propose an evidence retrieval module with a rule-based approach to obtain logiclevel evidence as supplementary information for the original table to benefit the verification model. 5.2 Reasoning over Semi-Structured Data Understanding semi-structured data is supposed to understand the structure and the content in every cell"
2021.emnlp-main.16,2020.findings-emnlp.27,0,0.0572842,"Missing"
2021.emnlp-main.16,2020.acl-main.210,0,0.0222489,"select programs to serve the verification process. Different from their work, we propose an evidence retrieval module with a rule-based approach to obtain logiclevel evidence as supplementary information for the original table to benefit the verification model. 5.2 Reasoning over Semi-Structured Data Understanding semi-structured data is supposed to understand the structure and the content in every cell simultaneously. There are a lot of approaches in this direction spread over different tasks, such as question answering (Pasupat and Liang, 2015; Nan et al., 2021), natural language inference (Gupta et al., 2020; Neeraja et al., 2021), fact verification (Chen et al., 2020b), etc. And some methods 5 Related Work put attention on the pre-training strategies on the 5.1 Fact Verification semi-structured data along with the textual input Fact verification aims to verify the correctness of (Herzig et al., 2020; Yin et al., 2020; Eisenschlos the input claim by the given evidence. Most of the et al., 2020; Yu et al., 2020). Besides, a range existing methods on fact verification focus on deal- of approaches reason on mixed evidence sources ing with the unstructured text as evidence. FEVER incorporating semi-s"
2021.emnlp-main.16,2020.acl-main.398,0,0.0157815,"Data Understanding semi-structured data is supposed to understand the structure and the content in every cell simultaneously. There are a lot of approaches in this direction spread over different tasks, such as question answering (Pasupat and Liang, 2015; Nan et al., 2021), natural language inference (Gupta et al., 2020; Neeraja et al., 2021), fact verification (Chen et al., 2020b), etc. And some methods 5 Related Work put attention on the pre-training strategies on the 5.1 Fact Verification semi-structured data along with the textual input Fact verification aims to verify the correctness of (Herzig et al., 2020; Yin et al., 2020; Eisenschlos the input claim by the given evidence. Most of the et al., 2020; Yu et al., 2020). Besides, a range existing methods on fact verification focus on deal- of approaches reason on mixed evidence sources ing with the unstructured text as evidence. FEVER incorporating semi-structured data, such as rea(Thorne et al., 2018) is one of the most popular soning over table and text together (Chen et al., datasets in this direction, which develops automatic 2020c,a) and multi-modal evidence including table, fact verification systems to check the veracity of text and image (T"
2021.emnlp-main.16,2020.findings-emnlp.309,0,0.099654,"Missing"
2021.emnlp-main.16,2020.acl-main.655,0,0.0300384,"error examples are caused by this category. claims by extracting evidence from Wikipedia. After that, FEVER 2.0 share task (Thorne et al., 2019) is built, which is more challenging by the addition of an adversarial attack task. Recently, HOVER (Jiang et al., 2020) is proposed to focus on the many-hop evidence extraction and fact verification task. Previous work mainly follows the pipeline composed of document retrieval, evidence sentence selection, and claim verification. Most of the proposed models focus on the claim verification stage and graph-based reasoning approaches (Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b) are popular in this stage. Studies on fact verification over semi-structured evidence achieve much attention recently due to the proposal of the TABFACT dataset (Chen et al., 2020b). Two official baselines are provided along with this dataset named Table-BERT and LPA, which treat the task in a soft linguistic reasoning manner and hard symbolic reasoning manner respectively. Some approaches on this dataset focus on the representation of the semi-structured data (Zhang et al., 2020; Dong and Smith, 2021). And some approaches focus on the combination of linguistic reasoning"
2021.emnlp-main.16,2021.naacl-main.224,0,0.026877,"erve the verification process. Different from their work, we propose an evidence retrieval module with a rule-based approach to obtain logiclevel evidence as supplementary information for the original table to benefit the verification model. 5.2 Reasoning over Semi-Structured Data Understanding semi-structured data is supposed to understand the structure and the content in every cell simultaneously. There are a lot of approaches in this direction spread over different tasks, such as question answering (Pasupat and Liang, 2015; Nan et al., 2021), natural language inference (Gupta et al., 2020; Neeraja et al., 2021), fact verification (Chen et al., 2020b), etc. And some methods 5 Related Work put attention on the pre-training strategies on the 5.1 Fact Verification semi-structured data along with the textual input Fact verification aims to verify the correctness of (Herzig et al., 2020; Yin et al., 2020; Eisenschlos the input claim by the given evidence. Most of the et al., 2020; Yu et al., 2020). Besides, a range existing methods on fact verification focus on deal- of approaches reason on mixed evidence sources ing with the unstructured text as evidence. FEVER incorporating semi-structured data, such as"
2021.emnlp-main.16,P15-1142,0,0.017922,"t al., 2020; Yang et al., 2020) mainly by building a semantic parser to select programs to serve the verification process. Different from their work, we propose an evidence retrieval module with a rule-based approach to obtain logiclevel evidence as supplementary information for the original table to benefit the verification model. 5.2 Reasoning over Semi-Structured Data Understanding semi-structured data is supposed to understand the structure and the content in every cell simultaneously. There are a lot of approaches in this direction spread over different tasks, such as question answering (Pasupat and Liang, 2015; Nan et al., 2021), natural language inference (Gupta et al., 2020; Neeraja et al., 2021), fact verification (Chen et al., 2020b), etc. And some methods 5 Related Work put attention on the pre-training strategies on the 5.1 Fact Verification semi-structured data along with the textual input Fact verification aims to verify the correctness of (Herzig et al., 2020; Yin et al., 2020; Eisenschlos the input claim by the given evidence. Most of the et al., 2020; Yu et al., 2020). Besides, a range existing methods on fact verification focus on deal- of approaches reason on mixed evidence sources ing"
2021.emnlp-main.16,2020.coling-main.466,1,0.903542,"modules. Figure 2 shows the overview of our proposed approach. Given the table and the statement, the program synthesis module first synthesizes all possible programs with valid combinations. Then the evidence retrieval module selects, decomposes, and filters over the programs to obtain valuable logic-level evidence as supplementary information to the original table. Finally, a fact verification model takes the input of the table, statement, and obtained evidence to predict whether the statement is supported by the given table. Following previous work (Zhong et al., 2020a; Yang et al., 2020; Shi et al., 2020), we perform program synthesis with the latent program search algorithm (LPA) (Chen et al., 2020b), followed by an evidence retrieval module and a fact verification module. We will present above modules in § 3. 3 Methodology We propose a Logic-level Evidence Retrieval and Graph-based Verification network (LERGV) for the table-based fact verification task. Given a table and a statement, LERGV works as follows. First, we start with the latent program algorithm (Chen et al., 2020b) to synthesize programs and then use a rule-based retrieval approach to select, decompose, and filter among all synth"
2021.emnlp-main.16,N18-1074,0,0.0652234,"Missing"
2021.emnlp-main.16,2020.emnlp-main.628,0,0.385595,"linguistic reasoning and symbolic reasoning (Chen et al., 2020b). Symbolic reasoning with logical operations like &quot;count&quot; and &quot;argmax&quot; plays an important role in the table-based fact verification task. Figure 1 shows an example. Ideally, to verify the correctness of such statements, logical operations provide strong hints to classify the entailment relation. Therefore, how to utilize such logical operations is crucial in this task. Program is a kind of logic form derived from tables, which contains rich logical operations. Following (Chen et al., 2020b), existing methods (Zhong et al., 2020a; Yang et al., 2020; Shi et al., ∗ Corresponding author. 1 2020) mostly use programs to perform symbolic Our code is available at: https://github.com/ qshi95/LERGV reasoning. Specifically, they derive one or sev175 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 175–184 c November 7–11, 2021. 2021 Association for Computational Linguistics eral programs with a semantic parser based on the given table and statement, and then leverage the obtained programs to enhance the verification process. However, these models may select spurious programs (i.e. wrong programs with c"
2021.emnlp-main.16,2020.acl-main.745,0,0.0123595,"mi-structured data is supposed to understand the structure and the content in every cell simultaneously. There are a lot of approaches in this direction spread over different tasks, such as question answering (Pasupat and Liang, 2015; Nan et al., 2021), natural language inference (Gupta et al., 2020; Neeraja et al., 2021), fact verification (Chen et al., 2020b), etc. And some methods 5 Related Work put attention on the pre-training strategies on the 5.1 Fact Verification semi-structured data along with the textual input Fact verification aims to verify the correctness of (Herzig et al., 2020; Yin et al., 2020; Eisenschlos the input claim by the given evidence. Most of the et al., 2020; Yu et al., 2020). Besides, a range existing methods on fact verification focus on deal- of approaches reason on mixed evidence sources ing with the unstructured text as evidence. FEVER incorporating semi-structured data, such as rea(Thorne et al., 2018) is one of the most popular soning over table and text together (Chen et al., datasets in this direction, which develops automatic 2020c,a) and multi-modal evidence including table, fact verification systems to check the veracity of text and image (Talmor et al., 2021"
2021.emnlp-main.16,2020.emnlp-main.126,0,0.0610416,"Missing"
2021.emnlp-main.16,2020.acl-main.539,0,0.360222,"which requires both linguistic reasoning and symbolic reasoning (Chen et al., 2020b). Symbolic reasoning with logical operations like &quot;count&quot; and &quot;argmax&quot; plays an important role in the table-based fact verification task. Figure 1 shows an example. Ideally, to verify the correctness of such statements, logical operations provide strong hints to classify the entailment relation. Therefore, how to utilize such logical operations is crucial in this task. Program is a kind of logic form derived from tables, which contains rich logical operations. Following (Chen et al., 2020b), existing methods (Zhong et al., 2020a; Yang et al., 2020; Shi et al., ∗ Corresponding author. 1 2020) mostly use programs to perform symbolic Our code is available at: https://github.com/ qshi95/LERGV reasoning. Specifically, they derive one or sev175 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 175–184 c November 7–11, 2021. 2021 Association for Computational Linguistics eral programs with a semantic parser based on the given table and statement, and then leverage the obtained programs to enhance the verification process. However, these models may select spurious programs (i.e. w"
2021.emnlp-main.16,2020.acl-main.549,0,0.494638,"which requires both linguistic reasoning and symbolic reasoning (Chen et al., 2020b). Symbolic reasoning with logical operations like &quot;count&quot; and &quot;argmax&quot; plays an important role in the table-based fact verification task. Figure 1 shows an example. Ideally, to verify the correctness of such statements, logical operations provide strong hints to classify the entailment relation. Therefore, how to utilize such logical operations is crucial in this task. Program is a kind of logic form derived from tables, which contains rich logical operations. Following (Chen et al., 2020b), existing methods (Zhong et al., 2020a; Yang et al., 2020; Shi et al., ∗ Corresponding author. 1 2020) mostly use programs to perform symbolic Our code is available at: https://github.com/ qshi95/LERGV reasoning. Specifically, they derive one or sev175 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 175–184 c November 7–11, 2021. 2021 Association for Computational Linguistics eral programs with a semantic parser based on the given table and statement, and then leverage the obtained programs to enhance the verification process. However, these models may select spurious programs (i.e. w"
2021.emnlp-main.16,P19-1085,0,0.017951,"e same meanings. 5 error examples are caused by this category. claims by extracting evidence from Wikipedia. After that, FEVER 2.0 share task (Thorne et al., 2019) is built, which is more challenging by the addition of an adversarial attack task. Recently, HOVER (Jiang et al., 2020) is proposed to focus on the many-hop evidence extraction and fact verification task. Previous work mainly follows the pipeline composed of document retrieval, evidence sentence selection, and claim verification. Most of the proposed models focus on the claim verification stage and graph-based reasoning approaches (Zhou et al., 2019; Liu et al., 2020; Zhong et al., 2020b) are popular in this stage. Studies on fact verification over semi-structured evidence achieve much attention recently due to the proposal of the TABFACT dataset (Chen et al., 2020b). Two official baselines are provided along with this dataset named Table-BERT and LPA, which treat the task in a soft linguistic reasoning manner and hard symbolic reasoning manner respectively. Some approaches on this dataset focus on the representation of the semi-structured data (Zhang et al., 2020; Dong and Smith, 2021). And some approaches focus on the combination of li"
2021.emnlp-main.16,2021.semeval-1.39,0,0.0169022,"er 3230 is 1 Label ENTAILED Program-like Evidence l eq { 1 ; min { all_rows ; number in fleet } } = True l eq { 1 ; hop { filter_eq { all_rows ; fleet numbers ; 3230 } ; number in fleet } } = True l eq { scania ; hop { filter_eq { all_rows ; number in fleet ; 1 } ; chassis manufacturer } } = True l … Figure 1: Example of TABFACT dataset. Given a table and a statement, the goal is to verify the correctness of the statement by the table. Program-like evidence contains logical operations and can be used as supplementary information to a table. fake news detection, scientific paper understanding (Wang et al., 2021), etc. This task aims to verify the correctness of the given statement by the given table, which requires both linguistic reasoning and symbolic reasoning (Chen et al., 2020b). Symbolic reasoning with logical operations like &quot;count&quot; and &quot;argmax&quot; plays an important role in the table-based fact verification task. Figure 1 shows an example. Ideally, to verify the correctness of such statements, logical operations provide strong hints to classify the entailment relation. Therefore, how to utilize such logical operations is crucial in this task. Program is a kind of logic form derived from tables,"
2021.emnlp-main.257,2020.emnlp-main.367,0,0.0571655,"Missing"
2021.emnlp-main.257,2020.tacl-1.30,0,0.0249007,"es on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacity each Implementation Details We adapt the Trans- language requires. former architecture from the base model setting Since increasing vocabulary size brings the isin Conneau et al. (2020), i.e., 12 layers and 768 sues of model size and pre-training speed, we study hidd"
2021.emnlp-main.257,2020.acl-main.747,0,0.24302,". In order to address the issues, we propose k-NN-based target sampling to accelerate the expensive softmax. Our experiments show that the multilingual vocabulary learned with VO C AP benefits cross-lingual language model pre-training. Moreover, k-NN-based target sampling mitigates the side-effects of increasing the vocabulary size while achieving comparable performance and faster pre-training speed. The code and the pretrained multilingual vocabularies are available at https://github. com/bozheng-hit/VoCapXLM. 1 Introduction Pretrained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2021b; Xue et al., 2020) have recently shown great success in improving cross-lingual transferability. These models encode texts from different languages into universal representations with a shared multilingual vocabulary and a shared Transformer encoder (Vaswani et al., 2017). By pretraining cross-lingual language models on the largescale multilingual corpus, the models achieve stateof-the-art performance on various downstream tasks, e.g., cross-lingual question answering and cross-lingual sentence classification. Although the Transformer architecture used in most pretrained mo"
2021.emnlp-main.257,P18-1007,0,0.374283,"020). Meanwhile, state-of-the-art pretrained cross-lingual language models use the shared multilingual vocabulary of 250K subword units to represent more than 100 languages (Conneau et al., 2020; Chi et al., 2021b; Xue et al., 2020). Although some subword units are shared across languages, no more than 2.5K language-specific subword units on average are allocated for each language, which is still relatively small. Besides, the multilingual vocabulary is trained on the combined multilingual corpus with subword segmentation algorithms like BPE (Sennrich et al., 2015) and unigram language model (Kudo, 2018). During vocabulary construction, these algorithms tend to select more subword units shared across languages with common scripts like Latin and Cyrillic (Chung et al., 2020b), but have a lower chance to select language-specific subword units. It is hard to determine how much vocabulary capacity a particular language requires and whether the shared multilingual vocabulary has allocated enough vocabulary capacity to represent the language. In this paper, we propose VO C AP, an algorithm to allocate large vocabulary for cross-lingual language model by separately evaluating the required vocabulary"
2021.emnlp-main.257,2020.acl-main.653,0,0.0420862,"uestion answering ness of our methods, we conduct experiments on datasets but degrades on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacity each Implementation Details We adapt the Trans- language requires. former architecture from the base model setting Since increasing vocabulary size brings the isin Conneau et al. (202"
2021.emnlp-main.257,2021.ccl-1.108,0,0.0879979,"Missing"
2021.emnlp-main.257,P17-1178,0,0.0880255,"us. 94 92 −280 −160 −280 −260 −240 −220 ALP −200 −180 −160 −150 Low res. Mid res. High res. Figure 3: F1 score on NER task with different vocabularies versus their ALP on the monolingual corpus. Figure 4: Comparison of vocabulary capacity of different-resourced languages. Shorter bars indicate larger vocabulary capacity. vocabularies for each language on the corresponding monolingual corpus, with vocabulary size ranging from 1K to 30K. Then we pretrain monolingual language models with the corresponding monolingual vocabularies. We evaluate these pretrained models on two downstream tasks: NER (Pan et al., 2017) and POS (Zeman et al., 2019) from the XTREME benchmark since there is annotated task data for a large number of languages. The vocabularies are learned on the reconstructed CommonCrawl corpus (Chi et al., 2021b; Conneau et al., 2020) using SentencePiece (Kudo and Richardson, 2018) with the unigram language model (Kudo, 2018). The unigram distributions are also counted on the CommonCrawl corpus. The Wikipedia corpus is used for all pre-training experiments in this paper since it is easier to run experiments due to its smaller size. More details about the pre-training data can be found in the a"
2021.emnlp-main.257,D19-1382,0,0.0210914,"ulary directly learned on multilingual corpus with 4.1 Setup SentencePiece, i.e., XLM-R250K and J OINT250K , Fine-Tuning Datasets To validate the effectiveour VO C AP250K improves on question answering ness of our methods, we conduct experiments on datasets but degrades on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacit"
2021.emnlp-main.298,D17-2011,0,0.0269186,"liu, qinb}@ir.hit.edu.cn Abstract Neural networks have recently become the mainstream models for QA (Lukovnikov et al., 2017; Jia Many open-domain question answering probet al., 2018; Yang et al., 2019). Most of the models, lems can be cast as a textual entailment task, however, are unable to give explainable inference where a question and candidate answers are results. Developing effective and yet explainable concatenated to form hypotheses. A QA system then determines if the supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natura"
2021.emnlp-main.298,D14-1059,0,0.405853,"A, aiming to keep the backbone of inferproof paths. Entailment scores between the ence based on the natural logic formalism, while acquired intermediate hypotheses and candiintegrating neural networks to make the systems date premises are measured to determine if a powerful and robust. Conventional natural logic premise entails the hypothesis. As the natural has been designed for natural language inference logic reasoning process forms a tree-like, hierarchical structure, we embed hypotheses and and question answering (MacCartney and Manning, premises in a Hyperbolic space rather than Eu2009; Angeli and Manning, 2014). As opposed to clidean space to acquire more precise represenperforming deduction on an abstract logical form, tations. Empirically, our method outperforms e.g., first-order logic (FOL) or its fragments, in prior work on answering multiple-choice sciwhich obtaining representation for abstract logic ence questions, achieving the best results on forms is known to face many thorny challenges, two publicly available datasets. The natural natural logic provides a formal proof framework logic inference process inherently provides evbased on the monotonicity calculus or projectivity. idence to help"
2021.emnlp-main.298,P16-1042,0,0.289245,"plication but also a challenging task for as- candidate premises by following natural logic insessing how well AI systems understand human ference steps and incorporating neural models to language and perform reasoning to answer ques- help build the proof paths. NeuNLI first converts tions. A main challenge of QA is that the answers a question and candidate answers to form declaraoften do not explicitly exist in a supporting knowl- tive sentences, namely hypotheses. It then rewrites edge base but instead need to be inferred from it. these original hypotheses to obtain intermediate Prior work (Angeli et al., 2016) has viewed QA hypotheses and repeats this process to construct a as a textual entailment problem performed on a proof tree for each question-answer pair. large premise set, where a question and candidate Since the reasoning process forms a tree-like, answers are formulated as hypotheses that need to hierarchical structure (Angeli and Manning, 2014), be proved. it can lead to structural distortion when learning * Corresponding author. embeddings for hypotheses and premises in the 3673 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3673–3684 c Nove"
2021.emnlp-main.298,W18-5308,0,0.023179,"del NeuNLI performs the state-of-the-art. For example, the hypothesis is “in order to survive, all animals need food, water and air”. By lexical mutation in NeuNLI, we get the sentence “in order to live, all animals need food, water and air”, which is closer to the premise “animals need air, water, and food in order to live and thrive”. E and NeuNLI-E w/o reasoning. It indicates that exploiting natural logic-based reasoning is very effective for QA. 6 Related Work Question answering systems that integrate deep learning methods have made great progress in recent years (Lukovnikov et al., 2017; Bhandwaldar and Zadrozny, 2018; Jia et al., 2018; Yang et al., 2019). Many works first adopt learnable encoders for sentence representation like convolutional encoders (Zhang et al., 2017), recurrent encoders (Tay et al., 2017) and transformers (Yang et al., 2019). Then an interaction layer is devised to calculate the semantic similarity, which is the main difference in many models. Severyn and Moschitti (2015) utilize a multi-layered perceptron to combine the CNN encoded representations. Yang et al. (2016) perform a soft-attention alignment to measure word similarity between the question and the answer. Though neural netw"
2021.emnlp-main.298,N19-1423,0,0.067616,"Missing"
2021.emnlp-main.298,W18-1708,0,0.071441,"e the reasoning process forms a tree-like, answers are formulated as hypotheses that need to hierarchical structure (Angeli and Manning, 2014), be proved. it can lead to structural distortion when learning * Corresponding author. embeddings for hypotheses and premises in the 3673 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3673–3684 c November 7–11, 2021. 2021 Association for Computational Linguistics Euclidean space (Sarkar, 2011; Sala et al., 2018). Additionally, natural language text exhibits hierarchical structure in a variety of respects (Dhingra et al., 2018). NeuNLI projects the question and answer embeddings to the Hyperbolic space. For a proof tree, NeuNLI computes an entailment score between tree nodes and candidate premises in a Hyperbolic space and use that to help select the answer. We demonstrate modelling entailment score in the Hyperbolic space improves the performance. To train the above process in an end-to-end differentiable manner, we utilize the Gumbel-Softmax technique (Jang et al., 2017), which can effectively approximate the discrete variable, as an approximation of the non-differentiable selecting process of candidate mutations."
2021.emnlp-main.298,N19-1357,0,0.0209611,"he main difference in many models. Severyn and Moschitti (2015) utilize a multi-layered perceptron to combine the CNN encoded representations. Yang et al. (2016) perform a soft-attention alignment to measure word similarity between the question and the answer. Though neural networks-based models make great advances in QA, they are short of illustrating the step-by-step prediction derivation process, where the logic-based method is adept (Rocktäschel and Riedel, 2017; Weber et al., 2019; Minervini et al., 2020), which differs from the widely used attention mechanism (Doshi-Velez and Kim, 2017; Jain and Wallace, 2019). Angeli et al. (2016) proposed a Natural Logic Inference framework to utilize natural logic to conduct interpretable question answering and viewed the open-domain question answering as a textual entailment problem. Our NeuNLI is inspired by natural logic inference but can achieve better performance by modeling the contextual information during natural logic proving using two pre-trained language models and training the whole process in an end-to-end fashion. Ablation Study. We conduct the ablation study on the QA-S test set with the Barron’s knowledge base. The experimental results are shown"
2021.emnlp-main.298,2020.acl-main.282,0,0.0170551,"idean space grows polynomially, which would lead to structural distortion in the Euclidean space (Sarkar, 2011; Sala et al., 2018). Additionally, natural language text itself exhibits hierarchical structure. Thus, we calculate the entailment scores between them in Hyperbolic space as shown in the right part of Figure 2. Here, we choose the Poincaré ball model (Cannon et al., 1997) to project the candidate premise and intermediate hypothesis into the Hyperbolic space to acquire more precise representations. We exploit the re-parameterization technique (Dhingra et al., 2018; López et al., 2019; Cao et al., 2020) to implement it, which involves calculating a direction vector m and a norm magnitude µ. Take vpEj as an example to illustrate the procedure: ´ ¯ mp mpj “ ψdir v E mpj “ m j pj , } pj } ´ ¯ ` ˘ E µ pj “ σ µ ¯ pj µ ¯pj “ ψnorm v pj , (2) where ψdir : Rd Ñ RdH is a multi-layer perceptron. ψnorm : Rd Ñ R is a linear function. σ is the sigmoid function to ensure the resulting norm µpj P p0, 1q. The re-parameterized premise representation is defined as v H pj “ µpj mpj , which lies in Hyperbolic space B dH . The re-parameterization technique has the ability to avoid the need to adopt the stochasti"
2021.emnlp-main.298,2021.ccl-1.108,0,0.037808,"Missing"
2021.emnlp-main.298,W19-4319,0,0.0124268,"y. However, the Euclidean space grows polynomially, which would lead to structural distortion in the Euclidean space (Sarkar, 2011; Sala et al., 2018). Additionally, natural language text itself exhibits hierarchical structure. Thus, we calculate the entailment scores between them in Hyperbolic space as shown in the right part of Figure 2. Here, we choose the Poincaré ball model (Cannon et al., 1997) to project the candidate premise and intermediate hypothesis into the Hyperbolic space to acquire more precise representations. We exploit the re-parameterization technique (Dhingra et al., 2018; López et al., 2019; Cao et al., 2020) to implement it, which involves calculating a direction vector m and a norm magnitude µ. Take vpEj as an example to illustrate the procedure: ´ ¯ mp mpj “ ψdir v E mpj “ m j pj , } pj } ´ ¯ ` ˘ E µ pj “ σ µ ¯ pj µ ¯pj “ ψnorm v pj , (2) where ψdir : Rd Ñ RdH is a multi-layer perceptron. ψnorm : Rd Ñ R is a linear function. σ is the sigmoid function to ensure the resulting norm µpj P p0, 1q. The re-parameterized premise representation is defined as v H pj “ µpj mpj , which lies in Hyperbolic space B dH . The re-parameterization technique has the ability to avoid the need to"
2021.emnlp-main.298,W09-3714,0,0.0425453,"llenges. In this Example–1: research, we investigate developing neural natural Question: The main function of a fish’s fins is to logic models for QA, which provide insight into the help the fish _____. derivation process but also sidestep the difficulties (A) reproduce (B) see (C) breathe (D) move of translating sentences into FOL. Knowledge Base: . . . A fish has a flipper or fin that Natural logic proving is operated by inserting, helps them swim. The dorsal fin can help to keep deleting, or mutating words following monotonicthe fish stable in the water. . . . ity calculus or projectivity (MacCartney and Manning, 2009; Valencia, 1991). In their recent work Given a science question, four candidate an- MacCartney and Manning (2009) utilize seven logswers, and relevant knowledge, a model needs ical relations as shown in Table 1. For example, to choose the correct answer supported by the mutating animals to dogs corresponds to a reverse knowledge base. Following Clark et al. (2018), entailment relation, i.e., animals Ě dogs. Natural we explore to solve the multiple-choice question logic then projects the lexical relation based on answering as a textual entailment problem. Specif- the monotonicity or projectivi"
2021.emnlp-main.298,P14-5010,0,0.00253515,"where wl is assigned to segment 0 and wl1 is assigned to segment 1. The predicted result is negation relation (N), calculated by the representation of the [CLS] token. Then, we use the projection function φ to obtain the sentence-level semantic relation according to the predicted lexical relation and the lexical polarity of the word wl . If the lexical polarity is upward, the sentence-level relation will be identical to the lexical relation. Otherwise, the projection from the word-level relation to the sentence-level relation is performed as shown in Table 2. We employ Stanford natlog parser (Manning et al., 2014) to acquire the lexical polarity of words. For example, as the polarity of the mutated word “longest” is upward, and the logical relation between “longest” and “shortest” is N, the semantic relation of the hypothesis hi and the intermediate hypothesis h1i still maintains N. If the predicted polarity of “longest” is downward, the sentence-level relation will be ë. As we only conduct inference on the sentence-level relation of ” or Ě, this mutation would be filtered out. of intermediate hypotheses grows exponentially. However, the Euclidean space grows polynomially, which would lead to structura"
2021.emnlp-main.298,W06-3907,0,0.045731,"supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natural logic inference steps to build tures for QA, aiming to keep the backbone of inferproof paths. Entailment scores between the ence based on the natural logic formalism, while acquired intermediate hypotheses and candiintegrating neural networks to make the systems date premises are measured to determine if a powerful and robust. Conventional natural logic premise entails the hypothesis. As the natural has been designed for natural language inference logic reasoning process f"
2021.emnlp-main.298,D14-1162,0,0.085042,"sentence hi , i.e., “In New York state, the longest period of daylight occurs during June”. 3.1 Candidate Premises Retrieval The knowledge base K consists of unstructured text. This makes available the great amount of text as knowledge source to help perform question answering. Given a hypothesis, as shown in the right part of Figure 2, NeuNLI first retrieves candidate premises. Specifically, a premise is one of the sentences in the knowledge base K “ tp1 , . . . , pn u. Given a hypothesis hi , we obtain the representation of hi and each pj in K by computing the average Glove word embeddings (Pennington et al., 2014) of it, respectively. Then we calculate the cosine similarity between hi and each pj in K, respectively, to find the top k relevant candidate premises (k is tuned on the development set). In this paper we propose the Neural Natural Logic Inference (NeuNLI) framework, aiming to combine the advantages of natural logic and deep neural networks for question answering, which builds explainability in the model and leverages the powerful 3.2 Contextualized Neural Natural Logic capacity and robustness of neural models. Figure Prover 2 depicts the overall architecture of NeuNLI; the pseudocode of NeuNL"
2021.emnlp-main.298,P19-1488,0,0.0174838,"ikov et al., 2017; Jia Many open-domain question answering probet al., 2018; Yang et al., 2019). Most of the models, lems can be cast as a textual entailment task, however, are unable to give explainable inference where a question and candidate answers are results. Developing effective and yet explainable concatenated to form hypotheses. A QA system then determines if the supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natural logic inference steps to build tures for QA, aiming to keep the backbone of inferproof paths. Entailment"
2021.emnlp-main.298,P19-1618,0,0.026418,"017) and transformers (Yang et al., 2019). Then an interaction layer is devised to calculate the semantic similarity, which is the main difference in many models. Severyn and Moschitti (2015) utilize a multi-layered perceptron to combine the CNN encoded representations. Yang et al. (2016) perform a soft-attention alignment to measure word similarity between the question and the answer. Though neural networks-based models make great advances in QA, they are short of illustrating the step-by-step prediction derivation process, where the logic-based method is adept (Rocktäschel and Riedel, 2017; Weber et al., 2019; Minervini et al., 2020), which differs from the widely used attention mechanism (Doshi-Velez and Kim, 2017; Jain and Wallace, 2019). Angeli et al. (2016) proposed a Natural Logic Inference framework to utilize natural logic to conduct interpretable question answering and viewed the open-domain question answering as a textual entailment problem. Our NeuNLI is inspired by natural logic inference but can achieve better performance by modeling the contextual information during natural logic proving using two pre-trained language models and training the whole process in an end-to-end fashion. Abl"
2021.emnlp-main.298,N19-4013,0,0.0664828,"Missing"
2021.emnlp-main.298,D18-1259,0,0.0248612,"n Abstract Neural networks have recently become the mainstream models for QA (Lukovnikov et al., 2017; Jia Many open-domain question answering probet al., 2018; Yang et al., 2019). Most of the models, lems can be cast as a textual entailment task, however, are unable to give explainable inference where a question and candidate answers are results. Developing effective and yet explainable concatenated to form hypotheses. A QA system then determines if the supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natural logic inference s"
2021.emnlp-main.298,C18-1171,0,0.0236837,"dels for QA (Lukovnikov et al., 2017; Jia Many open-domain question answering probet al., 2018; Yang et al., 2019). Most of the models, lems can be cast as a textual entailment task, however, are unable to give explainable inference where a question and candidate answers are results. Developing effective and yet explainable concatenated to form hypotheses. A QA system then determines if the supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natural logic inference steps to build tures for QA, aiming to keep the backbone of inferp"
2021.findings-acl.207,D18-1316,0,0.0174526,"n return x(t) else return None end if This helps to determine the word substituting order in the proposed method. In this work, we use a combination of the changes found in the unlabelled attachment score (UAS) and in the labelled attachment score (LAS) to measure word importance. Specifically, the importance of a word xi in sentence x is computed as 2.3.1 Generating Process We collect substitutes from the following methods: BERT-Based Method: We use BERT to generate candidates for each target word from its context. This method generates only single subwords. Embedding-Based Method: Following Alzantot et al. (2018), we use word embeddings of Mrkˇsi´c et al. (2016)2 to compute the N nearest neighbours of each target word according to their cosine similarity and use them as candidates. Sememe-Based Method: The sememes of a word represent its core meaning (Dong and Dong, 2006). Following Zang et al. (2020), we collect the substitutes of the target word x based on the rule that one of the substitutes the senses of x∗ must have the same sememe annotations as one of senses of x. Synonym-Based Method: We use WordNet3 to extract synonyms of each target word as candidates. 2.3.2 Filtering Process Generation of S"
2021.findings-acl.207,N19-1423,0,0.0124486,"Preserving the syntactic structure enables us to use the gold syntactic structure of the original sentence in the evaluation process. While preserving the fluency ensures that ungrammatical adversarial examples, which not only fool the target model but also confuse humans, will not be considered valid. Therefore in this paper, we evaluate the quality of an adversarial example in two aspects, namely the fluency and syntactic structure preservation. Recently, Zheng et al. (2020) proposed the first dependency parser attacking algorithm based on word-substitution which depended entirely on BERT (Devlin et al., 2019) to generate candidate substitutes. The rational was that the use of the pre-trained language model will ensure fluency of the adversarial examples. However, we find that using BERT alone is far from enough to preserve fluency. Therefore, in this paper, we propose a method to generate better adversarial examples for dependency parsing with four types of candidate generators and filters. Specifically, our method consists of three steps: (i) determining the substitution order, (ii) generating and filtering candidate substitutes for each word, (iii) searching for the best possible combination of"
2021.findings-acl.207,2020.emnlp-main.182,0,0.356771,"operties of adversarial attacks. We find that (i) the introduction of out-of-vocabulary (OOV, words not in the embedding’s vocabulary) and out-of-training (OOT, words not in the training set of the parser) words in adversarial examples are two main factors that harm models’ performance; (ii) adversarial examples generated against a parser strongly depend on the type of the parser, the token embeddings and even the random seed. Adversarial training (Goodfellow et al., 2015), where adversarial examples are added in the training stage, has been commonly used in previous work (Zheng et al., 2020; Han et al., 2020) to improve a parser’s robustness. Only a limited number of adversarial examples have been used in such cases, and Zheng et al. (2020) argued that overuse of them may lead to a performance drop on the clean data. However, we show that with improvement in the quality of adversarial examples produced in our method, more adversarial examples can be used in the training stage to further improve the parsing models’ robustness without producing any apparent harm in their performance on the clean data. Inspired by our second finding, we propose to improve the parsers’ robustness by combining models t"
2021.findings-acl.207,2020.iwpt-1.7,1,0.754643,"or, and we propose to generate better examples by using more generators and stricter filters. Han et al. (2020) proposed an approach to attack structured prediction models with a seq2seq model (Wang et al., 2016) and evaluated this model on dependency parsing. They used two reference parsers in addition to the victim parser to supervise the training of the adversarial example generator, and found that the three parsers produce better results when they have different inductive biases embedded to make the attack successful. This finding is quite close in spirit to our conclusion in Section 4.5. Hu et al. (2020) also put forth efforts to modify the text in syntactic tasks while preserving the original syntactic structure. However, their goal is to preserve privacy via the modification of words that could disclose sensitive information. 6 Conclusion In this paper, we propose a method for generating high-quality adversarial examples for dependency parsing and show its effectiveness based on automatic and human evaluation. We investigate This work was supported by the National Key R&D Program of China via grant 2020AAA0106501 and the National Natural Science Foundation of China (NSFC) via grant 61976072"
2021.findings-acl.207,N16-1082,0,0.0336606,"ii) the true dependency tree of x∗ should be the same as that of x. In this paper, these two constraints are ensured through the use of various filters (see Section 2.3) and are used to evaluate the quality of adversarial examples (see details on fluency and syntactic structure preservation in Section 3.3). 2.2 Word Importance Ranking Word importance ranking in our model is based on the observation that some words have a stronger influence on model prediction than others. Such word importance is typically computed by setting each word to unknown and examining the changes in their predictions (Li et al., 2016; Ren et al., 2019). 2345 Algorithm 1 Dependency Parsing Attack generation methods, then apply filters to discard inappropriate substitutes, ensuring both diversity and quality of the generated candidates. Input: Sentence example x(0) = {x1 , x2 , . . . , xN }, maximum percentage of words allowed to be modified γ Output: Adversarial example x(i) 1: for i = 1 to N do 2: Compute word importance I(x(0) , xi ) via Eq. 1 3: end for 4: Create a set W of all words xi ∈ x(0) sorted by the descending order of their importance I(x(0) , xi ). 5: t = 0 6: for each word xj in W do 7: Build candidate set Cj"
2021.findings-acl.207,2021.ccl-1.108,0,0.0377807,"Missing"
2021.findings-acl.207,P18-1130,0,0.0126623,"rds in the sentence exceeds a threshold γ, we stop the process. Otherwise, we search for a substitute for the next target word. 3 3.1 Experimental Setup Target Parsers and Token Embeddings We choose the following two strong and commonly used English parsers, one graph-based, the other transition-based, as target models, both of which achieve performance close to the state-of-the-art. Deep Biaffine Parser (Dozat and Manning, 2017) is a graph-based parser that scores each candidate arc independently and relies on a decoding algorithm to search for the highest-scoring tree. Stack-Pointer Parser (Ma et al., 2018) is a transition-based parser that incrementally builds the dependency tree with pre-defined operations. We used the following four types of token embeddings to study their influence on each parsers’ robustness. To focus on the influence of the embeddings, we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece emb"
2021.findings-acl.207,de-marneffe-etal-2006-generating,0,0.0517672,"Missing"
2021.findings-acl.207,2020.findings-emnlp.341,0,0.0200122,"model (i.e., a random seed). We use these insights to improve the robustness of English parsing models, relying on adversarial training and model ensembling.1 1 Introduction Neural network-based models have achieved great successes in a wide range of NLP tasks. However, recent work has shown that their performance can be easily undermined with adversarial examples that would pose no confusion for humans (Zhang et al., 2020). As an increasing number of successful adversarial attackers have been developed for NLP tasks, the quality of the adversarial examples they generate has been questioned (Morris et al., 2020). The definition of a valid successful adversarial example differs across target tasks. In semantic tasks such as sentiment analysis (Zhang et al., 2019) and textual entailment (Jin et al., 2020), a valid successful adversarial example needs to be able to alter the prediction of the target model while ∗ Work partially done while at the University of Edinburgh. 1 Our code is available at: https://github.com/ WangYuxuan93/DepAttacker.git preserving the semantic content and fluency of the original text. In contrast, in the less explored field of attacking syntactic tasks, the syntactic structure,"
2021.findings-acl.207,N16-1018,0,0.0609948,"Missing"
2021.findings-acl.207,D14-1162,0,0.0855441,"ich achieve performance close to the state-of-the-art. Deep Biaffine Parser (Dozat and Manning, 2017) is a graph-based parser that scores each candidate arc independently and relies on a decoding algorithm to search for the highest-scoring tree. Stack-Pointer Parser (Ma et al., 2018) is a transition-based parser that incrementally builds the dependency tree with pre-defined operations. We used the following four types of token embeddings to study their influence on each parsers’ robustness. To focus on the influence of the embeddings, we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece embeddings. ELECTRA (Clark et al., 2020) is a pre-trained language model based on a replaced token detection object, which learns to predict whether each token in the corrupted input has been replaced. It produces contextualised word piece embeddings. ELMo (Peters et al., 2018) is a pre-trained language representation model based on characte"
2021.findings-acl.207,N18-1202,0,0.0174184,", we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece embeddings. ELECTRA (Clark et al., 2020) is a pre-trained language model based on a replaced token detection object, which learns to predict whether each token in the corrupted input has been replaced. It produces contextualised word piece embeddings. ELMo (Peters et al., 2018) is a pre-trained language representation model based on character embeddings and bidirectional language modelling. 3.2 Datasets and Experimental Settings We train the target parsers and evaluate the proposed method on the English Penn Treebank (PTB) dataset,7 converted into Stanford dependencies using version 3.3.0 of the Stanford dependency converter (de Marneffe et al., 2006) (PTB-SD-3.3.0). We follow the standard PTB split, using section 2-21 for training, section 22 as a development set and 23 as a test set. It is important to note that when converting PTB into Stanford dependencies, Zhen"
2021.findings-acl.207,P19-1103,1,0.805871,"ndency tree of x∗ should be the same as that of x. In this paper, these two constraints are ensured through the use of various filters (see Section 2.3) and are used to evaluate the quality of adversarial examples (see details on fluency and syntactic structure preservation in Section 3.3). 2.2 Word Importance Ranking Word importance ranking in our model is based on the observation that some words have a stronger influence on model prediction than others. Such word importance is typically computed by setting each word to unknown and examining the changes in their predictions (Li et al., 2016; Ren et al., 2019). 2345 Algorithm 1 Dependency Parsing Attack generation methods, then apply filters to discard inappropriate substitutes, ensuring both diversity and quality of the generated candidates. Input: Sentence example x(0) = {x1 , x2 , . . . , xN }, maximum percentage of words allowed to be modified γ Output: Adversarial example x(i) 1: for i = 1 to N do 2: Compute word importance I(x(0) , xi ) via Eq. 1 3: end for 4: Create a set W of all words xi ∈ x(0) sorted by the descending order of their importance I(x(0) , xi ). 5: t = 0 6: for each word xj in W do 7: Build candidate set Cj for xj following t"
2021.findings-acl.207,D16-1058,0,0.0368721,"Morris et al. (2020) reported that quite a number of these techniques introduce grammatical errors. In syntactic tasks, Zheng et al. (2020) recently proposed the first dependency parser attacking method which depends entirely on BERT to generate candidates. However, we show that the quality of adversarial examples generated by their method is relatively low due to the limitation of the BERTbased generator, and we propose to generate better examples by using more generators and stricter filters. Han et al. (2020) proposed an approach to attack structured prediction models with a seq2seq model (Wang et al., 2016) and evaluated this model on dependency parsing. They used two reference parsers in addition to the victim parser to supervise the training of the adversarial example generator, and found that the three parsers produce better results when they have different inductive biases embedded to make the attack successful. This finding is quite close in spirit to our conclusion in Section 4.5. Hu et al. (2020) also put forth efforts to modify the text in syntactic tasks while preserving the original syntactic structure. However, their goal is to preserve privacy via the modification of words that could"
2021.findings-acl.207,2020.acl-main.540,0,0.0323383,"e importance of a word xi in sentence x is computed as 2.3.1 Generating Process We collect substitutes from the following methods: BERT-Based Method: We use BERT to generate candidates for each target word from its context. This method generates only single subwords. Embedding-Based Method: Following Alzantot et al. (2018), we use word embeddings of Mrkˇsi´c et al. (2016)2 to compute the N nearest neighbours of each target word according to their cosine similarity and use them as candidates. Sememe-Based Method: The sememes of a word represent its core meaning (Dong and Dong, 2006). Following Zang et al. (2020), we collect the substitutes of the target word x based on the rule that one of the substitutes the senses of x∗ must have the same sememe annotations as one of senses of x. Synonym-Based Method: We use WordNet3 to extract synonyms of each target word as candidates. 2.3.2 Filtering Process Generation of Substitute Candidates We apply the following four types of filters to discard candidates which are likely inappropriate, either in terms of syntactic preservation or fluency. POS Filter: We first filter out substitutes with different part-of-speech (POS) tags from the original word.4 This filte"
2021.findings-acl.207,P19-1559,0,0.0216978,"ing.1 1 Introduction Neural network-based models have achieved great successes in a wide range of NLP tasks. However, recent work has shown that their performance can be easily undermined with adversarial examples that would pose no confusion for humans (Zhang et al., 2020). As an increasing number of successful adversarial attackers have been developed for NLP tasks, the quality of the adversarial examples they generate has been questioned (Morris et al., 2020). The definition of a valid successful adversarial example differs across target tasks. In semantic tasks such as sentiment analysis (Zhang et al., 2019) and textual entailment (Jin et al., 2020), a valid successful adversarial example needs to be able to alter the prediction of the target model while ∗ Work partially done while at the University of Edinburgh. 1 Our code is available at: https://github.com/ WangYuxuan93/DepAttacker.git preserving the semantic content and fluency of the original text. In contrast, in the less explored field of attacking syntactic tasks, the syntactic structure, rather than the semantic content, must be preserved while also maintaining the fluency. Preserving the syntactic structure enables us to use the gold sy"
2021.findings-acl.207,2020.acl-main.590,0,0.151193,"tacking syntactic tasks, the syntactic structure, rather than the semantic content, must be preserved while also maintaining the fluency. Preserving the syntactic structure enables us to use the gold syntactic structure of the original sentence in the evaluation process. While preserving the fluency ensures that ungrammatical adversarial examples, which not only fool the target model but also confuse humans, will not be considered valid. Therefore in this paper, we evaluate the quality of an adversarial example in two aspects, namely the fluency and syntactic structure preservation. Recently, Zheng et al. (2020) proposed the first dependency parser attacking algorithm based on word-substitution which depended entirely on BERT (Devlin et al., 2019) to generate candidate substitutes. The rational was that the use of the pre-trained language model will ensure fluency of the adversarial examples. However, we find that using BERT alone is far from enough to preserve fluency. Therefore, in this paper, we propose a method to generate better adversarial examples for dependency parsing with four types of candidate generators and filters. Specifically, our method consists of three steps: (i) determining the su"
2021.findings-acl.216,2020.findings-emnlp.184,0,0.0894579,"s more intuitive to use non-autoregressive language models such as BERT to directly correct the Chinese spelling errors. Hong et al. (2019) propose the FASPell model to predict candidate characters based on the BERT model and exploit the phonological and visual similarity information to select candidate characters. Zhang et al. (2020) propose a model named Soft-Masked BERT, which consists of a detection network and a correction network based on BERT. Cheng et al. (2020) propose to incorporate phonological and visual similarity knowledge into BERT via a specialized graph convolutional network. Bao et al. (2020) design a chunk-based framework and extend the traditional confusion sets with semantical candidates to cover different types of errors. Although these non-autoregressive methods mentioned above have achieved state of the art in the CSC task so far, these methods still suffer from the incoherent problems that exist in nonautoregressive models (Gu et al., 2018; Gu and Kong, 2020). In this paper, we propose a novel model DCN which learns the dependencies between the adjacent Chinese characters and alleviates the incoherent problem. 3 3.1 Chinese spelling check (CSC) is a challenging task that re"
2021.findings-acl.216,2020.acl-main.81,0,0.122708,"nese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling error and corrected to the most likely character. Based on the powerful generalization ability of BERT (Devlin et al., 2019), these works have achieved better performance than other models. However, these"
2021.findings-acl.216,N12-1067,0,0.0326906,"egressive language models in the CSC task. • We propose a simple and effective Pinyin Enhanced Candidate Generator to incorporate phonological information and generate better candidate characters. • Experimental results show that our proposed method achieves state-of-the-art performance on three human-annotated datasets. For reproducibility, our code for this paper is available at https://github.com/destwang/DCN. 2 Related Work ability. With the development of deep learning techniques, the CSC task has recently made more progress. CSC is similar to the grammatical error correction (GEC) task (Dahlmeier and Ng, 2012). The difference between them is that CSC only focuses on Chinese spelling errors, while GEC also includes errors that need insertion and deletion. Most models in the GEC task use an autoregressive Seq2Seq model to correct a sentence. Similarly, Seq2Seq models can also be used in the CSC task. Wang et al. (2019) propose an autoregressive pointer network which generates a Chinese character from the confusion set rather than the entire vocabulary. Although the autoregressive Seq2Seq model has the ability to correct the spelling errors, it is usually slow. The input and output are so similar that"
2021.findings-acl.216,N19-1423,0,0.00705992,"te-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling error and corrected to the most likely character. Based on the powerful generalization ability of BERT (Devlin et al., 2019), these works have achieved better performance than other models. However, these works on the CSC task rely on the incorrect independence assumption, which may lead to an incoherent problem. Concretely, they assume that the predicted tokens are independent of each other, which generally does not hold in natural language (Yang et al., 2019; Gu and Kong, 2020). For the CSC task, one spelling error may have multiple corrections. Ignoring the corrected context may result in a correction conflict. As shown in Table 1, “户秃” may be corrected as “糊涂” (confused) or “尴尬” (embarrassed). Because of the in"
2021.findings-acl.216,D19-5522,0,0.114934,"idate Chinese characters via a Pinyin Enhanced Candidate Generator and then utilizes an attention-based network to model the dependencies between two adjacent Chinese characters. The experimental results show that our proposed method achieves a new state-ofthe-art performance on three human-annotated datasets. 1 Table 1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at ea"
2021.findings-acl.216,C10-2085,0,0.0377928,"een two adjacent Chinese characters. The experimental results show that our proposed method achieves a new state-ofthe-art performance on three human-annotated datasets. 1 Table 1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling erro"
2021.findings-acl.216,W13-4409,0,0.0302237,"Missing"
2021.findings-acl.216,2021.ccl-1.108,0,0.041413,"Missing"
2021.findings-acl.216,D19-1510,0,0.0126642,"nese spelling errors, while GEC also includes errors that need insertion and deletion. Most models in the GEC task use an autoregressive Seq2Seq model to correct a sentence. Similarly, Seq2Seq models can also be used in the CSC task. Wang et al. (2019) propose an autoregressive pointer network which generates a Chinese character from the confusion set rather than the entire vocabulary. Although the autoregressive Seq2Seq model has the ability to correct the spelling errors, it is usually slow. The input and output are so similar that it would be “wasteful” to completely regenerate a sequence (Malmi et al., 2019). Since the input and output have the same number of Chinese characters, and the correct and incorrect Chinese characters correspond to each other, it is more intuitive to use non-autoregressive language models such as BERT to directly correct the Chinese spelling errors. Hong et al. (2019) propose the FASPell model to predict candidate characters based on the BERT model and exploit the phonological and visual similarity information to select candidate characters. Zhang et al. (2020) propose a model named Soft-Masked BERT, which consists of a detection network and a correction network based on"
2021.findings-acl.216,D18-1273,0,0.0447473,"generates the candidate Chinese characters via a Pinyin Enhanced Candidate Generator and then utilizes an attention-based network to model the dependencies between two adjacent Chinese characters. The experimental results show that our proposed method achieves a new state-ofthe-art performance on three human-annotated datasets. 1 Table 1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a"
2021.findings-acl.216,P19-1578,0,0.0124904,"ets. For reproducibility, our code for this paper is available at https://github.com/destwang/DCN. 2 Related Work ability. With the development of deep learning techniques, the CSC task has recently made more progress. CSC is similar to the grammatical error correction (GEC) task (Dahlmeier and Ng, 2012). The difference between them is that CSC only focuses on Chinese spelling errors, while GEC also includes errors that need insertion and deletion. Most models in the GEC task use an autoregressive Seq2Seq model to correct a sentence. Similarly, Seq2Seq models can also be used in the CSC task. Wang et al. (2019) propose an autoregressive pointer network which generates a Chinese character from the confusion set rather than the entire vocabulary. Although the autoregressive Seq2Seq model has the ability to correct the spelling errors, it is usually slow. The input and output are so similar that it would be “wasteful” to completely regenerate a sequence (Malmi et al., 2019). Since the input and output have the same number of Chinese characters, and the correct and incorrect Chinese characters correspond to each other, it is more intuitive to use non-autoregressive language models such as BERT to direct"
2021.findings-acl.216,W13-4406,0,0.0507596,"Missing"
2021.findings-acl.216,W14-6825,0,0.0130065,"characters. The experimental results show that our proposed method achieves a new state-ofthe-art performance on three human-annotated datasets. 1 Table 1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling error and corrected to the mo"
2021.findings-acl.216,2020.acl-main.82,0,0.105649,"1: An example of Chinese spelling errors. Here, “户秃” should be corrected to “糊涂” (confused). Introduction Chinese spelling check (CSC) is an important task which can be utilized in many natural language applications such as optical character recognition (OCR) (Wang et al., 2018; Hong et al., 2019) and essay scoring. Meanwhile, CSC is a challenging task which requires human-level natural language understanding ability (Liu et al., 2010, 2013; Xin et al., 2014). Recently, BERT-based non-autoregressive language models have achieved state-of-the-art performance in the CSC task (Hong et al., 2019; Zhang et al., 2020; Cheng et al., 2020). These works fine-tune BERT-based models using CSC training data. During the training phase, all the target Chinese characters will be involved as labels. In the inference stage, the models predict the most likely Chinese character from a candidate set at each position. When the most likely character is different from the input character, the input character will be considered as a spelling error and corrected to the most likely character. Based on the powerful generalization ability of BERT (Devlin et al., 2019), these works have achieved better performance than other mo"
2021.findings-acl.282,D19-1649,0,0.0227033,"tly learns the intent and slot tasks by sharing the BERT encoder. LD-Proto is also a prototypical model similar to JointProto. The only difference is that it is enhanced by the logits-dependency tricks (Goo et al., 2018), where joint learning is achieved by depending on the intent and slot prediction on the logits of the accompanying task. Implements For both ours and baseline models, we determine the hyperparameters on the development set. We use ADAM (Kingma and Ba, 2015) for training and set batch size as 4 and learning rate 3195 as 10−5 . We adopt embedding tricks of Pairs-Wise Embedding (Gao et al., 2019; Hou et al., 2020a) and Gradual Unfreezing (Howard and Ruder, 2018). The λ and α in Section 3.2 are both set as 0.5. We implement both our and baseline models with the few-shot platform MetaDialog.5 Besides, to use the information in target domains and make a fair comparison with fine-tuning baselines, we explore the performance of the similarity-based model under fine-tuning setting (+FT) and enhance the model with a fine-tune process similar to Meta-JOSFIN. In addition, following the suggestions of Hou et al. (2020a), we investigate adding Transition Rules (+TR) between slot tags, which ban"
2021.findings-acl.282,N19-1423,0,0.0298506,"her crossattention scores. For example, “PlayVideo” and “film” are more related, so the corresponding score is larger. x being associated with intent label li as: p(li |x, S) exp (S IM(Eintent (x), Cintenti )) , =P j exp (S IM (Eintent (x), Cintentj )) and estimates the probability of the kth token in x belonging to the ith slot class as: p(ti |k, x, S) exp (S IM(Eslot (xk ), Csloti )) , =P j exp (S IM (Eslot (xk ), Cslotj )) where Cintenti and Csloti are prototypes derived with support examples. Eintent (·) and Eslot (·) are embedder functions for intent and slot respectively. We adopt BERT (Devlin et al., 2019) as the embedder, and the sentence embedding Eintent (x) is calculated as the averaged embedding of its tokens. We use the dot-product similarity for function S IM(·, ·). 3.2 Proposed Method In this section, we introduce the proposed Contrastive Prototype Merging network (ConProm). Firstly, we describe the few-shot intent detection and slot filling with Prototypical network (§3.1). Based on that, we present two key components of ConProm: the Prototype Merging mechanism that adaptively connects two metric spaces of intent and slot (§3.2) and the Contrastive Alignment Learning that jointly refin"
2021.findings-acl.282,2020.coling-main.438,0,0.0330009,"d by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al.,"
2021.findings-acl.282,P19-1544,0,0.0471014,"Missing"
2021.findings-acl.282,D19-1403,0,0.0136268,"ntence Level Slot Accuracy, which considers a sentence to be correct when all slots are correct. As shown in Table 4, there is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explo"
2021.findings-acl.282,N18-2118,0,0.370359,"ery Example: Play The Lord of the Rings on my mobile. Few-Shot Learning (FSL) that committed to learning new problems with only a few examples (Miller et al., 2000; Vinyals et al., 2016) is promising to break the data-shackles of current deep learning. Commonly, existing FSL methods learn a single few-shot task each time. But, real-world applications, such as dialogue language understanding, usually contain multiple closely related tasks (e.g., intent detection and slot filling) and often benefit from jointly learning these tasks (Worsham and Kalita, 2020; Chen et al., 2019; Qin et al., 2019; Goo et al., 2018). In few-shot scenarios, such requirements of joint learning present new challenges for FSL techniques to capture task relations from only a few examples and jointly learn multiple tasks. † Where can I buy face masks product nearby? |intent: FindShop Find the nearest barbecue food. |intent: FindRestaurant Introduction * Equal Support Examples: Figure 1: Examples of the few-shot joint dialogue language understanding. On each domain, given a few labeled support examples, the model predicts the intent and slot labels for unseen query examples. Joint learning benefits from capturing the relation b"
2021.findings-acl.282,2020.acl-main.128,1,0.873147,"Cintenti , i F F where Cintent and Cslot are the fused prototypes i j of ith intent and the jth slot respectively. At last, we obtain the representation of merged prototypes C 0 by combining the origin prototype 1 C with the fused prototype C F : 0 F Cintent = α × Cintent + (1 − α) × Cintent , 0 F Cslot = α × Cslot + (1 − α) × Cslot , where the α is a hyper-parameter that controls the importance of intent-slot relation. 3.3 Contrastive Alignment Learning Similarity-based few-shot learning relies heavily on a good metric space, where different classes should be well separated from each other (Hou et al., 2020a; Yoon et al., 2019). In joint-learning scenarios, there are further requests to connect metric spaces of joint learned tasks and jointly optimize these metric spaces. In response to the above requests, we argue that the distribution of prototypes of dialogue language understanding should fit these intuitions: (1) different intent prototypes should be far away and the same as slot prototypes (Intra-Contrastive); (2) the slot prototypes should close to the related intent prototypes and should be far away from the unrelated intent prototypes (Inter-Contrastive).2 To achieve these, we introduce"
2021.findings-acl.282,C18-1105,1,0.816207,"to and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue under"
2021.findings-acl.282,P18-1031,0,0.0206039,"der. LD-Proto is also a prototypical model similar to JointProto. The only difference is that it is enhanced by the logits-dependency tricks (Goo et al., 2018), where joint learning is achieved by depending on the intent and slot prediction on the logits of the accompanying task. Implements For both ours and baseline models, we determine the hyperparameters on the development set. We use ADAM (Kingma and Ba, 2015) for training and set batch size as 4 and learning rate 3195 as 10−5 . We adopt embedding tricks of Pairs-Wise Embedding (Gao et al., 2019; Hou et al., 2020a) and Gradual Unfreezing (Howard and Ruder, 2018). The λ and α in Section 3.2 are both set as 0.5. We implement both our and baseline models with the few-shot platform MetaDialog.5 Besides, to use the information in target domains and make a fair comparison with fine-tuning baselines, we explore the performance of the similarity-based model under fine-tuning setting (+FT) and enhance the model with a fine-tune process similar to Meta-JOSFIN. In addition, following the suggestions of Hou et al. (2020a), we investigate adding Transition Rules (+TR) between slot tags, which bans illegal slot prediction, such as “I” tag after “O” tag. 4.3 Main R"
2021.findings-acl.282,2020.findings-emnlp.163,1,0.79015,"Missing"
2021.findings-acl.282,2020.nlp4convai-1.12,0,0.119962,"he loss for intent detection and slot filling. Combining with the loss of Contrastive Alignment Learning, we train the entire model with the following objective function: Lall = CEintent + CEslot + LContrastive 4 Experiments We evaluate our method on the dialogue language understanding task of 1-shot/5-shot setting, which transfers knowledge from source domains (training) to an unseen target domain (testing) containing only 1-shot/5-shot support set. 4.1 Few-shot Dataset Construction To simulate the few-shot learning situation, we follow previous few-shot learning works (Vinyals et al., 2016; Krone et al., 2020a; Finn et al., 2017) and construct the dataset into a few-shot episode style, where the model is trained and evaluated with a series of few-shot episodes. Each episode contains a support set and query set. However, different from the single-task problem, joint-learning examples are associated with multiple labels. Therefore, we cannot guarantee that each label appears K times while sampling examples for the K-shot support set. To remedy this, we build support sets with the Mini-Including Algorithm (Hou et al., 2020a), which is intended for such situations. It constructs support set generally"
2021.findings-acl.282,D17-1035,0,0.0217313,"etrics for evaluation: Intent Accuracy, Slot F1-score, Joint Accuracy.4 For joint dialogue language understanding, Joint Accuracy is the most important metric among all three metrics (Hou et al., 2020c). It evaluates the sentence level accuracy, which considers one sentence is correct only when all its slots and intents are correct. To conduct a robust evaluation under few-shot setting, we validate the models on multiple fewshot episodes (i.e., support-query set pairs) from different domains and take the average score as final results. To control the non-deterministic neural network training (Reimers and Gurevych, 2017), we report the average score of 5 random seeds for all results. 4.2 Baselines We compare our model with two kinds of strong baseline: fine-tune based transfer learning methods 4 We calculate the Slot F1-score with the conlleval script https://www.clips.uantwerpen.be/ conll2000/chunking/conlleval.txt 3194 Snips Models FewJoint Intent Acc. Slot F1 Joint Acc. Intent Acc. Slot F1 Joint Acc. SepProto JointProto LD-Proto LD-Proto+TR ConProm (Ours) ConProm+TR (Ours) 98.23±0.66 92.57±0.57 97.25±0.71 97.53±0.30 96.67±1.45 96.17±0.76 43.90±1.98 42.63±2.03 47.81±2.53 51.03±2.40 53.05±0.81 55.84±0.85 9.4"
2021.findings-acl.282,P19-1547,0,0.0563973,"Missing"
2021.findings-acl.282,D18-1417,0,0.0286999,"al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understanding is less investigated. Krone et al. (2020b) and Bhathiya and Thayasivam (2020) make the earliest attempts by directly adopt general and classic few-shot learning methods such as MAML and prototypical network. These methods achieve joint learning by sharing the embedding between intent detection and slot filling task, which model the relation between intent and slot task implicitly. By contras"
2021.findings-acl.282,D19-1097,0,0.0138918,"enario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understanding is less investigated. Krone et al. (2020b) and Bhathiya and Thayasivam (2020) make the earliest attempts by directly adopt general and classic few-shot learning methods such as MAML and prototypical network. These methods achieve joint learning by sharing the embedding between intent detection and slot filling task, which model the relation between intent and slot task implicitly. By contrast, we explicitly model the interaction between intent and slot with attentive information fusion and constrastive loss. Experiment results also dem"
2021.findings-acl.282,2020.acl-main.3,0,0.0131908,"Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al."
2021.findings-acl.282,P18-1194,0,0.021577,"ore between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on jo"
2021.findings-acl.282,D19-1334,0,0.0271581,"correct. As shown in Table 4, there is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold."
2021.findings-acl.282,D19-1214,1,0.914392,"tent: PlayVoice Query Example: Play The Lord of the Rings on my mobile. Few-Shot Learning (FSL) that committed to learning new problems with only a few examples (Miller et al., 2000; Vinyals et al., 2016) is promising to break the data-shackles of current deep learning. Commonly, existing FSL methods learn a single few-shot task each time. But, real-world applications, such as dialogue language understanding, usually contain multiple closely related tasks (e.g., intent detection and slot filling) and often benefit from jointly learning these tasks (Worsham and Kalita, 2020; Chen et al., 2019; Qin et al., 2019; Goo et al., 2018). In few-shot scenarios, such requirements of joint learning present new challenges for FSL techniques to capture task relations from only a few examples and jointly learn multiple tasks. † Where can I buy face masks product nearby? |intent: FindShop Find the nearest barbecue food. |intent: FindRestaurant Introduction * Equal Support Examples: Figure 1: Examples of the few-shot joint dialogue language understanding. On each domain, given a few labeled support examples, the model predicts the intent and slot labels for unseen query examples. Joint learning benefits from captu"
2021.findings-acl.282,D19-1045,0,0.0235225,"evaluating the Sentence Level Slot Accuracy, which considers a sentence to be correct when all slots are correct. As shown in Table 4, there is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou e"
2021.findings-acl.282,N18-2050,0,0.0137719,"a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understanding is less investigated. Krone et al. (2020b) and Bhathiya and Thayasivam (2020) make the earliest attempts by directly adopt general and classic few-shot learning methods such as MAML and prototypical network. These methods achieve joint learning by sharing the embedding between intent detection and slot filling task, which model the relation between intent and slot task implicitly. By contrast, we explicitly model the interaction between intent and"
2021.findings-acl.282,2020.emnlp-main.152,0,0.0310237,"Missing"
2021.findings-acl.282,D18-1348,0,0.0283958,"2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understand"
2021.findings-acl.282,P19-1277,0,0.0267868,"e is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a sing"
2021.findings-acl.282,N18-1109,0,0.0174798,"siders a sentence to be correct when all slots are correct. As shown in Table 4, there is a huge gap in the slot accuracy score between LD-Proto and ConProm, which explains the gap in Joint score. 5 Related Work Few-shot learning is one of the most important direction for machine learning area (Fei-Fei, 2006; Fink, 2004) and often achieved by similarity-based method (Vinyals et al., 2016) and fine-tuning based method (Finn et al., 2017). FSL in natural language processing has been explored for various tasks, including text classification (Sun et al., 2019; Geng et al., 2019; Yan et al., 2018; Yu et al., 2018), entity relation classification (Lv et al., 2019; Gao et al., 2020; Ye and Ling, 2019), sequence labeling (Luo et al., 2018; Hou et al., 2018; Shah et al., 2019; Hou et al., 2020a; Liu et al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent dete"
2021.findings-acl.282,P19-1519,0,0.0195517,"t al., 2020). As the important part of a dialog system, dialogue language understanding attract a lot of attention in few-shot scenario. Dopierre et al. (2020); Vlasov et al. (2018); Xia et al. (2018) explored fewshot intent detection technique. Luo et al. (2018) and Hou et al. (2020a) investigated few-shot slot tagging by using prototypical network. Hou et al. (2020b) explored few-shot multi-label intent detection with an adaptive logit adapting threshold. But all of these works focus on a single task. Despite a lot of works on joint dialogue understanding (Goo et al., 2018; Li et al., 2018; Zhang et al., 2019; Qin et al., 2019; Wang et al., 2018; E et al., 2019; Wu et al., 2020; Gangadharaiah and Narayanaswamy, 2019; Liu et al., 2019; Qin et al., 2020), few-shot joint dialogue understanding is less investigated. Krone et al. (2020b) and Bhathiya and Thayasivam (2020) make the earliest attempts by directly adopt general and classic few-shot learning methods such as MAML and prototypical network. These methods achieve joint learning by sharing the embedding between intent detection and slot filling task, which model the relation between intent and slot task implicitly. By contrast, we explicitly mod"
2021.findings-acl.450,P19-1358,0,0.0275336,"these models all face challenges on the proposed dialogue understanding task. The DEQA dataset will release for research use. 1 Q2 A2 Co-reference Chain (OntoNotes style) Chain 1 (IDENT) Chain 3 (IDENT) 1.6-8 a clean house 1.12-12 grandma 2.5-5 it 2.8-8 she (b) Chain 2 (IDENT) 3.1-1 she 3.4-9 head janitor at St. 4.3-3 she Mary’s Hospital 5.4-4 she 6.3-3 she 4.5-6 that job Introduction Driven by the growth of interest in social chatbot, online customer service and virtual mobile assistant, social dialogue systems have received increasing research attention (Cui et al., 2017; Zhou et al., 2018; Hancock et al., 2019). The current dominant method has been sequence-to-sequence models, trained over large dialogue data end-to-end. Such models use neural network architectures such as Transformer (Vaswani et al., 2017) to encode a user utterance and a dialogue history before generating a system utterance (Adiwardana et al., 2020; Roller et al., 2020; Bao et al., 2020). A major advantage is the use of standard and general model architecture, which facilitates end-to-end training process over large scale dialogue text (Shang et al., 2015; Zhang et al., 2018, 2019, 2020). 1 Q1 A1 IDENT denotes the entities in a co"
2021.findings-acl.450,I17-1099,0,0.0241126,"gues include: • Ellipsis 1) Zero anaphora (Noun phrase ellipsis) U1 : “你喜欢邦乔维的音乐吗？” (“Do you like music of Bon Jovi?”) U2 : “是的(Yes)，我(I)喜欢(like)。” The noun phrase “邦乔维的音乐” (“music of Bon Jovi”) is omitted in the second utterance. 2) Verbal phrase ellipsis U1 : “I like the V6 engine of Audi S4.” U2 : “So do I.” Here, “do” is a trigger word which indicates the ellipsis of verbal phrase “like the V6 engine of Audi S4”. • Anaphor 1) Personal pronoun In this case, “Swifty” and “Taylor Swift” are coreference. 2.3 Data Annotation The English dialogue data are sourced from the DailyDialogue dataset (Li et al., 2017). The Chinese dialogue data are collected by ourself from Douban3 , a Chinese online forum. We randomly sample a subset of dialogues from the above two dataset respectively, and then annotate these dialogues in question answering. For annotation, the first step is to identify ellipsis, anaphor and co-reference phenomenons of utterances in dialogue data. For each utterance, annotators determine whether the meaning of the utterance is complete when ignoring the dialogue context. If the meaning of an utterance is determined as incomplete, we can further identify ellipsis. Both zero anaphora and v"
2021.findings-acl.450,P17-4017,0,0.0165461,"esentative dialogue models show that these models all face challenges on the proposed dialogue understanding task. The DEQA dataset will release for research use. 1 Q2 A2 Co-reference Chain (OntoNotes style) Chain 1 (IDENT) Chain 3 (IDENT) 1.6-8 a clean house 1.12-12 grandma 2.5-5 it 2.8-8 she (b) Chain 2 (IDENT) 3.1-1 she 3.4-9 head janitor at St. 4.3-3 she Mary’s Hospital 5.4-4 she 6.3-3 she 4.5-6 that job Introduction Driven by the growth of interest in social chatbot, online customer service and virtual mobile assistant, social dialogue systems have received increasing research attention (Cui et al., 2017; Zhou et al., 2018; Hancock et al., 2019). The current dominant method has been sequence-to-sequence models, trained over large dialogue data end-to-end. Such models use neural network architectures such as Transformer (Vaswani et al., 2017) to encode a user utterance and a dialogue history before generating a system utterance (Adiwardana et al., 2020; Roller et al., 2020; Bao et al., 2020). A major advantage is the use of standard and general model architecture, which facilitates end-to-end training process over large scale dialogue text (Shang et al., 2015; Zhang et al., 2018, 2019, 2020)."
2021.findings-acl.450,D18-1134,0,0.015898,"llenges, namely CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018). Rather 6 The average numbers of tokens in the answer types of entity, phrase, clause and fragment are 1.22, 2.15, 9.07, 5.48 in English dialogue data and 1.95, 3.28, 6.86 and 5.77 characters in Chinese dialogue data. than understanding the meaning of a given passage/document through the form of conversational question answering, the proposed task focuses on measuring the capability of understanding the dialogue itself. Besides the two challenges, several conversational machine reading/comprehension datasets were proposed (Elgohary et al., 2018; Dinan et al., 2018; Huang et al., 2018; Saeidi et al., 2018). The most common characteristic of these datasets are that their questions are open-domain and sequentially (or contextually) related, which shows a recent recognition in the research community that understanding the semantics of a complete conversation, including historical question and answer contexts, is crucial for these tasks. Our work is similar in spirit, but concentrating on clarification requests. Clarification Request in Dialogue Clarification requests (CR) in dialogue are mainly motivated by acoustic understanding and se"
2021.findings-acl.450,D19-1462,0,0.135062,"mplicit mentions by using zero pronouns. Take Table 1 (a) as an example, where the dialogue history consists of 7 utterances and the second utterance contains a pronoun “it”. At this point, we can measure system understanding of the dialogue state by checking whether the system can resolve the anaphora concerning “a clean house”. Our goal is to provide a large-scale benchmark and to evaluate the performance of social chatbot systems on dialogue understanding concerning entities. One way to define the task is to cast it as a co-reference resolution problem (Yin et al., 2017; Kong et al., 2019; Quan et al., 2019), where a benchmark can be constructed by manually labeling co-reference information on a dialogue dataset, as shown in Table 1 (b). However, such a benchmark does not fully meet our goal because a separate model is necessary for achieving co-reference resolution, and it may be challenging to seamlessly integrate such a co-reference module into a dialogue model being tested. We take a different method, checking dialogue understanding of dialogue systems by inserting clarification requests (Schlangen, 2004; Stoyanchev and Johnston, 2015) into dialogues, and evaluating the response of dialogue s"
2021.findings-acl.450,P05-1030,0,0.0693166,"ntextually) related, which shows a recent recognition in the research community that understanding the semantics of a complete conversation, including historical question and answer contexts, is crucial for these tasks. Our work is similar in spirit, but concentrating on clarification requests. Clarification Request in Dialogue Clarification requests (CR) in dialogue are mainly motivated by acoustic understanding and semantic understanding (Schlangen, 2004; Stoyanchev and Johnston, 2015). They are used mainly as a way to establish mutual knowledge or grounding in communication (Gabsdil, 2003; Rieser and Moore, 2005). Purver et al. (2003) proposed to classify the forms of clarification requests into 8 categories, including non-reprise clarifications, reprise sentences, reprise sluices, reprise fragments, gaps, gap fillers, conventional and other. Rodr´ıguez and Schlangen (2004) further summarized the surface forms, intonations and functions of clarification requests in spoken dialogue systems. Ginzburg (2016) detailed the semantics of dialogue and the fundamental problems to tackle for the semantic analysis in dialogue. In their work, a clarification request is defined to be a core function for dialogue s"
2021.findings-acl.450,2020.acl-main.626,0,0.0257303,"ted responses. This makes our benchmark directly useful for evaluating arbitrary social dialogue models. In contrast to open-ended responses in chit-chats, responses for the proposed clarification requests are factual thus facilitating automatic evaluation. Second, it allows easier crowdsourcing for dataset construction as compared with co-reference resolution, which requires strict training of manual labelers for understanding linguistic concepts. It is thus useful for acquiring large-scale datasets. Such observation is consistent with recent work on other NLP tasks (FitzGerald et al., 2018; Roit et al., 2020). Third, this method allows easy extension to dialogue understanding beyond the entity reference level, such as event co-references, semantic relations and discourse level understanding. No new labeling standards are necessary for adding a new task. According to the above observations, we create a large scale benchmark, open domain Dialogue Entity via Question Answering (DEQA), which consists of one English dataset and one Chinese dataset, of 8,415 and 6,203 dialogues, respectively. Each dialogue contains one or more questions similar to the one in Table 1. We choose to evaluate representative"
2021.findings-acl.450,D18-1233,0,0.0607599,"Missing"
2021.findings-acl.450,W04-2325,0,0.361542,"to cast it as a co-reference resolution problem (Yin et al., 2017; Kong et al., 2019; Quan et al., 2019), where a benchmark can be constructed by manually labeling co-reference information on a dialogue dataset, as shown in Table 1 (b). However, such a benchmark does not fully meet our goal because a separate model is necessary for achieving co-reference resolution, and it may be challenging to seamlessly integrate such a co-reference module into a dialogue model being tested. We take a different method, checking dialogue understanding of dialogue systems by inserting clarification requests (Schlangen, 2004; Stoyanchev and Johnston, 2015) into dialogues, and evaluating the response of dialogue systems on such requests. One example is shown in Table 1 (a), where we break a dialogue in the middle, adding clarification requests. For example, for the question “Who is just a neat freak?”, the correct system response should be “Grandma”, which reflects that the model has correct understanding of the dialogue context. The advantage is three fold. First, this method allows the evaluation of a dialogue system without using an external probe task, by directly evaluating system generated responses. This ma"
2021.findings-acl.450,P15-1152,0,0.0324938,"eived increasing research attention (Cui et al., 2017; Zhou et al., 2018; Hancock et al., 2019). The current dominant method has been sequence-to-sequence models, trained over large dialogue data end-to-end. Such models use neural network architectures such as Transformer (Vaswani et al., 2017) to encode a user utterance and a dialogue history before generating a system utterance (Adiwardana et al., 2020; Roller et al., 2020; Bao et al., 2020). A major advantage is the use of standard and general model architecture, which facilitates end-to-end training process over large scale dialogue text (Shang et al., 2015; Zhang et al., 2018, 2019, 2020). 1 Q1 A1 IDENT denotes the entities in a co-reference chain are identical. “1.6-8” indicates that “a clean house” is from the 6th to 8th tokens in the 1st utterance. Table 1: (a) Sample of English dialogue in the proposed dataset. U1 and U2 are two interlocutors in the dialogue. Qi and Ai (i=1,2) are clarification requests and the corresponding answers. (b) Co-reference chain annotation in OntoNotes 5.0 style.1 Despite showing effectiveness in empirical evaluation, existing work has a few important limitations. First, it is difficult to visualize or interpret"
2021.findings-acl.450,D17-1135,1,0.835148,"s can include explicit anaphora and implicit mentions by using zero pronouns. Take Table 1 (a) as an example, where the dialogue history consists of 7 utterances and the second utterance contains a pronoun “it”. At this point, we can measure system understanding of the dialogue state by checking whether the system can resolve the anaphora concerning “a clean house”. Our goal is to provide a large-scale benchmark and to evaluate the performance of social chatbot systems on dialogue understanding concerning entities. One way to define the task is to cast it as a co-reference resolution problem (Yin et al., 2017; Kong et al., 2019; Quan et al., 2019), where a benchmark can be constructed by manually labeling co-reference information on a dialogue dataset, as shown in Table 1 (b). However, such a benchmark does not fully meet our goal because a separate model is necessary for achieving co-reference resolution, and it may be challenging to seamlessly integrate such a co-reference module into a dialogue model being tested. We take a different method, checking dialogue understanding of dialogue systems by inserting clarification requests (Schlangen, 2004; Stoyanchev and Johnston, 2015) into dialogues, an"
2021.findings-acl.450,P19-1362,0,0.0178376,"proposed to alleviate the generation of vague and generic response, which is caused by the gradient vanishing of HRED model, by introducing a hidden variable z. Therefore, vHRED is a variational enhanced HRED model. CVAE: (Zhao et al., 2017) uses a prior network to model the gold response into a hidden variable z, which is as a condition in training step to improve the generation diversity. Static/Dynamic Attention: the mechanisms (Zhang et al., 2018) alternatively model the contextual representations of multi-turn dialogue history using two types of attentions rather than using RNN. ReCoSa: (Zhang et al., 2019) models the dialogue history in various granularity, e.g. context and response, using interactive attention and selfattention, respectively. Transformer: (Vaswani et al., 2017) is used as a representative pretrained encoder-decoder model for dialogue generation. DialoGPT: DialoGPT (Zhang et al., 2020) is a generative pretrained Transformer decoder for dialogue generation. To further conclude the characteristics of these models, Table 6 presents an overview of the characteristics of the chosen representative dialogue generation models in the proposed dialogue understanding task. 3.3 Implementat"
2021.findings-acl.450,C18-1206,1,0.905023,"Missing"
2021.findings-acl.450,2020.acl-demos.30,0,0.215735,"as event co-references, semantic relations and discourse level understanding. No new labeling standards are necessary for adding a new task. According to the above observations, we create a large scale benchmark, open domain Dialogue Entity via Question Answering (DEQA), which consists of one English dataset and one Chinese dataset, of 8,415 and 6,203 dialogues, respectively. Each dialogue contains one or more questions similar to the one in Table 1. We choose to evaluate representative multi-turn neural dialogue systems, including models using Transformer (Vaswani et al., 2017) and DialoGPT (Zhang et al., 2020). Results show that the prevalent models of multi-turn dialogue generation face challenges in the co-reference questions. We will release the dataset at Github 2 for research use. 2 Dataset We present the task (Section 2.1), the linguistic structures to evaluate (Section 2.2), the dataset construction (Section 2.3), the dataset characteristics (Section 2.4) and the evaluation metrics (Section 2.5) below. 2.1 Task Definition Given a multi-turn dialogue, the task is to answer questions concerning one or more turns of the dialogue history. In particular, the model needs to answer questions about"
2021.findings-acl.450,P17-1061,0,0.026974,"llowing 8 multi-turn dialogue generation models. HRED: (Serban et al., 2016) is a hierarchical RNNbased encoder-decoder framework to sequentially model multi-turn dialogue and generate responses. It consists of two directional RNNs. One RNN is modeling the tokens in an utterance. The other RNN is modeling the utterances in a dialogue context. vHRED: (Serban et al., 2017) is proposed to alleviate the generation of vague and generic response, which is caused by the gradient vanishing of HRED model, by introducing a hidden variable z. Therefore, vHRED is a variational enhanced HRED model. CVAE: (Zhao et al., 2017) uses a prior network to model the gold response into a hidden variable z, which is as a condition in training step to improve the generation diversity. Static/Dynamic Attention: the mechanisms (Zhang et al., 2018) alternatively model the contextual representations of multi-turn dialogue history using two types of attentions rather than using RNN. ReCoSa: (Zhang et al., 2019) models the dialogue history in various granularity, e.g. context and response, using interactive attention and selfattention, respectively. Transformer: (Vaswani et al., 2017) is used as a representative pretrained encode"
2021.findings-acl.56,N19-1246,0,0.290912,"how that state-of-the-art (SOTA) models are vulnerable to all of these attacks. We conclude that there is substantial room for building more robust MRC models and our benchmark can help motivate and measure progress in this area. We release our data and code at https://github.com/ NoviScl/AdvRACE. 1 Introduction The goal of Machine Reading Comprehension (MRC) is to examine whether the model can understand the text and perform certain types of reasoning. To this end, many MRC benchmarks have been constructed in different domains, styles, and languages (Rajpurkar et al., 2016; Lai et al., 2017; Dua et al., 2019b; Cui et al., 2019, inter alia). While most of these benchmarks use leaderboards to compare different models’ performance on in-domain test sets, they have ignored the important aspect of evaluating models’ robustness. The research on robustness of MRC models can be generally categorised into two directions: generalization to out-of-domain distributions and ro∗ Work done during an internship at HFL. bustness under test-time perturbations. Regarding generalization to out-of-domain distributions, Talmor and Berant (2019) and the MRQA shared task (Fisch et al., 2019) investigated how well do MRC"
2021.findings-acl.56,2020.acl-main.441,0,0.0385379,", such attacks cannot be directly applied when there is no access to model internal parameters. Another type is black-box attack where the perturbation is constructed without accessing model parameters and is often based on heuristic rules or produced by specifically designed models (Jia and Liang, 2017; Ribeiro et al., 2018; Iyyer et al., 2018). While the above approaches construct attacks automatically, there are also attempts on human-in-the-loop adversarial example generation where human annotators are employed to write new test data that can fool the models into making wrong predictions (Nie et al., 2020; Wallace et al., 2018). However, such approach requires significant human and time resources, which may not be easily accessible. Robustness benchmarks for MRC. Tang et al. (2020) created a Chinese MRC robustness benchmark by constructing distracting questions via retrieval from existing database and human writing. Wu et al. (2020) applied various common synthetic perturbations on the SQuAD dataset to analyse the factors affecting model robustness. Ribeiro et al. (2020) provided a general framework for automatically identifying various types of failures in NLP systems with CheckList. Our adve"
2021.findings-acl.56,D14-1162,0,0.0936079,"Missing"
2021.findings-acl.56,P19-1103,0,0.123335,"Missing"
2021.findings-acl.56,P18-1079,0,0.140829,"AdvRACE benchmark. We choose to construct our benchmark based on RACE because: 1) The multiple-choice format supports more types of attacks. We generated new distractors to replace the original ones as a novel attack method. 2) RACE covers a diverse set of linguistic phenomena and reasoning types, and has been used widely for evaluating NLU performance (Yang et al., 2019; Liu 634 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 634–644 August 1–6, 2021. ©2021 Association for Computational Linguistics Method AdvRACE (Ours) † † AddSent (Jia and Liang, 2017) SEA (Ribeiro et al., 2018) † SCPN (Iyyer et al., 2018) † charNMT (Belinkov and Bisk, 2018) † breakNLI (Glockner et al., 2018) † HotFlip (Ebrahimi et al., 2018) Genetic (Alzantot et al., 2018) PWWS (Ren et al., 2019) UniversalTriggers (Wallace et al., 2019) ORB (Dua et al., 2019a) † TextFooler (Jin et al., 2020) SememePSO (Zang et al., 2020) AddSent CharSwap WordReplace Paraphrase DE DG 4 4 < < 4 4 4 7 7 7 7 7 7 7 4 7 7 7 7 7 7 4 7 4 7 7 7 7 7 7 7 7 7 7 4 4 4 4 7 4 4 4 7 4 4 7 7 7 7 7 7 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 Table 1: Comparison of AdvRACE to other adversarial attack work. < indicates that"
2021.findings-acl.56,2020.acl-main.442,0,0.0212651,"ple generation where human annotators are employed to write new test data that can fool the models into making wrong predictions (Nie et al., 2020; Wallace et al., 2018). However, such approach requires significant human and time resources, which may not be easily accessible. Robustness benchmarks for MRC. Tang et al. (2020) created a Chinese MRC robustness benchmark by constructing distracting questions via retrieval from existing database and human writing. Wu et al. (2020) applied various common synthetic perturbations on the SQuAD dataset to analyse the factors affecting model robustness. Ribeiro et al. (2020) provided a general framework for automatically identifying various types of failures in NLP systems with CheckList. Our adversarial attacks have similar goals as the invariance test in CheckList. However, their template-based checks only examine limited failure types, thus less effective in revealing weaknesses in model robustness as shown by the relative small drop of model performances under invariance tests, as compared to our proposed attacks on MRC. 3 Adversarial Attacks In this section, we describe the different types of adversarial attacks explored in AdvRACE. In Table 1, we present a"
2021.findings-emnlp.222,P18-1168,0,0.0340651,"Missing"
2021.findings-emnlp.222,D18-1188,0,0.086457,"ilver rows)) ✕ Synthesized Utterance-Logical Form Pairs from Mistakes ?ො4 Which nation is listed first? Consistent Text Generator ?4 (hop Nation (first rows)) ?ො5 Which nation has the least silver? ?5 (hop Nation (argmin Silver rows)) Figure 2: Visualization of a training step in L FM. The input utterance is “Who won the most silver medals?”. tic parser using data sampled from the SCFG. The parser is then finetuned via utterance-denotation pairs using MML. Similar ideas have also been adopted to address the Text-to-SQL problem (Iyer et al., 2017; Yu et al., 2018, 2021). Instead of using SCFG, Guo et al. (2018), Zhong et al. (2020a), and Wang et al. (2021) train a SQL-to-question neural model via utterance-logical form pairs. They synthesize more training data by randomly sampling SQL queries and generating corresponding questions with the model. One shortcoming of the data augmentation work above is that they need to carefully design logical form sampling procedures and pre-define the amount of data to synthesize. It has been found that over-extensive data augmentation will cause a deep-learning model to overfit, leading to even worse performance than that without data augmentation (Shorten and Kho"
2021.findings-emnlp.222,P17-1097,0,0.129164,". Instead, it creates utterance-logical sion datasets. form pairs from mistakes on the fly to overcome the cold-start problem. Second, L FM can facilitate Dynamic approaches iteratively search consisa parser learning the correct mapping between ut- tent logical forms using a parser and optimize the 2604 parser via the search result in turn. For example, Liang et al. (2013) and Berant et al. (2013) perform a beam search on a parser at each training step to search consistent logical forms, and they optimize the parser with an approximated MML that sums over consistent logical forms in the beam. Guu et al. (2017) propose a randomized beam search and a β-meritocratic update strategy to improve the searching of consistent logical forms. Instead of using MML, Liang et al. (2017) optimize a parser with the REINFORCE algorithm (Williams, 1992). They sample logical forms at each training step to compute an unbiased estimate of the gradient. Liang et al. (2018) leverage a memory buffer of consistent logical forms to reduce the variance of policy gradient estimate. Agarwal et al. (2019) introduce an auxiliary reward function to provide fine-grained feedback for dealing with spurious logical forms. Our L FM fr"
2021.findings-emnlp.222,2020.acl-main.398,0,0.277409,"rwal et al. (2019) introduce an auxiliary reward function to provide fine-grained feedback for dealing with spurious logical forms. Our L FM framework also falls into this dynamic category. Unlike the approaches introduced above that primarily leverage consistent logical forms for optimization, L FM fully utilizes the mistakes made by a parser during searching to address the cold-start and spuriousness problems. Hence, L FM can be considered orthogonal to prior dynamic approaches. There is another line of work that tackles WSP without logical forms (Neelakantan et al., 2017; Mou et al., 2017; Herzig et al., 2020). Neelakantan et al. (2017) propose a neural model that sequentially predicts symbolic operations over semistructured tables, and the model can be trained end-to-end with utterance-denotation pairs. Herzig et al. (2020) and Eisenschlos et al. (2020) pre-train a language model for table understanding. They show that the pre-trained model can be used to address WSP with a simple cell selection module and a set of differentiable aggregation operators. 2.2 Data Augmentation for Semantic Parsing Consistent Logical Forms ?1 (hop Nation (argmax Silver rows)) ?2 (hop Nation (argmax Rank rows)) ?3 (hop"
2021.findings-emnlp.222,P17-1089,0,0.0267896,"d rows)) ✓ ?4 (hop Nation (first rows)) ✕ ?5 (hop Nation (argmin Silver rows)) ✕ Synthesized Utterance-Logical Form Pairs from Mistakes ?ො4 Which nation is listed first? Consistent Text Generator ?4 (hop Nation (first rows)) ?ො5 Which nation has the least silver? ?5 (hop Nation (argmin Silver rows)) Figure 2: Visualization of a training step in L FM. The input utterance is “Who won the most silver medals?”. tic parser using data sampled from the SCFG. The parser is then finetuned via utterance-denotation pairs using MML. Similar ideas have also been adopted to address the Text-to-SQL problem (Iyer et al., 2017; Yu et al., 2018, 2021). Instead of using SCFG, Guo et al. (2018), Zhong et al. (2020a), and Wang et al. (2021) train a SQL-to-question neural model via utterance-logical form pairs. They synthesize more training data by randomly sampling SQL queries and generating corresponding questions with the model. One shortcoming of the data augmentation work above is that they need to carefully design logical form sampling procedures and pre-define the amount of data to synthesize. It has been found that over-extensive data augmentation will cause a deep-learning model to overfit, leading to even wors"
2021.findings-emnlp.222,P16-1002,0,0.0239717,"mance than that without data augmentation (Shorten and Khoshgoftaar, 2019). In L FM, the mistakes made by a parser serve as the source for data augmentation, and therefore, we do not need extra logical form sampling procedures. Also, we do not need to pre-define the amount of data to synthesize, because synthesis data are created at each training step. As we will show in Section 5.2, L FM is more effective than the other data augmentation techniques in WSP. Our work also closely relates to the area of data augmentation for semantic parsing, since L FM synthesizes utterance-logical form pairs. Jia and Liang (2016) induce a synchronous context-free gram3 Learning Framework mar (SCFG) (Chiang, 2005) from manually labeled utterance-logical form pairs. They randomly sam- In this section, we formally define the task of WSP ple new pairs from the SCFG and train a parser and describe L FM in detail. using both labeled and sampled data, leading to sig3.1 Preliminaries nificant improvements on several fully supervised semantic parsing tasks. Goldman et al. (2018) man- Task Formulation Given a training set of N exually induce an SCFG and pre-train a neural seman- amples {(xi , ωi , yi )}N i=1 , where xi is an ut"
2021.findings-emnlp.222,D17-1160,0,0.0978925,"ext generator cannot generate meaningful utterances for such invalid logical forms. Hence, to improve the utilization of mistakes, L FM tries to pinpoint the source of violations and automatically fixes them. For example, z1 is fixed by replacing “won” with a randomly generated number “2.0”. In addition, L FM attempts to enrich the diversity of mistake logical forms by randomly replacing a logical form’s entities with proper ones in its associated knowledge base. For example, the logical Metric Following prior WSP work (Liang et al., form zˆ2 in Table 1 is generated by (1) replacing the 2013; Krishnamurthy et al., 2017), we evaluate the column Gold in z2 with Silver which has the same performance of a semantic parser via Denotation data type with Gold; and (2) replacing the value Accuracy: a predicted logical form is considered “France” in z2 with another cell value (“Turkey”) in correct if it executes to the correct denotation. the Nation column. We perform this fixing and diversifying proce- 4.2 Neural Semantic Parser dures for mistake logical forms before synthesizing We develop a simple neural semantic parser for their utterances (Line 7 in Algorithm 1). experiments. Given an utterance and a table, the 2"
2021.findings-emnlp.222,P17-1003,0,0.171217,"pproaches iteratively search consisa parser learning the correct mapping between ut- tent logical forms using a parser and optimize the 2604 parser via the search result in turn. For example, Liang et al. (2013) and Berant et al. (2013) perform a beam search on a parser at each training step to search consistent logical forms, and they optimize the parser with an approximated MML that sums over consistent logical forms in the beam. Guu et al. (2017) propose a randomized beam search and a β-meritocratic update strategy to improve the searching of consistent logical forms. Instead of using MML, Liang et al. (2017) optimize a parser with the REINFORCE algorithm (Williams, 1992). They sample logical forms at each training step to compute an unbiased estimate of the gradient. Liang et al. (2018) leverage a memory buffer of consistent logical forms to reduce the variance of policy gradient estimate. Agarwal et al. (2019) introduce an auxiliary reward function to provide fine-grained feedback for dealing with spurious logical forms. Our L FM framework also falls into this dynamic category. Unlike the approaches introduced above that primarily leverage consistent logical forms for optimization, L FM fully ut"
2021.findings-emnlp.222,2021.acl-long.318,0,0.0550713,"Missing"
2021.findings-emnlp.222,2020.coling-main.466,1,0.796783,"Missing"
2021.findings-emnlp.222,D19-1391,0,0.0358335,"Missing"
2021.findings-emnlp.222,2021.naacl-main.220,0,0.025212,"Form Pairs from Mistakes ?ො4 Which nation is listed first? Consistent Text Generator ?4 (hop Nation (first rows)) ?ො5 Which nation has the least silver? ?5 (hop Nation (argmin Silver rows)) Figure 2: Visualization of a training step in L FM. The input utterance is “Who won the most silver medals?”. tic parser using data sampled from the SCFG. The parser is then finetuned via utterance-denotation pairs using MML. Similar ideas have also been adopted to address the Text-to-SQL problem (Iyer et al., 2017; Yu et al., 2018, 2021). Instead of using SCFG, Guo et al. (2018), Zhong et al. (2020a), and Wang et al. (2021) train a SQL-to-question neural model via utterance-logical form pairs. They synthesize more training data by randomly sampling SQL queries and generating corresponding questions with the model. One shortcoming of the data augmentation work above is that they need to carefully design logical form sampling procedures and pre-define the amount of data to synthesize. It has been found that over-extensive data augmentation will cause a deep-learning model to overfit, leading to even worse performance than that without data augmentation (Shorten and Khoshgoftaar, 2019). In L FM, the mistakes made b"
2021.findings-emnlp.222,P19-1176,0,0.0824079,"irs, which already performs on par with previous state-of-the-art approaches. 2 2.1 Related Work Weakly Supervised Semantic Parsing As mentioned in the previous section, prior approaches for WSP can be categorized into static and dynamic. Static approaches, such as Krishnamurthy et al. (2017), heuristically search consistent logical forms offline and train a parser with the MML objective. When there are too many consistent logical forms for an utterance-denotation pair, they only consider top K shortest logical forms (typically K ≤ 100) and perform a beam search to approximate the sum in MML. Wang et al. (2019a) introduce an alignment model to distinguish between spurious and correct logical forms. The L FM has two major advantages over existing dy- alignment model is jointly optimized with a parser via MML. Min et al. (2019) replace MML with a namic approaches. First, L FM does not need to discrete hard EM objective and observe improvepre-search consistent logical forms to warm start ments on WikiSQL and some reading comprehenthe training. Instead, it creates utterance-logical sion datasets. form pairs from mistakes on the fly to overcome the cold-start problem. Second, L FM can facilitate Dynamic"
2021.findings-emnlp.222,2020.emnlp-main.628,0,0.0501395,"Missing"
2021.findings-emnlp.222,P17-1041,0,0.0135723,"them with BERT (Devlin et al., 2019). Since a column may be composed of multiple tokens, the encoder obtains its representation by taking the average of its tokens’ representations (Yin et al., 2020). In addition, following prior WSP work (Krishnamurthy et al., 2017; Liang et al., 2018; Wang et al., 2019b), we add binary indicator features to specify (1) whether a token in the utterance appears in the table and (2) whether a column is mentioned in the utterance. These features are mapped to learnable embeddings and concatenated to the output of BERT. Decoder We employ a grammar-based decoder (Yin and Neubig, 2017) with LSTM cells. It interacts with three types of actions to generate a logical form, namely, A PPLY RULE, S EL C OLUMN, and S ELVALUE. A PPLY RULE selects a production rule from the query language’s context-free grammar (CFG) and applies it to the abstract syntax tree of a logical form. S EL C OLUMN employs a pointer network (Vinyals et al., 2015) to select a column from the table. S ELVALUE employs two pointer networks to select a span (beg token, end token) from the utterance. Interested readers can refer to (Yin and Neubig, 2017) for more details about the grammar-based decoder. Logical F"
2021.findings-emnlp.222,2020.acl-main.745,0,0.014869,"their utterances (Line 7 in Algorithm 1). experiments. Given an utterance and a table, the 2607 parser jointly encodes them via an input encoder and generates a logical form with a decoder. Input Encoder The goal of an input encoder is to obtain distributed representations for a given utterance and table. To achieve the goal, the encoder concatenates the utterance with all columns in the table and jointly encodes them with BERT (Devlin et al., 2019). Since a column may be composed of multiple tokens, the encoder obtains its representation by taking the average of its tokens’ representations (Yin et al., 2020). In addition, following prior WSP work (Krishnamurthy et al., 2017; Liang et al., 2018; Wang et al., 2019b), we add binary indicator features to specify (1) whether a token in the utterance appears in the table and (2) whether a column is mentioned in the utterance. These features are mapped to learnable embeddings and concatenated to the output of BERT. Decoder We employ a grammar-based decoder (Yin and Neubig, 2017) with LSTM cells. It interacts with three types of actions to generate a logical form, namely, A PPLY RULE, S EL C OLUMN, and S ELVALUE. A PPLY RULE selects a production rule fro"
2021.findings-emnlp.222,D18-1193,0,0.0359857,"Missing"
2021.findings-emnlp.222,D17-1125,0,0.01938,"ce: Who won the most silver medals? Denotation: Iran Logical Form Consistent Spurious (hop Nation (argmax Silver rows)) ✓ (hop Nation (argmax Rank rows)) ✓ ✓ (hop Nation (argmin Gold rows)) ✓ ✓ (hop Nation (first rows)) ✕ (hop Nation (argmin Silver rows)) ✕ Figure 1: An illustrative example of weakly supervised semantic parsing. utterance-denotation pairs is much cheaper, because it can be performed by non-experts. Hence, it is tempting to train a semantic parser via utterancedenotation pairs, framing a weakly supervised semantic parsing problem (WSP) (Clarke et al., 2010; Liang et al., 2013; Zhang et al., 2017). Training a parser from denotations rather than logical forms complicates training in two ways. First, training a parser requires exploring the huge space of logical forms to find those that execute to correct denotations, which we call “consistent” logical forms. This is a very difficult search prob1 Introduction lem due to the combinatorial nature of the search space. Figure 1 presents five logical forms for an Semantic parsing is the task of mapping a natuutterance-denotation pair, among which the first ral language utterance to a logical form that can three are consistent and the rest are"
2021.findings-emnlp.222,2020.emnlp-main.558,0,0.180801,"hesized Utterance-Logical Form Pairs from Mistakes ?ො4 Which nation is listed first? Consistent Text Generator ?4 (hop Nation (first rows)) ?ො5 Which nation has the least silver? ?5 (hop Nation (argmin Silver rows)) Figure 2: Visualization of a training step in L FM. The input utterance is “Who won the most silver medals?”. tic parser using data sampled from the SCFG. The parser is then finetuned via utterance-denotation pairs using MML. Similar ideas have also been adopted to address the Text-to-SQL problem (Iyer et al., 2017; Yu et al., 2018, 2021). Instead of using SCFG, Guo et al. (2018), Zhong et al. (2020a), and Wang et al. (2021) train a SQL-to-question neural model via utterance-logical form pairs. They synthesize more training data by randomly sampling SQL queries and generating corresponding questions with the model. One shortcoming of the data augmentation work above is that they need to carefully design logical form sampling procedures and pre-define the amount of data to synthesize. It has been found that over-extensive data augmentation will cause a deep-learning model to overfit, leading to even worse performance than that without data augmentation (Shorten and Khoshgoftaar, 2019). In"
2021.findings-emnlp.222,2020.acl-main.539,0,0.049757,"hesized Utterance-Logical Form Pairs from Mistakes ?ො4 Which nation is listed first? Consistent Text Generator ?4 (hop Nation (first rows)) ?ො5 Which nation has the least silver? ?5 (hop Nation (argmin Silver rows)) Figure 2: Visualization of a training step in L FM. The input utterance is “Who won the most silver medals?”. tic parser using data sampled from the SCFG. The parser is then finetuned via utterance-denotation pairs using MML. Similar ideas have also been adopted to address the Text-to-SQL problem (Iyer et al., 2017; Yu et al., 2018, 2021). Instead of using SCFG, Guo et al. (2018), Zhong et al. (2020a), and Wang et al. (2021) train a SQL-to-question neural model via utterance-logical form pairs. They synthesize more training data by randomly sampling SQL queries and generating corresponding questions with the model. One shortcoming of the data augmentation work above is that they need to carefully design logical form sampling procedures and pre-define the amount of data to synthesize. It has been found that over-extensive data augmentation will cause a deep-learning model to overfit, leading to even worse performance than that without data augmentation (Shorten and Khoshgoftaar, 2019). In"
2021.findings-emnlp.97,N19-1423,0,0.0111711,"twork method, and can identify important words that should be included in the summary of group discussion. Furthermore, many researchers have focused on improving the abstractive meeting summarization model. (Liu et al., 2019) used the pointer generation network, which can sense the topic transfer of conversation, integrates the external topic information to improve the quality of summary generation. In the work of HMNet, a hierarchical conference summary network is proposed, which is pre-trained with news datasets, and obtained good results in AMI and ICSI. Pre-trained Language Models. BERT (Devlin et al., 2019) introduces Masked Language Modelling and Next Sentence Prediction, which leads to the upsurge of pre-training research in NLP field. However, BERT does not perform well in the field of text generation due to the feature of autoencoding model. The pre-training task for text generation task is designed based on MASS (Song et al., 2019) and BART. In MASS, an input sequence with a masked span of tokens is mapped to a sequence consisting of the missing tokens, while BART is trained to reconstruct the original text from corrupted input with some masked tokens. Furthermore, Pegasus build GSG task fo"
2021.findings-emnlp.97,P15-1107,0,0.0284841,". Unfortunately, ∗ Corresponding authors: YuZhuo Fu, MengNan Qi RNNs lack global modeling capability and is difficult to deal with long-term dependency. To overcome this limitation, more and more researchers introduce convolution or transformer (Vaswani et al., 2017) model. These methods are easy to modify to capture more global information. However, recent studies indicate that they may not be sufficient to build long-term dependency models, which makes them significantly less effective in the context of long-term multi-human dialogue. Therefore, constructing hierarchical encoding structure (Li et al., 2015) to capture the content information of each speaker and the high-level semantic information hidden among utterances has become the mainstream method in the field of meeting summary. Different from news texts, utterances are often turned from different interlocutors, which leads to the topic drifts, and lower information density. These problems need to be overcome by introducing external high-level semantic information, such as conversation behavior, topic mining and so on. (Goo and Chen, 2018) proposed to use the dialogue act signals in a neural summarization model. (Li et al., 2019) introduce"
2021.findings-emnlp.97,P19-1210,0,0.0198894,"tructure (Li et al., 2015) to capture the content information of each speaker and the high-level semantic information hidden among utterances has become the mainstream method in the field of meeting summary. Different from news texts, utterances are often turned from different interlocutors, which leads to the topic drifts, and lower information density. These problems need to be overcome by introducing external high-level semantic information, such as conversation behavior, topic mining and so on. (Goo and Chen, 2018) proposed to use the dialogue act signals in a neural summarization model. (Li et al., 2019) introduced Visual Focus Of Attention (VFOA),which represents the common concerns of all conference participants in each time stamp, to keep the keep meeting summaries on topic. (Zhao et al., 2020) improved abstractive dialogue summarization with Graph Structures and Topic Words. These studies have proved that the introduction of external high-level semantic information has positive feedback on the results of meeting summary. Meanwhile, the use of carefully designed unsupervised pre-training tasks and large scale pretraining corpus has achieved great success in the field of document summary an"
2021.findings-emnlp.97,W04-1013,0,0.0310914,"4 Pretraining network. Gap Sentences Generation (GSG). This pretraining task is proposed in Pegasus for the first time. It is based on the assumption that the model can achieve better and faster fine-tuning performance when the pre-training target is very similar to the downstream task. The principle is to mask the whole key sentences from the document and concatenate the gap-sentences into a pseudosummary. In order to obtain the key sentences in the original text unsupervised, the researchers select top-m scored sentences according to importance. As a proxy for importance they compute ROUGE (Lin, 2004) between the sentence and the rest of the document. Different from Pegasus, we try the graph based sorting algorithms TextRank and Maximum Margin Relevance(MMR) to get the key sentences in the original text. We expect that our model can fully extract the semantic information of different levels in the hierarchical encoder-decoder structure through carefully designed pre-training tasks. The following three sections describe our tasks built on a hierarchical 1124 TextRank regards each sentence in the text as a node V. If two sentences are similar, it is considered that there is a non-directional"
2021.findings-emnlp.97,D19-1387,0,0.0180343,"r role tags. on addressing the reproducing and repeating probMetrics. We evaluated performance with the lem in general abstractive text summarization task. widely used ROUGE-1, ROUGE-2 and ROUGE- HASMR (Zheng et al., 2020) proposes a hierarSU4 metrics in automatic summarization. More chical neural encoder based on adaptive recurrent specifically, it focuses on measuring the number of network to learn the semantic representation of overlapping units such as n-gram between the gen- conference session based on adaptive session segerated summary and the reference summary. When mentation. BertSum (Liu and Lapata, 2019) is a matching the reference summary and the summary pretraining model with good performance in the to be evaluated, ROUGE-SU4 does not require that field of text generation. MM is a multimodal model, 1127 Model R-1 NoPretrain GSG(MMR) GSG(MMR)+dRA GSG(MMR)+dRA+Topic GSG(Rouge)+dRA+Topic GSG(TextRank)+dRA+Topic R-2 R-SU4 AMI 16.34 22.26 20.68 24.75 21.09 24.64 21.16 25.12 20.03 24.44 20.75 24.68 49.17 52.06 51.59 51.90 50.96 51.11 R-1 40.75 43.79 44.03 44.41 42.86 43.71 R-2 R-SU4 ICSI 10.02 18.47 11.44 19.72 11.73 19.47 11.86 19.95 11.28 19.88 11.46 19.62 Table 1: The ablation experiment about"
2021.findings-emnlp.97,D19-1184,0,0.058425,"raining task for text generation task is designed based on MASS (Song et al., 2019) and BART. In MASS, an input sequence with a masked span of tokens is mapped to a sequence consisting of the missing tokens, while BART is trained to reconstruct the original text from corrupted input with some masked tokens. Furthermore, Pegasus build GSG task for text summary scenario, it masks the key sentences in the original text and requires the model to generate those designated sentences at the decoder. In order to make the model fully learn the high-level semantic information hidden between dialogues, (Mehri and Eskenazi, 2019) proposed a transformer based hierarchical model and various unsupervised goals for the pre-training of the context semantics of dialogue discourse. DialogBERT (Gu et al., 2020) applied Next Utterance Generation, Masked Utter1122 ance Regression and Distributed Utterance Order Ranking tasks to capture discourse-level coherence among utterances. 3 Models We mainly follow the hierarchical meeting summarization network model structure and make some improvements on the utterance-level encoder, which its fusion blocks come from ERNIE (Zhang et al., 2019). The problem of meeting summarization can be"
2021.findings-emnlp.97,W04-3252,0,0.0308958,"cument summary and dialogue understanding to get a better result. Therefore, we propose a hierarchical transformer encoder-decoder network with auxiliary multi-task learning. We mainly follow the model structure of HMNet (Zhu et al., 2020) and construct new pretraining tasks on different levels of encoder. Our contributions are as follows: (1) In word-level encoder, we construct the GSG pre-training task proposed by Pegasus, and extract the key sentences for every utterance, and then generate them in the decoder. The difference is that we improve the meeting summary results by using TextRank (Mihalcea and Tarau, 2004) and MMR (Carbonell and Goldstein, 1998) algorithm to extract key sentences from the original text. (2) In utterance-level encoder, our model integrates role representations into the underlying layers of the semantic module based on the alignments between text and roles. Besides, for the better fusion of textual and role features, we design a new pre-training objective by randomly mask some of the role alignments in the input text and asking the model to recover the original role tags to complete the alignments. Unlike the existing pre-trained language representation models only utilizing loca"
2021.findings-emnlp.97,P17-1099,0,0.0443616,"e posed method with previous methods for the probPOS and ENT tags are added. Secondly, we also lem of abstractive meeting summarization. randomly stitch several different dialogues on the CoreRank (Shang et al., 2018) construct undicleaned data to simulate the change of conversation rected weighted graph and calculate the corerank topic in real conference scene. Finally, aiming at value of nodes, it is state-of-the-art extractive sumthe problem that the label quality of speakers in marization method. PGN method is the pointerpre-training datasets are uneven, we combine some generator network (See et al., 2017), which focuses of the similar role tags. on addressing the reproducing and repeating probMetrics. We evaluated performance with the lem in general abstractive text summarization task. widely used ROUGE-1, ROUGE-2 and ROUGE- HASMR (Zheng et al., 2020) proposes a hierarSU4 metrics in automatic summarization. More chical neural encoder based on adaptive recurrent specifically, it focuses on measuring the number of network to learn the semantic representation of overlapping units such as n-gram between the gen- conference session based on adaptive session segerated summary and the reference summa"
2021.findings-emnlp.97,P19-1139,0,0.0223075,"information hidden between dialogues, (Mehri and Eskenazi, 2019) proposed a transformer based hierarchical model and various unsupervised goals for the pre-training of the context semantics of dialogue discourse. DialogBERT (Gu et al., 2020) applied Next Utterance Generation, Masked Utter1122 ance Regression and Distributed Utterance Order Ranking tasks to capture discourse-level coherence among utterances. 3 Models We mainly follow the hierarchical meeting summarization network model structure and make some improvements on the utterance-level encoder, which its fusion blocks come from ERNIE (Zhang et al., 2019). The problem of meeting summarization can be formalized as follows. The input meeting transcripts X contain some of meeting participants and the corresponding speech content. Each meeting transcript consists of multiple utterances U, where each utterance belongs to a specific topic T. The input meeting transcripts X = {(u1 , r1 , t1 ), (u2 , r2 , t1 ), . . . , (um , pn , tl )}, where uj , 1 ≤ j ≤ m is an utterance, tj , 1 ≤ j ≤ l is a topic and rj , 1 ≤ j ≤ k is the role tag of participants. The golden summary Y written by human beings is a sequence of tokens. an utterance is made up with w1"
2021.findings-emnlp.97,2020.coling-main.39,0,0.0319133,"ing summary. Different from news texts, utterances are often turned from different interlocutors, which leads to the topic drifts, and lower information density. These problems need to be overcome by introducing external high-level semantic information, such as conversation behavior, topic mining and so on. (Goo and Chen, 2018) proposed to use the dialogue act signals in a neural summarization model. (Li et al., 2019) introduced Visual Focus Of Attention (VFOA),which represents the common concerns of all conference participants in each time stamp, to keep the keep meeting summaries on topic. (Zhao et al., 2020) improved abstractive dialogue summarization with Graph Structures and Topic Words. These studies have proved that the introduction of external high-level semantic information has positive feedback on the results of meeting summary. Meanwhile, the use of carefully designed unsupervised pre-training tasks and large scale pretraining corpus has achieved great success in the field of document summary and dialogue understanding. BART (Lewis et al., 2019) corrupted text with an arbitrary noising function and learned to reconstruct the original text. Pegasus (Zhang et al., 2020) masks the key senten"
2021.findings-emnlp.97,2021.naacl-main.474,0,0.0424432,"seen average conference record was 10189 words, 464 that GSG task is the most obvious to improve the laps, 6.2 speakers, and 534 words in the abstract. effect of the model, followed by the topic segmenWe roughly divide datasets into training, valid and tation task, and finally the dRA task. Many words test part according to the ratio of 8:1:1. in the human annotation of AMI and ICSI are obExcept using some news domain pre-training tained directly from the original text. GSG task data in hmnet, we also introduce dialog do- simulates human to extract key information from main datasets MediaSum (Zhu et al., 2021) and the original text of the meeting, so it has a signifiTV4Dialog (Leilan Zhang, 2019) as the pre- cant impact on the final summary generation. The training datasets. MediaSum is a large scale mod- addition of the dRA task did not change the results ular media interview dataset composed of 463.6k much. The reason may be that there are few particitexts and abstracts. The dataset is mainly from the pants in the meeting scene, and the communication interview records of NPR and CNN, with an av- order between speakers is repeated, so the model erage of 30 rounds per conversation, six to seven can"
2021.findings-emnlp.97,2020.findings-emnlp.19,0,0.0350601,"in the field for supervised fine tuning can get quite good results. Unlike the context of many people’s short conversations or texts in the form of news or papers, each participant’s speech contains not only a complete fragment of their own views, but also a common discussion and exchange of views with other speakers. We think that the meeting summary task can combine the document summary and dialogue understanding to get a better result. Therefore, we propose a hierarchical transformer encoder-decoder network with auxiliary multi-task learning. We mainly follow the model structure of HMNet (Zhu et al., 2020) and construct new pretraining tasks on different levels of encoder. Our contributions are as follows: (1) In word-level encoder, we construct the GSG pre-training task proposed by Pegasus, and extract the key sentences for every utterance, and then generate them in the decoder. The difference is that we improve the meeting summary results by using TextRank (Mihalcea and Tarau, 2004) and MMR (Carbonell and Goldstein, 1998) algorithm to extract key sentences from the original text. (2) In utterance-level encoder, our model integrates role representations into the underlying layers of the semant"
2021.starsem-1.30,D17-1215,0,0.0672231,"Missing"
2021.starsem-1.30,2020.acl-main.197,0,0.0289178,"Missing"
2021.starsem-1.30,D17-1082,0,0.160345,"2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multiple-choice RC (Lai et al., 2017). To apply adversarial training on MRC tasks, we notice that there are several salient characteristics of MRC compared to other tasks such as image classification: (1) The inputs are discrete. Unlike pixels, which can take continuous values, words are discrete tokens. (2) The tokens in the input sequences are not independent. A word may occur in an input sequence several times. After the embedding layer, these occurrences are represented by the word vectors with the same value and hold the same semantic meaning (although the word may be polysemous). (3) The roles of passages and questions are"
2021.starsem-1.30,2021.ccl-1.108,0,0.0274086,"Missing"
2021.starsem-1.30,D18-1307,0,0.0170897,". Introduction Neural networks have achieved superior performance on many tasks, but they are vulnerable to adversarial examples (Szegedy et al., 2014) – examples that have been mixed with certain perturbations. Adversarial training (AT) (Goodfellow et al., 2015) uses both clean and adversarial examples to improve the robustness of the model for image classification. In the field of NLP, Miyato et al. (2017) have applied adversarial training on text classification tasks and improved the model performance. From then on, many AT methods has been proposed (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multip"
2021.starsem-1.30,2020.repl4nlp-1.8,0,0.0185849,"ance on many tasks, but they are vulnerable to adversarial examples (Szegedy et al., 2014) – examples that have been mixed with certain perturbations. Adversarial training (AT) (Goodfellow et al., 2015) uses both clean and adversarial examples to improve the robustness of the model for image classification. In the field of NLP, Miyato et al. (2017) have applied adversarial training on text classification tasks and improved the model performance. From then on, many AT methods has been proposed (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multiple-choice RC (Lai et al., 2017). To apply adversarial traini"
2021.starsem-1.30,P18-2124,0,0.0345611,"Missing"
2021.starsem-1.30,D16-1264,0,0.333566,"et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multiple-choice RC (Lai et al., 2017). To apply adversarial training on MRC tasks, we notice that there are several salient characteristics of MRC compared to other tasks such as image classification: (1) The inputs are discrete. Unlike pixels, which can take continuous values, words are discrete tokens. (2) The tokens in the input sequences are not independent. A word may occur in an input sequence several times. After the embedding layer, these occurrences are represented by the word vectors with the same value and hold the same semantic meaning (although the w"
2021.starsem-1.30,2020.emnlp-main.583,1,0.696811,"amUpdate({θ, E}, gK ) end Experiments Setup Datasets. We perform experiments on several English MRC tasks, including span-based extractive MRC tasks – SQuAD 1.1 (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), HotpotQA (Yang et al., 2018), and multiple-choice MRC task RACE (Lai et al., 2017). We also test model robustness on the adversarial datasets AddSent andAddOneSent (Jia and Liang, 2017). Model Settings. We build the MRC model with RoBERTa (Liu et al., 2019), following the standard model structure for SQuAD and RACE (Devlin et al., 2018). For HotpotQA, we follow the model in Shao et al. (2020). It uses RoBERTa as the encoder followed by a multi-task prediction layer. We denote the passage as P and the question as Q. To construct the inputs, for span-based extractive RC, we concatenate each P and Q with modeldependent special tokens; for multiple-choice RC with m options for each example, we append each option to the concatenation of P and Q, and construct m input sequences from each example. When applying AT or PQAT, we only perturb 310 Model SQuAD 1.1 EM F1 SQuAD 2.0 EM F1 HotpotQA joint EM joint F1 RACE Acc BASE setting RoBERTa PQAT 84.72 85.87 91.54 92.33 79.77 81.66 83.18 84.79"
2021.starsem-1.30,D17-1187,0,0.0306079,"bed differently depending on their roles. Introduction Neural networks have achieved superior performance on many tasks, but they are vulnerable to adversarial examples (Szegedy et al., 2014) – examples that have been mixed with certain perturbations. Adversarial training (AT) (Goodfellow et al., 2015) uses both clean and adversarial examples to improve the robustness of the model for image classification. In the field of NLP, Miyato et al. (2017) have applied adversarial training on text classification tasks and improved the model performance. From then on, many AT methods has been proposed (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et"
2021.starsem-1.30,D18-1259,0,0.0870263,", 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang et al., 2018) and multiple-choice RC (Lai et al., 2017). To apply adversarial training on MRC tasks, we notice that there are several salient characteristics of MRC compared to other tasks such as image classification: (1) The inputs are discrete. Unlike pixels, which can take continuous values, words are discrete tokens. (2) The tokens in the input sequences are not independent. A word may occur in an input sequence several times. After the embedding layer, these occurrences are represented by the word vectors with the same value and hold the same semantic meaning (although the word may be polysemous). (3"
2021.starsem-1.30,N18-1089,0,0.0193435,"epending on their roles. Introduction Neural networks have achieved superior performance on many tasks, but they are vulnerable to adversarial examples (Szegedy et al., 2014) – examples that have been mixed with certain perturbations. Adversarial training (AT) (Goodfellow et al., 2015) uses both clean and adversarial examples to improve the robustness of the model for image classification. In the field of NLP, Miyato et al. (2017) have applied adversarial training on text classification tasks and improved the model performance. From then on, many AT methods has been proposed (Wu et al., 2017; Yasunaga et al., 2018; Bekoulis et al., 2018; Zhu et al., 2020; Jiang et al., 2019; Pereira et al., 2020; Liu et al., 2020). They mostly adopt a general AT strategy, but focus less on the adaptation of AT to NLP tasks. To explore this adaptation, in this work, we aim to apply adversarial training on machine reading comprehension (MRC) tasks, which exhibit complex NLP characteristics. The objective of MRC is to let a machine read the given passages and ask it to answer the related questions. There are several types of MRC tasks. In this work we focus on span-based extractive RC (Rajpurkar et al., 2016, 2018; Yang e"
C04-1189,M98-1007,0,0.128606,"Missing"
C04-1189,N03-1004,0,0.0326839,"Missing"
C04-1189,W00-0303,0,0.115113,"Missing"
C04-1189,N04-4014,1,0.094326,"Missing"
C04-1189,C96-2157,1,0.726879,"cues and the expected entity types, which are domain adaptable. Domain adaptation is desirable for obtaining more focused dialogue, but it is not necessary for HITIQA to work. We used both setups under different conditions: the generic frames were used with TREC document collection to measure impact of IR precision on QA accuracy (Small et al., 2004). The domain-adapted frames were used for sessions with intelligence analysts working with the WMD Domain (see below). Currently, the adaptation process includes manual tuning followed by corpus bootstrapping using an unsupervised learning method (Strzalkowski & Wang, 1996). We generally rely on BBN’s Identifinder for extraction of basic entities, and use bootstrapping to define additional entity types as well as to assign roles to attributes. The version of HITIQA reported here and used by analysts during the evaluation has been adapted to the Weapons of Mass Destruction NonProliferation domain (WMD domain, henceforth). Figure 3 contains an example passage from this data set. In the WMD domain, the typed frames were mapped onto WMDTransfer 3-role frame, and two 2-role frames WMDTreaty and WMDDevelop. Adapting the frames to the WMD domain required very minimal m"
C08-1123,J97-1002,0,0.152959,"Missing"
C08-1123,W02-0221,0,0.047744,"Missing"
C08-1123,P98-2188,0,0.0750752,"Missing"
C08-1123,P04-1088,0,0.19203,"Missing"
C08-1123,W04-2319,0,0.331187,"Missing"
C08-1123,J00-3003,0,0.765113,"Missing"
C08-1123,J93-3003,0,\N,Missing
C08-1123,J86-3001,0,\N,Missing
C10-1019,W09-1207,1,0.940573,"at the word at position w has the sense s. For semantic role labeling, the predicate role(p, a, r) is defined as mentioned in above. Different from Meza-Ruiz and Riedel (2009), which only used sense number as word sense representation, we use a triple (lemma, part-ofspeech, sense num) to represent the word sense s. For example, (hit, v, 01) denotes that the verb “hit” has sense number 01. Obviously, our representation can distinguish different word senses which have the identical sense number. In addition, we use one argument classification stage with predicate role to label semantic roles as Che et al. (2009). Similarly, no argument identification stage is used in our model. The approach can improve the recall of the system. In addition to the hidden predicates, we define observable predicates to represent the information available in the corpus. Table 1 presents these predicates. 4.1 Local Formula A local formula means that its groundings relate any number of observed ground atoms to exactly one hidden ground atom. For example lemma(p, +l1 )∧lemma(a, +l2 ) ⇒ role(p, a, +r) 163 Predicates word(i, w) pos(i, t) lemma(i, l) chdpos(i, t) chddep(i, d) f irstLemma(i, l) lastLemma(i, l) posF rame(i, f r)"
C10-1019,N10-1030,1,0.878373,"9) have joined semantic role labeling and predicate senses disambiguation with Markov logic. The above idea, that the predicate senses and the semantic role labeling can help each other, may be inspired by Hajiˇc et al. (2009), Surdeanu et al. (2008), and Dang and Palmer (2005). They have shown that semantic role features are helpful to disambiguate verb senses and vice versa. Besides predicate senses, Dahlmeier et al. (2009) proposed a joint model to maximize probability of the preposition senses and the semantic role of prepositional phrases. 162 Except for predicate and preposition senses, Che et al. (2010) explored all word senses for semantic role labeling. They showed that all word senses can improve the semantic role labeling performance significantly. However, the golden word senses were used in their experiments. The results are still unknown when an automatic word sense disambiguation system is used. In this paper, we not only use all word senses disambiguated by an automatic system, but also make the semantic role labeling results to help word sense disambiguation synchronously with a joint model. 3 Markov Logic Markov logic can be understood as a knowledge representation with a weight a"
C10-1019,P09-1058,0,0.0396312,"he verb sense disambiguation (Dang and Palmer, 2005). More people used predicate senses in semantic role labeling (Hajiˇc et al., 2009; Surdeanu et al., 2008). However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169,"
C10-1019,N09-1018,0,0.294815,"and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riedel, 2009). Markov logic combines the first order logic and Markov networks, to develop a joint probability model over all related rules. Global constraints (introduced by Punyakanok et al. (2008)) among semantic roles can be easily added into Markov logic. And the more important, the jointly modeling can be realized using Markov logic naturally. Besides predicates and prepositions, other word senses are also important information for recognizing semantic roles. For example, if we know “cat” is an “agent” of the predicate “hit” in a sentence, we can guess that “dog” can also be an “agent” of “hit”, thou"
C10-1019,D08-1068,0,0.0380997,"task (semantic role labeling), their methods have been used to jointly learn for two tasks (semantic role labeling and syntactic parsing). However, it is easy for the re-ranking model to loss the optimal result, if it is not included in the top n results. In addition, the integer linear programming model can only use hard constraints. A lot of engineering work is also required in both models. Recently, Markov logic (Domingos and Lowd, 2009) became a hot framework for joint model. It has been successfully used in temporal relations recognition (Yoshikawa et al., 2009), co-reference resolution (Poon and Domingos, 2008), etc. It is very easy to do joint modeling using Markov logic. The only work is to define relevant formulas. Meza-Ruiz and Riedel (2009) have joined semantic role labeling and predicate senses disambiguation with Markov logic. The above idea, that the predicate senses and the semantic role labeling can help each other, may be inspired by Hajiˇc et al. (2009), Surdeanu et al. (2008), and Dang and Palmer (2005). They have shown that semantic role features are helpful to disambiguate verb senses and vice versa. Besides predicate senses, Dahlmeier et al. (2009) proposed a joint model to maximize"
C10-1019,D09-1047,0,0.0844701,"ural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riedel, 2009). Markov logic combines the first order logic and Markov networks, to develop a joint probability model over all related"
C10-1019,J08-2005,0,0.438317,"robability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riedel, 2009). Markov logic combines the first order logic and Markov networks, to develop a joint probability model over all related rules. Global constraints (introduced by Punyakanok et al. (2008)) among semantic roles can be easily added into Markov logic. And the more important, the jointly modeling can be realized using Markov logic naturally. Besides predicates and prepositions, other word senses are also important information for recognizing semantic roles. For example, if we know “cat” is an “agent” of the predicate “hit” in a sentence, we can guess that “dog” can also be an “agent” of “hit”, though it does not appear in the training data. Similarly, the semantic role information can also help to disambiguate word senses. In addition, the predicate sense and the argument sense ca"
C10-1019,P05-1006,0,0.376097,"nd the sense label is “hit.01”. The argument headed by the token “cat” at position 1 with sense “feline mammal” (cat.01) is referring to the player (A0), and the argument headed by the token “ball” at position 5 with sense “round object that is hit in games” (ball.01) is referring to the game object (A1) being hit. Normally, semantic role labeling and word sense disambiguation are regarded as two independent tasks, i.e., the word sense information is rarely used in a semantic role labeling system and vice versa. A few researchers have used semantic roles to help the verb sense disambiguation (Dang and Palmer, 2005). More people used predicate senses in semantic role labeling (Hajiˇc et al., 2009; Surdeanu et al., 2008). However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jian"
C10-1019,W08-2121,0,0.0926125,"Missing"
C10-1019,N09-1037,0,0.291477,"nguage processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riedel, 2009). Markov logic combines the first order logic and Markov networks,"
C10-1019,P08-1036,0,0.0282814,"rror propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) was introduced to join semantic role labeling and predicate senses (Meza-Ruiz and Riede"
C10-1019,P08-1043,0,0.0380625,"possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a novel statistical relational learning framework, Markov logic (Domingos and Lowd, 2009) w"
C10-1019,P09-1055,0,0.0609524,"c et al., 2009; Surdeanu et al., 2008). However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 tic role of preposition phrases and the preposition sense. In order to do better joint learning, a nov"
C10-1019,J08-2002,0,0.0296886,"sent a Markov logic model which can easily express useful global constraints and jointly disambiguate all word senses and label semantic roles. Experiments on the OntoNotes 3.0 corpus show that (1) the automatic all word sense disambiguation and semantic role labeling tasks can help each other when using pipeline approaches, and more important, (2) the joint approach using Markov logic leads to higher accuracy for word sense disambiguation and performance (F1 ) for semantic role labeling than pipeline approaches. 2 Related Work Joint models were often used in semantic role labeling community. Toutanova et al. (2008) and Punyakanok et al. (2008) presented a re-ranking model and an integer linear programming model respectively to jointly learn a global optimal semantic roles assignment. Besides jointly learning semantic role assignment of different constituents for one task (semantic role labeling), their methods have been used to jointly learn for two tasks (semantic role labeling and syntactic parsing). However, it is easy for the re-ranking model to loss the optimal result, if it is not included in the top n results. In addition, the integer linear programming model can only use hard constraints. A lot"
C10-1019,P09-1046,0,0.0336476,"role assignment of different constituents for one task (semantic role labeling), their methods have been used to jointly learn for two tasks (semantic role labeling and syntactic parsing). However, it is easy for the re-ranking model to loss the optimal result, if it is not included in the top n results. In addition, the integer linear programming model can only use hard constraints. A lot of engineering work is also required in both models. Recently, Markov logic (Domingos and Lowd, 2009) became a hot framework for joint model. It has been successfully used in temporal relations recognition (Yoshikawa et al., 2009), co-reference resolution (Poon and Domingos, 2008), etc. It is very easy to do joint modeling using Markov logic. The only work is to define relevant formulas. Meza-Ruiz and Riedel (2009) have joined semantic role labeling and predicate senses disambiguation with Markov logic. The above idea, that the predicate senses and the semantic role labeling can help each other, may be inspired by Hajiˇc et al. (2009), Surdeanu et al. (2008), and Dang and Palmer (2005). They have shown that semantic role features are helpful to disambiguate verb senses and vice versa. Besides predicate senses, Dahlmeie"
C10-1019,N06-2015,0,0.0544942,"Missing"
C10-1019,P08-1101,0,0.0511629,"ion (Dang and Palmer, 2005). More people used predicate senses in semantic role labeling (Hajiˇc et al., 2009; Surdeanu et al., 2008). However, both of the pipeline methods ignore possible dependencies between the word senses and semantic roles, and can result in the error propagation problem. The same problem also appears in other natural language processing tasks. In order to make different natural language processing tasks be able to help each other, jointly modeling methods become popular recently, such as joint Chinese word segmentation and part-ofspeech tagging (Kruengkrai et al., 2009; Zhang and Clark, 2008; Jiang et al., 2008), joint lemmatization and part-of-speech prediction (Toutanova and Cherry, 2009), joint morphological segmentation and syntactic parsing (Goldberg and Tsarfaty, 2008), joint text and aspect ratings for sentiment summarization (Titov and McDonald, 2008), and joint parsing and named entity recognition (Finkel and Manning, 2009). For semantic role labeling, Dahlmeier et al. (2009) proposed a method to maximize the joint probability of the seman161 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 161–169, Beijing, August 2010 t"
C10-1019,P08-1102,0,0.0863211,"Missing"
C10-1019,D08-1105,0,0.0586961,"Missing"
C10-1148,H05-1120,0,0.0317472,"rases. If a query q hits titles t1 and t2 , then t1 and t2 are likely to be paraphrases. 1. FOR any q ∈ Q and t ∈ T 2. IF q hits t 3. IF IsP araphrase(q, t) 4. Add ⟨q, t⟩ to Pqt 5. END IF 6. END IF 7. END FOR Table 1: Hypotheses for extracting paraphrases. expansion terms for new queries. Note that the expansion terms are merely related terms of the queries, not necessarily paraphrases. There are other studies that use query logs for constructing ontologies (Sekine and Suzuki, 2007), learning named entities (Pas¸ca, 2007), building user profiles (Richardson, 2008), correcting spelling errors (Ahmad and Kondrak, 2005), and so forth. 3 8. FOR any q1 , q2 ∈ Q and t ∈ T 9. IF ⟨q1 , t⟩ ∈ Pqt and ⟨q2 , t⟩ ∈ Pqt 10. IF IsP araphrase(q1 , q2 ) 11. Add ⟨q1 , q2 ⟩ to Pqq 12. END IF 13. END IF 14. END FOR The Proposed Method 15. FOR any t1 , t2 ∈ T and q ∈ Q 16. IF ⟨q, t1 ⟩ ∈ Pqt and ⟨q, t2 ⟩ ∈ Pqt 17. IF IsP araphrase(t1 , t2 ) 18. Add ⟨t1 , t2 ⟩ to Ptt 19. END IF 20. END IF 21. END FOR 3.1 Basic Idea Nowadays, more and more users tend to search long queries with search engines. Many users even directly search questions to get exact answers. By analyzing our query log that records rich information including user qu"
C10-1148,P05-1074,0,0.0613276,"and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypot"
C10-1148,N03-1003,0,0.0965788,"ection 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bh"
C10-1148,P01-1008,0,0.099256,"10 paraphrase Q-Q extraction paraphrase Q-T extraction query title both query and title paraphrase T-T extraction paraphrase relation Figure 1: Illustration of the proposed method. sults. Section 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained"
C10-1148,P08-1077,0,0.181494,"03; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur in similar contexts, their meanings tend to be similar. The disadvantage of these methods is that the underlying assumption does not always hold. Phrases with opposite meanings can also occur in similar contexts, such as “X solves Y” and “X worsens Y” (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (variables) at both ends. 2.2 Query Log Mining in IR Query logs"
C10-1148,N06-1003,0,0.119041,"Missing"
C10-1148,D08-1021,0,0.0863245,"on retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two"
C10-1148,C04-1051,0,0.0764975,"aper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran"
C10-1148,N06-2009,0,0.299558,"Missing"
C10-1148,W03-1608,0,0.0259217,"paraphrase Q-T extraction query title both query and title paraphrase T-T extraction paraphrase relation Figure 1: Illustration of the proposed method. sults. Section 5 concludes the paper and discusses future directions. 2 Related Work In this section, we briefly review previous studies on paraphrase extraction and query log mining in information retrieval (IR). 2.1 Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and d"
C10-1148,I05-1011,0,0.147229,"Missing"
C10-1148,P02-1006,0,0.426591,"ual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur in similar contexts, their meanings tend to be similar. The disadvantage of these methods is that the underlying assumption does not always hold. Phrases with opposite meanings can also occur in similar contexts, such as “X solves Y” and “X worsens Y” (Lin and Pantel, 2001). In addition, the extracted paraphrases are generally short fragments with two slots (variables) at both ends. 2.2 Query Log Mining in IR Query logs are widely used in the IR community, especially for mining si"
C10-1148,P08-1089,1,0.86364,"Paraphrase Extraction A variety of data resources have been exploited for paraphrase extraction. For example, some researchers extract paraphrases from multiple translations of the same foreign novel (Barzilay and McKeown, 2001; Ibrahim et al., 2003), while some others make use of comparable news articles that report on the same event within a small time interval (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). Besides the monolingual corpora, bilingual parallel corpora have also been used for extracting paraphrases (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Zhao et al., 2008). Their basic assumption is that phrases that align with the same foreign phrase may have the same meaning. The above methods have achieved promising results. However, their performances are usually constrained due to the scale and domain limitation. As an alternative, researchers have tried to acquire paraphrases from large-scale web corpora (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008) or directly based on web mining (Ravichandran and Hovy, 2002). These methods are guided by an extended version of distributional hypothesis, namely, if two phrases often occur"
C10-1148,N06-1058,0,0.111969,"Missing"
C10-1148,P07-1059,0,\N,Missing
C10-1149,P05-1074,0,0.367449,"provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction. Pivot approach can also be used in paraphrase generation. It generates paraphrases by translating sentences from a source language to one (singlepivot) or"
C10-1149,W03-1601,0,0.180993,"Missing"
C10-1149,N03-1003,0,0.245626,"del so as to generate varied paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the"
C10-1149,W09-2503,0,0.114168,"hrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction. Pivot approach can also be used in paraphrase generation. It generates paraphrases by translating sentences from a source language to one (singlepivot) or more (multi-pivot) pivot languages and then translating them back to the source language. Duboue et al. (2006) first proposed the multipivot approach for paraphrase generation, which was specially designed for question expansion in QA. In addition, Max (2009) presented a singlepivot approach for generating sub-sentential paraphrases. A clear difference between our method and the above works is that we propose selectionbased and decoding-based techniques to generate high-quality paraphrases using the candidates yielded from the pivot approach. 3 Multi-pivot Approach for Acquiring Candidate Paraphrases A single-pivot PG approach paraphrases a sentence S by translating it into a pivot language P L with a MT engine M T1 and then translating it back into the source language with M T2 . In this paper, a single-pivot PG system is represented as a triple"
C10-1149,P00-1056,0,0.0233439,"paraphrase from the 54 candidates for each source sentence S. 4.2 Decoding-based Technique The selection-based technique introduced above has an inherent limitation that it can only select a paraphrase from the candidates. That is to say, it can never produce a perfect paraphrase if all the candidates have some tiny flaws. To solve this problem, we propose the decoding-based technique, which trains a MT model using the candidate paraphrases of each source sentence S and generates a new paraphrase T for S with a MT decoder. In this work, we implement the decoding-based technique using Giza++ (Och and Ney, 2000) and Moses (Hoang and Koehn, 2008), both of which are commonly used SMT tools. For a sentence S, we first construct a set of parallel sentences by pairing S with each of its candidate paraphrases: {(S,T1 ),(S,T2 ),...,(S,TN )} (N = 54). We then run word alignment on the set using Giza++ and extract aligned phrase pairs as described in (Koehn, 2004). Here we only keep the phrase pairs that are aligned ≥3 times on the set, so as to filter errors brought by the noisy sentence pairs. The extracted phrase pairs are stored in a phrase table. Table 4 shows some extracted phrase pairs. Note that Giza+"
C10-1149,N03-1024,0,0.0905158,"ried paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have pro"
C10-1149,H93-1040,0,0.0370119,"Missing"
C10-1149,N06-1003,0,0.0513578,"oth the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases. Moreover, these two techniques are complementary. (3) The proposed method outperforms a state-of-the-art paraphrase generation approach. 1 Introduction This paper addresses the problem of paraphrase generation (PG), which seeks to generate paraphrases for sentences. PG is important in many natural language processing (NLP) applications. For example, in machine translation (MT), a sentence can be paraphrased so as to make it more translatable (Zhang and Yamamoto, 2002; Callison-Burch et al., 2006). In question answering (QA), a question can be paraphrased to improve the coverage of answer extraction (Duboue and Chu-Carroll, 2006; Riezler et al., 2007). In natural language generation (NLG), paraphrasing can help to increase the expressive power of the NLG systems (Iordanskaja et al., 1991). In this paper, we propose a novel PG method. For an English sentence S, the method first acquires a set of candidate paraphrases with a multipivot approach, which uses MT engines to automatically translate S into multiple pivot languages and then translate them back into English. Furthermore, the met"
C10-1149,N06-2009,0,0.132255,"r, these two techniques are complementary. (3) The proposed method outperforms a state-of-the-art paraphrase generation approach. 1 Introduction This paper addresses the problem of paraphrase generation (PG), which seeks to generate paraphrases for sentences. PG is important in many natural language processing (NLP) applications. For example, in machine translation (MT), a sentence can be paraphrased so as to make it more translatable (Zhang and Yamamoto, 2002; Callison-Burch et al., 2006). In question answering (QA), a question can be paraphrased to improve the coverage of answer extraction (Duboue and Chu-Carroll, 2006; Riezler et al., 2007). In natural language generation (NLG), paraphrasing can help to increase the expressive power of the NLG systems (Iordanskaja et al., 1991). In this paper, we propose a novel PG method. For an English sentence S, the method first acquires a set of candidate paraphrases with a multipivot approach, which uses MT engines to automatically translate S into multiple pivot languages and then translate them back into English. Furthermore, the method employs two kinds of techniques to produce a best paraphrase T for S using the candidates, i.e., the selection-based and decoding-"
C10-1149,W08-0510,0,0.015504,"ates for each source sentence S. 4.2 Decoding-based Technique The selection-based technique introduced above has an inherent limitation that it can only select a paraphrase from the candidates. That is to say, it can never produce a perfect paraphrase if all the candidates have some tiny flaws. To solve this problem, we propose the decoding-based technique, which trains a MT model using the candidate paraphrases of each source sentence S and generates a new paraphrase T for S with a MT decoder. In this work, we implement the decoding-based technique using Giza++ (Och and Ney, 2000) and Moses (Hoang and Koehn, 2008), both of which are commonly used SMT tools. For a sentence S, we first construct a set of parallel sentences by pairing S with each of its candidate paraphrases: {(S,T1 ),(S,T2 ),...,(S,TN )} (N = 54). We then run word alignment on the set using Giza++ and extract aligned phrase pairs as described in (Koehn, 2004). Here we only keep the phrase pairs that are aligned ≥3 times on the set, so as to filter errors brought by the noisy sentence pairs. The extracted phrase pairs are stored in a phrase table. Table 4 shows some extracted phrase pairs. Note that Giza++ is sensitive to the data size. H"
C10-1149,N06-1058,0,0.0601524,"MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction. Pivot approac"
C10-1149,koen-2004-pharaoh,0,0.102502,"ose the decoding-based technique, which trains a MT model using the candidate paraphrases of each source sentence S and generates a new paraphrase T for S with a MT decoder. In this work, we implement the decoding-based technique using Giza++ (Och and Ney, 2000) and Moses (Hoang and Koehn, 2008), both of which are commonly used SMT tools. For a sentence S, we first construct a set of parallel sentences by pairing S with each of its candidate paraphrases: {(S,T1 ),(S,T2 ),...,(S,TN )} (N = 54). We then run word alignment on the set using Giza++ and extract aligned phrase pairs as described in (Koehn, 2004). Here we only keep the phrase pairs that are aligned ≥3 times on the set, so as to filter errors brought by the noisy sentence pairs. The extracted phrase pairs are stored in a phrase table. Table 4 shows some extracted phrase pairs. Note that Giza++ is sensitive to the data size. Hence it is interesting to examine if the alignment can be improved by augmenting the parallel sentence pairs. To this end, we have tried augmenting the parallel set for each sentence S by pairing any 2 two candidate paraphrases. In this manner, CN sentence pairs are augmented for each S. We con2 ) sentence duct wor"
C10-1149,P02-1040,0,0.105062,"Missing"
C10-1149,I05-5010,0,0.113084,"paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005). 2.2 Pivot Approach for Paraphrasing Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction. Pivot approach can also be used in paraphrase generation. It generates paraphrases b"
C10-1149,W04-3219,0,0.877056,"based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al., 2009). 2 2.1 Related Work Methods for Paraphrase Generation MT-based method is the mainstream method on PG. It regards PG as a monolingual machine translation problem, i.e., “translating” a sentence S into another sentence T in the same language. 1326 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326–1334, Beijing, August 2010 Quirk et al. (2004) first presented the MT-based method. They trained a statistical MT (SMT) model on a monolingual parallel corpus extracted from comparable news articles and applied the model to generate paraphrases. Their work shows that SMT techniques can be extended to PG. However, its usefulness is limited by the scarcity of monolingual parallel data. To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Th"
C10-1149,C02-1056,0,0.127176,"ndidate paraphrases. (2) Both the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases. Moreover, these two techniques are complementary. (3) The proposed method outperforms a state-of-the-art paraphrase generation approach. 1 Introduction This paper addresses the problem of paraphrase generation (PG), which seeks to generate paraphrases for sentences. PG is important in many natural language processing (NLP) applications. For example, in machine translation (MT), a sentence can be paraphrased so as to make it more translatable (Zhang and Yamamoto, 2002; Callison-Burch et al., 2006). In question answering (QA), a question can be paraphrased to improve the coverage of answer extraction (Duboue and Chu-Carroll, 2006; Riezler et al., 2007). In natural language generation (NLG), paraphrasing can help to increase the expressive power of the NLG systems (Iordanskaja et al., 1991). In this paper, we propose a novel PG method. For an English sentence S, the method first acquires a set of candidate paraphrases with a multipivot approach, which uses MT engines to automatically translate S into multiple pivot languages and then translate them back into"
C10-1149,P09-1094,1,0.816899,"extracted from comparable news articles and applied the model to generate paraphrases. Their work shows that SMT techniques can be extended to PG. However, its usefulness is limited by the scarcity of monolingual parallel data. To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Their results show that bilingual parallel corpora are the most useful among the exploited resources. Zhao et al. (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming. In addition to the MT-based method, researchers have also i"
C10-1149,P08-1116,1,0.903826,"ng” a sentence S into another sentence T in the same language. 1326 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326–1334, Beijing, August 2010 Quirk et al. (2004) first presented the MT-based method. They trained a statistical MT (SMT) model on a monolingual parallel corpus extracted from comparable news articles and applied the model to generate paraphrases. Their work shows that SMT techniques can be extended to PG. However, its usefulness is limited by the scarcity of monolingual parallel data. To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Their results show that bilingual parallel corpora are the most useful among the exploited resources. Zhao et al. (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such a"
C10-1149,P08-1089,1,0.875364,"ng” a sentence S into another sentence T in the same language. 1326 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1326–1334, Beijing, August 2010 Quirk et al. (2004) first presented the MT-based method. They trained a statistical MT (SMT) model on a monolingual parallel corpus extracted from comparable news articles and applied the model to generate paraphrases. Their work shows that SMT techniques can be extended to PG. However, its usefulness is limited by the scarcity of monolingual parallel data. To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Their results show that bilingual parallel corpora are the most useful among the exploited resources. Zhao et al. (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such a"
C10-1149,1993.mtsummit-1.24,0,\N,Missing
C10-1149,P07-1059,0,\N,Missing
C10-1149,W07-0718,0,\N,Missing
C10-1149,N03-1017,0,\N,Missing
C10-3004,W09-1207,1,0.237403,"nnotated with lexical tags, including word segmentation, part-of-speech tagging, and named entity recognition tags2 . 2.2 NER WSD Parser SRL 3 Speed 185KB/s 56.3KB/s 14.4KB/s 7.2KB/s 0.2KB/s 1.3KB/s Table 1: The performance and speed for each module. the dependency syntactic parsing subtask in the CoNLL-2009 Syntactic and Semantic Dependencies in Multiple Languages Shared Task (Hajiˇc et al., 2009). 6. Semantic Role Labeling (SRL): SRL is to identify the relations between predicates in a sentence and their associated arguments. The module is based on syntactic parser. A maximum entropy model (Che et al., 2009) is adopted here which achieved the ﬁrst place in the joint task of syntactic and semantic dependencies of the CoNLL2009 Shared Task. Table 1 shows the performance and speed of each module in detail. The performances are obtained with n-fold cross-validation method. The speed is gotten on a machine with Xeon 2.0GHz CPU and 4G Memory. At present, LTP processes these modules with a cascaded mechanism, i.e., some higher-level processing modules depend on other lower-level modules. For example, WSD needs to take the output of POSTag as input; while before POSTag, the document must be processed wit"
C10-3004,S07-1034,1,0.697039,"Missing"
C10-3004,J96-1002,0,\N,Missing
C12-1103,P11-1048,0,0.0128095,"cient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one integrating a phrase-structure parser and a dependency parser, and the other integrating a phrase-structure parser and a POS tagger. Both cases show that DD can help the individual subtasks. (Auli and Lopez, 2011) conduct an extensive comparison of LBP and DD for joint CCG supertagging and parsing. They show that LBP and DD achieves similar parsing accuracy improvement but has largely different convergence characteristics. Moreover, their work focuses on integrating two separately-trained sub-models, and they find that training the integrated model on LBP leads to large improvement drops compared with separately-trained models. 3 Pipeline POS tagging and dependency parsing The pipeline method treats POS tagging and dependency parsing as two cascaded problems. First, an optimal POS tag sequence ˆt is de"
C12-1103,C10-1011,0,0.178642,"Dependency features fdep (x, t, h, m, l) Sibling features fsib (x, t, h, m, l, s) Grandchild features fgrd (x, t, h, m, l, g) Atomic features incorporated l, wh, w m , t h , t m , t h±1 , t m±1 , t b , d i r(h, m), d ist(h, m) l, wh, ws , w m , t h , t m , t s , t h±1 , t m±1 , t s±1 , d i r(h, m), d ist(h, m) l, wh, w m , w g , t h , t m , t g , t h±1 , t m±1 , t g±1 , d i r(h, m), d i r(m, g) Table 1: Brief illustration of the syntactic features. b is an index between h and m. d i r(i, j) and d ist(i, j) denote the direction and distance of the dependency (i, j). Please refer to Table 4 of (Bohnet, 2010) for the complete feature list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt"
C12-1103,D12-1133,0,0.229202,"peline models in the parsing accuracy but lead to substantial tagging accuracy drop. Compared with their work, we propose a better training algorithm for the joint models that can improve both tagging and parsing accuracies. In addition, our joint model adopts richer features and handles labeled dependency parsing. (Hatori et al., 2011) propose the first transition-based joint model for Chinese POS tagging and unlabeled dependency parsing and gain large improvement in the parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as laten"
C12-1103,D07-1101,0,0.509725,"t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three categories corres"
C12-1103,W08-2102,0,0.0186802,"Missing"
C12-1103,P05-1022,0,0.0567146,"Missing"
C12-1103,P12-2003,1,0.88498,"Missing"
C12-1103,W02-1001,0,0.0361869,"ted by x = w1 ...w n , part-of-speech (POS) tagging aims to find an optimal tag sequence t = t 1 ...t n , where t i ∈ T (1 ≤ i ≤ n) and T is a predefined tag set. POS tags are designed to represent word classes so that words of the same POS tag play a similar role in syntactic structures. The size of T is usually much less than the vocabulary size. Typically, POS tagging is treated as a sequence labeling problem, and has been previously addressed by machine learning algorithms, such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). Figure 1 gives an example sentence from Penn Chinese Treebank 5.1 (CTB5). The lowest three rows present the n-best POS tags for each word, produced by a state-ofthe-art CRF model. Looking at the 1-best POS tags, we can see that the CRF model makes four errors, i.e. de/DEC→DEG, ouwen/NR→NN, xiaoli/VV→NN, and liwupudui/NR→NN. In fact, (DEC,DEG) and (NN,VV) ambiguities, which usually require long-distance syntactic knowledge to resolve, are very difficult for the sequential labeling models. NMOD ROOT DEP VMOD VMOD  1 SUB AMOD á 2 193 gang man just turned 19 AD VV CD JJ 19 P VMOD  4 VMOD  &apos;"
C12-1103,W09-1201,0,0.0527682,"Missing"
C12-1103,I11-1136,0,0.318758,"is most closely related to (Li et al., 2011) who present the first work on joint models for Chinese POS tagging and unlabeled dependency parsing. Similar to us, their joint models are based on graph-based dependency parsing. They find that the joint models largely outperform the pipeline models in the parsing accuracy but lead to substantial tagging accuracy drop. Compared with their work, we propose a better training algorithm for the joint models that can improve both tagging and parsing accuracies. In addition, our joint model adopts richer features and handles labeled dependency parsing. (Hatori et al., 2011) propose the first transition-based joint model for Chinese POS tagging and unlabeled dependency parsing and gain large improvement in the parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy."
C12-1103,P10-1110,0,0.190632,"the discriminative power of the POS features in resolving such syntaxinsensitive POS ambiguities are suppressed in the joint models when trained with AP or PA. Compared with AP and PA, SPA raises the weight of the POS features and can better utilize the disambiguation power of both the POS and syntactic features, leading to large tagging accuracy boost. On the other hand, better tagging results can further help parsing. 6 Experiments Data. We conduct experiments on CTB5 (Xue et al., 2005). Following the standard practice, we adopt the data split of (Duan et al., 2007; Zhang and Clark, 2008b; Huang and Sagae, 2010) and adopt Penn2Malt2 for constituent-to-dependency conversion with the head-finding rules of (Zhang and Clark, 2008b). We also evaluate our models on another version of CTB5 used in (Bohnet and Nivre, 2012) to compare with their joint model. We thank Bernd Bohnet for sharing their dataset. We refer to their dataset as CTB5-Bohnet. We carefully compare CTB5 with CTB5-Bohnet and find that except for the mismatch of about 30 sentence, the datasets differ in both dependency structures and dependency labels. After discussions with Bernd Bohnet, we find out that they adopt Yue Zhang’s constituent-t"
C12-1103,D08-1008,0,0.0157962,") , ˆt, d) AP wjoint (7) PA computes the update step τjoint by considering the loss of the best result, the score distance, and the feature vector distance.  ˆ − Scorejoint (x( j) , t( j) , d( j) ) + ρpos (t( j) , ˆt) + ρsyn (d( j) , d) ˆ Scorejoint (x( j) , ˆt, d)  τjoint = ( j) ( j) ( j) ( j) ˆ ˆ 2 kfjoint (x , t , d ) − fjoint (x , t, d)k (8) PA  (k+1) (k) ˆ w =w + τjoint (fjoint (x( j) , t( j) , d( j) ) − fjoint (x( j) , ˆt, d)) joint joint ( j) ˆ is the where ρpos (t , ˆt) is the incorrect POS tag number in ˆt according to t( j) , and ρsyn (d( j) , d) ˆ according to d( j) . Following (Johansson and Nugues, 2008), dependency error number in d ˆ increases by 1 for an incorrect dependency and by 0.5 for a correct dependency ρsyn (d( j) , d) with a wrong label. Theoretically, Eq. 8 computes the smallest update that makes the correct hypothesis outscores the returned highest-scoring hypothesis by the overall error. We can see that AP and PA use the same update step for the POS features fpos (.) and syntactic features fsyn (.). Therefore, the weights of the POS features and the syntactic features are of the same scale after training is completed. We argue that this is problematic since the number of the sy"
C12-1103,P08-1068,0,0.333682,"Missing"
C12-1103,P10-1001,0,0.392309,"igram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three categories corresponding to the three typ"
C12-1103,P11-1089,0,0.0153191,"ctive dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one inte"
C12-1103,D11-1109,1,0.93535,"score that are previously defined in the pipeline models. Scorejoint (x, t, d) = Scorepos (x, t) + Scoresyn (x, t, d) = wpos · fpos (x, t) + wsyn · fsyn (x, t, d) (6) = wpos⊕syn · fpos⊕syn (x, t, d) = wjoint · fjoint (x, t, d) where ⊕ denotes vector concatenation. Note that our joint model incorporates the same POS and syntactic features with the pipeline models. Under the joint model, the weights of POS and syntactic features, denoted by wpos⊕syn or wjoint , are simultaneously learned. Therefore, they can interact with each other to determine an optimal joint result. 4.1 Decoding Similar to (Li et al., 2011), we extend the parsing algorithm of (Carreras, 2007) using the idea of (Eisner, 2000) and propose a dynamic programming (DP) based decoding algorithm for our joint model. Figure 3 illustrates the basic DP structures and operations. The key idea is to augment the basic DP structures in the parsing algorithm (namely spans) with a few POS tags. A span means a partially built structure spanning a sub-sentence. For example, the leftside span in Figure 3(a), which is called an incomplete span and is denoted by I(h,m,l)(t h ,t m ) , represents a partial tree spanning wh...w m with wh being tagged as"
C12-1103,D10-1004,0,0.0469191,"Missing"
C12-1103,P05-1012,0,0.236675,"ctors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing accuracy on a variety of languages (Koo and Collins, 2010; Bohnet, 2010). The score of a dependency tree is factored into scores of the three kinds of subtrees in Figure 2. Scoresyn (x, t, d) = wsyn · fsyn (x, t, d) = X {(h,m,l)}⊆d + X wdep · fdep (x, t, h, m, l) {(h,m,l),(h,s)}⊆d + X wsib · fsib (x, t, h, m, l, s) {(h,m,l),(m,g)}⊆d (4) wgrd · fgrd (x, t, h, m, l, g) For syntactic features, we adopt those of (Bohnet, 2010) which include three c"
C12-1103,N07-1051,0,0.0863404,"Missing"
C12-1103,W96-0213,0,0.889178,"NG 2012, Mumbai, December 2012. 1681 1 Introduction Given an input sentence of n words, denoted by x = w1 ...w n , part-of-speech (POS) tagging aims to find an optimal tag sequence t = t 1 ...t n , where t i ∈ T (1 ≤ i ≤ n) and T is a predefined tag set. POS tags are designed to represent word classes so that words of the same POS tag play a similar role in syntactic structures. The size of T is usually much less than the vocabulary size. Typically, POS tagging is treated as a sequence labeling problem, and has been previously addressed by machine learning algorithms, such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). Figure 1 gives an example sentence from Penn Chinese Treebank 5.1 (CTB5). The lowest three rows present the n-best POS tags for each word, produced by a state-ofthe-art CRF model. Looking at the 1-best POS tags, we can see that the CRF model makes four errors, i.e. de/DEC→DEG, ouwen/NR→NN, xiaoli/VV→NN, and liwupudui/NR→NN. In fact, (DEC,DEG) and (NN,VV) ambiguities, which usually require long-distance syntactic knowledge to resolve, are very difficult for the sequential labeling models. NMOD ROOT DEP VMO"
C12-1103,D10-1001,0,0.012255,"nts out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction between morphology and syntax and achieve gains on both subtasks. (Rush et al., 2010) propose dual decomposition (DD) for integrating different NLP subtasks at the test phase. They experiment with two cases, one integrating a phrase-structure parser and a dependency parser, and the other integrating a phrase-structure parser and a POS tagger. Both cases show that DD can help the individual subtasks. (Auli and Lopez, 2011) conduct an extensive comparison of LBP and DD for joint CCG supertagging and parsing. They show that LBP and DD achieves similar parsing accuracy improvement but has largely different convergence characteristics. Moreover, their work focuses on integrating tw"
C12-1103,D08-1016,0,0.0105209,"e parsing accuracy. However, their joint models only slightly improve the tagging accuracy over a sequential tagging model. (Bohnet and Nivre, 2012) propose a transition-based joint model which can handle labeled non-projective dependency parsing. They conduct experiments on a variety of languages including Chinese, English, Czech, and German. Similarly, their joint model largely improves the parsing accuracy but only slightly increases the tagging accuracy. Differently, we are the first work on joint POS tagging and dependency parsing that achieves large improvement in the tagging accuracy. (Smith and Eisner, 2008) apply loopy belief propagation (LBP) to dependency parsing and points out that LBP can naturally represent POS tags as latent variables so that the POS tags can be inferred jointly with the parse. (Lee et al., 2011) extend the LBP based approach of (Smith and Eisner, 2008) and study joint morphological disambiguation and dependency parsing for morphologically-rich languages including Latin, Czech, Ancient Greek, and Hungarian. For these languages, morphological analysis requires the disambiguation of POS tags, gender, case, etc. They show that the joint model can well capture the interaction"
C12-1103,D09-1058,0,0.154484,"Missing"
C12-1103,P08-1101,0,0.469583,"list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing ac"
C12-1103,D08-1059,0,0.230453,"list. CRF-based POS tagging. We adopt the first-order CRF to build our baseline POS tagger. As a conditional log-linear probabilistic model, CRF defines the probability of a tag sequence as X exp(Scorepos (x, t′ )) P(t|x) = exp(Scorepos (x, t))/ Scorepos (x, t) = wpos · fpos (x, t) = X 1≤i≤n t′ wpu · fpu (x, t i ) + wpb · fpb (x, t i−1 , t i ) (3) where fpos/pu/pb (.) refers to the feature vectors and wpos/pu/pb is the corresponding weight vectors. We call fpu (x, t i ) the POS unigram features, and fpb (x, t i−1 , t i ) the POS bigram features. For Chinese, we adopt the features proposed by (Zhang and Clark, 2008a). They use Chinese characters contained in a word to compose rich features, which turns out to be helpful for low-frequency words. For English, we adopt the features of (Ratnaparkhi, 1996) which exploit suffixes and prefixes to improve tagging performance over rare words. Second-order graph-based dependency parsing. The graph-based approach views dependency parsing as finding a highest scoring tree in a directed graph (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of (Carreras, 2007) since previous studies show that it leads to best parsing ac"
C12-1103,J11-1005,0,0.00551549,"Missing"
C12-1103,P11-2033,0,0.150722,"Missing"
C12-1155,J99-1001,0,0.0607346,"ons. Any member of the expert panel may exhibit the sociolinguistic behavior consistent with being an influencer. In a peer-oriented group discussion however, it could occur that the task and thought leader (leader and influencer) are the same person. Human-human interaction affords a rich resource for research. Much prior work has been done in communication that focuses on the communicative dimension of discourse. For example, the Speech Act theory (Austin, 1962; Searle 1969) provides a generalized framework of multiple levels of discourse analysis; work on dialogue analysis (Blaylock, 2002; Carberry and Lambert, 1999; Stolcke et al., 2000) focuses on information content and structure of dialogues. Somewhat more relevant to social roles is research that models sequences of dialogue acts (Bunt, 1994), in order to predict the next dialogue act (Samuel et al. 1998; Stolcke, et al., 2000; Ji & Bilmes, 2006, inter alia) or to map them onto subsequences or “dialogue games” (Carlson 1983; Levin et al., 1998), from which participants’ functional roles in conversation (though not social roles) may be extrapolated (e.g., Linell, 1990; Poesio and Mikheev, 1998; Field et al., 2008). However, the effects of speech acts"
C12-1155,H92-1086,0,0.379027,"Missing"
C12-1155,P11-2059,0,0.0128008,"ying online chat relies on the more explicit linguistic devices necessary to convey social and cultural nuances than is typical in face-to-face or telephonic conversations. The use of language by participants as a feature to determine interpersonal relations has been studied by Bracewell et al. (2011) who developed a learning framework to determine collegiality between discourse participants. Their approach, however, looks at singular instances of linguistic markers or single utterances rather than a sustained demonstration of sociolinguistic behavior over the 2536 course of entire discourse. Freedman et al. (2011) have developed an approach that takes into account the entire discourse to detect behaviors such as persuasion; however their analysis is conducted on and models developed upon online discussion threads where the social phenomena of interest may be rare. By contrast, we build our models based on analysis of a data corpus of online chat discourse, where data collection experiments were specifically designed so that the resulting corpus may be rich in sociolinguistic phenomena. Our research extends the work of Strzalkowski et al. (2010) and Broadwell et al. (2012), who first proposed the two-ti"
C12-1155,liu-etal-2012-extending,1,0.822134,"Missing"
C12-1155,P98-2188,0,0.0301205,"n-human interaction affords a rich resource for research. Much prior work has been done in communication that focuses on the communicative dimension of discourse. For example, the Speech Act theory (Austin, 1962; Searle 1969) provides a generalized framework of multiple levels of discourse analysis; work on dialogue analysis (Blaylock, 2002; Carberry and Lambert, 1999; Stolcke et al., 2000) focuses on information content and structure of dialogues. Somewhat more relevant to social roles is research that models sequences of dialogue acts (Bunt, 1994), in order to predict the next dialogue act (Samuel et al. 1998; Stolcke, et al., 2000; Ji & Bilmes, 2006, inter alia) or to map them onto subsequences or “dialogue games” (Carlson 1983; Levin et al., 1998), from which participants’ functional roles in conversation (though not social roles) may be extrapolated (e.g., Linell, 1990; Poesio and Mikheev, 1998; Field et al., 2008). However, the effects of speech acts on social behaviors and roles of conversation participants have not been systematically studied. Research in anthropology and communication has concentrated on how certain social norms and behaviors may be reflected in language (e.g., Scollon and"
C12-1155,shaikh-etal-2010-mpc,1,0.928668,"nor role to play in computing Influence, and hence we do not include them while combining behaviors. Similarly, while Task Control and Disagreement are most indicative of Task Leadership, other behaviours such as Network Centrality and Argument Diversity do not correlate with this role. Hence, we do not include them in computation of Leadership. We shall elaborate on this in Section 5, Evaluation and Results. 2540 3 Corpus, Annotation and Computational Modules The models described in this paper are derived from online chat dialogues. The corpus we use for this analysis is the MPC chat corpus (Shaikh et al., 2010, Liu et al., 2012). This is a corpus of over 90 hours of online chat dialogues in English, Urdu and Mandarin. Participants in these chats are native speakers of these languages. Each chat session is a task-oriented dialogue around 90 minutes in length, with at least 4 participants. This corpus is particularly useful for the type of sociolinguistic analysis we are interested in due to the characteristics of interaction in each chat session – the participants are focused on some task, they form a fairly stable group and the dynamics of conversation unfold naturally through discourse. Other corp"
C12-1155,W04-2319,0,0.0395466,"over 90 hours of online chat dialogues in English, Urdu and Mandarin. Participants in these chats are native speakers of these languages. Each chat session is a task-oriented dialogue around 90 minutes in length, with at least 4 participants. This corpus is particularly useful for the type of sociolinguistic analysis we are interested in due to the characteristics of interaction in each chat session – the participants are focused on some task, they form a fairly stable group and the dynamics of conversation unfold naturally through discourse. Other corpora exist such as the ICSI-MRDA corpus (Shriberg et al., 2004) and the AMI meeting corpus (Carletta, 2007), however these are spoken language resources rather than online chat. Where other corpora of online chat do exist, like the NPS Internet chat corpus (Forsyth and Martell, 2007) and StrikeCom corpus (Twitchell et al., 2004), they do not contain any information about the participants themselves or their reactions to the discussion. In order to create a ground truth of assessments of sociolinguistic behavior, we needed certain information to be captured through questionnaires or survey following each data collection session. In the data that comprise M"
C12-1155,J00-3003,0,0.308716,"Missing"
C12-1155,C10-1117,1,0.852338,"olinguistic behavior over the 2536 course of entire discourse. Freedman et al. (2011) have developed an approach that takes into account the entire discourse to detect behaviors such as persuasion; however their analysis is conducted on and models developed upon online discussion threads where the social phenomena of interest may be rare. By contrast, we build our models based on analysis of a data corpus of online chat discourse, where data collection experiments were specifically designed so that the resulting corpus may be rich in sociolinguistic phenomena. Our research extends the work of Strzalkowski et al. (2010) and Broadwell et al. (2012), who first proposed the two-tiered approach to sociolinguistic modeling and have demonstrated that a subset of mid-level sociolinguistic behaviors may be accurately inferred by a combination of low-level language features. We have adopted their approach and extended it to modeling of leadership and influence. Furthermore, we enhanced the method by adding the evidence learnt from correlations of indices and measures to compute weights through which sociolinguistic behaviors may be combined appropriately to infer higher-level social phenomena. In this paper, we descr"
C12-1155,C10-2150,0,0.023678,"Missing"
C12-1188,J04-4004,0,0.0330972,"rithm that is first proposed by (Wolpert, 1992) and (Breiman, 1996). It has been exploited in a number of NLP tasks for integration. We mainly concern the works of stacked learning applied on POS tagging and dependency parsing. The work of (Li et al., 2011a) presented a mostly recent work for stacking POS taggers. They exploit the output of a CRF POS tagger to help a perceptronbased POS tagger with syntactic features. (McDonald, 2006) proposed the first stacking work of dependency parsing. The author incorporated parse decisions of two constituent-based parsers, Collins parser (Collins, 1999; Bikel, 2004) and Charniak parser (Charniak, 2000), into the second-order MST parser. Then (Nivre and McDonald, 2008) suggested integrating graph- and transition-based models by stacking, and more detailed analysis was given in (McDonald and Nivre, 2011). (Martins et al., 2008) also demonstrated that stacking transition- and graphbased parsers can improve parsing performance significantly and meanwhile offer theoretical interpretations for stacking. In our paper, stacked leaning is applied on the joint tasks of Chinese POS tagging and dependency parsing. 3 Two Models for Joint Chinese POS Tagging and Depen"
C12-1188,D12-1133,0,0.0193385,"out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS taggin"
C12-1188,D07-1101,0,0.172376,"nd the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error pattern"
C12-1188,A00-2018,0,0.0563471,"olpert, 1992) and (Breiman, 1996). It has been exploited in a number of NLP tasks for integration. We mainly concern the works of stacked learning applied on POS tagging and dependency parsing. The work of (Li et al., 2011a) presented a mostly recent work for stacking POS taggers. They exploit the output of a CRF POS tagger to help a perceptronbased POS tagger with syntactic features. (McDonald, 2006) proposed the first stacking work of dependency parsing. The author incorporated parse decisions of two constituent-based parsers, Collins parser (Collins, 1999; Bikel, 2004) and Charniak parser (Charniak, 2000), into the second-order MST parser. Then (Nivre and McDonald, 2008) suggested integrating graph- and transition-based models by stacking, and more detailed analysis was given in (McDonald and Nivre, 2011). (Martins et al., 2008) also demonstrated that stacking transition- and graphbased parsers can improve parsing performance significantly and meanwhile offer theoretical interpretations for stacking. In our paper, stacked leaning is applied on the joint tasks of Chinese POS tagging and dependency parsing. 3 Two Models for Joint Chinese POS Tagging and Dependency parsing A dependency tree for a"
C12-1188,P05-1022,0,0.0310453,"specially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat"
C12-1188,P12-2003,1,0.910792,"stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 2012) proposed to improve the Chinese parsing acc"
C12-1188,W02-1001,0,0.0331342,"odifier word (or child) is w m . The task of dependency parsing is to find an optimum dependency tree d for the input sentence x. Generally, the POS tag sequence of the sentence t = t 1 · · · t n (where t i ∈ T, 1 ≤ i ≤ n, T is the POS tag set) is taken as an input for dependency parsing, which is determined by the task of POS tagging, thus forming a pipeline model of the two tasks. POS tagging is a typical sequence labeling problems which can be resolved by algorithms such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and averaged perceptron (Collins, 2002). The goal of joint models of the two tasks is to find an optimum dependency tree and an optimum POS ˆ for x concurrently. tag sequence (ˆt, d) 3.1 Graph-based Joint Model The graph-based joint model is first proposed by (Li et al., 2011b). Such a model is extended from a graph-based model for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). In the model, the score of a dependency tree along with POS tags on each node is factored into scores of small parts. (Li et al., 2011b) have introduced several different graph-based joint model"
C12-1188,W03-0433,0,0.0480722,"Missing"
C12-1188,I11-1136,0,0.107354,"tem: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependen"
C12-1188,P10-1110,0,0.0722255,"cy Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG parser. In Chinese POS tagg"
C12-1188,P10-1001,0,0.0624319,"ansition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different"
C12-1188,I11-1171,1,0.250948,"Mumbai, December 2012. 3071 1 Introduction Part-of-speech (POS) tagging and dependency parsing are two fundamental natural language processing (NLP) tasks. Typically, POS tagging is a preprocessing step for dependency parsing, especially in a pipeline architecture. There are two main problems in a pipeline system: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked lear"
C12-1188,D11-1109,1,0.320009,"Mumbai, December 2012. 3071 1 Introduction Part-of-speech (POS) tagging and dependency parsing are two fundamental natural language processing (NLP) tasks. Typically, POS tagging is a preprocessing step for dependency parsing, especially in a pipeline architecture. There are two main problems in a pipeline system: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked lear"
C12-1188,D08-1017,0,0.11201,"he performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic con"
C12-1188,P05-1012,0,0.376473,"dels including the guided graph-based joint model and the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al.,"
C12-1188,J11-1007,0,0.0608408,"ey propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parse"
C12-1188,E06-1011,0,0.241065,"ed graph-based joint model and the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certa"
C12-1188,J08-4003,0,0.161113,"and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG pars"
C12-1188,P08-1108,0,0.37237,"odel can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that,"
C12-1188,P06-1055,0,0.370553,"that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent"
C12-1188,N07-1051,0,0.546016,"of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can als"
C12-1188,C10-1120,0,0.0226267,"ndency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) p"
C12-1188,P11-1139,0,0.0211513,"ing accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from grap"
C12-1188,P12-1026,0,0.110473,"uent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 2012) proposed to improve the Chinese parsing accuracy by a PCFG parser. Similarly, (Sun and Uszkoreit, 2012) exploited a PCFG parser to enhance Chinese POS tagging. Thus it is reasonable to investigate the performance of constituent-based joint models and to improve the performance of joint Chinese POS tagging and dependency parsing by a constituent-based joint model. In this paper, first we study the integration of a graph-based joint model (JGraph) and a transition-based joint model (JTrans) by stacked learning. The stacked learning is implemented using a two-level architecture, where the level-0 consists of one or more predictors of which the results are exploited as input to enhance the level-1"
C12-1188,W03-3023,0,0.373203,"er. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 201"
C12-1188,D08-1059,0,0.0212821,"odels, the probability should be much lower with a negative impact. We name the features related with the consistency of the two level-0 models as guided consistent features. Table 2 lists the guided consistent features used in this work. Both the guided graph-based joint model and the guided transition-based joint model are considered. 6 6.1 Experiments Experimental Settings We use CTB5.1 to conduct our experiments. Following the works of (Li et al., 2011b) and (Hatori et al., 2011), we use the standard split of CTB5.1 described in (Duan et al., 2007) and the conversion rules of CS-to-DS in (Zhang and Clark, 2008). We use the standard tagging accuracy to evaluate POS tagging. For dependency parsing, we use word accuracy (also known as dependency accuracy or UAS), root accuracy and complete 3078 The Guided Graph-based Joint Model: JGraph(JTrans, JConst) pos dep JTrans JConst JTrans JTrans {Whether ˆt m is identical to ˆt m ?} ⊗{ˆt m ◦ t m , ˆt m ◦ wm ◦ t m } ˆ JTrans and d ˆ JConst ?} ⊗ {Whether hx m is in {Whether the heads of m are identical in d ˆ JTrans ?} ⊗{t h , t m , t h ◦ t m } d The Guided Transition-based Joint Model: JTrans(JGraph, JConst) pos syn JGraph JConst JGraph JGraph {Whether ˆt m is"
C12-1188,P11-2033,0,0.0805884,"2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG parser. In Chinese POS tagging, (Sun and Uszkoreit,"
C12-1188,J03-4003,0,\N,Missing
C12-1190,de-marneffe-etal-2006-generating,0,0.0176657,"Missing"
C12-1190,P08-1019,0,0.125719,"Search results comparison between TLM and drTLM for query “How do you charge a farad capacitor?”.Questions in bold font are relevant ones. 5 Related Work The existing IR technologies are frequently based on Bag-of-Words models and regard both the query and documents in collections as composition of individual and independent words. For example, Ponte et al. (Ponte and Croft, 1998) utilized unigram language model for information retrieval. Jones et al. (Jones et al., 2000) proposed the binary independent retrieval (BIR) model to capture the relevance between queries and documents. Duan et al. (Duan et al., 2008) proposed a new language model to capture the relation between question topic and focus. They may not be directly applicable in the question retrieval domain due to at least two reasons. First, compared to the simple keywords based search, the querying questions are usually represented in natural language and depict some concepts linked by intrinsic semantic relationships. Second, the to be searched documents are also questions, which are far shorter than the verbose documents in traditional search approaches. Jeon et al. (Jeon et al., 2005a,b), moving forward one step, provided comparison of"
C12-1190,N03-1032,0,0.0464781,"citor”, hence, the dependency relation path length d r_path_l en equals to 1 as shown in Figure 1(b) (1). However, “charge” is a bit farther away from the term “farad” as the d r_path_l en between term “charge” and “farad” equals to 2 as shown in Figure 1(b) (3). This implies that “charge” should be weighted more closely with “capacitor” than with “farad”. 2.2 Dependency based Closeness Estimation for Pairwise Terms Several existing methods can be employed to compute the closeness between pairwise terms, such as pointwise mutual information (pmi), Chi and mutual information (Gao et al., 2004; Terra and Clarke, 2003). However, few of them take the syntactic dependency into consideration. Instead our approach estimates the dependency relevance of term pairs by linearly integrating multi-faceted cues, i.e., dependency relation path analysis as well as probabilistic analysis. First, from the perspective of dependency relation path, we denote d r_path_l en(t i , t j ) as the length of dependency relation path between term t i and t j . The dependency relevance can be defined as: 1 (1) Dep(t i , t j ) = d r_path_len(t ,t ) i j b where b is a constant larger than 1, which is selected based on a development set"
C12-1192,P05-1074,0,0.0235464,"In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this method to paraphrase pattern extraction. To our knowledge, few studies have been conducted on learning paraphrases from query logs. Zhao et al. (2010)’s study might be the closest to our work. Their method is motivated by the assumption that"
C12-1192,N03-1003,0,0.0448077,"n introduce the pattern pair induction method in Section 3. The paraphrase pattern recognition method is proposed in Section 4, in which the user behavior features are described in detail. We present the experiment results in Section 5 and conclude the paper in Section 6. 2 2.1 Related Work Paraphrase Learning Plenty of methods have been proposed to extract paraphrases from various data sources. In (Barzilay and McKeown, 2001), the authors viewed multiple translation versions of the same literary works as monolingual parallel corpora and extracted paraphrases with a co-training algorithm. In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch"
C12-1192,P01-1008,0,0.0568895,"aphrases from query-click pairs of query logs, which inspires us to integrate them in our future research. 1 www.baidu.com 3138 In what follows, we first review related studies in Section 2. We then introduce the pattern pair induction method in Section 3. The paraphrase pattern recognition method is proposed in Section 4, in which the user behavior features are described in detail. We present the experiment results in Section 5 and conclude the paper in Section 6. 2 2.1 Related Work Paraphrase Learning Plenty of methods have been proposed to extract paraphrases from various data sources. In (Barzilay and McKeown, 2001), the authors viewed multiple translation versions of the same literary works as monolingual parallel corpora and extracted paraphrases with a co-training algorithm. In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or pat"
C12-1192,P08-1077,0,0.0171147,"araphrases from various data sources. In (Barzilay and McKeown, 2001), the authors viewed multiple translation versions of the same literary works as monolingual parallel corpora and extracted paraphrases with a co-training algorithm. In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this meth"
C12-1192,I05-5001,0,0.0241781,"rn pairs induced from all sessions in the search log and sum up the frequencies for each pair. Pattern pairs satisfying the following requirement are retained: (1) the frequency of the pattern pair exceeds a threshold T1 , (2) the number of unique fillers for each slot exceeds a threshold T2 . 3140 qi 大象 的 重量 weight of an elephant qj 大象 有 多重 How much does an elephant weigh pi [n-1] 的 重量 weight of an [n-1] pj [n-1] 有 多重 How much does an [n-1] weigh slot filler: 大象(elephant) slot: [n-1] Figure 1: Example of pattern pair induction. 4 Paraphrase Pattern Recognition Following the previous studies (Brockett and Dolan, 2005; Finch et al., 2005; Malakasiotis, 2009), we recast paraphrase pattern recognition as a classification problem. Each induced pattern pair is classified into one of the two classes, i.e., paraphrase and non-paraphrase. A Support Vector Machines (SVM) classifier is used in our experiments, since it has proven effective in this task (Brockett and Dolan, 2005; Finch et al., 2005). Our classification features can be divided into two groups: the baseline features examined in previous studies (Section 4.1) and user behavior based features proposed in this work (Section 4.2). 4.1 Baseline Features (F"
C12-1192,D08-1021,0,0.0174319,"ns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this method to paraphrase pattern extraction. To our knowledge, few studies have been conducted on learning paraphrases from query logs. Zhao et al. (2010)’s study might be the closest to our work. Their method is motivated by the assumption that user queries and the clicked titles are potential paraphrases. Accordingly, they train a classifier to recognize paraphrases from query-title pairs. They further extract query-query and title-titl"
C12-1192,N06-1003,0,0.0700054,"Missing"
C12-1192,C04-1051,0,0.0382211,"pair induction method in Section 3. The paraphrase pattern recognition method is proposed in Section 4, in which the user behavior features are described in detail. We present the experiment results in Section 5 and conclude the paper in Section 6. 2 2.1 Related Work Paraphrase Learning Plenty of methods have been proposed to extract paraphrases from various data sources. In (Barzilay and McKeown, 2001), the authors viewed multiple translation versions of the same literary works as monolingual parallel corpora and extracted paraphrases with a co-training algorithm. In (Barzilay and Lee, 2003; Dolan et al., 2004), researchers collected comparable news articles reporting on the same event, and further extracted parallel sentences for learning paraphrase phrases and patterns. There are also studies focusing on extracting paraphrases from large-scale monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first present"
C12-1192,N06-2009,0,0.0675003,"Missing"
C12-1192,I05-5003,0,0.0357257,"sessions in the search log and sum up the frequencies for each pair. Pattern pairs satisfying the following requirement are retained: (1) the frequency of the pattern pair exceeds a threshold T1 , (2) the number of unique fillers for each slot exceeds a threshold T2 . 3140 qi 大象 的 重量 weight of an elephant qj 大象 有 多重 How much does an elephant weigh pi [n-1] 的 重量 weight of an [n-1] pj [n-1] 有 多重 How much does an [n-1] weigh slot filler: 大象(elephant) slot: [n-1] Figure 1: Example of pattern pair induction. 4 Paraphrase Pattern Recognition Following the previous studies (Brockett and Dolan, 2005; Finch et al., 2005; Malakasiotis, 2009), we recast paraphrase pattern recognition as a classification problem. Each induced pattern pair is classified into one of the two classes, i.e., paraphrase and non-paraphrase. A Support Vector Machines (SVM) classifier is used in our experiments, since it has proven effective in this task (Brockett and Dolan, 2005; Finch et al., 2005). Our classification features can be divided into two groups: the baseline features examined in previous studies (Section 4.1) and user behavior based features proposed in this work (Section 4.2). 4.1 Baseline Features (FBL ) Conventional fe"
C12-1192,N03-1017,0,0.01133,"entage of paraphrases decreases as the rank gets lower. We therefore design the frequency based feature as: F f r (p1 , p2 ) = P f r eq(p1 , p2 ) p f r eq(p1 , p) + C1 , (1) where f r eq(p1 , p2 ) is the frequency of 〈p1 , p2 〉 on the whole set of pattern pairs, C1 is a constant parameter used to avoid overestimating the feature value when p1 is too infrequent. We also compute the frequency based feature in the other direction, i.e., F f r (p2 , p1 ) in the same way. Lexical Score Features (Fl s ). Inspired by lexical weight features used to measure phrase pair quality in machine translation (Koehn et al., 2003), we introduce lexical score features to measure the lexical level paraphrase likelihood of each pattern pair. We design a lexical scoring approach based on the observation that many words keep unchanged when users rewrite their queries within sessions. It is reasonable to assume that those unchanged words across queries should exclusively align with themselves, while the changed words may likely form paraphrase word pairs. Accordingly, given a pair of related queries q1 = w11 ...w1m and q2 = w21 ...w2n extracted from the same session, we compute two scores for any word pair 〈w1i , w2 j 〉 (1 ≤"
C12-1192,W07-0716,0,0.0460809,"Missing"
C12-1192,P09-3004,0,0.0141166,"ch log and sum up the frequencies for each pair. Pattern pairs satisfying the following requirement are retained: (1) the frequency of the pattern pair exceeds a threshold T1 , (2) the number of unique fillers for each slot exceeds a threshold T2 . 3140 qi 大象 的 重量 weight of an elephant qj 大象 有 多重 How much does an elephant weigh pi [n-1] 的 重量 weight of an [n-1] pj [n-1] 有 多重 How much does an [n-1] weigh slot filler: 大象(elephant) slot: [n-1] Figure 1: Example of pattern pair induction. 4 Paraphrase Pattern Recognition Following the previous studies (Brockett and Dolan, 2005; Finch et al., 2005; Malakasiotis, 2009), we recast paraphrase pattern recognition as a classification problem. Each induced pattern pair is classified into one of the two classes, i.e., paraphrase and non-paraphrase. A Support Vector Machines (SVM) classifier is used in our experiments, since it has proven effective in this task (Brockett and Dolan, 2005; Finch et al., 2005). Our classification features can be divided into two groups: the baseline features examined in previous studies (Section 4.1) and user behavior based features proposed in this work (Section 4.2). 4.1 Baseline Features (FBL ) Conventional features for paraphrase"
C12-1192,D09-1040,0,0.0272263,"Missing"
C12-1192,P02-1006,0,0.216653,"Missing"
C12-1192,C08-1093,0,0.0241134,"ogs and applying them in query rewriting and recommendation. Such research can be classified into three groups. The first group of methods utilizes user clicks when computing query similarity. The underlying assumption is that if users tend to click on similar documents for two queries, then the meanings of the queries should be similar (Wen et al., 2002; Baeza-Yates and Tiberi, 2007). In the second group of methods, researchers mine query rewriting terms directly from user clicked documents. Their basic idea is that terms from queries and user clicked documents are related (Cui et al., 2002; Riezler et al., 2008). The third group of methods learns related queries from query sessions. The assumption is that queries submitted by the same user within a short time might be related in meaning (Fonseca et al., 2005; Jones et al., 2006; Zhang and Nasraoui, 2006; Szpektor et al., 2011). Our work is close to the third group. However, what we learn are 3139 paraphrase patterns rather than related queries or patterns. 3 3.1 Pattern Pair Induction Concepts Query. In this work, we collect user queries and other useful information from the used search logs and represent a query q as a triplet: q = 〈qc, qt, cn〉, whe"
C12-1192,P07-1059,0,0.0582929,"Missing"
C12-1192,P07-1058,0,0.0202016,"differences, such as inserting or deleting a stop word. The libsvm toolkit4 was used as the classifier, with its default parameter settings. Some other parameters used in our method were set empirically: T1 = 5, T2 = 3, C1 = 20, C2 = 10. 5.1 Evaluation of the Classifier We randomly sampled 5115 candidate pattern pairs to form the experimental data set. Two Chinese native speakers were asked to annotate the pattern pairs separately. A pattern pair should be annotated as positive (correct paraphrase patterns) or negative (otherwise). We follow the instance-based evaluation approach proposed by Szpektor et al. (2007). Particularly, we provide pattern slot fillers to the annotators along with the pattern pairs. A pattern pair is judged as paraphrase only when most of the instances generated by filling the slots with the provided fillers are paraphrases. We calculated the annotation agreement between two annotators. The result shows that the observed agreement is 0.96 and the Kappa value is 0.90. We believe that the high annotation agreement is due to the careful training of the annotators and the instance-based evaluation approach. A third annotator was asked to decide the final annotation for the disagree"
C12-1192,C10-1148,1,0.720372,"rpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this method to paraphrase pattern extraction. To our knowledge, few studies have been conducted on learning paraphrases from query logs. Zhao et al. (2010)’s study might be the closest to our work. Their method is motivated by the assumption that user queries and the clicked titles are potential paraphrases. Accordingly, they train a classifier to recognize paraphrases from query-title pairs. They further extract query-query and title-title paraphrases from the query-title paraphrases based on the assumption that queries clicking on the same title and titles clicked on for the same query are also likely to be paraphrases. Additionally, they induce paraphrase patterns from the mined paraphrases. Our work differs from Zhao et al.’s mainly in that"
C12-1192,P08-1089,1,0.844859,"2001; Bhagat and Ravichandran, 2008). The basic idea is that phrases or patterns appearing in similar contexts tend to have the same meaning. Besides monolingual corpora, bilingual corpora have also been exploited for paraphrase extraction. Bannard and Callison-Burch (2005) first presented the method to learn paraphrase phrases from a bilingual phrase table. The key idea is that phrases aligned with the same foreign phrase could be paraphrases. Callison-Burch (2008) then improved the method by imposing syntax constraints to filter paraphrases with different syntactic structures. In addition, Zhao et al. (2008) extended this method to paraphrase pattern extraction. To our knowledge, few studies have been conducted on learning paraphrases from query logs. Zhao et al. (2010)’s study might be the closest to our work. Their method is motivated by the assumption that user queries and the clicked titles are potential paraphrases. Accordingly, they train a classifier to recognize paraphrases from query-title pairs. They further extract query-query and title-title paraphrases from the query-title paraphrases based on the assumption that queries clicking on the same title and titles clicked on for the same q"
C12-1192,C02-1161,0,0.0978369,"Missing"
C14-1018,baccianella-etal-2010-sentiwordnet,0,0.227345,"or negative score reflecting its sentiment polarity and strength. Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale se"
C14-1018,J81-4005,0,0.756294,"Missing"
C14-1018,P07-1054,0,0.0130165,"component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym"
C14-1018,P12-1092,0,0.137724,"ons between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment"
C14-1018,C04-1200,0,0.143042,"ng the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are"
C14-1018,P13-2087,0,0.0738275,"hrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words an"
C14-1018,P12-1043,0,0.0177108,"with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item."
C14-1018,C94-1079,0,0.0290494,"nt lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentim"
C14-1018,P11-1015,0,0.623804,"ntation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase em"
C14-1018,S13-2053,0,0.722061,"(2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentiment lexicon, Mohammad et al. (2013) use pointwise mutual information (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and :(. Chen et al. (2012) utilize the Urban Dictionary and extract the target-dependent sentiment expressions from Twitter. Unlike Mohammad et al. (2013) that only capture the relations between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of word"
C14-1018,S13-2052,0,0.0305225,"Missing"
C14-1018,W02-1011,0,0.0230923,"m tweets, leveraging massive tweets containing positive and negative emoticons as training set without any manual annotation. To obtain more training data for building the phrase-level sentiment classifier, we exploit the similar words from Urban Dictionary 2 , which is a crowd-sourcing resource, to expand a small list of sentiment seeds. Finally, we utilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting in the sentiment lexicon. We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. T"
C14-1018,J11-1002,0,0.0117166,"timent information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approach, as illustrated in Figure 1. We cast sentiment lexicon learning as a phrase-level cla"
C14-1018,E09-1077,0,0.0168758,"uilt manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resource"
C14-1018,D11-1014,0,0.0867976,"an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodology In this section, we describe our method for building large-scale sentiment lexicon from Twitter within a classification framework, as illustrated in Figure 1. We leverage the continuous representation of phrases as features, without parsers or hand-crafted rules, and automatically obtain the training data by seed expansion from Urban Dictionary. After the classifier is built, we employ it to predict the sentiment distribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon. To encode the sentimen"
C14-1018,D13-1170,0,0.0317488,"Missing"
C14-1018,P14-1146,1,0.621391,"d in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodolo"
C14-1018,P02-1053,0,0.0288544,"eets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment sco"
C14-1018,N10-1119,0,0.0358758,"entiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approac"
C14-1018,H05-1044,0,0.940027,"eness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. The main contributions of this work are as follows: • To our best knowledge, this is the first work that leverages the continuous representation of phrases for building large-scale sentiment lexicon from Twitter; • We propose a tailored neural architecture for learning the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework fo"
C14-1018,P13-1173,0,0.0128539,"negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this directi"
C14-1018,P10-1040,0,\N,Missing
C14-1048,apidianaki-2008-translation,0,0.144502,"(we |wc ) exceed some threshold 0 < δ < 1. The second column of Table 1 presents the extraction results on a sample of source language words with the corresponding translation words. 3.2 Clustering of Translation Words For each source language word, its translation words are then clustered so as to separate different senses. At the clustering time, we first represent each translation word with a feature vector (point), so that we can measure the similarities between points. Then we perform clustering on these feature vectors, representing different senses in different clusters. Different from Apidianaki (2008) who represents all occurrences of the translation words with their contexts in the foreign language for clustering, we adopt the embeddings of the translation words as the representations and directly perform clustering on the translation words,3 rather than the contexts of occurrences. The embedding representation is chosen for two reasons: (1) Word embeddings encode rich lexical semantics. They can be directly used to measure word similarities. (2) Embedding representation of the translation words leads to extremely high-efficiency clustering, because the number of translation words is orde"
C14-1048,J93-2003,0,0.0375018,"different senses in different clusters (). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each word occurrence in the source language text with its associated sense cluster (®). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a standard manner (¯). 3.1 Translation Words Extraction Given bilingual data after word alignment, we present a way of extracting translation words for source language words by exploiting the translation probability produced by word alignment models (Brown et al., 1993; Och and Ney, 2003; Liang et al., 2006). More formally, we notate the Chinese sentence as c = (c1 , ..., cI ) and English sentence as e = (e1 , ..., eJ ). The alignment models can be generally factored as: P p(c|e) = a p(a, c|e) Q p(a, c|e) = Jj=1 pd (aj |aj− , j)pt (cj |eaj ) (3) (4) where a is the alignment specifying the position of an English word aligned to each Chinese word, pd (aj |aj− , j) is the distortion probability, and pt (cj |eaj ) is the translation probability which we use. 499 SL Word 制服 花 法 领导 Translation Words Translation Word Clusters Nearest Neighbours investment, overpow"
C14-1048,N13-1006,1,0.799372,"Missing"
C14-1048,J81-4005,0,0.804567,"Missing"
C14-1048,P14-1113,1,0.133577,"word embeddings 9 10 www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp Person, Location, Organization, Date, Time, Number and Miscellany 504 0.90 +SingleEmb Polysemous(2) Polysemous(3) +SenseEmb 0.80 0.75 0.70 0.60 0.65 per−token accuracy 0.85 Baseline Monosemous Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polysemous(k) represents the set of words that have more than or equal to k senses defined in HowNet. are shown to capture many relational similarities, which can be recovered by vector arithmetic in the embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013) learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to constrain translational equivalence. Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010) and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each word. They did this by clustering the contexts of words. These multi-prototype models simply induced a fixed number"
C14-1048,1992.tmi-1.9,0,0.332596,"ng stochastic gradient descent (SGD), in which back propagation through time (BPTT) is used to efficiently compute the gradients. In the RNNLM, U is the embedding matrix, where each column vector represents a word. As discussed in Section 1, the RNNLM and even most NNLMs ignore the polysemy phenomenon in natural languages and induce a single embedding for each word. We address this issue and introduce an effective approach for capturing polysemy in the next section. 3 Sense-specific Word Embedding Learning In our approach, WSI is performed prior to the training of word embeddings. Inspired by Gale et al. (1992) and Chan and Ng (2005), who used bilingual data for automatically generating training examples of WSD, we present a bilingual approach for unsupervised WSI, as shown in Figure 1. First, we extract the translations of the source language words from bilingual data (¬). Since there may be multiple translations for the same sense of a source language word, it is straightforward to cluster the translation words, exhibiting different senses in different clusters (). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each"
C14-1048,P12-1092,0,0.872734,"). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed value. These models still may not capture the real senses of words, because different words may have different number of senses. We present a novel and simple method of learning sense-specific word embeddings by using bilingual parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs. We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the source language with diff"
C14-1048,S12-1049,0,0.0285058,"ich benefits many practical applications. Therefore, we first evaluate our embeddings using a similarity measurement. Word similarities are calculated using the MaxSim and AvgSim metric (Reisinger and Mooney, 2010): M axSim(u, v) = max1≤i≤ku ,1≤j≤kv s(ui , v j ) Pku Pkv 1 i j AvgSim(u, v) = ku ×k i=1 j=1 s(u , v ) v (5) (6) where ku and kv are the number of the induced senses for words u and v, respectively. s(·, ·) can be any standard similarity measure. In this study, we use the cosine similarity. Previous works used the WordSim-353 dataset (Finkelstein et al., 2002) or the Chinese version (Jin and Wu, 2012) for the evaluation of general word similarity. These datasets rarely contain polysemous words, and thus is unsuitable for our evaluation. To the best of our knowledge, no datasets for polysemous word similarity evaluation have been published yet, either in English or Chinese. In order to fill this gap in the research community, we manually construct a Chinese polysemous word similarity dataset. 5.2.1 Chinese Polysemous Word Similarity Dataset Construction We adopt the HowNet database (Dong and Dong, 2006) in constructing the dataset. HowNet is a Chinese knowledge database that maintains compr"
C14-1048,W13-3513,0,0.0127418,"language words is varied, the commonlyused k-means algorithm becomes inappropriate for this situation. Instead, we employ affinity propagation (AP) algorithm (Frey and Dueck, 2007) for clustering. In AP, each cluster is represented by one of the samples of it, which we call an exemplar. AP finds the exemplars iteratively based on the concept of “message passing”. AP has the major advantage that the number of the resulting clusters is dynamic, which mainly depends on the distribution of the data. Compared with other possible clustering approaches, such as hierarchical agglomerative clustering (Kartsaklis et al., 2013), AP determines the number of resulting clusters automatically without using any partition criterions. The third column of Table 1 lists the resulting clusters of the translation words for the sampled polysemous words. We can see that the resulting clusters are meaningful: senses are well represented by clusters of translation words. 3.3 Cross-lingual Word Sense Projection The produced clusters are then projected back into the source language to identify word senses. 3 The publicly available word embeddings proposed by Collobert et al. (2011) are used. 500 For each occurrence wo of the word w"
C14-1048,C12-1089,0,0.0290816,"10 www.icl.pku.edu.cn/icl groups/corpus/dwldform1.asp Person, Location, Organization, Date, Time, Number and Miscellany 504 0.90 +SingleEmb Polysemous(2) Polysemous(3) +SenseEmb 0.80 0.75 0.70 0.60 0.65 per−token accuracy 0.85 Baseline Monosemous Figure 4: Per-token accuracy on the polysemous and monosemous words in the NER test data. Polysemous(k) represents the set of words that have more than or equal to k senses defined in HowNet. are shown to capture many relational similarities, which can be recovered by vector arithmetic in the embedding space (Mikolov et al., 2013b; Fu et al., 2014). Klementiev et al. (2012) and Zou et al. (2013) learned cross-lingual word embeddings by utilizing MT word alignments in bilingual parallel data to constrain translational equivalence. Most previous NNLMs induce single embedding for each word, ignoring the polysemous property of languages. In an attempt to capture the different senses or usage of a word, Reisinger and Mooney (2010) and Huang et al. (2012) proposed multi-prototype models for inducing multiple embeddings for each word. They did this by clustering the contexts of words. These multi-prototype models simply induced a fixed number of embeddings for every wo"
C14-1048,N06-1014,0,0.488068,"(). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each word occurrence in the source language text with its associated sense cluster (®). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a standard manner (¯). 3.1 Translation Words Extraction Given bilingual data after word alignment, we present a way of extracting translation words for source language words by exploiting the translation probability produced by word alignment models (Brown et al., 1993; Och and Ney, 2003; Liang et al., 2006). More formally, we notate the Chinese sentence as c = (c1 , ..., cI ) and English sentence as e = (e1 , ..., eJ ). The alignment models can be generally factored as: P p(c|e) = a p(a, c|e) Q p(a, c|e) = Jj=1 pd (aj |aj− , j)pt (cj |eaj ) (3) (4) where a is the alignment specifying the position of an English word aligned to each Chinese word, pd (aj |aj− , j) is the distortion probability, and pt (cj |eaj ) is the translation probability which we use. 499 SL Word 制服 花 法 领导 Translation Words Translation Word Clusters Nearest Neighbours investment, overpower, investment, uniform 穿着dress , 警服poli"
C14-1048,N13-1090,0,0.281323,"g the sense-level word similarities. We further feed our embeddings as features in Chinese named entity recognition and obtain noticeable improvements against single embeddings. 1 Introduction Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector representations for words. Each dimension of word embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightforwardly for computing word similarities, which benefits many practical applications (Socher et al., 2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010;"
C14-1048,J03-1002,0,0.00534514,"different clusters (). Once word senses are effectively induced for each word, we are able to form the sense-labeled training data of RNNLMs by tagging each word occurrence in the source language text with its associated sense cluster (®). Finally, the sense-tagged corpus is used to train the sense-specific word embeddings in a standard manner (¯). 3.1 Translation Words Extraction Given bilingual data after word alignment, we present a way of extracting translation words for source language words by exploiting the translation probability produced by word alignment models (Brown et al., 1993; Och and Ney, 2003; Liang et al., 2006). More formally, we notate the Chinese sentence as c = (c1 , ..., cI ) and English sentence as e = (e1 , ..., eJ ). The alignment models can be generally factored as: P p(c|e) = a p(a, c|e) Q p(a, c|e) = Jj=1 pd (aj |aj− , j)pt (cj |eaj ) (3) (4) where a is the alignment specifying the position of an English word aligned to each Chinese word, pd (aj |aj− , j) is the distortion probability, and pt (cj |eaj ) is the translation probability which we use. 499 SL Word 制服 花 法 领导 Translation Words Translation Word Clusters Nearest Neighbours investment, overpower, investment, uni"
C14-1048,N10-1013,0,0.776045,"2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed value. These models still may not capture the real senses of words, because different words may have different number of senses. We present a novel and simple method of learning sense-specific word embeddings by using bilingual parallel data. In this method, word sense induction (WSI) is performed prior to the training of NNLMs. We exploit bilingual parallel data for WSI, which is motivated by the intuition that the same word in the sour"
C14-1048,J98-1004,0,0.56021,"Missing"
C14-1048,P10-1040,0,0.632179,"ents against single embeddings. 1 Introduction Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector representations for words. Each dimension of word embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightforwardly for computing word similarities, which benefits many practical applications (Socher et al., 2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefine"
C14-1048,N13-1063,0,0.015,"mbeddings. 1 Introduction Word embeddings are conventionally defined as compact, real-valued, and low-dimensional vector representations for words. Each dimension of word embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic characteristics. Word embeddings can be used straightforwardly for computing word similarities, which benefits many practical applications (Socher et al., 2011; Mikolov et al., 2013a). They are also shown to be effective as input to NLP systems (Collobert et al., 2011) or as features in various NLP tasks (Turian et al., 2010; Yu et al., 2013). In recent years, neural network language models (NNLMs) have become popular architectures for learning word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Mikolov et al., 2013b). Most of the previous NNLMs represent each word with a single embedding, which ignores polysemy. In an attempt to better capture the multiple senses or usages of a word, several multi-prototype models have been proposed (Reisinger and Mooney, 2010; Huang et al., 2012). These multi-prototype models simply induce K prototypes (embeddings) for every word in the vocabulary, where K is predefined as a fixed value"
C14-1048,D13-1141,0,0.169902,"d similarities from the dataset. The whole evaluation dataset will be publicly available for the research community.8 Word 制服 出 花 面 Paired word Category Mean.Sim Std.Dev 征服conquer synonym 8.60 0.29 重点key point unrelated 0.12 0.19 进enter autonym 7.90 0.97 发表publish near-synonym 7.86 0.76 茎plant stem sibling 7.80 0.12 费用cost topic-related 5.86 0.90 食物f ood hypernym 6.50 0.71 Table 2: Sample word pairs of our dataset. The unrelated words are randomly sampled. M ean.Sim represents the mean similarity of the annotations, Std.Dev represents the standard deviation. 5.2.2 Evaluation Results Following Zou et al. (2013), we use Spearman’s ρ correlation and Kendall’s τ correlation for evaluation. The results are shown in Table 3. By utilizing sense-specific embeddings, our approach significantly outperforms the single-version using either MaxSim or AvgSim measurement. For comparison with multi-prototype methods, we borrow the context-clustering idea from Huang et al. (2012), which was first presented by Sch¨utze (1998). The occurrences of a word are represented by the average embeddings of its context words. Following Huang et al.’s settings, we use a context window of size 10 and all occurrences of a word ar"
C14-1051,D12-1133,0,0.168726,"r the Zhang scheme, and increases of 0.30 on UAS and 0.36 on LAS for the Stanford scheme. The results also demonstrate similar conclusions with the experiments on English dataset. 5 Related Work Our work is mainly inspired by the work of joint models. There are a number of successful studies on joint modeling pipelined tasks where one task is a prerequisite step of another task, for example, the joint model of word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et Model Baseline Our joint model UAS 79.07 80.20‡ Zhang LAS 76.08 77.07‡ CM 27.96 30.10 UAS 80.33 80.63 Stanford LAS CM 75.29 31.14 75.65 31.20 Table 6: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees"
C14-1051,Q13-1034,0,0.0186811,"nd Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et Model Baseline Our joint model UAS 79.07 80.20‡ Zhang LAS 76.08 77.07‡ CM 27.96 30.10 UAS 80.33 80.63 Stanford LAS CM 75.29 31.14 75.65 31.20 Table 6: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have"
C14-1051,P13-1104,0,0.0498278,"r the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 2008) and the LTH scheme (Johansson and Nugues, 2007). There are different conversion schemes for the Chinese Penn Tr"
C14-1051,P04-1015,0,0.0101157,"top element S0 and the second top element S1 on the stack, with the dependency label being specified by l; and the pop-root action defines the root node of a dependency tree when there is only one element on the stack and no element in the queue. During decoding, each state may have several actions. We employ a fixed beam to reduce the search space. The low-score states are pruned from the beam when it is full. The feature templates in our baseline are shown by Table 1, referring to baseline feature templates. We learn the feature weights by the averaged percepron algorithm with early-update (Collins and Roark, 2004; Zhang and Clark, 2011). 3 The Proposed Joint Model The aforementioned baseline model can only handle a single dependency tree. In order to parse multiple dependency trees for a sentence, we usually use individual dependency parsers. This method is not able to exploit the correlations across different dependency schemes. The joint model to parse multiple dependency trees with a single model is an elegant way to exploit these correlations fully. Inspired by this, we make a novel extension to the baseline arc-standard transition system, arriving at a joint model to parse two heterogeneous depen"
C14-1051,W08-1301,0,0.151442,"Missing"
C14-1051,N13-1070,0,0.051856,"Missing"
C14-1051,I11-1136,0,0.0705947,"it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments on PTB with the Yamad"
C14-1051,P12-1110,0,0.240838,"lations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments on PTB with the Yamada and the Stanford schemes, and also on CTB 5.1 with the Zhang and the Stanford schemes. Th"
C14-1051,D09-1127,0,0.595101,"full use of the correlations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 2008) and the LTH schem"
C14-1051,P08-1102,0,0.0759747,"Missing"
C14-1051,P09-1059,0,0.241947,"se two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have been done for Chinese word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their work, we study the heterogeneous annotations derived from the same source. We use a unified model to parsing heterogeneous dependencies together. Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011), extended from a arc-standard transition-based parsing model (Huang et al., 2009). The transition-based framework is easily adapted to a number of joint models, including joint word segmentation and POS-tagging (Zhang and Clark, 2010), the joint POS-tagging and parsing (Hatori e"
C14-1051,W07-2416,0,0.0902035,"Missing"
C14-1051,P10-1001,0,0.141219,"elations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 2008) and the LTH scheme (Johansson and Nugues"
C14-1051,P09-1058,0,0.0793126,"rom the Yamada scheme and the label “pobj” from the Stanford scheme on a same dependency “atypoint” can make it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a"
C14-1051,D11-1109,1,0.915722,"typoint” can make it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments"
C14-1051,P12-1071,1,0.867181,"ectively exploited according to the above analysis. We assume that the first and second processing schemes are s1 and s2 respectively, to facilitate the below descriptions. We can see that the joint model behaves similarly to a pipeline reranking model, in optimizing scheme s1 ’s parsing performances. First we get K-best (K equals the beam size of the joint model) candidates for scheme s1 , and then employ additional evidences from scheme s2 ’s result, to rerank the K-best candidates, obtaining a better result. The joint model also behaves similarly to a pipeline feature-based stacking model (Li et al., 2012), in optimizing scheme s2 ’s parsing performances. After acquiring the best result of scheme s1 , we can use it to generate guided features to parse dependencies of scheme s2 . Thus additional information from scheme s1 can be imported into the parsing model of scheme s2 . Different with the pipeline reranking and the feature-based stacking models, we employ a single model to achieve the two goals, making the interactions between the two schemes be better performed. 4 Experiments 4.1 Experimental Settings In order to evaluate the baseline and joint models, we conduct experiments on English and"
C14-1051,P13-2109,0,0.0479035,"LAS are shown without the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results wi"
C14-1051,P05-1012,0,0.146151,"Missing"
C14-1051,J08-4003,0,0.0685433,"Missing"
C14-1051,N12-1054,0,0.272811,"heme, losses of 0.15 on UAS and 0.21 on LAS are shown without the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford p"
C14-1051,P13-1014,0,0.0321302,"Missing"
C14-1051,P12-1025,0,0.32961,"dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have been done for Chinese word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their work, we study the heterogeneous annotations derived from the same source. We use a unified model to parsing heterogeneous dependencies together. Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011), extended from a arc-standard transition-based parsing model (Huang et al., 2009). The transition-based framework is easily adapted to a number of joint models, including joint word segmentation and POS-tagging (Zhang and Clark, 2010), the joint POS-tagging and parsing (Hatori et al., 2012; Bohnet"
C14-1051,W03-3023,0,0.121476,"Missing"
C14-1051,D08-1059,0,0.558312,"t model which can make full use of the correlations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 200"
C14-1051,D10-1082,0,0.259738,"the label “pobj” from the Stanford scheme on a same dependency “atypoint” can make it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transi"
C14-1051,J11-1005,0,0.149178,"d POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments on PTB with the Yamada and the Stanford schemes, and also on CTB 5.1 with the Zhang and the Stanford schemes. The results show that our joint model gives improved performances over the individual baseline models for both schemes on both English and Chinese languages, demonstrating positive effects of the correlations between the two schemes. We make the source code freely available at http://sourceforge.net/ projects/zpar/,version0.7. 2 Baseline Traditionally, the dependency"
C14-1051,D12-1030,0,0.0156056,"h the Zhang and the Stanford schemes. The results show that our joint model gives improved performances over the individual baseline models for both schemes on both English and Chinese languages, demonstrating positive effects of the correlations between the two schemes. We make the source code freely available at http://sourceforge.net/ projects/zpar/,version0.7. 2 Baseline Traditionally, the dependency parsers of different schemes are trained with their corpus separately, using a state-of-the-art dependency parsing algorithm (Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and McDonald, 2012; Choi and McCallum, 2013). In this work, we exploit a transition-based arc-standard dependency parsing model combined with global learning and beam-search decoding as the baseline. which is initially proposed by Huang et al. (2009). In the following, we give a detailed description of the model. In a typical transition-based system for dependency parsing, we define a transition state, which consists of a stack to save partial-parsed trees and a queue to save unprocessed words. The parsing is performed incrementally via a set of transition actions. The transition actions are used to change cont"
C14-1051,P14-2107,0,0.0472412,"rd scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results with mark ∗∗ show the numbers for the Stanford dependencies fro"
C14-1051,P11-2033,0,0.160805,"ures proposed by us. For the Yamada scheme, losses of 0.15 on UAS and 0.21 on LAS are shown without the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the"
C14-1051,D13-1093,0,0.10027,"the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results with mark ∗∗ show the"
C14-1051,P13-1013,1,0.684759,"the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results with mark ∗∗ show the"
C14-1051,P14-1125,1,0.90386,"word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et Model Baseline Our joint model UAS 79.07 80.20‡ Zhang LAS 76.08 77.07‡ CM 27.96 30.10 UAS 80.33 80.63 Stanford LAS CM 75.29 31.14 75.65 31.20 Table 6: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All"
C14-1051,P13-2019,0,0.0201629,"s with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have been done for Chinese word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their work, we study the heterogeneous annotations derived from the same source. We use a unified model to parsing heterogeneous dependencies together. Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011),"
C14-1051,D07-1096,0,\N,Missing
C14-1087,E06-1002,0,0.241458,"tain of Iraqi” in the text of Figure 1 (a), which is well-known to readers. During reading process, these gaps will be automatically plugged effortlessly by the background knowledge in human brain. However, the situation is different for machine because it lacks the ability to acquire and select the proper background knowledge, which limits the performances of certain NLP applications. Document enrichment has been proved helpful in these tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Bunescu and Pasca, 2006; Sen, 2012). In the past, there are mainly two kinds of document enrichment researches according to the resource they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad 1 in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013). Despite the great success of these methods, there remain a great challenge that not al"
C14-1087,D07-1074,0,0.663248,"as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Bunescu and Pasca, 2006; Sen, 2012). In the past, there are mainly two kinds of document enrichment researches according to the resource they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad 1 in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013). Despite the great success of these methods, there remain a great challenge that not all information in the linked Wiki page is helpful to the understanding of corresponding document. For example, the Wiki page of Baghdad contains lots of information about city history and culture, which are not quite relevant to the semantic of context in Figure 1 (a). So treating the whole Wiki page as the enrichment to document may cause noise ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 Internati"
C14-1087,doddington-etal-2004-automatic,0,0.0397574,"xtracted from source document automatically by open information extraction technology (Banko et al., 2007), especially Reverb, the famous Open IE system developed by University of Washington (Etzioni et al., 2011). The output of ReVerb is formed as “argument1 , predicate, argument2 ”, which is naturally presented as triple. In this study, we use ACE corpus as source documents and all sd-nodes are extracted by ReVerb. The setup of automatic extraction makes our method usable in many real applications. To evaluate the effect of automatic extraction, we also use the golden annotation within ACE (Doddington et al., 2004) corpus as source document information and compare the performance that with automatic extraction. Background knowledge node (bk-node) Bk-nodes consist of the background knowledge extracted from external corpus resources automatically by Reverb too. We do not rely on certain existed knowledge base and extract background knowledge from external corpus resources for corresponding source document. This setup makes our methods usable in many real applications. Although we do not rely on special knowledge base, we do adapt our method for the existed knowledge base such as YAGO (Hoffart et al., 2013"
C14-1087,P13-2006,0,0.275026,"ryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Bunescu and Pasca, 2006; Sen, 2012). In the past, there are mainly two kinds of document enrichment researches according to the resource they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad 1 in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013). Despite the great success of these methods, there remain a great challenge that not all information in the linked Wiki page is helpful to the understanding of corresponding document. For example, the Wiki page of Baghdad contains lots of information about city history and culture, which are not quite relevant to the semantic of context in Figure 1 (a). So treating the whole Wiki page as the enrichment to document may cause noise ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the or"
C14-1087,D11-1072,0,0.188078,"Missing"
C14-1087,D10-1123,0,0.0428279,"Missing"
C14-1087,nastase-etal-2010-wikinet,0,0.330162,"Missing"
C14-1087,P11-1009,0,0.181009,"thors usually omit basic but well-known information to make the document more concise. For example, author omits “Baghdad is the captain of Iraqi” in the text of Figure 1 (a), which is well-known to readers. During reading process, these gaps will be automatically plugged effortlessly by the background knowledge in human brain. However, the situation is different for machine because it lacks the ability to acquire and select the proper background knowledge, which limits the performances of certain NLP applications. Document enrichment has been proved helpful in these tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Bunescu and Pasca, 2006; Sen, 2012). In the past, there are mainly two kinds of document enrichment researches according to the resource they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad 1 in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007; Han et al., 2011; Katari"
C14-1087,P10-1044,0,0.0604382,"Missing"
C14-1087,D10-1106,0,0.0214427,"cuses on extracting assertions from massive corpora without a pre-specied vocabulary (Banko et al., 2007). Open IE systems are unlexicalized-formed only in terms of syntactic tokens and closed-word classes, instead of specific nouns and verbs at all costs. There are existing Open IE systems such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), and StatSnowball (Zhu et al., 2009). The output of these systems has been used to support many NLP tasks such as learning selectional preference (Ritter et al., 2010), acquiring sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010). In this work, we use the famous Open IE system Reverb (Etzioni et al., 2011), which is generated from TextRunner (Etzioni et al., 2008), to extract source document information and background knowledge automatically. We use the newest version of ReVerb (version 1.3) without modification, which is free download on-line 3 . Source document node (sd-node) Sd-nodes consists of the information extracted from source document automatically by open information extraction technology (Banko et al., 2007), especially Reverb, the famous Open IE system developed by University of Washington (Etzioni et al."
C14-1087,P10-1013,0,0.0125683,"., 2013). 2.2 Nodes in the Graph There are two kinds of nodes in the triple graph: source document nodes (sd-nodes) and background knowledge nodes (bk-nodes). Both of them are extracted automatically with Open Information Extraction (Open IE) technology which focuses on extracting assertions from massive corpora without a pre-specied vocabulary (Banko et al., 2007). Open IE systems are unlexicalized-formed only in terms of syntactic tokens and closed-word classes, instead of specific nouns and verbs at all costs. There are existing Open IE systems such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), and StatSnowball (Zhu et al., 2009). The output of these systems has been used to support many NLP tasks such as learning selectional preference (Ritter et al., 2010), acquiring sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010). In this work, we use the famous Open IE system Reverb (Etzioni et al., 2011), which is generated from TextRunner (Etzioni et al., 2008), to extract source document information and background knowledge automatically. We use the newest version of ReVerb (version 1.3) without modification, which is free download on-line 3 . Sour"
C14-1087,N10-1072,0,0.158765,"ge into source document. There are mainly two kinds of works in this topic according to the resource they relying on. The first line of works make use of WikiPedia and enrich source document by linking the entity to its corresponding Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007). In early stage, most researches rely on the similarity between the context of the mention and the definition of candidate entities by proposing different measuring criteria such as dot product, cosine similarity, KL divergence, Jaccard distance and more complicated ones (Bunescu and Pasca, 2006; Cucerzan, 2007; Zheng et al., 2010; Hoffart et al., 2011; Zhang et al., 2011). However, these methods mainly rely on text similarity but neglect the internal structure between mentions. So another kind of works explore the structure information with collective disambiguation (Kulkarni et al., 2009; Kataria et al., 2011; Sen, 2012; He et al., 2013). These methods make use of structure information within context and resolve different mentions based on the coherence among decisions. Despite the success, the entity linking methods rely on WikiPedia which has great coverage but less accuracy. Another line of works try to improve th"
C14-1129,N07-1039,0,0.193334,"外形” (appearance). According, h外形, 新颖i (happearance, noveli) is the T-P collocation. Generally, T-P collocation is a basic and complete sentiment unit, thus is very useful for many sentiment analysis applications. Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (AbATT basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation “Adj x Noun”, where the ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P collocation h外形, 新颖i (happearance, noveli) in the above sentiment sentence (Bloom et al., 2007; Qiu et al., 2011; Xu et al., 2013). However, one major problem of these approaches is the “naturalness” of sentiment sentences, that is, such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a) as an example, because the word “多亏” (fortunately) is so chatty,1 the parsing result is wrong. Thus, are unable to extract the T-P collocation h键盘, 好i (hke"
C14-1129,J92-4003,0,0.0373749,"ures (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence compression. For example, the ROOT relation typically indicates that the word should not be removed because it is the main verb of a sentence. 4 Experiments 4.1 Experimental Setup 4.1.1 Corpus We conducted the experiments on a Chinese corpus of four product domains, w"
C14-1129,C10-3004,1,0.802646,"3.1 Problem Analysis First, we conducted an error analysis for the results of current T-P collocation extraction, from which we observed that the “naturalness” of sentiment sentences is one of the main problems. For examples: • Chatty form: some sentiment sentences are so chatty, that they bring many difficulties to the parser. For example, in the sentence “多亏键盘好” (fortunately the keyboard is good) shown in Figure 1, the usage of the chatty word “多亏” (fortunately) affects the accuracy of the syntactic parser. 2 A Chinese natural language processing toolkit, Language Technology Platform (LTP) (Che et al., 2010), was used as our dependency parser. More information about the syntactic relations can be found in their paper. The state-of-the-art graph-based dependency parsing model, in the toolkit, was trained on Chinese Dependency Treebank 1.0 (LDC2012T05). 1362 ROOT ROOT ROOT ROOT SBV ADV comp POB 除了 照片 好 besides photo good ATT RAD POB SBV 照片 好 photo good SBV comp 屏幕 给 人 的 感觉 不错 screen for people feel good (a) parse tree 1 before and after compression SBV 屏幕 不错 screen good (b) parse tree 2 before and after compression Figure 3: “Naturalness” problem of sentiment sentences. • Conjunction word usage: co"
C14-1129,N13-1006,1,0.818055,"ehicle), which is also the suffix of the three words. Given that all of them may become targets, they tend to be retained in compressed sentences. The verbs, 感 觉 and 感 到, can be denoted by their prefix feel (感), and can be removed from original sentences because they are feeling words. We used word clustering features (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependenc"
C14-1129,1993.eamt-1.1,0,0.165369,"Missing"
C14-1129,P08-1109,0,0.0239906,"ing the Chinese data. 1360 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1360–1369, Dublin, Ireland, August 23-29 2014. ROOT ROOT SBV SBV VOB 多亏 键盘 好 fortunately keyboard good 键盘 好 keyboard good (a) before compression (b) after compression Figure 1: Parse trees before and after compression. This idea is motivated by the observation that, current syntactic parsers usually perform accurately for short, simple and formal sentences, whereas error rates increase for longer, more complex or more natural and spontaneous sentences (Finkel et al., 2008). Hence, the improvement in syntactic parsing performance would have a ripple effect over T-P collocation extraction. For example, we can compress the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part “多亏” (fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree is correct, making it easier to accurately extract T-P collocation. Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining important information (usually important grammar structure) (Jing, 2000). For example,"
C14-1129,N10-1131,0,0.0556384,"model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive approach, which compresses an original sentence by reorderi"
C14-1129,N07-1023,0,0.019638,"is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive a"
C14-1129,A00-1043,0,0.0568815,"ces (Finkel et al., 2008). Hence, the improvement in syntactic parsing performance would have a ripple effect over T-P collocation extraction. For example, we can compress the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part “多亏” (fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree is correct, making it easier to accurately extract T-P collocation. Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining important information (usually important grammar structure) (Jing, 2000). For example, the sentence “Overall, this is a great camera.” can be compressed into “This is a camera.” by removing the adverbial “overall” and the modifier “great”. However, the modifier “great” is a polarity word and very important for sentiment analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional compression models, because it needs to retain the important sentiment information, such as the polarity word. Hence, using Sent Comp, the above sentence can be compressed into “This is a great camera.” We regard Sent Comp as a sequence labeling task, whi"
C14-1129,P08-1068,0,0.0190562,"hree words. Given that all of them may become targets, they tend to be retained in compressed sentences. The verbs, 感 觉 and 感 到, can be denoted by their prefix feel (感), and can be removed from original sentences because they are feeling words. We used word clustering features (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out se"
C14-1129,D13-1047,0,0.0165015,"ly affect the performance of syntactic parser. Once our sentiment sentence compression method can improve the quality of parsing, the performance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sente"
C14-1129,E06-1038,0,0.441306,"mation and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence compression. For example, the ROOT relation typically indicates that the word should not be removed because it is the main verb of a sentence. 4 Experiments 4.1 Experimental Setup 4.1.1 Corpus We conducted the experiments on a Chinese corpus of four product domains, which came from the Task3 of the Chinese Opinion Analysis Evaluation (COAE) (Zhao et al., 2008).6 Table 2 describes the corpus, 5 6 www.keenage.com www.ir-china."
C14-1129,N04-1043,0,0.0112461,"various kinds of 车 (vehicle), which is also the suffix of the three words. Given that all of them may become targets, they tend to be retained in compressed sentences. The verbs, 感 觉 and 感 到, can be denoted by their prefix feel (感), and can be removed from original sentences because they are feeling words. We used word clustering features (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuiti"
C14-1129,C10-1089,0,0.0122581,"hod can improve the quality of parsing, the performance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem"
C14-1129,J11-1002,0,0.535699,"cording, h外形, 新颖i (happearance, noveli) is the T-P collocation. Generally, T-P collocation is a basic and complete sentiment unit, thus is very useful for many sentiment analysis applications. Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (AbATT basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation “Adj x Noun”, where the ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P collocation h外形, 新颖i (happearance, noveli) in the above sentiment sentence (Bloom et al., 2007; Qiu et al., 2011; Xu et al., 2013). However, one major problem of these approaches is the “naturalness” of sentiment sentences, that is, such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a) as an example, because the word “多亏” (fortunately) is so chatty,1 the parsing result is wrong. Thus, are unable to extract the T-P collocation h键盘, 好i (hkeyboard, goodi). To"
C14-1129,W13-3508,0,0.0116971,"compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive approach, which compresses an original sentence by reordering, substituting, and inserting, as well as removing (C"
C14-1129,P05-1036,0,0.0267549,". Note that, to date, there is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research l"
C14-1129,P08-1040,0,0.0661594,"modifier “great”. However, the modifier “great” is a polarity word and very important for sentiment analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional compression models, because it needs to retain the important sentiment information, such as the polarity word. Hence, using Sent Comp, the above sentence can be compressed into “This is a great camera.” We regard Sent Comp as a sequence labeling task, which can be solved by a conditional random fields (CRF) model. Instead of seeking the manual rules on parse trees for compression, as in other studies (Vickrey and Koller, 2008), this method is an automatic procedure. In this work, we introduce some sentiment-related features to retain the sentiment information for Sent Comp. We apply Sent Comp as the first step in the T-P collocation extraction task. First, we compress the sentiment sentences into easier to parse ones using Sent Comp, after which we employ the state-of-theart T-P collocation extraction approach on the compressed sentences. Experimental results on a Chinese corpus of four product domains show the effectiveness of our approach. The main contributions of this paper are as follows: • We present a framew"
C14-1129,D11-1038,0,0.0140719,"ntence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive approach, which compresses an original sentence by reordering, substituting, and inser"
C14-1129,P13-1173,0,0.156232,"(happearance, noveli) is the T-P collocation. Generally, T-P collocation is a basic and complete sentiment unit, thus is very useful for many sentiment analysis applications. Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (AbATT basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation “Adj x Noun”, where the ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P collocation h外形, 新颖i (happearance, noveli) in the above sentiment sentence (Bloom et al., 2007; Qiu et al., 2011; Xu et al., 2013). However, one major problem of these approaches is the “naturalness” of sentiment sentences, that is, such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a) as an example, because the word “多亏” (fortunately) is so chatty,1 the parsing result is wrong. Thus, are unable to extract the T-P collocation h键盘, 好i (hkeyboard, goodi). To solve the “natura"
C16-1002,Q16-1031,0,0.148922,"008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of ∼3K tokens. Their models may sacrifice accuracy on target"
C16-1002,P16-1231,0,0.0254544,"stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et al., 2015) for the following major reasons: 3 Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary. 14 Char-BiLSTM … ? ?? &lt;w&gt; ?? ?? Stack LSTM RecNN ?? ?? v e &lt;/w&gt; … ?? ??+1 … ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local features by modeling the full history inform"
C16-1002,D15-1041,0,0.052963,". (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. To the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks. 3.1 Transition-based Neural Parsing Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three primary components, a stack, a buffer and a set of dependency arcs. Tradition"
C16-1002,D12-1133,0,0.0622979,"r approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning was applied to sequence-to-"
C16-1002,W06-2920,0,0.0880729,"us referred to as deep multi-task learning. We find that different parameter sharing strategies should be applied for different typed source treebanks adaptively, due to the different types of consistencies and inconsistencies (Figure 1). We investigate the effect of multilingual transfer parsing using the Universal Dependency Treebanks (UDT) (McDonald et al., 2013). We show that our approach improves significantly over strong supervised baseline systems in six languages. We further study the effect of monolingual heterogeneous transfer parsing using UDT and the C O NLL-X shared task dataset (Buchholz and Marsi, 2006). We consider using UDT and CoNLL-X as source treebanks respectively, to investigate their mutual benefits. Experiment results show significant improvements under both settings. Moreover, indirect comparisons on the Chinese Penn Treebank 5.1 (CTB5) using the Chinese Dependency Treebank (CDT)1 as source treebank show the merits of our approach over previous work.2 2 Related Work The present work is related to several strands of previous studies. Monolingual resources for parsing Exploiting heterogeneous treebanks for parsing has been explored in various ways. Niu et al. (2009) automatically con"
C16-1002,D08-1092,0,0.0308853,"ank of ∼3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) and Vilares et al. (2016) instead train a single parser on a multilingual set of rich-resource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharin"
C16-1002,D14-1082,0,0.0307899,"g, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks. 3.1 Transition-based Neural Parsing Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three primary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as"
C16-1002,P10-1003,0,0.0239738,"e the target language has a small treebank of ∼3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) and Vilares et al. (2016) instead train a single parser on a multilingual set of rich-resource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving mul"
C16-1002,P15-1166,1,0.812783,"works can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. To the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that exte"
C16-1002,P15-2139,0,0.461231,"(Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of ∼3K tokens. Their models may sa"
C16-1002,D15-1040,0,0.405925,"(Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small treebank of ∼3K tokens. Their models may sa"
C16-1002,P15-1033,0,0.12877,"translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. To the best of our knowledge, we present the first work that successfully integrate both monolingual and multilingual treebanks for parsing, with or without consistent annotation schemes. 3 Approach This section describes the deep multi-task learning architecture, using a formalism that extends on the transition-based dependency parsing model with LSTM networks (Dyer et al., 2015) which is further enhanced by modeling characters (Ballesteros et al., 2015). We first revisit the parsing approach of Ballesteros et al. (2015), then present our framework for learning with multi-typed source treebanks. 3.1 Transition-based Neural Parsing Neural models for parsing have gained a lot of interests in recent years, particularly boosted by Chen and Manning (2014). The heart of transition-based parsing is the challenge of representing the state (configuration) of a transition system, based on which the most likely transition action is determined. Typically, a state includes three p"
C16-1002,P15-1119,1,0.866599,"en made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target"
C16-1002,P12-1110,0,0.0139446,"ilar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi"
C16-1002,J13-4006,0,0.0462287,"ial of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence m"
C16-1002,D09-1127,0,0.0242498,"ge has a small treebank of ∼3K tokens. Their models may sacrifice accuracy on target languages with a large treebank. Ammar et al. (2016) and Vilares et al. (2016) instead train a single parser on a multilingual set of rich-resource treebanks, which is a more similar setting to ours. We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks with"
C16-1002,N13-1013,0,0.666564,".e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks. Cross-lingual supervision has proven highly beneficial for parsing low-resource lan"
C16-1002,D11-1109,1,0.821766,"We refer to their approach as shallow multi-task learning (SMTL) and will include as one of our baseline systems (Section 4.2). Note that SMTL is a special case of our approach in which all tasks use the same set of parameters. Bilingual parallel data has also proven beneficial in various ways (Chen et al., 2010; Huang et al., 2009; Burkett and Klein, 2008), demonstrating the potential of cross-lingual transfer learning. Multi-task learning for NLP There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging and parsing (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural models for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. Henderson et al. (2013) present a joint dependency parsing and semantic role labeling model with the Incremental Sigmoid Belief Networks (ISBN) (Henderson and Titov, 2010). More recently, the idea of neural multi-task learning wa"
C16-1002,P12-1071,1,0.935143,"availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks. Cross-lingual supervision has p"
C16-1002,J93-2004,0,0.0535212,"datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models. 1 Introduction As a long-standing central problem in natural language processing (NLP), dependency parsing has been dominated by data-driven approaches for decades. The foundation of data-driven parsing is the availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingua"
C16-1002,D11-1006,0,0.0824227,"rsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual heterogeneous treebanks. Cross-lingual supervision has proven highly beneficial for parsing low-resource languages (Hwa et al., 2005; McDonald et al., 2011), implying that different languages have a great deal of common ground in grammars. But unfortunately, linguistic inconsistencies also exist in both typologies and lexical representations across languages. Figure 1(a) illustrates two sentences in German and English with universal dependency annotations. The typological differences (subject-verb-object order) results in the opposite directions of the dobj arcs, while the rest arcs remain consistent. Similar problems also come with monolingual heterogeneous treebanks. Figure 1(b) shows an English sentence annotated with respectively the universa"
C16-1002,P09-1006,1,0.95269,"decades. The foundation of data-driven parsing is the availability and scale of annotated training data (i.e., treebanks). Numerous efforts have been made towards the construction of treebanks which established the benchmark research on dependency parsing, such as the widely-used Penn Treebank (Marcus et al., 1993). However, the heavy cost of treebanking typically limits the existing treebanks in both scale and coverage of languages. To address the problem, a variety of authors have proposed to exploit existing heterogeneous treebanks with different annotation schemes via grammar conversion (Niu et al., 2009), quasi-synchronous grammar features (Li et al., 2012) or shared feature representations (Johansson, 2013) for the enhancement of parsing models. Despite their effectiveness in specific datasets, these methods typically lack the scalability of exploiting richer source treebanks, such as cross-lingual treebanks. In this paper, we aim at developing a universal framework for transfer parsing that can exploit multityped source treebanks to improve parsing of a target treebank. Specifically, we will consider two kinds of source treebanks, that are multilingual universal treebanks and monolingual he"
C16-1002,P08-1108,0,0.0257172,"t feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt"
C16-1002,P09-1040,0,0.0260673,"hin the stack and buffer are modeled with a recursive neural network (RecNN) as described in Dyer et al. (2015). Next, a linear mapping (W) is applied to the concatenation of st , bt and at , and passed through a component-wise ReLU: pt = ReLU(W[st ; bt ; at ] + d) (2) Finally, the probability of next action z ∈ A(S, B) is estimated using a softmax function: p(z∣pt ) = exp(gz⊺ pt + qz ) Σz ′ ∈A(S,B) exp(gz⊺′ pt + qz ′ ) (3) where A(S, B) represents the set of valid actions given the current content in the stack and buffer. We apply the non-projective transition system originally introduced by Nivre (2009) since most of the treebanks we consider in this study has a noticeable proportion of non-projective trees. In the S WAPbased system, both the stack and buffer may contain tree fragments, so RecNN is applied both in S and B to obtain representations of each position. 3.2 Deep Multi-task Learning Multi-task learning (MTL) is the procedure of inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasks. It does this by learning tasks in parallel while using a shared representation. A good overview, especially focusing on"
C16-1002,petrov-etal-2012-universal,0,0.248618,"cific task with hierarchical abstractions, which gives us the flexibility to control parameter sharing in different levels accordingly. In this study, different parameter sharing strategies are applied according to the source and target treebanks being used. We consider two different scenarios: MTL with multilingual universal treebanks as source (M ULTI -U NIV) and MTL with monolingual heterogeneous treebanks as source (M ONO H ETERO). Table 1 presents our parameter sharing strategies for each setting. M ULTI -U NIV Multilingual universal treebanks are annotated with the same set of POS tags (Petrov et al., 2012), dependency relations, and share the same set of transition actions. However, the vocabularies (word, characters) are language-specific. Therefore, it makes sense to share the lookup tables (embeddings) of POS tags (Epos ), relations (Erel ) and actions (Eact ), but separate the character embeddings (Echar ) as well as the Char-BiLSTMs (BiLSTM(chars)). Additionally, linguistic typologies such as the order of subject-verb-object and adjective-noun (Figure 1(a)) varies across languages, which result in the divergence of inherent grammars of transition action sequences. So we set the action LSTM"
C16-1002,D15-1039,0,0.0201532,"ebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition"
C16-1002,D09-1086,0,0.0258726,"sent work is related to several strands of previous studies. Monolingual resources for parsing Exploiting heterogeneous treebanks for parsing has been explored in various ways. Niu et al. (2009) automatically convert the dependency-structure CDT into the phrasestructure style of CTB5 using a trained constituency parser on CTB5, and then combine the converted treebanks for constituency parsing. Li et al. (2012) capture the annotation inconsistencies among different treebanks by designing several types of transformation patterns, based on which they introduce quasi-synchronous grammar features (Smith and Eisner, 2009) to augment the baseline parsing models. Johansson (2013) also adopts the idea of parameter sharing to incorporate multiple treebanks. They focuse on parameter sharing at feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus mor"
C16-1002,N12-1052,0,0.0751652,"Missing"
C16-1002,C14-1175,0,0.0149779,"o multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks tha"
C16-1002,D07-1099,0,0.0297611,"… ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local features by modeling the full history information of a state with stack LSTMs. • By modeling words, stack, buffer and action sequence separately which indicate hierarchical abstractions of representations, we can control the information flow across tasks via parameter sharing with more flexibility (Section 3.2). Besides, we did not use the earlier ISBN parsing model (Titov and Henderson, 2007) due to its lack of scalability to large vocabulary. Figure 2(a) illustrates the transition-based parsing architecture using LSTMs. Bidirectional LSTMs are used for modeling the word representations (Figure 2(b)), which we refer to as Char-BiLSTMs henceforth. Char-BiLSTMs learn features for each word, and then the representation of each token can be calculated as: → ← Ð t] + b) x = ReLU(V[Ð w; w; (1) where t is the POS tag embedding. The token embeddings are then fed into subsequent LSTM layers to obtain representations of the stack, buffer and action sequence respectively referred to as st ,"
C16-1002,D08-1017,0,0.0311419,"on parameter sharing at feature-level with discrete representations, which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Amm"
C16-1002,P16-2069,0,0.0406272,"Missing"
C16-1002,P15-1032,0,0.011917,"te includes three primary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et al., 2015) for the following major reasons: 3 Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary. 14 Char-BiLSTM … ? ?? &lt;w&gt; ?? ?? Stack LSTM RecNN ?? ?? v e &lt;/w&gt; … ?? ??+1 … ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local feat"
C16-1002,D15-1213,0,0.0299657,"te different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing, but with a few important differences to our work. In both of their models, most of the neural network parameters are shared in two (or multiple) parsers except the feature embeddings,3 which ignores the important syntactical inconsistencies of different languages and is also inapplicable for heterogeneous treebanks that have different transition actions. Besides, Duong et al. (2015b) focus on low resource parsing where the target language has a small tree"
C16-1002,D08-1059,0,0.0374319,"which limits its scalability to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual"
C16-1002,C14-1051,1,0.858613,"bility to multilingual treebanks where feature surfaces might be totally different. On the contrary, our approach is capable of utilizing representation-level parameter sharing, making full use of the multi-level abstractive representations generated by deep neural network. This is the key that makes our framework scalable to multi-typed treebanks and thus more practically useful. Aside from resource utilization, attempts have also been made to integrate different parsing models through stacking (Torres Martins et al., 2008; Nivre and McDonald, 2008) or joint inference (Zhang and Clark, 2008; Zhang et al., 2014). 1 2 https://catalog.ldc.upenn.edu/LDC2012T05 Our code is available at: https://github.com/jiangfeng1124/mtl-nndep. 13 Multilingual resources for parsing Cross-lingual transfer has proven to be a promising way of inducing parsers for low-resource languages, either through data transfer (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015) or model transfer (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Guo et al., 2015; Zhang and Barzilay, 2015; Guo et al., 2016). Duong et al. (2015b) and Ammar et al. (2016) both adopt parameter sharing to exploit multilingual treebanks in parsing"
C16-1002,P15-1117,0,0.055438,"imary components, a stack, a buffer and a set of dependency arcs. Traditional parsing models deal with features extracted from manually defined feature templates in a discrete feature space, which suffers from the problems of Sparsity, Incompleteness and Expensive feature computation. The neural network model proposed by Chen and Manning (2014) instead represents features as continuous, low-dimensional vectors and use a cube activation function for implicit feature composition. More recently, this architecture has been improved in several different ways (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015; Andor et al., 2016). Here, we employ the LSTM-based architecture enhanced with character bidirectional LSTMs (Ballesteros et al., 2015) for the following major reasons: 3 Duong et al. (2015b) used L2 regularizers to tie the lexical embeddings with a bilingual dictionary. 14 Char-BiLSTM … ? ?? &lt;w&gt; ?? ?? Stack LSTM RecNN ?? ?? v e &lt;/w&gt; … ?? ??+1 … ?1 o Buffer LSTM Action LSTM … l ?? ?? ? ? ?1 ? ? (a) (b) Figure 2: The LSTM-based neural parser (a) and the Char-BiLSTM for modeling words (b). • Compared with Chen & Manning’s architecture, it makes full use of the non-local features by modeling th"
C16-1027,N01-1016,0,0.880966,"Missing"
C16-1027,C10-3004,1,0.742669,"itchboard data. and ‘i mean’ into single token. Automatic POS-tags generated from pocket crf (Qian and Liu, 2013) are used as POS-tag in our experiments. No public Chinese corpus is available now. For our Chinese experiments, we collect about 200k spoken sentences from minutes of meetings and annotate them with only disfluency annotations according to the guideline proposed by (Meteer et al., 1995). We respectively select about 20k sentences for development and testing. The rest are used for training. We use the word segmentation and POS-tag tools provided by the Language Technology Platform (Che et al., 2010) for preprocessing the original data in our experiments. Metric. Following previous works (Ferguson et al., 2015; Wu et al., 2015), token-based precision (P), recall (R), and f-score (F1) are used as the evaluation metrics. 5.2 Performance of disfluency detection on English Swtichboard corpus We build a baseline system using the Conditional Random Field (CRF) model. The hand-crafted discrete features of our CRF refer to those in (Ferguson et al., 2015). Table 3 shows the result of our model on both the development and test set. We compare our neural attention-based model to four previous top p"
C16-1027,2013.iwslt-papers.12,0,0.632629,"Missing"
C16-1027,D14-1179,0,0.0408974,"Missing"
C16-1027,P15-1033,0,0.0303834,"nce xk is selected, we will use it to update the state of the decoder RNN and select another word from the words ranging from 282 (xk+1 , ..., xl ) (as shown in Figure 2). To learn the parameters of our neural attention-based model, we minimize the negative log-probability of the output sequence over the input data {(xi , yi )}N n=1 during training: − N X log(p(yi |xi )) = − i=1 N X T Y log( sof tmax(ut )) i=1 (7) t=1 The detailed learning algorithm is shown in Algorithm 1. 4 Network training 4.1 Parameters Pretrained word embedding. There are lots of methods for creating word embeddings. As (Dyer et al., 2015) does, we use a variant of the skip n-gram model introduced by (Ling et al., 2015), named “structured skip n-gram”, where a different set of parameters are used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov et al., 2013). We set the window size to 5, and use a negative sampling rate to 10. The AFP portion of English Gigaword corpus (version 5) is used as the training corpus. Hyper-Parameters. The LSTMs of both encoder and decoder has two hidden layers and"
C16-1027,N15-1029,0,0.627938,"ous disfluency detection works focus on detecting the repair type disfluencies. A flight to um mean Denver ! Boston !&quot;# I!&quot;# !&quot; $ $ # Tuesday FP RM IM RP Figure 1: A sentence with disfluencies annotated in the style of (Shriberg, 1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on C"
C16-1027,N09-2028,0,0.214941,"n works focus on detecting the repair type disfluencies. A flight to um mean Denver ! Boston !&quot;# I!&quot;# !&quot; $ $ # Tuesday FP RM IM RP Figure 1: A sentence with disfluencies annotated in the style of (Shriberg, 1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on Computational Lin"
C16-1027,Q14-1011,0,0.620852,"Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 278–287, Osaka, Japan, December 11-17 2016. they/E had/E they/E used/E to/E have/E well they do have television monitors stationed throughout our buildings. and the/E other/E one/E is/E her husband"
C16-1027,P04-1005,0,0.474051,"hinese annotated corpus In addition to English experiments, we also apply our method on Chinese annotated data. As there is no standard Chinese corpus, no Chinese experimental results are reported in (Honnibal and Johnson, 2014) and (Qian and Liu, 2013). We only use the CRF-based labeling model as our baselines. Table 6 shows the results of Chinese disfluency detection. Our models outperform the CRF model by more than 7 points on f-score which shows that our method is more effective. 6 Related Work Most related works on disfluency detection are aimed at detecting repair type of disfluencies. (Johnson and Charniak, 2004) proposed a TAG-based noisy channel model for disfluency detection. The TAG model was used to find rough copies. Following the work of (Johnson and Charniak, 2004), (Zwarts and Johnson, 2011) extended the TAG model using minimal expected f-loss oriented n-best reranking with additional corpus for language model training. (Qian and Liu, 2013) proposed a muiti-step learning method using weighted max-margin markov network (M3 N). They showed that M3 N model outperformed many other labeling models such as CRF model. (Ferguson et al., 2015) used the Semi-Markov CRF model for disfluency detection an"
C16-1027,N06-2019,0,0.687698,"nd Johnson, 2011) extended the TAG model using minimal expected f-loss oriented n-best reranking with additional corpus for language model training. (Qian and Liu, 2013) proposed a muiti-step learning method using weighted max-margin markov network (M3 N). They showed that M3 N model outperformed many other labeling models such as CRF model. (Ferguson et al., 2015) used the Semi-Markov CRF model for disfluency detection and achieved high f-score by integrating prosodic features. Many syntax-based approaches have been proposed which jointly perform dependency parsing and disfluency detection. (Lease and Johnson, 2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disfluencies. (Rasooli and Tetreault, 2013) designed a joint model for both disfluency detection and dependency parsing. (Honnibal and Johnson, 2014) presented a new joint model by extending the original transition actions with a new “Edit” transition. This model achieved good performance on both disfluency detection and parsing. (Wu et al., 2015) proposed a right-toleft transition-based joint method and achieved the state-of-the-art performance compared with previous syntax-based approaches. RNN had been u"
C16-1027,N15-1142,0,0.0156349,"ct another word from the words ranging from 282 (xk+1 , ..., xl ) (as shown in Figure 2). To learn the parameters of our neural attention-based model, we minimize the negative log-probability of the output sequence over the input data {(xi , yi )}N n=1 during training: − N X log(p(yi |xi )) = − i=1 N X T Y log( sof tmax(ut )) i=1 (7) t=1 The detailed learning algorithm is shown in Algorithm 1. 4 Network training 4.1 Parameters Pretrained word embedding. There are lots of methods for creating word embeddings. As (Dyer et al., 2015) does, we use a variant of the skip n-gram model introduced by (Ling et al., 2015), named “structured skip n-gram”, where a different set of parameters are used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov et al., 2013). We set the window size to 5, and use a negative sampling rate to 10. The AFP portion of English Gigaword corpus (version 5) is used as the training corpus. Hyper-Parameters. The LSTMs of both encoder and decoder has two hidden layers and their dimensions are set to 100. Pretrained word embeddings have 100 dimensions a"
C16-1027,N13-1102,0,0.357147,"detecting the repair type disfluencies. A flight to um mean Denver ! Boston !&quot;# I!&quot;# !&quot; $ $ # Tuesday FP RM IM RP Figure 1: A sentence with disfluencies annotated in the style of (Shriberg, 1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical P"
C16-1027,D13-1013,0,0.761413,"guage model training. (Qian and Liu, 2013) proposed a muiti-step learning method using weighted max-margin markov network (M3 N). They showed that M3 N model outperformed many other labeling models such as CRF model. (Ferguson et al., 2015) used the Semi-Markov CRF model for disfluency detection and achieved high f-score by integrating prosodic features. Many syntax-based approaches have been proposed which jointly perform dependency parsing and disfluency detection. (Lease and Johnson, 2006) involved disfluency detection in a PCFG parser to parse the input along with detecting disfluencies. (Rasooli and Tetreault, 2013) designed a joint model for both disfluency detection and dependency parsing. (Honnibal and Johnson, 2014) presented a new joint model by extending the original transition actions with a new “Edit” transition. This model achieved good performance on both disfluency detection and parsing. (Wu et al., 2015) proposed a right-toleft transition-based joint method and achieved the state-of-the-art performance compared with previous syntax-based approaches. RNN had been used to disfluency detection. (Hough and Schlangen, 2015) explored incremental detection, with an objective that combines detection"
C16-1027,D15-1044,0,0.043799,"Missing"
C16-1027,P15-1048,0,0.771224,"ed Pause, RM=Reparandum, IM=Interregnum, RP=Repair. We follow previous works in evaluating the system on the accuracy with which it identifies speech-repairs, marked reparandum above. Modeling long-range dependencies between repair phrases is one of the core problems for disfluency detection. Previous sequence tagging methods (Ferguson et al., 2015; Georgila, 2009; Qian and Liu, 2013) require carefully designed features to capture the information of long distance, but usually suffer from the sparsity problem. Another line of syntax-based disfluency detection works (Honnibal and Johnson, 2014; Wu et al., 2015) try to model the repair phrases on a syntax tree by compressing the unrelated * Corresponding author: Wanxiang Che This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 278 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 278–287, Osaka, Japan, December 11-17 2016. they/E had/E they/E used/E to/E have/E well they do have television monitors stationed throughout our buildings. and the/E other/E one/E is/E her husband is in the navy. th"
C16-1027,P11-1071,0,0.0538044,"ted in (Honnibal and Johnson, 2014) and (Qian and Liu, 2013). We only use the CRF-based labeling model as our baselines. Table 6 shows the results of Chinese disfluency detection. Our models outperform the CRF model by more than 7 points on f-score which shows that our method is more effective. 6 Related Work Most related works on disfluency detection are aimed at detecting repair type of disfluencies. (Johnson and Charniak, 2004) proposed a TAG-based noisy channel model for disfluency detection. The TAG model was used to find rough copies. Following the work of (Johnson and Charniak, 2004), (Zwarts and Johnson, 2011) extended the TAG model using minimal expected f-loss oriented n-best reranking with additional corpus for language model training. (Qian and Liu, 2013) proposed a muiti-step learning method using weighted max-margin markov network (M3 N). They showed that M3 N model outperformed many other labeling models such as CRF model. (Ferguson et al., 2015) used the Semi-Markov CRF model for disfluency detection and achieved high f-score by integrating prosodic features. Many syntax-based approaches have been proposed which jointly perform dependency parsing and disfluency detection. (Lease and Johnson"
C16-1076,D13-1181,0,0.0254912,"tences to measure semantic similarity of texts, such as the tree kernel emthods (Moschitti, 2006; Mooney and Bunescu, 2005; Culotta and Sorensen, 2004). 6.3 Text Quality Analysis Some work focuses on dealing with rhetorical device such as recognizing metaphor in texts (Shutova, 2010). Parallelism is also an important rhetorical device. Hobbs and Kehler (1997) study the clause level parallelism. However, little work has been done on sentence-level parallelism identification. These is work on predicting the quality of articles (Louis and Nenkova, 2013; Pitler and Nenkova, 2008), writing styles (Ashok et al., 2013) and student essays (Attali and Burstein, 2006). They mainly use simple shallow features, but seldomly use rhetorical device related features. Automated rhetorical device analysis should help to improve the above tasks. 7 Conclusion We have investigated identifying sentence parallelism in student essays. We adopt machine learning to learn a prediction model based on an annotated dataset. We study various alignment measures and different word alignment strategies for deriving features. The evaluation demonstrates that our proposed method can effectively identify sentence parallelism, achieving"
C16-1076,W02-1022,0,0.0617726,"Nonetheless, the observations should potentially help to design features for automated writing evaluation. 801 6 Related Work 6.1 Sequence Alignment Finding the common parts among sequences have been a set of classic computer science problems. The typical problems include finding the longest common subsequence (Hirschberg, 1977), longest common substring and multiple sequence alignment (Carrillo and Lipman, 1988; Needleman and Wunsch, 1970). These techniques are commonly used in computational biology and also applied to natural language processing for constructing concept mapping dictionary (Barzilay and Lee, 2002), identifying sentence level paraphrases (Barzilay and Lee, 2003) and modeling the organization of student essays (Persing et al., 2010). In this work, we exploit these alignment measures for deriving features, since parallel sentences should have a kind of alignment. 6.2 Semantic Similarity of Texts A large of body of previous work focuses on measuring the semantic similarity of texts. Semantic similarity of text usually depends on exploiting the semantic similarity of words and concepts (Corley and Mihalcea, 2005; Mitchell and Lapata, 2008). While the semantic similarity of words and concept"
C16-1076,N03-1003,0,0.0963243,"features for automated writing evaluation. 801 6 Related Work 6.1 Sequence Alignment Finding the common parts among sequences have been a set of classic computer science problems. The typical problems include finding the longest common subsequence (Hirschberg, 1977), longest common substring and multiple sequence alignment (Carrillo and Lipman, 1988; Needleman and Wunsch, 1970). These techniques are commonly used in computational biology and also applied to natural language processing for constructing concept mapping dictionary (Barzilay and Lee, 2002), identifying sentence level paraphrases (Barzilay and Lee, 2003) and modeling the organization of student essays (Persing et al., 2010). In this work, we exploit these alignment measures for deriving features, since parallel sentences should have a kind of alignment. 6.2 Semantic Similarity of Texts A large of body of previous work focuses on measuring the semantic similarity of texts. Semantic similarity of text usually depends on exploiting the semantic similarity of words and concepts (Corley and Mihalcea, 2005; Mitchell and Lapata, 2008). While the semantic similarity of words and concepts are learned based on distributional statistics (Lin, 1998; Pad´"
C16-1076,J96-2004,0,0.208265,"onsists of multiple sentences. The labelers recognized the sentence parallelism chunks in essays and assigned a distinct number to all parallel sentences from the same chunk in order to distinguish different chunks. Item #Essay avg. #sentence per essay avg.#parallelism chunk per essay avg. #sentence per chunk Number 544 28.47 2.03 2.68 Table 1: Statistics of the annotated sentence parallelism dataset. After annotation, we collected 544 student essays, each of which has at least one sentence parallelism chunk. 30 essays were annotated by both labelers, and the Kappa value between them is 0.71 (Carletta, 1996), which indicates a moderate consistence. The mainly disagreement lies in their different judgement standards in terms of the quality of parallelism between sentences. After discussion and reaching a consensus, they reviewed all the annotations. Table 1 shows the basic statistics of the dataset. 3 Sentence Parallelism Identification We cast sentence parallelism identification as a classification problem. Given an essay, we conduct a binary classification for every pair of sentences to determine whether they are parallel or non-parallel. Further, we get parallelism chunks according to the resul"
C16-1076,C10-3004,1,0.811532,"COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 794–803, Osaka, Japan, December 11-17 2016. 2 Data We collected student essays written by Chinese students from a senior high school during mock examinations. The essay types include narrative and argumentative essays, covering multiple topics. Two labelers were asked to label parallelism in randomly sampled essays at sentence level. They were guided by the definition of parallelism. Sentences are obtained by the sentence splitter provided by the Chinese language processing toolkit — HIT-LTP (Che et al., 2010). If a sentence contains less than four words, it is not allowed to be labeled. A sentence parallelism chunk consists of multiple sentences. The labelers recognized the sentence parallelism chunks in essays and assigned a distinct number to all parallel sentences from the same chunk in order to distinguish different chunks. Item #Essay avg. #sentence per essay avg.#parallelism chunk per essay avg. #sentence per chunk Number 544 28.47 2.03 2.68 Table 1: Statistics of the annotated sentence parallelism dataset. After annotation, we collected 544 student essays, each of which has at least one sen"
C16-1076,W05-1203,0,0.0647054,"plied to natural language processing for constructing concept mapping dictionary (Barzilay and Lee, 2002), identifying sentence level paraphrases (Barzilay and Lee, 2003) and modeling the organization of student essays (Persing et al., 2010). In this work, we exploit these alignment measures for deriving features, since parallel sentences should have a kind of alignment. 6.2 Semantic Similarity of Texts A large of body of previous work focuses on measuring the semantic similarity of texts. Semantic similarity of text usually depends on exploiting the semantic similarity of words and concepts (Corley and Mihalcea, 2005; Mitchell and Lapata, 2008). While the semantic similarity of words and concepts are learned based on distributional statistics (Lin, 1998; Pad´o and Lapata, 2007). Recently, neural networks based methods are proposed to learn the distributed representation of words on large scale of corpus (Mikolov et al., 2013). The learned word embeddings enable similar words to have a close distance in the vector space. There is also work on sentential paraphrase identification (Madnani and Dorr, 2010). Paraphrases are different expressions that convey the same meaning. Although it is similar to our task,"
C16-1076,P04-1054,0,0.0838836,"ings enable similar words to have a close distance in the vector space. There is also work on sentential paraphrase identification (Madnani and Dorr, 2010). Paraphrases are different expressions that convey the same meaning. Although it is similar to our task, the goals are different, since parallel sentences are not expected to have the same meaning and paraphrases are not required to have similar structures. Many researchers also exploit the structural properties of sentences to measure semantic similarity of texts, such as the tree kernel emthods (Moschitti, 2006; Mooney and Bunescu, 2005; Culotta and Sorensen, 2004). 6.3 Text Quality Analysis Some work focuses on dealing with rhetorical device such as recognizing metaphor in texts (Shutova, 2010). Parallelism is also an important rhetorical device. Hobbs and Kehler (1997) study the clause level parallelism. However, little work has been done on sentence-level parallelism identification. These is work on predicting the quality of articles (Louis and Nenkova, 2013; Pitler and Nenkova, 2008), writing styles (Ashok et al., 2013) and student essays (Attali and Burstein, 2006). They mainly use simple shallow features, but seldomly use rhetorical device related"
C16-1076,P97-1051,0,0.252729,"e meaning. Although it is similar to our task, the goals are different, since parallel sentences are not expected to have the same meaning and paraphrases are not required to have similar structures. Many researchers also exploit the structural properties of sentences to measure semantic similarity of texts, such as the tree kernel emthods (Moschitti, 2006; Mooney and Bunescu, 2005; Culotta and Sorensen, 2004). 6.3 Text Quality Analysis Some work focuses on dealing with rhetorical device such as recognizing metaphor in texts (Shutova, 2010). Parallelism is also an important rhetorical device. Hobbs and Kehler (1997) study the clause level parallelism. However, little work has been done on sentence-level parallelism identification. These is work on predicting the quality of articles (Louis and Nenkova, 2013; Pitler and Nenkova, 2008), writing styles (Ashok et al., 2013) and student essays (Attali and Burstein, 2006). They mainly use simple shallow features, but seldomly use rhetorical device related features. Automated rhetorical device analysis should help to improve the above tasks. 7 Conclusion We have investigated identifying sentence parallelism in student essays. We adopt machine learning to learn a"
C16-1076,Q13-1028,0,0.108614,"ures. Many researchers also exploit the structural properties of sentences to measure semantic similarity of texts, such as the tree kernel emthods (Moschitti, 2006; Mooney and Bunescu, 2005; Culotta and Sorensen, 2004). 6.3 Text Quality Analysis Some work focuses on dealing with rhetorical device such as recognizing metaphor in texts (Shutova, 2010). Parallelism is also an important rhetorical device. Hobbs and Kehler (1997) study the clause level parallelism. However, little work has been done on sentence-level parallelism identification. These is work on predicting the quality of articles (Louis and Nenkova, 2013; Pitler and Nenkova, 2008), writing styles (Ashok et al., 2013) and student essays (Attali and Burstein, 2006). They mainly use simple shallow features, but seldomly use rhetorical device related features. Automated rhetorical device analysis should help to improve the above tasks. 7 Conclusion We have investigated identifying sentence parallelism in student essays. We adopt machine learning to learn a prediction model based on an annotated dataset. We study various alignment measures and different word alignment strategies for deriving features. The evaluation demonstrates that our proposed"
C16-1076,J10-3003,0,0.0270402,"mantic similarity of text usually depends on exploiting the semantic similarity of words and concepts (Corley and Mihalcea, 2005; Mitchell and Lapata, 2008). While the semantic similarity of words and concepts are learned based on distributional statistics (Lin, 1998; Pad´o and Lapata, 2007). Recently, neural networks based methods are proposed to learn the distributed representation of words on large scale of corpus (Mikolov et al., 2013). The learned word embeddings enable similar words to have a close distance in the vector space. There is also work on sentential paraphrase identification (Madnani and Dorr, 2010). Paraphrases are different expressions that convey the same meaning. Although it is similar to our task, the goals are different, since parallel sentences are not expected to have the same meaning and paraphrases are not required to have similar structures. Many researchers also exploit the structural properties of sentences to measure semantic similarity of texts, such as the tree kernel emthods (Moschitti, 2006; Mooney and Bunescu, 2005; Culotta and Sorensen, 2004). 6.3 Text Quality Analysis Some work focuses on dealing with rhetorical device such as recognizing metaphor in texts (Shutova,"
C16-1076,P08-1028,0,0.0555057,"rocessing for constructing concept mapping dictionary (Barzilay and Lee, 2002), identifying sentence level paraphrases (Barzilay and Lee, 2003) and modeling the organization of student essays (Persing et al., 2010). In this work, we exploit these alignment measures for deriving features, since parallel sentences should have a kind of alignment. 6.2 Semantic Similarity of Texts A large of body of previous work focuses on measuring the semantic similarity of texts. Semantic similarity of text usually depends on exploiting the semantic similarity of words and concepts (Corley and Mihalcea, 2005; Mitchell and Lapata, 2008). While the semantic similarity of words and concepts are learned based on distributional statistics (Lin, 1998; Pad´o and Lapata, 2007). Recently, neural networks based methods are proposed to learn the distributed representation of words on large scale of corpus (Mikolov et al., 2013). The learned word embeddings enable similar words to have a close distance in the vector space. There is also work on sentential paraphrase identification (Madnani and Dorr, 2010). Paraphrases are different expressions that convey the same meaning. Although it is similar to our task, the goals are different, si"
C16-1076,J07-2002,0,0.153971,"Missing"
C16-1076,D10-1023,0,0.146785,"e Alignment Finding the common parts among sequences have been a set of classic computer science problems. The typical problems include finding the longest common subsequence (Hirschberg, 1977), longest common substring and multiple sequence alignment (Carrillo and Lipman, 1988; Needleman and Wunsch, 1970). These techniques are commonly used in computational biology and also applied to natural language processing for constructing concept mapping dictionary (Barzilay and Lee, 2002), identifying sentence level paraphrases (Barzilay and Lee, 2003) and modeling the organization of student essays (Persing et al., 2010). In this work, we exploit these alignment measures for deriving features, since parallel sentences should have a kind of alignment. 6.2 Semantic Similarity of Texts A large of body of previous work focuses on measuring the semantic similarity of texts. Semantic similarity of text usually depends on exploiting the semantic similarity of words and concepts (Corley and Mihalcea, 2005; Mitchell and Lapata, 2008). While the semantic similarity of words and concepts are learned based on distributional statistics (Lin, 1998; Pad´o and Lapata, 2007). Recently, neural networks based methods are propos"
C16-1076,D08-1020,0,0.0362403,"so exploit the structural properties of sentences to measure semantic similarity of texts, such as the tree kernel emthods (Moschitti, 2006; Mooney and Bunescu, 2005; Culotta and Sorensen, 2004). 6.3 Text Quality Analysis Some work focuses on dealing with rhetorical device such as recognizing metaphor in texts (Shutova, 2010). Parallelism is also an important rhetorical device. Hobbs and Kehler (1997) study the clause level parallelism. However, little work has been done on sentence-level parallelism identification. These is work on predicting the quality of articles (Louis and Nenkova, 2013; Pitler and Nenkova, 2008), writing styles (Ashok et al., 2013) and student essays (Attali and Burstein, 2006). They mainly use simple shallow features, but seldomly use rhetorical device related features. Automated rhetorical device analysis should help to improve the above tasks. 7 Conclusion We have investigated identifying sentence parallelism in student essays. We adopt machine learning to learn a prediction model based on an annotated dataset. We study various alignment measures and different word alignment strategies for deriving features. The evaluation demonstrates that our proposed method can effectively iden"
C16-1076,P10-1071,0,0.0162221,"rr, 2010). Paraphrases are different expressions that convey the same meaning. Although it is similar to our task, the goals are different, since parallel sentences are not expected to have the same meaning and paraphrases are not required to have similar structures. Many researchers also exploit the structural properties of sentences to measure semantic similarity of texts, such as the tree kernel emthods (Moschitti, 2006; Mooney and Bunescu, 2005; Culotta and Sorensen, 2004). 6.3 Text Quality Analysis Some work focuses on dealing with rhetorical device such as recognizing metaphor in texts (Shutova, 2010). Parallelism is also an important rhetorical device. Hobbs and Kehler (1997) study the clause level parallelism. However, little work has been done on sentence-level parallelism identification. These is work on predicting the quality of articles (Louis and Nenkova, 2013; Pitler and Nenkova, 2008), writing styles (Ashok et al., 2013) and student essays (Attali and Burstein, 2006). They mainly use simple shallow features, but seldomly use rhetorical device related features. Automated rhetorical device analysis should help to improve the above tasks. 7 Conclusion We have investigated identifying"
C16-1120,C10-3009,0,0.119928,"ctic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015), which have shown great effectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) us"
C16-1120,D12-1133,0,0.0489228,"(2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to sequence-to-sequence pro"
C16-1120,H05-1091,0,0.100275,"generated by our model to get the global optimization. We use the three constraints defined in Che et al. (2008): • C1: Each word should be labeled with one and only one label (including NULL). • C2: Roles with a small probability (smaller than 0.3) should never be labeled (except for NULL). • C3: Some roles (except for NULL) usually appear once for a predicate in a sentence. Hence a non-duplicate-roles list is utilized for each language. 5 Multi-task Learning The commonalities between SRL and RC inspire us to explore their potential mutual benefits. According to the Shortest Path Hypothesis (Bunescu and Mooney, 2005), if e1 and e2 are two entities mentioned in the same sentence such that they are observed to be in a certain relationship R, they often indicate two arguments of the same predicate or a sequence of predicates. To gain more insights, let’s look at the following example in RC: 1268 Instrument-Agency(e2 , e1 ) “The author[e1 ] of a keygen uses a disassembler[e2 ] to look at the raw assembly code.” Here, the “Instrument-Agency” relation provides significant evidences that author and disassembler are two arguments of a certain predicate, most likely with semantic roles A0 (agent) and A1 (patient)."
C16-1120,W08-2134,1,0.792996,"¶ ´                           ¸                            ¶ (4) p(c∣p) = softmax(gc⊺ p + qc ) (5) Global Context Syntactic Path Our model is trained by minimizing the cross-entropy loss: L(θ) = − ∑N i=0 log p(ci ∣pi ), where N is number of training instances. 4.4 Post-Inference with Integer Linear Programming for SRL SRL is a structure prediction problem and the predicted results should satisfy some structural constraints. For instance, some roles only appear once for a predicate in a sentence. Following Punyakanok et al. (2004) and Che et al. (2008), we apply ILP on the probability distributions at each token generated by our model to get the global optimization. We use the three constraints defined in Che et al. (2008): • C1: Each word should be labeled with one and only one label (including NULL). • C2: Roles with a small probability (smaller than 0.3) should never be labeled (except for NULL). • C3: Some roles (except for NULL) usually appear once for a predicate in a sentence. Hence a non-duplicate-roles list is utilized for each language. 5 Multi-task Learning The commonalities between SRL and RC inspire us to explore their potentia"
C16-1120,W09-1207,1,0.825439,"C, which effectively captures global contextual features, syntactical features and lexical semantic features. • We show that SRL can be significantly improved by jointly training with RC, reaching new stateof-the-art performance. 2 Related Work The present work ties together several strands of previous studies. Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introdu"
C16-1120,C10-3004,1,0.762732,"rix-Vector Recursive Neural Network (MV-RNN) model of Socher et al. (2012), the CNN model of Zeng et al. (2014), the tensor-based model of Yu et al. (2014), the CNN model using ranking loss (dos Santos et al., 2015), and the dependency-based neural network models (Liu et al., 2015; Xu et al., 2015b). Word embeddings are pretrained using word2vec on large-scale unlabeled data. For English, Catalan, German and Spanish, we use the latest Wikipedia data. For Chinese, we obtain the raw text from Xinhua news section (2000–2010) of the fifth edition of Chinese Gigaword (LDC2011T13). The LTP toolkit (Che et al., 2010) is applied to segment Chinese text into words. We adopt predicate-wise training for SRL and sentence-wise training for RC, and use stochastic gradient descent for optimization. Initial learning rate is set to η0 = 0.1 and updated as ηt = η0 /(1 + 0.1t) on each epoch t. Our hyperparameters for the unified model are listed in Table 1. When training RC-only models, the LSTM input/hidden dimension is set to 200, and the dimension of hidden layer is 400. Dimension of embeddings word POS NE WordNet 200 25 25 25 Dimension of layers LSTM input LSTM hidden hidden 100 100 200 Table 1: Hyperparameters s"
C16-1120,J81-4005,0,0.764154,"Missing"
C16-1120,P15-1166,1,0.725108,"., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. Liu et al. (2016) incorporate different kinds of corpus for implicit discourse relation classification using multi-task neural networks. More recently, multi-task learning has also been applied to sentence compression (Klerke et al., 2016) and machine translation quality estimation (Shah and"
C16-1120,P15-1061,0,0.10748,"midhuber (1997). The LSTMs take as input the token representation xi in each position. The hidden state vectors of the two directions’ LSTM units corresponding to each target word are then concatenated as its global context representation: Ð → ← Ð Ð → ← Ð Rgc Rgc (2) e1 = [ h e1 ; h e1 ]; e2 = [ h e2 ; h e2 ] Note that an important difference between our model and previous neural models is that we utilize the hidden state vectors of e1 and e2 instead of the representation of the whole sentence, which frees us from using position-related features (Zeng et al., 2014; Collobert et al., 2011; dos Santos et al., 2015). 4.3 Syntactic Path Representation We define the nearest common ancestor token of e1 and e2 as nca(e1 , e2 ). Then the path from e1 , e2 to nca(e1 , e2 ), i.e., e1 → . . . → nca(e1 , e2 ) and nca(e1 , e2 ) ← . . . ← e2 , are also modeled with bidirectional LSTMs, as shown in Figure 2 (right panel). We use two kinds of syntactic paths, including a generic path that takes the token representation xi as input, and a relation path that takes the dependency relations along the path as input (Figure 2). These two paths are modeled with BiLSTMgen and BiLSTMrel respectively. The hidden state vectors"
C16-1120,D15-1112,0,0.036994,"Missing"
C16-1120,J02-3001,0,0.823726,"owards the understanding of natural language sentences. Multi-typed semantic relations have been defined between two terms in a sentence in natural language processing (NLP) to promote various applications. For instance, the task of Semantic Role Labeling (SRL) defines shallow semantic dependencies between arguments and predicates, identifying the semantic roles, e.g., who did what to whom, where, when, and how. SRL has been a long-standing and challenging problem in NLP, primarily because it is strongly dependent on rich contextual and syntactical features used by the underlying classifiers (Gildea and Jurafsky, 2002). Another instance is Relation Classification (RC) which assigns sentences with two marked entities (or nominals) to a predefined set of relations (Hendrickx et al., 2010). Compared with SRL, relations defined in RC express much deeper semantics. Figure 1 shows example annotations of SRL and RC respectively. These two problems are typically studied separately in different communities. Hence the connections between them are neglected, both in data resources and approaches. In this paper, we show that SRL and RC have a lot of common ground and can be modeled with a unified model. We start by loo"
C16-1120,P12-1110,0,0.0555057,"16). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learnin"
C16-1120,J13-4006,0,0.0450169,"or-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to sequence-to-sequence problems with recurrent neu"
C16-1120,S10-1006,0,0.134728,"Missing"
C16-1120,N16-1179,0,0.0329688,"e-to-sequence problems with recurrent neural networks. Dong et al. (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. Liu et al. (2016) incorporate different kinds of corpus for implicit discourse relation classification using multi-task neural networks. More recently, multi-task learning has also been applied to sentence compression (Klerke et al., 2016) and machine translation quality estimation (Shah and Specia, 2016). 3 Problem Definition This section gives formal definitions of the two tasks to be investigated: SRL and RC. 3.1 Semantic Role Labeling We follow the setup of the CoNLL-2009 shared task. Given a sentence s, each token is annotated with a predicated POS tag and predicted word lemma. Some tokens are also marked as predicates. Besides, 1266 UNESCO ?1 Lexical Semantic Features ?2 is ?1 ?2 holding ?1 ?2 its ?1 meetings ?1 ?2 ?2 in ?1 Paris ?1 ?2 ROOT ?2 SBJ VC UNESCO is is holding nearest common ancestor ???? ???????? ????????? ???"
C16-1120,N15-1121,0,0.0244141,"Missing"
C16-1120,D11-1109,1,0.853687,"et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llu´ıs et al., 2013). Most multi-task learning or joint training frameworks can be summarized as parameter sharing approaches proposed by Ando and Zhang (2005). In the context of neural modeling for NLP, the most notable work was proposed by Collobert and Weston (2008), which aims at solving multiple NLP tasks within one framework by sharing common word embeddings. This work also inspires us in this study to develop a unified architecture for SRL and RC in prior to joint training. Recently, the idea of neural multi-task learning was applied to"
C16-1120,P15-2047,0,0.130443,"ave seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS taggin"
C16-1120,Q13-1018,0,0.135887,"Missing"
C16-1120,P14-5010,0,0.00386489,"rve that SRL converges about 4 times slower than RC by running them separately, hence we sample from SRL 4 times often than RC during training. Despite the lack of theoretical guarantee, we found it working well in practice. Second, the key for multi-task learning to work is parameter sharing. Given the unified architecture, we can share most of the network parameters for knowledge transfer. Note that different dependency parses might be used for SRL and RC in practice. In this work, we use the officially provided predicted parses from CoNLL-2009 shared task in SRL, but adopt Stanford parser (Manning et al., 2014) to obtain parses for sentences in RC. These kinds of parses are quite different in terms of both the head-finding rules and the dependency relations. Therefore, we set the parameters involving dependency path modeling as task-specific, i.e., BiLSTMgen , BiLSTMrel and Wsp (Figure 2). The output weights (g) are task-specific as standard of multi-task learning, in order to handle different set of relations to be classified in SRL and RC. 6 Experiment In this section, we first describe data and our experimental settings, then the results and analysis. 6.1 Data and Settings For SRL, we evaluate on"
C16-1120,S14-2082,0,0.0236389,"reat deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. Howev"
C16-1120,P16-1105,0,0.0255862,"ng neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and semantic role labeling (Hat"
C16-1120,J08-2003,0,0.0372843,"re templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research f"
C16-1120,C04-1197,0,0.144135,"Missing"
C16-1120,S10-1057,0,0.218415,"ffectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs"
C16-1120,P16-1113,0,0.0848515,"gh computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015), which have shown great effectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviat"
C16-1120,D14-1045,0,0.0554788,"Missing"
C16-1120,N16-1069,0,0.0260159,". (2015) use multiple decoders in neural machine translation systems that allows translating one source language to many target languages. Luong et al. (2015) study the ensemble of a wide range of tasks (e.g., syntactic parsing, machine translation, image caption, etc.) with multi-task sequence-to-sequence models. Liu et al. (2016) incorporate different kinds of corpus for implicit discourse relation classification using multi-task neural networks. More recently, multi-task learning has also been applied to sentence compression (Klerke et al., 2016) and machine translation quality estimation (Shah and Specia, 2016). 3 Problem Definition This section gives formal definitions of the two tasks to be investigated: SRL and RC. 3.1 Semantic Role Labeling We follow the setup of the CoNLL-2009 shared task. Given a sentence s, each token is annotated with a predicated POS tag and predicted word lemma. Some tokens are also marked as predicates. Besides, 1266 UNESCO ?1 Lexical Semantic Features ?2 is ?1 ?2 holding ?1 ?2 its ?1 meetings ?1 ?2 ?2 in ?1 Paris ?1 ?2 ROOT ?2 SBJ VC UNESCO is is holding nearest common ancestor ???? ???????? ????????? ????????? SBJ Global Context Representation ??? ??? ROOT VC Syntactic"
C16-1120,D12-1110,0,0.304758,"yntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination"
C16-1120,P03-1002,0,0.0441816,"ificantly improved by jointly training with RC, reaching new stateof-the-art performance. 2 Related Work The present work ties together several strands of previous studies. Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, t"
C16-1120,J08-2002,0,0.0306309,"mantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds o"
C16-1120,N16-1065,0,0.0356887,"Missing"
C16-1120,D15-1062,0,0.151139,"eal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and se"
C16-1120,D15-1206,0,0.0976347,"eal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL, they have a substantial amount of commonalities. It inspires us to develop a potentially unified architecture to take advantage of the progress in each research direction. Multi-task Learning There has been a line of research on joint modeling pipelined NLP tasks, such as word segmentation, POS tagging, parsing and se"
C16-1120,W04-3212,0,0.0766239,"ointly training with RC, reaching new stateof-the-art performance. 2 Related Work The present work ties together several strands of previous studies. Semantic Role Labeling A great deal of previous SRL research has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approa"
C16-1120,D14-1041,0,0.0363967,"esearch has been dedicated to designing rich and expressive features, pioneered by Gildea and Jurafsky (2002). For instance, the top performing system on the CoNLL-2009 shared task employs over 50 language-specific feature templates (Che et al., 2009). These features mostly involve the predicate, the candidate argument, their contexts and the syntactic path between them (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). Besides, higher-order features involving several arguments or multiple predicates have also been explored (Toutanova et al., 2008; Martins and Almeida, 2014; Yang and Zong, 2014). 1 Our code is available at: https://github.com/jiangfeng1124/nnsrl-rc. 1265 Several approaches have been studied to alleviate the intensive feature engineering in SRL and get better generalization. Moschitti et al. (2008) introduce different kinds of tree kernels for capturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based appro"
C16-1120,N15-1155,0,0.0531329,"Missing"
C16-1120,C14-1220,0,0.564155,"Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also been relying heavily on human-engineered features (Rink and Harabagiu, 2010). Recent years have seen a great deal of work on using neural networks to alleviate the intensive engineering on contextual and syntactic features. For example, Socher et al. (2012) propose recursive neural networks for modeling the syntactic paths between the two entities whose relation is to be determined. Zeng et al. (2014) use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features. Later approaches have used more sophisticated models for better handling long-term dependencies, such as sequential LSTMs and tree LSTMs (Liu et al., 2015; Xu et al., 2015b; Miwa and Bansal, 2016). In addition, Yu et al. (2014) and (2015) investigate tensor-based approaches for learning the combination of embedding features and lexicalized sparse features. Therefore, despite that relation classification has mostly been studied separately from SRL"
C16-1120,P15-1109,0,0.0258044,"pturing the structural similarity of syntactic trees. While attractive in automatic feature learning, the kernel-based approaches typically suffer from high computational cost. Lei et al. (2015) instead use low-rank tensors for automatic feature composition based on four kinds of basic feature sets. However, tensor-based approaches cannot well generalize the high-sparsity structural features like syntactic path. Besides, they still need a relatively small amount of feature engineering to make use of the local contexts. Another line of research focuses on neural models (Collobert et al., 2011; Zhou and Xu, 2015; FitzGerald et al., 2015), which have shown great effectiveness in automatic feature learning on a variety of NLP tasks. Most recently, Roth and Lapata (2016) employ LSTM-based recurrent neural networks to obtain the representations of syntactic path features, which is similar to our work. Aside from the distributed path features, they also use a set of binary input feature sets from Anders et al. (2010). In contrast to these prior work, our model jointly leverages both global contexts and syntactic path features using bidirectional LSTMs. Relation Classification Early research on RC has also"
C16-1167,C10-3004,1,0.278223,"matically generated test set. People Daily. We roughly collected 60K news articles from the People Daily website4 . Following Liu et al. (2016), we process the news articles into the triple form hD, Q, Ai. The detailed procedures are as follows. • Given a certain document D, which is composed by a set of sentences D = {s1 , s2 , ..., sn }, we randomly choose an answer word A in the document. Note that, we restrict the answer word A to be a noun, as well as the answer word should appear at least twice in the document. The partof-speech and sentence segmentation is identified using LTP Toolkit (Che et al., 2010). We do not distinguish the named entities and common nouns as Hill et al. (2015) did. • Second, after the answer word A is chosen, the sentence that contains A is defined as the query Q, in which the answer word A is replaced by a specific placeholder hXi. • Third, given the query Q and document D, the target of the prediction is to recover the answer A. In this way, we can generate tremendous triples of hD, Q, Ai for training the proposed neural network, without any assumptions on the nature of the original corpus. Note that, unlike the previous work, using the method mentioned above, the do"
C16-1167,P16-1223,0,0.227124,"l et al. (2015) released the Children’s Book Test (CBT) corpus for further research, where the training samples are generated through automatic approaches. As we can see that, automatically generating large-scale training data for neural network training is essential for reading comprehension. Furthermore, more difficult problems, such as reasoning or summarization of context, need much more data to learn the higher-level interactions. Though we have seen many improvements on these public datasets, some researchers suggested that these dataset requires less high-level inference than expected (Chen et al., 2016). Furthermore, the public datasets are all automatically generated, which indicate that the pattern in training and testing phase are nearly the same, and this will be easier for the machine to learn these patterns. In this paper, we will release Chinese reading comprehension datasets, including People Daily news datasets and Children’s Fairy Tale datasets. As a highlight in our datasets, there is a human evaluated ∗ This work was done by the Joint Laboratory of HIT and iFLYTEK (HFL). This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.o"
C16-1167,D14-1179,0,0.010026,"Missing"
C16-1167,P16-1086,0,0.504135,"building human evaluation test set, we have eliminated these types of samples, which makes it harder for the machine to comprehend. Intuitively, the human evaluation test set is harder than any other previously published Cloze-style test sets. The statistics of People Daily news datasets as well as Children’s Fairy Tale datasets are listed in the Table 1. 3 Consensus Attention Sum Reader In this section, we will introduce our attention-based neural network model for Cloze-style reading comprehension task, namely Consensus Attention Sum Reader (CAS Reader). Our model is primarily motivated by Kadlec et al. (2016), which aims to directly estimate the answer from the document, instead of making a prediction over the full vocabularies. But we have noticed that by just concatenating the final representations of the query RNN states are not enough for representing the whole information of query. So we propose to utilize every time slices of query, and make a consensus attention among different steps. Formally, when given a set of training triple hD, Q, Ai, we will construct our network in the following way. We first convert one-hot representation of the document D and query Q into continuous representation"
C16-1167,D14-1162,0,0.0968678,"Missing"
C16-1167,P17-1010,1,\N,Missing
C16-1201,D13-1160,0,0.0183853,"btaining better event embeddings and making more accurate prediction on stock market volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2) they can be used to alleviate the sparsity of discrete events compared with one-hot featur"
C16-1201,D13-1167,0,0.0251,"conventional features, as they can capture structured relations. However, one disadvantage of structured representations of events is that they lead to increased sparsity, which potentially limits the predictive power. Ding et al. (2015) propose to address this issue by representing structured events using event embeddings, which are dense vectors. This paper proposes to leverage ground truth from knowledge graph to enhance event embeddings. Knowledge Graph Embedding Recently, several methods have been explored to represent and encode knowledge graph (Bordes et al., 2013; Bordes et al., 2014; Chang et al., 2013; Ji et al., 2015; Lin et al., 2015) in distributed vectors. In this line of work, each entity is represented as a d-dimensional vector and each relation between two entities is modeled by using a matrix or a tensor. Most existing methods learn knowledge embeddings by minimizing a global loss function over all the entities and relations in 2134 C S2 S1 T3 A T1 PT T2 O Figure 2: Baseline event-embedding model. a knowledge graph. Entity vectors can encode global information over the knowledge graph, and hence are useful for knowledge graph completion (Socher et al., 2013). In this paper, we enco"
C16-1201,D14-1148,1,0.855022,"rket volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2) they can be used to alleviate the sparsity of discrete events compared with one-hot feature vectors. The learning principle is that events are syntactically or semantic"
C16-1201,D11-1142,0,0.0820265,"s on event similarity and stock market prediction show that our model is more capable of obtaining better event embeddings and making more accurate prediction on stock market volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2"
C16-1201,P15-1067,0,0.0163423,"s, as they can capture structured relations. However, one disadvantage of structured representations of events is that they lead to increased sparsity, which potentially limits the predictive power. Ding et al. (2015) propose to address this issue by representing structured events using event embeddings, which are dense vectors. This paper proposes to leverage ground truth from knowledge graph to enhance event embeddings. Knowledge Graph Embedding Recently, several methods have been explored to represent and encode knowledge graph (Bordes et al., 2013; Bordes et al., 2014; Chang et al., 2013; Ji et al., 2015; Lin et al., 2015) in distributed vectors. In this line of work, each entity is represented as a d-dimensional vector and each relation between two entities is modeled by using a matrix or a tensor. Most existing methods learn knowledge embeddings by minimizing a global loss function over all the entities and relations in 2134 C S2 S1 T3 A T1 PT T2 O Figure 2: Baseline event-embedding model. a knowledge graph. Entity vectors can encode global information over the knowledge graph, and hence are useful for knowledge graph completion (Socher et al., 2013). In this paper, we encode entity vectors"
C16-1201,P15-1045,1,0.793893,"ore accurate prediction on stock market volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2) they can be used to alleviate the sparsity of discrete events compared with one-hot feature vectors. The learning principle is that"
C16-1201,P13-1086,0,0.245908,"ddings, and we define a corresponding loss function to incorporate knowledge graph information, by following recent work on multi-relation models (Socher et al., 2013). Large-scale experiments on a YAGO corpus show that incorporating knowledge graph brings promising improvements to event embeddings. With better embeddings, we achieve better performance on stock prediction compared to the state-of-the-art methods. 2 Related Work Stock Market Prediction There has been a line of work predicting stock markets using text information from daily news (Lavrenko et al., 2000; Schumaker and Chen, 2009; Xie et al., 2013; Peng and Jiang, 2015; Li et al., 2016). Pioneering work extracts different types of textual features from news documents, such as bags-of-words, noun phrases, named entities and structured events. Ding et al. (2014) show that structured events from open information extraction (Yates et al., 2007; Fader et al., 2011) can achieve better performance compared to conventional features, as they can capture structured relations. However, one disadvantage of structured representations of events is that they lead to increased sparsity, which potentially limits the predictive power. Ding et al. (2015)"
C16-1201,D14-1071,0,0.0215393,"ddings, so that information of knowledge graphs can be used for event-driven text mining and other tasks. Socher et al. (2013) has shown that previous work (Bordes et al., 2011; Jenatton et al., 2012; Bordes et al., 2012; Sutskever et al., 2009; Collobert and Weston, 2008) are special cases of their model, which is based on a neural tensor network. We follow Socher et al. (2013) and use tensors to represent relations in knowledge graph embeddings. Our work is also related to prior research on joint embedding of words and knowledge graphs (Xu et al., 2014; Wang et al., 2014; Tian et al., 2016; Yang et al., 2014). Such work focuses on injecting semantic knowledge into distributed word representations, thus enhancing their information content. The resulting embeddings of words and phrases have been shown useful for improving NLP tasks, such as question answering and topic prediction. In comparison, our work integrates knowledge into vector representations of events, which was shown more useful than words for certain text mining tasks. 3 Knowledge-Driven Event Representations We begin by introducing the baseline event embedding learning model, which serves as the basis of proposed framework. Then, we sh"
C16-1201,N07-4013,0,0.0251176,"y and stock market prediction show that our model is more capable of obtaining better event embeddings and making more accurate prediction on stock market volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2) they can be used to"
C16-1201,J11-1005,1,0.214117,"ing the quality of learned event embeddings on two tasks: event similarity and stock market prediction. 4.1 Experimental Settings We use publicly available financial news from Reuters and Bloomberg over the period from October 2006 to November 2013, released by Ding et al. (2014). There are 106,521 documents in total from Reuters News, from which we extracted 83,468 structured events. From Bloomberg News, there are 447,145 documents, from which 282,794 structured events are extracted. The structured events are extracted from news text using Open IE (Fader et al., 2011) and dependency parsing (Zhang and Clark, 2011), by strictly following the method of Ding et al. (2015). The timestamps of the news are also extracted, for alignment with stock price information. We conduct stock market prediction experiments on predicting the Standard & Poor’s 500 stock (S&P 500) index and its individual stocks, obtaining indices and prices from Yahoo Finance. Detail statistics of the training, development (tuning) and test sets are shown in Table 1. For training knowledge-driven event embeddings, we use YAGO as the knowledge graph. The full knowledge graph consists of 10 million entities and 120 million facts, in more th"
C16-1244,D15-1008,0,0.0269883,"re understandable and closes the semantic relatedness gap between the stories and user interested topics. (3) We focus on a specific application scenario: anecdote recommendation for argumentative writing. 2.3 Narrative Modeling We extract anecdote stories by extracting human centric events. Story extraction is a kind of narrative modeling. A story is usually viewed as a sequence of events (Chambers and Jurafsky, 2008) based on information extraction (Etzioni et al., 2011). Much work has been done for extracting facts and events from various resources (Banko et al., 2007; Ritter et al., 2012; Bamman and Smith, 2015; Bamman et al., 2014). Similar techniques have been used in educational applications. For example, factual information in argumentative essays has be exploited for essay scoring (Klebanov and Higgins, 2012). Our work differs from existing work in two folds: (1) We combine event extraction and discourse role identification for extracting anecdote stories. (2) We also uncover the implications of these stories in order to close the gap between objective facts and subjective human intent. 3 Data and Task 3.1 Data The task we propose is recognizing and recommending anecdotes for assisting argument"
C16-1244,P14-1035,0,0.0256633,"oses the semantic relatedness gap between the stories and user interested topics. (3) We focus on a specific application scenario: anecdote recommendation for argumentative writing. 2.3 Narrative Modeling We extract anecdote stories by extracting human centric events. Story extraction is a kind of narrative modeling. A story is usually viewed as a sequence of events (Chambers and Jurafsky, 2008) based on information extraction (Etzioni et al., 2011). Much work has been done for extracting facts and events from various resources (Banko et al., 2007; Ritter et al., 2012; Bamman and Smith, 2015; Bamman et al., 2014). Similar techniques have been used in educational applications. For example, factual information in argumentative essays has be exploited for essay scoring (Klebanov and Higgins, 2012). Our work differs from existing work in two folds: (1) We combine event extraction and discourse role identification for extracting anecdote stories. (2) We also uncover the implications of these stories in order to close the gap between objective facts and subjective human intent. 3 Data and Task 3.1 Data The task we propose is recognizing and recommending anecdotes for assisting argumentative writing. We focu"
C16-1244,N07-4013,0,0.0290944,"cdotal stories. This makes the stories more understandable and closes the semantic relatedness gap between the stories and user interested topics. (3) We focus on a specific application scenario: anecdote recommendation for argumentative writing. 2.3 Narrative Modeling We extract anecdote stories by extracting human centric events. Story extraction is a kind of narrative modeling. A story is usually viewed as a sequence of events (Chambers and Jurafsky, 2008) based on information extraction (Etzioni et al., 2011). Much work has been done for extracting facts and events from various resources (Banko et al., 2007; Ritter et al., 2012; Bamman and Smith, 2015; Bamman et al., 2014). Similar techniques have been used in educational applications. For example, factual information in argumentative essays has be exploited for essay scoring (Klebanov and Higgins, 2012). Our work differs from existing work in two folds: (1) We combine event extraction and discourse role identification for extracting anecdote stories. (2) We also uncover the implications of these stories in order to close the gap between objective facts and subjective human intent. 3 Data and Task 3.1 Data The task we propose is recognizing and"
C16-1244,P08-1090,0,0.540843,") and the ability to retrieve and figure out right ones from memory. This process brings in great challenges for both novice and more sophisticated writers. Anecdotal evidence is one of the most commonly used evidence types (Hornikx, 2005). For a given claim, a system that can provide relevant anecdotes would help writers find good evidence and potentially improve the organization and the quality of essays. Recommending anecdotes can be the first step to recommend all types of evidence. Recognizing anecdotes is related to previous work that extracts story-like elements from text. For example, Chambers and Jurafsky (2008) propose an unsupervised approach to extract narrative event chains from newswire text. A narrative event chain is a set of narrative events that share a common participant. However, extracting stories only is not sufficient. Suppose that we are writing an essay about the value of life, how can a system understand which stories are related to this topic? The stories are This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2592 Proceedings of COLING 2016, the 26th International Conference on Computati"
C16-1244,C10-3004,1,0.608615,"The reason we distinguish these roles is because they control different ranges of an essay. The thesis and the conclusion set up the main tune and conclude the whole essay, while the main ideas mainly control the local zones. According to our motivation, anecdote recognition is divided into three steps: discourse role identification, story extraction and story-implication linking. Before anecdote extraction, an essay text is preprocessed by a pipeline of components including word segmentation, POS tagging, named entity 2595 tagging, dependency parsing and semantic role labeling with HIT-LTP (Che et al., 2010). Also, we use pairs of quotations to locate quotes and view each quote as a whole. 4.1 Discourse Role Identification We identify discourse roles based on a linear-chain Conditional Random Field (CRF) model (Lafferty et al., 2001) in order to capture the correlations among sequential predictions. The features for each sentence are mainly inspired by the previous work (Burstein et al., 2003b; Stab and Gurevych, 2014): • Position features We use the relative position (beginning, middle, end) of the sentence in its paragraph and its paragraph in the essay, and the number of the sentence in the do"
C16-1244,P11-1099,0,0.0180452,"oposes to recognize and recommend anecdotes for assisting argumentative writing. Our work is related to work on quote recommendation (Tan et al., 2015) and citation recommendation (He et al., 2010). But the focuses and techniques are quite different. To the best of our knowledge, our work is the first to recommend structured factual evidence to support argumentative writing. 2.2 Argumentation Mining Argumentation mining aims to identify the components and their relationships in argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Abbas and Sawamura, 2012; Lippi and Torroni, 2015; Feng and Hirst, 2011). Similar work focuses on identifying discourse roles in student essays and scientific abstracts (Burstein et al., 2003b; Guo et al., 2010). Our work focuses on recognizing anecdotes based on discourse role identification in student essays. Moreover, we are also interested in the association between roles, such as factual evidence and the arguments they support. Our work is close to (Rinott et al., 2015), which aims to recommend evidence according to given claims. The main differences include: (1) In (Rinott et al., 2015), the system runs over dedicated manually labeled data. Instead, our appr"
C16-1244,W10-1913,0,0.0121413,"., 2015) and citation recommendation (He et al., 2010). But the focuses and techniques are quite different. To the best of our knowledge, our work is the first to recommend structured factual evidence to support argumentative writing. 2.2 Argumentation Mining Argumentation mining aims to identify the components and their relationships in argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Abbas and Sawamura, 2012; Lippi and Torroni, 2015; Feng and Hirst, 2011). Similar work focuses on identifying discourse roles in student essays and scientific abstracts (Burstein et al., 2003b; Guo et al., 2010). Our work focuses on recognizing anecdotes based on discourse role identification in student essays. Moreover, we are also interested in the association between roles, such as factual evidence and the arguments they support. Our work is close to (Rinott et al., 2015), which aims to recommend evidence according to given claims. The main differences include: (1) In (Rinott et al., 2015), the system runs over dedicated manually labeled data. Instead, our approach automates all aspects of anecdote extraction and recommendation based on information extraction and discourse role identification. (2)"
C16-1244,W12-2007,0,0.023332,"ng. 2.3 Narrative Modeling We extract anecdote stories by extracting human centric events. Story extraction is a kind of narrative modeling. A story is usually viewed as a sequence of events (Chambers and Jurafsky, 2008) based on information extraction (Etzioni et al., 2011). Much work has been done for extracting facts and events from various resources (Banko et al., 2007; Ritter et al., 2012; Bamman and Smith, 2015; Bamman et al., 2014). Similar techniques have been used in educational applications. For example, factual information in argumentative essays has be exploited for essay scoring (Klebanov and Higgins, 2012). Our work differs from existing work in two folds: (1) We combine event extraction and discourse role identification for extracting anecdote stories. (2) We also uncover the implications of these stories in order to close the gap between objective facts and subjective human intent. 3 Data and Task 3.1 Data The task we propose is recognizing and recommending anecdotes for assisting argumentative writing. We focus on dealing with argumentative essays, because there exist rich evidence used by the authors to support their claims. We aim to extract anecdotes from archived essays and use them as s"
C16-1244,D15-1110,0,0.0143694,"Robert, 2004; Nebeling et al., 2016). This paper extends existing work and proposes to recognize and recommend anecdotes for assisting argumentative writing. Our work is related to work on quote recommendation (Tan et al., 2015) and citation recommendation (He et al., 2010). But the focuses and techniques are quite different. To the best of our knowledge, our work is the first to recommend structured factual evidence to support argumentative writing. 2.2 Argumentation Mining Argumentation mining aims to identify the components and their relationships in argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Abbas and Sawamura, 2012; Lippi and Torroni, 2015; Feng and Hirst, 2011). Similar work focuses on identifying discourse roles in student essays and scientific abstracts (Burstein et al., 2003b; Guo et al., 2010). Our work focuses on recognizing anecdotes based on discourse role identification in student essays. Moreover, we are also interested in the association between roles, such as factual evidence and the arguments they support. Our work is close to (Rinott et al., 2015), which aims to recommend evidence according to given claims. The main differences include: (1) In (Rinott et al., 2015"
C16-1244,D15-1050,0,0.127088,"value of life is to change the world. Person: Bruno Story: Bruno, who was burned to death, was a martyr of modern science. Implication: The value of life depends on its donation rather than its duration. Person:Helen Keller Story:Helen Keller is known for her efforts in learning and her arduous work for the disabled. Implication: The life means fighting against the fate. Table 2: An example of anecdote recommendation. Anecdotes are ranked according to the given topic. objective facts, but the writing goals are subjective. The semantic relatedness between them are less direct as discussed in (Rinott et al., 2015). To close this gap, we have to figure out the meanings or intents that these stories want to express. Considering the above issues, we define an anecdote as a structured tuple, including person, story and implication. The story describes the factual information about the story of specific persons. The implication indicates the meanings and significance of the story. We recognize anecdotes from essays and re-use the anecdotes to assist future argumentative writing. Our approach is based on discourse role identification, which automatically recognizes discourse roles such as the thesis, main id"
C16-1244,D14-1006,0,0.115282,"collaboration (No¨el and Robert, 2004; Nebeling et al., 2016). This paper extends existing work and proposes to recognize and recommend anecdotes for assisting argumentative writing. Our work is related to work on quote recommendation (Tan et al., 2015) and citation recommendation (He et al., 2010). But the focuses and techniques are quite different. To the best of our knowledge, our work is the first to recommend structured factual evidence to support argumentative writing. 2.2 Argumentation Mining Argumentation mining aims to identify the components and their relationships in argumentation (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Abbas and Sawamura, 2012; Lippi and Torroni, 2015; Feng and Hirst, 2011). Similar work focuses on identifying discourse roles in student essays and scientific abstracts (Burstein et al., 2003b; Guo et al., 2010). Our work focuses on recognizing anecdotes based on discourse role identification in student essays. Moreover, we are also interested in the association between roles, such as factual evidence and the arguments they support. Our work is close to (Rinott et al., 2015), which aims to recommend evidence according to given claims. The main differences include: ("
C16-1244,P15-4024,0,0.117877,"stories and their implications in essays. The experiments show that informative and interpretable anecdotes can be recognized. These anecdotes are used for anecdote recommendation. The anecdote recommender can recommend proper anecdotes in response to given topics. The anecdote implications contribute most for bridging user interested topics and relevant anecdotes. 1 Introduction Building technical tools to assist learning and writing is of great significance and challenging. While a number of tools have been developed for giving feedback on spelling (Bangert-Drowns, 1993), grammar patterns (Yen et al., 2015) and organization (Burstein et al., 2003b), little exists to provide support during planning and composition process. During the process, an automated system, that can effectively collect topic oriented evidence and reading materials, will greatly reduce the cognitive load. This paper introduces the anecdote recognition and recommendation task. An anecdote is a story with a point revealing account of an individual person or an incident. We aim to recognize anecdotes from texts and recommend anecdotes according to given topics. The recommended anecdotes can be used as evidence to support argume"
C16-1276,W09-1604,0,0.076015,"Missing"
C16-1276,P16-2011,1,0.871694,"Missing"
C16-1276,P16-1212,0,0.0613039,"Missing"
C16-1276,D11-1014,0,0.087629,"previously obtained entity representation and relational representation, both of which play important roles for representing the meaning of a triple. Furthermore, a better approach should benefit from both aspects, and integrate them in triple semantic with an automatic method. To this end, we introduce a gated neural network in this part. It takes entity and relational vectors of a triple as input, and adaptively produces the composed continuous representation of them. Given ve and vr as inputs, a traditional compositional function is to concatenate ve and vr and feed them to a linear layer (Socher et al., 2011), which is calculated as Equation 2. Despite its computational efficiency, tied parameters cannot easily capture the complex linguistic phenomena in natural language expressions. v˜ = tanh(We ve + Wr vr + b) (2) α = σ(Weg ve + Wrg vr + bg ) (3) v(t) = α · vr + (1 − α) · v˜ (4) Therefore, we add a neural gate to change parameter values for different input vectors ve and vr , which is partly inspired by the recent success of gated recurrent neural network (Cho et al., 2014; Chung et al., 2015) and Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Tai et al., 2015). And our gated neural n"
C16-1276,P13-1045,0,0.111394,"c calculating process based on different triples rather than a fixed model. In this way, source triple and target triple are naturally encoded in the same semantic vector space. We design a ranking-type hinge loss function to effectively train the parameters of neural networks. We evaluate the effectiveness of our method on a manually created corpus. We conduct experiments in two settings. Empirical results show that the proposed method consistently outperforms baseline methods. We also show that the use of gated neural network improves strong composition models such as neural tensor network (Socher et al., 2013b) in terms of translation accuracy. The main contributions of this work are as follows: • We introduce an approach based on representation learning for English-Chinese KB translation in this paper. • We present a gated neural network to adaptively integrate entity and relational level evidences in triple representation. • We build a dataset for English-Chinese KB translation, and report the superior performance of our method over baseline methods on it. 2 The Approach In this section, we present our neural network method for KB Translation in detail. Figure 2 displays a high-level overview of"
C16-1276,D13-1170,0,0.158804,"c calculating process based on different triples rather than a fixed model. In this way, source triple and target triple are naturally encoded in the same semantic vector space. We design a ranking-type hinge loss function to effectively train the parameters of neural networks. We evaluate the effectiveness of our method on a manually created corpus. We conduct experiments in two settings. Empirical results show that the proposed method consistently outperforms baseline methods. We also show that the use of gated neural network improves strong composition models such as neural tensor network (Socher et al., 2013b) in terms of translation accuracy. The main contributions of this work are as follows: • We introduce an approach based on representation learning for English-Chinese KB translation in this paper. • We present a gated neural network to adaptively integrate entity and relational level evidences in triple representation. • We build a dataset for English-Chinese KB translation, and report the superior performance of our method over baseline methods on it. 2 The Approach In this section, we present our neural network method for KB Translation in detail. Figure 2 displays a high-level overview of"
C16-1276,P15-1150,0,0.152104,"m to a linear layer (Socher et al., 2011), which is calculated as Equation 2. Despite its computational efficiency, tied parameters cannot easily capture the complex linguistic phenomena in natural language expressions. v˜ = tanh(We ve + Wr vr + b) (2) α = σ(Weg ve + Wrg vr + bg ) (3) v(t) = α · vr + (1 − α) · v˜ (4) Therefore, we add a neural gate to change parameter values for different input vectors ve and vr , which is partly inspired by the recent success of gated recurrent neural network (Cho et al., 2014; Chung et al., 2015) and Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Tai et al., 2015). And our gated neural network is inspired by highway network, which allow the model to suffer less from the vanishing gradient problem (Srivastava et al., 2015a; Srivastava et al., 2015b). The gate takes ve and vr as inputs, and outputs as a weight α ∈ [0, 1], which linearly weights the two parts. Specifically, the gate is calculated as Equation 3, where σ is standard sigmoid function, Weg , Wrg and bg are parameters. Triple representation v(t) is calculated as given in Equation 4, which linearly weights the candidate composed representation v˜ and relational representation vr . In this way,"
C16-1276,D13-1141,0,0.02453,"ing bilingual parallel corpus, these word vectors are mapped into different semantic spaces. This is not desirable for comparing the semantic relatedness between English triple and Chinese triple. We use linear layers to transform English and Chinese word vectors in a same semantic vector space. A simple linear layer is calculated as ve = W e + b, where W and b are the parameters. One could also learn bilingual 3 code.google.com/p/word2vec/ https://dumps.wikimedia.org/ 5 http://baike.baidu.com/ 4 2937 word vectors simultaneously from bilingual parallel corpus with tailored learning algorithm (Zou et al., 2013). We leave this as a future work and we believe our method could benefit from the bilingual word embeddings. 2.2.2 Relational Representation We model the semantic relatedness between entities in this part. The basic idea is that the semantic relatedness between entities is determined by the semantics of entities and their relations. Based on this, we utilize neural tensor network, which is one of state-of-the-art semantic composition approach for natural language processing tasks (Mitchell and Lapata, 2010; Socher et al., 2013a; Jenatton et al., 2012). A standard neural tensor with rank 3 is e"
C16-1284,D14-1179,0,0.00676876,"Missing"
C16-1284,C10-2028,0,0.0199908,"Missing"
C16-1284,C12-2027,0,0.0732102,"he contents of the corpus at a very high level. Given a collection of microblog posts, LDA is able to learn a sparse topic representation for each post. The topic representation is viewed as a kind of global semantic information of a post, which we can utilize to learn the interactions between each words and the whole microblog post. 3 The Approach In this section, we will present our proposed model for hashtag recommendation. We formulate the hashtag recommendation task as a multi-class classification problem. It has been observed that hashtags indicate the primary topics of microblog posts (Ding et al., 2012; Godin et al., 2013). To incorporate the topics of microblogs, we take into account the attention mechanism and develop a novel Topical Attention-Based LSTM model, or TAB-LSTM for short. The basic idea of TAB-LSTM is to combine local hidden representations with global topic vectors through an attention mechanism. We believe that in this way our model can capture the importance of different local words according to the global topics of a microblog post. Our overall model is illustrated in Figure 2. The model mainly consists of three parts, namely, LSTM based sequence encoder, topic modeling, a"
C16-1284,D15-1046,0,0.390199,"s and the baseline methods, we use the validation data to tune the hyperparameters, we report the results of the test data in the same setting of hyperparameters. Furthermore, the word embeddings used in all methods are pre-trained from the original twitter data released by (Yang and Leskovec, 2011) with the word2vec toolkit (Mikolov et al., 2013). We use hashtags annotated by users as the golden set. To evaluate the performance, we use precision (P ), recall (R), and F1-score (F ) as the evaluation metrics. The same settings are adopted by previous work (Ding et al., 2012; Ding et al., 2013; Gong et al., 2015). 4.3 Comparison to Other Methods In Table 2, we compare the results of our method and the state-of-the-art discriminative and generative methods on the dataset. TAB-LSTM denotes our proposed model. We have the following observations. (1) First of all, SVM performs much better than LDA, showing that the embedding features capture more semantic information than bag-of-words (BoW). (2) Both TAB-LSTM and the degenerate models significantly outperform the baseline methods LDA, SVM and TTM. The results demonstrate that the neural network can achieve better performance on this task. (3) If we compar"
C16-1284,D11-1146,0,0.0120099,"ent each candidate hashtag as a feature vector and use pairwise learning to rank method to find the top ranked hashtags from the candidate set. Mazzia and Juett (2009) apply a Naive Bayes model to estimate the maximum a posteriori probability of each hashtag class given the words of the tweet. Furthermore, Godin et al. (2013) propose to incorporate topic models to learn the underlying topic assignment of language classified tweets, and suggest hashtags to a tweet based on the topic distribution. Under the assumption “hashtags and tweets are parallel description of a resource” that proposed by Liu et al. (2011), Ding et al. try to integrate latent topical information into translation model. The model uses topic-specific word trigger to bridge the vocabulary gap between the words in tweets and hashtags (Ding et al., 2012; Ding et al., 2013). Most of the works mentioned above are based on textual information. There have also been some attempts that combine text with other types of data. Kywe et al. propose a collaborative filtering model to incorporate user preferences in hashtag recommendation (Kywe et al., 2012). Besides that, Zhang et al. (2014) and Ma et al. (2014) try to incorporate temporal info"
C16-1284,D15-1166,0,0.0318503,"ic distribution of the post. Using these words, we can predict its hashtag #travel. necessary information of the input post has to be compressed into a fixed-length vector. This may make it difficult to cope with long sentences (Bahdanau et al., 2015). One possible solution is to perform an average pooling operation over the hidden vectors of LSTM (Boureau et al., 2011), but not all words in a microblog post contribute equally for hashtag recommendation. Inspired by the success of attention mechanism in computer vision and natural language processing (Mnih et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), we investigate the use of attention mechanism to automatically capture the most relevant words in a microblog to the recommendation task. Furthermore, it has been observed that most hashtags indicate the topics of a microblog (Ding et al., 2012; Godin et al., 2013), as illustrated in Figure 1. To this end, we propose a novel attention-based LSTM model which incorporates LDA topics of microblogs into the LSTM architecture through an attention mechanism. By modeling the interactions between the words and the global topics, our model can learn effective representations of microblogs for hashtag"
C16-1284,D15-1044,0,0.14038,"llaborative filtering to probabilistic models such as naive Bayes and topic models. Most of these methods depend on sparse lexical features including bag-of-word (BoW) models and exquisitely designed patterns. However, feature engineering is labor-intensive and the sparse and discrete features cannot effectively encode semantic and syntactic information of words. On the other hand, neural models recently have shown great potential for learning effective representations and delivered state-of-the-art performance on various natural language processing tasks (Cho et al., 2014; Tang et al., 2015; Rush et al., 2015). Among these methods, the long short-term memory (LSTM), a variant of recurrent neural network (RNN), is widely adopted due to its capability of capturing long-term dependencies in learning sequential representations (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Palangi et al., 2016). In this work, we model the hashtag recommendation task as a multi-class classification problem. A typical approach is to adopt LSTM to learn the representation of a microblog post and then perform text classification based on this representation. However, a potential issue with this approach is that all"
C16-1284,D15-1167,1,0.119957,"Missing"
C16-1284,N16-1170,1,0.573558,"on mechanism. Experiments on data from a real microblogging service show that our model achieves significantly better performance than various state-of-the-art methods. 2 Background Before going to the details of our method, we provide some background on two topics relevant to our work: the attention mechanism and Latent Dirichlet Allocation (LDA). 2.1 Attention Mechanism Attention-based models have demonstrated success in a wide range of NLP tasks including sentence summarization (Rush et al., 2015), reading comprehension (Hermann et al., 2015) and text entailment (Rockt¨aschel et al., 2016; Wang and Jiang, 2016). The basic idea of the attention mechanism is that it assigns a weight to each position in a lower-level of the neural network when computing an upper-level 3020 representation (Bahdanau et al., 2015; Luong et al., 2015). Bahdanau et al. (2015) made the first attempt to use an attention-based neural machine translation (NMT) approach to jointly translate and align words. The model is based on the basic encoder-decoder model (Cho et al., 2014). Differently, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively through the attention mechanism"
C16-1284,C14-1021,0,0.0746909,"re parallel description of a resource” that proposed by Liu et al. (2011), Ding et al. try to integrate latent topical information into translation model. The model uses topic-specific word trigger to bridge the vocabulary gap between the words in tweets and hashtags (Ding et al., 2012; Ding et al., 2013). Most of the works mentioned above are based on textual information. There have also been some attempts that combine text with other types of data. Kywe et al. propose a collaborative filtering model to incorporate user preferences in hashtag recommendation (Kywe et al., 2012). Besides that, Zhang et al. (2014) and Ma et al. (2014) try to incorporate temporal information. Gong et al. (2015) propose to model type of hashtag as a hidden variable into their DPMM (Dirichlet Process Mixture Models) based method. 3026 More recently, Gong and Zhang (2016) propose an attention-based convolutional neural network, which incorporates a local attention channel and global channel for hashtag recommendation. However, to the best of our knowledge, there is no work yet on employing both topic models and deep neural networks for this task. 6 Conclusion In this paper, we investigated a novel topical attention-based L"
C16-1311,D15-1263,0,0.00993495,"compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that 3305 incorporating target information could boost the performance of a long short-term memory model. The target-dependent LSTM model obtains state-of-the-art classification accuracy. Acknowledgements We greatly thank Yaming Sun for tremendously helpful discus"
C16-1311,P14-2009,1,0.714033,"ccuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.1 1 Introduction Sentiment analysis, also known as opinion mining (Pang and Lee, 2008; Liu, 2012), is a fundamental task in natural language processing and computational linguistics. Sentiment analysis is crucial to understanding user generated text in social networks or product reviews, and has drawn a lot of attentions from both industry and academic communities. In this paper, we focus on target-dependent sentiment classification (Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015), which is a fundamental and extensively studied task in the field of sentiment analysis. Given a sentence and a target mention, the task calls for inferring the sentiment polarity (e.g. positive, negative, neutral) of the sentence towards the target. For example, let us consider the sentence: “I bought a new camera. The picture quality is amazing but the battery life is too short”. If the target string is picture quality, the expected sentiment polarity is “positive” as the sentence expresses a positive opinion towards picture quality. If we consider the target as battery"
C16-1311,P11-1016,0,0.0890004,"the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.1 1 Introduction Sentiment analysis, also known as opinion mining (Pang and Lee, 2008; Liu, 2012), is a fundamental task in natural language processing and computational linguistics. Sentiment analysis is crucial to understanding user generated text in social networks or product reviews, and has drawn a lot of attentions from both industry and academic communities. In this paper, we focus on target-dependent sentiment classification (Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015), which is a fundamental and extensively studied task in the field of sentiment analysis. Given a sentence and a target mention, the task calls for inferring the sentiment polarity (e.g. positive, negative, neutral) of the sentence towards the target. For example, let us consider the sentence: “I bought a new camera. The picture quality is amazing but the battery life is too short”. If the target string is picture quality, the expected sentiment polarity is “positive” as the sentence expresses a positive opinion towards picture quality. If we consider th"
C16-1311,P14-1062,0,0.0547208,"entence/document level sentiment classification, previous studies mostly have two steps. They first learn continuous word vector embeddings from data (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, an"
C16-1311,D14-1181,0,0.00785348,"iment classification, previous studies mostly have two steps. They first learn continuous word vector embeddings from data (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that"
C16-1311,D15-1278,0,0.108959,"r semantic composition in the area of sentiment 3299 Softmax Target-Connection Long Short-Term Memory ℎ1 ℎ? LSTML LSTML ?? ?1 …… ℎ?+1 LSTML LSTML ??+1 ℎ?−1 ℎ?+1 LSTMR LSTMR ??+1 ??−1 ℎ? ℎ?−1 …… ??−1 …… ℎ? LSTMR LSTMR ?? ?? …… ??????? Figure 2: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {wl+1 , wl+2 , ..., wr−1 } are target words, vtarget is target representation, {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is"
C16-1311,P15-1107,0,0.565302,"r semantic composition in the area of sentiment 3299 Softmax Target-Connection Long Short-Term Memory ℎ1 ℎ? LSTML LSTML ?? ?1 …… ℎ?+1 LSTML LSTML ??+1 ℎ?−1 ℎ?+1 LSTMR LSTMR ??+1 ??−1 ℎ? ℎ?−1 …… ??−1 …… ℎ? LSTMR LSTMR ?? ?? …… ??????? Figure 2: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {wl+1 , wl+2 , ..., wr−1 } are target words, vtarget is target representation, {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is"
C16-1311,W02-1011,0,0.0501051,"e model. A potential reason might be that the attention based LSTM has larger number of parameters, which cannot be easily optimized with the small number of corpus. 4 Related Work We briefly review existing studies on target-dependent sentiment classification and neural network approaches for sentiment classification in this section. 4.1 Target-Dependent Sentiment Classification Target-dependent sentiment classification is typically regarded as a kind of text classification problem in literature. Therefore, standard text classification approach such as feature-based Supported Vector Machine (Pang et al., 2002; Jiang et al., 2011) can be naturally employed to build a sentiment classifier. Despite the effectiveness of feature engineering, it is labor intensive and unable to discover the discriminative or explanatory factors of data. To handle this problem, some recent studies (Dong et al., 2014; Vo and Zhang, 2015) use neural network methods and encode each sentence in continuous and low-dimensional vector space without feature engineering. Dong et al. (2014) transfer a dependency tree of a sentence into a target-specific recursive structure, and get higher level representation based on that structu"
C16-1311,D14-1162,0,0.120323,", {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. In this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms (Pennington et al., 2014; Tang et al., 2014) to make better use of semantic and grammatical associations of words. We use LSTM to compute the vector of a sentence from the vectors of words it contains, an illustration of the model is shown in Figure 1. LSTM is a kind of recurrent neural network (RNN), which is capable of mapping"
C16-1311,D13-1170,0,0.0826717,"ew of TC-LSTM is illustrated in Figure 2. The input of TC-LSTM is a sentence consisting of n words {w1 , w2 , ...wn } and a target string t occurs in the sentence. We represent target t as {wl+1 , wl+2 ...wr−1 } because a target could be a word sequence of variable length, such as “google” or “harry potter”. When processing a sentence, we split it into three components: target words, preceding context words and following context words. We obtain target vector vtarget by averaging the vectors of words it contains, which has been proven to be simple and effective in representing named entities (Socher et al., 2013a; Sun et al., 2015). When compute the hidden vectors of preceding and following context words, we use two separate long short-term memory models, which are similar with the strategy used in TD-LSTM. The difference is that in TC-LSTM the input at each position is the concatenation of word embedding and target vector vtarget , while in TD-LSTM the input at each position only includes 3301 the embedding of current word. We believe that TC-LSTM could make better use of the connection between target and each context word when building the representation of a sentence. 2.4 Model Training We train L"
C16-1311,P15-1150,0,0.0526433,"ontinuous word vector embeddings from data (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that 3305 incorporating target information could boost the performance of a long short-term"
C16-1311,P14-1146,1,0.913339,"e preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. In this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms (Pennington et al., 2014; Tang et al., 2014) to make better use of semantic and grammatical associations of words. We use LSTM to compute the vector of a sentence from the vectors of words it contains, an illustration of the model is shown in Figure 1. LSTM is a kind of recurrent neural network (RNN), which is capable of mapping vectors of words wi"
C16-1311,D15-1167,1,0.590323,"tion in the area of sentiment 3299 Softmax Target-Connection Long Short-Term Memory ℎ1 ℎ? LSTML LSTML ?? ?1 …… ℎ?+1 LSTML LSTML ??+1 ℎ?−1 ℎ?+1 LSTMR LSTMR ??+1 ??−1 ℎ? ℎ?−1 …… ??−1 …… ℎ? LSTMR LSTMR ?? ?? …… ??????? Figure 2: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {wl+1 , wl+2 , ..., wr−1 } are target words, vtarget is target representation, {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of wor"
C16-1311,N16-1174,0,0.0298263,"f a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that 3305 incorporating target information could boost the performance of a long short-term memory model. The target-dependent LSTM model obtains state-of-the-art classification accuracy. Acknowledgements We greatly thank Yaming Sun for tremendously helpful discussions. This work was"
C18-1002,D13-1135,0,0.575604,"is the first rule-based study that integrate Hobbs-algorithm into the resolution of zero pronoun in the Chinese Treebank (Xue et al., 2005). After that, a variety of learning-based methods have been investigated. Zhao and Ng (2007) use the learning-based model to locate and resolve zero anaphoras. They investigate a serious of features and apply the decision-tree algorithm to train the classifier. To better capture the syntactic-level information, Kong and Zhou (2010) introduce the context sensitive tree-kernel unified framework for zero anaphor resolution. On the base of Zhao and Ng (2007), Chen and Ng (2013) further investigate their model, introducing two extensions to the resolver, namely, novel features and zero pronoun links. However, these work deeply rely on annotation dataset. To alleviate this issue, Chen and Ng (2014) present the first unsupervised model that first convert zero anaphoras into ten pre-defined pronouns and then apply a ranking-based pronoun resolution model to select antecedent mentions. Chen and Ng (2015) build a discourse-aware model that can jointly locate and resolve zero anaphoras. More recently, with the advance of neural network techniques, deep-learning-based metho"
C18-1002,P15-2053,0,0.802613,"tic-level information, Kong and Zhou (2010) introduce the context sensitive tree-kernel unified framework for zero anaphor resolution. On the base of Zhao and Ng (2007), Chen and Ng (2013) further investigate their model, introducing two extensions to the resolver, namely, novel features and zero pronoun links. However, these work deeply rely on annotation dataset. To alleviate this issue, Chen and Ng (2014) present the first unsupervised model that first convert zero anaphoras into ten pre-defined pronouns and then apply a ranking-based pronoun resolution model to select antecedent mentions. Chen and Ng (2015) build a discourse-aware model that can jointly locate and resolve zero anaphoras. More recently, with the advance of neural network techniques, deep-learning-based methods are introduced and have been demonstrated to be effective for this task. Chen and Ng (2016) first introduce a feed-forward neural network framework, where zero anaphoras are encoded by its previous word and headword. However, their model overlooks context of a zero anaphora, which inevitably misses some valuable information. Naturally, some works try to alleviate this issue by investigating information from associate texts."
C18-1002,P16-1074,0,0.227082,"in Chinese 基于注意力神经网络模型的零指代消解研究 传统的关于零指代的方法提出了多种关于先行语和零代词的表示模型。这些模型中，研 究者们用零代词的上下文信息来帮助表示缺省的信息。为了更好的帮助建模零代词，我 们提出了一种基于注意力机制的神经网络模型，通过注意力模型来获取更有表示性信 息的上下文信息。实验结果表明：我们的方法能够有效提升效果，并在中文OntoNotes 5.0数据集上取得了最好的结果，超越了现有的基准系统。 1 Introduction In natural languages, expressions that can be deduced contextually by people are frequently omitted in texts. This is special the case in pro-dropped languages, such as Chinese, where a kind of anaphoric expression is frequently eliminated. A zero pronoun is a gap in the sentence that is found when a phonetically null form is used to refer to a real-world entity (Chen and Ng, 2016). We here show a case of zero pronouns from the OntoNotes-5.0 dataset. 这 次 地震 φ1 有 一些 房屋 塌 的 , 这 里面 如果 有 建房 的 质量 问题 ， φ2 是 要 追究 责任 的 。 In this earthquck φ1 some rooms collapsed, if there exsit some room quality issues, φ2 will need to call to account. We use φ to represent the zero pronouns in this example. Among these zero anaphoras, we can assign the mention “政府/the government” that appears in leading text, to be the antecedent of φ2 while there are no such mentions for φ1 . Hence, φ2 is an anaphoric zero pronoun, and φ1 is the un-anaphoric case. ∗ Corresponding author. This work is licensed"
C18-1002,P00-1022,0,0.865194,"Missing"
C18-1002,P11-1081,0,0.442913,"hnique for modeling candidates, our model learns to encode more informative parts of the mentions. All these bring advantages to the resolution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphor"
C18-1002,P06-1079,0,0.736449,"to encode more informative parts of the mentions. All these bring advantages to the resolution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolution. Compared to the"
C18-1002,D15-1260,0,0.44975,"lution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolution. Compared to the prior studies that have the underutilized context of zero pronouns, we investigate an att"
C18-1002,D16-1132,0,0.184762,"ouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolution. Compared to the prior studies that have the underutilized context of zero pronouns, we investigate an attention 15 mechanism"
C18-1002,W03-1024,0,0.737876,"didates, our model learns to encode more informative parts of the mentions. All these bring advantages to the resolution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolutio"
C18-1002,D10-1086,0,0.651867,"summary of early efforts for zero pronoun resolution both for Chinese and other languages. 2.1 Zero Pronoun Resolution for Chinese Converse (2006) is the first rule-based study that integrate Hobbs-algorithm into the resolution of zero pronoun in the Chinese Treebank (Xue et al., 2005). After that, a variety of learning-based methods have been investigated. Zhao and Ng (2007) use the learning-based model to locate and resolve zero anaphoras. They investigate a serious of features and apply the decision-tree algorithm to train the classifier. To better capture the syntactic-level information, Kong and Zhou (2010) introduce the context sensitive tree-kernel unified framework for zero anaphor resolution. On the base of Zhao and Ng (2007), Chen and Ng (2013) further investigate their model, introducing two extensions to the resolver, namely, novel features and zero pronoun links. However, these work deeply rely on annotation dataset. To alleviate this issue, Chen and Ng (2014) present the first unsupervised model that first convert zero anaphoras into ten pre-defined pronouns and then apply a ranking-based pronoun resolution model to select antecedent mentions. Chen and Ng (2015) build a discourse-aware"
C18-1002,P17-1010,1,0.759771,"vitably misses some valuable information. Naturally, some works try to alleviate this issue by investigating information from associate texts. Yin et al. (2017a) introduce a novel memory-based network neural network model that learns to encode zero anaphoras by its texts and antecedent mentions. They take advantage of multihops architecture, producing abstract information from external-memories as hints for explaining zero anaphoras. Yin et al. (2017b) focus on encoding global-information for candidates, where a hierarchical candidate encoder is introduced that learns to model the candidates. Liu et al. (2017) investigate the issue of generating pseudo training-data for the task of zero anaphora resolution. They use a novelty two-step training strategy that helps to overcome the diversity between the generated pseudo training-data and the real one. Even though these above-mentioned methods can reveal the semantic of zero anaphoras by its context, they regard all the words equally, overlooking the diversity of different words. In this paper, we focus on exploring an effective way of modeling zero pronoun by using the associated texts. More specifically, we integrate a novel self-attentive mechanism,"
C18-1002,I11-1085,0,0.630928,"he mentions. All these bring advantages to the resolution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolution. Compared to the prior studies that have the underutilized conte"
C18-1002,D16-1021,1,0.813139,"argue that some of the words in noun phrases contains more important information than others, a need that leads to the usage of attention mechanism. To alleviate the above-mentioned issues, in this paper, we propose a novel attention-based neural network model to deal with the task. Following existing neural network work for Chinese zero pronoun resolution (Chen and Ng, 2016; Yin et al., 2017a; Yin et al., 2017b), we focus on anaphoric zero pronoun resolution task, introducing a pair-wise model to resolve anaphoric zero pronouns. For some natural language processing tasks (Mnih et al., 2014; Tang et al., 2016), people investigate to apply attention mechanism on top of the convolutional neural network or recurrent neural network to introduce an extra source of information to guide the modeling of useful information. However, since zero pronouns are simple gaps that have no such kind of extra information, the above-mentioned attention mechanism can rarely be directly practiced for modeling these gaps. Inspired by (Lin et al., 2017), we here investigate the usage of a self-attentive mechanism for encoding the zero anaphoras. With the help of self-attentive mechanism, our model is able to effectively f"
C18-1002,D17-1135,1,0.274906,". ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 13 Proceedings of the 27th International Conference on Computational Linguistics, pages 13–23 Santa Fe, New Mexico, USA, August 20-26, 2018. With the fact that zero pronouns are gaps that have no text, it is almost impracticable to representant the zero pronouns by themselves. This issue has received increasingly attention. In recent time, deep neural network methods for Chinese zero pronoun resolution (Chen and Ng, 2016; Yin et al., 2017a; Yin et al., 2017b) have been proposed and are intended to encode zero pronouns into the vector-space semantic by additional elements. Chen and Ng (2016) propose a neural network model that learns to encode the anaphoric zero pronoun by using the leading word and governing verb, which leads to the insufficient text issue. To better use associated text information for expressing zero pronouns, Yin et al. (2017a) present the ZP-centered-LSTM architecture that learns to encode zero pronouns by their text words. However, it could bring with some defects: their model regards all the words in the"
C18-1002,D07-1057,0,0.802724,"noun resolution. Next, we will intorduce our attention-based neural network model in Section 3. In Section 4, empirical evaluation results are shown. And finally, we conclude in Section 5. 14 2 Related Work In this section, we give a brief summary of early efforts for zero pronoun resolution both for Chinese and other languages. 2.1 Zero Pronoun Resolution for Chinese Converse (2006) is the first rule-based study that integrate Hobbs-algorithm into the resolution of zero pronoun in the Chinese Treebank (Xue et al., 2005). After that, a variety of learning-based methods have been investigated. Zhao and Ng (2007) use the learning-based model to locate and resolve zero anaphoras. They investigate a serious of features and apply the decision-tree algorithm to train the classifier. To better capture the syntactic-level information, Kong and Zhou (2010) introduce the context sensitive tree-kernel unified framework for zero anaphor resolution. On the base of Zhao and Ng (2007), Chen and Ng (2013) further investigate their model, introducing two extensions to the resolver, namely, novel features and zero pronoun links. However, these work deeply rely on annotation dataset. To alleviate this issue, Chen and"
C18-1088,P08-1090,0,0.414924,"ion again received increasing interests in the natural language processing (NLP) community. In this paper, we propose using adversarial training augmented Seq2Seq model to generate story ending given a story context. For example, given the story context “Sara had lost her cat. She was so sad! She put up signs all over the neighborhood. Then a wonderful thing happened.”, our goal is to generate a possibly reasonable story ending “Somebody found her cat”. Much previous work in story generation or prediction focused on learning statistical models of event sequences from large-scale text corpora. Chambers and Jurafsky (2008) proposed unsupervised induction of narrative event chains from raw newswire texts, with narrative cloze as the evaluation metric. However, they utilized a very impoverished representation of events as the form of (event, dependency). To overcome the drawback of this event representation, Pichotta and Mooney (2014) presented a script learning approach that employed events with multiple arguments. Pichotta and Mooney (2016a) showed that the LSTM-based event sequence model outperformed previous co-occurrence-based methods for event prediction. However, this line of work build their models based"
C18-1088,D17-1168,0,0.0189047,"redict sentences, finding it is roughly comparable with systems operating on structured verbargument events. Granroth-Wilding and Clark (2016) described a feedforward neural network which composed verbs and arguments into low-dimensional vectors, evaluating on a multiple-choice version of the Narrative Cloze task. However, most of this line of work build their models based on discrete verbs and tokens, which is far from being a complete sentence or story segment as used in this paper. A line of works has studied the problem of Story Cloze Test on ROCStories corpus (Mostafazadeh et al., 2016). Chaturvedi et al. (2017) explored three distinct semantic aspects including sequence of 1040 events, emotional trajectory and plot consistency, and used a hidden variable coherence model to join these aspects together. Lin et al. (2017) adopted a similar method, and also exploited heterogeneous knowledge for this task. Wang et al. (2017) applied adversarial networks on this task, which is most similar to our work. However, all these studies put their focuses on choosing the correct story ending through discriminative approaches. We mainly aim to generate reasonable and diversified story endings. 4.2 Sequence to Seque"
C18-1088,E12-1034,0,0.0509143,"Missing"
C18-1088,P15-1107,0,0.0286644,"ord prediction, the embedding of which is then combined with preceding output for next step token prediction. 2.2 Discriminator In the idea of adversarial training, we want to produce more natural, diversified and logically reasonable 2 in Figure 2) that judges whether the story endings. To this end, we introduce a discriminator (see  input is generated by human-beings or machines, and transfers this signal to the Seq2Seq generator, to help adjust its parameters for generating our expected story endings. 1035 The discriminator D is a binary classifier. We use a hierarchical sequence encoder (Li et al., 2015) to map a complete story {X, Y } into a vector representation, which is then fed to a 2-class softmax function, returning the probability of the story being a machine-generated story ending (denoted by R− ({X, Y })) or a human-generated one (denoted by R+ ({X, Y })). 2.3 Adversarial Training Process In the adversarial training augmented Seq2Seq model, the generator is encouraged to generate story endings that are indistinguishable from human generated story endings. We use policy gradient methods 3 in Figure 2). The score R+ ({X, Y }) of the current generated story ending is to achieve such go"
C18-1088,N16-1014,0,0.073395,"Missing"
C18-1088,D17-1230,0,0.0325056,"perty of text generation makes the error computed by the discriminator hard to backpropagate to the generator. Some recent work try to address this issue: (Lamb et al., 2016) proposed providing the discriminator with intermediate hidden vectors of the generator, which makes the system differentiable and achieves promising results in language modeling and handwriting generation tasks. Yu et al. (2016) used policy gradient reinforcement learning to backpropagate the error from the discriminator, showing improvement in multiple generation tasks such as poem, speech language and music generation. Li et al. (2017) used a similar strategy to boost the dialogue generation quality, which achieved good experimental results. Not limited to the task of sequence generation, Chen et al. (2016) applied the idea of adversarial training to sentiment analysis, and (Zhang et al., 2017) investigated the problem of domain adaptation based on adversarial networks. To our knowledge, this is the first paper to study adversarial training augmented Seq2Seq model on the task of generating story endings. 5 Conclusion Story generation is a challenging problem in AI. In this paper, we explore new ways to generate story ending"
C18-1088,D17-1216,0,0.0376771,"Missing"
C18-1088,1985.tmi-1.17,0,0.444122,"fectly. Hence, we believe that Seq2Seq-Adversarial model can generate more diversiform knowledge for this world. In case three, Seq2Seq-MLE generates a better story ending than Seq2Seq-Adversarial. This is mainly because in the adversarial training process, the MC search sampled the go to church phrase and it got a high reward score from the discriminator. This can be improved by carefully controlling the MC search process and filtering the meaningless words or phrases. 4 Related Work 4.1 Script Learning The use of scripts in AI dates back to the 1970s (Minksy, 1975; Schank and Abelson, 1977; Mooney and DeJong, 1985). In this conception, scripts are composed of complex events without probabilistic semantics. In recent years, a growing body of research has investigated learning probabilistic co-occurrencebased models with simpler events. Chambers and Jurafsky (2008) proposed unsupervised induction of narrative event chains from raw newswire text, with narrative cloze as the evaluation metric, and pioneered the recent line of work on statistical script learning (Jans et al., 2012; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016b; Granroth-Wilding and Clark, 2016). However, they utilized a very impoveri"
C18-1088,N16-1098,0,0.315334,"that labels story endings as human-generated or machinegenerated. In adversarial training augmented Seq2Seq model, the generator is encouraged to generate story endings that are indistinguishable from human generated story endings, and the discriminator gives a score of judging whether the current story ending is generated by the human or the machine, which can be used as a reward for the generator. Then the generator is trained to maximize the expected reward of the generated story ending using REINFORCE algorithm (Williams, 1992). We conducted extensive experiments on the ROCStories corpus (Mostafazadeh et al., 2016). Carefully designed human evaluation demonstrates that adversarial training augmented Seq2Seq model can generate logically better story endings than conventional Seq2Seq model. Automatic evaluation metrics also suggest that adversarial training can improve the diversity of the generated story endings. Furthermore, we evaluate the effectiveness of our approach on the task of Story Cloze Test, which requires to choose the right story ending from two candidates given a story context. Our model chooses the right ending based on average word embedding similarities between our generated ending and"
C18-1088,P02-1040,0,0.111747,"lly correct to the story context. We give the following three aspects judgement criteria for the annotators: (a) Grammar and Fluency: Endings should be natural language and free of fluency and grammar errors. (b) Context Relevance: Person names, pronouns and phrases in the endings should be relevant to the story context. (c) Logic Consistency: Endings should be logically consistent with the story context. Furthermore, we ask the annotators to directly compare the story endings that generated by the baseline method and our approach, and choose the better one. We did not use perplexity or BLEU (Papineni et al., 2002) as evaluation metric, as neither of them is likely to be an effective evaluation metric in our scenario. Because our proposed model is designed to steer away from the standard Seq2Seq model, in order to generate more reasonable and diversified story endings. Following Li et al. (2016), we report the degree of diversity by calculating the number of distinct unigrams and bigrams in generated story endings. In order to avoid favoring long sentences, the value is scaled by total number of generated tokens. 3.3 Baselines and Proposed Models For the evulation of story ending generation quality, the"
C18-1088,E14-1024,0,0.0766748,"r the neighborhood. Then a wonderful thing happened.”, our goal is to generate a possibly reasonable story ending “Somebody found her cat”. Much previous work in story generation or prediction focused on learning statistical models of event sequences from large-scale text corpora. Chambers and Jurafsky (2008) proposed unsupervised induction of narrative event chains from raw newswire texts, with narrative cloze as the evaluation metric. However, they utilized a very impoverished representation of events as the form of (event, dependency). To overcome the drawback of this event representation, Pichotta and Mooney (2014) presented a script learning approach that employed events with multiple arguments. Pichotta and Mooney (2016a) showed that the LSTM-based event sequence model outperformed previous co-occurrence-based methods for event prediction. However, this line of work build their models based on discrete verbs and tokens, which is far from being a complete sentence or a story. There have been a number of recent work for story generation focusing on complete sentences. Kiros et al. (2015) described an approach for unsupervised learning of a generic, distributed sentence representations called skip-though"
C18-1088,P16-1027,0,0.0549661,"ing “Somebody found her cat”. Much previous work in story generation or prediction focused on learning statistical models of event sequences from large-scale text corpora. Chambers and Jurafsky (2008) proposed unsupervised induction of narrative event chains from raw newswire texts, with narrative cloze as the evaluation metric. However, they utilized a very impoverished representation of events as the form of (event, dependency). To overcome the drawback of this event representation, Pichotta and Mooney (2014) presented a script learning approach that employed events with multiple arguments. Pichotta and Mooney (2016a) showed that the LSTM-based event sequence model outperformed previous co-occurrence-based methods for event prediction. However, this line of work build their models based on discrete verbs and tokens, which is far from being a complete sentence or a story. There have been a number of recent work for story generation focusing on complete sentences. Kiros et al. (2015) described an approach for unsupervised learning of a generic, distributed sentence representations called skip-thought vectors. Such vectors can be used to predict neighboring sentences in the context. Pichotta and Mooney (201"
C18-1088,P15-1152,0,0.030274,"ong. Hence, sampling negative wrong endings from other stories is unreasonable. Detailed statistics of training, development and test datasets are shown in Table 1. All methods are evaluated on the test dataset, and only the development dataset could be used for tuning purposes. 3.2 Evaluation Metrics For the task of Story Cloze Test, we use accuracy to evaluate the effectiveness of our approach. For the task of story ending generation, we ask human annotators to evaluate the performance of our approach, as it is difficult to find a generally accepted automatic metric for this task. Following Shang et al. (2015), we carefully designed the human evaluation procedure to evaluate the ability of our proposed model on generating story endings. We randomly pick 100 story endings generated by the baseline method and our approach on the test dataset respectively, and distribute them to three annotators. The annotators read the story context, and judge whether the generated story ending is appropriate and reasonable according to the story context. Four levels are assigned to each ending with scores from 0 to 3: • • • • Bad (0): The ending doesn’t make sense and unrelated to the story context. Relevant (+1): T"
C18-1088,Q17-1036,0,0.0300465,", which makes the system differentiable and achieves promising results in language modeling and handwriting generation tasks. Yu et al. (2016) used policy gradient reinforcement learning to backpropagate the error from the discriminator, showing improvement in multiple generation tasks such as poem, speech language and music generation. Li et al. (2017) used a similar strategy to boost the dialogue generation quality, which achieved good experimental results. Not limited to the task of sequence generation, Chen et al. (2016) applied the idea of adversarial training to sentiment analysis, and (Zhang et al., 2017) investigated the problem of domain adaptation based on adversarial networks. To our knowledge, this is the first paper to study adversarial training augmented Seq2Seq model on the task of generating story endings. 5 Conclusion Story generation is a challenging problem in AI. In this paper, we explore new ways to generate story ending given a four-sentence story context. In order to generate high-quality story endings, we adopt the idea of adversarial training from Generative Adversarial Nets and propose using adversarial training augmented Seq2Seq model to generate reasonable and diversified"
C18-1105,P05-1074,0,0.093071,"ese classic approaches, adding noise to the image, randomly interpolating a pair of images (Zhang et al., 2018) are also proposed in previous works. However, these signal transformation approaches are not directly applicable to language because order of words in language may form rigorous syntactic and semantic meaning (Zhang et al., 2015). Therefore, the best way of data augmentation in language usually involves generating the alternative expressions. Paraphrasing is the most studied techniques in natural language processing for generating alternative expressions (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Callison-Burch, 2008). However, generic paraphrasing technique has been reported not helpful for specific problem (Narayan et al., 2016). Most of the successful work that applying paraphrasing for data augmentation requires special tailored paraphrasing techniques. For example, Wang and Yang (2015) performed word-level paraphrasing to extend their corpus on twitter that contains annoying behaviors. Fader et al. (2013) derived question templates from seed paraphrases and bootstrap the templates to achieve the enlarged open-domain QA dataset. Narayan et al. (2016) constructed latent variable P"
C18-1105,P01-1008,0,0.0305918,"sky et al., 2012). Beyond these classic approaches, adding noise to the image, randomly interpolating a pair of images (Zhang et al., 2018) are also proposed in previous works. However, these signal transformation approaches are not directly applicable to language because order of words in language may form rigorous syntactic and semantic meaning (Zhang et al., 2015). Therefore, the best way of data augmentation in language usually involves generating the alternative expressions. Paraphrasing is the most studied techniques in natural language processing for generating alternative expressions (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Callison-Burch, 2008). However, generic paraphrasing technique has been reported not helpful for specific problem (Narayan et al., 2016). Most of the successful work that applying paraphrasing for data augmentation requires special tailored paraphrasing techniques. For example, Wang and Yang (2015) performed word-level paraphrasing to extend their corpus on twitter that contains annoying behaviors. Fader et al. (2013) derived question templates from seed paraphrases and bootstrap the templates to achieve the enlarged open-domain QA dataset. Narayan et al. (2"
C18-1105,D08-1021,0,0.0143971,"se to the image, randomly interpolating a pair of images (Zhang et al., 2018) are also proposed in previous works. However, these signal transformation approaches are not directly applicable to language because order of words in language may form rigorous syntactic and semantic meaning (Zhang et al., 2015). Therefore, the best way of data augmentation in language usually involves generating the alternative expressions. Paraphrasing is the most studied techniques in natural language processing for generating alternative expressions (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Callison-Burch, 2008). However, generic paraphrasing technique has been reported not helpful for specific problem (Narayan et al., 2016). Most of the successful work that applying paraphrasing for data augmentation requires special tailored paraphrasing techniques. For example, Wang and Yang (2015) performed word-level paraphrasing to extend their corpus on twitter that contains annoying behaviors. Fader et al. (2013) derived question templates from seed paraphrases and bootstrap the templates to achieve the enlarged open-domain QA dataset. Narayan et al. (2016) constructed latent variable PCFG for questions and a"
C18-1105,W17-5506,0,0.0578439,"nd syntactical alternatives. To further encourage diverse generation, we incorporate a novel diversity rank into the utterance representation. When training the seq2seq model, the diversity rank is also used to filter the over-alike pairs of alternatives. These approaches lead to diversely augmented data that significantly improves the LU performance in the domains that labeled data is scarce. We conduct experiments on the Airline Travel Information System dataset (ATIS, Price 1990) along with a newly annotated layer of slot filling over the Stanford Multi-turn, Multi-domain Dialogue Dataset (Eric and Manning, 2017).1 On the small proportion of ATIS which contains 129 utterances, our method outperforms the baseline by a 6.38 F-score on slot filling. On the medium proportion, this improvement is 2.87. Similar trends are witnessed on our LU annotation over Stanford dialogue dataset which the average improvement on three new domains is 10.04 on 100 utterances and 0.47 on 500 utterances. The major contributions of this paper include: • We propose a data augmentation framework for LU (§2) using the seq2seq model. A novel diversity rank (§3) is used to encourage our seq2seq model to generate diverse utterances"
C18-1105,P13-1158,0,0.640951,"training data makes LU vulnerable to unseen utterances which are syntactically different but semantically related to the existing training data, and further harms the whole task-oriented dialogue system pipeline. Data augmentation, which enlarges the size of training data in machine learning systems, is an effective solution to the data insufficiency problem. Success has been achieved with data augmentation on a wide range of problems including computer vision (Krizhevsky et al., 2012), speech recognition (Hannun et al., 2014), text classification (Zhang et al., 2015), and question answering (Fader et al., 2013). However, its application in the task-oriented dialogue system is less studied. Kurata et al. (2016a) presented the only work we know that tried to augment data for LU. In their paper, an encoder-decoder is This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ * Email corresponding. Licence details: http:// 1234 Proceedings of the 27th International Conference on Computational Linguistics, pages 1234–1245 Santa Fe, New Mexico, USA, August 20-26, 2018. learned to reconstruct the utterances in the training data. During the aug"
C18-1105,D13-1111,0,0.024998,"diverse rank k, we use the standard seq2seq model to generate the alternative delexicalised utterance d0 . In our seq2seq model, we append #k to the end of the input utterances and the model is formalized as Y p(d0 |d, k) = p(d0t |d1 , ..., dn , #k, d01 , ..., d0t−1 ) t where n is the number of words for the input utterance d. In this paper, we follow the seq2seq model for neural machine translation and use the input-feeding network in (Luong et al., 2015) with attention as our seq2seq model. During testing, we use beam search with beam size of 10 to yield more than one translation following Gimpel et al. (2013) and Vijayakumar et al. (2016). To train the seq2seq model, our basic assumption is that if d and d0 contain the same semantic frames, they can be generated from each other. Generally, we assume each pair of delexicalised utterances in the cluster Cs makes a pair of generation. However, it’s nontrivial to assign diverse ranks to training data. What’s more, to prevent the model from just producing produce lexical paraphrases (like “show me” to “give me”), we propose to also consider the diversities when generating training translations for the seq2seq model. We will talk about the details in Se"
C18-1105,P16-1002,0,0.0493762,"encoder’s output hidden states are randomly perturbed to yield different utterances. The work of Kurata et al. (2016a) augments one single utterance by adding noise without considering its relation with other utterances. Besides theirs, there are also works which explicitly consider the paraphrasing relations between instances that share the same output. These works achieve improvements on tasks like text classification and question answering. Paraphrasing techniques including word-level substitution (Zhang et al., 2015; Wang and Yang, 2015), hand-crafted rules generation (Fader et al., 2013; Jia and Liang, 2016), and grammar-tree generation (Narayan et al., 2016) have been explored. Compared with these work, Kurata et al. (2016a) has the advantage of fully data-driven method and can easily switch to new domain without too much domain-specific knowledge, but doesn’t make use of the relations between instances within the training data. In this paper, we study the problem of data augmentation for LU and propose a novel data-driven framework that models relations between utterances of the same semantic frame in the training data. A sequence-to-sequence (seq2seq, Sutskever et al. 2014) model lies in the c"
C18-1105,P17-4012,0,0.0152664,"’t associate the semantic class of the slot with corresponding segment in the utterance. Our annotation focus on assigning the slot to its corresponding segment. During the annotation, each dialogue was processed by two annotators. Data statistics, Kappa value (Snow et al., 2008), and inner annotator agreement measured by F-score on the three domains are shown in Table 1. Evaluation. We evaluate our data augmentation’s effect on LU with F-score. conlleval is used in the same way with previous works (Mesnil et al., 2013; Mesnil et al., 2015; Chen et al., 2016a). Implementation. We use OpenNMT (Klein et al., 2017) as the implementation of our seq2seq model. We set the number of layers in LSTM as 2 and the size of hidden states as 500. Utterances that are longer than 50 are truncated. We adopt the same training setting as Luong et al. (2015) and use Adam (Kingma and Ba, 2014) to train the seq2seq model. Learning rate is halved when perplexity on the development set doesn’t decrease. During generation, we replace the model-yielded unknown token (unk) with the source word that has the highest attention score. For the slot tagging model, we set both the dimension for word embedding and the size of hidden s"
C18-1105,W18-5007,0,0.0132646,"features (Chen et al., 2016a) and representation in broader scope on sentence-level (Kurata et al., 2016c) and dialogue history-level (Chen et al., 2016b) have also been studied. Our augmentation method is orthogonal to these works and it’s hopeful to achieve more improvements with their works. Dialogue management is also a key component of task-oriented dialogue system, which mainly focuses on dialogue policy. However, optimal dialogue policy is hard to obtain from a static corpus due to the vast space of conversation process. A solution is to transform the static corpus into user simulator (Kreyssig et al., 2018), and most user simulators work on user semantics level. (Eckert et al., ; Schatzmann et al., 2007a; Asri et al., 2016; Scheffler and Young, 2000; Scheffler and Young, 2001; Pietquin and Dutoit, 2006; Georgila et al., 2005; Cuay´ahuitl et al., 2005). Recent work starts to generate user utterance directly to reduce data annotation(Kreyssig et al., 2018). In recent years, Generative Adversarial Network (GAN, Goodfellow et al. 2014) draws a lot of research attention. Its ability of generating adversarial examples is attractive for data augmentation. However, it hasn’t been tried in data augmentat"
C18-1105,D16-1223,0,0.144508,"cally related to the existing training data, and further harms the whole task-oriented dialogue system pipeline. Data augmentation, which enlarges the size of training data in machine learning systems, is an effective solution to the data insufficiency problem. Success has been achieved with data augmentation on a wide range of problems including computer vision (Krizhevsky et al., 2012), speech recognition (Hannun et al., 2014), text classification (Zhang et al., 2015), and question answering (Fader et al., 2013). However, its application in the task-oriented dialogue system is less studied. Kurata et al. (2016a) presented the only work we know that tried to augment data for LU. In their paper, an encoder-decoder is This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ * Email corresponding. Licence details: http:// 1234 Proceedings of the 27th International Conference on Computational Linguistics, pages 1234–1245 Santa Fe, New Mexico, USA, August 20-26, 2018. learned to reconstruct the utterances in the training data. During the augmenting process, the encoder’s output hidden states are randomly perturbed to yield different uttera"
C18-1105,D15-1166,0,0.0513772,"ective numbers of ranks during testing in Section 3. Data Augmentation as Seq2Seq Generation. When given the delexicalised input utterance d and the specified diverse rank k, we use the standard seq2seq model to generate the alternative delexicalised utterance d0 . In our seq2seq model, we append #k to the end of the input utterances and the model is formalized as Y p(d0 |d, k) = p(d0t |d1 , ..., dn , #k, d01 , ..., d0t−1 ) t where n is the number of words for the input utterance d. In this paper, we follow the seq2seq model for neural machine translation and use the input-feeding network in (Luong et al., 2015) with attention as our seq2seq model. During testing, we use beam search with beam size of 10 to yield more than one translation following Gimpel et al. (2013) and Vijayakumar et al. (2016). To train the seq2seq model, our basic assumption is that if d and d0 contain the same semantic frames, they can be generated from each other. Generally, we assume each pair of delexicalised utterances in the cluster Cs makes a pair of generation. However, it’s nontrivial to assign diverse ranks to training data. What’s more, to prevent the model from just producing produce lexical paraphrases (like “show m"
C18-1105,W16-6625,0,0.148768,"ed to yield different utterances. The work of Kurata et al. (2016a) augments one single utterance by adding noise without considering its relation with other utterances. Besides theirs, there are also works which explicitly consider the paraphrasing relations between instances that share the same output. These works achieve improvements on tasks like text classification and question answering. Paraphrasing techniques including word-level substitution (Zhang et al., 2015; Wang and Yang, 2015), hand-crafted rules generation (Fader et al., 2013; Jia and Liang, 2016), and grammar-tree generation (Narayan et al., 2016) have been explored. Compared with these work, Kurata et al. (2016a) has the advantage of fully data-driven method and can easily switch to new domain without too much domain-specific knowledge, but doesn’t make use of the relations between instances within the training data. In this paper, we study the problem of data augmentation for LU and propose a novel data-driven framework that models relations between utterances of the same semantic frame in the training data. A sequence-to-sequence (seq2seq, Sutskever et al. 2014) model lies in the core of our framework which takes a delexicalised utt"
C18-1105,D14-1162,0,0.0824305,"and Ba, 2014) to train the seq2seq model. Learning rate is halved when perplexity on the development set doesn’t decrease. During generation, we replace the model-yielded unknown token (unk) with the source word that has the highest attention score. For the slot tagging model, we set both the dimension for word embedding and the size of hidden state to 100. We also vary dropout rate in {0, 0.1, 0.2} considering its regularization power on small size of data. The batch size is set to 16 in all the experiments. Best hyperparameter settings are determined on the development set. GloVe embedding (Pennington et al., 2014) is used to initialize the word embedding in the model. Adam with the suggested settings in Kingma and Ba (2014) is used to train the parameters. Reimers and Gurevych (2017) pointed out that neural network training is nondeterministic and depends on the seed for the random number generator. We witness dramatic changes of the slot tagging performance using different random seeds. To control for this effect, we take their suggestions and report the average of 5 differently-seeded runs. 5.2 Results on ATIS Table 2 shows the slot tagging results on the ATIS dataset. Our baseline model is the vanil"
C18-1105,H90-1020,0,0.0685793,"kever et al. 2014) model lies in the core of our framework which takes a delexicalised utterance and generates its lexical and syntactical alternatives. To further encourage diverse generation, we incorporate a novel diversity rank into the utterance representation. When training the seq2seq model, the diversity rank is also used to filter the over-alike pairs of alternatives. These approaches lead to diversely augmented data that significantly improves the LU performance in the domains that labeled data is scarce. We conduct experiments on the Airline Travel Information System dataset (ATIS, Price 1990) along with a newly annotated layer of slot filling over the Stanford Multi-turn, Multi-domain Dialogue Dataset (Eric and Manning, 2017).1 On the small proportion of ATIS which contains 129 utterances, our method outperforms the baseline by a 6.38 F-score on slot filling. On the medium proportion, this improvement is 2.87. Similar trends are witnessed on our LU annotation over Stanford dialogue dataset which the average improvement on three new domains is 10.04 on 100 utterances and 0.47 on 500 utterances. The major contributions of this paper include: • We propose a data augmentation framewor"
C18-1105,D17-1035,0,0.0171839,"unknown token (unk) with the source word that has the highest attention score. For the slot tagging model, we set both the dimension for word embedding and the size of hidden state to 100. We also vary dropout rate in {0, 0.1, 0.2} considering its regularization power on small size of data. The batch size is set to 16 in all the experiments. Best hyperparameter settings are determined on the development set. GloVe embedding (Pennington et al., 2014) is used to initialize the word embedding in the model. Adam with the suggested settings in Kingma and Ba (2014) is used to train the parameters. Reimers and Gurevych (2017) pointed out that neural network training is nondeterministic and depends on the seed for the random number generator. We witness dramatic changes of the slot tagging performance using different random seeds. To control for this effect, we take their suggestions and report the average of 5 differently-seeded runs. 5.2 Results on ATIS Table 2 shows the slot tagging results on the ATIS dataset. Our baseline model is the vanilla BiLSTM slot tagger and our augmented slot tagger use the same architecture but is trained with the augmented data generated by our method. Compared with the vanilla tagge"
C18-1105,N07-2038,0,0.0354073,"ve the enlarged open-domain QA dataset. Narayan et al. (2016) constructed latent variable PCFG for questions and augment the training data by sampling from the grammar. All these works assume the same output (i.e. 1242 class in text classification, answer in question answering) for input paraphrases. Our method resembles theirs in the assumption for input paraphrases, but differs on using the seq2seq generation which is purely data-driven and doesn’t rely on special tailored domain knowledge. Besides these methods, works that introduce errors to language understanding have also been proposed (Schatzmann et al., 2007b; Sagae et al., 2012). Language understanding, as an important component in the task-oriented dialogue system pipeline, has drawn a lot of research attention in recent year, especially when enhanced by the rich representation power of the neural network, like recurrent neural network, LSTM (Yao et al., 2013; Yao et al., 2014; Mesnil et al., 2013; Mesnil et al., 2015) and memory network (Chen et al., 2016b). Rich linguistic features (Chen et al., 2016a) and representation in broader scope on sentence-level (Kurata et al., 2016c) and dialogue history-level (Chen et al., 2016b) have also been st"
C18-1105,D08-1027,0,0.0119547,"Missing"
C18-1105,D15-1306,0,0.316183,"the utterances in the training data. During the augmenting process, the encoder’s output hidden states are randomly perturbed to yield different utterances. The work of Kurata et al. (2016a) augments one single utterance by adding noise without considering its relation with other utterances. Besides theirs, there are also works which explicitly consider the paraphrasing relations between instances that share the same output. These works achieve improvements on tasks like text classification and question answering. Paraphrasing techniques including word-level substitution (Zhang et al., 2015; Wang and Yang, 2015), hand-crafted rules generation (Fader et al., 2013; Jia and Liang, 2016), and grammar-tree generation (Narayan et al., 2016) have been explored. Compared with these work, Kurata et al. (2016a) has the advantage of fully data-driven method and can easily switch to new domain without too much domain-specific knowledge, but doesn’t make use of the relations between instances within the training data. In this paper, we study the problem of data augmentation for LU and propose a novel data-driven framework that models relations between utterances of the same semantic frame in the training data. A"
C18-1206,S17-1008,0,0.0177871,"of a conversation by not only considering the quality of single turn response generation, but also considering long-term goal of the conversation. To address the problems of generating generic and repetitive response of the RNN encoder-decoder framework, Li et al. (2016c) proposed a deep reinforcement learning approach to either generate meaningful and diverse response or increase the length of the generated dialogues. Dhingra et al. (2017) presented an end-to-end dialogue system for information accquisition, which is called KB-InfoBot from knowledge base (KB) by using reinforcement learning. Asghar et al. (2017) proposed an active learning approach to learn user explicit feedback online and combine the offline supervised learning for response generation of conversational agents. 5 Conclusion and Future Work This paper proposed a novel context-sensitive generation approach for open-domain conversational responses. The proposed model gained from the proposed static and dynamic attention for context or inter-utterance representation. Experimental results show that the proposed model generally outperforms all the baselines in automatic and human evaluations. It is also verified the impact of context leng"
C18-1206,P12-3007,0,0.187023,"perimental results on two public datasets show that the proposed static attention based approach outperforms all the baselines on automatic and human evaluation. 1 Introduction Until recently, training open-domain conversational systems that can imitate the way of human conversing is still not a well-solved problem and non-trivial task. Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010), a phrasebased statistical machine translation task (Ritter et al., 2011) and a search problem based on the vector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017; Xing et al., 2017)."
C18-1206,P12-2040,0,0.0260643,"Ubuntu test result. α indicates the attention value of each utterance in context calculated by the proposed static attention mechanism. 4 Related Work Ritter et al. (2010) proposed an unsupervised approach to model dialogue response by clustering the raw utterances. They then presented an end-to-end dialogue response generator by using a phrase-based statistical machine translation model (Ritter et al., 2011). Banchs and Li (2012) introduced a search-based system, named IRIS, to generate dialogues using vector space model and then released the experimental corpus for research and development (Banchs, 2012). Recently, benefit from the advantages of the sequence-to-sequence learning framework with neural networks, Sutskever et al. (2014) and Shang et al. (2015) had drawn inspiration from the neural machine translation (Bahdanau et al., 2014) and proposed an RNN encoder-decoder based approach to generate dialogue by considering the last one sentence and a larger range of context respectively. Serban et al. (2016b) proposed a parallel stochastic generation framework which first generates a coarse sequence and then generates an utterance conditioned on the coarse sequence. Shao et al. (2017) introdu"
C18-1206,P17-1045,0,0.0209324,"cent advantages of reinforcement learning on modeling human-computer interactions, such as the AlphaGo (Silver et al., 2016), researchers begin to focus on modeling the success of a conversation by not only considering the quality of single turn response generation, but also considering long-term goal of the conversation. To address the problems of generating generic and repetitive response of the RNN encoder-decoder framework, Li et al. (2016c) proposed a deep reinforcement learning approach to either generate meaningful and diverse response or increase the length of the generated dialogues. Dhingra et al. (2017) presented an end-to-end dialogue system for information accquisition, which is called KB-InfoBot from knowledge base (KB) by using reinforcement learning. Asghar et al. (2017) proposed an active learning approach to learn user explicit feedback online and combine the offline supervised learning for response generation of conversational agents. 5 Conclusion and Future Work This paper proposed a novel context-sensitive generation approach for open-domain conversational responses. The proposed model gained from the proposed static and dynamic attention for context or inter-utterance representati"
C18-1206,N16-1014,0,0.703089,"l task. Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010), a phrasebased statistical machine translation task (Ritter et al., 2011) and a search problem based on the vector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017; Xing et al., 2017). Table 1 illustrates how contextual information in conversations impact on the response generation. For instance, given a message1 “How should I tell my mom?”, as input, to a single-turn A: B: A: B: Conversation 1 I got a high score on my exam. Oh! Great! How should I tell my mom? Go and give her a big surprise! A: B: A: B: C"
C18-1206,P16-1094,0,0.375633,"l task. Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010), a phrasebased statistical machine translation task (Ritter et al., 2011) and a search problem based on the vector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017; Xing et al., 2017). Table 1 illustrates how contextual information in conversations impact on the response generation. For instance, given a message1 “How should I tell my mom?”, as input, to a single-turn A: B: A: B: Conversation 1 I got a high score on my exam. Oh! Great! How should I tell my mom? Go and give her a big surprise! A: B: A: B: C"
C18-1206,D16-1127,0,0.485833,"l task. Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010), a phrasebased statistical machine translation task (Ritter et al., 2011) and a search problem based on the vector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017; Xing et al., 2017). Table 1 illustrates how contextual information in conversations impact on the response generation. For instance, given a message1 “How should I tell my mom?”, as input, to a single-turn A: B: A: B: Conversation 1 I got a high score on my exam. Oh! Great! How should I tell my mom? Go and give her a big surprise! A: B: A: B: C"
C18-1206,W15-4640,0,0.0739438,"d has less number of parameters than the hierarchical attention approach. Meanwhile, rather than use a heuristic approach to weigh the importance of each utterance in the context (Tian et al., 2017), in our proposed approach, the weights of utterance in the context are learned by two attention mechanisms from the data, which is more reasonable and flexible than the heuristic based approach. 3 3.1 Experimental Results Experiment Settings Dataset: Two datasets are selected for the experiment of generation of open-domain conversational responses. First is the Ubuntu dataset which is developed by Lowe et al. (2015). The dataset is extracted from the Ubuntu Internet Relayed Chat (IRC) channel and recently used for the generation of conversational responses in (Serban et al., 2016a; Serban et al., 2017b; Serban et al., 2017a). We follow the train-test split proposed by Serban et al. (2017a). It is worthy to note that there is no development set in Serban et al. (2017a). In this paper, we randomly select the same number of sessions to that in the test set from the training set. Second is the OpenSubtitles dataset which is proposed by Tiedemann (2009) and also used by Li et al. (2016a; Li et al. (2016c). Th"
C18-1206,P02-1040,0,0.106259,"ng, is proposed by Serban et al. (2017b). • CVAE: The conditional variational autoencoder based approach, which is proposed by Zhao et al. (2017), to learn context diversity for conversational responses generation. • WSI and HRAN are proposed by Tian et al. (2017) and Xing et al. (2017) respectively. We detailed describe and compare the two models in Section 2.1 and 2.2 and their frameworks are shown in Figure 1. 3.2 3.2.1 Evaluation and Results Automatic Evaluation Until now, automatically evaluating the quality of open-domain conversational response is still an open problem. The BLEU score (Papineni et al., 2002), which is a widely used evaluation metric for machine translation, is not a suitable metric for conversation generation, as the appropriate responses to the same message may share less common words. Moreover, it is also impossible to construct a reference set, which includes all appropriate responses, of each message. The perplexity that is used to evaluate language model, is also not suitable to evaluate the relevance between messages and responses (Shang 2441 Models LSTM HRED VHRED CVAE WSI HRAN Dynamic Dynamic→ Static Static→ Average 0.2300 0.5770 0.5419 0.5672 0.5775 0.5964 0.5750 0.596"
C18-1206,N10-1020,0,0.279946,"s, this paper proposed the static and dynamic attention based approaches for context-sensitive generation of open-domain conversational responses. Experimental results on two public datasets show that the proposed static attention based approach outperforms all the baselines on automatic and human evaluation. 1 Introduction Until recently, training open-domain conversational systems that can imitate the way of human conversing is still not a well-solved problem and non-trivial task. Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010), a phrasebased statistical machine translation task (Ritter et al., 2011) and a search problem based on the vector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversat"
C18-1206,D11-1054,0,0.497624,"or context-sensitive generation of open-domain conversational responses. Experimental results on two public datasets show that the proposed static attention based approach outperforms all the baselines on automatic and human evaluation. 1 Introduction Until recently, training open-domain conversational systems that can imitate the way of human conversing is still not a well-solved problem and non-trivial task. Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010), a phrasebased statistical machine translation task (Ritter et al., 2011) and a search problem based on the vector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et"
C18-1206,P15-1152,0,0.390016,"l not a well-solved problem and non-trivial task. Previous efforts focus on generating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010), a phrasebased statistical machine translation task (Ritter et al., 2011) and a search problem based on the vector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017; Xing et al., 2017). Table 1 illustrates how contextual information in conversations impact on the response generation. For instance, given a message1 “How should I tell my mom?”, as input, to a single-turn A: B: A: B: Conversation 1 I got a high score on my exam. Oh! Great! How should I tell my mom? Go and"
C18-1206,D17-1235,0,0.0690174,"nerating opendomain conversational responses as an unsupervised clustering process (Ritter et al., 2010), a phrasebased statistical machine translation task (Ritter et al., 2011) and a search problem based on the vector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017; Xing et al., 2017). Table 1 illustrates how contextual information in conversations impact on the response generation. For instance, given a message1 “How should I tell my mom?”, as input, to a single-turn A: B: A: B: Conversation 1 I got a high score on my exam. Oh! Great! How should I tell my mom? Go and give her a big surprise! A: B: A: B: Conversation 2 I failed to pass the exa"
C18-1206,P17-2036,0,0.126165,"ector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017; Xing et al., 2017). Table 1 illustrates how contextual information in conversations impact on the response generation. For instance, given a message1 “How should I tell my mom?”, as input, to a single-turn A: B: A: B: Conversation 1 I got a high score on my exam. Oh! Great! How should I tell my mom? Go and give her a big surprise! A: B: A: B: Conversation 2 I failed to pass the exam. That’s too bad. How should I tell my mom? Just tell her the truth and do well next time. Table 1: An example of the impact of contextual information on human conversations. “A” and “B” denote two speakers in the"
C18-1206,D17-1233,0,0.0685272,"conversational responses as an unsupervised clustering process (Ritter et al., 2010), a phrasebased statistical machine translation task (Ritter et al., 2011) and a search problem based on the vector space model (Banchs and Li, 2012), etc. With the booming of deep learning, particularly the neural network based sequence-to-sequence models, generating open-domain conversational responses gradually turns into an end-to-end encoding and decoding process (Sutskever et al., 2014; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016b; Li et al., 2016a; Li et al., 2016b; Shao et al., 2017; Yao et al., 2017). Despite the success of the above research on single-turn conversational response generation, human conversations are usually coherent (Li et al., 2016c) and context-sensitive (Tian et al., 2017; Xing et al., 2017). Table 1 illustrates how contextual information in conversations impact on the response generation. For instance, given a message1 “How should I tell my mom?”, as input, to a single-turn A: B: A: B: Conversation 1 I got a high score on my exam. Oh! Great! How should I tell my mom? Go and give her a big surprise! A: B: A: B: Conversation 2 I failed to pass the exam. That’s too bad."
C18-1206,P17-1061,0,0.327805,"-domain conversational response”. 2437 Proceedings of the 27th International Conference on Computational Linguistics, pages 2437–2447 Santa Fe, New Mexico, USA, August 20-26, 2018. Recent studies on generating open-domain conversational responses begin to explore the context information to generate more informative and coherent responses. Serban et al. (2016a) presented a hierarchical recurrent encoder-decoder (HRED) to recurrently model the dialogue context. Serban et al. (2017b) further introduced a stochastic latent variable at each dialogue turn to improve the diversity of the HRED model. Zhao et al. (2017) proposed a conditional variational autoencoder based approach to learning contextual diversity for neural response generation. Xing et al. (2017) proposed a hierarchical recurrent attention network (HRAN) to jointly model the importance of tokens and utterances. Tian et al. (2017) treated the hierarchical modeling of contextual information as a recurrent process in encoding. We could make two conclusions from these works. • First, existing studies of utterance modeling mainly focus on representing utterances by using bidirectional GRU (Xing et al., 2017) or unidirectional GRU (Tian et al., 20"
C18-1239,C16-1303,1,0.895256,", which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding et al. (2015) show that deep learning representations of event structures yield better accuracies for stock market prediction compared to discrete event features. Chang et al. (2016), Duan et al. (2018) use neural networks to directly learn representations of news abstracts, showing that it is effective for predicting the cumulative abnormal returns of public companies. One limitation of Ding et al. (2015) and Chang et al. (2016), however, is that these methods only model news titles and abstract texts, which are typically single sentences. Ding et al. (2015) show that a model that uses only news content gives inferior results compared to one trained using news titles only, and that adding news content information to a title-driven model does not significantly improve the"
C18-1239,D14-1148,1,0.936087,"tion Texts from the Internet was shown to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding et al. (2015) show that deep learning representations of event structures yield better accuracies fo"
C18-1239,C16-1201,1,0.788438,"to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding et al. (2015) show that deep learning representations of event structures yield better accuracies for stock market prediction compared to d"
C18-1239,N18-1051,1,0.840339,"feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding et al. (2015) show that deep learning representations of event structures yield better accuracies for stock market prediction compared to discrete event features. Chang et al. (2016), Duan et al. (2018) use neural networks to directly learn representations of news abstracts, showing that it is effective for predicting the cumulative abnormal returns of public companies. One limitation of Ding et al. (2015) and Chang et al. (2016), however, is that these methods only model news titles and abstract texts, which are typically single sentences. Ding et al. (2015) show that a model that uses only news content gives inferior results compared to one trained using news titles only, and that adding news content information to a title-driven model does not significantly improve the results. Intuitivel"
C18-1239,P16-1089,0,0.0690722,"Missing"
C18-1239,lee-etal-2014-importance,0,0.0211814,"most informative sentences for market modeling. Results show that document representations can give better performance for estimating cumulative abnormal returns of companies when compared to titles and abstracts. Our model is especially effective when it used to combine information from multiple document sources compared to the sentence-level baselines. 1 Introduction Texts from the Internet was shown to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations"
C18-1239,P15-1107,0,0.46682,"al., 2015), using an embedding vector of the target firm of concern et (c) as the initial state vector for the sentence-level Bi-LSTMs. We average the hidden states of each word in the sentence to obtain the target-dependent news abstract representation va . The vector 2825 et (c) for the company of interest is initialized by averaging the words of its constituents and are finetuned during training. 3.2 Context-sensitive Sentence Representation To preserve the semantic structures of documents and make the sentence representations aware of their contexts, we leverage a hierarchical structure (Li et al., 2015) to encode sentences (the abstracts are not considered) in a document. A sentence-level LSTM is first used to encode words into hidden state vectors, and then a document-level LSTM is applied to encode sentences into hidden state vectors. At the sentence level, given a sentence {w1 , w2 , · · · , wn }, we obtain their embedding forms {~e(w1 ), ~e(w2 ), · · · , ~e(wn )} via a lookup table. A Bi-LSTM is used to capture sentence-level con− → − → − → w w text from {~e(w1 ), ~e(w2 ), · · · , ~e(wn )}, yielding two sequences of hidden states {hw 1 , h2 , · · · , hn } and ← − ← − ← − ← − − → w w w w"
C18-1239,P15-1150,0,0.00951371,"method with a set of baseline representation learning methods for df learning including target-dependent abstract representation method of Chang et al. (2016), and state-of-the-art targetindependent document representations. Target-dependent News Abstract representation (TGT-CTX-LSTM) Chang et al. (2016) give the current state-of-the-art accuracies for CAR prediction by using the abstract only. They used a syntactic parser to analyze the dependency structure of the abstract (as a sentence), and then transform it to a target-specific dependency tree form. They leverage a tree structured LSTM (Tai et al., 2015) to learn abstract representation according to the target-specific tree form. Paragraph Vector We take the paragraph vector embedding of Le and Mikolov (2014) as an unsupervised full document representation baseline. This method gives remarkable performances on tasks such as document classification (Yang et al., 2016) and sentiment analysis (Tang et al., 2015). Target-dependent sentence combination (TD-AVG) This model first learns a context-sensitive representation for all sentences using Bi-LSTM and conditional encoding (Rockt¨aschel et al., 2015). Average pooling is then applied on the sente"
C18-1239,D15-1167,1,0.770456,"rediction by using the abstract only. They used a syntactic parser to analyze the dependency structure of the abstract (as a sentence), and then transform it to a target-specific dependency tree form. They leverage a tree structured LSTM (Tai et al., 2015) to learn abstract representation according to the target-specific tree form. Paragraph Vector We take the paragraph vector embedding of Le and Mikolov (2014) as an unsupervised full document representation baseline. This method gives remarkable performances on tasks such as document classification (Yang et al., 2016) and sentiment analysis (Tang et al., 2015). Target-dependent sentence combination (TD-AVG) This model first learns a context-sensitive representation for all sentences using Bi-LSTM and conditional encoding (Rockt¨aschel et al., 2015). Average pooling is then applied on the sentence representations to obtain the final document representation. This baseline is equivalent to a conditionally encoded version of the target-specific model of Li et al. (2015), but with a different training objective (i.e., classification instead of auto-encoder). We refer to it as TDAVG. We use conditional encoding (Rockt¨aschel et al., 2015) on the sentence"
C18-1239,P14-1109,0,0.134517,"mbine information from multiple document sources compared to the sentence-level baselines. 1 Introduction Texts from the Internet was shown to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding"
C18-1239,P13-1086,0,0.143318,"ing. Results show that document representations can give better performance for estimating cumulative abnormal returns of companies when compared to titles and abstracts. Our model is especially effective when it used to combine information from multiple document sources compared to the sentence-level baselines. 1 Introduction Texts from the Internet was shown to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the spa"
C18-1239,N16-1174,0,0.690061,"tence in the news content to automatically learn its relative importance with regard to the target company. The attention weights are learned automatically towards a final predictive goal. The model is full data-driven, which do not rely on an external syntactic parser as the first step to obtain its target-specific linguistic structures. In addition to Chang et al. (2016) model, which uses only abstract information, we also compare with several state-of-the-art baselines for learning document representations, such as paragraph vector (Le and Mikolov, 2014) and hierarchical attention network (Yang et al., 2016), giving the best reported performances. The advantage of our approach over sentence-level baseline is especially obvious when it is used to combine information from multiple news document sources. In addition, a case study shows that our model can select the sentences that most intuitively help predict stock returns from a full news document. Our contributions can be summarized as follows: • We propose a target-specific document representation model, which leverages the abstracts as evidences to select informative sentences from the documents while disregarding noise. • We are the first, to o"
C18-1320,D17-1040,0,0.114797,"uses key representations to retrieve the corresponding values. However, not all KBs are presented in key-value forms. Besides, an important component of classic pipeline, dialogue state tracker, is not properly modeled, making it difficult to precisely retrieve from KB. In this paper, we propose a novel framework that takes the advantages from both classic pipeline models and Seq2Seq models. We introduce dialogue states into Seq2Seq learning, but in a implicit way. Distributions in classic state tracking are modeled as a group of representation vectors computed by an attention-based network (Britz et al., 2017), which can be considered as a dialogue state representation that aggregates information for each slot. And training this representation doesn’t require annotation of dialogue state tracking. Our model queries the KB entries in an attention-based method as well, so that the querying is differentiable, without domain-specific pre-defined action spaces. Meanwhile we compute the representation for KB using entry-level attention and aggregate the representation with dialogue state representation to form a memory matrix of dialogue history and KB information. While decoding, we perform an attention"
C18-1320,P17-1045,0,0.0265912,"ch tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities based on (subject, relation, object) triples. While Dhingra et al. (2017) applied a soft-KB lookup on an entity-centric knowledge base to compute the probability of that the user knows the values of slots, and has tried to model the posterior distributions over all slots. However, our framework doesn’t require entity-centric knowledge base. 5 Conclusion In this paper, we proposed a framework that leverages dialogue state representation, which is tracked by an attention-based methods. Our framework performed an entry-level soft lookup over the knowledge base, and applied copying mechanism to retrieve entities from knowledge base while decoding. This framework was tr"
C18-1320,E17-2075,0,0.182949,"achieved success on machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015). This success spurs the interests to apply Seq2Seq models into dialogue systems. Seq2Seq models map dialogue history directly into the response in current turn, while requires a minimum amount of hand-crafting. However, conventional Seq2Seq doesn’t model the exterior data retrieval explicitly, which makes it hard for Seq2Seq to generate information stored in KB like meeting time and address, but this kind of retrieval is easy to achieve for classic pipeline. To tackle with the problem, Eric and Manning (2017) use an additional copy mechanism to retrieve entities that occurs in both KB and dialogue history. Eric et al. (2017) further introduced retrieval from key-value KB where the model uses key representations to retrieve the corresponding values. However, not all KBs are presented in key-value forms. Besides, an important component of classic pipeline, dialogue state tracker, is not properly modeled, making it difficult to precisely retrieve from KB. In this paper, we propose a novel framework that takes the advantages from both classic pipeline models and Seq2Seq models. We introduce dialogue s"
C18-1320,W17-5506,0,0.353214,"spurs the interests to apply Seq2Seq models into dialogue systems. Seq2Seq models map dialogue history directly into the response in current turn, while requires a minimum amount of hand-crafting. However, conventional Seq2Seq doesn’t model the exterior data retrieval explicitly, which makes it hard for Seq2Seq to generate information stored in KB like meeting time and address, but this kind of retrieval is easy to achieve for classic pipeline. To tackle with the problem, Eric and Manning (2017) use an additional copy mechanism to retrieve entities that occurs in both KB and dialogue history. Eric et al. (2017) further introduced retrieval from key-value KB where the model uses key representations to retrieve the corresponding values. However, not all KBs are presented in key-value forms. Besides, an important component of classic pipeline, dialogue state tracker, is not properly modeled, making it difficult to precisely retrieve from KB. In this paper, we propose a novel framework that takes the advantages from both classic pipeline models and Seq2Seq models. We introduce dialogue states into Seq2Seq learning, but in a implicit way. Distributions in classic state tracking are modeled as a group of"
C18-1320,P16-1154,0,0.290294,"s information for each slot. And training this representation doesn’t require annotation of dialogue state tracking. Our model queries the KB entries in an attention-based method as well, so that the querying is differentiable, without domain-specific pre-defined action spaces. Meanwhile we compute the representation for KB using entry-level attention and aggregate the representation with dialogue state representation to form a memory matrix of dialogue history and KB information. While decoding, we perform an attention over memory and an attention over input, incorporating copying mechanism (Gu et al., 2016) that allows model to copy words from KBs to enhance the capability of retrieving accurate entities. We evaluate the proposed framework on Stanford Multi-turn, Multi-domain Dialogue Dataset (Eric et al., 2017), to test the effectiveness of our framework and flexibility to apply to different domains. We compare our model with other Seq2Seq models and discovered that our model has outperformed other 3782 address distance poi_type poi traffic_info moderate traffic 899 Ames Ct 5 miles hospital Stanford Childrens Health … … … … … 409 Bollard St 5 miles grocery store willows market no traffic Entry"
C18-1320,P17-1162,0,0.0157569,"n and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities based on (subject, relation, object) triples. While Dhingra et al. (2017) applied a soft-KB lookup on an entity-centric knowledge base to compute the probability of that the user knows the values of slots, and has tried to model the posterior distributions over all slots. However, our framework doesn’t require entity-centric knowledge base. 5 Conclusion"
C18-1320,I17-1074,0,0.0308298,"-to-end training. In contrast to their work, our framework trained the state tracker jointly with the end-to-end dialogue training. Liu and 3789 Lane (2017) built a turn-level LSTM to model the dialogue state and generate probability distribution for each slot. Bordes and Weston (2017) built a system by applying memory network to store the previous dialogue history. But the responses are retrieved from templates, which is significantly different from our neural generative responses. Another type of work tried to build an end-to-end system as a task completion dialogue system (Li et al., 2016; Li et al., 2017; Peng et al., 2017). These modeled are trained through an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to"
C18-1320,D15-1166,0,0.111171,"y involves querying the knowledge base. The action is then converted to its natural language expression using natural language generation. Both natural language understanding and dialogue state tracking require a large amount of domain specific annotation for training, which is expensive to obtain. Besides, the design of actions and the explicit forms of semantic frames require a lot of knowledge from human experts, which are domain-specific as well. Neural generative models, typically Seq2Seq models, have achieved success on machine translation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015). This success spurs the interests to apply Seq2Seq models into dialogue systems. Seq2Seq models map dialogue history directly into the response in current turn, while requires a minimum amount of hand-crafting. However, conventional Seq2Seq doesn’t model the exterior data retrieval explicitly, which makes it hard for Seq2Seq to generate information stored in KB like meeting time and address, but this kind of retrieval is easy to achieve for classic pipeline. To tackle with the problem, Eric and Manning (2017) use an additional copy mechanism to retrieve entities that occurs in both KB and dia"
C18-1320,D17-1237,0,0.0154634,"In contrast to their work, our framework trained the state tracker jointly with the end-to-end dialogue training. Liu and 3789 Lane (2017) built a turn-level LSTM to model the dialogue state and generate probability distribution for each slot. Bordes and Weston (2017) built a system by applying memory network to store the previous dialogue history. But the responses are retrieved from templates, which is significantly different from our neural generative responses. Another type of work tried to build an end-to-end system as a task completion dialogue system (Li et al., 2016; Li et al., 2017; Peng et al., 2017). These modeled are trained through an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve e"
C18-1320,W15-4639,0,0.0683434,"Missing"
C18-1320,E17-1042,0,0.0949969,"Missing"
C18-1320,W13-4065,0,0.196477,"a task-oriented dialogue, where an agent provides with location information for a user. The requirements for the agents to accomplish users’ demands usually involve querying the knowledge base (KB), like acquiring address from location information KB in Figure 1. Typical machine learning approaches model the problem as a partially observable Markov Decision Process (POMDP) (Williams and Young, 2007; Young et al., 2013), where a pipeline system is introduced. The pipeline system consists of four components: natural language understanding (NLU, Tur and De Mori, 2011) , dialogue state tracking (Williams et al., 2013; Williams, 2012), dialogue policy learning (Young et al., 2010) and natural language generation (Wen et al., 2015). Taking the utterance in Figure 1 for example, NLU maps the utterance “Address to the gas station” into semantic slot “POI type”. Dialogue state tracker keeps the probability of “gas station” close to 1 against other values of slot “POI type”. Given a semantic frame as a dialogue state, which is the combination of distributions of these slots, dialogue policy learning generates the next pre-defined system action, This work is licenced under a Creative Commons Attribution 4.0 Inte"
C18-1320,P17-1062,0,0.0361382,"nd system as a task completion dialogue system (Li et al., 2016; Li et al., 2017; Peng et al., 2017). These modeled are trained through an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities base"
C18-1320,W12-1812,0,0.027281,"ue, where an agent provides with location information for a user. The requirements for the agents to accomplish users’ demands usually involve querying the knowledge base (KB), like acquiring address from location information KB in Figure 1. Typical machine learning approaches model the problem as a partially observable Markov Decision Process (POMDP) (Williams and Young, 2007; Young et al., 2013), where a pipeline system is introduced. The pipeline system consists of four components: natural language understanding (NLU, Tur and De Mori, 2011) , dialogue state tracking (Williams et al., 2013; Williams, 2012), dialogue policy learning (Young et al., 2010) and natural language generation (Wen et al., 2015). Taking the utterance in Figure 1 for example, NLU maps the utterance “Address to the gas station” into semantic slot “POI type”. Dialogue state tracker keeps the probability of “gas station” close to 1 against other values of slot “POI type”. Given a semantic frame as a dialogue state, which is the combination of distributions of these slots, dialogue policy learning generates the next pre-defined system action, This work is licenced under a Creative Commons Attribution 4.0 International Licence"
C18-1320,W16-0106,0,0.0308129,"hrough an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities based on (subject, relation, object) triples. While Dhingra et al. (2017) applied a soft-KB lookup on an entity-centric knowledg"
C18-1320,W16-0105,0,0.0316997,"hrough an end-to-end fashion via reinforcement learning algorithm using the reward from user simulators. However, their dialogue responses are not generated from the dialogue history directly but from a set of pre-defined action and explicit semantic frames. Our soft KB attention can be considered as a process to retrieve entries, which has been explored in many QA and dialogue work. One line of this research includes creating well-defined API calls to query the KB (Williams et al., 2017; Wen et al., 2017a). And another line of research tried to directly retrieve entities from knowledge base. Yin et al. (2016b) has built a system to encode all table cells and assign a score vector to each row. Our framework resembles the second line of research, but can generate multiple entities to form natural language responses. He et al. (2017) has built two symmetric dialogue agents with private knowledge, and has applied knowledge graph reasoning into Seq2Seq learning, which is distantly related with our framework. In the sense of the KB forms, Yin et al. (2016a) retrieved entities based on (subject, relation, object) triples. While Dhingra et al. (2017) applied a soft-KB lookup on an entity-centric knowledg"
D11-1109,D07-1101,0,0.442565,"d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 according to their naming. Model 1 incorporates only grand-sibling parts, while Model 2 incorporates both grand-sibling and tri-sibling parts. Their experiments on English and Czech show that Model 1 and Model 2 obtain nearly the same parsing ac"
D11-1109,P05-1022,0,0.355797,"Missing"
D11-1109,C10-1019,1,0.878284,"Missing"
D11-1109,D07-1022,0,0.0164053,"Missing"
D11-1109,W02-1001,0,0.285248,"et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently"
D11-1109,N09-1046,0,0.0525872,"Missing"
D11-1109,C96-1058,0,0.797918,"me with Model 1 in Koo and Collins (2010), but without using grand-sibling features.2 • The third-order model (O3): the same with Model 1 in Koo and Collins (2010). We adopt linear models to define the score of a dependency tree. For the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for"
D11-1109,N09-1037,0,0.0607158,"Missing"
D11-1109,P08-1043,0,0.0977694,"Missing"
D11-1109,P10-1110,0,0.400604,"tructures. On the contrary, joint models of version 2 can incorporate both aforementioned feature sets, but have higher complexity. These two versions of models will be thoroughly compared in the experiments. 1185 We then define the allowable candidate POS tags of the word wi to be Ti (x) = {t : t ∈ T , P (ti = t|x) ≥ λt × pmaxi (x)} where λt is the pruning threshold. Ti (x) is used to constrain the POS search space by replacing T in Algorithm 1. 5 Experiments We use the Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005). Following the setup of Duan et al. (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 11371147) sets. We use the head-finding rules of Zhang and Clark (2008b) to turn the bracketed sentences into dependency structures. We use the standard tagging accuracy to evaluate POS tagging. For dependency parsing, we use word accuracy (also known as dependency accuracy), root accuracy and complete match rate (all excluding punctuation) . For the averaged training, we train each model for 15 iterations and select the parameters that perform best on the development"
D11-1109,P08-1102,0,0.111307,"Missing"
D11-1109,P10-1001,0,0.270992,"2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently, graph-based dependency parsing has gained more and more interest due to its state-ofthe-art accuracy. Graph-based dependency parsing views the problem as finding the highest scoring tree from a directed graph. Based on dynamic programming decoding, it can efficiently find an optimal tree in a huge search space. In a graph-based model, the score"
D11-1109,P09-1058,0,0.227854,"Missing"
D11-1109,P03-1056,0,0.0427835,"where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is an artificial root token which is used to simplify the formalization of the problem. The pipelined method treats POS tagging and dependency parsing as two cascaded problems. First, 1 It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 20"
D11-1109,P10-1113,0,0.0383995,"Missing"
D11-1109,C10-1080,0,0.0394632,"Missing"
D11-1109,E06-1011,0,0.689881,"the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 according to their naming. Model 1 incorporates only grand-sibling parts, while Model 2 incorporates both grand-sibling and tri-sibling parts"
D11-1109,P05-1012,0,0.930686,"der model (O3): the same with Model 1 in Koo and Collins (2010). We adopt linear models to define the score of a dependency tree. For the third-order model, the score of a dependency tree is represented as: ∑ wdep · fdep (x, t, h, m) Scoresyn (x, t, d) = {(h,m)}⊆d m h dependency h s m sibling g + m h grandparent {(h,s)(h,m)}⊆d + g s h grand-sibling m h s t tri-sibling ∑ ∑ {(g,h),(h,m)}⊆d ∑ m + Figure 1: Different types of scoring parts used in current graph-based models (Koo and Collins, 2010). Eisner (1996) proposes an O(n3 ) decoding algorithm for dependency parsing. Based on the algorithm, McDonald et al. (2005) propose the firstorder model, in which the scoring parts only contains dependencies. The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3 ) parsing time. The secondorder model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4 ) parsing time. However, the grandparent parts are restricted to those composed of outermost grandchildren. Koo and Collins (2010) propose efficient decoding algorithms of O(n4 ) for third-order models. In their paper, they implement two versions of third-order models, Model 1 and Model 2 ac"
D11-1109,J08-4003,0,0.155196,"Missing"
D11-1109,N07-1051,0,0.116748,"Missing"
D11-1109,W96-0213,0,0.524192,"d as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score o"
D11-1109,D10-1001,0,0.0739428,"POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is an artificial root token which is used to simplify the formalization of the problem. The pipelined method treats POS tagging and dependency parsing as two cascaded problems. First, 1 It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). In addition, Rush et al. (2010) describes an efficient and simple inference algorithm based on dual decomposition and linear programming relaxation to combine a lexicalized constituent parser and a trigram POS tagger. 1181 ˆt = arg max Scorepos (x, t) t ˆ is determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002"
D11-1109,P07-1096,0,0.0207597,"s determined Then, an optimal dependency tree d based on x and ˆt. ˆ = arg max Scoresyn (x, ˆt, d) d d 2.1 POS Tagging POS tagging is a typical sequence labeling problem. Many models have been successfully applied to sequence labeling problems, such as maximumentropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). We use perceptron to build our POS tagging baseline for two reasons. Firstly, as a linear model, perceptron is simple, fast, and effective. It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al., 2007). Secondly, perceptron has been successfully applied to dependency parsing as well (Koo and Collins, 2010). In this paper, perceptron is used in all models including the POS tagging model, the dependency parsing models and the joint models. In a perceptron, the score of a tag sequence is Scorepos (x, t) = wpos · fpos (x, t) 2.2 Dependency Parsing Recently, graph-based dependency parsing has gained more and more interest due to its state-ofthe-art accuracy. Graph-based dependency parsing views the problem as finding the highest scoring tree from a directed graph. Based on dynamic programming de"
D11-1109,W08-2121,0,0.0983419,"Missing"
D11-1109,P09-1055,0,0.0150803,"Missing"
D11-1109,C10-1135,0,0.0414692,"Missing"
D11-1109,P08-1101,0,0.518379,"of this paper is organized as follows. Section 2 describes the pipelined method, including the POS tagging and parsing models. Section 3 discusses the joint models and the decoding algorithms, while Section 4 presents the pruning techniques. Section 5 reports the experimental results and error analysis. We review previous work closely related to our method in Section 6, and conclude this paper in Section 7. an optimal POS tag sequence ˆt is determined. 2 where fpos (x, t) refers to the feature vector and wpos is the corresponding weight vector. For POS tagging features, we follow the work of Zhang and Clark (2008a). Three feature sets are considered: POS unigram, bigram and trigram features. For brevity, we will refer to the three sets as wi ti , ti−1 ti and ti−2 ti−1 ti . Given wpos , we adopt the Viterbi algorithm to get the optimal tagging sequence. The Baseline Pipelined Method Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is"
D11-1109,D08-1059,0,0.828377,"of this paper is organized as follows. Section 2 describes the pipelined method, including the POS tagging and parsing models. Section 3 discusses the joint models and the decoding algorithms, while Section 4 presents the pruning techniques. Section 5 reports the experimental results and error analysis. We review previous work closely related to our method in Section 6, and conclude this paper in Section 7. an optimal POS tag sequence ˆt is determined. 2 where fpos (x, t) refers to the feature vector and wpos is the corresponding weight vector. For POS tagging features, we follow the work of Zhang and Clark (2008a). Three feature sets are considered: POS unigram, bigram and trigram features. For brevity, we will refer to the three sets as wi ti , ti−1 ti and ti−2 ti−1 ti . Given wpos , we adopt the Viterbi algorithm to get the optimal tagging sequence. The Baseline Pipelined Method Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. A dependency tree is denoted by d = {(h, m) : 0 ≤ h ≤ n, 0 &lt; m ≤ n}, where (h, m) represents a dependency wh → wm whose head word (or father) is wh and modifier (or child) is wm . w0 is"
D11-1109,W09-1201,0,\N,Missing
D12-1015,N07-1039,0,0.0153534,"satisfy the “attribute-head” relation. The only difference is that the polarity of this kind of queries is opposite to that of the collocation. Similar to the queries from Strategy0, the queries generated by Strategy1∼3 are all searched with quotes. In addition, note that the modifier and the negation word are taken from Modifier Lexicon and Negation Lexicon introduced in Table 2. 3.2.2 Query Expansion Strategy 3.2.3 Pseudo Context Acquisition We first investigate the modifying relations between polarity words and the targets, and then construct effective queries. Observed from previous work (Bloom et al., 2007; Kobayashi et al., 2004; Popescu and Etzioni, 2005), there are two kinds of common relations between the polarity words and their targets. One is the “subject-copula-predicate” relation, such as the relationship between “long” and “battery life” in the sentence “The battery life of this camera is long”. The other is the “attribute-head” relation, such as the relationship between them in the sentence “This camera has long battery life”. As a result, three heuristic query expansion strategies are adopted to construct efficient queries for searching. Take the collocation ⟨长,电 池 寿 命⟩ (⟨long, batt"
D12-1015,P11-1014,0,0.0218584,"rity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach start A Chinese collocation in a review Original context acquisition Sentiment analysis Query expansion Combination end Pos/Neg Searching Web sn"
D12-1015,P97-1023,0,0.885827,"ion Using Web-based Pseudo Contexts Yanyan Zhao, Bing Qin and Ting Liu∗ Harbin Institute of Technology, Harbin, China {yyzhao, bqin, tliu}@ir.hit.edu.cn Abstract Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, startup⟩), in which the sentiment orientation of the polarity word (“long”) changes along w"
D12-1015,P11-1013,0,0.115538,"Missing"
D12-1015,P10-1060,0,0.133343,"Missing"
D12-1015,D07-1115,0,0.0232739,"s. Section 4 and 5 presents the experiments and results. Finally we conclude this paper in Section 6. 2 Related Work The key of the collocation polarity disambiguation task is to recognize the polarity word’s sentiment orientation of a collocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the same domain, a word may indicate different polarities depending on what ta"
D12-1015,kamps-etal-2004-using,0,0.276264,"Missing"
D12-1015,W06-1642,0,0.0885996,"on what targets it is applied to, especially for the polarity-ambiguous words, such as “长” (“long” in English) shown in Section 1. Based on these, we need to consider both the polarity words and their modified targets, i.e., the collocations mentioned in this paper, rather than only the polarity words. To date, the task in this paper is similar with much previous work. Some researchers exploited the features of the sentences containing collocations to help disambiguate the polarity of the polarity-ambiguous word. For example, Hatzivassiloglou (Hatzivassiloglou and McKeown, 1997) and Kanayama (Kanayama and Nasukawa, 2006) used conjunction rules to solve this problem from large domain corpora. Suzuki (Suzuki et al., 2006) took into account many contextual information of the word within the sentence, such as exclamation words, emoticons and so on. However, the experimental results show that these in-sentence features are not rich enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to di"
D12-1015,I05-2011,0,0.0173915,"he [battery life]t of this camera is [long]p . (Positive) Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; ∗ Correspondence author: tliu@ir.hit.edu.cn 2. 该相机的[启动时间]t 很[长]p 。(Negative) Translated as: This camera has [long]p [startup]t . (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by the polarity words. In the above two sentences, the sentiment orientation of the polarity word “长” (“long” in English) changes along with different targets. When modifying the target “电池寿命” (“battery life” in English), its polarity is positive; and when modifying “启动时间” (“startup” in English), its polarity is 160 Proce"
D12-1015,N06-1026,0,0.0190447,"llocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the same domain, a word may indicate different polarities depending on what targets it is applied to, especially for the polarity-ambiguous words, such as “长” (“long” in English) shown in Section 1. Based on these, we need to consider both the polarity words and their modified targets, i.e., the collocation"
D12-1015,P10-1139,0,0.0372585,"Missing"
D12-1015,D09-1063,0,0.0584889,"∗ Harbin Institute of Technology, Harbin, China {yyzhao, bqin, tliu}@ir.hit.edu.cn Abstract Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, startup⟩), in which the sentiment orientation of the polarity word (“long”) changes along with different targets (“battery life” or “startup”). To d"
D12-1015,W02-1011,0,0.0134375,"e) Translated as: The [battery life]t of this camera is [long]p . (Positive) Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; ∗ Correspondence author: tliu@ir.hit.edu.cn 2. 该相机的[启动时间]t 很[长]p 。(Negative) Translated as: This camera has [long]p [startup]t . (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by the polarity words. In the above two sentences, the sentiment orientation of the polarity word “长” (“long” in English) changes along with different targets. When modifying the target “电池寿命” (“battery life” in English), its polarity is positive; and when modifying “启动时间” (“startup” in English), its p"
D12-1015,H05-1043,0,0.330857,"nly difference is that the polarity of this kind of queries is opposite to that of the collocation. Similar to the queries from Strategy0, the queries generated by Strategy1∼3 are all searched with quotes. In addition, note that the modifier and the negation word are taken from Modifier Lexicon and Negation Lexicon introduced in Table 2. 3.2.2 Query Expansion Strategy 3.2.3 Pseudo Context Acquisition We first investigate the modifying relations between polarity words and the targets, and then construct effective queries. Observed from previous work (Bloom et al., 2007; Kobayashi et al., 2004; Popescu and Etzioni, 2005), there are two kinds of common relations between the polarity words and their targets. One is the “subject-copula-predicate” relation, such as the relationship between “long” and “battery life” in the sentence “The battery life of this camera is long”. The other is the “attribute-head” relation, such as the relationship between them in the sentence “This camera has long battery life”. As a result, three heuristic query expansion strategies are adopted to construct efficient queries for searching. Take the collocation ⟨长,电 池 寿 命⟩ (⟨long, battery life⟩ in English) as an example, the strategies"
D12-1015,W03-1014,0,0.348015,"thod is effective. 1 1. 该相机的[电池寿命]t 很[长]p 。(Positive) Translated as: The [battery life]t of this camera is [long]p . (Positive) Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; ∗ Correspondence author: tliu@ir.hit.edu.cn 2. 该相机的[启动时间]t 很[长]p 。(Negative) Translated as: This camera has [long]p [startup]t . (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by the polarity words. In the above two sentences, the sentiment orientation of the polarity word “长” (“long” in English) changes along with different targets. When modifying the target “电池寿命” (“battery life” in English), its polarity is positive; and when m"
D12-1015,N09-1001,0,0.0167187,"pora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach start A Chinese collocation in a review Original context acquisition Sentiment analysis Query expansion Combination end Pos/Neg Searching Web snippets Pseudo context acquisition Sentiment analysis Figure 1: The framework of our approach. ity with this collocation. Searching these queries in the domain-related websites, lots of snippets can be acquired. Then we can extract the pseudo contexts from these snippets. 2. Sentiment Analysis: For both original contexts and the expanded pseudo contexts from web, a simple lexicon-bas"
D12-1015,N10-1119,0,0.21066,"echnology, Harbin, China {yyzhao, bqin, tliu}@ir.hit.edu.cn Abstract Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, startup⟩), in which the sentiment orientation of the polarity word (“long”) changes along with different targets (“battery life” or “startup”). To disambiguate a collocation’"
D12-1015,P06-1134,0,0.0228362,"h need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach start A Chinese collocation in a review Original context acquisition Sentiment analysis Query expansion Combination end Pos/Neg Searching Web snippets Pseudo context acquisition Sentiment analysis Figure 1: The framework of our approach. ity with this collocation. Searching these queries in the domain-related websites, lots of snippets can be acquired. Then we can extract the pseudo contexts from these snippets. 2. Sentiment Analysis: For both original contexts and the expanded pseudo contexts from we"
D12-1015,H05-1044,0,0.103651,"features are not rich enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to disambiguate its polarity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguati"
D12-1015,J09-3003,0,0.0248787,"enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to disambiguate its polarity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to clas"
D12-1015,W03-1017,0,0.0199994,"polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective. 1 1. 该相机的[电池寿命]t 很[长]p 。(Positive) Translated as: The [battery life]t of this camera is [long]p . (Positive) Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; ∗ Correspondence author: tliu@ir.hit.edu.cn 2. 该相机的[启动时间]t 很[长]p 。(Negative) Translated as: This camera has [long]p [startup]t . (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by"
D12-1015,H05-2017,0,\N,Missing
D13-1045,P11-1011,0,0.0198444,"ries.” Comparing with previous work, the advantages of our approach are on the following aspects. First, we generate structured annotation of queries based on top search results, not some global knowledge base or query logs. Second, they mainly focus on the method of generating structured annotation of queries, rather than leverage the generated query structures to improve web search rankings. In this paper, we not only offer a novel solution for generating structured annotation of queries, but also propose a re-ranking approach to improve Web search based on structured annotation of queries. Bendersky et al., (2011) also used top search results to generate structured annotation of queries. However, the annotations in their definition are capitalization, POS tags, and segmentation indicators, which are different from ours. 2.2 Query Template Generation The concept of query template has been discussed in a few recent papers (Agarwal et al., 2010; Pasca 2011; Liu et al., 2011; Szpektor et al., 2011). A query template is a sequence of terms, where each term could be a word or an attribute. For example, <#artist_name lyrics #lyrics> is a query template, “#artist_name” and “#lyrics” are attributes, and “lyrics"
D13-1045,D07-1086,0,\N,Missing
D13-1085,D07-1074,0,0.14966,"ot, and his opinion on flood tax policy. To understand that this post mentions Tony Abbot is not trivial because the name Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to o"
D13-1085,N13-1122,0,0.171725,"ask is that, for a name mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Method A typical entity linking system can be broken down into two steps: candidate generation This step narrows down the candidate entity range from any entity in the world to a limited set. candidate ranking This step ranks the candidates and output the top ranked entity as the result. Figure 1: An example of the GMEL"
D13-1085,P11-1095,0,0.0370182,"Missing"
D13-1085,P11-1115,0,0.0346173,"EL can significantly improve the performance • We annotate a microblog entity linking corpus which is comparable to an existing long text corpus. 864 • We show the inefficiency of previous method on the microblog corpus and our method can significantly improve the results. 2 Task defination The microblog entity linking task is that, for a name mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Metho"
D13-1085,P13-1128,0,0.0311755,"mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Method A typical entity linking system can be broken down into two steps: candidate generation This step narrows down the candidate entity range from any entity in the world to a limited set. candidate ranking This step ranks the candidates and output the top ranked entity as the result. Figure 1: An example of the GMEL graph. p1 . . . p4 are"
D13-1085,N13-1039,0,0.0970558,"Missing"
D13-1085,P11-1138,0,0.021886,"Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to overcome the feature sparseness, we turn to another property of microblog: 863 Proceedings of the 2013 Conference on Empirical Me"
D13-1085,P10-1149,0,0.0300152,"Missing"
D13-1085,N10-1072,0,0.0212542,"25 target names and manually link the target name mentions in the posts to the TAC-KBP knowledge base. In order to evaluate the assumption in CEMEL: similar posts tend to co-reference, we randomly select 10 posts for 5 target names respectively and search for the posts in the post collection. From the search result of each of the 50 posts, we select the top 20 posts and manually annotate if they coreference with the query post. 4.2 Settings We generate candidates with the method described in (Guo et al., 2013b) and use Vector Space Model (VSM) (Varma et al., 2009) and Learning to Rank (LTR) (Zheng et al., 2010) as the ranking model. We 866 Figure 4: Accuracy of GMEL with different rate of extra post nodes use Lucene and ListNet with default settings for the VSM and LTR implementation respectively. We use bigram feature for VSM and the feature set of (Chen et al., 2011) for LTR. LTR is evaluated with 10-fold cross validation. Given a target name, the GMEL graph includes all the evaluation posts as well as a set of extra post nodes searched from the post collection with the query of the target name. We filter out determiners, interjections, punctuations, emoticons, discourse markers and URLs in the po"
D13-1122,P99-1016,0,0.409111,"s for hypernym discovery can be summarized into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2 ”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns t"
D13-1122,C10-3004,1,0.782048,"Missing"
D13-1122,W03-1022,0,0.0264129,"into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2 ”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as"
D13-1122,C92-2082,0,0.643931,"2006), question answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity n"
D13-1122,D12-1082,0,0.0676915,"cally extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, our goal is to discover its hypernyms by leveraging knowledge from multiple sources. Considering the case where a person wants to know the meaning of an unknown entity, he/she may search it in a search engine and then finds out the answer after going through the search results. Furthermore, if he/she finds an entry about the entity in an aut"
D13-1122,I08-2112,0,0.253715,"covery is a task to extract such noun pairs that one noun is a hypernym of the other (Snow et al., 2005). A noun H is a hypernym of another noun E if E is an instance or subclass of H. In other word, H is a semantic class of E. For instance, “actor” is a hypernym of “Mel Gibson”; “dog” is a hypernym of “Caucasian sheepdog”; “medicine” is a hypernym of “Aspirin”. Hypernym discovery is an important subtask of semantic relation extraction ∗ Email correspondence. and has many applications in ontology construction (Suchanek et al., 2008), machine reading (Etzioni et al., 2006), question answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al.,"
D13-1122,W03-0415,0,0.0299393,"scovery can be summarized into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2 ”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede t"
D13-1122,P11-1116,0,0.100561,"et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, our goal is to discover its hypernyms"
D14-1012,P05-1001,0,0.0142042,"Missing"
D14-1012,J92-4003,0,0.478376,"tomatically extracted prototypes for each target label. More foundationally, the reason for the above factors lies in the high-dimensional and sparse lexical feature representation, which completely ignores the similarity between features, especially word features. To overcome this weakness, an effective way is to learn more generalized representations of words by exploiting the numerous unlabeled data, in a semi-supervised manner. After which, the generalized word representations can be used as extra features to facilitate the supervised systems. Liang (2005) learned Brown clusters of words (Brown et al., 1992) from unlabeled data and use them as features to promote the supervised NER and Chinese word segmentation. Brown clusters of words can be seen as a generalized word representation distributed in a discrete and low-dimensional vocabulary space. Contextually similar words are grouped in the same cluster. The Brown clustering of words was also adopted in dependency parsing (Koo et al., 2008) and POS tagging for online conversational text (Owoputi et al., 2013), demonstrating significant improvements. We carefully compare and analyze these approaches in the task of NER. Experimental results are pr"
D14-1012,P05-1045,0,0.0106467,"Missing"
D14-1012,C14-1048,1,0.150649,"Missing"
D14-1012,N04-1043,0,0.0453105,"rd is an NE. To verify this, we extract new prototypes considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. Table 5: Performance of the NE/non-NE classification on the CoNLL-2003 development dataset using different embedding features. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by t"
D14-1012,N06-1041,0,0.0307987,"Similarities between words and clusters are measured by Euclidean distance. Moreover, different number of clusters n contain information of different granularities. Therefore, we combine the cluster features of different ns to better utilize the embeddings. 3.4 NE Type B-PER I-PER B-ORG I-ORG B-LOC I-LOC B-MISC I-MISC O Table 1: Prototypes extracted from the CoNLL2003 NER training data using NPMI. Distributional Prototype Features We propose a novel kind of embedding features, named distributional prototype features for supervised models. This is mainly inspired by prototype-driven learning (Haghighi and Klein, 2006) which was originally introduced as a primarily unsupervised approach for sequence modeling. In prototype-driven learning, a few prototypical examples are specified for each target label, which can be treated as an injection of prior knowledge. This sparse prototype information is then propagated across an unlabeled corpus through distributional similarities. The basic motivation of the distributional prototype features is that similar words are supposed to be tagged with the same label. This hypothesis makes great sense in tasks such as NER and POS tagging. For example, suppose Michael is a p"
D14-1012,P09-1056,0,0.00803374,"es considering only two labels, namely, NE and non-NE, using the same metric in Section 3.4. As shown in the last row of Table 5, higher performance is achieved. Table 5: Performance of the NE/non-NE classification on the CoNLL-2003 development dataset using different embedding features. 6 Related Studies Semi-supervised learning with generalized word representations is a simple and general way of improving supervised NLP systems. One common approach for inducing generalized word representations is to use clustering (e.g., Brown clustering) (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Huang and Yates, 2009). Aside from word clustering, word embeddings have been widely studied. Bengio et al. (2003) propose a feed-forward neural network based language model (NNLM), which uses an embedding layer to map each word to a dense continuousvalued and low-dimensional vector (parameters), and then use these vectors as the input to predict the probability distribution of the next word. The NNLM can be seen as a joint learning framework for language modeling and word representations. Alternative models for learning word embeddings are mostly inspired by the feed-forward NNLM, including the Hierarchical Log-Bi"
D14-1012,N13-1039,0,0.0161671,"Missing"
D14-1012,W09-1119,0,0.0510686,"gorithm is a binary tree, where each word is uniquely identified by its path from the root. Thus each word can be represented as a bit-string with a specific length. Following the setting of Owoputi et al. (2013), we will use the prefix features of hierarchical clusters to take advantage of the word similarity in different granularities. Concretely, the Brown cluster feature template is: 5.2 Table 3 shows the performances of NER on the test dataset. Our baseline is slightly lower than that of Turian et al. (2010), because they use the BILOU encoding of NE types which outperforms BIO encoding (Ratinov and Roth, 2009).8 Nonetheless, our conclusions hold. As we can see, all of the three approaches we investigate in this study achieve better performance than the direct use of the dense continuous embedding features. To our surprise, even the binarized embedding features (BinarizedEmb) outperform the continuous version (DenseEmb). This provides clear evidence that directly using the dense continuous embeddings as features in CRF indeed cannot fully • bci+k , −2 ≤ k ≤ 2. • prefix (bci+k , p), p ∈ {2,4,6,...,16}, −2 ≤ k ≤ 2. prefix takes the p-length prefix of the Brown cluster coding bci+k . 5 Experiments 5.1"
D14-1012,P12-1092,0,0.0416081,"points higher than the performance of the dense embedding features 9 Statistical significant with p-value &lt; 0.001 by two-tailed t-test. 116 Setting Baseline +DenseEmb +BinarizedEmb +ClusterEmb +DistPrototype Time (ms) / sent 1.04 4.75 1.25 1.16 2.31 very frequent words, while lower sparsity for midfrequent words. It indicates that for words that are very rare or very frequent, BinarizedEmb just omit most of the features. This is reasonable also for the very frequent words, since they usually have rich and diverse context distributions and their embeddings cannot be well learned by our models (Huang et al., 2012). 0.70 Table 4: Running time of different features on a Intel(R) Xeon(R) E5620 2.40GHz machine. to accelerate the DistPrototype, by increasing the threshold of DistSim(z, w). However, this is indeed an issue of trade-off between efficiency and accuracy. ● ● 0.60 Sparsity 0.65 ● ● ● ● Analysis 0.55 5.3 In this section, we conduct analyses to show the reasons for the improvements. ● ● ● 0.50 256 5.3.1 ● ● Rare words 1k 4k 16k 64k Frequency of word in unlabeled data As discussed by Turian et al. (2010), much of the NER F1 is derived from decisions regarding rare words. Therefore, in order to show"
D14-1012,W03-0419,0,0.0367445,"Missing"
D14-1012,P08-1068,0,0.70957,"tributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score. 1 Introduction Learning generalized representation of words is an effective way of handling data sparsity caused by high-dimensional lexical features in NLP systems, such as named entity recognition (NER) and dependency parsing. As a typical lowdimensional and generalized word representation, Brown clustering of words has been studied for a long time. For example, Liang (2005) and Koo et al. (2008) used the Brown cluster features for semi-supervised learning of various NLP tasks and achieved significant improvements. ∗ • Are the continuous embedding features fit for the generalized linear models that are most widely adopted in NLP? • How can the generalized linear models better utilize the embedding features? According to the results provided by Turian et 1 Generalized linear models refer to the models that describe the data as a combination of linear basis functions, either directly in the input variables space or through some transformation of the probability distributions (e.g., logl"
D14-1012,P10-1040,0,0.526341,"on is on, word embedding preserves rich linguistic regularities of words with each dimension hopefully representing a latent feature. Similar words are expected to be distributed close to one another in the embedding space. Consequently, word embeddings can be beneficial for a variety of NLP applications in different ways, among which the most simple and general way is to be fed as features to enhance existing supervised NLP systems. Previous work has demonstrated effectiveness of the continuous word embedding features in several tasks such as chunking and NER using generalized linear models (Turian et al., 2010).1 However, there still remain two fundamental problems that should be addressed: Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism. However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain. In this study, we investigate and analyze three different approaches, including a new proposed distributional prototype approach, for utilizing the embedding features. The presen"
D14-1012,P06-1141,0,0.0341421,"Missing"
D14-1012,I13-1183,0,0.226518,"Missing"
D14-1012,W13-5708,1,0.852635,"Missing"
D14-1012,N13-1063,0,0.380291,"ith the one-hot vector of w). The probability of its context word c is then computed using a log-linear function: exp(vc&gt; vw ) &gt; c0 ∈V exp(vc0 vw ) P (c|w; θ) = P Binarization of Embeddings where mean(v) is the mean value of vector v, U+ is a string feature which turns on when the value (Cij ) falls into the upper part of the positive list. Similarly, B− refers to the bottom part of the negative list. The insight behind φ is that we only consider the features with strong opinions (i.e., positive or negative) on each dimension and omit the values close to zero. 3.3 (1) Clustering of Embeddings Yu et al. (2013) introduced clustering embeddings to overcome the disadvantage that word embeddings are not suitable for linear models. They suggested that the high-dimensional cluster features make samples from different classes better separated by linear models. where V is the vocabulary. The parameters θ are vwi , vci for w, c ∈ V and i = 1, ..., d. Then, the 2 The term similar should be viewed depending on the specific task. 112 In this study, we again investigate this approach. Concretely, each word is treated as a single sample. The batch k-means clustering algorithm (Sculley, 2010) is used,3 and each c"
D14-1015,P00-1054,0,0.0612324,"for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word s"
D14-1015,D10-1005,0,0.02413,"2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this"
D14-1015,P07-1066,0,0.0239875,"ethods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bil"
D14-1015,D07-1007,0,0.035916,"h as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality. 3 f (x, y; W, U) = cos(WT x, UT y) where x is contextual feature vector in source sentence, and y is the representation of target phrase, W ∈ R|X|×k , U ∈ R|Y|×k are low rank matrix. In our model, we allow y to be bag-of-words representation. Our embedding model is memoryefficient in that dimensionality of x and y can be very large in practical setting. We use |X |and |Y| means dimensionality o"
D14-1015,N13-1011,0,0.0470984,"Missing"
D14-1015,P14-1066,0,0.232256,"u1 Daxiang Dong1 Wei He1 Xiaoguang Hu1 Dianhai Yu1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai, wu hua,wanghaifeng@baidu.com tliu@ir.hit.edu.cn Abstract where labels are automatically generated from phrase-pairs. For each source phrase, the aligned target phrase is marked as the positive label whereas other phrases in our phrase table are treated as negative labels. Different from previous work in bilingual embedding learning(Zou et al., 2013; Gao et al., 2014), our framework is a supervised model that utilizes contextual information in source sentence as features and make use of phrase pairs as weak labels. Bilingual semantic embeddings are trained automatically from our supervised learning task. Our learned bilingual semantic embedding model is used to measure the similarity of phrase pairs which is treated as a feature in decoding. We integrate our learned model into a phrase-based translation system and experimental results indicate that our system significantly outperform the baseline system. On the NIST08 Chinese-English translation task, we o"
D14-1015,P12-1092,0,0.0602901,"guation highly depends on the language model which is trained only on target corpus. To solve this problem, we present to learn context-sensitive bilingual semantic embedding. Our methodology is to train a supervised model 2 Related Work Using vectors to represent word meanings is the essence of vector space models (VSM). The representations capture words’ semantic and syntactic information which can be used to measure semantic similarities by computing distance between the vectors. Although most VSMs represent one word with only one vector, they fail to capture homonymy and polysemy of word. Huang et al. (2012) introduced global document context and multiple word prototypes which distinguishes and uses both local and global context via a joint training objective. Much of the research focus on the task of inducing representations for single languages. Recently, a lot of progress has 142 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between"
D14-1015,P06-2124,0,0.0365331,"uage Processing (EMNLP), pages 142–146, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics tween bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embedding"
D14-1015,D13-1141,0,0.491805,"ng Model Haiyang Wu1 Daxiang Dong1 Wei He1 Xiaoguang Hu1 Dianhai Yu1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai, wu hua,wanghaifeng@baidu.com tliu@ir.hit.edu.cn Abstract where labels are automatically generated from phrase-pairs. For each source phrase, the aligned target phrase is marked as the positive label whereas other phrases in our phrase table are treated as negative labels. Different from previous work in bilingual embedding learning(Zou et al., 2013; Gao et al., 2014), our framework is a supervised model that utilizes contextual information in source sentence as features and make use of phrase pairs as weak labels. Bilingual semantic embeddings are trained automatically from our supervised learning task. Our learned bilingual semantic embedding model is used to measure the similarity of phrase pairs which is treated as a feature in decoding. We integrate our learned model into a phrase-based translation system and experimental results indicate that our system significantly outperform the baseline system. On the NIST08 Chinese-English tra"
D14-1015,N12-1005,0,0.0245693,"n bilingual embedding representation is considered as score function. The score function should be discriminative between target phrases and other candidate phrases. Our score function is in the form: been made at representation learning for bilingual words. Bilingual word representations have been presented by Peirsman and Pad´o (2010) and Sumita (2000). Also unsupervised algorithms such as LDA and LSA were used by Boyd-Graber and Resnik (2010), Tam et al. (2007) and Zhao and Xing (2006). Zou et al. (2013) learn bilingual embeddings utilizes word alignments and monolingual embeddings result, Le et al. (2012) and Gao et al. (2014) used continuous vector to represent the source language or target language of each phrase, and then computed translation probability using vector distance. Vuli´c and Moens (2013) learned bilingual vector spaces from non-parallel data induced by using a seed lexicon. However, none of these work considered the word sense disambiguation problem which Carpuat and Wu (2007) proved it is useful for SMT. In this paper, we learn bilingual semantic embeddings for source content and target phrase, and incorporate it into a phrasebased SMT system to improve translation quality. 3"
D14-1015,J03-1002,0,0.00507761,"mantic model into the phrase-based SMT system. Experimental results show that our method achieves significant improvements over the baseline on large scale Chinese-English translation task. Our model is memory-efficient and practical for industrial usage that training can be done on large scale data set with large number of classes. Prediction time is also negligible with regard to SMT decoding phase. In the future, we will explore more features to refine the model and try to utilize contextual information in target sentences. For word alignment, we align all of the training data with GIZA++ (Och and Ney, 2003), using the grow-diag-final heuristic to improve recall. For language model, we train a 5-gram modified Kneser-Ney language model and use Minimum Error Rate Training (Och, 2003) to tune the SMT. For both OpenMT08 task and WebData task, we use NIST06 as the tuning set, and use NIST08 as the testing set. Our baseline system is a standard phrase-based SMT system, and a language model is trained with the target side of bilingual corpus. Results on Chinese-English translation task are reported in Table 1. Word position features and partof-speech tagging features are both useful for our bilingual se"
D14-1015,P03-1021,0,0.0157548,"ask. Our model is memory-efficient and practical for industrial usage that training can be done on large scale data set with large number of classes. Prediction time is also negligible with regard to SMT decoding phase. In the future, we will explore more features to refine the model and try to utilize contextual information in target sentences. For word alignment, we align all of the training data with GIZA++ (Och and Ney, 2003), using the grow-diag-final heuristic to improve recall. For language model, we train a 5-gram modified Kneser-Ney language model and use Minimum Error Rate Training (Och, 2003) to tune the SMT. For both OpenMT08 task and WebData task, we use NIST06 as the tuning set, and use NIST08 as the testing set. Our baseline system is a standard phrase-based SMT system, and a language model is trained with the target side of bilingual corpus. Results on Chinese-English translation task are reported in Table 1. Word position features and partof-speech tagging features are both useful for our bilingual semantic embedding learning. Based on our trained bilingual embedding model, we can easily compute a translation score between any bilingual phrase pair. We list some cases in tab"
D14-1015,N10-1135,0,0.0461665,"Missing"
D14-1016,P03-1021,0,0.010994,"ested our system on WMT test sets from 2010 to 2013. The baseline systems are trained on the training corpus with initial word alignment, which was obtained via GIZA++ and “grow-diag-final” method. Based on the initial word alignment, we computed word translation probabilities and used the proposed method to obtain a refined word alignment. Then we used the refined word alignment to train our SMT systems. The translation results are evaluated by caseinsensitive BLEU-4 (Papineni et al., 2002). The feature weights of the translation system are tuned with the standard minimum-error-ratetraining (Och, 2003) to maximize the systems BLEU score on the development set. Experiment To demonstrate the effect of the proposed method, we use the state-of-the-art phrase-based system and hierarchical phrase-based system implemented in Moses (Koehn et al., 2007). The phrasebased system uses continuous phrase pair as the main translation knowledge. While the hierarchical phrase-based system uses both continuous and discontinuous phrase pairs, which has an ability to capture long distance phrase reordering. we carried out experiments on two translation tasks: the Chinese-to-English task comes from the NIST Ope"
D14-1016,J93-2003,0,0.0521236,"Continuous Word Alignment Improves Translation Quality Zhongjun He1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China {hezhongjun,wu hua,wanghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. Howeve"
D14-1016,P02-1040,0,0.0889285,"h translation quality of the phrase-based system. the shared translation task 2013. We used WMT08 as the development set and tested our system on WMT test sets from 2010 to 2013. The baseline systems are trained on the training corpus with initial word alignment, which was obtained via GIZA++ and “grow-diag-final” method. Based on the initial word alignment, we computed word translation probabilities and used the proposed method to obtain a refined word alignment. Then we used the refined word alignment to train our SMT systems. The translation results are evaluated by caseinsensitive BLEU-4 (Papineni et al., 2002). The feature weights of the translation system are tuned with the standard minimum-error-ratetraining (Och, 2003) to maximize the systems BLEU score on the development set. Experiment To demonstrate the effect of the proposed method, we use the state-of-the-art phrase-based system and hierarchical phrase-based system implemented in Moses (Koehn et al., 2007). The phrasebased system uses continuous phrase pair as the main translation knowledge. While the hierarchical phrase-based system uses both continuous and discontinuous phrase pairs, which has an ability to capture long distance phrase re"
D14-1016,P05-1033,0,0.501326,"glish translation tasks. Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems. Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system, and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system. 1 Introduction Word alignment, indicating the correspondence between the source and target words in bilingual sentences, plays an important role in statistical machine translation (SMT). Almost all of the SMT models, not only phrase-based (Koehn et al., 2003), but also syntax-based (Chiang, 2005; Liu et al., 2006; Huang et al., 2006), derive translation knowledge from large amount bilingual text annotated with word alignment. Therefore, the quality 147 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147–152, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1 {I meiguo The 1 2 ´ shi United 2 3 ê shaoshu States 3 was 4 4 A‡ jige among 5 5 Ý tou the 6 6 e xia 7 ‡é fandui handful 7 of 8 8 ¦ piao nations 9 9 10 I[ guojia de that 10 cast 11 a 12 11 ƒ˜ zhiyi nay 13 note 14 Figure 1: An example of word al"
D14-1016,P10-1017,0,0.0192555,"n Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually very noisy. The example in Figure 1 shows a Chinese and English sentence pair, with word alignment automatically trained by GIZA++ and the “grow-diag-final” method. We find many errors (dashed links) are caused by discontinuous alignme"
D14-1016,C96-2141,0,0.376364,"slation Quality Zhongjun He1 Hua Wu1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China {hezhongjun,wu hua,wanghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually"
D14-1016,2006.amta-papers.8,0,0.030158,"ental results show statistically significant improvements of BLEU score in both cases over the baseline systems. Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system, and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system. 1 Introduction Word alignment, indicating the correspondence between the source and target words in bilingual sentences, plays an important role in statistical machine translation (SMT). Almost all of the SMT models, not only phrase-based (Koehn et al., 2003), but also syntax-based (Chiang, 2005; Liu et al., 2006; Huang et al., 2006), derive translation knowledge from large amount bilingual text annotated with word alignment. Therefore, the quality 147 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147–152, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1 {I meiguo The 1 2 ´ shi United 2 3 ê shaoshu States 3 was 4 4 A‡ jige among 5 5 Ý tou the 6 6 e xia 7 ‡é fandui handful 7 of 8 8 ¦ piao nations 9 9 10 I[ guojia de that 10 cast 11 a 12 11 ƒ˜ zhiyi nay 13 note 14 Figure 1: An example of word alignment between a Chinese and English s"
D14-1016,D07-1103,0,0.0248816,"tandard definition of phrase in SMT, phrase pairs cannot be extracted from the discontinuous alignments. By transforming discontinuous alignments into continuous alignment, we can extract more phrase pairs. Table 7 shows the number of standard phrases and hierarchical phrases extracted from the initial and refined word alignments. We find that the number of both phrases and hierarchical phrases grows heavily. This is because that the word alignment constraint for phrase extraction is loosed by removing noisy links. Although the phrase table becomes larger, fortunately, there are some methods (Johnson et al., 2007; He et al., 2009) to prune phrase table without hurting translation quality. For further illustration, we compare the phrase pairs extracted from the initial alignment and refined alignment in Figure 1. From the initial alignments, we extracted only 3 standard phrase pairs and no hierarchical phrase pairs (Table 8). After discarding noisy alignments (dashed links) by using the proposed method, we extracted 21 standard phrase pairs and 36 hierarchical phrases. Table 9 and Table 10 show selected phrase pairs and hierarchical phrase pairs, respectively. Acknowlegement This paper is supported by"
D14-1016,N03-1017,0,0.0381442,"1 Haifeng Wang1 Ting Liu2 1 Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China 2 Harbin Institute of Technology, Harbin, China {hezhongjun,wu hua,wanghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually very noisy. The exam"
D14-1016,P07-2045,0,0.0254523,"any external knowledge, as the word translation probabilities can be estimated from the bilingual corpus with the original word alignment. We notice that the discontinuous alignment is helpful for hierarchical phrase-based model, as the model allows discontinuous phrases. Thus, for the hierarchical phrase-based model, our method may lost some discontinuous phrases. To solve the problem, we keep the original discontinuous alignment in the training corpus. We carry out experiment with the state-of-theart phrase-based and hierarchical phrase-based (Chiang, 2005) SMT systems implemented in Moses (Koehn et al., 2007). Experiments on large scale Chinese-to-English and German-to-English translation tasks demonstrate significant improvements in both cases over the baseline systems. 2 into several continuous groups, and select the best group with the highest score computed by word translation probabilities as the final alignment. For further understanding, we first describe some definitions. Given a word-aligned sentence pair (F1I , E1J , A), an alignment set Aset (i) is the set of target word positions that aligned to the source word Fii : Aset (i) = {j|(i, j) ∈ A} For example, in Figure 1, the alignment set"
D14-1016,P05-1057,0,0.0305017,"nghaifeng}@baidu.com tliu@ir.hit.edu.cn Abstract of the word alignment has big impact on the quality of translation output. Word alignments are usually automatically obtained from a large amount of bilingual training corpus. The most widely used toolkit for word alignment in SMT community is GIZA++ (Och and Ney, 2004), which implements the well known IBM models (Brown et al., 1993) and the HMM model (Vogel and Ney, 1996). Koehn et al. (2003) proposed some heuristic methods (e.g. the “grow-diag-final” method) to refine word alignments trained by GIZA++. Another group of word alignment methods (Liu et al., 2005; Moore et al., 2006; Riesa and Marcu, 2010) define feature functions to describe word alignment. They need manually aligned bilingual texts to train the model. However, the manually annotated data is too expensive to be available for all languages. Although these models reported high accuracy, the GIZA++ and “grow-diag-final” method are dominant in practice. However, automatic word alignments are usually very noisy. The example in Figure 1 shows a Chinese and English sentence pair, with word alignment automatically trained by GIZA++ and the “grow-diag-final” method. We find many errors (dashe"
D14-1016,P06-1077,0,0.0327986,"ion tasks. Experimental results show statistically significant improvements of BLEU score in both cases over the baseline systems. Our method produces a gain of +1.68 BLEU on NIST OpenMT04 for the phrase-based system, and a gain of +1.28 BLEU on NIST OpenMT06 for the hierarchical phrase-based system. 1 Introduction Word alignment, indicating the correspondence between the source and target words in bilingual sentences, plays an important role in statistical machine translation (SMT). Almost all of the SMT models, not only phrase-based (Koehn et al., 2003), but also syntax-based (Chiang, 2005; Liu et al., 2006; Huang et al., 2006), derive translation knowledge from large amount bilingual text annotated with word alignment. Therefore, the quality 147 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 147–152, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1 {I meiguo The 1 2 ´ shi United 2 3 ê shaoshu States 3 was 4 4 A‡ jige among 5 5 Ý tou the 6 6 e xia 7 ‡é fandui handful 7 of 8 8 ¦ piao nations 9 9 10 I[ guojia de that 10 cast 11 a 12 11 ƒ˜ zhiyi nay 13 note 14 Figure 1: An example of word alignment between a"
D14-1016,P06-1065,0,0.0610096,"Missing"
D14-1016,J04-4002,0,0.277293,"Missing"
D14-1054,D08-1083,0,0.0209071,"Sentiment Analysis Duyu Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contai"
D14-1054,P14-2009,1,0.766966,"hou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does"
D14-1054,P13-2087,0,0.0212728,"using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is"
D14-1054,P11-2008,0,0.131527,"Missing"
D14-1054,P11-1015,0,0.3622,"cial Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic ch"
D14-1054,D13-1171,0,0.046832,"Missing"
D14-1054,J11-2001,0,0.0176143,"notated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like e"
D14-1054,S13-2053,0,0.183839,"such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does not capture the phrasal information. The segmentations based on syntactic chunkers typically aim to identify noun groups, verb groups or named entities from a sentence. However, many sentiment indicators are phrases constituted of adjectives, negations, adverbs or idioms (Liu, 2012; Mohammad et al., 2013a), which are splitted by syntactic chunkers. Besides, a better approach would be to utilize the sentiment information to improve the segmentor. Accordingly, the sentiment-specific segmentor will enhance the performance of sentiment classification in turn. In this paper, we propose a joint segmentation and classification framework (JSC) for sentiment analysis, which simultaneous conducts sentence segmentation and sentence-level sentiment classification. The framework is illustrated in FigIn this paper, we propose a joint segmentation and classification framework for sentiment analysis. Existin"
D14-1054,C14-1018,1,0.854768,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,N10-1120,0,0.161612,"Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, b"
D14-1054,P14-1146,1,0.670131,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,pak-paroubek-2010-twitter,0,0.0427636,"pically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like emoticons (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Zhao et al., 2012). Majority of existing approaches follow Pang et al. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learnin"
D14-1054,P10-1141,0,0.148379,"Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-w"
D14-1054,P02-1053,0,0.0058165,"the joint model from sentences annotated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corp"
D14-1054,W02-1011,0,0.0553792,"fication performance. The joint model is trained only based on the annotated sentiment polarity of sentences, without any segmentation annotations. Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that, our joint model performs comparably with the state-of-the-art methods. 1 Introduction Sentiment classification, which classifies the sentiment polarity of a sentence (or document) as positive or negative, is a major research direction in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Majority of existing approaches follow Pang et al. (2002) and treat sen∗ This work was partly done when the first and fourth authors were visiting Microsoft Research. 477 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477–487, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics CG SC SEG SEG SC that is not bad -1 <+1,-1> NO 0.6 0.6 that is not bad that is not bad -1 <+1,-1> NO 0.4 0.4 Polarity: +1 that is not bad +1 <+1,+1> YES 2.3 2.3 that is not bad +1 <+1,+1> YES 1.6 1.6 Segmentations Polarity Update Rank Update Input Top K Figure 1: The joint segmentation and c"
D14-1054,P12-2018,0,0.141338,"l. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learning effective features to obtain better classification performance. On movie or product reviews, Wang and Manning (2012) present NBSVM, which trades-off • To our knowledge, this is the first work that automatically produces sentence segmentation for sentiment classification within a joint framework. • We show that the joint model yields comparable performance with the state-of-the-art methods on the benchmark Twitter sentiment classification datasets in SemEval 2013. 478 overview of the proposed joint segmentation and classification model (JSC) for sentiment analysis. The segmentation candidate generation model and the segmentation ranking model are described in Section 4. The details of the sentiment classific"
D14-1054,H05-1044,0,0.0828802,"Missing"
D14-1054,D11-1014,0,0.256554,"ytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a sepa"
D14-1054,D13-1016,0,0.0546336,"Missing"
D14-1054,D12-1110,0,0.0455462,"Missing"
D14-1054,D11-1016,0,0.0130106,"the document feature. On Twitter, Mohammad et al. (2013b) develop a state-of-the-art Twitter sentiment classifier in SemEval 2013, using a variety of sentiment lexicons and hand-crafted features. With the revival of deep learning (representation learning (Hinton and Salakhutdinov, 2006; Bengio et al., 2013; Jones, 2014)), more recent studies focus on learning the low-dimensional, dense and real-valued vector as text features for sentiment classification. Glorot et al. (2011) investigate Stacked Denoising Autoencoders to learn document vector for domain adaptation in sentiment classification. Yessenalina and Cardie (2011) represent each word as a matrix and compose words using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabili"
D14-1054,P14-1011,1,0.830375,"ords or phrases of variable length. Under this scenario, phrase embedding is highly suitable as it is capable to represent phrases with different length into a consistent distributed vector space (Mikolov et al., 2013). For each phrase, phrase embedding is a dense, real-valued and continuous vector. After the phrase embedding is trained, the nearest neighbors in the embedding space are favored to have similar grammatical usages and semantic meanings. The effectiveness of phrase embedding has been verified for building large-scale sentiment lexicon (Tang et al., 2014a) and machine translation (Zhang et al., 2014). We learn phrase embedding with Skip-Gram model (Mikolov et al., 2013), which is the state-of(2) k where φij is the segmentation score of Ωij ; sf eijk is the k-th segmentation feature of Ωij ; w and b are the parameters of the segmentation ranking model. During training, given a sentence si and its gold sentiment polarity polig , the optimization objec2 j∈Hi Segmentation-Specific Feature We empirically design four segmentation-specific features to reflect the information of each segmentation, as listed in Table 3. The objective of the segmentation ranking model is to assign a scalar to each"
D14-1054,D13-1170,0,0.203308,"ng Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal"
D14-1054,J13-3004,0,\N,Missing
D14-1054,P11-1016,1,\N,Missing
D14-1093,J09-4006,0,0.05641,"rces by joining the partially annotated sentences derived using each resource, training our CRF model with these partially annotated sentences and the fully annotated PD sentences. The tests are performed on medicine and computer domains. Table 7 shows the results, where further improvements are made on both domains when the two types of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) 871 0.78 0.9525 0.9522 0.8775 0.9225 0.8750 0.9200 0.76 0.9175 0.75 0.8725 0.77 0.9519 0.74 0.9150 0.8700 0.73 0.9516 0 50 100 150 0 (a) Finance 50 100 150 (b) Medicine 0.87 0.750 0.938 0.9222 0.745 0.936 0.9219 0.740 0.934 0.9225 0.86 0.9216 0.85 0.84 0.735"
D14-1093,C12-2073,1,0.579408,"om different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computation"
D14-1093,P08-1099,0,0.00794877,"e structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we directly incorporate the likelihood of partial annotation into the objective function. The work that is the most similar to ours is Tsuboi et al. (2008), who modify the CRF learning objective for partial data. They focus on Japanese lexical analysis using manually collected partial data, while we investigate the effec"
D14-1093,W02-1001,0,0.0713606,"main difference into two types: different vocabulary and different POS distributions. While the first type of difference can be effectively resolved by using lexicon for each domain, the second type of difference needs to be resolved by using annotated sentences. They found that given the same manual annotation time, a combination of the lexicon and sentence is the most effective. Jiang et al. (2013) use 160K Wikipedia sentences to improves segmentation accuracies on several domains. Both Zhang et al. (2014) and Jiang et al. (2013) work on discriminative models using the structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introd"
D14-1093,J05-4005,0,0.291234,"Missing"
D14-1093,C12-2116,0,0.0277869,"pedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional rand"
D14-1093,D12-1075,0,0.028153,"he best reported in the literature. 1 浦 东 开 发 与 法 制 建 设 b m b m b m b m b m b m b m b m b m e e e e e e e e e s s s s s s s s s Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under each character, where b, m, e, s stand for the beginning, middle, end of a multi-character word, and a single character word, respectively. The path shows the correct segmentation by choosing one label for each character. methods are competitive given the same amount of annotation effects (Garrette and Baldridge, 2012; Zhang et al., 2014). However, obtaining manually annotated data can be expensive. On the other hand, there are free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semiannotated web pages such as Wikipedia. In the last case, word-boundary information is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available."
D14-1093,D11-1090,0,0.225529,"is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token-"
D14-1093,P08-1076,0,0.0121424,"formation under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we directly incorporate the likelihood of partial annotation into the objective function. The work that is the most similar to ours is Tsuboi et al. (2008), who modify the CRF learning objective for partial data. They focus on Japanese lexical analysis using manually collected partial data, while we investigate the effect of partial annotation from freely available sources for Chinese segmentation. 7 Conclusion In this paper, we investigated the problem of domain adapt"
D14-1093,P09-1059,0,0.511663,"and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled"
D14-1093,P13-1075,0,0.533434,"Missing"
D14-1093,C08-1113,0,0.183793,"-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional random fields (Lafferty et al., 2001; Tsuboi et al., 2008) variant that can leverage the partial annotations obtained from different sources of free annotation. Training is achieved by a modification to the learning objective, incorporating partial annotation likelihood, so that a single model can be trained consistently with a mixture of full and partial annotation. Experimental results show that our method of using partially annotated data can consistently improves cross-domain segmentation performance. We obtain results which are competitive to the best reported in the literature. Our segmentor is freely released at https://github.com/ ExpResults/"
D14-1093,I11-1035,0,0.0409989,"perlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised"
D14-1093,C96-1035,0,0.0297556,"form of partial annotation with same unresolved ambiguities, as shown in Figure 2, and use them together with available full annotation (Figure 1) as the training data for the segmentor. In this section, we describe in detail how to obtain partially annotated sentences from each resource, respectively. 2.1 Lexicons In this scenario, we assume that there are unlabeled sentences along with a lexicon for the target domain. We obtain partially segmented sentences by extracting word boundaries from the unlabeled sentences with the help of the lexicon. Previous matching methods (Wu and Tseng, 1993; Wong and Chan, 1996) for Chinese word segmentation largely rely on the lexicons, and are generally considered being weak in ambiguity resolution (Gao 865 People’s Daily Wikipedia 看到 (saw) 海南 (Hainan) 旅游业 (tourist industry) 充满 (full) 希望 (hope) saw tourist industry in Hainan is full of hope 主要(mainly) 是(is) 旅游 (tourist) 业 (industry) 和(and) 软件 (software) 产业(industry) mainly is tourist industry and software industry (a) Case of incompatible annotation on “旅游业(tourist industry)” between People’s Daily and Wikipedia. Literature Computer 《说文解字 (Shuo Wen Jie Zi, a book) 段(segmented) 注(annotated) 》 the segmented and annot"
D14-1093,W03-1728,0,0.317988,"omains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-sup"
D14-1093,P07-1106,1,0.561664,"identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverag"
D14-1093,E14-1062,1,0.310899,"ture. 1 浦 东 开 发 与 法 制 建 设 b m b m b m b m b m b m b m b m b m e e e e e e e e e s s s s s s s s s Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under each character, where b, m, e, s stand for the beginning, middle, end of a multi-character word, and a single character word, respectively. The path shows the correct segmentation by choosing one label for each character. methods are competitive given the same amount of annotation effects (Garrette and Baldridge, 2012; Zhang et al., 2014). However, obtaining manually annotated data can be expensive. On the other hand, there are free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semiannotated web pages such as Wikipedia. In the last case, word-boundary information is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we in"
D14-1093,I08-1002,0,0.0330375,"kipedia data. (3) along with To make the most use of free annotation, we combine available free lexicon and natural annotation resources by joining the partially annotated sentences derived using each resource, training our CRF model with these partially annotated sentences and the fully annotated PD sentences. The tests are performed on medicine and computer domains. Table 7 shows the results, where further improvements are made on both domains when the two types of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) 871 0.78 0.9525 0.9522 0.8775 0.9225 0.8750 0.9200 0.76 0.9175 0.75 0.8725 0.77 0.9519 0.74 0.9150 0.8700 0.73 0.9516 0 50 100 15"
D14-1148,N07-4013,0,0.00860597,"Missing"
D14-1148,I13-1036,1,0.810861,"n of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extract the predicate verb P of a sentence based on the dependency parser of Zhang and Clark (2011), and then find the longest sequence of words Pv , such that Pv starts at P and satisfies the syntactic and lexical constraints proposed by Fader et al. (2011). The content of these two constraints are as follows: • Syntactic constraint: every multi-word event phra"
D14-1148,D11-1142,0,0.072062,"vent. In this paper, we propose using structured information to represent events, and develop a prediction model to analyze the relationship between events and the stock market. The problem is important because it provides insights into understanding the underlying mechanisms of the influence of events on the stock market. There are two main challenges to this method. On the one hand, how to obtain structured event information from large-scale news streams is a challenging problem. We propose to apply Open Information Extraction techniques (Open IE; Banko et al. (2007); Etzioni et al. (2011); Fader et al. (2011)), which do not require predefined event types or manually labeled corpora. Subsequently, two ontologies (i.e. VerbNet and WordNet) are used to generalize structured event features in order to reduce their sparseness. On the other hand, the problem of accurately predicting stock price movement using structured events is challenging, since events and the stock market can have complex relations, which can be influenced by hidden factors. In addition to the commonly used linear models, we build a deep neural network model, which takes structured events as input and learn the potential relationshi"
D14-1148,P08-1030,0,0.00859551,".}. However, terms alone might fail to accurately predict the stock price movement of Microsoft, Nokia, Oracle and Google, because they cannot indicate the actor and object of the event. To our knowledge, no effort has been reported in the literature to empirically investigate structured event representations for stock market prediction. 2.2 Event Extraction A main contribution of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Ext"
D14-1148,kipper-etal-2006-extending,0,0.0190528,"respectively. The structured event is extracted as (Private sector, adds, 114,000 jobs). 2.3 Event Generalization Our goal is to train a model that is able to make predictions based on various expressions of the same event. For example, “Microsoft swallows Nokia’s phone business for $7.2 billion” and “Microsoft purchases Nokia’s phone business” report the same event. To improve the accuracy of our prediction model, we should endow the event extraction algorithm with generalization capacity. To this end, we leverage knowledge from two well-known ontologies, WordNet (Miller, 1995) and VerbNet (Kipper et al., 2006). The process of event generalization consists of two steps. First, we construct a morphological analysis tool based on the WordNet stemmer to extract lemma forms of inflected words. For example, in “Instant view: Private sector adds 114,000 jobs in July.”, the words “adds” and “jobs” are transformed to “add” and “job”, respectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event gen"
D14-1148,N09-1031,0,0.144143,"is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the day after its former CEO Steve Jobs passed away. Google’s stock fell after grim earnings came out. Accurate extraction of events from financial news may play an important role in stock market prediction. However, previous work represents news documents mainly using simple features, such as bags-of-words, noun phrases, and named entities (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009). With these unstructured features, it is difficult to capture key events embedded in financial news, and even more difficult to model the impact of events on stock market prediction. For example, representing the event “Apple has sued Samsung Electronics for copying ‘the look and feel’ 1415 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415–1425, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics of its iPad tablet and iPhone smartphone.” using term-le"
D14-1148,P13-1008,0,0.0238942,"e might fail to accurately predict the stock price movement of Microsoft, Nokia, Oracle and Google, because they cannot indicate the actor and object of the event. To our knowledge, no effort has been reported in the literature to empirically investigate structured event representations for stock market prediction. 2.2 Event Extraction A main contribution of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extrac"
D14-1148,P14-1109,0,0.0332284,"n a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock ∗ This work was done while the first author was visiting Singapore University of Technology and Design and price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the day after its former CE"
D14-1148,P13-1086,0,0.18568,"nts. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock ∗ This work was done while the first author was visiting Singapore University of Technology and Design and price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the da"
D14-1148,J11-1005,1,0.270595,"as been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extract the predicate verb P of a sentence based on the dependency parser of Zhang and Clark (2011), and then find the longest sequence of words Pv , such that Pv starts at P and satisfies the syntactic and lexical constraints proposed by Fader et al. (2011). The content of these two constraints are as follows: • Syntactic constraint: every multi-word event phrase must begin with a verb, end with a preposition, and be a contiguous sequence of words in the sentence. • Lexical constraint: an event phrase should appear with at least a minimal number of distinct argument pairs in a large corpus. For each event 2. Argument Extraction. phrase Pv identified in the step above, we find the nearest n"
D14-1148,P13-2005,0,0.189604,"nies and governments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock ∗ This work was done while the first author was visiting Singapore University of Technology and Design and price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursd"
D15-1167,P14-1023,0,0.00747045,"xt generation task. Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks f"
D15-1167,W06-3808,0,0.0107953,"edNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). sition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local"
D15-1167,P13-1088,0,0.0400688,"er line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic compo5 Conclusion We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives. We conduct extensive experiments on four review datasets with two evaluation metrics. Empirical results show that our"
D15-1167,N15-1011,0,0.619314,"can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order t"
D15-1167,P14-1062,0,0.857869,"y size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolu"
D15-1167,W02-1011,0,0.120139,"gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1 1 Introduction Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn∗ Corresponding author. Codes and datasets are publicly available at http://ir.hit.edu.cn/˜dytang. 1 ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in t"
D15-1167,P13-2087,0,0.00447156,"Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Car"
D15-1167,D14-1162,0,0.131068,"scribe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or consti"
D15-1167,D15-1278,0,0.409141,"Missing"
D15-1167,C10-1103,0,0.0117788,"d is crucial to understand user generated content in social networks or product reviews (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn∗ Corresponding author. Codes and datasets are publicly available at http://ir.hit.edu.cn/˜dytang. 1 ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typ"
D15-1167,P15-1107,0,0.501227,"Missing"
D15-1167,P13-1045,0,0.0387591,"ection 2.3). 2.1 Average Tanh Pooling Filter 1 Filter 2 Filter 3 Convolution Lookup Sentence Composition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors"
D15-1167,P11-1015,0,0.552207,"ngio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recu"
D15-1167,P14-5010,0,0.019412,"Missing"
D15-1167,P10-1141,0,0.0122869,"nderstand user generated content in social networks or product reviews (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn∗ Corresponding author. Codes and datasets are publicly available at http://ir.hit.edu.cn/˜dytang. 1 ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capt"
D15-1167,P05-1015,0,0.42979,"re our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification. (1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set. (2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5 . (3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al. 5 We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). However, its performance is obviously worse than SVM classifier. 1426 Majority SVM + Unigrams SVM + Bigrams SVM + TextFeatures SVM + AverageSG SVM + SSWE JMARS Paragraph Vector Convolutional NN Conv-GRNN LSTM-GRNN Yelp 2013 Accuracy MSE 0.356 3.06 0.589 0.79 0.576 0.75 0.598 0.68 0.543 1.11 0.535 1.12 N/A – 0.577 0.86 0.597 0.76 0.637 0.56 0.651 0.50 Yelp 2014 Accuracy MSE 0.361 3.28 0.600 0.78 0.616 0.65 0.618 0.63 0.557 1.08 0.543 1.13 N/A – 0.592 0.70 0.610 0.68 0.655 0.51 0.671 0.48 Yelp 2015 Accuracy MSE 0.369 3.30 0.611 0.75 0.62"
D15-1167,D13-1170,0,0.490494,"ection 2.3). 2.1 Average Tanh Pooling Filter 1 Filter 2 Filter 3 Convolution Lookup Sentence Composition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors"
D15-1167,P15-1150,0,0.772158,"NN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of uni"
D15-1167,P14-1146,1,0.843593,"entation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results"
D15-1167,P15-1098,1,0.437565,"ateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of unigrams, bigramw1 w2 w3 w4 …… w?−1 w? Figure 2: Sentence composition with convolutional neural network. s and trigrams in a sentence. Each"
D15-1167,P02-1053,0,0.0607185,"es, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings. The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word"
D15-1167,P12-2018,0,0.86286,"rformance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capture such information. For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that. Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its “sparse” and “discrete” characteristics make it clumsy in taking into account of side information like relations between sentences. Recently, Le and Mikolov (2014) exploit neural networks to learn continuous document representation from data. Essentially, they use local ngram information and do not capture semantic relations between sentences. Furthermore, a person asked to do this task will naturally carry it o"
D15-1167,C10-2153,0,0.0144539,"ed learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). sition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of s"
D15-1167,C14-1064,0,0.0121979,"d Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic compo5 Conclusion We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives. We conduct extensive experiments on four review datasets with two evaluation metrics. Empirical results show that our approaches achieve state-of-the-art performances on all the"
D15-1167,D11-1016,0,0.00442251,"on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use sta"
D15-1167,D11-1015,0,0.00575452,"ument composition, while adding neural gates dramatically boosts the performance, (2) LSTM performs better than a multi-filtered CNN in modeling sentence representation. We briefly discuss some future plans. How to effectively compose sentence meanings to document meaning is a central problem in natural language processing. In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results. From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as “contrast”, “condition”, “cause”, etc. Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a). However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document 1429 representation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms"
D15-1167,D14-1181,0,\N,Missing
D15-1167,D14-1179,0,\N,Missing
D15-1270,J08-1001,0,0.0538297,"summarized in Table 1. Previous work mainly exploits the properties of a sentence itself or adjacent sentences for this task. In this work, we explore cohesion to express relations among sentences through the whole text. Cohesion can be defined as a set of resources linking within a text that organize the text together (Halliday and Hasan, 1976). It can be achieved through the use of reference, ellipsis, substitution, conjunction and lexical cohesion. Among them, lexical cohesion has been widely used for modeling local coherence and applied to related applications (Barzilay and Elhadad, 1999; Barzilay and Lapata, 2008; Galley et al., 2003; Hsueh et al., 2006; Filippova and Strube, 2006). Since cohesion is closely related to the structure of text (Morris and Hirst, 1991), it motivates us to explore similar techniques for discourse element identification. In addition, its ease of implementation is also attractive. Other options for representing text structure such as full-text discourse parsers (Marcu, 2000) may be not available or don’t have satisfied performance, especially for non-English languages. However, modeling local coherence alone is not adequate to distinguish discourse elements in persuasive ess"
D15-1270,P03-1071,0,0.114043,"Missing"
D15-1270,D11-1025,0,0.0315028,"Missing"
D15-1270,P01-1014,0,0.0531775,"senting text structure such as full-text discourse parsers (Marcu, 2000) may be not available or don’t have satisfied performance, especially for non-English languages. However, modeling local coherence alone is not adequate to distinguish discourse elements in persuasive essays. For example, a main idea may be followed by a supporting idea sentence. The two sentences can be coherent but their discourse elements are different. To deal with this, global cohesion should be exploited. Considering that in persuasive writing, thesis, main ideas and conclusion, which are termed thesis statements by Burstein et al. (2001), are expected to relate to each other (Higgins et al., 2004). It is likely that cohesive relations exist among them through the whole text. We make a focused contribution by investigating global and local cohesive relations. We create sentence chains based on cohesive resources and examine whether the chains represent global cohesion or local cohesion. Our hypothesis is that global cohesion can better capture thesis state2255 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2255–2261, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for C"
D15-1270,C10-3004,1,0.743322,"2014). It can also be solved in a sequence labeling framework, which models the whole sentence sequence and captures the correlations among predictions. For example, Conditional Random Fields (CRFs) have be studied for similar task on argumentative zoning of scientific documents (Guo et al., 2011). We will evaluate different types of features using two representative models respectively: the SVM model and the linear-chain CRF model. 2256 3.2 Basic Features Before feature extraction, sentence splitting, word segmentation,POS and NE tagging are done using a Chinese language processing toolkit (Che et al., 2010). Most basic features are adapted from previous work (Burstein et al., 2003a; Stab and Gurevych, 2014; Persing et al., 2010). For each sentence, the following feature sets are extracted. Position features The relative position of its paragraph (first, last or body) in the essay and its relative position (first, last or body) in the paragraph are modeled as a set of binary features. The index of the sentence is also used as a feature. Indicator features Cue words/phrases like “我认 为(in my opinion)” and “总之(in conclusion)” are used as indicators. Partial indicators are adapted from the ones used"
D15-1270,W06-1632,0,0.0532253,"Missing"
D15-1270,N04-1024,0,0.104954,"rcu, 2000) may be not available or don’t have satisfied performance, especially for non-English languages. However, modeling local coherence alone is not adequate to distinguish discourse elements in persuasive essays. For example, a main idea may be followed by a supporting idea sentence. The two sentences can be coherent but their discourse elements are different. To deal with this, global cohesion should be exploited. Considering that in persuasive writing, thesis, main ideas and conclusion, which are termed thesis statements by Burstein et al. (2001), are expected to relate to each other (Higgins et al., 2004). It is likely that cohesive relations exist among them through the whole text. We make a focused contribution by investigating global and local cohesive relations. We create sentence chains based on cohesive resources and examine whether the chains represent global cohesion or local cohesion. Our hypothesis is that global cohesion can better capture thesis state2255 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2255–2261, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Corpus C1 C2 C3 Avg. #Essays Avg.#p"
D15-1270,E06-1035,0,0.0280247,"oits the properties of a sentence itself or adjacent sentences for this task. In this work, we explore cohesion to express relations among sentences through the whole text. Cohesion can be defined as a set of resources linking within a text that organize the text together (Halliday and Hasan, 1976). It can be achieved through the use of reference, ellipsis, substitution, conjunction and lexical cohesion. Among them, lexical cohesion has been widely used for modeling local coherence and applied to related applications (Barzilay and Elhadad, 1999; Barzilay and Lapata, 2008; Galley et al., 2003; Hsueh et al., 2006; Filippova and Strube, 2006). Since cohesion is closely related to the structure of text (Morris and Hirst, 1991), it motivates us to explore similar techniques for discourse element identification. In addition, its ease of implementation is also attractive. Other options for representing text structure such as full-text discourse parsers (Marcu, 2000) may be not available or don’t have satisfied performance, especially for non-English languages. However, modeling local coherence alone is not adequate to distinguish discourse elements in persuasive essays. For example, a main idea may be foll"
D15-1270,J91-1002,0,0.645632,"on to express relations among sentences through the whole text. Cohesion can be defined as a set of resources linking within a text that organize the text together (Halliday and Hasan, 1976). It can be achieved through the use of reference, ellipsis, substitution, conjunction and lexical cohesion. Among them, lexical cohesion has been widely used for modeling local coherence and applied to related applications (Barzilay and Elhadad, 1999; Barzilay and Lapata, 2008; Galley et al., 2003; Hsueh et al., 2006; Filippova and Strube, 2006). Since cohesion is closely related to the structure of text (Morris and Hirst, 1991), it motivates us to explore similar techniques for discourse element identification. In addition, its ease of implementation is also attractive. Other options for representing text structure such as full-text discourse parsers (Marcu, 2000) may be not available or don’t have satisfied performance, especially for non-English languages. However, modeling local coherence alone is not adequate to distinguish discourse elements in persuasive essays. For example, a main idea may be followed by a supporting idea sentence. The two sentences can be coherent but their discourse elements are different."
D15-1270,D10-1023,0,0.347712,"e correlations among predictions. For example, Conditional Random Fields (CRFs) have be studied for similar task on argumentative zoning of scientific documents (Guo et al., 2011). We will evaluate different types of features using two representative models respectively: the SVM model and the linear-chain CRF model. 2256 3.2 Basic Features Before feature extraction, sentence splitting, word segmentation,POS and NE tagging are done using a Chinese language processing toolkit (Che et al., 2010). Most basic features are adapted from previous work (Burstein et al., 2003a; Stab and Gurevych, 2014; Persing et al., 2010). For each sentence, the following feature sets are extracted. Position features The relative position of its paragraph (first, last or body) in the essay and its relative position (first, last or body) in the paragraph are modeled as a set of binary features. The index of the sentence is also used as a feature. Indicator features Cue words/phrases like “我认 为(in my opinion)” and “总之(in conclusion)” are used as indicators. Partial indicators are adapted from the ones used by Persing et al. (2010). More Chinese specific indicators are then augmented manually. We use a binary feature denoting a r"
D15-1270,D14-1006,0,0.0615786,"m overwhelming supporting idea sentences is a major challenge. 3 Discourse Element Identification Identifying discourse elements in student essays can be seen as a functional segmentation of discourse (Webber et al., 2011). In this work, we focus on utilizing supervised feature-based machine learning models for this task. 3.1 Learning Models Discourse element identification can be casted as a classification problem that sentences are classified independently using a classifier, e.g. naive Bayes (Burstein et al., 2001), decision tree (Burstein et al., 2003b) and Support Vector Machines (SVMs) (Stab and Gurevych, 2014). It can also be solved in a sequence labeling framework, which models the whole sentence sequence and captures the correlations among predictions. For example, Conditional Random Fields (CRFs) have be studied for similar task on argumentative zoning of scientific documents (Guo et al., 2011). We will evaluate different types of features using two representative models respectively: the SVM model and the linear-chain CRF model. 2256 3.2 Basic Features Before feature extraction, sentence splitting, word segmentation,POS and NE tagging are done using a Chinese language processing toolkit (Che et"
D15-1270,W97-0703,0,\N,Missing
D15-1270,J97-1003,0,\N,Missing
D16-1021,D15-1263,0,0.00556126,"use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are r"
D16-1021,P10-2050,0,0.00966793,"n and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, research"
D16-1021,P14-2009,1,0.264452,"finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014). Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in liter221 ature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether i"
D16-1021,D14-1080,0,0.0185064,"g et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising au"
D16-1021,P14-1062,0,0.00487534,"part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts"
D16-1021,D14-1181,0,0.00616185,"positionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et"
D16-1021,S14-2076,0,0.468058,"ontext word and then utilizes this information to calculate continuous text representation. The text representation in the last layer is regarded as the feature for sentiment classification. As every component is differentiable, the entire model could be efficiently trained end-toend with gradient descent, where the loss function is the cross-entropy error of sentiment classification. We apply the proposed approach to laptop and restaurant datasets from SemEval 2014 (Pontiki et al., 2014). Experimental results show that our approach performs comparable to a top system using feature-based SVM (Kiritchenko et al., 2014). On both datasets, our approach outperforms both LSTM and attention-based LSTM models (Tang et al., 2015a) in terms of classification accuracy and running speed. Lastly, we show that using multiple computational layers over external memory could achieve improved performance. put representation given a new input and the current memory state, R outputs a response based on the output representation. Let us take question answering as an example to explain the work flow of memory network. Given a list of sentences and a question, the task aims to find evidences from these sentences and generate an"
D16-1021,P15-1107,0,0.0116738,"phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded a"
D16-1021,D15-1278,0,0.007366,"phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded a"
D16-1021,D15-1168,0,0.0594919,"growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot e"
D16-1021,D15-1166,0,0.0247409,"014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “where to look” by assigning a weight/importance to each lower position when computing an upper level representation. Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015). 6 Conclusion We develop deep memory networks that capture importances of context words for aspect level sentiment classification. Compared with recurrent neural models like LSTM, this approach is simpler and faster. Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures. We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text represen"
D16-1021,D15-1298,0,0.234316,"t analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014). Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in liter221 ature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2"
D16-1021,D14-1162,0,0.11812,"classification aims at determining the sentiment polarity of 1 In practice, an aspect might be a multi word expression such as “battery life”. For simplicity we still consider aspect as a single word in this definition. sentence s towards the aspect wi . For example, the sentiment polarity of sentence “great food but the service was dreadful!” towards aspect “food” is positive, while the polarity towards aspect “service” is negative. When dealing with a text corpus, we map each word into a low dimensional, continuous and real-valued vector, also known as word embedding (Mikolov et al., 2013; Pennington et al., 2014). All the word vectors are stacked in a word embedding matrix L ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. The word embedding of wi is notated as ei ∈ Rd×1 , which is a column in the embedding matrix L. 3.2 An Overview of the Approach softmax hop 3 Attention ∑ Linear word embedding hop 2 Attention ∑ Linear Linear hop 1 context words context words Attention ∑ Tanh Linear Linear sentence: ?1 , ?2 … ??−1 , ?? , ??+1 … ??−1 , ?? aspect word ??1 wi Figure 1: An illustration of our deep memory network with We present an overview of the deep memory network for aspe"
D16-1021,S14-2004,0,0.648481,"er an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation. 1 Introduction Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014). Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect. For example, in sentence “great food but the service was dreadful!”, the sentiment polarity of aspect “food” is positive while the polarity of aspect “service” is ∗ Corresponding author. Despite these advantages, conventional neural models like long short-term memory (LSTM) (Tang et al., 2015a) capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect. We believe tha"
D16-1021,D15-1044,0,0.037867,"t al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “where to look” by assigning a weight/importance to each lower position when computing an upper level representation. Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015). 6 Conclusion We develop deep memory networks that capture importances of context words for aspect level sentiment classification. Compared with recurrent neural models like LSTM, this approach is simpler and faster. Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures. We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text representation. We also demonstrate that using m"
D16-1021,D13-1170,0,0.00729358,"ts constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g"
D16-1021,P15-1150,0,0.0288958,"e, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writ"
D16-1021,D15-1167,1,0.358709,"1 Introduction Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014). Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect. For example, in sentence “great food but the service was dreadful!”, the sentiment polarity of aspect “food” is positive while the polarity of aspect “service” is ∗ Corresponding author. Despite these advantages, conventional neural models like long short-term memory (LSTM) (Tang et al., 2015a) capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect. We believe that only some subset of context words are needed to infer the sentiment towards an aspect. For example, in sentence “great food but the service was dreadful!”, “dreadful” is an important clue for the aspect “service” but “great” is not needed. Standard LSTM works in a sequential way and manipulates each context word with the same operation, so that it cannot explicitly reveal the importance of each context word. A desirable solution should be capable o"
D16-1021,S14-2036,0,0.0874602,"ntiment Classification Aspect level sentiment classification is a finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014). Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in liter221 ature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, w"
D16-1021,N16-1174,0,0.012777,"pute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “wher"
D16-1021,D11-1016,0,0.0129842,"on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014;"
D16-1021,K15-1021,0,0.00778988,"Missing"
D17-1135,D13-1135,0,0.327852,"ic. Unlike overt pronouns, ZPs lack grammatical attributes such as gender and number that have been proven to be essential in pronoun resolution (Chen and Ng, 2014a), which makes ZP resolution a more challenging task than overt pronoun resolution. Automatic Chinese ZP resolution is typically composed of two steps, i.e., anaphoric zero pronoun (AZP) identification that identifies whether a ZP is anaphoric; and AZP resolution, which determines antecedents for AZPs. For AZP identification, state-of-the-art resolvers use machine learning algorithms to build AZP classifiers in a supervised manner (Chen and Ng, 2013, 2016). For AZP resolution, literature approaches include unsupervised methods (Chen and Ng, 2014b, 2015), feature-based supervised models (Zhao and Ng, 2007; Kong and Zhou, 2010), and neural network models (Chen and Ng, 2016). Neural network models for AZP resolution are of growing interest for their capacity to learn task-specific representations without extensive feature engineering and to effectively exploit lexical information for ZPs and their candidate antecedents in a more scalable manner than feature-based models. Despite these advantages, existing supervised approaches (Zhao and Ng,"
D17-1135,P00-1022,0,0.605522,"Missing"
D17-1135,P06-1079,0,0.64852,"m the task. By encoding ZPs and candidate antecedents through the composition of texts based on the representation of words, our model benefits from the semantic information when resolving the ZPs. Zero pronoun resolution for other languages. There have been various studies on ZP resolution for other languages besides Chinese. Ferr´andez and Peral (2000) propose a set of hand-crafted rules for resolving ZPs in Spanish texts. Recently, supervised approaches have been widely exploited for ZP resolution in Korean (Han, 2006), Italian (Iida and Poesio, 2011) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Imamura et al., 2009; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015). Iida et al. (2016) propose a multi-column convolutional neural network for Japanese intra-sentential subject zero anaphora resolution, where both the surface word sequence and dependency tree of a target sentence are exploited as clues in their model. 4.2 Attention and Memory Network Attention mechanisms have been widely used in many studies and have achieved promising performances on a variety of NLP tasks (Rockt¨aschel et al., 2015; Rush et al., 2015; Liu et al., 2017). Recently, the memory n"
D17-1135,P11-1081,0,0.452192,"ies, in this work, we propose a novel memory network to perform the task. By encoding ZPs and candidate antecedents through the composition of texts based on the representation of words, our model benefits from the semantic information when resolving the ZPs. Zero pronoun resolution for other languages. There have been various studies on ZP resolution for other languages besides Chinese. Ferr´andez and Peral (2000) propose a set of hand-crafted rules for resolving ZPs in Spanish texts. Recently, supervised approaches have been widely exploited for ZP resolution in Korean (Han, 2006), Italian (Iida and Poesio, 2011) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Imamura et al., 2009; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015). Iida et al. (2016) propose a multi-column convolutional neural network for Japanese intra-sentential subject zero anaphora resolution, where both the surface word sequence and dependency tree of a target sentence are exploited as clues in their model. 4.2 Attention and Memory Network Attention mechanisms have been widely used in many studies and have achieved promising performances on a variety of NLP tasks (Rockt¨aschel et al., 2015; Rus"
D17-1135,D15-1260,0,0.539198,"Missing"
D17-1135,D16-1132,0,0.304861,"words, our model benefits from the semantic information when resolving the ZPs. Zero pronoun resolution for other languages. There have been various studies on ZP resolution for other languages besides Chinese. Ferr´andez and Peral (2000) propose a set of hand-crafted rules for resolving ZPs in Spanish texts. Recently, supervised approaches have been widely exploited for ZP resolution in Korean (Han, 2006), Italian (Iida and Poesio, 2011) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Imamura et al., 2009; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015). Iida et al. (2016) propose a multi-column convolutional neural network for Japanese intra-sentential subject zero anaphora resolution, where both the surface word sequence and dependency tree of a target sentence are exploited as clues in their model. 4.2 Attention and Memory Network Attention mechanisms have been widely used in many studies and have achieved promising performances on a variety of NLP tasks (Rockt¨aschel et al., 2015; Rush et al., 2015; Liu et al., 2017). Recently, the memory network has been proposed and applied to question answering task (Weston et al., 2014), which is defined to have four co"
D17-1135,P09-2022,0,0.222201,"Missing"
D17-1135,W03-1024,0,0.787696,"memory network to perform the task. By encoding ZPs and candidate antecedents through the composition of texts based on the representation of words, our model benefits from the semantic information when resolving the ZPs. Zero pronoun resolution for other languages. There have been various studies on ZP resolution for other languages besides Chinese. Ferr´andez and Peral (2000) propose a set of hand-crafted rules for resolving ZPs in Spanish texts. Recently, supervised approaches have been widely exploited for ZP resolution in Korean (Han, 2006), Italian (Iida and Poesio, 2011) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Imamura et al., 2009; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015). Iida et al. (2016) propose a multi-column convolutional neural network for Japanese intra-sentential subject zero anaphora resolution, where both the surface word sequence and dependency tree of a target sentence are exploited as clues in their model. 4.2 Attention and Memory Network Attention mechanisms have been widely used in many studies and have achieved promising performances on a variety of NLP tasks (Rockt¨aschel et al., 2015; Rush et al., 2015; Liu et al., 2017). Rec"
D17-1135,D10-1086,0,0.837205,"s ZP resolution a more challenging task than overt pronoun resolution. Automatic Chinese ZP resolution is typically composed of two steps, i.e., anaphoric zero pronoun (AZP) identification that identifies whether a ZP is anaphoric; and AZP resolution, which determines antecedents for AZPs. For AZP identification, state-of-the-art resolvers use machine learning algorithms to build AZP classifiers in a supervised manner (Chen and Ng, 2013, 2016). For AZP resolution, literature approaches include unsupervised methods (Chen and Ng, 2014b, 2015), feature-based supervised models (Zhao and Ng, 2007; Kong and Zhou, 2010), and neural network models (Chen and Ng, 2016). Neural network models for AZP resolution are of growing interest for their capacity to learn task-specific representations without extensive feature engineering and to effectively exploit lexical information for ZPs and their candidate antecedents in a more scalable manner than feature-based models. Despite these advantages, existing supervised approaches (Zhao and Ng, 2007; Chen and Ng, 2013, 2016) for AZP resolution typically utilize only syntactical and lexical information through features. They overlook semantic information that is regarded"
D17-1135,J81-4005,0,0.754001,"Missing"
D17-1135,P07-1068,0,0.164088,"k that is capable of encoding zero pronouns into vector representations with information obtained from their contexts and potential antecedents. Consequently, our resolver takes advantage of semantic information by using these continuous distributed representations. Experiments on the OntoNotes 5.0 dataset show that the proposed memory network could substantially outperform the state-of-the-art systems in various experimental settings. 1 Introduction A zero pronoun (ZP) is a gap in a sentence, which refers to an entity that supplies the necessary information for interpreting the gap (Zhao and Ng, 2007). A ZP can be either anaphoric if it corefers to one or more preceding noun phrases (antecedents) in the associated text, or non-anaphoric if there are no such noun phrases. Below is an example of ZPs and their antecedents, where “φ” denotes the ZP. [警方] 表示 他们 自杀 的 可能性 很高， 不 过 φ1 也 不 排除 φ2 有 他杀 的 可能。 ∗ Email corresponding. ([The police] said that they are more likely to commit suicide, but φ1 could not rule out φ2 the possibility of homicide.) In this example, the ZP “φ1 ” is an anaphoric ZP that refers to the antecedent “警方/The police” while the ZP “φ2 ” is non-anaphoric. Unlike overt pronoun"
D17-1135,D14-1162,0,0.0891928,"nts are extracted, our task is to determine the correct antecedent of zp from its candidate antecedent set A(zp) = {c1 , c2 , ..., ck }. Specifically, these candidate antecedents are represented in form of vectors {vc1 , vc2 , ..., vck }, which are stacked and regarded as the external memory mem ∈ Rl×k , where l is the dimension of vc . Meanwhile, we represent each word as a continuous and real-valued vector, which is known as word embedding (Bengio et al., 2003). These word vectors can be randomly initialized, or be pre-trained from text corpus with learning algorithms (Mikolov et al., 2013; Pennington et al., 2014). In this work, we adopt the latter strategy since it can better exploit the semantics of words. All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of the word vector and |V |is the size of the word vocabulary. The embedding of word w is 1310 vc1 vc2 vc3 vc4 vc5 Weighted Sum vc1 vc2 vc3 vc4 vc5 + ∑ ∑ ∑ Antecedent Attention vzp Linear vzp hop 1 hop 2 hop 3 Figure 1: Illustration of the zero pronoun-specific memory network with three computational layers (hops). vzp and vc denote the vector representation of an AZP and its candidate antecedents. Th"
D17-1135,W12-4501,0,0.143632,"Missing"
D17-1135,D15-1044,0,0.0155227,"11) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Imamura et al., 2009; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015). Iida et al. (2016) propose a multi-column convolutional neural network for Japanese intra-sentential subject zero anaphora resolution, where both the surface word sequence and dependency tree of a target sentence are exploited as clues in their model. 4.2 Attention and Memory Network Attention mechanisms have been widely used in many studies and have achieved promising performances on a variety of NLP tasks (Rockt¨aschel et al., 2015; Rush et al., 2015; Liu et al., 2017). Recently, the memory network has been proposed and applied to question answering task (Weston et al., 2014), which is defined to have four compo1316 nents: input (I), generalization (G), output (O) and response (R). After then, memory networks have been adopted in many other NLP tasks, such as aspect sentiment classification (Tang et al., 2016), dialog systems (Dodge et al., 2015), and information extraction (Xiaocheng et al., 2017). Chen Chen and Vincent Ng. 2015. Chinese zero pronoun resolution: A joint unsupervised discourseaware model rivaling state-of-the-art resolver"
D17-1135,I11-1085,0,0.762045,"edents through the composition of texts based on the representation of words, our model benefits from the semantic information when resolving the ZPs. Zero pronoun resolution for other languages. There have been various studies on ZP resolution for other languages besides Chinese. Ferr´andez and Peral (2000) propose a set of hand-crafted rules for resolving ZPs in Spanish texts. Recently, supervised approaches have been widely exploited for ZP resolution in Korean (Han, 2006), Italian (Iida and Poesio, 2011) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Imamura et al., 2009; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015). Iida et al. (2016) propose a multi-column convolutional neural network for Japanese intra-sentential subject zero anaphora resolution, where both the surface word sequence and dependency tree of a target sentence are exploited as clues in their model. 4.2 Attention and Memory Network Attention mechanisms have been widely used in many studies and have achieved promising performances on a variety of NLP tasks (Rockt¨aschel et al., 2015; Rush et al., 2015; Liu et al., 2017). Recently, the memory network has been proposed and applied to question answeri"
D17-1135,D16-1021,1,0.92682,"y”). Meanwhile, considering that the antecedents of a ZP provide the necessary information for interpreting the gap (ZP), it is a natural way to express a ZP by its potential antecedents. However, only some subsets of candidate antecedents are needed to represent a ZP1 . To achieve this goal, a desirable solution should be capable of explicitly capturing the importance of each candidate antecedent and using them to build up the representation for the ZP. In this paper, inspired by the recent success of computational models with attention mechanism and explicit memory (Sukhbaatar et al., 2015; Tang et al., 2016; Kumar et al., 2015), we focus on AZP resolution, proposing the zero pronounspecific memory network (ZPMN) that is competent for representing a ZP with information obtained from its contexts and candidate antecedents. These representations provide our system with an ability to take advantage of semantic information when resolving ZPs. Our ZPMN consists of multiple computational layers with shared parameters. With the underlying intuition that not all candidate antecedents are equally relevant for representing the ZP, we develop each computational layer as an attention-based model, which first"
D17-1135,C16-1027,1,0.835485,"n both outside and inside the phrase, which provides our model a strong ability to access to sentence-level information when modeling the candidate antecedents. In this manner, we generate the vector representations of the candidate antecedents, and regard them as the external memory, i.e., mem = {vc1 , vc2 , ..., vck }. 2.4 Attention Mechanism In this part, we introduce our attention mechanism. This strategy has been widely used in many nature language processing tasks, such as factoid question answering (Hermann et al., 2015), entailment (Rockt¨aschel et al., 2015) and disfluency detection (Wang et al., 2016). The basic idea of attention mechanism is that it assigns a weight/importance to each lower position when computing an upper-level representation (Bahdanau et al., 2015). With the underlying intuition that not all candidate antecedents are equally relevant for representing the AZP, we employ the attention mechanism as to dynamically align the more informative candidate antecedents from the external memory, mem = {vc1 , vc2 , ..., vck } with regard to the given AZP, and use them to build up the representation of the AZP. As shown in Chen and Ng (2016), traditional hand-crafted features are cru"
D17-1135,D07-1057,0,0.241126,"ry network that is capable of encoding zero pronouns into vector representations with information obtained from their contexts and potential antecedents. Consequently, our resolver takes advantage of semantic information by using these continuous distributed representations. Experiments on the OntoNotes 5.0 dataset show that the proposed memory network could substantially outperform the state-of-the-art systems in various experimental settings. 1 Introduction A zero pronoun (ZP) is a gap in a sentence, which refers to an entity that supplies the necessary information for interpreting the gap (Zhao and Ng, 2007). A ZP can be either anaphoric if it corefers to one or more preceding noun phrases (antecedents) in the associated text, or non-anaphoric if there are no such noun phrases. Below is an example of ZPs and their antecedents, where “φ” denotes the ZP. [警方] 表示 他们 自杀 的 可能性 很高， 不 过 φ1 也 不 排除 φ2 有 他杀 的 可能。 ∗ Email corresponding. ([The police] said that they are more likely to commit suicide, but φ1 could not rule out φ2 the possibility of homicide.) In this example, the ZP “φ1 ” is an anaphoric ZP that refers to the antecedent “警方/The police” while the ZP “φ2 ” is non-anaphoric. Unlike overt pronoun"
D17-1135,P15-2053,0,\N,Missing
D17-1135,P16-1074,0,\N,Missing
D17-1296,P16-1231,0,0.109656,"ons taken always equals to the number of input sentence for every valid path, it is straightforward to use beam search. We use beamsearch for both training and testing. The early update strategy from Collins and Roark (2004) is applied for training. In particular, each training sequence is decoded, and we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step t, decoding process is stopped and parameter update is performed using the gold path as a positive example, and beam items as negative examples. We also use the global optimization method (Andor et al., 2016; Zhou et al., 2015) to train our beam-search model. Scheduled Sampling Scheduled sampling (Bengio et al., 2015) can also be used to reduce error propagation. The training goal of the greedy baseline is to maximize the likelihood of each action given the current model state, which means that the correct action is taken at each step. Doing inference, the action predicted by the model itself is taken instead. This discrepancy between training and inference can yield errors that accumulate quickly along the searching process. Scheduled sampling is used to solve the discrepancy by gently changing"
D17-1296,D16-1254,0,0.0289924,"Missing"
D17-1296,N01-1016,0,0.785208,"nnotation layers are provided: one for syntactic bracketing (MRG files), and the other for disfluencies (DPS files). The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluency detection training data have gold-standard syntactic parses. To directly compare with transitionbased parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we also use the subcorpus of PARSED/MRG/SWBD. Following the experiment settings in Charniak and Johnson (2001), the training subcorpus contains directories 2 and 3 in PARSED/MRG/SWBD and directory 4 is split into test, development sets and others. Following Honnibal and Johnson (2014), we lower-case the text and remove all punctuations and partial words2 . We also discard the ‘um’ and ‘uh’ tokens and merge ‘you know’ and ‘i mean’ into single tokens. Automatic POS-tags generated from pocket crf (Qian and Liu, 2013) are used as POStag in our experiments. For Chinese experiments, we collect 25k spoken sentences from meeting minutes, which are transcribed using the iflyrec toolkit3 , and annotate them wit"
D17-1296,P04-1015,0,0.377117,"As shown in Figure 2, the model state consists of four components: (i) O, a conventional sequential LSTM (Hochreiter and Schmidhuber, 1997) to store the words that have been labeled as fluency. (ii) S, a stack LSTM to represent partial disfluency chunks, which captures chunklevel information. (iii) A, a conventional sequential LSTM to represent history of actions. (iiii) B, a Bi-LSTM to represent words that have not yet been processed. A sequence of transition actions are used to consume input tokens and construct the output from left to right. To reduce error propagation, we use beam-search (Collins and Roark, 2004) and scheduled sampling (Bengio et al., 2015), respectively. We evaluate our model on the commonly used English Switchboard test set and a in-house annotated Chinese data set. Results show that our model outperforms previous state-of-the-art systems. The code is released1 . 2 Background For a background, we briefly introduce transitionbased parsing and its extention for joint disfluency detection. An arc-eager transition-based parsing system consists of a stack σ containing words being processed, a buffer β containing words to be processed and a memory A storing dependency 1 https://github.com"
D17-1296,P15-1033,0,0.413262,"S to boston to denver Figure 2: model state when processing the sentence “want a flight to boston to denver”. arcs which have been generated. There are four types of transition actions (Nivre, 2008) • Shift : Remove the front of the buffer and push it to the stack. • Reduce : Pop the top of the stack. • LeftArc : Pop the top of the stack, and link the popped word to the front of the buffer. • RightArc : Link the front of the buffer to the top of the stack, remove the front of the buffer and push it to the stack. Many neural network parsers have been constructed under this framework, such as (Dyer et al., 2015), who use different LSTM structure to represent information from σ to β. For disfluency detection, the input is a sentence with disfluencies from automatic speech recognition (ASR). We denote the word sequence as w1n = (w1 , ..., wn ). The output of the task is a sequence of binary tags denoted as D1n = (d1 , ..., dn ), where each di corresponds to the word wi , indicating whether wi is a disfluent word or not. Hence the task can be modeled as searching for the best sequenc D∗ given the stream of words w1n D∗ = argmaxD P (D1n |w1n ) Wu et al. (2015) proposes a statistical transitionbased disfl"
D17-1296,N15-1029,0,0.310716,"Missing"
D17-1296,N09-2028,0,0.441731,", it is very important to capture long-range dependencies for disfluency detection. Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in Figure 1, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level representation, which explicitly makes use of resulted infelicity disfluency chunks. Common approaches take disfluency detection as a sequence labeling problem, where each sentential word is assigned with a label (Zayats et al., 2016; Hough and Schlangen, 2015; Qian and Liu, 2013; Georgila, 2009). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chun"
D17-1296,Q14-1011,0,0.60128,"spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. A different line of work (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) adopts transition-based parsing models for disfluency detection. This line of work can be seen as a joint of disfluency detection and parsing. The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information. However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space. Inspired by the above observations, we investigate a transition-based model without syntactic information. Our model increment"
D17-1296,N16-1030,0,0.0415195,"uencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state"
D17-1296,N06-2019,0,0.507578,"lish Switchboard test data. attention-based model can capture a global representation of the input sentence by using a RNN when encoding. It can strongly capture long-range dependencies and achieves good performance, but are also not powerful enough to capture chunklevel information. To capture chunk-level information, Ferguson et al. (2015) try to use semi-CRF for disfluency detection, and reports improved results. Semi-CRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. Many syntax-based approaches (Lease and Johnson, 2006; Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre,"
D17-1296,P14-1038,0,0.0262712,"n, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced"
D17-1296,N15-1142,0,0.0289563,"d Liu, 2013) are used as POStag in our experiments. For Chinese experiments, we collect 25k spoken sentences from meeting minutes, which are transcribed using the iflyrec toolkit3 , and annotate them with only disfluency annotations according to the guideline proposed by Meteer et al. (1995). 2 words are recognized as partial words if they are tagged as ‘XX’ or end with ‘-’ 3 the iflyrec toolkit is available at http://www.iflyrec.com/ Neural Network Training Pretrained Word Embeddings. Following Dyer et al. (2015) and Wang et al. (2016), we use a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram”, to create word embeddings. The AFP portion of English Gigaword corpus (version 5) is used as the training corpus. Word embeddings for Chinese are trained on Chinese baike corpus. We use an embedding dimension of 100 for English, 300 for chinese. Hyper-Parameters. Both the Bi-LSTMs and the stack LSTMs have two hidden layers and their dimensions are set to 100. Pretrained word embeddings have 100 dimensions and the learned word embeddings have also 100 dimensions. Pos-tag embeddings have 12 dimensions. The dimension of action embeddings is set to 20. 4.3 Perform"
D17-1296,J93-2004,0,0.0579037,"(wi wi+1 , wi+k wi+k+1 ), −4 ≤ k ≤ +4 and k 6= 0: if wi wi+1 equals wi+k wi+k+1 , the value is 1, others 0 Duplicate(pi pi+1 , pi+k pi+k+1 ), −4 ≤ k ≤ +4 and k 6= 0: if pi pi+1 equals pi+k pi+k+1 , the value is 1, others 0 similarity features f uzzyM atch(wi , wi+k ), k ∈ {−1, +1}: similarity = 2 ∗ num same letters/(len(wi ) + len(wi+k )). if similarity > 0.8, the value is 1, others 0 Table 3: Discrete features used in our transition-based neural networks. p-POS tag. w-word. 4 Experiments 4.1 4.2 Settings Dataset. Our training data include the Switchboard portion of the English Penn Treebank (Marcus et al., 1993) and a in-house Chinese data set. For English, two annotation layers are provided: one for syntactic bracketing (MRG files), and the other for disfluencies (DPS files). The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluency detection training data have gold-standard syntactic parses. To directly compare with transitionbased parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we also use the subcorpus of PA"
D17-1296,J08-4003,0,0.0365412,"y introduce transitionbased parsing and its extention for joint disfluency detection. An arc-eager transition-based parsing system consists of a stack σ containing words being processed, a buffer β containing words to be processed and a memory A storing dependency 1 https://github.com/hitwsl/transition disfluency OUT DEL et=max{0,W[st;bt;ot;at]+d} O ot at st B bt Bi-LSTM Subtraction want a flight A DEL DEL TOP S to boston to denver Figure 2: model state when processing the sentence “want a flight to boston to denver”. arcs which have been generated. There are four types of transition actions (Nivre, 2008) • Shift : Remove the front of the buffer and push it to the stack. • Reduce : Pop the top of the stack. • LeftArc : Pop the top of the stack, and link the popped word to the front of the buffer. • RightArc : Link the front of the buffer to the top of the stack, remove the front of the buffer and push it to the stack. Many neural network parsers have been constructed under this framework, such as (Dyer et al., 2015), who use different LSTM structure to represent information from σ to β. For disfluency detection, the input is a sentence with disfluencies from automatic speech recognition (ASR)."
D17-1296,N13-1102,0,0.666823,"fifteen words. Hence, it is very important to capture long-range dependencies for disfluency detection. Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in Figure 1, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level representation, which explicitly makes use of resulted infelicity disfluency chunks. Common approaches take disfluency detection as a sequence labeling problem, where each sentential word is assigned with a label (Zayats et al., 2016; Hough and Schlangen, 2015; Qian and Liu, 2013; Georgila, 2009). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only u"
D17-1296,D13-1013,0,0.490055,"ated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. A different line of work (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) adopts transition-based parsing models for disfluency detection. This line of work can be seen as a joint of disfluency detection and parsing. The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information. However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space. Inspired by the above observations, we investigate a transition-based model without syntactic info"
D17-1296,C16-1027,1,0.916833,"t set and a set of in-house annotated Chinese data. 1 RP Figure 1: Sentence with disfluencies annotated in English Switchboard corpus. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. Type repair repair repetition restart Annotation [ I just + I ] enjoy working [ we want + {well} in our area we want ] to [it’s + {uh} it’s ] almost like [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015). As shown in Figure 1, standard annotation of disfluency structure (Shriberg, 1994) indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.). Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. Table 1 give"
D17-1296,P16-1218,0,0.0207443,"Scheduled sampling is used to solve the discrepancy by gently changing the training process from a fully guided scheme using the true previous action, towards a less guided scheme which mostly uses the predicting action instead. We take the action gaining higher p(zt |et ) with a certain probability p, and a probability (1 − p) for the correct action when training. 3.2 State Representation For better capturing non-local context information, we use LSTM structures to represent different components of each state, including buffer, action, stack, and output. In particular, we exploit LSTM-Minus (Wang and Chang, 2016) to model the buffer segment, conventional LSTM to model the action and ouptut segment, and stack LSTM (Dyer et al., 2015) to model the stack segments, which demonstrates highly effectively in parsing task. Buffer Representation In order to construct more informative representation, we use a Bi-LSTM to represent the buffer following the work of Wang and Chang (2016), where the subtraction between a unidirectional 2788 O want a flight S to boston B to denver hb(to) hf(to) hb(denver) hf(denver) to boston to bb = hb(denver) - hb(to) denver bf = hf(to) - hf(denver) Figure 3: Illustration for learn"
D17-1296,P15-1113,0,0.024077,"ngineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state-of-the-art F-scores on both the commonly used English Switchboard test set and a in-house annotated Chinese data set. Acknowledgments We thank the anonymous reviewers for their valuable suggestions. This work wa"
D17-1296,P15-1048,0,0.410628,"in-house annotated Chinese data. 1 RP Figure 1: Sentence with disfluencies annotated in English Switchboard corpus. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. Type repair repair repetition restart Annotation [ I just + I ] enjoy working [ we want + {well} in our area we want ] to [it’s + {uh} it’s ] almost like [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015). As shown in Figure 1, standard annotation of disfluency structure (Shriberg, 1994) indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.). Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. Table 1 gives a few examples."
D17-1296,P13-1013,1,0.811416,"which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detect"
D17-1296,P14-1125,1,0.862598,"Missing"
D17-1296,P16-1040,1,0.852594,"luency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state-of-the-art F-scores on both the commonly used English Switchboard test set and a in-ho"
D17-1296,P11-2033,1,0.739192,"nd Johnson, 2006; Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has"
D17-1296,P15-1117,1,0.930687,"als to the number of input sentence for every valid path, it is straightforward to use beam search. We use beamsearch for both training and testing. The early update strategy from Collins and Roark (2004) is applied for training. In particular, each training sequence is decoded, and we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step t, decoding process is stopped and parameter update is performed using the gold path as a positive example, and beam items as negative examples. We also use the global optimization method (Andor et al., 2016; Zhou et al., 2015) to train our beam-search model. Scheduled Sampling Scheduled sampling (Bengio et al., 2015) can also be used to reduce error propagation. The training goal of the greedy baseline is to maximize the likelihood of each action given the current model state, which means that the correct action is taken at each step. Doing inference, the action predicted by the model itself is taken instead. This discrepancy between training and inference can yield errors that accumulate quickly along the searching process. Scheduled sampling is used to solve the discrepancy by gently changing the training process"
D17-1296,P13-1043,1,0.849581,"li and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigat"
D18-1048,D14-1179,0,0.0559999,"Missing"
D18-1048,I17-1013,0,0.146431,"cording to the complexity of source sentences and the quality of the generated translations. Extensive experiments on ChineseEnglish translation demonstrate the effectiveness of our proposed adaptive multi-pass decoder upon the conventional NMT with a significant improvement about 1.55 BLEU. 1 Reference 1st-pass 2nd-pass 3rd-pass 4th-pass Table 1: Translation examples of more decoding passes with the proposed multi-pass decoder. draft and then polish it based on global understanding of the whole draft (Niehues et al., 2016; Chatterjee et al., 2016; Zhou et al., 2017; Xia et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) . Moreover, Zhang et al. (2018) introduces a backward decoder to better exploit the right-toleft target-side contexts. Generally these methods employ two separate decoders to accomplish the polishing task. Introduction In the past several years, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) has attracted increasing attention from both academic and industry communities. Compared with conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003), which needs to explicitl"
D18-1048,D13-1176,0,0.0724593,"3rd-pass 4th-pass Table 1: Translation examples of more decoding passes with the proposed multi-pass decoder. draft and then polish it based on global understanding of the whole draft (Niehues et al., 2016; Chatterjee et al., 2016; Zhou et al., 2017; Xia et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) . Moreover, Zhang et al. (2018) introduces a backward decoder to better exploit the right-toleft target-side contexts. Generally these methods employ two separate decoders to accomplish the polishing task. Introduction In the past several years, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) has attracted increasing attention from both academic and industry communities. Compared with conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003), which needs to explicitly model latent structures, NMT adopts a unified encoder-decoder framework to directly transform a source sentence into a target sentence. Furthermore, the introduction of attention mechanism (Bahdanau et al., 2014) enhances the capability of NMT in capturing long-distance dependencies. Recently, a number of authors have"
D18-1048,P07-2045,0,0.0146892,"er or not the source sentence is easy to π(al |spolicy ; θp ) = sof tmax(Wp spolicy + bp ) l l (12) where Wp and bp are the parameters of the policy network. In this work we use REINFORCE algorithm (Williams, 1992), which is an instance of a broader class of algorithms called policy gradient methods (Sutton and Barto, 1998), to learn the parameter set θp such that the sequence of actions 526 1. Moses3 : an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data. Note that we used all data to train MOSES (Koehn et al., 2007). a = {a1 , . . . , al , . . . , aL(x,y) } maximizes the total expected reward. The expected reward for an instance is defined as: J policy (θp ) = Eπ(a|spolicy ;θp ) r(ˆ yL(x,y) ) (13) where r(ˆ yL(x,y) ) is the reward at the L(x,y) -th decoding pass. In this work, we use BLEU (Papineni ˆ L(x,y) generet al., 2002) of the final translation y ated by greedy search as input to compute our reward as follows: r(ˆ y 4 L(x,y) ) = BLEU(ˆ y L(x,y) , y) 2. RNNSearch: a variant of the attention-based NMT system (Bahdanau et al., 2014) with slight changes from dl4mt tutorial4 . 3. Deliberation Network5 :"
D18-1048,J93-2003,0,0.0704305,", 2017; Xia et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) . Moreover, Zhang et al. (2018) introduces a backward decoder to better exploit the right-toleft target-side contexts. Generally these methods employ two separate decoders to accomplish the polishing task. Introduction In the past several years, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) has attracted increasing attention from both academic and industry communities. Compared with conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003), which needs to explicitly model latent structures, NMT adopts a unified encoder-decoder framework to directly transform a source sentence into a target sentence. Furthermore, the introduction of attention mechanism (Bahdanau et al., 2014) enhances the capability of NMT in capturing long-distance dependencies. Recently, a number of authors have endeavored to adopt the polishing mechanism into NMT. Similar to human cognitive process for writing a good paper, their models first create a complete ∗ h´esh`ızh`u xi¯ansh¯eng d¯e wˇeir`enq¯ı w´ei y¯ı ni´an , yˇı p`eih´e q´ı w´ei"
D18-1048,W16-2377,0,0.0446959,"Missing"
D18-1048,N03-1017,0,0.0779442,"Missing"
D18-1048,D15-1166,0,0.328147,"odel. 2 cn = P (yn |x, y&lt;n ; θ) (1) n=1 where θ is a set of model parameters and y&lt;n denotes a partial translation. Prediction of n-th word is generally made in an encoder-decoder framework: P (yn |x, y&lt;n ; θ) = g(yn−1 , sn , cn ) (2) where g(·) is a non-linear function, yn−1 denotes the previously generated word, sn is n-th decoding hidden state, and cn is a context vector for generating n-th target word. The decoder state sn is computed by RNNs as follows: sn = f (sn−1 , yn−1 , cn ) (4) where αm,n measures how well xm and yn are aligned, calculated by attention model (Bahdanau et al., 2014; Luong et al., 2015), and hm is the encoder hidden state of the m-th source word. For the purpose of capturing both forward and backward contexts, bidirectional RNN (Schuster and Paliwal, 1997) is often employed as the encoder which converts the source sentence into an annotation sequence h = {h1 , . . . , hm , . . . , hM }, where → − ← − hm = [ h m , h m ] captures information about mth word with respect to the preceding and following words in the source sentence respectively. Although the introduction of RNNs as a decoder has resulted in substantial improvements in terms of translation quality, simultaneously i"
D18-1048,C16-1172,0,0.15093,"Missing"
D18-1048,P02-1040,0,0.105229,"NMT system with two independent left-to-right decoders (Xia et al., 2017). The first-pass decoder is identical to one of RNNSearch to generate a draft translation, while the second-pass decoder polishes it with an extra attention over the first pass decoder. The second-pass decoder is integrated with the first-pass decoder via reinforcement learning. (14) Experiments In this section, we describe experimental settings and report empirical results. 4.1 Setup We evaluated the proposed adaptive multipass decoder on Chinese-English translation task. The evaluation metric was case-insensitive BLEU (Papineni et al., 2002) calculated by the multi-bleu.perl1 script. The training corpus2 consisted of 1.25M bilingual sentences with 27.9M Chinese words and 34.5M English words. We used the NIST 2002 (MT02) as the validation set for hyper-parameter optimization and model selection, and NIST 2003 (MT03), 2004 (MT04), 2005 (MT05) and 2006 (MT06) as test sets. To effectively train the NMT model, we trained each model with sentences of length up to 50 words. Besides, we limited vocabulary size to 30K for both languages and map all the out-ofvocabulary words in the Chinese-English corpus to a special token UNK. We applied"
D18-1048,P17-2060,0,0.0744219,"e and effective number of decoding passes, according to the complexity of source sentences and the quality of the generated translations. Extensive experiments on ChineseEnglish translation demonstrate the effectiveness of our proposed adaptive multi-pass decoder upon the conventional NMT with a significant improvement about 1.55 BLEU. 1 Reference 1st-pass 2nd-pass 3rd-pass 4th-pass Table 1: Translation examples of more decoding passes with the proposed multi-pass decoder. draft and then polish it based on global understanding of the whole draft (Niehues et al., 2016; Chatterjee et al., 2016; Zhou et al., 2017; Xia et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) . Moreover, Zhang et al. (2018) introduces a backward decoder to better exploit the right-toleft target-side contexts. Generally these methods employ two separate decoders to accomplish the polishing task. Introduction In the past several years, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) has attracted increasing attention from both academic and industry communities. Compared with conventional statistical machine translation (SMT) (Brown"
D18-1183,P82-1020,0,0.787437,"Missing"
D18-1183,P16-1021,0,0.0182363,"efit from multitask learning. Simile sentence classification can be solved very well, while simile component extraction is more chellenging. With multitask learning enhanced classifier and extractor, a classification-then-extraction method achieves the best performance for simile component extraction. 2 2.1 Related Work Metaphor/Simile Analysis Metaphor analysis becomes active in recent years. The tasks include metaphor recognition, metaphor 1 The dataset is at https://github.com/cnunlp/ Chinese-Simile-Recognition-Dataset explanation and metaphor generation (Shutova et al., 2013; Veale, 1995; Jang et al., 2016). Simile is a special type of metaphor with the comparator and it is relatively easier to locate metaphorical parts. Niculae and DanescuNiculescu-Mizil (2014) aimed to distinguish a comparison from figurative or literal in product reviews using a series of linguistic cues as features. It is similar to simile sentence classification. So we take it as a baseline. In their work, they assumed that the components can be correctly recognized. In our work, we use an automated component extractor instead. Syntactic patterns are often used for extracting potential simile components and semantic analysi"
D18-1183,N16-1030,0,0.0521748,"roblem. We convert the annotated dataset to IOBES scheme (indicating Inside, Outside, Beginning, Ending, Single) (Ratinov and Roth, 2009). We use different prefixes to distinguish the tenor and the vehicle components. For example, tb and vb indicate the beginning of a tenor and a vehicle respectively. 4.3.1 Neural Sequence Labeling Model Conditional Random Field (CRF) (Lafferty et al., 2001) is a standard solution in such scenario to exploit the dependency among labels.To further make use of the dense representation of words, we build a CRF layer on the shared representation layers following (Lample et al., 2016). Formally, H = (h1 , h2 , ..., hn ) is a sequence of hidden states produced by the bidirectional LSTM for a sentence X and y = (y1 , y2 , ..., yn ) is the tag sequence, yi 2 L and |L |= k. Define (H, y) as the score of the sequence. (H, y) = n X t=0 Ayt ,yt+1 + n X Pt,yt (2) t=1 where A 2 Rk⇥k is a transition matrix and Ayt ,yt+1 records the score of a transition from current label yt to next label yt+1 ; P = (p1 , ..., pn ) 2 Rn⇥k is the emission matrix and Pt,yt represents the score of assigning tag yt to xt . Here, ht is the tth hidden state that is as assigned by the bidirectional LSTM. I"
D18-1183,D15-1104,0,0.0212826,"ecognition. In Chinese, Li et al. (2008) proposed a featurebased method for simile recognition. Their evaluation was done on a small dataset. The annotated data in this work is much larger. 2.2 Multitask Learning for NLP Many researchers have proposed to jointly learn multiple tasks with shared representations (Collobert and Weston, 2008). Improvements are reported on joint models between closely related tasks, such as text classification (Liu et al., 2016), POS tagging and parsing (Zhang and Weiss, 2016), parsing and named entity recognition (NER) (Finkel and Manning, 2010), NER and linking (Luo et al., 2015), extraction of entities and relations (Miwa and Bansal, 2016). Bingel and Søgaard (2017) offered a systematic view of relations between different tasks. 1544 3 3.1 Task and Data Task Description A metaphor is as a matter of cross-domain mappings in conceptual structure which are expressed in language. Lakoff and Johnson (2008) explains it as a mapping between the target and the source, corresponding to the terms tenor and vehicle. The tenor is the subject to which attributes are ascribed, while the vehicle is the object whose attributes are borrowed. Simile can be seen as a special type of me"
D18-1183,J04-1002,0,0.0412706,"ize rhetorical effects through an analogical procedure. Metaphor analysis has been drawn more attention for expanding current natural language processing (NLP) to high-level semantic tasks (Carbonell, 1980). Metaphors reflect creative thought of humans. On the other hand, inferring the meaning of a metaphor has to integrate background knowledge, which makes it difficult to automatically recognize metaphors in language. Previous work on metaphor recognition mainly depends on linguistic cues (Goatly, 2011) and selectional preference violation on a pair of concepts (Fass, 1991) or their domains (Mason, 2004). The domains can be created by knowledge bases such as WordNet (Mason, 2004) or based on automatic clustering (Shutova et al., 2010). In this paper, we focus on a special type of metaphor—simile. A simile is a figure of speech that directly compares two things using connecting words such as like, as, than in English and “œ” or “πÇ” in Chinese. Due to the use of such comparators, it is much easier to locate similes compared with locating other types of metaphors. As a result, it is possible to collect and annotate large scale of simile sentences and investigate data driven simile recognition."
D18-1183,P16-1105,0,0.0297621,"rebased method for simile recognition. Their evaluation was done on a small dataset. The annotated data in this work is much larger. 2.2 Multitask Learning for NLP Many researchers have proposed to jointly learn multiple tasks with shared representations (Collobert and Weston, 2008). Improvements are reported on joint models between closely related tasks, such as text classification (Liu et al., 2016), POS tagging and parsing (Zhang and Weiss, 2016), parsing and named entity recognition (NER) (Finkel and Manning, 2010), NER and linking (Luo et al., 2015), extraction of entities and relations (Miwa and Bansal, 2016). Bingel and Søgaard (2017) offered a systematic view of relations between different tasks. 1544 3 3.1 Task and Data Task Description A metaphor is as a matter of cross-domain mappings in conceptual structure which are expressed in language. Lakoff and Johnson (2008) explains it as a mapping between the target and the source, corresponding to the terms tenor and vehicle. The tenor is the subject to which attributes are ascribed, while the vehicle is the object whose attributes are borrowed. Simile can be seen as a special type of metaphor, which is signaled by explicit markers such as like or"
D18-1183,W13-3829,0,0.0198199,"locate metaphorical parts. Niculae and DanescuNiculescu-Mizil (2014) aimed to distinguish a comparison from figurative or literal in product reviews using a series of linguistic cues as features. It is similar to simile sentence classification. So we take it as a baseline. In their work, they assumed that the components can be correctly recognized. In our work, we use an automated component extractor instead. Syntactic patterns are often used for extracting potential simile components and semantic analysis is then used to distinguish similes from literal comparisons (Niculae and Yaneva, 2013; Niculae, 2013). The main limitation is that such pattern based method is difficult to deal with sentences with complex structures. As a result, the coverage is relatively small. Qadir et al. (2016) used syntactic structures, dictionary definitions, statistical cooccurrence, and word embedding vectors to infer implicit properties in similes. Qadir et al. (2015) also built a classifier with lexical features, semantic features, and sentiment features to infer the affective polarity of simile in twitters. Veale and Hao (2007) and Veale (2012a) utilized knowledge generated by similes to deal with metaphor and ir"
D18-1183,D14-1215,0,0.0435518,"Missing"
D18-1183,P13-3013,0,0.178833,"t is relatively easier to locate metaphorical parts. Niculae and DanescuNiculescu-Mizil (2014) aimed to distinguish a comparison from figurative or literal in product reviews using a series of linguistic cues as features. It is similar to simile sentence classification. So we take it as a baseline. In their work, they assumed that the components can be correctly recognized. In our work, we use an automated component extractor instead. Syntactic patterns are often used for extracting potential simile components and semantic analysis is then used to distinguish similes from literal comparisons (Niculae and Yaneva, 2013; Niculae, 2013). The main limitation is that such pattern based method is difficult to deal with sentences with complex structures. As a result, the coverage is relatively small. Qadir et al. (2016) used syntactic structures, dictionary definitions, statistical cooccurrence, and word embedding vectors to infer implicit properties in similes. Qadir et al. (2015) also built a classifier with lexical features, semantic features, and sentiment features to infer the affective polarity of simile in twitters. Veale and Hao (2007) and Veale (2012a) utilized knowledge generated by similes to deal with"
D18-1183,D15-1019,0,0.0208683,"gnized. In our work, we use an automated component extractor instead. Syntactic patterns are often used for extracting potential simile components and semantic analysis is then used to distinguish similes from literal comparisons (Niculae and Yaneva, 2013; Niculae, 2013). The main limitation is that such pattern based method is difficult to deal with sentences with complex structures. As a result, the coverage is relatively small. Qadir et al. (2016) used syntactic structures, dictionary definitions, statistical cooccurrence, and word embedding vectors to infer implicit properties in similes. Qadir et al. (2015) also built a classifier with lexical features, semantic features, and sentiment features to infer the affective polarity of simile in twitters. Veale and Hao (2007) and Veale (2012a) utilized knowledge generated by similes to deal with metaphor and irony, and Veale (2012b) built a lexical stereotype model from similes. These work demonstrates the wide applications of simile recognition. In Chinese, Li et al. (2008) proposed a featurebased method for simile recognition. Their evaluation was done on a small dataset. The annotated data in this work is much larger. 2.2 Multitask Learning for NLP"
D18-1183,P12-2015,0,0.0139643,"inguish similes from literal comparisons (Niculae and Yaneva, 2013; Niculae, 2013). The main limitation is that such pattern based method is difficult to deal with sentences with complex structures. As a result, the coverage is relatively small. Qadir et al. (2016) used syntactic structures, dictionary definitions, statistical cooccurrence, and word embedding vectors to infer implicit properties in similes. Qadir et al. (2015) also built a classifier with lexical features, semantic features, and sentiment features to infer the affective polarity of simile in twitters. Veale and Hao (2007) and Veale (2012a) utilized knowledge generated by similes to deal with metaphor and irony, and Veale (2012b) built a lexical stereotype model from similes. These work demonstrates the wide applications of simile recognition. In Chinese, Li et al. (2008) proposed a featurebased method for simile recognition. Their evaluation was done on a small dataset. The annotated data in this work is much larger. 2.2 Multitask Learning for NLP Many researchers have proposed to jointly learn multiple tasks with shared representations (Collobert and Weston, 2008). Improvements are reported on joint models between closely re"
D18-1183,N16-1146,0,0.0149488,"cues as features. It is similar to simile sentence classification. So we take it as a baseline. In their work, they assumed that the components can be correctly recognized. In our work, we use an automated component extractor instead. Syntactic patterns are often used for extracting potential simile components and semantic analysis is then used to distinguish similes from literal comparisons (Niculae and Yaneva, 2013; Niculae, 2013). The main limitation is that such pattern based method is difficult to deal with sentences with complex structures. As a result, the coverage is relatively small. Qadir et al. (2016) used syntactic structures, dictionary definitions, statistical cooccurrence, and word embedding vectors to infer implicit properties in similes. Qadir et al. (2015) also built a classifier with lexical features, semantic features, and sentiment features to infer the affective polarity of simile in twitters. Veale and Hao (2007) and Veale (2012a) utilized knowledge generated by similes to deal with metaphor and irony, and Veale (2012b) built a lexical stereotype model from similes. These work demonstrates the wide applications of simile recognition. In Chinese, Li et al. (2008) proposed a feat"
D18-1183,W09-1119,0,0.015504,"sensitive to word order, and the bidirectional LSTM (Schuster and Paliwal, 2002) allows model to look arbitrarily far at both the past and the future for the sake of grasping the whole sentence. Noted the forward ! LSTM as h , and the backward as h . Bidirectional LSTM concatenates the forward and backward states as hthe representation at the tth time ! i step, i.e., ht = ht ; ht . 4.3 Task 1: Simile Component Extraction We view simile component extraction as a sequence labeling problem. We convert the annotated dataset to IOBES scheme (indicating Inside, Outside, Beginning, Ending, Single) (Ratinov and Roth, 2009). We use different prefixes to distinguish the tenor and the vehicle components. For example, tb and vb indicate the beginning of a tenor and a vehicle respectively. 4.3.1 Neural Sequence Labeling Model Conditional Random Field (CRF) (Lafferty et al., 2001) is a standard solution in such scenario to exploit the dependency among labels.To further make use of the dense representation of words, we build a CRF layer on the shared representation layers following (Lample et al., 2016). Formally, H = (h1 , h2 , ..., hn ) is a sequence of hidden states produced by the bidirectional LSTM for a sentence"
D18-1183,P17-1194,0,0.268741,"s a new resource for related research. 1 • We propose a neural multitask learning framework jointly optimizing three tasks: simile sentence classification, simile component extraction and language modeling. Simile classification is to determine whether a sentence with a comparator contains a simile, without knowing exactly what the tenor and the vehicle are. Simile component extraction aims to locate the tenor and the vehicle in a simile sentence. Intuitively, the two tasks should benefit each other. We design our model to enhance interactions between the two tasks. We also borrow the idea of Rei (2017) by incorporating a language modeling task, which attempts to predict neighbor words. All three tasks consider the whole sentence so that rich context information is involved. • We conduct comprehensive experiments. The results demonstrate that the neural endto-end framework is superior to featurebased and rule-based baselines and every single model can benefit from multitask learning. Simile sentence classification can be solved very well, while simile component extraction is more chellenging. With multitask learning enhanced classifier and extractor, a classification-then-extraction method a"
D18-1183,P16-1147,0,0.0300342,"Missing"
D18-1183,C10-1113,0,0.0133556,"t natural language processing (NLP) to high-level semantic tasks (Carbonell, 1980). Metaphors reflect creative thought of humans. On the other hand, inferring the meaning of a metaphor has to integrate background knowledge, which makes it difficult to automatically recognize metaphors in language. Previous work on metaphor recognition mainly depends on linguistic cues (Goatly, 2011) and selectional preference violation on a pair of concepts (Fass, 1991) or their domains (Mason, 2004). The domains can be created by knowledge bases such as WordNet (Mason, 2004) or based on automatic clustering (Shutova et al., 2010). In this paper, we focus on a special type of metaphor—simile. A simile is a figure of speech that directly compares two things using connecting words such as like, as, than in English and “œ” or “πÇ” in Chinese. Due to the use of such comparators, it is much easier to locate similes compared with locating other types of metaphors. As a result, it is possible to collect and annotate large scale of simile sentences and investigate data driven simile recognition. This task is to find simile sentences and extract simile components, i.e., the tenor and the vehicle. The mined simile structures can"
D18-1183,shutova-teufel-2010-metaphor,0,0.0224447,"work framework for jointly optimizing three tasks: simile sentence classification, simile component extraction and language modeling. The experimental results show that the neural network based approaches can outperform all rule-based and feature-based baselines. Both simile sentence classification and simile component extraction can benefit from multitask learning. The former can be solved very well, while the latter is more difficult. 1 Introduction A metaphor is a figure of speech that describes an object or action in a way that isn’t literally true. Metaphors are common in human language. Shutova and Teufel (2010) reported that 241 among 760 sentences in an annotated corpus contain a metaphor. The use of metaphors helps to explain an idea or realize rhetorical effects through an analogical procedure. Metaphor analysis has been drawn more attention for expanding current natural language processing (NLP) to high-level semantic tasks (Carbonell, 1980). Metaphors reflect creative thought of humans. On the other hand, inferring the meaning of a metaphor has to integrate background knowledge, which makes it difficult to automatically recognize metaphors in language. Previous work on metaphor recognition main"
D18-1183,J13-2003,0,0.0189206,"ines and every single model can benefit from multitask learning. Simile sentence classification can be solved very well, while simile component extraction is more chellenging. With multitask learning enhanced classifier and extractor, a classification-then-extraction method achieves the best performance for simile component extraction. 2 2.1 Related Work Metaphor/Simile Analysis Metaphor analysis becomes active in recent years. The tasks include metaphor recognition, metaphor 1 The dataset is at https://github.com/cnunlp/ Chinese-Simile-Recognition-Dataset explanation and metaphor generation (Shutova et al., 2013; Veale, 1995; Jang et al., 2016). Simile is a special type of metaphor with the comparator and it is relatively easier to locate metaphorical parts. Niculae and DanescuNiculescu-Mizil (2014) aimed to distinguish a comparison from figurative or literal in product reviews using a series of linguistic cues as features. It is similar to simile sentence classification. So we take it as a baseline. In their work, they assumed that the components can be correctly recognized. In our work, we use an automated component extractor instead. Syntactic patterns are often used for extracting potential simil"
D18-1189,P17-1097,0,0.024818,"e impact on the effectiveness (T = 0 refers to using MLE to train the model). As shown in Figure 4, when more distinguishing test cases are used, higher accuracy is reached. However, more random test cases make limited improvement. 5 Related Work Program Synthesis. Our work falls into the general topic of program synthesis. Program synthesis is the problem of automatically generating programs from high-level specifications (Gulwani et al., 2017). There has been a lot of progress made in this area, classified based on (1) the form of specifications, e.g., NL descriptions (Yin and Neubig, 2017; Guu et al., 2017; Lin et al., 2017; Krishnamurthy and Mitchell, 2012; Liu et al., 2018), input-output examples (Balog et al., 2017; Chen et al., 2018; Kalyan et al., 2018), and hybrid of the two preceding types of specifications (Manshadi 66 0 2 4 6 8 10 T: number of pos/neg examples Figure 4: Impact of the number of distinguishing or random test cases on accuracy on the KB13 dataset. et al., 2013; Raza et al., 2015); (2) the programming languages, e.g., LISP (Biermann, 1978), Python (Yin and Neubig, 2017; Rabinovich et al., 2017), SQL (Zhong et al., 2017; Sun et al., 2018), and Domain-Specific Languages (DSL"
D18-1189,P82-1020,0,0.826788,"Missing"
D18-1189,D12-1069,0,0.0331371,"refers to using MLE to train the model). As shown in Figure 4, when more distinguishing test cases are used, higher accuracy is reached. However, more random test cases make limited improvement. 5 Related Work Program Synthesis. Our work falls into the general topic of program synthesis. Program synthesis is the problem of automatically generating programs from high-level specifications (Gulwani et al., 2017). There has been a lot of progress made in this area, classified based on (1) the form of specifications, e.g., NL descriptions (Yin and Neubig, 2017; Guu et al., 2017; Lin et al., 2017; Krishnamurthy and Mitchell, 2012; Liu et al., 2018), input-output examples (Balog et al., 2017; Chen et al., 2018; Kalyan et al., 2018), and hybrid of the two preceding types of specifications (Manshadi 66 0 2 4 6 8 10 T: number of pos/neg examples Figure 4: Impact of the number of distinguishing or random test cases on accuracy on the KB13 dataset. et al., 2013; Raza et al., 2015); (2) the programming languages, e.g., LISP (Biermann, 1978), Python (Yin and Neubig, 2017; Rabinovich et al., 2017), SQL (Zhong et al., 2017; Sun et al., 2018), and Domain-Specific Languages (DSL) such as FlashFill (Gulwani, 2011). In this paper,"
D18-1189,N13-1103,0,0.73654,"antic correctness by checking whether it can pass all the test cases. However, a regular expression may have infinite positive (i.e., matched) or negative (i.e., unmatched) string examples; thus, we cannot perfectly represent the semantics. To use limited string examples to differentiate whether a generated regular expression is semantically correct or not, we propose an intelligent strategy for test generation to generate distinguishing test cases instead of just using random test cases. We evaluate SemRegex on three public datasets: NL-RX-Synth, NL-RX-Turk (Locascio et al., 2016), and KB13 (Kushman and Barzilay, 2013). We compare SemRegex with the existing state-ofthe-art approaches on the task of generating regular expressions from NL specifications. Our evaluation results show that SemRegex outperforms the start-of-the-art approaches on all of three datasets. The evaluation results confirm that by maximizing semantic correctness, the model can output more correct regular expressions even when the regular expressions are syntactically different from the ground truth. In summary, this paper makes the following three main contributions. (1) We propose a semantics-based approach to optimize the semantics-bas"
D18-1189,D16-1197,0,0.605856,"Missing"
D18-1189,P17-1105,0,0.0322274,"d based on (1) the form of specifications, e.g., NL descriptions (Yin and Neubig, 2017; Guu et al., 2017; Lin et al., 2017; Krishnamurthy and Mitchell, 2012; Liu et al., 2018), input-output examples (Balog et al., 2017; Chen et al., 2018; Kalyan et al., 2018), and hybrid of the two preceding types of specifications (Manshadi 66 0 2 4 6 8 10 T: number of pos/neg examples Figure 4: Impact of the number of distinguishing or random test cases on accuracy on the KB13 dataset. et al., 2013; Raza et al., 2015); (2) the programming languages, e.g., LISP (Biermann, 1978), Python (Yin and Neubig, 2017; Rabinovich et al., 2017), SQL (Zhong et al., 2017; Sun et al., 2018), and Domain-Specific Languages (DSL) such as FlashFill (Gulwani, 2011). In this paper, we focus on an important subtask of the program-synthesis problem: generating regular expressions from NL. Generating Regular Expressions. Recent research has attempted to automatically generate regular expressions from NL specifications. Ranta (1998) propose a rule-based approach to build an NL interface for regular expressions. Kushman and Barzilay (2013) develop an approach for learning a probabilistic grammar model to parse an NL description into a regular exp"
D18-1189,W98-1308,0,0.858826,"e number of distinguishing or random test cases on accuracy on the KB13 dataset. et al., 2013; Raza et al., 2015); (2) the programming languages, e.g., LISP (Biermann, 1978), Python (Yin and Neubig, 2017; Rabinovich et al., 2017), SQL (Zhong et al., 2017; Sun et al., 2018), and Domain-Specific Languages (DSL) such as FlashFill (Gulwani, 2011). In this paper, we focus on an important subtask of the program-synthesis problem: generating regular expressions from NL. Generating Regular Expressions. Recent research has attempted to automatically generate regular expressions from NL specifications. Ranta (1998) propose a rule-based approach to build an NL interface for regular expressions. Kushman and Barzilay (2013) develop an approach for learning a probabilistic grammar model to parse an NL description into a regular expression. Locascio et al. (2016) regard the problem as a black-box task of machine translation, and train a sequenceto-sequence learning model to address the problem. There exists also a lot of work focusing on generating regular expressions from string examples. Recent work typically uses an evolutionary algorithm to address the problem (Svingen, 1998; Cetinkaya, 2007; Bartoli et"
D18-1189,P18-1034,1,0.839352,"descriptions (Yin and Neubig, 2017; Guu et al., 2017; Lin et al., 2017; Krishnamurthy and Mitchell, 2012; Liu et al., 2018), input-output examples (Balog et al., 2017; Chen et al., 2018; Kalyan et al., 2018), and hybrid of the two preceding types of specifications (Manshadi 66 0 2 4 6 8 10 T: number of pos/neg examples Figure 4: Impact of the number of distinguishing or random test cases on accuracy on the KB13 dataset. et al., 2013; Raza et al., 2015); (2) the programming languages, e.g., LISP (Biermann, 1978), Python (Yin and Neubig, 2017; Rabinovich et al., 2017), SQL (Zhong et al., 2017; Sun et al., 2018), and Domain-Specific Languages (DSL) such as FlashFill (Gulwani, 2011). In this paper, we focus on an important subtask of the program-synthesis problem: generating regular expressions from NL. Generating Regular Expressions. Recent research has attempted to automatically generate regular expressions from NL specifications. Ranta (1998) propose a rule-based approach to build an NL interface for regular expressions. Kushman and Barzilay (2013) develop an approach for learning a probabilistic grammar model to parse an NL description into a regular expression. Locascio et al. (2016) regard the p"
D18-1264,P17-1044,0,0.0254208,"d in our intrinsic evaluation. 8 e.g., “North Koreans” cannot be parsed into (name :op1 &quot;North&quot; :op2 &quot;Korea&quot;) 9 e.g., “Wi Sung - lac” cannot be parsed into (name :op1 &quot;Wi&quot; :op2 &quot;Sung-lac&quot;) 10 e.g. the first “nuclear” aligned to nucleus˜1 in Fig. 1 exp{ga · S TACK LSTM(s) + ba } , a0 exp{ga0 · S TACK LSTM(s) + ba0 } where S TACK LSTM(s) encodes the state s into a vector and ga is the embedding vector of action a. We encourage the reader to refer Ballesteros and Al-Onaizan (2017) for more details. Ensemble. Ensemble has been shown as an effective way of improving the neural model’s performance (He et al., 2017). Since the transitionbased parser directly parse a sentence into its AMR graph, ensemble of several parsers is easier compared to the two-staged AMR parsers. In this paper, we ensemble the parsers trained with different initialization by averaging their probability distribution over the actions. 5 5.1 Alignment Experiments Settings We evaluate our aligner on the LDC2014T12 dataset. Two kinds of evaluations are carried out including the intrinsic and extrinsic evaluations. For the intrinsic evaluation, we follow Flanigan et al. (2014) and evaluate the F1 score of the alignments produced by our"
D18-1264,P17-1014,0,0.380708,"scu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation (Pourdamghani et al.,"
D18-1264,D15-1198,0,0.220272,"Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either gree"
D18-1264,N15-1142,0,0.0374791,"uced by the semantic matching especially by the word embedding similarity component. Without filtering the noise by our oracle parser, just introducing more matching rules will harm the performance. 6 6.1 newswire 73.1 72.7 67.6 71.3 Parsing Experiments Settings We use the same settings in our aligner extrinsic evaluation for the experiments on our transitionbased parser. For the input to the parser, we tried two settings: 1) using only words as input, and 2) using words and POS tags as input. Automatic POS tags are assigned with Stanford POS tagger (Manning et al., 2014). Word embedding from Ling et al. (2015) is used in the same way with Ballesteros and Al-Onaizan (2017). To opt Results Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works. When compared with our transition-based counterpart (Ballesteros and AlOnaizan, 2017), our word-only model outperforms theirs using the same JAMR alignment. The same trend is witnessed using words and POS tags as input. When replacing the JAMR alignments with ours, the parsing performances are improved in the same way as in Table 4, which further confirms the effectiveness of our aligner. The seco"
D18-1264,D17-1130,0,0.0550349,"ar reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique"
D18-1264,W13-2322,0,0.262658,"sistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017). 1 ARG1 ARG0 country act-02 name name op1 &quot;North&quot; ARG1 reactor mod nucleus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2"
D18-1264,P13-2131,0,0.475771,"Missing"
D18-1264,P13-1104,0,0.266774,"rst element of the buffer (a word or span) into a concept c. a special form of C ONFIRM that derives the first element into an entity and builds the internal entity AMR fragment. generates a new concept c and pushes it to the front of the buffer. Table 2: The transition system. The letters in monospace font represent the concepts, the italic letters represent the word, and the letters in normal font are either concepts or words. Their work presented the possibility for the oracle parser, but their oracle parser was not touched explicitly. What’s more, in the non-projective dependency parsing, Choi and McCallum (2013)’s extension to the list-based system (Nivre, 2008) with caching mechanism achieves expected linear time complexity and requires fewer actions to parse a non-projective tree than the swap-based system. Their extension to transition-based AMR parsing is worth studying. In this paper, we propose to extend Choi and McCallum (2013)’s transition system to AMR parsing and present the corresponding oracle parser. The oracle parser is used for tuning our aligner and training our parser. We also present a comprehensive comparison of our system with that of Ballesteros and Al-Onaizan (2017) in Section 6"
D18-1264,E17-1051,0,0.619876,"exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the uns"
D18-1264,P15-1033,0,0.0258826,"ctions in Table 2 are our ex2426 4. If b0 is a word or span and it aligns to one or more concepts, perform C ONFIRM (c) where c is the concept b0 aligns and has the longest graph distance to the root. 5. If b0 is a concept and its head concept c has the same alignment as b0 , perform N EW (c). 6. If b0 is a concept and there is an unprocessed edge r between s0 and t0 , perform L EFT (r) or R IGHT (r) according to r’s direction. g1 a1 Training Data Aligner .. . an Oracle .. . raw sentence directly into its AMR graph. In this paper, we follow Ballesteros and Al-Onaizan (2017) and use StackLSTM (Dyer et al., 2015) to model the states. The score of a transition action a on state s is calculated as s1 Eval. gn .. . sn p(a|s) = P highest-scored, pick Figure 2: The workflow of tuning the aligner with the oracle parser. ai denotes the i-th alignment, gi denotes the i-th AMR graph, and si denotes the score of the i-th AMR graph. 7. If s0 has unprocessed edge, perform C ACHE. 8. If s0 doesn’t have unprocessed edge, perform R EDUCE. 9. perform S HIFT. We test our oracle parser on the hand-align data created by Flanigan et al. (2014) and it achieves 97.4 Smatch F1 score.7 Besides the errors resulted from incorr"
D18-1264,S16-1186,0,0.252255,"ieves 68.4 Smatch F1 score with only words and POS tags as input (§6) and outperforms the parser of Wang and Xue (2017). Our code and the alignments for LDC2014T12 dataset are publicly available at https:// github.com/Oneplus/tamr 2 Related Work AMR Parsers. AMR parsing maps a natural language sentence into its AMR graph. Most current parsers construct the AMR graph in a two-staged manner which first identifies concepts (nodes in the graph) from the input sentence, then identifies relations (edges in the graph) between the identified concepts. Flanigan et al. (2014) and their follow-up works (Flanigan et al., 2016; Zhou et al., 2016) model the parsing problem as finding the maximum spanning connected graph. Wang et al. (2015b) proposes to greedily transduce the dependency tree into AMR graph and a bunch of works (Wang et al., 2015a; Goodman et al., 2016; Wang and Xue, 2017) further improve the transducer’s performance with rich features and imitation learning.2 Transition-based methods 2 Wang et al. (2015b) and the follow-up works refer their transducing process as “transition-based”. However, to dis2423 that directly parse an input sentence into its AMR graph have also been studied (Ballesteros and Al"
D18-1264,P14-1134,0,0.0948162,"&quot; ARG1 reactor mod nucleus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In"
D18-1264,P17-1043,0,0.136676,"resentation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation"
D18-1264,P16-1001,0,0.124026,"ts nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 201"
D18-1264,P14-5010,0,0.0056679,"servation to that alignment noise is introduced by the semantic matching especially by the word embedding similarity component. Without filtering the noise by our oracle parser, just introducing more matching rules will harm the performance. 6 6.1 newswire 73.1 72.7 67.6 71.3 Parsing Experiments Settings We use the same settings in our aligner extrinsic evaluation for the experiments on our transitionbased parser. For the input to the parser, we tried two settings: 1) using only words as input, and 2) using words and POS tags as input. Automatic POS tags are assigned with Stanford POS tagger (Manning et al., 2014). Word embedding from Ling et al. (2015) is used in the same way with Ballesteros and Al-Onaizan (2017). To opt Results Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works. When compared with our transition-based counterpart (Ballesteros and AlOnaizan, 2017), our word-only model outperforms theirs using the same JAMR alignment. The same trend is witnessed using words and POS tags as input. When replacing the JAMR alignments with ours, the parsing performances are improved in the same way as in Table 4, which further confirms th"
D18-1264,J08-4003,0,0.0603115,"special form of C ONFIRM that derives the first element into an entity and builds the internal entity AMR fragment. generates a new concept c and pushes it to the front of the buffer. Table 2: The transition system. The letters in monospace font represent the concepts, the italic letters represent the word, and the letters in normal font are either concepts or words. Their work presented the possibility for the oracle parser, but their oracle parser was not touched explicitly. What’s more, in the non-projective dependency parsing, Choi and McCallum (2013)’s extension to the list-based system (Nivre, 2008) with caching mechanism achieves expected linear time complexity and requires fewer actions to parse a non-projective tree than the swap-based system. Their extension to transition-based AMR parsing is worth studying. In this paper, we propose to extend Choi and McCallum (2013)’s transition system to AMR parsing and present the corresponding oracle parser. The oracle parser is used for tuning our aligner and training our parser. We also present a comprehensive comparison of our system with that of Ballesteros and Al-Onaizan (2017) in Section 6.3. 4.1 tended actions, and they are used to derivi"
D18-1264,K15-1004,0,0.283535,"for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set"
D18-1264,E17-1035,0,0.292067,"; Damonte et al., 2017). In these works, the concept identification and relation identification are performed jointly. An aligner which maps a span of words into its concept serves to the generation of training data for the concept identifier, thus is important to the parser training. Missing or incorrect alignments lead to poor concept identification, which then hurt the overall AMR parsing performance. Besides the typical two-staged methods, the aligner also works in some other AMR parsing algorithms like that using syntax-based machine translation (Pust et al., 2015), sequence-to-sequence (Peng et al., 2017; Konstas et al., 2017), Hyperedge Replacement Grammar (Peng et al., 2015) and Combinatory Category Grammar (Artzi et al., 2015). Previous aligner works solve the alignment problem in two different ways. The rule-based aligner (Flanigan et al., 2014) defines a set of heuristic rules which align a span of words to the graph fragment and greedily applies these rules. The unsupervised aligner (Pourdamghani et al., 2014; Wang and Xue, 2017) uncovers the word-toconcept alignment from the linearized AMR graph through EM. All these approaches yield a single alignment for one sentence and its effect o"
D18-1264,D14-1162,0,0.081,"rs because of the dependencies and mutual exclusions between rules. In the JAMR aligner, rules that recall more alignments but introduce errors are carefully opted out and it influences the aligner’s performance. Our motivation is to use rich semantic resources to recall more alignments. Instead of resolving the resulted conflicts and errors by greedy search, we keep the multiple alignments produced by the aligner and let a parser decide the best alignment. In this paper, we use two kinds of semantic resources to recall more alignments, which include the similarity drawn from Glove embedding (Pennington et al., 2014)3 and the morphosemantic database (Fellbaum et al., 2009) in the WordNet project4 . Two additional matching schemes semantic match and morphological match are proposed as: Semantic Match. Glove embedding encodes a word into its vector representation. We define semantic match of a concept as a word in the sentence that has a cosine similarity greater than 0.7 in the embedding space with the concept striping off trailing number (e.g. run-01 → run). Morphological Match. Morphosemantic is a database that contains links among derivational 3 nlp.stanford.edu/projects/glove/ wordnet.princeton.edu/wor"
D18-1264,D14-1048,0,0.483311,"Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation (Pourdamghani et al., 2014; Wang and Xue, 2017). The rule-based aligner – JAMR aligner proposed by Flanigan et al. (2014) is widely used in previous works thanks to its flexibility of incorporating additional linguistic resources like WordNet. However, achieving good alignments with the JAMR aligner still faces some difficult challenges. The first challenge is deriving an optimal alignment in ambiguous situations. Taking the sentence-AMR-graph pair in Figure 1 for example, the JAMR aligner doesn’t distinguish between the two “nuclear”s in the sentence and can yield sub-optimal alignment in which the first “nuclear” is"
D18-1264,D15-1136,0,0.286353,"cleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extr"
D18-1264,D17-1035,0,0.0145197,"Aligner JAMR Our Alignment F1 (on hand-align) 90.6 95.2 Oracle’s Smatch (on dev. dataset) 91.7 94.7 model JAMR parser + Our aligner - Semantic matching - Oracle Parser Tuning JAMR parser + JAMR aligner Table 3: The intrinsic evaluation results. model newswire JAMR parser: Word, POS, NER, DEP + JAMR aligner 71.3 + Our aligner 73.1 CAMR parser: Word, POS, NER, DEP + JAMR aligner 68.4 + Our aligner 68.8 Table 5: The ablation test results. all out the effect of different initialization in training the neural network, we run 10 differently seeded runs and report their average performance following Reimers and Gurevych (2017). 65.9 67.6 64.6 65.1 6.2 Table 4: The parsing results. Extrinsic Evaluation. Table 4 shows the results. From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7. Both the intrinsic and the extrinsic evaluations show the effectiveness our aligner. 5.3 Ablation To have a better understanding of our aligner, we conduct ablation test by removing the semantic matching and oracle parser tuning respectively and retrain the JAMR parser on the newswire proportion. The results are shown in Table 5. From this table, we can see that removing"
D18-1264,D17-1129,0,0.460076,"racle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highestscored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017). 1 ARG1 ARG0 country act-02 name name op1 &quot;North&quot; ARG1 reactor mod nucleus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent"
D18-1264,P15-2141,0,0.39251,"eus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, su"
D18-1264,N15-1040,0,0.456906,"eus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, su"
D18-1264,D16-1065,0,0.64792,"North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules"
D19-1013,P16-1014,0,0.199952,"Gumbel(0, 1)1 and τ is a constant that controls the smoothness of the distribution. approx Tj replaces Tj in equation 1 and goes through the same flattening and expanding process as V to approx0 get vt and the training signal from Seq2Seq generation is passed via the logit approx ot 0 ˜ t, h ˜ 0 ] + vtapprox . = U [h t To make training with Gumbel-Softmax more stable, we first initialize the parameters by pretraining the KB-retriever with distant supervision and further fine-tuning our framework. 1 We sample g by drawing u ∼ Uniform(0, 1) then computing g = − log(− log(u)). 137 4.3 • Ptr-UNK (Gulcehre et al., 2016): Ptr-UNK is the model which augments a sequenceto-sequence architecture with attention-based copy mechanism over the encoder context. Experimental Settings We choose the InCar Assistant dataset (Eric et al., 2017) including three distinct domains: navigation, weather and calendar domain. For weather domain, we follow Wen et al. (2018) to separate the highest temperature, lowest temperature and weather attribute into three different columns. For calendar domain, there are some dialogues without a KB or incomplete KB. In this case, we padding a special token “-” in these incomplete KBs. Our fra"
D19-1013,E17-1001,0,0.0371657,"the corresponding KB to every dialogues. All hyper-parameters are selected according to validation set. We use a three-hop memory network to model our KB-retriever. The dimensionalities of the embedding is selected from {100, 200} and LSTM hidden units is selected from {50, 100, 150, 200, 350}. The dropout we use in our framework is selected from {0.25, 0.5, 0.75} and the batch size we adopt is selected from {1, 2}. L2 regularization is used on our model with a tension of 5 × 10−6 for reducing overfitting. For training the retriever with distant supervision, we adopt the weight typing trick (Liu and Perez, 2017). We use Adam (Kingma and Ba, 2014) to optimize the parameters in our model and adopt the suggested hyper-parameters for optimization. We adopt both the automatic and human evaluations in our experiments. 4.4 • KV Net (Eric et al., 2017): The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities. • Mem2Seq (Madotto et al., 2018): Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the"
D19-1013,D15-1166,0,0.798183,"based on Seq2Seq generation, there is a trend in recent study towards modeling the KB query as an attention network over the entire KB entity representations, hoping to learn a model to pay more attention to the relevant entities (Eric et al., 2017; Madotto et al., 2018; Reddy et al., 2018; Wen et al., 2018). Introduction Task-oriented dialogue system, which helps users to achieve specific goals with natural language, is attracting more and more research attention. With the success of the sequence-to-sequence (Seq2Seq) models in text generation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Nallapati et al., 2016b,a), several works tried to model the task-oriented dialogue as the Seq2Seq generation of response from the dialogue Though achieving good end-to-end dialogue generation with over-the-entire-KB attention mechanism, these methods do not guarantee the generation consistency regarding KB entities and ∗ * Email corresponding. 133 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 133–142, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computat"
D19-1013,P18-1136,0,0.488682,"ated at 200 Alester Ave. OK , please give me directions via a route that avoids all heavy traffic. Since there is a road block nearby, I found another route for you and I sent it on your screen. Awesome thank you. Figure 1: An example of a task-oriented dialogue that incorporates a knowledge base (KB). The fourth row in KB supports the second turn of the dialogue. A dialogue system will produce a response with conflict entities if it includes the POI in the fourth row and the address in the fifth row, like “Valero is located at 899 Ames Ct”. history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018). This kind of modeling scheme frees the task-oriented dialogue system from the manually designed pipeline modules and heavy annotation labor for these modules. Different from typical text generation, the successful conversations for task-oriented dialogue system heavily depend on accurate knowledge base (KB) queries. Taking the dialogue in Figure 1 as an example, to answer the driver’s query on the gas station, the dialogue system is required to retrieve the entities like “200 Alester Ave” and “Valero”. For the task-oriented system based on Seq2Seq generation, there is a trend in recent study"
D19-1013,N13-1095,0,0.0304659,"|V |+|E|. In vt , lower |V |is zero and the rest|E |is retrieved entity scores. 4 Training the KB-Retriever As mentioned in section 3.3.1, we adopt the memory network to train our KB-retriever. However, in the Seq2Seq dialogue generation, the training data does not include the annotated KB row retrieval results, which makes supervised training the KBretriever impossible. To tackle this problem, we propose two training methods for our KB-rowretriever. 1) In the first method, inspired by the recent success of distant supervision in information extraction (Zeng et al., 2015; Mintz et al., 2009; Min et al., 2013; Xu et al., 2013), we take advantage of the similarity between the surface string of KB entries and the reference response, and design a set of heuristics to extract training data for the KB-retriever. 2) In the second method, instead of training the KB-retriever as an independent component, we train it along with the training of the Seq2Seq dialogue generation. To make the retrieval process in Equation 1 differentiable, we use Gumbel-Softmax (Jang et al., 2017) as an approximation of the argmax during training. 4.1 4.2 Training with Gumbel-Softmax In addition to treating the row retrieval re"
D19-1013,P09-1113,0,0.0347149,"’s dimensionality is |V |+|E|. In vt , lower |V |is zero and the rest|E |is retrieved entity scores. 4 Training the KB-Retriever As mentioned in section 3.3.1, we adopt the memory network to train our KB-retriever. However, in the Seq2Seq dialogue generation, the training data does not include the annotated KB row retrieval results, which makes supervised training the KBretriever impossible. To tackle this problem, we propose two training methods for our KB-rowretriever. 1) In the first method, inspired by the recent success of distant supervision in information extraction (Zeng et al., 2015; Mintz et al., 2009; Min et al., 2013; Xu et al., 2013), we take advantage of the similarity between the surface string of KB entries and the reference response, and design a set of heuristics to extract training data for the KB-retriever. 2) In the second method, instead of training the KB-retriever as an independent component, we train it along with the training of the Seq2Seq dialogue generation. To make the retrieval process in Equation 1 differentiable, we use Gumbel-Softmax (Jang et al., 2017) as an approximation of the argmax during training. 4.1 4.2 Training with Gumbel-Softmax In addition to treating th"
D19-1013,K16-1028,0,0.0493305,"eration, there is a trend in recent study towards modeling the KB query as an attention network over the entire KB entity representations, hoping to learn a model to pay more attention to the relevant entities (Eric et al., 2017; Madotto et al., 2018; Reddy et al., 2018; Wen et al., 2018). Introduction Task-oriented dialogue system, which helps users to achieve specific goals with natural language, is attracting more and more research attention. With the success of the sequence-to-sequence (Seq2Seq) models in text generation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Nallapati et al., 2016b,a), several works tried to model the task-oriented dialogue as the Seq2Seq generation of response from the dialogue Though achieving good end-to-end dialogue generation with over-the-entire-KB attention mechanism, these methods do not guarantee the generation consistency regarding KB entities and ∗ * Email corresponding. 133 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 133–142, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tion th"
D19-1013,P17-1045,0,0.0384064,"of neural network in natural language processing, efforts have been made to replacing the discrete and predefined dialogue state with the distributed representation (Bordes and Weston, 2017; Wen et al., 2017b,a; Liu and Lane, 2017). In our framework, our retrieval result can be treated as a numeric representation of the API call return. Instead of interacting with the KB via API calls, more and more recent works tried to incorporate KB query as a part of the model. The most popular way of modeling KB query is treating it as an attention network over the entire KB entities (Eric et al., 2017; Dhingra et al., 2017; Reddy et al., 2018; Raghu et al., 2019; Wu et al., 2019) and the return can be a fuzzy summation of the entity representations. Madotto et al. (2018)’s practice of modeling the KB query with memory network can also be considered as learning an attentive preferHuman Evaluation We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is prese"
D19-1013,N19-1126,0,0.176416,"cessing, efforts have been made to replacing the discrete and predefined dialogue state with the distributed representation (Bordes and Weston, 2017; Wen et al., 2017b,a; Liu and Lane, 2017). In our framework, our retrieval result can be treated as a numeric representation of the API call return. Instead of interacting with the KB via API calls, more and more recent works tried to incorporate KB query as a part of the model. The most popular way of modeling KB query is treating it as an attention network over the entire KB entities (Eric et al., 2017; Dhingra et al., 2017; Reddy et al., 2018; Raghu et al., 2019; Wu et al., 2019) and the return can be a fuzzy summation of the entity representations. Madotto et al. (2018)’s practice of modeling the KB query with memory network can also be considered as learning an attentive preferHuman Evaluation We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is presented with the dialogue history, an outpu"
D19-1013,W17-5506,0,0.0605833,"tion. Valero is located at 200 Alester Ave. OK , please give me directions via a route that avoids all heavy traffic. Since there is a road block nearby, I found another route for you and I sent it on your screen. Awesome thank you. Figure 1: An example of a task-oriented dialogue that incorporates a knowledge base (KB). The fourth row in KB supports the second turn of the dialogue. A dialogue system will produce a response with conflict entities if it includes the POI in the fourth row and the address in the fifth row, like “Valero is located at 899 Ames Ct”. history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018). This kind of modeling scheme frees the task-oriented dialogue system from the manually designed pipeline modules and heavy annotation labor for these modules. Different from typical text generation, the successful conversations for task-oriented dialogue system heavily depend on accurate knowledge base (KB) queries. Taking the dialogue in Figure 1 as an example, to answer the driver’s query on the gas station, the dialogue system is required to retrieve the entities like “200 Alester Ave” and “Valero”. For the task-oriented system based on Seq2Seq generation, there is"
D19-1013,C18-1320,1,0.746102,"or task-oriented dialogue system heavily depend on accurate knowledge base (KB) queries. Taking the dialogue in Figure 1 as an example, to answer the driver’s query on the gas station, the dialogue system is required to retrieve the entities like “200 Alester Ave” and “Valero”. For the task-oriented system based on Seq2Seq generation, there is a trend in recent study towards modeling the KB query as an attention network over the entire KB entity representations, hoping to learn a model to pay more attention to the relevant entities (Eric et al., 2017; Madotto et al., 2018; Reddy et al., 2018; Wen et al., 2018). Introduction Task-oriented dialogue system, which helps users to achieve specific goals with natural language, is attracting more and more research attention. With the success of the sequence-to-sequence (Seq2Seq) models in text generation (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Nallapati et al., 2016b,a), several works tried to model the task-oriented dialogue as the Seq2Seq generation of response from the dialogue Though achieving good end-to-end dialogue generation with over-the-entire-KB attention mechanism, these methods do not guarantee the generation consi"
D19-1013,E17-1042,0,0.107666,"Missing"
D19-1013,P13-2117,0,0.0555894,"Missing"
D19-1013,D15-1203,0,0.0161679,"t = U [ h where ot ’s dimensionality is |V |+|E|. In vt , lower |V |is zero and the rest|E |is retrieved entity scores. 4 Training the KB-Retriever As mentioned in section 3.3.1, we adopt the memory network to train our KB-retriever. However, in the Seq2Seq dialogue generation, the training data does not include the annotated KB row retrieval results, which makes supervised training the KBretriever impossible. To tackle this problem, we propose two training methods for our KB-rowretriever. 1) In the first method, inspired by the recent success of distant supervision in information extraction (Zeng et al., 2015; Mintz et al., 2009; Min et al., 2013; Xu et al., 2013), we take advantage of the similarity between the surface string of KB entries and the reference response, and design a set of heuristics to extract training data for the KB-retriever. 2) In the second method, instead of training the KB-retriever as an independent component, we train it along with the training of the Seq2Seq dialogue generation. To make the retrieval process in Equation 1 differentiable, we use Gumbel-Softmax (Jang et al., 2017) as an approximation of the argmax during training. 4.1 4.2 Training with Gumbel-Softmax In add"
D19-1029,P15-1034,0,0.0252062,", we use pair-wise comparison to match the extracted and groundtruth tuples. We evaluate the correctness on the tuple’s five slots using the same metrics. Baselines: We compare with statistical sequence labeling methods: Structured Support Vector Machine (SVM) (Tsochantaridis et al., 2005) and Conditional random field (CRF) (Lafferty et al., 2001). We compare with a neural sequence labeling method, BiLSTM-LSTMd (Zheng et al., 2017). We replace its encoder with BERT (Devlin et al., 2018) to make it a more competitive baseline. We also compare against two renown OpenIE systems, Stanford OpenIE (Angeli et al., 2015) and AllenNLP OpenIE (Stanovsky et al., 2018) followed by a condition/fact classification. We enhance statistical sequence labeling models with multi-input signals for fairness, and train them for fact tuple and condition tuple extrac4.2 Results on BioCFE In this section, we present overall performance, ablation study, error analysis, and efficiency. 4.2.1 Overall Performance Table 1 shows that the proposed multi-input multioutput sequence labeling model with a BERT encoder consistently performs the best over all the baselines on tag prediction and tuple extraction. Compared to BiLSTM-LSTMd, B"
D19-1029,D15-1104,0,0.081185,"Missing"
D19-1029,W13-2001,0,0.297343,"Missing"
D19-1029,W14-1609,0,0.0193652,"Missing"
D19-1029,D14-1162,0,0.0821539,"rain the RNT layer with the multi-input module till the relation name’s tag prediction achieves a higherthan-0.8 F1 score. Then we plug the TCT layer onto the RNT layer and train the entire framework to generate the multi-output tag sequences. 4 Experiments tion separately. In the neural baselines (BiLSTMLSTMd and BERT-LSTMd), fact extraction and condition extraction share the encoder-decoder model and use different, proper parameters in the linear-softmax layer. Hyperparameters: The multi-input module has a BiLSTM/BERT encoder and a LSTM decoder. The word embeddings were obtained from GloVe (Pennington et al., 2014) with the dimension size dW E = 50. The language model dimension size ns dLM = 200. The size of POS tag embedding is dP OS = 6. The size of CAP tag embedding is dCAP = 3. The number of LSTM units in the encoding layer is 300. The number of transformer units in the BERT encoding layer is 768. We evaluate the performance of condition/fact tag prediction and tuple extraction by the proposed MIMO model, its variants, and state-of-the-art models on the newly annotated BioCFE dataset and transferred to the BioNLP2013 dataset. 4.1 Experimental Setup Datasets: Statistics of BioCFE has been given in th"
D19-1029,D15-1025,0,0.0624245,"Missing"
D19-1029,C18-1194,0,0.0270153,"Missing"
D19-1029,N18-1081,0,0.151065,"entific literature (Miller, 1947). Existing ScienceIE methods, which extract (subject, relational phrase, object)-tuples from scientific text, do not distinguish the roles of fact and condition. Simply adding a tuple classification module has two weak points: (1) one tuple may have different roles in different sentences; (2) the tuples in one sentence have high dependencies with each other, for example, given a statement sentence in a biochemistry paper (Tomilin et al., 2016): “We observed that ... alkaline pH increases the activity of TRPV5/V6 channels in Jurkat T cells.” an existing system (Stanovsky et al., 2018) would return one tuple as below: Multi-Input Module Multi-Head Input-Gates Multi-Head Input-Gates Statement Sentence Figure 1: Our framework has two modules: (1) a multiinput module (bottom) based on a multi-head encoderdecoder model with multi-input gates; (2) a multioutput module (top) of a relation name tagging layer and a tuple completion tagging layer. (alkaline pH, increases, activity of TRPV5/V6 channels in Jurkat T cells). where (a) the object should just be the channel’s activity and (b) the condition tuple (TRPV5/V6 channels, in, Jurkat T cells) was not found. Note that the term “TR"
D19-1029,D18-1360,0,0.18642,"taset 3.1 We built a system with GUI (Figure 2) to collect a new dataset for the joint tuple extraction purpose, named Biomedical Conditional Fact Extraction (BioCFE). Three participants (experts in biomedical domain) manually annotated the fact and condition tuples from statement sentences from 31 paper abstracts in the MEDLINE database. The anThe Multi-Input Module Preprocessing for input sequences: Following fundamental NLP techniques have achieved high accuracy requiring no additional training with labeled data: Language Model (LM) (Howard and Ruder, 2018), POS (Labeau et al., 2015), CAP (Luan et al., 2018; Jiang et al., 2017; Shang 303 et al., 2018; Wang et al., 2018a). For any given input sentence, we tokenize it and represent each token by its word embedding (pre-trained GloVe vector in this paper). Then we get another three input sequences by the input sentence and the above three fundamental NLP techniques. (1) A pre-trained LSTM-based language model takes the sentence as input and returns semantic embedding sequence, where the dependencies between a token and its predecessors in distant contexts are preserved. (2) We employ NLTK tool to generate the POS tag sequence for the given sentence"
D19-1029,D17-1279,0,0.0259644,"the conditions of the factual claims. They describe the methodology of the observation (e.g., “using”, “in combination with”) or the context (e.g., “in” a specific disease or “from” specific animals). In some other cases, we find the temperature and pH values are detected as the conditions of observations. 5 5.1 Related Work Scientific Information Extraction Information extraction in scientific literature, e.g., computer science, biology and chemistry, has been receiving much attention in recent years. ScienceIE in computer science focus on concept recognition and factual relation extraction (Luan et al., 2017; G´abor et al., 2018; Luan et al., 2018). ScienceIE in biological literature aims at identifying the relationships between biological concepts tion and fact tuples from scientific text. We proposed a multi-input multi-output sequence labeling model to utilize results from well-established related tasks and extract an uncertain number of fact(s)/condition(s). Our model yields improvement over all the baselines on a newly annotated dataset BioCFE and a public dataset BioNLP2013. We argue that structured representations of knowledge, such as fact/condition tuple, for scientific statements will e"
D19-1029,P10-1013,0,0.151675,"Missing"
D19-1029,P17-1132,0,0.0442362,"Missing"
D19-1029,P17-1113,0,0.107708,"s of words. Dependencies between the POS tag and tuple tag (e.g., “VBD” and “B-f2p”) can be modeled. We also spot high dependencies between the CAP tag and tuple tag. For example, the tokens of “B/I-c” (concept) and “B/I-a” (attribute) tags have high probability of being labeled as “B/I-XYc” and “B/IXYa” in the output sequences, respectively. Multi-head Encoder-Decoder: We investigate two neural models as encoder: one is bidirectional LSTM (BiLSTM), the other is the renown, bidirectional encoder representations from Transformers (BERT). We adopt a LSTM structure as the decoding layer (LSTMd) (Zheng et al., 2017). We observe that the input sequences may have different tag predictability on different sentences. For short sentences, POS and CAP are more useful (modeling local dependencies); for long sentences, LM is more effective (modeling distant dependencies). In order to secure the model’s robustness on massive data, we apply a multi-head mechanism to the encoder-decoder model. Each 304 Finally, we apply the matching function in (Stanovsky et al., 2018) to complete and extract the tuples (i.e., the concepts and/or attributes in the subjects and objects) for each output sequence. and the softmax pred"
D19-1169,P18-1163,0,0.0509824,"Missing"
D19-1169,P17-1055,1,0.877462,"erimental results on two public Chinese reading comprehension datasets show that the proposed cross-lingual approaches yield significant improvements over various baseline systems and set new state-of-the-art performances. 2 Related Works Machine Reading Comprehension (MRC) has been a trending research topic in recent years. Among various types of MRC tasks, spanextraction reading comprehension has been enormously popular (such as SQuAD (Rajpurkar et al., 2016)), and we have seen a great progress on related neural network approaches (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Cui et al., 2017; Hu et al., 2019), especially those were built on pre-trained language models, such as BERT (Devlin et al., 2019). While massive achievements have been made by the community, reading comprehension in other than English has not been well-studied mainly due to the lack of large-scale training data. Asai et al. (2018) proposed to use runtime machine translation for multilingual extractive reading comprehension. They first translate the data from the target language to English and then obtain an answer using an English reading comprehension model. Finally, they recover the corresponding answer in"
D19-1169,D19-1600,1,0.855968,"when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target language. We first translate target language training data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the &lt;passage, question, answer&gt; in both languages, and fuse the representations of both to generate final predictions. Experimental results on two Chinese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art performances on both datasets, and even surpass human performance on some metrics. Also, we 1586 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1586–1595, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics conduct experiments on the Japanese and French SQuAD (Asai et al.,"
D19-1169,N19-1423,0,0.681076,"tween &lt;passage, question, answer&gt;. In this paper, we propose the Cross-Lingual Machine Reading Comprehension (CLMRC) task that aims to help reading comprehension in lowresource languages. First, we present several back-translation approaches when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target language. We first translate target language training data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the &lt;passage, question, answer&gt; in both languages, and fuse the representations of both to generate final predictions. Experimental results on two Chinese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art performances on both datasets, and even surpass human performance on some metrics. Also, we 1586 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the"
D19-1169,P17-1168,0,0.0185237,"emonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and massive progresses have been made in creating largescale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1 Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically generated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is far beyond the usable t"
D19-1169,P16-1086,0,0.0304847,"and DRCD. The results show consistent and significant improvements over various stateof-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and massive progresses have been made in creating largescale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1 Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale"
D19-1169,P17-1010,1,0.839775,". et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically generated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is far beyond the usable threshold. Another way is to exploit cross-lingual approaches to utilize the data in richresource language to implicitly learn the relations between &lt;passage, question, answer&gt;. In this paper, we propose the Cross-Lingual Machine Reading Comprehension (CLMRC) task that aims to help reading comprehension in lowresource languages. First, we present several back-translation approaches when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when ther"
D19-1169,D16-1264,0,0.760855,"a large margin, which demonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and massive progresses have been made in creating largescale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1 Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically generated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is"
D19-1169,L18-1431,1,0.946224,"ly available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target language. We first translate target language training data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the &lt;passage, question, answer&gt; in both languages, and fuse the representations of both to generate final predictions. Experimental results on two Chinese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art performances on both datasets, and even surpass human performance on some metrics. Also, we 1586 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1586–1595, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics conduct experiments on the Japanese and French SQuAD (Asai et al., 2018) and achieves substanti"
D19-1169,1983.tc-1.13,0,0.231539,"Missing"
D19-1214,P19-1544,0,0.389705,"Missing"
D19-1214,N18-2118,0,0.12366,"ge understanding (SLU) is a critical component in task-oriented dialogue systems. It usually consists of intent detection to identify users’ intents and slot filling task to extract semantic constituents from the natural language utterances (Tur and De Mori, 2011). As shown in Table 1, given a movie-related utterance “watch action movie”, there are different slot labels for each token and an intent for the whole utterance. Usually, intent detection and slot filling are implemented separately. But intuitively, these two tasks are not independent and the slots often highly depend on the intent (Goo et al., 2018). For example, if the intent of a utterance is * Email corresponding. action movie B-movie name I-movie name WatchMovie Table 1: An example with intent and slot annotation (BIO format), which indicates the slot of movie name from an utterance with an intent WatchMovie. Introduction ∗ watch O WatchMovie, it is more likely to contain the slot movie name rather than the slot music name. Hence, it is promising to incorporate the intent information to guide the slot filling. Considering this strong correlation between the two tasks, some joint models are proposed based on the multi-task learning fr"
D19-1214,H90-1021,0,0.583959,"ointly, the final joint objective is formulated as Lθ = L1 + L2 . (12) Through the joint loss function, the shared representations learned by the shared self-attentive encoder can consider two tasks jointly and further ease the error propagation compared with pipeline models (Zhang and Wang, 2016). 2081 4 Experiments 4.1 • Bi-Model. Wang et al. (2018) proposed the Bi-model to consider the intent and slot filling cross-impact to each other. Experimental Settings To evaluate the efficiency of our proposed model, we conduct experiments on two benchmark datasets. One is the publicly ATIS dataset (Hemphill et al., 1990) containing audio recordings of flight reservations, and the other is the custom-intent-engines collected by Snips (SNIPS dataset) (Coucke et al., 2018). 1 Both datasets used in our paper follows the same format and partition as in Goo et al. (2018). The dimensionalities of the word embedding is 256 for ATIS dataset and 512 for SNIPS dataset. The self-attentive encoder hidden units are set as 256. L2 regularization is used on our model is 1 × 10−6 and dropout ratio is adopted is 0.4 for reducing overfit. We use Adam (Kingma and Ba, 2014) to optimize the parameters in our model and adopted the"
D19-1214,N18-2050,0,0.136863,"lot filling task objection is defined as: nS m X   X ˆ ji,S log yji,S , L2 , − y (11) j=1 i=1 ˆ ji,I y ˆ ji,S are the gold intent label and where and y gold slot label separately; nS is the number of slot labels. To obtain both slot filling and intent detection jointly, the final joint objective is formulated as Lθ = L1 + L2 . (12) Through the joint loss function, the shared representations learned by the shared self-attentive encoder can consider two tasks jointly and further ease the error propagation compared with pipeline models (Zhang and Wang, 2016). 2081 4 Experiments 4.1 • Bi-Model. Wang et al. (2018) proposed the Bi-model to consider the intent and slot filling cross-impact to each other. Experimental Settings To evaluate the efficiency of our proposed model, we conduct experiments on two benchmark datasets. One is the publicly ATIS dataset (Hemphill et al., 1990) containing audio recordings of flight reservations, and the other is the custom-intent-engines collected by Snips (SNIPS dataset) (Coucke et al., 2018). 1 Both datasets used in our paper follows the same format and partition as in Goo et al. (2018). The dimensionalities of the word embedding is 256 for ATIS dataset and 512 for S"
D19-1214,P18-1053,1,0.836806,"e matrix of input vectors X ∈ RT ×d (d represents the mapped dimension) to queries (Q), keys (K) and values (V) matrices by using different linear projections and the self-attention output C ∈ RT ×d is a weighted sum of values:   QK> √ C = softmax V. (1) dk Self-Attentive Encoder In our Stack-Propagation framework, intent detection task and slot filling task share one encoder, In the self-attentive encoder, we use BiLSTM with self-attention mechanism to leverage both advantages of temporal features and contextual information, which are useful for sequence labeling tasks (Zhong et al., 2018; Yin et al., 2018). The BiLSTM (Hochreiter and Schmidhuber, 1997) reads input utterance X = (x1 , x2 , .., xT ) (T is the number of tokens in the input utterance) (2) where E ∈ RT ×2d and ⊕ is concatenation operation. 3.2 Token-Level Intent Detection Decoder In our framework, we perform a token-level intent detection, which can provide token-level intent features for slot filling, different from regarding the intent detection task as the sentence-level classification problem (Liu and Lane, 2016). The token-level intent detection method can be formalized as a sequence labeling problem that maps a input word sequ"
D19-1214,P19-1519,0,0.125076,"twork (RNN) architecture. • Attention BiRNN. Liu and Lane (2016) leveraged the attention mechanism to allow the network to learn the relationship between slot and intent. • Slot-Gated Atten. Goo et al. (2018) proposed the slot-gated joint model to explore the correlation of slot filling and intent detection better. • Self-Attentive Model. Li et al. (2018) proposed a novel self-attentive model with the intent augmented gate mechanism to utilize the semantic correlation between slot and intent. 1 https://github.com/snipsco/ nlu-benchmark/tree/master/ 2017-06-custom-intent-engines • CAPSULE-NLU. Zhang et al. (2019) proposed a capsule-based neural network model with a dynamic routing-by-agreement schema to accomplish slot filling and intent detection. • SF-ID Network. (E et al., 2019) introduced an SF-ID network to establish direct connections for the slot filling and intent detection to help them promote each other mutually. For the Joint Seq, Attention BiRNN, Slot-gated Atten, CAPSULE-NLU and SF-ID Network, we adopt the reported results from Goo et al. (2018); Zhang et al. (2019); E et al. (2019). For the SelfAttentive Model, Bi-Model, we re-implemented the models and obtained the results on the same d"
D19-1214,P16-1147,0,0.0376135,"Joint Conference on Natural Language Processing, pages 2078–2087, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics intent semantic knowledge to guide the slot filling and make our joint model more interpretable. Task B Task A Task B differentiable link Task A Encoder (a) Multi-task framework • We perform the token-level intent detection for Stack-Propagation framework, which improves the intent detection performance and further alleviate the error propagation. Encoder (b) Stack-propagation Figure 1: Multi-task framework vs. Stack-Propagation. proposed by Zhang and Weiss (2016) to leverage the POS tagging features for parsing and achieved good performance, we propose a joint model with Stack-Propagation for SLU tasks. Our framework directly use the output of the intent detection as the input for slot filling to better guide the slot prediction process. In addition, the framework make it easy to design oracle intent experiment to intuitively show how intent information enhances slot filling task. For the second issue, we perform a token-level intent prediction in our framework, which can provide the token-level intent information for slot filling. If some token-level"
D19-1214,P18-1135,0,0.0468615,"l intent information for slot filling by concatenating the output of intent detection decoder and the representations from encoder as the input for slot filling decoder. Both intent detection and slot filling are optimized simultaneously via a joint learning scheme. 3.1 forwardly and backwardly to produce contextsensitive hidden states H = (h1 , h2 , ..., hT ) by repeatedly applying the recurrence hi = BiLSTM φemb (xi ) , hi−1 . Self-attention is a very effective method of leveraging context-aware features over variablelength sequences for natural language processing tasks (Tan et al., 2018; Zhong et al., 2018). In our case, we use self-attention mechanism to capture the contextual information for each token. In this paper, we adopt Vaswani et al. (2017), where we first map the matrix of input vectors X ∈ RT ×d (d represents the mapped dimension) to queries (Q), keys (K) and values (V) matrices by using different linear projections and the self-attention output C ∈ RT ×d is a weighted sum of values:   QK> √ C = softmax V. (1) dk Self-Attentive Encoder In our Stack-Propagation framework, intent detection task and slot filling task share one encoder, In the self-attentive encoder, we use BiLSTM with"
D19-1270,P08-1090,0,0.021658,"er effectively learning event background information to guide the If-Then reasoning. Experimental results show that our approach improves the accuracy and diversity of inferences compared with state-of-the-art baseline methods. 1 Figure 1: A illustration of two challenging problems in IfThen reasoning. (a) Given an observed event, the feelings about this event could be multiple. (b) Background knowledge is need for generating reasonable inferences, which is absent in the dataset (marked by dashed lines). Introduction Recently, event-centered commonsense knowledge has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018), because of understanding events is an important component of NLP. Given a daily-life event, human can easily understand it and reason about its causes, effects, and so on. However, it still remains a challenging task for NLP systems. This is partly due to most of them are trained for task-specific datasets or objectives, which results in models that are adapt at finding task-specific underlying correlation patterns but have limited capability in simple and explainable commonsense reasoning (Sap et al., 2018). ∗ Corresponding author To"
D19-1270,P18-1082,0,0.0213906,"ilt upon three humanwritten story corpora: ROCStories (Mostafazadeh 2686 Context 1 jason had been really stressed out at work. 2 he decided he needed a different kind of job. 3 jason applied for a job in a different field. Event Inference Target 4 he got the job . 5 jason was much happier at his new job . Table 3: An example for the construction of auxiliary dataset. For a five-sentence-paragraph, the first three sentences are taken as event context, while the fourth and fifth sentence is taken as base event and target respectively. et al., 2016), VIST (Huang et al., 2016) and WritingPrompts (Fan et al., 2018). ROCStories and VIST are composed of short stories with five sentences. We filter out stories of more than 1,000 words in WritingPrompts, and cut the remaining stories into five-sentence-paragraphs. For each five-sentence-paragraph, we define the first three sentences as contexts of the base event, the fourth sentence as the base event, and the fifth sentence as the inference target. For example, as shown in Table 3, the first three sentences describe a context that Jason was unsatisfied about his job and applied for a new job. Hence, after happening the event “he got the job”, a plausible re"
D19-1270,Q16-1038,0,0.0264144,". While the semantics of generations using baseline RNN-based Seq2Seq model is relatively limited. Furthermore, the first three kinds of semantic overlap the three ground truth targets, and the fourth kind of semantic is in accordance with daily-life commonsense. Compared to RNNbased Seq2Seq model, our approach can increase the diversity and rationality of generations, meanwhile keep the accuracy. 5 5.1 Related Work Event-Centered Commonsense Reasoning Understanding events and constructing eventcentered commonsense knowledge are crucial to many NLP applications, such as intention recognition (Goldwasser and Zhang, 2016) and dialog generation (Wen et al., 2017). Recently a growing number of studies focus on event-centered commonsense reasoning, which mainly concentrates on two areas, script event prediction and story ending generation/choosing. Script event prediction concerns with the temporal relationships between script events (Granroth-Wilding and Clark, 2016), which requires models to choose a correct subsequent triple-organized event among the candidates (Wang et al., 2017). Prior work mainly focused on modeling event pairs (Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017) and event g"
D19-1270,N16-1014,0,0.208321,"d participant characteristic about events. Together with these datasets, a simple RNN-based encoder-decoder framework is proposed to conduct the If-Then reasoning. However, there still remains two challenging problems. First, as illustrated in Figure 1, given an event “PersonX finds a new job”, the plausible feeling of PersonX about that event could be multiple (such as “needy/stressed out” and “relieved/joyful”). Previous work showed that for the one-to-many problem, conventional RNN-based encoder-decoder models tend to generate generic responses, rather than meaningful and specific answers (Li et al., 2016; Serban et al., 2016). Second, as a commonsense reasoning problem, rich background knowledge is necessary for generating reasonable inferences. For example, as shown in Figure 1, the feeling of PersonX upon the event “PersonX finds a new job” could be multiple. However, after given a context “PersonX was fired”, the plausible inferences would be narrowed down to “needy” or “stressed out”. To better solve these problems, we propose a context-aware variational autoencoder (CWVAE) together with a two-stage training procedure. Vari2682 Proceedings of the 2019 Conference on Empirical Methods in Na"
D19-1270,D16-1230,0,0.030612,"Missing"
D19-1270,N16-1098,0,0.0231851,"which mainly concentrates on two areas, script event prediction and story ending generation/choosing. Script event prediction concerns with the temporal relationships between script events (Granroth-Wilding and Clark, 2016), which requires models to choose a correct subsequent triple-organized event among the candidates (Wang et al., 2017). Prior work mainly focused on modeling event pairs (Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017) and event graph (Li et al., 2018) to predict the subsequent event. Story ending generation focuses on generating plausible story endings (Mostafazadeh et al., 2016), which requires models to understand the story context, and keep generated endings logically consistent with it (Peng et al., 2017; Guan et al., 2019). The above tasks mainly investigate the logical orders of events, whereas the IfThen reasoning task focuses on inferring the mental state of event participants. 5.2 Variational AutoEncoder-Decoder Based Natural Language Generation VAE (Kingma and Welling, 2014) has been widely applied in various of text generation tasks, such as dialogue and machine translation. In dialogue generation, Zhao et al. (2017) adapts VAE with encoder-decoder framewor"
D19-1270,P02-1040,0,0.103561,"NMT 0.0002 0.0002 0.0003 CWVAE-Unpretrained 0.0023 0.0017 0.0004 CWVAE 0.0052 0.0033 0.0025 RNN-based Seq2Seq 0.0005 0.0002 0.0002 Variational Seq2Seq 0.0014 0.0002 0.0001 dist-2 VRNMT 0.0005 0.0003 0.0001 CWVAE-Unpretrained 0.0061 0.0040 0.0013 CWVAE 0.0146 0.0099 0.0063 Table 5: Distinct-1 and distinct-2 scores for the top 10 generations under each inference dimension of Event2Mind. The the best result for each dimension is emboldened. able for evaluating the model performance on oneto-many problem (Serban et al., 2017). Further, we employ BLEU score to evaluate the accuracy of generations (Papineni et al., 2002), and the number of distinct n-gram to evaluate the diversity of generations (Li et al., 2016). The distinct is normalized to [0, 1] by dividing the total number of generated tokens. Human Evaluation Since automatic evaluation of generations is still a challenging task (Liu et al., 2016), we also conduct human evaluations on the model performance. Five human experts are employed to evaluate the coherence, diversity and fluency of generated targets. Experts are asked to vote for if a generation is fluent or coherent for each generated target, and give a 1-5 score for the diversity of generation"
D19-1270,D16-1244,0,0.103279,"Missing"
D19-1270,K17-1019,0,0.0243612,"the temporal relationships between script events (Granroth-Wilding and Clark, 2016), which requires models to choose a correct subsequent triple-organized event among the candidates (Wang et al., 2017). Prior work mainly focused on modeling event pairs (Granroth-Wilding and Clark, 2016), event chains (Wang et al., 2017) and event graph (Li et al., 2018) to predict the subsequent event. Story ending generation focuses on generating plausible story endings (Mostafazadeh et al., 2016), which requires models to understand the story context, and keep generated endings logically consistent with it (Peng et al., 2017; Guan et al., 2019). The above tasks mainly investigate the logical orders of events, whereas the IfThen reasoning task focuses on inferring the mental state of event participants. 5.2 Variational AutoEncoder-Decoder Based Natural Language Generation VAE (Kingma and Welling, 2014) has been widely applied in various of text generation tasks, such as dialogue and machine translation. In dialogue generation, Zhao et al. (2017) adapts VAE with encoder-decoder framework to model the latent semantic distribution of answers, which can increase the diversity of generations. For the task of machine tr"
D19-1270,P18-1043,0,0.0223687,"016; Wang et al., 2017; Li et al., 2018), because of understanding events is an important component of NLP. Given a daily-life event, human can easily understand it and reason about its causes, effects, and so on. However, it still remains a challenging task for NLP systems. This is partly due to most of them are trained for task-specific datasets or objectives, which results in models that are adapt at finding task-specific underlying correlation patterns but have limited capability in simple and explainable commonsense reasoning (Sap et al., 2018). ∗ Corresponding author To facilitate this, Rashkin et al. (2018) build the Event2Mind dataset and Sap et al. (2018) present the Atomic dataset, mainly focus on nine If-Then reasoning types to describe causes, effects, intents and participant characteristic about events. Together with these datasets, a simple RNN-based encoder-decoder framework is proposed to conduct the If-Then reasoning. However, there still remains two challenging problems. First, as illustrated in Figure 1, given an event “PersonX finds a new job”, the plausible feeling of PersonX about that event could be multiple (such as “needy/stressed out” and “relieved/joyful”). Previous work show"
D19-1270,L16-1233,0,0.070386,"Missing"
D19-1270,D17-1006,0,0.0981273,"on to guide the If-Then reasoning. Experimental results show that our approach improves the accuracy and diversity of inferences compared with state-of-the-art baseline methods. 1 Figure 1: A illustration of two challenging problems in IfThen reasoning. (a) Given an observed event, the feelings about this event could be multiple. (b) Background knowledge is need for generating reasonable inferences, which is absent in the dataset (marked by dashed lines). Introduction Recently, event-centered commonsense knowledge has attracted much attention (Chambers and Jurafsky, 2008; Segers et al., 2016; Wang et al., 2017; Li et al., 2018), because of understanding events is an important component of NLP. Given a daily-life event, human can easily understand it and reason about its causes, effects, and so on. However, it still remains a challenging task for NLP systems. This is partly due to most of them are trained for task-specific datasets or objectives, which results in models that are adapt at finding task-specific underlying correlation patterns but have limited capability in simple and explainable commonsense reasoning (Sap et al., 2018). ∗ Corresponding author To facilitate this, Rashkin et al. (2018)"
D19-1270,D16-1050,0,0.024155,"tasks mainly investigate the logical orders of events, whereas the IfThen reasoning task focuses on inferring the mental state of event participants. 5.2 Variational AutoEncoder-Decoder Based Natural Language Generation VAE (Kingma and Welling, 2014) has been widely applied in various of text generation tasks, such as dialogue and machine translation. In dialogue generation, Zhao et al. (2017) adapts VAE with encoder-decoder framework to model the latent semantic distribution of answers, which can increase the diversity of generations. For the task of machine translation, Su et al. (2018) and Zhang et al. (2016) employ a latent variable to capture the semantic interaction between the source and target sentence, and regard the latent variable as a supplementation of attention mechanism. While Wang et al. (2019) use the latent variable to model topic distributions in text generation. In this paper, we introduce an additional context-aware latent variable to effectively learn background knowledge and conduct If-Then reasoning on the guidance of it. 6 Conclusion In this paper, we propose a novel context-aware VAE (CWVAE) framework with two training stages for If-Then commonsense reasoning. By introducing"
D19-1270,P17-1061,0,0.105361,"o “needy” or “stressed out”. To better solve these problems, we propose a context-aware variational autoencoder (CWVAE) together with a two-stage training procedure. Vari2682 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2682–2691, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ational Autoencoder (VAE) based models have shown great potential in modeling the one-tomany problem and generate diversified inferences (Bowman et al., 2015; Zhao et al., 2017). In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc"
D19-1310,N18-2097,0,0.0869617,"representation of the row which may reflect the row (player or team)’s overall performance. C denotes the number of columns. rowi = M eanP ooling(˜ ri,1 , r˜i,2 , ..., r˜i,C ) (8) Then, we adopt content selection gate gi , which is proposed by Puduppully et al. (2019) on rows’ representations rowi , and obtain a new representation row ˜ i = gi rowi to choose more important information based on each row’s context. 3.4 Decoder with Dual Attention Since record encoders with record fusion gate provide record-level representation and row-level encoder provides row-level representation. Inspired by Cohan et al. (2018), we can modify 4.1 Experiments Dataset and Evaluation Metrics We conducted experiments on ROTOWIRE (Wiseman et al., 2017). For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary. The average length of game summary is 337.1. In this paper, we followed the data split introduced in Wiseman et al. (2017): 3398 examples in training set, 727 examples in development set and 728 examples in test set. We followed Wiseman et al. (2017)’s work and use BLEU (Papineni et al., 2002) and three extractive evaluation metrics RG, C"
D19-1310,P16-1014,0,0.207025,"son scored 18 points. Several related models have been proposed . They typically encode the table’s records separately or as a long sequence and generate a long descriptive summary 3143 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3143–3152, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics by a standard Seq2Seq decoder with some modifications. Wiseman et al. (2017) explored two types of copy mechanism and found conditional copy model (Gulcehre et al., 2016) perform better . Puduppully et al. (2019) enhanced content selection ability by explicitly selecting and planning relevant records. Li and Wan (2018) improved the precision of describing data records in the generated texts by generating a template at first and filling in slots via copy mechanism. Nie et al. (2018) utilized results from pre-executed operations to improve the fidelity of generated texts. However, we claim that their encoding of tables as sets of records or a long sequence is not suitable. Because (1) the table consists of multiple players and different types of information as s"
D19-1310,N18-2098,0,0.0418078,"on” in this match as “double-double” which requires ability to capture dependency from different columns (different type of record) in the same row (player). In addition, it models “Al Jefferson” history performance and correctly states that “It was his second double-double over his last three games”, which is also mentioned in gold texts included in Figure 1 in a similar way. 5 Related Work In recent years, neural data-to-text systems make remarkable progress on generating texts directly from data. Mei et al. (2016) proposes an encoderaligner-decoder model to generate weather forecast, while Jain et al. (2018) propose a mixed hierarchical attention. Sha et al. (2018) proposes a hybrid content- and linkage-based attention mechanism to model the order of content. Liu et al. (2018) propose to integrate field information into table representation and enhance decoder with dual attention. Bao et al. (2018) develops a table-aware encoder-decoder model. Wiseman et al. (2017) introduced a document-scale data-totext dataset, consisting of long text with more redundant records, which requires the model to select important information to generate. We describe recent works in Section 1. Also, some studies in ab"
D19-1310,C18-1089,0,0.132127,"Missing"
D19-1310,W17-4505,0,0.0124538,"ose to integrate field information into table representation and enhance decoder with dual attention. Bao et al. (2018) develops a table-aware encoder-decoder model. Wiseman et al. (2017) introduced a document-scale data-totext dataset, consisting of long text with more redundant records, which requires the model to select important information to generate. We describe recent works in Section 1. Also, some studies in abstractive text summarization encode long texts in a hierarchical manner. Cohan et al. (2018) uses a hierarchical encoder to encode input, paired with a discourse-aware decoder. Ling and Rush (2017) encode document hierarchically and propose coarse-to-fine attention for decoder. Recently, Liu et al. (2019) propose a hierarchical encoder for data-to-text generation which uses LSTM as its cell. Murakami et al. (2017) propose to model stock market time-series data and generate comments. As for incorporating historical background in generation, Robin (1994) proposed to build a draft with essential new facts at first, then incorporate background facts when revising the draft based on functional unification grammars. Different from that, we encode the historical (time dimension) information in"
D19-1310,Q18-1005,0,0.0298412,"xts. We describe model’s details in following parts. 3.1 3.1.1 Layer 1: Record Encoders Row Dimension Encoder Based on our observation, when someone’s points is mentioned in texts, some related records such as “field goals made” (FGM) and “field goals attempted” (FGA) will also be included in texts. Taken gold texts in Figure 1 as example, when Al Jefferson’s point 18 is mentioned, his FGM 9 and FGA 19 are also mentioned. Thus, when modeling a record, other records in the same row can be useful. Since the record in the row is not sequential, we use a self-attention network which is similar to Liu and Lapata (2018) to model records in the conrow be text of other records in the same row. Let ri,j the row dimension representation of the record of ith row and j th column. Then, we obtain the context vector in row dimension crow i,j by attending to other records in the same row as follows. Please row ∝ exp(r T W r 0 ) is normalnote that αi,j,j 0 o i,j i,j ized across records in the same row i. Wo is a trainable parameter. X row = αi,j,j (2) crow 0 ri,j 0 i,j j 0 ,j 0 6=j Then, we combine record’s representation with ci,j and obtain the row dimension record reprerow = tanh(W [r ; crow ]). W is sentation ri,j"
D19-1310,D15-1166,0,0.161286,"rcution In this paper, we construct timelines tl = {tle,c }E,C e=1,c=1 for records. E denotes the number of distinct record entities and C denotes the number of record types. For each timeline tle,c , we first extract records with the same entity e and type c from dataset. Then we sort them into a sequence according to the record’s date from old to new. This sequence is considered as timeline tle,c . For example, in Figure 2, the “Timeline” part in the lower-left corner represents a timeline for entity Al Jefferson and type PTS (points). 2.3 Baseline Model We use Seq2Seq model with attention (Luong et al., 2015) and conditional copy (Gulcehre et al., 2016) as the base model. During training, given tables S and their corresponding reference texts y, the model Q maximized the conditional probability P (y|S) = Tt=1 P (yt |y<t , S) . t is the timestep of decoder. First, for each record of the ith row and j th column in the table, we utilize 1-layer MLP to encode the embeddings of each record’s four types of information into a dense vector ri,j , ri,j = ReLU (Wa [ri,j .e; ri,j .c; ri,j .v; ri,j .f ] + ba ). Wa and ba are trainable parameters. The word embeddings for each type of information are trainable"
D19-1310,D18-1422,0,0.0374875,"history windows size as 3 from {3,5,7} based on the results. Code of our model can be found at https://github.com/ernestgong/data2textthree-dimensions/. 4.3 4.3.1 Results Automatic Evaluation Table 1 displays the automatic evaluation results on both development and test set. We chose Conditional Copy (CC) model as our baseline, which is the best model in Wiseman et al. (2017). We included reported scores with updated IE model by Puduppully et al. (2019) and our implementation’s result on CC in this paper. Also, we compared our models with other existing works on this dataset including OpATT (Nie et al., 2018) and Neural Content Planning with conditional copy (NCP+CC) (Puduppully et al., 2019). In addition, we implemented three other hierarchical encoders that encoded tables’ row dimension information in both record-level and row-level to compare with the hierarchical structure of encoder in our model. The decoder was equipped with dual attention (Cohan et al., 2018). The one with LSTM cell is similar to the one in Cohan et al. (2018) with 1 layer from {1,2,3}. The one with CNN cell 3148 (Gehring et al., 2017) has kernel width 3 from {3, 5} and 10 layer from {5,10,15,20}. The one with transformer-s"
D19-1310,P02-1040,0,0.103869,"r provides row-level representation. Inspired by Cohan et al. (2018), we can modify 4.1 Experiments Dataset and Evaluation Metrics We conducted experiments on ROTOWIRE (Wiseman et al., 2017). For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary. The average length of game summary is 337.1. In this paper, we followed the data split introduced in Wiseman et al. (2017): 3398 examples in training set, 727 examples in development set and 728 examples in test set. We followed Wiseman et al. (2017)’s work and use BLEU (Papineni et al., 2002) and three extractive evaluation metrics RG, CS and CO (Wiseman et al., 2017) for evaluation. The main idea of the extractive evaluation metrics is to use an Information Extraction (IE) model to identify records mentioned in texts. Then compare them with tables or records extracted from reference to evaluate the model. RG (Relation Generation) measures content fidelity of 3147 Model RG Gold Template CC (Wiseman et al., 2017) NCP+CC (Puduppully et al., 2019) Hierarchical LSTM Encoder Hierarchical CNN Encoder Hierarchical SA Encoder Hierarchical MHSA Encoder CC (Our implementation) Our Model -ro"
D19-1310,N16-1086,0,0.034777,"ta (column) between each rows (players). Also it correctly summarize performance of “Al Jefferson” in this match as “double-double” which requires ability to capture dependency from different columns (different type of record) in the same row (player). In addition, it models “Al Jefferson” history performance and correctly states that “It was his second double-double over his last three games”, which is also mentioned in gold texts included in Figure 1 in a similar way. 5 Related Work In recent years, neural data-to-text systems make remarkable progress on generating texts directly from data. Mei et al. (2016) proposes an encoderaligner-decoder model to generate weather forecast, while Jain et al. (2018) propose a mixed hierarchical attention. Sha et al. (2018) proposes a hybrid content- and linkage-based attention mechanism to model the order of content. Liu et al. (2018) propose to integrate field information into table representation and enhance decoder with dual attention. Bao et al. (2018) develops a table-aware encoder-decoder model. Wiseman et al. (2017) introduced a document-scale data-totext dataset, consisting of long text with more redundant records, which requires the model to select im"
D19-1495,P08-1090,0,0.393135,"Missing"
D19-1495,D14-1148,1,0.92176,"Missing"
D19-1495,C18-1088,1,0.842308,"lark, 2016; Modi, 2016), which passes the concatenation or addition of event arguments’ word embeddings to a parameterized function. The function maps the summed vectors into an event embedding space. Furthermore, Ding et al. (2015) and Weber et al. (2018) propose using neural tensor networks to perform semantic composition of event arguments, which can better capture the interactions between event arguments. Events are a kind of important objective information of the world. Structuralizing and representing such information as machine-readable knowledge are crucial to artificial intelligence (Li et al., 2018b, 2019). The main idea is to learn distributed representations for structured events (i.e. event embeddings) from text, and use them as the basis to induce textual features for downstream applications, such as script event prediction and stock market prediction. Parameterized additive models are among the most widely used for learning distributed event representations in prior work (Granroth-Wilding ∗ PersonX threw basketball Intent PersonZ attacked embassy Introduction Corresponding author The code and data are available on https://github.com/MagiaSN/CommonsenseERL EMNLP 2019. 1 PersonX thre"
D19-1495,C16-1201,1,0.689123,"eural Tensor Network This line of work use tensors to learn the interactions between the predicate and its subject/object (Ding et al., 2015; Weber et al., 2018). According to the different usage of tensors, we have three baseline methods: Role Factor Tensor (Weber et al., 2018) which represents the predicate as a tensor, Predicate Tensor (Weber et al., 2018) which uses two tensors learning the interactions between the predicate and its subject, and the predicate and its object, respectively, NTN (Ding et al., 2015), which we used as the baseline event embedding model in this paper, and KGEB (Ding et al., 2016), which incorporates knowledge graph information in NTN. Given a training event corpus with annotated intents and emotions, our model jointly minimizes a linear combination of the loss functions on events, intents and sentiment: L = αLE + βLI + γLS (8) where α, β, γ ∈ [0, 1] are model parameters to weight the three loss functions. We use the New York Times Gigaword Corpus (LDC2007T07) for pre-training event embeddings. Event triples are extracted based on the Open Information Extraction technology (Schmitz et al., 2012). We initialize the word embedding layer with 100 dimensional pre-trained G"
D19-1495,P81-1022,0,0.378881,"Missing"
D19-1495,K16-1008,0,0.116042,"izing the cross entropy error of the sentiment classification, whose loss function is given below. LS = − X X pgl (xe ) · log(pl (xe )) veloped in recent years. These models can be categorized into three groups: • Averaging Baseline (Avg) This represents each event as the average of the constituent word vectors using pre-trained GloVe embeddings (Pennington et al., 2014). • Compositional Neural Network (Comp. NN) The event representation in this model is computed by feeding the concatenation of the subject, predicate, and object embedding into a two layer neural network (Modi and Titov, 2013; Modi, 2016; Granroth-Wilding and Clark, 2016). (7) xe ∈C l∈L where C means all training instances, L is the collection of sentiment categories, xe means an event vector, pl (xe ) is the probability of predicting xe as class l, pgl (xe ) indicates whether class l is the correct sentiment category, whose value is 1 or -1. 2.4 • Element-wise Multiplicative Composition (EM Comp.) This method simply concatenates the element-wise multiplications between the verb and its subject/object. Joint Event, Intent and Sentiment Embedding • Neural Tensor Network This line of work use tensors to learn the interactions b"
D19-1495,D14-1162,0,0.09208,"nt. Simple additive transformations are incompetent. We follow Ding et al. (2015) modelling such informative interactions through tensor composition. The architecture of neural tensor network (NTN) for learning event embeddings is shown in Figure 3, where the bilinear tensors are used to explicitly model the relationship between the actor and the action, and that between the object and the action. The inputs of NTN are the word embeddings of A, P and O, and the outputs are event embeddings. We initialized our word representations using publicly available d-dimensional (d = 100) GloVe vectors (Pennington et al., 2014). As most event arguments consist of several words, we represent the actor, action and object as the average of their word embeddings, respectively. From Figure 3, S1 ∈ Rd is computed by:  S1 = f T A [1:k] T1 P  +W A P   +b (1) T g(S1 ) = g(A, P ) = U S1 [1:k] where T1 ∈ Rd×d×k is a tensor, which is a set of k matrices, each with d × d dimensions. [1:k] The bilinear tensor product AT T1 P is a vector r ∈ Rk , where each entry is computed by one [i] slice of the tensor (ri = AT T1 P, i = 1, · · · , k). The other parameters are a standard feed-forward neural network, where W ∈ Rk×2d is the w"
D19-1495,N18-1202,0,0.0220097,"set (ATOMIC), which consists of 25,000 event phrases covering a diverse range of daily-life events and situations. For example, given an event “PersonX drinks coffee in the morning”, the dataset labels PersonX’s likely intent is “PersonX wants to stay awake”. We notice that the intents labeled in ATOMIC is a sentence. Hence, intent embedding is actually a sentence representation learning task. Among various neural networks for encoding sentences, bi-directional LSTMs (BiLSTM) (Hochreiter and Schmidhuber, 1997) have been a dominant method, giving state-of-the-art results in language modelling (Peters et al., 2018) and syntactic parsing (Dozat and Manning, 2016). We use BiLSTM model to learn intent representations. BiLSTM consists of two LSTM components, which process the input in the forward leftto-right and the backward right-to-left directions, respectively. In each direction, the reading of input words is modelled as a recurrent process with a single hidden state. Given an initial value, the state changes its value recurrently, each time consuming an incoming word. Take the forward LSTM component for exam→ − ple. Denoting the initial state as h 0 , which is a model parameter, it reads the input word"
D19-1495,P18-1043,0,0.0309135,"y used to explain why the actor performed the action. For example, given two events “PersonX threw basketball” and “PersonX threw bomb”, there are only subtle differences in their surface realizations, however, the intents are totally different. “PersonX threw basketball” is just for fun, while “PersonX threw bomb” could be a terrorist attack. With the intents, we can easily distinguish these superficial similar events. One challenge for incorporating intents into event embeddings is that we should have a largescale labeled dataset, which annotated the event and its actor’s intents. Recently, Rashkin et al. (2018) and Sap et al. (2019) released such valuable commonsense knowledge dataset (ATOMIC), which consists of 25,000 event phrases covering a diverse range of daily-life events and situations. For example, given an event “PersonX drinks coffee in the morning”, the dataset labels PersonX’s likely intent is “PersonX wants to stay awake”. We notice that the intents labeled in ATOMIC is a sentence. Hence, intent embedding is actually a sentence representation learning task. Among various neural networks for encoding sentences, bi-directional LSTMs (BiLSTM) (Hochreiter and Schmidhuber, 1997) have been a"
D19-1495,D12-1048,0,0.0188303,"which we used as the baseline event embedding model in this paper, and KGEB (Ding et al., 2016), which incorporates knowledge graph information in NTN. Given a training event corpus with annotated intents and emotions, our model jointly minimizes a linear combination of the loss functions on events, intents and sentiment: L = αLE + βLI + γLS (8) where α, β, γ ∈ [0, 1] are model parameters to weight the three loss functions. We use the New York Times Gigaword Corpus (LDC2007T07) for pre-training event embeddings. Event triples are extracted based on the Open Information Extraction technology (Schmitz et al., 2012). We initialize the word embedding layer with 100 dimensional pre-trained GloVe vectors (Pennington et al., 2014), and fine-tune initialized word vectors during our model training. We use Adagrad (Duchi et al., 2011) for optimizing the parameters with initial learning rate 0.001 and batch size 128. 3 Experiments We compare the performance of intent and sentiment powered event embedding model with stateof-the-art baselines on three tasks: event similarity, script event prediction and stock prediction. 3.1 Baselines We compare the performance of our approach against a variety of event embedding"
D19-1495,D17-1006,0,0.239028,"Missing"
D19-1575,D18-1214,0,0.0435914,"Missing"
D19-1575,Q17-1010,0,0.0684081,"he transformed contextualized embeddings. We train our models using the Adam optimizer (Kingma and Ba, 2015), and most of the them converge within a few thousand epochs in several hours. More implementation details are reported in the Appendix. 4.2 Baseline Systems We compare our method with the following three baseline models: • mBERT (contextualized). Embeddings generated by the mBERT model are directly used in the training and testing procedures. • FT-SVD (Ahmad et al., 2018, off-line, static). SVD-based transformation (Smith et al., 2017) is applied on 300-dimensional FastText embeddings (Bojanowski et al., 2017) to obtain cross-lingual static embeddings, which represents the previous state-ofthe-art. We report results from their paper of the RNNGraph model which used the same architecture as ours. • XLM (Lample and Conneau, 2019, on-line, contextualized). A strong method which learns contextualized cross-lingual embeddings from scratch with cross-lingual data. 6 We also investigated non-linear transformation in our experiments, but didn’t observe any improvements. 7 We found the orthogonal constraint doesn’t help for GD. 8 We tried alternative strategies such as averaging, using the middle or right-m"
D19-1575,K18-2005,1,0.806607,"ethod, where contextualized embeddings of the word canal from Spanish is transformed to the semantic space of English. Introduction Email corresponding Our code is released at https://github.com/ WangYuxuan93/CLBT 2 In this paper, we refer to these embeddings as static as opposed to contextualized ones. 1 He loves the movie Devlin et al., 2018) have demonstrated dramatic superiority over traditional static word embeddings, establishing new state-of-the-arts in various monolingual NLP tasks (Ili´c et al., 2018; Schuster et al., 2018). The success has also been recognized in dependency parsing (Che et al., 2018). The great potential of these contextualized embeddings has inspired us to extend its power to crosslingual scenarios. Several recent works have been proposed to learn contextualized cross-lingual embeddings by training cross-lingual language models from scratch with parallel data as supervision, and has been demonstrated effective in several downstream tasks (Schuster et al., 2018; Mulcaire et al., 2019; Lample and Conneau, 2019). These methods are typically resource-demanding and timeconsuming.3 In this paper, we propose CrossLingual BERT Transformation (CLBT), a simple and efficient off-li"
D19-1575,D18-1024,0,0.0180757,"rces and less training time than XLM, yet achieving highly competitive results. 2 Related Work Static cross-lingual embedding learning methods can be roughly categorized as on-line and off-line methods. Typically, on-line approaches integrate monolingual and cross-lingual objectives to learn cross-lingual word embeddings in a joint manner (Klementiev et al., 2012; Koˇcisk´y et al., 2014; Guo et al., 2016), while off-line approaches take pretrained monolingual word embeddings of different languages as input and retrofit them into a shared semantic space (Xing et al., 2015; Lample et al., 2018; Chen and Cardie, 2018). Several approaches have been proposed recently to connect the rich expressiveness of contextualized word embeddings with cross-lingual transfer. Mulcaire et al. (2019) based their model on ELMo (Peters et al., 2018) and proposed a polyglot contextual representation model by capturing character-level information from multilingual data. Lample and Conneau (2019) adapted the objectives of BERT (Devlin et al., 2018) to incorporate cross-lingual supervision from parallel data to learn cross-lingual language models (XLMs), which have obtained state-of-the-art results on several cross-lingual tasks"
D19-1575,P10-4002,0,0.026285,"cally converges in several hours. To validate the effectiveness of our approach in cross-lingual dependency parsing, we first obtain the CLBT embeddings with the proposed approach, and then use them as input to a modern graph-based neural parser (described in next section), in replacement of the pre-trained static embeddings. Note that BERT produces embeddings in wordpiece-level, so we only use the left-most wordpiece embedding of each word.8 4 4.1 Data and Settings In our experiments, the contextual word pairs are obtained from the Europarl corpora (Koehn, 2005) using the fast align toolkit (Dyer et al., 2010). Only 10,000 sentence pairs are used for each target language. For the parsing datasets, we use the Universal Dependencies(UD) Treebanks (v2.2) (Nivre et al., 2018),9 following the settings of the previous state-of-the-art system (Ahmad et al., 2018). From the 31 languages they have analyzed, we select 18 whose Europarl data is publicly available.10 Statistics of the selected languages and treebanks can be found in the Appendix. We employ the Biaffine Graph-based Parser of Dozat and Manning (2017) and adopt their hyper-parameters for all of our models. In all the experiments, English is used"
D19-1575,S01-1001,0,0.0248893,"d by XLM ranges from 0.2 million (10 million tokens) for Bulgarian to 13.1 million (682 million tokens) for French. In comparison, only 10,000 parallel sentences (0.4 million tokens) are used for each language in CLBT, demonstrating the data-efficiency of our approach. Moreover, given the efficiency in both data and training, CLBT can be readily scaled to new language pairs in hours. 4.5 Analysis 4.5.1 Transformation of Cross-lingual BERT Embedding In order to investigate the properties of contextualized representations before and after the linear transformation, we employ the SENSEVAL2 data (Edmonds and Cotton, 2001),13 where words from different languages are tagged by their word senses in different contexts. We took contextualized representations of the English word nature and its Spanish transla5724 13 www.hipposmond.com/senseval2/ 80 La corrupción , un fenómeno de naturaleza esencialmente política , se ha convertido también en. (Corruption, a phenomenon of an essentially political nature, has also become a spectacle.) 75 70 65 WWF is respected worldwide for its knowledge of wildlife and nature conservation. 60 55 50 Perhaps it's due to the hightech nature of today's shoes, particularly the midsole. 45"
D19-1575,P15-1119,1,0.945046,"ot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results. 1 1 El canal está marcado por boyas (The channel is marked by buoys) One of the most promising directions for crosslingual dependency parsing, which also remains a challenge, is to bridge the gap of lexical features. Prior works (Xiao and Guo, 2014; Guo et al., 2015) have shown that cross-lingual word embeddings are able to significantly improve the transfer performance compared to delexicalized models (McDonald et al., 2011, 2013). These crosslingual word embeddings are static in the sense that they do not change with the context.2 Recently, contextualized word embeddings derived from large-scale pre-trained language models (McCann et al., 2017; Peters et al., 2017, 2018; ∗ channel WX Y Y The ship went aground in the channel Figure 1: A toy illustration of the method, where contextualized embeddings of the word canal from Spanish is transformed to the se"
D19-1575,W18-6202,0,0.0551155,"Missing"
D19-1575,C12-1089,0,0.0908096,"Missing"
D19-1575,P14-2037,0,0.0417422,"Missing"
D19-1575,2005.mtsummit-papers.11,0,0.0286546,"ach can be trained on a single GPU and typically converges in several hours. To validate the effectiveness of our approach in cross-lingual dependency parsing, we first obtain the CLBT embeddings with the proposed approach, and then use them as input to a modern graph-based neural parser (described in next section), in replacement of the pre-trained static embeddings. Note that BERT produces embeddings in wordpiece-level, so we only use the left-most wordpiece embedding of each word.8 4 4.1 Data and Settings In our experiments, the contextual word pairs are obtained from the Europarl corpora (Koehn, 2005) using the fast align toolkit (Dyer et al., 2010). Only 10,000 sentence pairs are used for each target language. For the parsing datasets, we use the Universal Dependencies(UD) Treebanks (v2.2) (Nivre et al., 2018),9 following the settings of the previous state-of-the-art system (Ahmad et al., 2018). From the 31 languages they have analyzed, we select 18 whose Europarl data is publicly available.10 Statistics of the selected languages and treebanks can be found in the Appendix. We employ the Biaffine Graph-based Parser of Dozat and Manning (2017) and adopt their hyper-parameters for all of our"
D19-1575,P15-1027,0,0.0160464,"ings as an anchor for each word type, and learn a transformation in the anchor space. Our approach, however, learns this transformation directly in the contextual space, and hence is explicitly designed to be word sense-preserving. 3 Cross-Lingual BERT Transformation This section describes our proposed approach, namely CLBT, to transform pre-trained monolingual contextualized embeddings to a shared semantic space. 3.1 Contextual Word Alignment Traditional methods of learning static crosslingual word embeddings have been relying on various sources of supervision such as bilingual dictionaries (Lazaridou et al., 2015; Smith et al., 2017), parallel corpus (Guo et al., 2015) or online Google Translate (Mikolov et al., 2013; Xing et al., 2015). To learn contextualized cross-lingual word embeddings, however, we require supervision at word token-level (or context-level) rather than type-level (i.e. dictionaries). Therefore, we assume a parallel corpus as our supervision, analogous to on-line methods such as XLM (Lample and Conneau, 2019). In our approach, unsupervised bidirectional word alignment is first applied to the parallel corpus to obtain a set of aligned word pairs with their contexts, or contextual wo"
D19-1575,D11-1006,0,0.114886,"e further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results. 1 1 El canal está marcado por boyas (The channel is marked by buoys) One of the most promising directions for crosslingual dependency parsing, which also remains a challenge, is to bridge the gap of lexical features. Prior works (Xiao and Guo, 2014; Guo et al., 2015) have shown that cross-lingual word embeddings are able to significantly improve the transfer performance compared to delexicalized models (McDonald et al., 2011, 2013). These crosslingual word embeddings are static in the sense that they do not change with the context.2 Recently, contextualized word embeddings derived from large-scale pre-trained language models (McCann et al., 2017; Peters et al., 2017, 2018; ∗ channel WX Y Y The ship went aground in the channel Figure 1: A toy illustration of the method, where contextualized embeddings of the word canal from Spanish is transformed to the semantic space of English. Introduction Email corresponding Our code is released at https://github.com/ WangYuxuan93/CLBT 2 In this paper, we refer to these embedd"
D19-1575,N19-1392,0,0.106687,"tatic word embeddings, establishing new state-of-the-arts in various monolingual NLP tasks (Ili´c et al., 2018; Schuster et al., 2018). The success has also been recognized in dependency parsing (Che et al., 2018). The great potential of these contextualized embeddings has inspired us to extend its power to crosslingual scenarios. Several recent works have been proposed to learn contextualized cross-lingual embeddings by training cross-lingual language models from scratch with parallel data as supervision, and has been demonstrated effective in several downstream tasks (Schuster et al., 2018; Mulcaire et al., 2019; Lample and Conneau, 2019). These methods are typically resource-demanding and timeconsuming.3 In this paper, we propose CrossLingual BERT Transformation (CLBT), a simple and efficient off-line approach that learns a linear transformation from contextual word alignments. With CLBT, contextualized embeddings 3 For instance, XLM was trained on 64 Volta GPUs (Lample and Conneau, 2019). While the time of training is not described in the paper, we may take the statistics from BERT as a reference, e.g., BERTBASE was trained on 4 Cloud TPUs for 4 days (Devlin et al., 2018). 5721 Proceedings of the 2"
D19-1575,P17-1161,0,0.032285,"ked by buoys) One of the most promising directions for crosslingual dependency parsing, which also remains a challenge, is to bridge the gap of lexical features. Prior works (Xiao and Guo, 2014; Guo et al., 2015) have shown that cross-lingual word embeddings are able to significantly improve the transfer performance compared to delexicalized models (McDonald et al., 2011, 2013). These crosslingual word embeddings are static in the sense that they do not change with the context.2 Recently, contextualized word embeddings derived from large-scale pre-trained language models (McCann et al., 2017; Peters et al., 2017, 2018; ∗ channel WX Y Y The ship went aground in the channel Figure 1: A toy illustration of the method, where contextualized embeddings of the word canal from Spanish is transformed to the semantic space of English. Introduction Email corresponding Our code is released at https://github.com/ WangYuxuan93/CLBT 2 In this paper, we refer to these embeddings as static as opposed to contextualized ones. 1 He loves the movie Devlin et al., 2018) have demonstrated dramatic superiority over traditional static word embeddings, establishing new state-of-the-arts in various monolingual NLP tasks (Ili´c"
D19-1575,N18-1202,0,0.0600481,"ne approaches integrate monolingual and cross-lingual objectives to learn cross-lingual word embeddings in a joint manner (Klementiev et al., 2012; Koˇcisk´y et al., 2014; Guo et al., 2016), while off-line approaches take pretrained monolingual word embeddings of different languages as input and retrofit them into a shared semantic space (Xing et al., 2015; Lample et al., 2018; Chen and Cardie, 2018). Several approaches have been proposed recently to connect the rich expressiveness of contextualized word embeddings with cross-lingual transfer. Mulcaire et al. (2019) based their model on ELMo (Peters et al., 2018) and proposed a polyglot contextual representation model by capturing character-level information from multilingual data. Lample and Conneau (2019) adapted the objectives of BERT (Devlin et al., 2018) to incorporate cross-lingual supervision from parallel data to learn cross-lingual language models (XLMs), which have obtained state-of-the-art results on several cross-lingual tasks. Similar to our approach, Schuster et al. (2019) also aligned pretrained contextualized word embeddings through linear transformation in an off-line fashion. They used the averaged contextualized embeddings as an anc"
D19-1575,P19-1493,0,0.078096,"Missing"
D19-1575,N19-1162,0,0.0486122,"een proposed recently to connect the rich expressiveness of contextualized word embeddings with cross-lingual transfer. Mulcaire et al. (2019) based their model on ELMo (Peters et al., 2018) and proposed a polyglot contextual representation model by capturing character-level information from multilingual data. Lample and Conneau (2019) adapted the objectives of BERT (Devlin et al., 2018) to incorporate cross-lingual supervision from parallel data to learn cross-lingual language models (XLMs), which have obtained state-of-the-art results on several cross-lingual tasks. Similar to our approach, Schuster et al. (2019) also aligned pretrained contextualized word embeddings through linear transformation in an off-line fashion. They used the averaged contextualized embeddings as an anchor for each word type, and learn a transformation in the anchor space. Our approach, however, learns this transformation directly in the contextual space, and hence is explicitly designed to be word sense-preserving. 3 Cross-Lingual BERT Transformation This section describes our proposed approach, namely CLBT, to transform pre-trained monolingual contextualized embeddings to a shared semantic space. 3.1 Contextual Word Alignmen"
D19-1575,D19-1077,0,0.0837086,"Missing"
D19-1575,W14-1613,0,0.0249298,"approach on zero-shot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results. 1 1 El canal está marcado por boyas (The channel is marked by buoys) One of the most promising directions for crosslingual dependency parsing, which also remains a challenge, is to bridge the gap of lexical features. Prior works (Xiao and Guo, 2014; Guo et al., 2015) have shown that cross-lingual word embeddings are able to significantly improve the transfer performance compared to delexicalized models (McDonald et al., 2011, 2013). These crosslingual word embeddings are static in the sense that they do not change with the context.2 Recently, contextualized word embeddings derived from large-scale pre-trained language models (McCann et al., 2017; Peters et al., 2017, 2018; ∗ channel WX Y Y The ship went aground in the channel Figure 1: A toy illustration of the method, where contextualized embeddings of the word canal from Spanish is tr"
D19-1575,N15-1104,0,0.0748489,"Missing"
D19-1600,C10-3004,1,0.797425,"Missing"
D19-1600,P17-1055,1,0.870124,"uch as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et"
D19-1600,L18-1431,1,0.831224,", 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose another dataset, which is gathered from children’s reading material. To add more diversity and for further investigation on transfer learning, they also provide another evaluation dataset, which is also annotated by human experts, but the query is more natural than the cloze type. The dataset was used in the first evaluation workshop on Chinese machine reading comprehension (CMRC 2017). In opendomain reading comprehension, He et al. (2017) propose a large-scale open-domain Chinese machine reading comprehension dataset (DuReader), which contains 200k queries annotated from the user que"
D19-1600,C16-1167,1,0.900263,"sted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the dataset could further accelerate the Chinese machine reading comprehension research.1 1 Introduction To read and comprehend natural languages is the key to achieve advanced artificial intelligence. Machine Reading Comprehension (MRC) aims to comprehend the context of given articles and answer the questions based on them. Various types of machine reading comprehension datasets have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017;"
D19-1600,N19-1423,0,0.0460362,"f three results as the final estimated human performance on this dataset. Estimated Human Performance We also report the estimated human performance in order to measure the difficulty of the proposed dataset. As we have illustrated in the previous section, there are three answers for each question in development, test, and challenge set. Unlike Rajpurkar et al. (2016), we use a cross-validation method to calculate the performance. We regard the first answer as human prediction and treat the rest of the answers as ground truths. In this way, 4 4.1 Experimental Results Baseline System Following Devlin et al. (2019), we adopt BERT for our baseline system. Specifically, we slightly modify the run squad.py script5 for adjusting our dataset, while keeping the most of the original implementation. For the baseline system, we used an initial learning rate of 3e-5 with a batch size of 32 and trained for two epochs. The maximum lengths of document and query are set to 512 and 64. 4.2 Results The results are shown in Table 2. Besides the baseline systems, we also include the participants’ results of CMRC 2018 evaluation. We release the training and development set to the public and accepted submissions from parti"
D19-1600,P17-1168,0,0.0420642,"reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose an"
D19-1600,P16-1086,0,0.0452388,"have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation"
D19-1600,D17-1082,0,0.0609544,"anguages is the key to achieve advanced artificial intelligence. Machine Reading Comprehension (MRC) aims to comprehend the context of given articles and answer the questions based on them. Various types of machine reading comprehension datasets have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chine"
D19-1600,P17-1010,1,0.86513,"Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose another dataset, which is gathered from children’s reading mat"
D19-1600,D16-1264,0,0.489181,"ding Comprehension (CMRC 2018). We hope the release of the dataset could further accelerate the Chinese machine reading comprehension research.1 1 Introduction To read and comprehend natural languages is the key to achieve advanced artificial intelligence. Machine Reading Comprehension (MRC) aims to comprehend the context of given articles and answer the questions based on them. Various types of machine reading comprehension datasets have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al"
D19-1600,D13-1020,0,0.138836,"and comprehend natural languages is the key to achieve advanced artificial intelligence. Machine Reading Comprehension (MRC) aims to comprehend the context of given articles and answer the questions based on them. Various types of machine reading comprehension datasets have been proposed, such as cloze-style reading comprehension (Hermann et al., 2015; Hill et al., 2015; Cui et al., 2016), span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (201"
D19-1600,P18-1158,0,0.0248253,"ion (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose another dataset, which is gathered from children’s reading material. To add more diversity and for further investigat"
D19-1600,P17-1018,0,0.0311655,", span-extraction reading comprehension (Rajpurkar et al., 2016; Trischler et al., 2016), open-domain reading comprehension (Nguyen et al., 2016; He et al., 2017), reading comprehension with multiple-choice (Richardson et al., 2013; Lai et al., 2017), etc. Along with 1 Resources are available: https://github.com/ ymcui/cmrc2018. the development of the reading comprehension dataset, various neural network approaches have been proposed and made a big advancement in this area (Kadlec et al., 2016; Cui et al., 2017; Dhingra et al., 2017; Wang and Jiang, 2016; Xiong et al., 2016; Liu et al., 2017; Wang et al., 2017; Hu et al., 2018; Wang et al., 2018; Yu et al., 2018). We also have seen various efforts on the construction of Chinese machine reading comprehension datasets. In cloze-style reading comprehension, Cui et al. (2016) proposed a Chinese cloze-style reading comprehension dataset: People’s Daily & Children’s Fairy Tale. To add difficulties to the dataset, along with the automatically generated evaluation sets (development and test), they also release a human-annotated evaluation set. Later, Cui et al. (2018) propose another dataset, which is gathered from children’s reading material. To add more"
D19-3017,C10-3004,1,0.807134,"Missing"
D19-3017,tran-2017-applying,0,0.0259797,"picted in Figure 1. As this paper focuses on legal context processing, we will provide details for principal modules related to legal services and omit the others like chit-chat. For better understanding, we will discuss experiments, user studies and use cases in the following sections. Related Work The current automated legal consultation applications usually rely on retrieving relevant text information from pre-constructed database containing legal question-answer pairs using text features such as TF-IDF and bag-of-words (BoW). Do et al. (2017) proposed QA models for legal consultation, and Hang (2017) preformed legal question classification with deep convolutional neural network trained in multi-task manner. Law searching is an unneglectable demand of legal workers such as lawyers and procurators, as they need to support their views with sufficient articles. In industry, article retrieval applications simply parse inputs into phrases and adopt common IR approaches. Academically, Zhang et al. (2017) built a Chinese legal consultation sys3.1 Consultation Block This block is responsible for legal QA. It contains four modules, one of which is called intention recognition and natural language u"
D19-3017,C18-1041,0,0.121433,"specially for Introduction The term Legal Tech refers to legal technologies that apply computer technologies to legal services, such as legal consultation and judicial document analysis. Such techniques are able to ease the load of legal workers and provide easily accessible services for clients. Recently, researchers are concentrating on enhancing Legal Tech with NLP techniques (e.g., named entity recognition (Yin et al., 2018), sequence labeling (Yan et al., 2018)). Studies have proven that NLP techniques are markedly effective regarding several legal tasks, for instance, charge prediction (Hu et al., 2018) and law area classification (Sulea et al., 2017). In industry, the majority of legal consultation products are merely platforms that redirect users to actual lawyers other than solving problems with a 97 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 97–102 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics hƐĞƌ dĞǆƚ ĂŶĂůǇƐŝƐ ZĞũĞĐƚŝŽŶŝŶĨŽƌŵĂƚŝŽŶ ĐŚĂƚƚŝŶŐŵŽĚƵůĞ ƌŝŵŝŶĂůĐŝǀŝů ĐůĂƐƐŝĨŝĐĂƚŝŽŶ ^ĞŵĂŶƚŝĐ ĂŶĂůǇƐŝƐ /ŶƚĞŶƚŝŽŶ ƌĞĐŽŐŶŝƚŝŽŶ dĞǆƚ ƵŶĚĞƌƐƚĂŶĚŝŶŐ YƵĞƌǇ ƌĞǁƌŝƚŝŶŐ dŽƉŝĐ ĐůĂƐƐŝĨŝĐĂƚŝŽŶ ZĞŐƵůĂƌ ĞǆƉƌĞƐƐŝŽŶ E>h ƌ"
E12-1030,P11-1113,0,0.283624,"Missing"
E12-1030,P08-1030,0,0.423182,"Missing"
E12-1030,P10-1081,0,0.295444,"Missing"
E12-1030,W02-1028,0,0.146322,"Missing"
E12-1030,A00-1039,0,0.0605015,"(Lin, 1998), that recognizes dependency relations between words is quite sufficient for deriving head-modifier relations and thus for construction of event templates. Event templates are obtained by stripping the parse tree of modifiers while preserving the basic dependency structure as shown in Figure 1, which is a stripped down parse tree of, “Also Monday, Israeli soldiers fired on four diplomatic vehicles in the northern Gaza town of Beit Hanoun, said diplomats” The model proposed here represents a significant advance over the current methods for relation extraction, such as the SVO model (Yangarber, et al. 2000) and its extension, e.g., the chain model (Sudo, et al. 2001) and other related variants (Riloff, 1996) all of which lack the expressive power to accurately recognize and represent complex event descriptions and to support successful machine learning. While Sudo’s subtree model (2003) overcomes some of the limitations of the chain models and is thus conceptually closer to our method, it nonetheless lacks efficiency required for practical applications. We represent complex relations as tree-like structures anchored at an event trigger (which is usually but not necessarily the main verb) with br"
E12-1030,H01-1009,0,0.766344,"quite sufficient for deriving head-modifier relations and thus for construction of event templates. Event templates are obtained by stripping the parse tree of modifiers while preserving the basic dependency structure as shown in Figure 1, which is a stripped down parse tree of, “Also Monday, Israeli soldiers fired on four diplomatic vehicles in the northern Gaza town of Beit Hanoun, said diplomats” The model proposed here represents a significant advance over the current methods for relation extraction, such as the SVO model (Yangarber, et al. 2000) and its extension, e.g., the chain model (Sudo, et al. 2001) and other related variants (Riloff, 1996) all of which lack the expressive power to accurately recognize and represent complex event descriptions and to support successful machine learning. While Sudo’s subtree model (2003) overcomes some of the limitations of the chain models and is thus conceptually closer to our method, it nonetheless lacks efficiency required for practical applications. We represent complex relations as tree-like structures anchored at an event trigger (which is usually but not necessarily the main verb) with branches extending to the event attributes (which are usually n"
E12-1030,P03-1029,0,0.652116,"Missing"
E12-1030,C96-2157,1,0.608192,"will occur in a variety of linguistic contexts, and some of these contexts may provide support for creating alternative extraction rules. When the new rules are subsequently applied to the text corpus, additional instances of the target concepts will be identified, some of which will be positive and some not. As this process continues to iterate over, the system acquires more extraction rules, fanning out from the seed set until no new rules can be learned. Thus defined, bootstrapping has been used in natural language processing research, notably in word sense disambiguation (Yarowsky, 1995). Strzalkowski and Wang (1996) were first to demonstrate that the technique could be applied to adaptive learning of named entity extraction 296 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 296–305, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics rules. For example, given a “naïve” rule for identifying company names in text, e.g., “capitalized NP followed by Co.”, their system would first find a large number of (mostly) positive instances of company names, such as “Henry Kauffman Co.” From the context surrounding eac"
E12-1030,M91-1028,0,\N,Missing
E12-1030,P95-1026,0,\N,Missing
E12-1030,P07-1074,0,\N,Missing
E12-1030,H92-1045,0,\N,Missing
E12-1030,M98-1011,0,\N,Missing
E14-1062,P08-1085,0,0.0501682,"Missing"
E14-1062,W06-1615,0,0.066482,"omain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et al., 2007). The baseline self-training approach (Liu and Zhang, 2012) belongs to this category. The second considers the differences in the two domains in terms of features (Blitzer et al., 2006; Daume III, 2007), classifying features into domain-independent source domain and target domain groups and training these types consistently. The third considers differences between the distributions of instances in the two domains, treating them differently (Jiang and Zhai, 2007). Our type-supervised method is closer to the second category. However, rather than splitting features into domain-independent and domain-specific types, we use domain-specific dictionaries to capture domain differences, and train a model on the source domain only. Our method can be treated as an approach specific to"
E14-1062,P07-1094,0,0.0159784,"Missing"
E14-1062,D10-1056,0,0.0427273,"Missing"
E14-1062,P12-1110,0,0.0349042,"Missing"
E14-1062,P04-1015,0,0.0111374,"the partial results from the beam to generate new partial results, using two types of actions: (1) Append, which appends ci to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds ci as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domai"
E14-1062,P11-1061,0,0.0132514,"from ours in several aspects: (1) they focus on in-domain POS-tagging, while our concern is cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods wi"
E14-1062,P09-1058,0,0.0311136,"omain data to find improved target domain accuracies over bare CTB training. are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (200"
E14-1062,P07-1033,0,0.154058,"Missing"
E14-1062,C12-2073,1,0.643353,"ually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-dependent versions of features using domain-specific data. Our method tunes a set of lexicon-based features, so that domain-dependent models are derived from ins"
E14-1062,I05-3025,0,0.271209,"Missing"
E14-1062,D12-1075,0,0.0619082,"Missing"
E14-1062,N13-1014,0,0.0106972,"ctical situation. In particular, we split Chinese words into domain-independent and domain-specific categories, and define unlexicalized features for domain-specific words. We train lexicalized domain-independent and unlexicalized domainspecific features using the source domain annotated sentences and a source-domain lexicon, and then apply the resulting model to the target domain by replacing the source-domain lexicon with a target domain lexicon. Combined with unsupervised learning with unlabeled target-domain of sentences, the conceptually simple method worked highly effectively. Following Garrette and Baldridge (2013), we address practical questions 590 Action Lexicon Feature templates Separate in-lex(w−1 ), l(w−1 ) ◦ in-lex(w−1 ), in-lex(w−1 , t−1 ), l(w−1 ) ◦ in-lex(w−1 , t−1 ) ing words those that occur more than 3 times for words specific to the source domain. We assume that the domain-independent lexicon applies to all target domains also. For some target domains, we can obtain domain-specific terminologies easily from the Internet. However, this can be a very small portion depending on the domain. Thus, it may still be necessary to obtain new lexicons by manual annotation. Table 1: Dictionary feature"
E14-1062,J94-2001,0,0.0592843,"Missing"
E14-1062,W04-3236,0,0.0174807,"Missing"
E14-1062,P13-1057,0,0.109698,"cate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually annotating target domain sentences, given similar efforts? Our investigations are performed under two Chines"
E14-1062,C04-1081,0,0.596785,"Missing"
E14-1062,P13-1076,0,0.0190385,"cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et"
E14-1062,petrov-etal-2012-universal,0,0.0207072,"Missing"
E14-1062,P09-1057,0,0.0155387,"s replaced with a raw tag dictionary gleaned from data, without any human intervention. These facts indicate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually anno"
E14-1062,P08-1101,1,0.450623,"Missing"
E14-1062,D10-1017,0,0.0102399,"ver, their efforts differ from ours in several aspects: (1) they focus on in-domain POS-tagging, while our concern is cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervi"
E14-1062,D10-1082,1,0.765811,"ed target domain accuracies over bare CTB training. are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-d"
E14-1062,P11-1139,0,0.023051,"fy CTB words into domainindependent and domain-specific categories. Consisting of semantic information for nearly 100,000 common Chinese words, HowNet can serve as a resource of domain-independent Chinese words. We choose out of all words in the source domain training data those that also occur in HowNet for domain-independent words, and out of the remain4 4.1 Experiments Setting We use annotated sentences from the CTB5 for source-domain training, splitting the corpus into training, development and test sections in the same way as previous work (Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Following Liu and Zhang (2012), we use the free Internet novel “Zhuxian” (henceforth referred to as ZX; also known as “Jade dynasty”) as our target domain data. The writing style of the novel is in the literature genre, with the style of Ming and Qing novels, very different from news in CTB. Ex591 CTB sentences ZX sentences 乔石会见俄罗斯议员团 天下之大，无奇不有，山川灵秀，亦多妖魔鬼怪。 (Qiaoshi meets the Russian delegates.) (The world was big. It held everything. There were fascinating 李鹏强调要加快推行公务员制度 landscapes. There were haunting ghosts.) (Lipeng stressed on speeding the reform of official regulations.) 时间无多，我去请出诛仙古剑。"
E14-1062,J11-1005,1,0.489487,"the beam to generate new partial results, using two types of actions: (1) Append, which appends ci to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds ci as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domain sentences. The trainin"
E14-1062,Q13-1001,0,0.0296522,"Missing"
E14-1062,I11-1035,0,0.0274135,"Missing"
I05-2023,A00-1011,0,0.0173885,"tities. As a kernel function, the Improved-Edit-Distance (IED) is used to calculate the similarity between two Chinese strings. By employing the Voted Perceptron and Support Vector Machine (SVM) kernel machines with the IED kernel as the classifiers, we tested the method by extracting person-affiliation relation from Chinese texts. By comparing with traditional feature-based learning methods, we conclude that our method needs less manual efforts in feature transformation and achieves a better performance. At the beginning, a number of manually engineered systems were developed for RE problem (Aone and Ramos-Santacruz, 2000). The automatic learning methods (Miller et al., 1998; Soderland, 1999) are not necessary to have someone on hand with detailed knowledge of how the RE system works, or how to write rules for it. Usually, the machine learning method represents the NLP objects as feature vectors in the feature extraction step. The methods are named feature-based learning methods. But in many cases, data cannot be easily represented explicitly via feature vectors. For example, in most NLP problems, the feature-based representations produce inherently local representations of objects, for it is computationally in"
I05-2023,1993.tmi-1.4,0,0.0777918,"= 2, the object for the pair 郭 士纳 (people) and IBM公司 (organization) can be written as “接 见 了 ORG 主 席 PEO 。” Through the objects transformed from the original texts, we can calculate the similarity between any two objects by using the kernel (similarity) function. For the Chinese relation extraction problem, we must consider the semantic similarity between words and the structure of strings while computing similarity. Therefore we must consider the kernel function which has a good similarity measure. The methods for computing the similarity between two strings are: the same-word based method (Nirenburg et al., 1993), the thesaurus based method (Qin et al., 2003), the Edit-Distance method (Ristad and Yianilos, 1998) and the statistical method (Chatterjee, 2001). We know that the same-word based method cannot solve the problem of synonyms. The thesaurus based method can overcome this difficulty but does not consider the structure of the text. Although the EditDistance method uses the structure of the text, it also has the same problem of the replacement of synonyms. As for the statistical method, it needs large corpora of similarity text and thus is difficult to use for realistic applications. For the reas"
I05-3028,O03-5002,0,0.0218739,"ve this problem, we first train from the training corpus the probability of a word to be one word and the probability to be two separate words. Then we perform a process of merging: if two adjoining words in the best word sequence are more likely to be one word, then we just merge them together. Named Entity Recognition Then we will recognize the named entities such as persons and locations. First, we select N3 best paths from the segment graph with Dijkstra algorithm. Then for every path of the N+1 paths4 (N best paths and the atom path), we perform a process of Roles Tagging with HMM model (Zhang et al. 2003). The process of it is much like that of Part of Speech Tagging. Then with the best role sequence of every path, we can find out all the named entities and add them to the segment graph as usual. Take the sentence “ᓴ Ӯ吣ᰃϾདᄺ⫳Ǆ” for example. After basic segmentation and factoid recognition, the N+1 paths are as follows: ᓴ/Ӯ/吣/ᰃ/Ͼ/ད/ᄺ⫳/Ǆ ᓴ/Ӯ/吣/ᰃ/Ͼ/ད/ᄺ/⫳/Ǆ 2.5 Then for each path, the process of Roles Tagging is performed and the following role sequences are generated: Morphologically Derived Word Recognition To deal with the words with the postfix like “ᗻ”, “㗙”, “⥛” and so on, we perform the proce"
I05-3028,W03-1721,0,0.0588549,"Missing"
I05-3028,W00-1207,0,\N,Missing
I05-3028,W03-1727,0,\N,Missing
I05-3028,W03-1730,0,\N,Missing
I05-5007,W04-3219,0,0.126464,"Missing"
I05-5007,W03-1604,0,0.0610289,"Missing"
I05-5007,W03-1610,0,0.0337717,"Missing"
I05-5007,P03-1016,0,0.0311076,"Missing"
I05-5007,W03-1605,0,0.0361287,"Missing"
I05-5007,P01-1008,0,0.159123,"Missing"
I05-5007,N03-1003,0,0.194012,"Missing"
I05-5007,W03-1609,0,0.0334474,"Missing"
I05-5007,C02-1056,0,0.0556653,"Missing"
I08-2109,P06-2010,1,0.900752,"hods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significant"
I08-2109,P02-1031,0,0.0363188,"verbs or nouns and some constituents of the sentence. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure"
I08-2109,P04-1043,0,0.078076,"ce. In previous work, data-driven techniques, including feature-based and kernel-based learning methods, have been extensively studied for SRL (Carreras and M`arquez, 2005). Although feature-based methods are regarded as the state-of-the-art methods and achieve much success in SRL, kernel-based methods are more effective in capturing structured features than featurebased methods. In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate"
I08-2109,N06-2025,0,0.0686209,"= opt([d]) = 1, p = 3. Then according to Eq (14), ∆p (cn1 , cn2 ) can be calculated recursively as Eq. (15) (Please refer to the next page). Finally, we have ∆p (cn1 , cn2 ) = λ1 × ∆0 (a, a) × 0 ∆ (b, b) × ∆0 (c, c) By means of the above algorithm, we can compute the ∆0 (n1 , n2 ) in O(p|cn1 |· |cn2 |2 ) (Lodhi et al., 2002). This means that the worst case complexity of the FGTK-I is O(pρ3 |N1 |· |N2 |2 ), where ρ is the maximum branching factor of the two trees. 3.2 Fast Grammar-driven Convolution Tree Kernel II (FGTK-II) Our FGTK-II algorithm is motivated by the partial trees (PTs) kernel (Moschitti, 2006). The PT kernel algorithm uses the following recursive formulas to evaluate ∆p (cn1 , cn2 ): |cn1 ||cn2 | ∆p (cn1 , cn2 ) = X X ∆0p (cn1 [1 : i], cn2 [1 : j]) (16) i=1 j=1 where cn1 [1 : i] and cn2 [1 : j] are the child subsequences of cn1 and cn2 from 1 to i and from 1 to j, respectively. Given two child node sequences s1 a = cn1 [1 : i] and s2 b = cn2 [1 : j] (a and b are the last children), the PT kernel computes ∆0p (·, ·) as follows:  0 ∆p (s1 a, s2 b) = µ2 ∆0 (a, b)Dp (|s1 |, |s2 |) 0 if a = b else (17) where ∆0 (a, b) is defined in Eq. (7) and Dp is recursively defined as follows: Dp ("
I08-2109,W05-0639,0,0.0418752,"Missing"
I08-2109,P07-1026,1,0.892395,"in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). Moschitti (2004) ∗ The work was mainly done when the author was a visiting student at I2 R 781 and Che et al. (2006) are two reported work to use convolution tree kernel (TK) methods (Collins and Duffy, 2001) for SRL and has shown promising results. However, as a general learning algorithm, the TK only carries out hard matching between two subtrees without considering any linguistic knowledge in kernel design. To solve the above issue, Zhang et al. (2007) proposed a grammar-driven convolution tree kernel (GTK) for SRL. The GTK can utilize more grammatical structure features via two grammar-driven approximate matching mechanisms over substructures and nodes. Experimental results show that the GTK significantly outperforms the TK (Zhang et al., 2007). Theoretically, the GTK method is applicable to any problem that uses syntax structure features and can be solved by the TK methods, such as parsing, relation extraction, and so on. In this paper, we use SRL as an application to test our proposed algorithms. Although the GTK shows promising results"
I08-2109,W05-0620,0,\N,Missing
I11-1030,P03-2031,0,0.167886,"and English is also more complex because of the tremendous difference between the two languages. Huang and Vogel (2002) presented an integrated approach to extract an NE translation dictionary from an English-Chinese parallel corpus while improving the monolingual NE annotation quality for both languages. They started with low-quality NE tagging for both languages and improved the annotation result using alignment information. But they did not filter the annotated data and evaluate its impact for NER as training data. Besides, some other resources have been used to generate NE tagged corpus. An et al. (2003) and Whitelaw et al. (2008) used seed sets of entities and search engines to collect NER training data from the web. However, constructing of a high-quality seed list is also a time-consuming work. Richman and Schone (2008) and Nothman et al. (2008) used similar methods to create NE training data. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into common entity types. But the article classification seeds also had to be hand-labeled in advance. In the biomedical domain, Vlachos and Gasperin (2006) automatically created training material for"
I11-1030,C02-1025,0,0.0550395,"Missing"
I11-1030,W03-0425,0,0.116048,"Missing"
I11-1030,W03-0428,0,0.0929965,"Missing"
I11-1030,zhang-etal-2004-interpreting,0,0.0186626,"Missing"
I11-1030,U08-1016,0,0.0560751,"oving the monolingual NE annotation quality for both languages. They started with low-quality NE tagging for both languages and improved the annotation result using alignment information. But they did not filter the annotated data and evaluate its impact for NER as training data. Besides, some other resources have been used to generate NE tagged corpus. An et al. (2003) and Whitelaw et al. (2008) used seed sets of entities and search engines to collect NER training data from the web. However, constructing of a high-quality seed list is also a time-consuming work. Richman and Schone (2008) and Nothman et al. (2008) used similar methods to create NE training data. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into common entity types. But the article classification seeds also had to be hand-labeled in advance. In the biomedical domain, Vlachos and Gasperin (2006) automatically created training material for the task of gene name recognition from the broader raw corpus using existing domain resources. In our work, we generate a large scale Chinese NER training data from a bilingual corpus without any NE seed lists and filter it by using effective strate"
I11-1030,P02-1060,0,0.0864602,"Missing"
I11-1030,W04-1221,0,0.0185845,"Missing"
I11-1030,W02-2029,0,0.0155673,"Missing"
I11-1030,W06-3328,0,0.0143561,"resources have been used to generate NE tagged corpus. An et al. (2003) and Whitelaw et al. (2008) used seed sets of entities and search engines to collect NER training data from the web. However, constructing of a high-quality seed list is also a time-consuming work. Richman and Schone (2008) and Nothman et al. (2008) used similar methods to create NE training data. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into common entity types. But the article classification seeds also had to be hand-labeled in advance. In the biomedical domain, Vlachos and Gasperin (2006) automatically created training material for the task of gene name recognition from the broader raw corpus using existing domain resources. In our work, we generate a large scale Chinese NER training data from a bilingual corpus without any NE seed lists and filter it by using effective strategies. And we prove that it can improve the performance of Chinese NER as additional training data. 3 Our Approach In this section we describe our approach of generating NER training data from a parallel corpus. The framework of our system consists of four components as shown in Figure 1.  Alignment: Sent"
I11-1030,I08-5007,0,0.0502059,"Missing"
I11-1030,H01-1035,0,0.0327142,"ty training data, which are very effective and important as the experiments show. And third, we prove that our generated training data can be used as an additional corpus to improve the NER performance. This paper is organized as follows. Section 2 discusses the related work. Section 3 describes our approach in detail. Section 4 presents and discusses the results of our experiments. Finally, we present our conclusions and future work in section 5. 2 Related Work In this section, we introduce some previous work about NER training data generation. The most closed related work to our approach is Yarowsky et al. (2001). They used word alignment on parallel corpora to induce several text analysis tools from English to other languages for which such resources are scarce. An NE tagger was transferred from English to French and achieved good classification accuracy. However, Chinese NER is more difficult than French and word alignment between Chinese and English is also more complex because of the tremendous difference between the two languages. Huang and Vogel (2002) presented an integrated approach to extract an NE translation dictionary from an English-Chinese parallel corpus while improving the monolingual"
I11-1030,J03-3002,0,\N,Missing
I11-1030,P08-1001,0,\N,Missing
I11-1030,C10-3004,1,\N,Missing
I11-1030,W10-2906,0,\N,Missing
I11-1030,N03-1017,0,\N,Missing
I11-1030,ma-2006-champollion,0,\N,Missing
I11-1030,W04-1200,0,\N,Missing
I11-1030,P00-1056,0,\N,Missing
I11-1030,C96-1079,0,\N,Missing
I11-1090,N03-1017,0,0.0601274,"Missing"
I11-1090,D09-1040,0,0.596584,"This problem becomes more serious for higher-order n-grams, and for morphologically richer languages. To overcome the coverage problem of SMT, besides the efforts of mining larger parallel corpora from various resources, some researchers have investigated to use paraphrasing approaches. The studies can be classified into two categories by the target of paraphrasing: (1) paraphrasing the input source sentences; (2) paraphrasing the training corpus. In the first category, the proposed approaches mainly focus on handling ngrams that are unknown to the SMT model. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite unknown terms with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) build paraphrase lattices for input sentences and select the best translations using a lattice-based SMT decoder. In the second category of paraphrasing training corpus, Bond et al. (2008) and Nakov (2008) paraphrase the source side of training corpus using hand-crafted rules. In this paper, we propose a method that enriches SMT tra"
I11-1090,P03-1021,0,0.0904712,"Missing"
I11-1090,P02-1040,0,0.0799961,"Missing"
I11-1090,P09-1094,1,0.947045,"ce on Natural Language Processing, pages 803–810, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Original training data e1 e2 … en Expanded training data f1 f2 … fn e1 e2 … en e 1’ e 2’ … en’ paraphrasing e 1’ e 2’ … en’ f1 f2 … fn f1 f 2’ … fn Figure 1. Sketch map of the paraphrasing based translation corpus expansion. tences of the bilingual parallel data, which are then paired with the target-side sentences to generate new parallel data. The procedure is illustrated in Figure 1. The SPG framework can be considered as an application-specific source-tosource translating procedure (Zhao et al. 2009) which is similar to phrase based statistical machine translation. We employ an object function, named Sentence Novelty, to select paraphrases that introduce the most novel information to the bilingual training corpus. In our approach, the context of paraphrasing substitution is considered during generating paraphrasing sentences, which yields paraphrases with higher precision. Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on"
I11-1090,P05-1074,0,0.177853,"Missing"
I11-1090,2008.iwslt-papers.2,0,0.277463,"the first category, the proposed approaches mainly focus on handling ngrams that are unknown to the SMT model. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite unknown terms with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) build paraphrase lattices for input sentences and select the best translations using a lattice-based SMT decoder. In the second category of paraphrasing training corpus, Bond et al. (2008) and Nakov (2008) paraphrase the source side of training corpus using hand-crafted rules. In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model. The method generates paraphrases for the source-side sen803 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 803–810, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Original training data e1 e2 … en Expanded training data f1 f2 … fn e1 e2 … en e 1’ e 2’ … en’ paraphrasing e 1’ e 2’ … en’ f1 f2 … fn f1 f 2’ … fn Figure 1. Sketch map"
I11-1090,N06-1003,0,0.527336,"select paraphrases that introduce the most novel information to the bilingual training corpus. In our approach, the context of paraphrasing substitution is considered during generating paraphrasing sentences, which yields paraphrases with higher precision. Experimental results show that the performance of a state-of-the-art phrase based SMT system (Moses in this work) can be improved from 17.91 to 19.57 in terms of BLEU on a small training set, and from 25.46 to 26.52 on a training corpus of medium size. Results also indicate that our method gains a significant improvement over the method of Callison-Burch et al. (2006). The rest of this paper is structured as follows. We review related work on improving SMT through paraphrasing in Section 2. The proposed statistical paraphrase generation model is described in Section 3. Section 4 presents our method of enlarging training data via paraphrasing. Section 5 and 6 present the experiments and results. We discuss our work in Section 7 and conclude the paper in Section 8. 2 Related Work Previous studies on improving SMT through paraphrasing input sentences mainly focus on finding translations for unknown terms using phrasal paraphrases. In these methods, an unknown"
I11-1090,D10-1041,0,0.229909,"hrasing approaches. The studies can be classified into two categories by the target of paraphrasing: (1) paraphrasing the input source sentences; (2) paraphrasing the training corpus. In the first category, the proposed approaches mainly focus on handling ngrams that are unknown to the SMT model. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite unknown terms with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) build paraphrase lattices for input sentences and select the best translations using a lattice-based SMT decoder. In the second category of paraphrasing training corpus, Bond et al. (2008) and Nakov (2008) paraphrase the source side of training corpus using hand-crafted rules. In this paper, we propose a method that enriches SMT training data using a statistical paraphrase generating (SPG) model. The method generates paraphrases for the source-side sen803 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 803–810, c Chiang Mai, Thailand, November 8 – 1"
I11-1090,P07-2045,0,\N,Missing
I11-1090,P09-1089,0,\N,Missing
I11-1090,P10-2001,0,\N,Missing
I11-1090,W04-3250,0,\N,Missing
I11-1104,2010.jeptalnrecital-court.36,0,0.0666853,"2 Question Generation Question generation is a branch of natural language generation, which is defined as the task of automatically generating questions from some form of input (Rus and Graesser, 2009). The input may vary from a deep semantic representation to a raw text. Previous studies on question generation have mostly focused on text-to-question generation, which generates questions from declarative sentences or paragraphs. This technique is useful in education, especially in reading tutoring. Most previous studies employed rule-based methods in their text-to-question generation systems (Ali et al., 2010; Kalady et al., 2010; Mannem et al., 2010; Pal et al., 2010; Piwek and Stoyanchev, 2010; Varga and Ha, 2010). Query-to-question generation is a sub-task of question generation, which was first proposed by Lin (2008). Lin has suggested to learn query-toquestion generation models with query logs. However, no detail method or evaluation has been presented. There has been no other research since, 935 either, which may be mainly because few researchers can access the query log data. tents. Paraphrase templates can be learned from monolingual corpora based on distributional hypothesis (Lin and Pant"
I11-1104,N03-1003,0,0.010863,"Piwek and Stoyanchev, 2010; Varga and Ha, 2010). Query-to-question generation is a sub-task of question generation, which was first proposed by Lin (2008). Lin has suggested to learn query-toquestion generation models with query logs. However, no detail method or evaluation has been presented. There has been no other research since, 935 either, which may be mainly because few researchers can access the query log data. tents. Paraphrase templates can be learned from monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001), from comparable news articles based on alignment (Barzilay and Lee, 2003), or from bilingual corpora based on pivot approaches (Zhao et al., 2008). The abovementioned studies are all related to our work. However, none of the previous work addresses the problem of query-to-question template generation. 4.3 Query Reformulation Query reformulation is an important topic in the IR community, since it can improve users’ search experience. Query reformulation mainly involves query reduction, expansion, and spelling correction. Query-to-question generation is closely related to query expansion. However, its goal is not only expanding useful information for the original que"
I11-1104,P02-1006,0,0.0158559,"Missing"
I11-1104,C08-1093,0,0.0196931,"uery sessions. The basic idea is that queries from the same session are more likely to be related to each other (Fonseca et al., 2005; Jones et al., 2006; Zhang and Nasraoui, 2006). The second kind of method identifies related queries using click-through information. They assume that queries leading to similar clicks are related in meaning (Wen et al., 2002; Baeza-Yates and Tiberi, 2007). The third category of method directly learns expansion terms from the clicked documents of the query. Their hypothesis is that terms in a query and a user-clicked document might be related (Cui et al., 2002; Riezler et al., 2008). 5 Conclusions and Future Work This paper addresses the problem of query-toquestion generation for cQA and proposes a method based on search engine query logs. Several conclusions can be drawn from the experimental results. First, search engine query logs are powerful data for the research of query-toquestion generation, from which we have acquired a large volume of question generation templates. Second, the proposed method is effective, which achieves promising precision and outperforms a baseline method. Third, the query-to-question generation technique can be used to improve the search of"
I11-1104,P08-1089,1,0.353017,"is a sub-task of question generation, which was first proposed by Lin (2008). Lin has suggested to learn query-toquestion generation models with query logs. However, no detail method or evaluation has been presented. There has been no other research since, 935 either, which may be mainly because few researchers can access the query log data. tents. Paraphrase templates can be learned from monolingual corpora based on distributional hypothesis (Lin and Pantel, 2001), from comparable news articles based on alignment (Barzilay and Lee, 2003), or from bilingual corpora based on pivot approaches (Zhao et al., 2008). The abovementioned studies are all related to our work. However, none of the previous work addresses the problem of query-to-question template generation. 4.3 Query Reformulation Query reformulation is an important topic in the IR community, since it can improve users’ search experience. Query reformulation mainly involves query reduction, expansion, and spelling correction. Query-to-question generation is closely related to query expansion. However, its goal is not only expanding useful information for the original query, but also organizing the information to produce a question, with which"
I11-1113,E06-1002,0,0.0456043,"mental settings, results and analysis are presented in 1010 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1010–1018, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Section 5. The last section offers some concluding remarks. 2 Related Work Linking name mentions to knowledge base entries has attracted more and more attentions in these years. As an open available resource, Wikipedia is a natural choice of knowledge source for its large scale and good quality. Early work mainly focused on the usage of the structure information in Wikipedia. Bunescu and Pasca (2006) trained a taxonomy kernel on Wikipedia data to disambiguate named entities in open domain. Cucerzan (2007) integrated Wikipedia’s category information in their vector space model for named entity disambiguation. Mihalcea and Csomai (2007) extracted sentences from Wikipedia, regarding the linking information as sense annotation, and used supervised machine learning models to train a classifier for disambiguation. Similarly, Milne and Witten (2008) adopted a learning approach for the disambiguation in Wikipedia. Their method is to balance the prior probability of a sense with its relatedness to"
I11-1113,D07-1074,0,0.922986,"Natural Language Processing, pages 1010–1018, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Section 5. The last section offers some concluding remarks. 2 Related Work Linking name mentions to knowledge base entries has attracted more and more attentions in these years. As an open available resource, Wikipedia is a natural choice of knowledge source for its large scale and good quality. Early work mainly focused on the usage of the structure information in Wikipedia. Bunescu and Pasca (2006) trained a taxonomy kernel on Wikipedia data to disambiguate named entities in open domain. Cucerzan (2007) integrated Wikipedia’s category information in their vector space model for named entity disambiguation. Mihalcea and Csomai (2007) extracted sentences from Wikipedia, regarding the linking information as sense annotation, and used supervised machine learning models to train a classifier for disambiguation. Similarly, Milne and Witten (2008) adopted a learning approach for the disambiguation in Wikipedia. Their method is to balance the prior probability of a sense with its relatedness to the surrounding context. Recently, an Entity Linking task in the Knowledge Base Population (KBP) track eva"
I11-1113,C10-1032,0,0.285777,"ation in Wikipedia. Their method is to balance the prior probability of a sense with its relatedness to the surrounding context. Recently, an Entity Linking task in the Knowledge Base Population (KBP) track evaluation (McNamee and Dang, 2009) provided a benchmark data set. The first KBP track was held at the Text Analysis Conference (TAC)1 , aiming to explore information about entities for Question Answering and Information Extraction. The knowledge base in the evaluation data is also based on Wikipedia. Many information retrieval based models have been proposed on this data set. For example, Dredze et al. (2010) presented a maximum margin approach to rank the candidates. They combined rich features including Wikipedia structure and entity’s popularity. Zheng et al. (2010) proposed learning to rank models for the entity linking problem and obtained high accuracy. One of the most important component of entity linking is to compute the relatedness between entities. Some of the previous works use vector space model and calculate the cosine similarity over the bag-of-word feature vectors (Mihalcea and Csomai, 2007) or the category feature vectors (Cucerzan, 2007). Others take into account citation overlap"
I11-1113,J97-4004,0,0.0148976,": 6: 7: 8: 9: 10: Similarly, for the in-degree measure we build the graph in Algorithm 2, where Cn is the name node set of the candidate entities and Na is the article node set of the neighboring entities. Algorithm 2 In-degree measure based graph construction Require: Cn and Na Ensure: Graph G = (V, E) Disambiguation 1: To build a graph for the disambiguation, we need to extract names from the context of the query (either as the name node or the article node). We use a segmentation technique which is inspired from a Chinese word segmentation algorithm, the forward maximum matching algorithm (Guo, 1997) on the context to find all the names which are included in the Wikipedia title list (i.e. all the name phrases in our Wikipedia graph are the Wikipedia article titles). This algorithm prefers to find the longest names that match with the string. Here we 6 See http://en.wikipedia.org/wiki/Wikipedia:Redirect for detailed instructions V := Ca ∪ Nn E := ∅ for all c ∈ Ca do for all n ∈ Nn do if n ∈ Article(c) then E := E ∪ (c, n) end if end for end for return (V, E) 2: 3: 4: 5: 6: 7: 8: 9: 10: V := Cn ∪ Na E := ∅ for all c ∈ Cn do for all n ∈ Na do if c ∈ Article(n) then E := E ∪ (n, c) end if end"
I11-1113,P10-1138,0,0.0597374,"Missing"
I11-1113,zesch-etal-2008-extracting,0,0.027243,"Missing"
I11-1113,N10-1072,0,0.0186403,"the Knowledge Base Population (KBP) track evaluation (McNamee and Dang, 2009) provided a benchmark data set. The first KBP track was held at the Text Analysis Conference (TAC)1 , aiming to explore information about entities for Question Answering and Information Extraction. The knowledge base in the evaluation data is also based on Wikipedia. Many information retrieval based models have been proposed on this data set. For example, Dredze et al. (2010) presented a maximum margin approach to rank the candidates. They combined rich features including Wikipedia structure and entity’s popularity. Zheng et al. (2010) proposed learning to rank models for the entity linking problem and obtained high accuracy. One of the most important component of entity linking is to compute the relatedness between entities. Some of the previous works use vector space model and calculate the cosine similarity over the bag-of-word feature vectors (Mihalcea and Csomai, 2007) or the category feature vectors (Cucerzan, 2007). Others take into account citation overlap of the relevant Wikipedia entry (Milne and Witten, 2008; Kulkarni et al., 2009; Radford et al., 2010), which implies the co1 http://www.nist.gov/tac occurrence of"
I11-1113,E09-1073,0,\N,Missing
I11-1171,A00-2018,0,0.0775912,"Missing"
I11-1171,E06-1011,0,0.174495,"Missing"
I11-1171,J05-1003,0,0.0537492,"Missing"
I11-1171,P08-1108,0,0.105721,"ndard averaged perceptron to learn the weight vector (Collins, 2002). As a probabilistic model, CRF defines the probability of a sequence to be µ(x, t) = P (t|x) = P ew·f (x,t) w·f (x,t′ ) t′ e Perceptron-based tagger with syntactic features Figure 2: Framework of our method. Based on a guide POS sequence t′ (tC in this paper) and a syntactic tree d, we propose three kinds of features, as shown in Table 1. Our use of guide POS Features fg (x, t′ , t) is mainly inspired by stacked learning, in which results of the firstlevel predicator are used to guide the second (Cohen and de Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). Syntactic features fs (x, d, t) explore features related with the head and children of the focus word. Syntactic features with guide POS tags 1448 Guide POS Features: fg (x, t′ , t) t′i ti t′i−1 t′i ti t′i−1 ti t′i t′i+1 ti t′i+1 ti t′i−1 t′i+1 ti ′ ti ∼ fb (x) t′i−1 t′i t′i+1 ti Syntactic Features: fs (x, d, t) Syntactic Features with Guide POS: fsg (x, t′ , d, t) #lc(i) ti wh(i) ti #lc(i) t′i ti #rc(i) t′i ti t′h(i) ti t′i d(i) ti wlc(i,k) ti d(i) ti t′lc(i,k) t′i ti t′rc(i,k) t′i ti t′h(i) t′i ti t′h(i) d(i) ti #rc(i) ti wh(i) d(i) ti t′lc(i,k) ti t′rc(i,k) ti t′h(i"
I11-1171,W02-1001,0,0.027099,"e to CRF in tagging accuracy but requires much less training time. During training phase, we adopt the 10-fold cross validation strategy to produce both tC and dA for the training set. Input sentence ˆt = arg max µ(x, t) CRF-based tagger tC Dependency Parser dA t We implement two baseline taggers, i.e., a Perceptron-based tagger and a CRF-based tagger. As a linear model, Perceptron defines the score of a tag sequence to be µ(x, t) = w · f (x, t) where f (x, t) refers to the feature vector and w is the corresponding weight vector. We use standard averaged perceptron to learn the weight vector (Collins, 2002). As a probabilistic model, CRF defines the probability of a sequence to be µ(x, t) = P (t|x) = P ew·f (x,t) w·f (x,t′ ) t′ e Perceptron-based tagger with syntactic features Figure 2: Framework of our method. Based on a guide POS sequence t′ (tC in this paper) and a syntactic tree d, we propose three kinds of features, as shown in Table 1. Our use of guide POS Features fg (x, t′ , t) is mainly inspired by stacked learning, in which results of the firstlevel predicator are used to guide the second (Cohen and de Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). Syntactic features"
I11-1171,W96-0213,0,0.369745,"Missing"
I11-1171,I05-3005,0,0.0485469,"Missing"
I11-1171,D07-1117,0,0.0424302,"Missing"
I11-1171,N09-2054,0,0.0424392,"Missing"
I11-1171,P08-1101,0,0.0183868,"ce information can be explored. Experimental results show that this effort is rewarding, and the tagging accuracy is significantly improved. Detailed error analysis confirms the usefulness of these syntactic features. 2 Baseline POS Taggers Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. The goal of POS tagging is to find the highest-scoring sequence: We adopt the exponentiated gradient algorithm to learn the weight vector (Collins et al., 2008). For POS tagging features f (x, t), we follow the work of Zhang and Clark (2008a). Besides standard POS unigram (wi ti ), bigram (ti−1 ti ) and trigram (ti−2 ti−1 ti ) features, they explore many features composed of Chinese characters, such as ci,0 ti and ci,−1 ti , where ci,0 and ci,−1 denote the start and end characters of wi . These characterbased features are very helpful for tagging accuracy. Due to space limitation, we refer to Zhang and Clark (2008a) for the complete feature description. In order to distinguish these features from our proposed syntactic features, we refer to them as the basic features and denote them as fb (x, t). Given w, we adopt the Viterbi al"
I11-1171,D08-1059,0,0.10179,"ce information can be explored. Experimental results show that this effort is rewarding, and the tagging accuracy is significantly improved. Detailed error analysis confirms the usefulness of these syntactic features. 2 Baseline POS Taggers Given an input sentence x = w1 ...wn , we denote its POS tag sequence by t = t1 ...tn , where ti ∈ T , 1 ≤ i ≤ n, and T is the POS tag set. The goal of POS tagging is to find the highest-scoring sequence: We adopt the exponentiated gradient algorithm to learn the weight vector (Collins et al., 2008). For POS tagging features f (x, t), we follow the work of Zhang and Clark (2008a). Besides standard POS unigram (wi ti ), bigram (ti−1 ti ) and trigram (ti−2 ti−1 ti ) features, they explore many features composed of Chinese characters, such as ci,0 ti and ci,−1 ti , where ci,0 and ci,−1 denote the start and end characters of wi . These characterbased features are very helpful for tagging accuracy. Due to space limitation, we refer to Zhang and Clark (2008a) for the complete feature description. In order to distinguish these features from our proposed syntactic features, we refer to them as the basic features and denote them as fb (x, t). Given w, we adopt the Viterbi al"
I11-1171,P08-1102,0,0.0604386,"Missing"
I11-1171,P09-1058,0,0.123831,"Missing"
I11-1171,D08-1017,0,0.0653507,"to learn the weight vector (Collins, 2002). As a probabilistic model, CRF defines the probability of a sequence to be µ(x, t) = P (t|x) = P ew·f (x,t) w·f (x,t′ ) t′ e Perceptron-based tagger with syntactic features Figure 2: Framework of our method. Based on a guide POS sequence t′ (tC in this paper) and a syntactic tree d, we propose three kinds of features, as shown in Table 1. Our use of guide POS Features fg (x, t′ , t) is mainly inspired by stacked learning, in which results of the firstlevel predicator are used to guide the second (Cohen and de Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). Syntactic features fs (x, d, t) explore features related with the head and children of the focus word. Syntactic features with guide POS tags 1448 Guide POS Features: fg (x, t′ , t) t′i ti t′i−1 t′i ti t′i−1 ti t′i t′i+1 ti t′i+1 ti t′i−1 t′i+1 ti ′ ti ∼ fb (x) t′i−1 t′i t′i+1 ti Syntactic Features: fs (x, d, t) Syntactic Features with Guide POS: fsg (x, t′ , d, t) #lc(i) ti wh(i) ti #lc(i) t′i ti #rc(i) t′i ti t′h(i) ti t′i d(i) ti wlc(i,k) ti d(i) ti t′lc(i,k) t′i ti t′rc(i,k) t′i ti t′h(i) t′i ti t′h(i) d(i) ti #rc(i) ti wh(i) d(i) ti t′lc(i,k) ti t′rc(i,k) ti t′h(i) wi ti t′h(i) t′i d(i)"
I11-1176,ambati-etal-2010-active,0,0.0374451,"Missing"
I11-1176,W10-0701,0,0.0352678,"Missing"
I11-1176,C10-3004,1,0.801732,"sers how to use the system. “系统产生的验证码” means “The confirmation codes provided by the system” and “选择正确的意思” means “Please choose the correct word sense”. built a trial system and took advantage of an annotated Chinese all-words WSD corpus consisting of about 10,000 sentences containing about 200,000 words sampled from Chinese news documents. In these words, there are about 78,800 ambiguous words and all of which have been annotated with their corresponding senses by human experts. Among them, we randomly set 5,000 words as unknown questions and remaining as known. 3.2 Thesaurus We use WordMap (Che et al., 2010) as a thesaurus to represent word senses. There are more than 100,000 Chinese words in WordMap. Each word sense belongs to a tree node with five levels. There are 12 top level nodes, such as “entity” and “human beings”. There are about 100 second, 15,000 third, and more fourth and fifth level nodes. Under the fifth level nodes, there are some synonyms which have the same word sense. For instance, the word “材料” has two senses which are represented by five level nodes as follows: 1. 物 (entity) → 统称 (common name) → 物资 (goods) → 物资 (goods) → 材料 (material) 1474 # of times an example is annotated 1"
I11-1176,W04-0811,0,0.0539821,"Missing"
I11-1176,P95-1026,0,0.207491,"Missing"
I13-1036,P01-1008,0,0.0570982,"Missing"
I13-1036,P09-1068,0,0.0832705,"Missing"
I13-1036,P11-1098,0,0.0468116,"Missing"
I13-1036,W09-1207,1,0.835918,"t, which is an important feature for recognizing the event type. Kiyoshi Sudo (2003) summarized three classical models for representing events. All of these three models rely on the syntactic tree structure and the trigger is specified as a predicate in this structure. In order to accurately extract event triggers, we employ the predicateargument model (Yangarder et al., 2000) which is based on a direct syntactic relation between a predicate and its arguments. We extract the syntactic relation for predicate-argument model by means of the HIT (Harbin Institute of Technology) Dependency Parser (Che et al., 2009). Based on the predicate-argument model, we propose a trigger extraction algorithm (TE). The details are shown in Figure 2. Take the following sentence as an example: 毛泽东 1893 年 出生 于 湖南湘潭。 1 2 3 4 5 → Mao Zedong was born in Xiangtan, Hunan 1 2 3 4 5 Province in 1893. 6 7 The HIT Chinese dependencies are: SBV (出生-3, 毛泽东-1) Dependency → (born-3, Mao Zedong-1) VOB (出生-3, 湖南湘潭-5) → (born-3, Xiangtan, Hunan Province-5) 312 Parser ADV (出生-3, 1893 年-2) → (born-3, 1893-7) POB (湖南湘潭-5, 于-4) → (Hunan Province-5, in-4) where each atomic formula represents a binary dependence from the governor (the first"
I13-1036,P12-1088,0,0.0206591,"ing Ji’s work, Liao et al., (2010) used document level cross-event inference to improve event extraction. Chen and Ji, (2009) combined word-based classifier with character-based classifier; and explored effective features for the Chinese event extraction task. Liao and Grishman, (2010) ranked two semi-supervised learning methods for adapting the event extraction system to new event types. Hong et al, (2011) proposed a blind cross-entity inference method for event extraction, which well uses the consistency of entity mention to achieve sentence-level trigger and argument (role) classification. Lu and Roth, (2012) presented a novel model based on the semi-Markov conditional random ﬁelds for the challenging event extraction task. The model takes in coarse mention boundary and type information and predicts complete structures indicating the corresponding argument role for each mention. However, for all the above approaches, it is necessary to specify the target event type in advance. Defining and identifying those types heavily rely on expert knowledge, and reaching an agreement among the experts or annotators requires a lot of human labor. Li et al., (2010) proposed a domain-independent novel event disc"
I13-1036,N04-1043,0,0.0488039,"rule (1) did. This is due to the fact that rule (2) is more effective on the domain-specific corpus. 4 4.1 Related Work Word Cluster Discovery Our approach of automatically building an event type paradigm is related to some prior work on word cluster discovery (e.g. Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003). Most of these works are based on machine translation techniques to solve paraphrase extraction problem. However, several recent researches have stressed the benefits of using word clusters to improve the performance of information extraction tasks. For example, Miller et al., (2004) proved that word clusters could significantly improve English name tagging performance. In the same vein, some studies work on the problem of relation extraction (Chambers and Jurasky, 2011 and 2009; Poon and Domingos, 2009 and 2008; Yates and Etzioni, 2009). In these work, “relation words” were extracted and clustered. In this paper, our work confirmed that trigger clusters are also effective for event type paradigm building. The problem of event trigger 317 words extraction and clustering is also a challenge problem. 4.2 Traditional Event Extraction The commonly used approaches for most eve"
I13-1036,P11-1113,0,0.014349,"exploited a correlation between senses of verbs (that are the triggers for events) and topics of documents. They first proposed refining event extraction through unsupervised cross-document inference. Following Ji’s work, Liao et al., (2010) used document level cross-event inference to improve event extraction. Chen and Ji, (2009) combined word-based classifier with character-based classifier; and explored effective features for the Chinese event extraction task. Liao and Grishman, (2010) ranked two semi-supervised learning methods for adapting the event extraction system to new event types. Hong et al, (2011) proposed a blind cross-entity inference method for event extraction, which well uses the consistency of entity mention to achieve sentence-level trigger and argument (role) classification. Lu and Roth, (2012) presented a novel model based on the semi-Markov conditional random ﬁelds for the challenging event extraction task. The model takes in coarse mention boundary and type information and predicts complete structures indicating the corresponding argument role for each mention. However, for all the above approaches, it is necessary to specify the target event type in advance. Defining and id"
I13-1036,W03-1608,0,0.0350848,"Missing"
I13-1036,P08-1030,0,0.085081,"ction The commonly used approaches for most event extraction systems are the knowledge engineering approach and the machine learning approach. Grishman et al., (2005) used a combination of pattern matching and statistical modeling techniques. They extract two kinds of patterns: 1) the sequence of constituent heads separating anchor and its arguments; and 2) a predicate argument sub-graph of the sentence connecting anchor to all the event arguments. In conjunction, they used a set of Maximum Entropy based classiﬁers for 1) Trigger labeling, 2) Argument classiﬁcation and 3) Event classiﬁcation. Ji and Grishman, (2008) further exploited a correlation between senses of verbs (that are the triggers for events) and topics of documents. They first proposed refining event extraction through unsupervised cross-document inference. Following Ji’s work, Liao et al., (2010) used document level cross-event inference to improve event extraction. Chen and Ji, (2009) combined word-based classifier with character-based classifier; and explored effective features for the Chinese event extraction task. Liao and Grishman, (2010) ranked two semi-supervised learning methods for adapting the event extraction system to new event"
I13-1036,C10-1077,0,0.0264754,"opy based classiﬁers for 1) Trigger labeling, 2) Argument classiﬁcation and 3) Event classiﬁcation. Ji and Grishman, (2008) further exploited a correlation between senses of verbs (that are the triggers for events) and topics of documents. They first proposed refining event extraction through unsupervised cross-document inference. Following Ji’s work, Liao et al., (2010) used document level cross-event inference to improve event extraction. Chen and Ji, (2009) combined word-based classifier with character-based classifier; and explored effective features for the Chinese event extraction task. Liao and Grishman, (2010) ranked two semi-supervised learning methods for adapting the event extraction system to new event types. Hong et al, (2011) proposed a blind cross-entity inference method for event extraction, which well uses the consistency of entity mention to achieve sentence-level trigger and argument (role) classification. Lu and Roth, (2012) presented a novel model based on the semi-Markov conditional random ﬁelds for the challenging event extraction task. The model takes in coarse mention boundary and type information and predicts complete structures indicating the corresponding argument role for each"
I13-1036,N03-1024,0,0.0297658,"Missing"
I13-1036,D08-1068,0,0.0922965,"Missing"
I13-1036,D09-1001,0,0.126104,"to be very data dependent. As a result, it  may prevent the event extraction from being widely applicable. Since event types among domains are different, the event type paradigm of ACE, which does not define music related events, is useless for the music domain event extraction. So we have to build a totally different event type paradigm for the music domain from scratch. Recently, some researchers have been aware of the limitations of only considering pre-defined paradigm as well. In the same vein, some studies work on the problem of relation extraction (Chambers and Jurasky, 2011 and 2009; Poon and Domingos, 2009 and 2008; Yates and Etzioni, 2009). Rosenfeld and Feldman (2006) built a high-performance unsupervised relation extraction system without target relations in advance. Hasegawa et al. (2004) discovered relations among named entities from large corpora by clustering pairs of named entities. However, most of the above work focuses on relation extraction rather than event extraction. In contrast to the well-studied problem of relation extraction, only a few works focused on event extraction. For example, Li (2010) proposed a domain-independent novel event discovery approach. They exploited a cros"
I13-1036,P06-2086,0,0.0215375,"event extraction from being widely applicable. Since event types among domains are different, the event type paradigm of ACE, which does not define music related events, is useless for the music domain event extraction. So we have to build a totally different event type paradigm for the music domain from scratch. Recently, some researchers have been aware of the limitations of only considering pre-defined paradigm as well. In the same vein, some studies work on the problem of relation extraction (Chambers and Jurasky, 2011 and 2009; Poon and Domingos, 2009 and 2008; Yates and Etzioni, 2009). Rosenfeld and Feldman (2006) built a high-performance unsupervised relation extraction system without target relations in advance. Hasegawa et al. (2004) discovered relations among named entities from large corpora by clustering pairs of named entities. However, most of the above work focuses on relation extraction rather than event extraction. In contrast to the well-studied problem of relation extraction, only a few works focused on event extraction. For example, Li (2010) proposed a domain-independent novel event discovery approach. They exploited a cross-lingual clustering algorithm based on sentence-aligned bilingua"
I13-1036,H01-1009,0,0.0443004,"Missing"
I13-1036,P03-1029,0,0.0873923,"Missing"
I13-1036,W02-1028,0,0.106828,"Missing"
I13-1036,P04-1053,0,\N,Missing
I13-1036,J93-3001,0,\N,Missing
I13-1036,E03-3004,0,\N,Missing
I13-1036,Y10-1027,0,\N,Missing
I13-1055,W03-1028,0,0.0486897,"patterns are carefully chosen according to morphological structure or special format of corpus (Nakayama et al., 2008), either manually or via automatic bootstrapping (Hearst, 1992). However, the methods suffer from sparse coverage of patterns in a given corpus. Some researchers try to map the words to a thesaurus or an existed ontology (WordNet or Wikipedia) automatically so as to get key concepts (Angeletou et al., 2008). The coverage and openness of existed ontologies seriously limit the scope of these works. Simple statistical methods http://movie.douban.com 481 such as TF-IDF weighting (Hulth, 2003) are not feasible for folksonomy since short text snippets only. Graph-based ranking methods are the state of the art. They are superior to the statistic-based methods because of considering structure information between words. Mihalcea and Tara (2004) propose to use TextRank, a modified PageRank algorithm to extract key concepts from text. But TextRank only maintain a single importance score for each word. Hotho et al. (2006) propose a graph-based ranking algorithm for folksonomy, named FolkRank. They convert triadic hypergraph in folksonomy into an undirected tripartite graph. But we conside"
I13-1055,P11-1039,0,0.0734293,"xtracted by LDA are more conformed to the actual situation than the topics of ODP. They pay more attention to the effect of the transfer action probability on the importance score of tags which benefit us to know the propagation process. It seems that mixing together the random 2 walks of all the topics in one graph may cause noise compared to independent topic graphs. Liu et al. (2010) decompose a traditional random walk into multiple random walks specific to various topics, named Topical PageRank (TPR). The novel contribution is the study on topicspecific preference value setting. And then, Zhao et al. (2011) argue that context-free propagation may cause the importance scores to be off-topic. They model the score propagation with topic context when setting the link weights and then denote this context-sensitive topical PageRank as cTPR. Enlightened by TPR and cTPR, we further propose a new link weight function to express the semantic similarity between two tags of folksonomy. The novel link weight function combines the local similarity (defined as cooccurrence of tags in a same resource assigned to the given topic) with the global similarity (defined as cosine similarity of two tags over all the t"
I13-1055,D10-1036,0,0.110987,"ectory Project)2 extracted manually. Based on their works, Jin et al. (2011) implement a topicsensitive tag ranking (TSTR) approach in folksonomy automatically through LDA. TSTR performs better than TSPR and TLA because topics extracted by LDA are more conformed to the actual situation than the topics of ODP. They pay more attention to the effect of the transfer action probability on the importance score of tags which benefit us to know the propagation process. It seems that mixing together the random 2 walks of all the topics in one graph may cause noise compared to independent topic graphs. Liu et al. (2010) decompose a traditional random walk into multiple random walks specific to various topics, named Topical PageRank (TPR). The novel contribution is the study on topicspecific preference value setting. And then, Zhao et al. (2011) argue that context-free propagation may cause the importance scores to be off-topic. They model the score propagation with topic context when setting the link weights and then denote this context-sensitive topical PageRank as cTPR. Enlightened by TPR and cTPR, we further propose a new link weight function to express the semantic similarity between two tags of folksono"
I13-1055,W04-3252,0,\N,Missing
I13-1055,C92-2082,0,\N,Missing
K17-3005,P13-1104,0,0.0329559,"a. We scale PMI 1 Vietnamese requires word segmentation because white spaces occur both inter- and intra-words. When segmenting Vietnamese, white space-separated tokens are used as inputs, rather than characters as in Chinese and Japanese. In addition, we don’t consider Korean here since the Korean input texts have already been segmented in the corpus provided by the task. Word Segmentation We develop our own word segmentation models particularly for languages which do not have ex53 2.3 The transition-based dependency parsing algorithm with a list-based arc-eager transition system proposed by Choi and McCallum (2013) is used in our parser. We base our parser mainly on the Stack-LSTM model proposed by Dyer et al. (2015), where three Stack-LSTMs are utilized to incrementally obtain the representations of the buffer β, the stack σ and the transition action sequence A. In addition, a dependency-based Recursive Neural Network (RecNN) is used to compute the partially constructed tree representation. However, compared with the arc-standard algorithm (Nivre, 2004) used by Dyer et al. (2015), the list-based arc-eager transition system has an extra component in each configuration, i.e., the deque δ. So we use an ad"
K17-3005,P16-2006,0,0.0510368,"sively combining head-modifier pairs. Whereas in Tree-LSTM, a head is combined with all of its modifiers simultaneously in each LSTM unit. However, our implementation of Tree-LSTM is different from the conventional one. Unlike traditional bottom-up Tree-LSTMs in which each head and all of its modifiers are combined simultaneously, the modifiers are found incrementally during our parsing procedure. Therefore, we propose Incremental Tree-LSTM, which obtains sub-tree representations incrementally. To be more specific, each time a dependency arc is generated, 2016; Kiperwasser and Goldberg, 2016; Cross and Huang, 2016), thus called Bi-LSTM Subtraction. The forward and backward subtractions are calculated independently, i.e., bf = hf (l)−hf (f ) and bb = bb (f ) − bb (l), where hf (f ) and hf (l) are the hidden vectors of the first and the last words in the forward LSTM, hb (f ) and hb (l) are the hidden vectors of the first and the last words in the backward LSTM. Then bf and bb are concatenated as the buffer representation. As illustrated in Figure 5, the forward and backward subtractions for the buffer are bf = hf (here) − hf (nice) and bb = hb (nice) − hb (here) respectively. 55 Target Source we collect"
K17-3005,P15-1033,0,0.103906,". When segmenting Vietnamese, white space-separated tokens are used as inputs, rather than characters as in Chinese and Japanese. In addition, we don’t consider Korean here since the Korean input texts have already been segmented in the corpus provided by the task. Word Segmentation We develop our own word segmentation models particularly for languages which do not have ex53 2.3 The transition-based dependency parsing algorithm with a list-based arc-eager transition system proposed by Choi and McCallum (2013) is used in our parser. We base our parser mainly on the Stack-LSTM model proposed by Dyer et al. (2015), where three Stack-LSTMs are utilized to incrementally obtain the representations of the buffer β, the stack σ and the transition action sequence A. In addition, a dependency-based Recursive Neural Network (RecNN) is used to compute the partially constructed tree representation. However, compared with the arc-standard algorithm (Nivre, 2004) used by Dyer et al. (2015), the list-based arc-eager transition system has an extra component in each configuration, i.e., the deque δ. So we use an additional Stack-LSTM to learn the representation of δ. More importantly, we introduce two LSTM-based tech"
K17-3005,N13-1073,0,0.0392245,"nt data. Target model Unified model Cross-lingual word embeddings train BiTexts Source fine-tuning Target 4.1.2 Figure 7: The cross-lingual transfer approach. Target Source bxr tr & ug & kk kmr fa sme fi ftb & fi We use the provided 100-dimensional multilingual word embeddings5 in our tokenization, POS tagging and parsing models, and use the Wikipedia and CommonCrawl data for training Brown clusters. The number of clusters is set to 256. For cross-lingual transfer parsing of lowresource languages, we use parallel data from OPUS to derive cross-lingual word embeddings.6 The fast align toolkit (Dyer et al., 2013) is used for word alignment.7 We use the Dynet toolkit for the implementation of all our neural models.8 hsb cs Table 2: Cross-lingual delexicalized transfer settings for surprise languages. After that, target language-specific parsers are obtained through fine-tuning on their own treebanks. Figure 7 illustrates the flow of our transfer approach. For the surprise languages in the final test phase, we use the transfer settings in Table 2. We use multi-source delexicalized transfer for surprise language parsing, considering that bilingual parallel data which is required for obtaining crosslingua"
K17-3005,P15-1119,1,0.848142,"raining treebank of each domain, to obtain target domain-specific parsers. In practice, for each language considered here, we treat the largest treebank as our source-domain data, and the rest as target-domain data. Only target-domain models are fine-tuned from the unified parser, while the source-domain parser is trained separately using the source treebank alone. For the new parallel test sets in test phase, we simply use the model trained on source-domain data, without any assumption on the target domain. 3.2 qa en resource, and employ the cross-lingual model transfer approach described in Guo et al. (2015, 2016) to benefit from existing resource-rich languages. The low-resource languages here include Ukrainian (uk), Irish (ga), Uyghur (ug) and Kazakh (kk). We determine their source language (treebank) according to the language families they belong to and their linguistic typological similarity. Specifically, the transfer setting is shown in Table 1. The transfer approach is similar to crossdomain transfer as described above, with one important difference. Here, we use cross-lingual word embeddings and Brown clusters derived by the robust projection approach (Guo et al., 2015) when training the"
K17-3005,L16-1262,0,0.0932897,"Missing"
K17-3005,L16-1680,0,0.0624147,"Missing"
K17-3005,N12-1052,0,0.0885217,"Missing"
K17-3005,P15-1150,0,0.128276,"Missing"
K17-3005,N16-1064,0,0.0315829,"Missing"
K17-3005,P15-1032,0,0.0259162,"or evaluation. Eliyahu Kiperwasser and Yoav Goldberg. 2016. Simple and accurate dependency parsing using bidirectional lstm feature representations. TACL 4:313– 327. 6 Percy Liang. 2005. Semi-supervised learning for natural language. Master thesis, Massachusetts Institute of Technology. Credits There are a few references we would like to give proper credit, especially to data providers: the core Universal Dependencies paper from LREC 2016 (Nivre et al., 2016), the UD version 2.0 datasets (Nivre et al., 2017b,a), the baseline UDPipe models (Straka et al., 2016b), the baseline SyntaxNet models (Weiss et al., 2015) and the evaluation platform TIRA (Potthast et al., 2014). Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. International Conference on Learning Representations (ICLR) Workshop . Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proc. of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together. pages 50–57. Acknowledgments ˇ Joakim Nivre, Zeljko Agi´c, Lars Ahrenberg, Lene Antonsen, Maria Jesus Aranzabe, Masayuki Asahara, Luma Ateyah, Mohammed Attia, Aitziber Atutxa, Ele"
K17-3005,Q16-1023,0,\N,Missing
K18-2005,P15-1001,0,\N,Missing
K18-2005,P15-1119,1,\N,Missing
K18-2005,I17-1007,0,\N,Missing
K18-2005,C16-1002,1,\N,Missing
K18-2005,K17-3005,1,\N,Missing
K18-2005,K17-3002,0,\N,Missing
K18-2005,K17-3004,0,\N,Missing
K18-2005,K17-3003,0,\N,Missing
K18-2005,D17-1002,0,\N,Missing
K18-2005,P18-1129,1,\N,Missing
K18-2005,N18-1088,1,\N,Missing
K19-1069,W16-3634,0,0.0282148,"Missing"
K19-1069,D14-1162,0,0.0851716,"Zhou et al., 2016), Deep Learning to Respond (DL2R) (Yan et al., 2016), Match-LSTM (Wang and Jiang, 2016), MVLSTM (Wan et al., 2016), and DualEncoder (Lowe et al., 2015). Attention-based Models. The attention-based models typically match the context and the candidate response based on the attention among them, including DAM (Zhou et al., 2018), DUA (Zhang et al., 2018), and RNN-CNN (Baudiˇs et al., 2016). Experiment Setup We implement our model by Keras (Chollet et al., 2015) with TensorFlow backend. In the Embedding Layer, the word embeddings are pre-trained using the training set via GloVe (Pennington et al., 2014), the weights of which are trainable. For char embedding, we set the kernel shape as 3 and filter number as 200 in the CNN layer. For all the Bidirectional LSTM layers, we set their hidden size to 200. We use Adamax (Kingma and Ba, 2014) for weight updating with an initial learning rate of 0.002. For ensemble models, we generate 6 models for each corpus using different random seeds and merge the result by voting. For better comparison with the baseline models, the main super parameters in TripleNet, such 4.4 Overall Results The overall results on two datasets are depicted in Table 1. Our resul"
K19-1069,N18-1202,0,0.0770022,"Missing"
K19-1069,P17-1055,1,0.78487,"wo utterances (including the query) are more important than the previous ones. If we only match the response with the context, the model may be misled by the high frequency word ‘install’ and choose the false candidate. (2) the information of different granularities is important, which includes not only the word, utterance, and context level, but also the char level. For example, the different tenses (‘install,’ ‘installed’) and the misspelling word (‘angry’) appear constantly in the conversation. Similar to the role of question for the task of machine reading comprehension (Seo et al., 2016; Cui et al., 2017; Chen et al., 2019), the query in this task is also the key to selecting the response. In this paper, we propose a model named TripleNet to excavate the role of query. The main contributions of our work are listed as follows. • we use a novel triple attention mechanism to model the relationships within hC, Q, Ri instead of hC, Ri; • we propose a hierarchical representation module to fully model the conversation from char to context level; • The experimental results on Ubuntu and Douban corpus show that TripleNet significantly outperform the state-of-the-art result. 2 Related Works Earlier wor"
K19-1069,D11-1054,0,0.0585199,"Missing"
K19-1069,C14-1088,0,0.0186159,"ation from char to context level; • The experimental results on Ubuntu and Douban corpus show that TripleNet significantly outperform the state-of-the-art result. 2 Related Works Earlier works on building the conversation systems are generally based on rules or templates (Walker et al., 2001), which are designed for the specific domain and need much human effort to collect the rules and domain knowledge. As the portability and coverage of such systems are far from satisfaction, people pay more attention to the data-driven approaches for the opendomain conversation system (Ritter et al., 2011; Higashinaka et al., 2014). The main challenge for open-domain conversation is to produce a corresponding response based on the current context. As mentioned previously, the retrieval-based and generation-based methods are the mainstream approaches for conversational response generation. 3 Model In this section, we will give a detailed introduction of the proposed model TripleNet. We first formal738 g(C,Q,R) Hier-LSTM Word embedding Char embedding Match C C Context Level C’ Input Q BAF Input Q Q’ R’ R BAF C BAF BAF R C1 C’ C C Utterance Level Q Q’ C2 R’ R BAF Output Q BAF C’ C BAF Input Q Q’ R1 R Q BAF C’ Q2 R’ R2 BAF"
K19-1069,P01-1066,0,0.0881331,"nse. In this paper, we propose a model named TripleNet to excavate the role of query. The main contributions of our work are listed as follows. • we use a novel triple attention mechanism to model the relationships within hC, Q, Ri instead of hC, Ri; • we propose a hierarchical representation module to fully model the conversation from char to context level; • The experimental results on Ubuntu and Douban corpus show that TripleNet significantly outperform the state-of-the-art result. 2 Related Works Earlier works on building the conversation systems are generally based on rules or templates (Walker et al., 2001), which are designed for the specific domain and need much human effort to collect the rules and domain knowledge. As the portability and coverage of such systems are far from satisfaction, people pay more attention to the data-driven approaches for the opendomain conversation system (Ritter et al., 2011; Higashinaka et al., 2014). The main challenge for open-domain conversation is to produce a corresponding response based on the current context. As mentioned previously, the retrieval-based and generation-based methods are the mainstream approaches for conversational response generation. 3 Mod"
K19-1069,N16-1170,0,0.020917,"ponses (Rn @k) as evaluation metrics, and we use MAP (Mean Average Precision), MRR (Mean Reciprocal Rank), and Precision-at-one as the additional metrics for Douban corpus, following the previous work (Wu et al., 2017). 4.2 4.3 Baseline Models We basically divided baseline models into two categories for comparisons. Non-Attention Models. The majority of the previous works on this task are designed without attention mechanisms, including the Sequential Matching Network (SMN) (Wu et al., 2017), Multi-View model (Zhou et al., 2016), Deep Learning to Respond (DL2R) (Yan et al., 2016), Match-LSTM (Wang and Jiang, 2016), MVLSTM (Wan et al., 2016), and DualEncoder (Lowe et al., 2015). Attention-based Models. The attention-based models typically match the context and the candidate response based on the attention among them, including DAM (Zhou et al., 2018), DUA (Zhang et al., 2018), and RNN-CNN (Baudiˇs et al., 2016). Experiment Setup We implement our model by Keras (Chollet et al., 2015) with TensorFlow backend. In the Embedding Layer, the word embeddings are pre-trained using the training set via GloVe (Pennington et al., 2014), the weights of which are trainable. For char embedding, we set the kernel shape"
K19-1069,P17-1046,0,0.602053,"practical in applications. Selecting a response from a set of candidates is an important and challenging task for the retrieval-based method. Many of the previous approaches are based on Deep Neural Network (DNN) to select the response for single-turn conversation (Lu and Li, 2013). We study multi-turn response selection in this paper, which is rather difficult because it not only requires identification of the important information such as keywords, phrases, and sentences, but also the latent dependencies between the context, query, and candidate response. Previous works (Zhou et al., 2018; Wu et al., 2017) show that representing the context at different granularities is vital for multi-turn response selection. However, it is not enough for multi-turn response selection. Figure 1 illustrates the problem with a real example in Ubuntu Corpus. As demonstrated, the following two points should be Introduction To establish a human-machine dialogue system is one of the most challenging tasks in Artificial Intelligence (AI). Existing works on building dialogue systems are mainly divided into two categories: retrieval-based method (Yan et al., 2016; Zhou et al., 2016), and generation-based method (Vinyal"
K19-1069,C18-1317,0,0.0709103,"ious features in a deep neural network. Baudiˇs et al. (2016) regarded the task as sentence pair scoring and implemented an RNN-CNN neural network model with attention. Zhou et al. (2016) proposed a multiview model with CNN and RNN, modeling the context in both word and utterance view. Further, Xu et al. (2017) proposed a deep neural network to incorporate background knowledge for conversation by LSTM with a specially designed recall gate. Wu et al. (2017) proposed matching the context and response by their word and phrase representations, which had significant improvement from previous work. Zhang et al. (2018) introduced a self-matching attention to route the vital information in each utterance, and used RNN to fuse the matching result. Zhou et al. (2018) used self-attention and cross-attention to construct the representations at different granularities, achieving a state-of-the-art result. Our model is different from the previous methods: first we model the task with the triple hC, Q, Ri instead of hC, Ri in the early works, and use a novel triple attention matching mechanism to model the relationships within the triple. Then we represent the context from low (character) to high (context) level, w"
K19-1069,D16-1036,0,0.032133,"Missing"
K19-1069,P18-1103,0,0.565581,"because it is more practical in applications. Selecting a response from a set of candidates is an important and challenging task for the retrieval-based method. Many of the previous approaches are based on Deep Neural Network (DNN) to select the response for single-turn conversation (Lu and Li, 2013). We study multi-turn response selection in this paper, which is rather difficult because it not only requires identification of the important information such as keywords, phrases, and sentences, but also the latent dependencies between the context, query, and candidate response. Previous works (Zhou et al., 2018; Wu et al., 2017) show that representing the context at different granularities is vital for multi-turn response selection. However, it is not enough for multi-turn response selection. Figure 1 illustrates the problem with a real example in Ubuntu Corpus. As demonstrated, the following two points should be Introduction To establish a human-machine dialogue system is one of the most challenging tasks in Artificial Intelligence (AI). Existing works on building dialogue systems are mainly divided into two categories: retrieval-based method (Yan et al., 2016; Zhou et al., 2016), and generation-ba"
K19-1069,W15-4640,0,\N,Missing
K19-2007,P13-1023,0,0.367278,"combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Recently, a lot of semantic graphbanks arise, which differ in the design of graphs (Kuhlmann and Oepen, 2016), or semantic scheme (Abend and Rappoport, 2017). More specifically, SDP (Oepen et al., 2015), including DM, PSD and PAS, treats the tokens as nodes and connect them with semantic relations; EDS (Flickinger et al., 2017) encodes MRS representations (Copestake et al., 1999) as graphs with the many-to-many relations between tokens and nodes; UCCA (Abend and Rappoport, 2013) represents semantic structures with the multi-layer framework; AMR (Banarescu 1 See http://mrp.nlpl.eu/ for further technical details, information on how to obtain the data, and official results. 76 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 76–85 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2007 post-processing buffer stack transition-based parser with stack LSTM classifier deque transition system action history pos lemma frame tagger tagger tagger BERT Figure 1: A"
K19-2007,W13-2322,0,0.123225,"Missing"
K19-2007,Q16-1023,0,0.0465041,"roposed in in Section 3. 2 We encourage the reader to read Dyer et al. (2015) for more details. 77 operation a a b b c stacks buffer batch LSTM states a state a b state b c state c state a state b state c c Figure 2: When some new I NSERT operations come, the data to be inserted are pushed into corresponding buffers. They will be merged into a batch once batch-processing is triggered. After that, new LSTM states will be pushed to corresponding stacks. 2.2 Batch Training there is already an I NSERT in the buffer; b) operation P OP or Q UERY comes. To clarify, the depth of buffer per data is 1. Kiperwasser and Goldberg (2016) shows that batch training increases the gradient stability and speeds up the training. Delaying the backward to simulate mini-batch update is a simple way to realize batch training, but it fails to compute over data in parallel. To solve this, we propose a method of maintaining stack LSTM structure and using operation buffer. 2.3 2.3.1 BERT-Enhance Word Representation Deep Contextualized Word Representations Neural parsers often use pretrained word embeddings as their primary input, i.e. word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which assign a single static represen"
K19-2007,P19-4002,0,0.105274,"Missing"
K19-2007,P17-1112,0,0.569697,"are sub-set of surface tokens; in EDS and UCCA, graph nodes are explicitly aligned with the tokens; in AMR, the alignments are implicit. Most semantic parsers are only designed for one or few specific graphbanks, due to the differences in annotation schemes. For example, the currently best parser for SDP is graph-based (Dozat and Manning, 2018), which assumes dependency graphs but cannot be directly applied to UCCA, EDS, and AMR, due the existence of concept node. Hershcovich et al. (2018) parses across different semantic graphbanks (UCCA, DM, AMR), but only works well on UCCA. The system of Buys and Blunsom (2017) is a good data-driven EDS parser, but does poorly on AMR. Lindemann et al. (2019) sets a new SOTA in DM, PAS, PSD, AMR and nearly SOTA in EDS, via representing each graph with the compositional tree structure (Groschwitz et al., 2017), but they do not expand this method to UCCA. Learning from multiple flavors of meaning representation in parallel has hardly been explored, and notable exceptions include the parsers of Peng et al. (2017, 2018); Hershcovich et al. (2018). Therefore, the main challenge in crossframework semantic parsing task is that diverse framework differs in the mapping way be"
K19-2007,D19-1279,0,0.0379668,"2014), which assign a single static representation to each word so that they cannot capture context-dependent meaning. By contrast, deep contextualized word representations, i.e. ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), encode words with respect to the context, which have been proven to be useful for many NLP tasks, achieving state-of-the-art performance in standard Natural Language Understanding (NLU) benchmarks, such as GLUE (Wang et al., 2018a). Che et al. (2018) adopted ELMo in CoNLL 2018 shared task (Zeman et al., 2018) and achieved first prize in terms of LAS metric. (Kondratyuk and Straka, 2019) exceeds the state-ofthe-art in UD with fine-tuning model with BERT. stack LSTM The stack LSTM augments the conventional LSTM with a ‘stack pointer’. And it supports the operation including: a) I NSERT adds elements to the end of the sequence; b) P OP moves the stack pointer to the previous element; c) Q UERY returns the output vector where the stack pointer points. Among these three operation, P OP and Q UERY only manipulates the stack without complex computing, but I NSERT performs lots of computing. Batch Data in Operation-Level Like conventional LSTM can’t form a batch inside a sequence du"
K19-2007,P13-2131,0,0.0357806,"ist the involved results they submitted. Feature DM LF1 MRP PSD LF1 MRP UCCA LF1 MRP EDS EDM MRP GloVe BERT(base) 87.1 94.3 74.1 83.6 56.3 64.3 82.5 87.6 87.3 90.5 73.7 76.7 87.5 92.8 88.2 91.5 AMR SMATCH MRP 64.8 71.0 65.3 71.4 Table 2: HIT-SCIR parser results on MRP split dataset with GloVe or BERT as pretrained word representation. MRP stands for cross-framework evaluation metric. LF1 stands for SDP Labeled F1 (Oepen et al., 2014) in DM/PSD, UCCA Labeled Dependency F1 (Hershcovich et al., 2019) in UCCA. And EDM (Dridan and Oepen, 2011) stands for Elementary Dependency Match in EDS. SMATCH (Cai and Knight, 2013) is an evaluation metric for semantic feature structures in AMR. groups differ in learning rate. For training we use Adam (Kingma and Ba, 2015). Code for our parser and model weights are available at https://github.com/DreamerDeo/ HIT-SCIR-CoNLL2019. as nodes and edges, we need an extra procedure to recognize which nodes should be properties in the final result. Once recognized, node along with the corresponding edge will be converted to the property of its parent node, edge label for the key, and node label for the value. We write some rules to perform the recognizing procedure. Rules come fr"
K19-2007,J16-4009,0,0.296551,"Missing"
K19-2007,K18-2005,1,0.746044,"often use pretrained word embeddings as their primary input, i.e. word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which assign a single static representation to each word so that they cannot capture context-dependent meaning. By contrast, deep contextualized word representations, i.e. ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), encode words with respect to the context, which have been proven to be useful for many NLP tasks, achieving state-of-the-art performance in standard Natural Language Understanding (NLU) benchmarks, such as GLUE (Wang et al., 2018a). Che et al. (2018) adopted ELMo in CoNLL 2018 shared task (Zeman et al., 2018) and achieved first prize in terms of LAS metric. (Kondratyuk and Straka, 2019) exceeds the state-ofthe-art in UD with fine-tuning model with BERT. stack LSTM The stack LSTM augments the conventional LSTM with a ‘stack pointer’. And it supports the operation including: a) I NSERT adds elements to the end of the sequence; b) P OP moves the stack pointer to the previous element; c) Q UERY returns the output vector where the stack pointer points. Among these three operation, P OP and Q UERY only manipulates the stack without complex comp"
K19-2007,D19-1277,0,0.0864988,"Missing"
K19-2007,P13-1104,0,0.0184641,"ame as DM and PSD. R EDUCE pops the stack, to allow removing a node once all its edges have been created. • N ODE transition creates new non-terminal nodes. For every X ∈ L, N ODEX creates a new node on the buffer as a parent of the first element on the stack, with an X-labeled edge. Transition Systems Building on previous work on parsing reentrancies, discontinuities, and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties. To solve cross-arc problem, we use list-based arceager algorithm for DM, PSD, and EDS framework as Choi and McCallum (2013); Nivre (2003, 2008); for UCCA framework, we employ S WAP operation to generate cross-arc as Hershcovich et al. (2017).4 3.1 UCCA • L EFT-E DGEX and R IGHT-E DGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. • L EFT-R EMOTEX and R IGHT-R EMOTEX do not have this restriction, and the created edge is additionally marked as remote. DM and PSD • S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Maier, 2015; Nivre, 2009"
K19-2007,P19-1450,0,0.241094,"Missing"
K19-2007,N19-1423,0,0.584823,"st-processing buffer stack transition-based parser with stack LSTM classifier deque transition system action history pos lemma frame tagger tagger tagger BERT Figure 1: A unified pipeline for meaning representation parsing, including transition-based parser, BERT-enhanced word representation, and post-processing, along with the additional taggers for label the nodes with pos, frame and lemma. 2.1 pects: 1) Efficient Training Aligning the homogeneous operation in stack LSTM within a batch and then computing them simultaneously; 2) Effective Encoding Fine-tuning the parser with pretrained BERT (Devlin et al., 2019) embedding, which enrich the context information to make accurate local decisions, and global learning for exact search. Together with the post-processing, we developed a unified pipeline for meaning representation parsing. Our contribution can be summarised as follows: In order to design the unified transition-based parser, we refer to the following frameworkspecific parsers: Wang et al. (2018b) for DM and PSD, Hershcovich et al. (2017) for UCCA, Buys and Blunsom (2017) for EDS, Liu et al. (2018) for AMR. Those parsers differ in the design of transition system to generate oracle action sequen"
K19-2007,D18-1264,1,0.946369,"puting them simultaneously; 2) Effective Encoding Fine-tuning the parser with pretrained BERT (Devlin et al., 2019) embedding, which enrich the context information to make accurate local decisions, and global learning for exact search. Together with the post-processing, we developed a unified pipeline for meaning representation parsing. Our contribution can be summarised as follows: In order to design the unified transition-based parser, we refer to the following frameworkspecific parsers: Wang et al. (2018b) for DM and PSD, Hershcovich et al. (2017) for UCCA, Buys and Blunsom (2017) for EDS, Liu et al. (2018) for AMR. Those parsers differ in the design of transition system to generate oracle action sequence, but similar in modeling the parsing state. A tuple (S, L, B, E, V ) is used to represent parsing state, where S is a stack holding processed words, L is a list holding words popped out of S that will be pushed back in the future, and B is a buffer holding unprocessed words. E is a set of labeled dependency arcs. V is a set of graph nodes include concept nodes and surface tokens. The initial state is ([0], [ ], [1, · · · , n], [ ], V ) , where V only contains surface tokens since the concept no"
K19-2007,P15-1116,0,0.0497308,"Choi and McCallum (2013); Nivre (2003, 2008); for UCCA framework, we employ S WAP operation to generate cross-arc as Hershcovich et al. (2017).4 3.1 UCCA • L EFT-E DGEX and R IGHT-E DGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. • L EFT-R EMOTEX and R IGHT-R EMOTEX do not have this restriction, and the created edge is additionally marked as remote. DM and PSD • S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Maier, 2015; Nivre, 2009). We follow the work of (Wang et al., 2018b) to design transition system for DM and PSD. 3 For further explanation, please visit the official website:http://mrp.nlpl.eu/index.php?page=5 4 The transition sets for each framework have been introduced with table format in supplementary material. • F INISH pops the root node and marks the state as terminal. 79 special single tokens if needed using operation M ERGE. (b) Then we use operation C ONFIRM to convert a single token on buffer to a graph node(concept). In order to process entity concepts like date-entity better, operation E NT"
K19-2007,P18-2077,0,0.0493883,"ir.hit.edu.cn Abstract et al., 2013) represents the meaning of each word using a concept graph. Koller et al. (2019) classifies these frameworks into three flavors of semantic graphs, based on the degree of alignment between the tokens and the graph nodes. In DM and PSD, nodes are sub-set of surface tokens; in EDS and UCCA, graph nodes are explicitly aligned with the tokens; in AMR, the alignments are implicit. Most semantic parsers are only designed for one or few specific graphbanks, due to the differences in annotation schemes. For example, the currently best parser for SDP is graph-based (Dozat and Manning, 2018), which assumes dependency graphs but cannot be directly applied to UCCA, EDS, and AMR, due the existence of concept node. Hershcovich et al. (2018) parses across different semantic graphbanks (UCCA, DM, AMR), but only works well on UCCA. The system of Buys and Blunsom (2017) is a good data-driven EDS parser, but does poorly on AMR. Lindemann et al. (2019) sets a new SOTA in DM, PAS, PSD, AMR and nearly SOTA in EDS, via representing each graph with the compositional tree structure (Groschwitz et al., 2017), but they do not expand this method to UCCA. Learning from multiple flavors of meaning r"
K19-2007,W11-2927,0,0.134231,"in DM framework, and it ranks 2nd in DM. Amazon achieves 1st in AMR. We only list the involved results they submitted. Feature DM LF1 MRP PSD LF1 MRP UCCA LF1 MRP EDS EDM MRP GloVe BERT(base) 87.1 94.3 74.1 83.6 56.3 64.3 82.5 87.6 87.3 90.5 73.7 76.7 87.5 92.8 88.2 91.5 AMR SMATCH MRP 64.8 71.0 65.3 71.4 Table 2: HIT-SCIR parser results on MRP split dataset with GloVe or BERT as pretrained word representation. MRP stands for cross-framework evaluation metric. LF1 stands for SDP Labeled F1 (Oepen et al., 2014) in DM/PSD, UCCA Labeled Dependency F1 (Hershcovich et al., 2019) in UCCA. And EDM (Dridan and Oepen, 2011) stands for Elementary Dependency Match in EDS. SMATCH (Cai and Knight, 2013) is an evaluation metric for semantic feature structures in AMR. groups differ in learning rate. For training we use Adam (Kingma and Ba, 2015). Code for our parser and model weights are available at https://github.com/DreamerDeo/ HIT-SCIR-CoNLL2019. as nodes and edges, we need an extra procedure to recognize which nodes should be properties in the final result. Once recognized, node along with the corresponding edge will be converted to the property of its parent node, edge label for the key, and node label for the v"
K19-2007,W03-3017,0,0.213811,"pops the stack, to allow removing a node once all its edges have been created. • N ODE transition creates new non-terminal nodes. For every X ∈ L, N ODEX creates a new node on the buffer as a parent of the first element on the stack, with an X-labeled edge. Transition Systems Building on previous work on parsing reentrancies, discontinuities, and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties. To solve cross-arc problem, we use list-based arceager algorithm for DM, PSD, and EDS framework as Choi and McCallum (2013); Nivre (2003, 2008); for UCCA framework, we employ S WAP operation to generate cross-arc as Hershcovich et al. (2017).4 3.1 UCCA • L EFT-E DGEX and R IGHT-E DGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. • L EFT-R EMOTEX and R IGHT-R EMOTEX do not have this restriction, and the created edge is additionally marked as remote. DM and PSD • S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Maier, 2015; Nivre, 2009). We follow"
K19-2007,P15-1033,0,0.0217357,"n 2.3. At last, to label the nodes with pos, frame and lemma, we use additional tagger models to predict these in Section 2.4. The framework-specific transition system is presented in Section 3 and post-processing for each framework is discussed in Section 4. p(a|s) = P exp{ga · STACK LSTM(s) + ba } , a0 exp{ga0 · STACK LSTM(s) + ba0 } where STACK LSTM(s) encodes the state s into a vector, ga and ba are embedding vector, bias vector of action a respectively. The oracle transition action sequence is obtained through transition system, proposed in in Section 3. 2 We encourage the reader to read Dyer et al. (2015) for more details. 77 operation a a b b c stacks buffer batch LSTM states a state a b state b c state c state a state b state c c Figure 2: When some new I NSERT operations come, the data to be inserted are pushed into corresponding buffers. They will be merged into a batch once batch-processing is triggered. After that, new LSTM states will be pushed to corresponding stacks. 2.2 Batch Training there is already an I NSERT in the buffer; b) operation P OP or Q UERY comes. To clarify, the depth of buffer per data is 1. Kiperwasser and Goldberg (2016) shows that batch training increases the gradi"
K19-2007,J08-4003,0,0.0593101,"Missing"
K19-2007,W17-6810,0,0.0838011,"Missing"
K19-2007,K19-2001,0,0.278585,"Missing"
K19-2007,P17-1104,0,0.563505,"g the homogeneous operation in stack LSTM within a batch and then computing them simultaneously; 2) Effective Encoding Fine-tuning the parser with pretrained BERT (Devlin et al., 2019) embedding, which enrich the context information to make accurate local decisions, and global learning for exact search. Together with the post-processing, we developed a unified pipeline for meaning representation parsing. Our contribution can be summarised as follows: In order to design the unified transition-based parser, we refer to the following frameworkspecific parsers: Wang et al. (2018b) for DM and PSD, Hershcovich et al. (2017) for UCCA, Buys and Blunsom (2017) for EDS, Liu et al. (2018) for AMR. Those parsers differ in the design of transition system to generate oracle action sequence, but similar in modeling the parsing state. A tuple (S, L, B, E, V ) is used to represent parsing state, where S is a stack holding processed words, L is a list holding words popped out of S that will be pushed back in the future, and B is a buffer holding unprocessed words. E is a set of labeled dependency arcs. V is a set of graph nodes include concept nodes and surface tokens. The initial state is ([0], [ ], [1, · · · , n], [ ], V"
K19-2007,S15-2153,0,0.237924,"Missing"
K19-2007,P18-1035,0,0.306636,"Missing"
K19-2007,S14-2008,0,0.162509,"Missing"
K19-2007,P17-1186,0,0.0644995,"Missing"
K19-2007,N18-1135,0,0.0708793,"Missing"
K19-2007,D14-1162,0,0.0840185,"depth of buffer per data is 1. Kiperwasser and Goldberg (2016) shows that batch training increases the gradient stability and speeds up the training. Delaying the backward to simulate mini-batch update is a simple way to realize batch training, but it fails to compute over data in parallel. To solve this, we propose a method of maintaining stack LSTM structure and using operation buffer. 2.3 2.3.1 BERT-Enhance Word Representation Deep Contextualized Word Representations Neural parsers often use pretrained word embeddings as their primary input, i.e. word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which assign a single static representation to each word so that they cannot capture context-dependent meaning. By contrast, deep contextualized word representations, i.e. ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), encode words with respect to the context, which have been proven to be useful for many NLP tasks, achieving state-of-the-art performance in standard Natural Language Understanding (NLU) benchmarks, such as GLUE (Wang et al., 2018a). Che et al. (2018) adopted ELMo in CoNLL 2018 shared task (Zeman et al., 2018) and achieved first prize in terms of LAS metric. (Kondra"
K19-2007,N18-1202,0,0.0436837,"s a simple way to realize batch training, but it fails to compute over data in parallel. To solve this, we propose a method of maintaining stack LSTM structure and using operation buffer. 2.3 2.3.1 BERT-Enhance Word Representation Deep Contextualized Word Representations Neural parsers often use pretrained word embeddings as their primary input, i.e. word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), which assign a single static representation to each word so that they cannot capture context-dependent meaning. By contrast, deep contextualized word representations, i.e. ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), encode words with respect to the context, which have been proven to be useful for many NLP tasks, achieving state-of-the-art performance in standard Natural Language Understanding (NLU) benchmarks, such as GLUE (Wang et al., 2018a). Che et al. (2018) adopted ELMo in CoNLL 2018 shared task (Zeman et al., 2018) and achieved first prize in terms of LAS metric. (Kondratyuk and Straka, 2019) exceeds the state-ofthe-art in UD with fine-tuning model with BERT. stack LSTM The stack LSTM augments the conventional LSTM with a ‘stack pointer’. And it supports the operatio"
K19-2007,C08-1095,0,0.238116,"ous concepts. If there is a concept node on top of buffer, operation N EW can be performed to parse this kind of concept nodes. After solving the problem of parsing concept nodes from surface string, the basic transition set used in DM and PSD is able to predict edges between concept nodes. As a UCCA node may only have one incoming primary edge, E DGE transitions are disallowed if the child node already has an incoming primary edge. To support the prediction of multiple parents, node and edge transitions leave the stack unchanged, as in other work on transition-based dependency graph parsing (Sagae and Tsujii, 2008). 3.3 EDS Based on the work of (Buys and Blunsom, 2017), we extended N ODE -S TARTL and N ODE -E ND actions for generating concept node and realizing node alignment. To clarify, wi is the top element in stack and wj is the top element in buffer. Moreover, wi could only be concept node (stack and list only contain concept node), and wj could be concept node or surface token. • R EDUCE and PASS operations are the same as DM and PSD. • S HIFT, L EFT-E DGEX and R IGHT-E DGEX are similar to operations in DM and PSD, but they can be performed only when the top of buffer is a concept node. • S HIFT a"
K19-2007,W18-5446,0,0.259079,"pects: 1) Efficient Training Aligning the homogeneous operation in stack LSTM within a batch and then computing them simultaneously; 2) Effective Encoding Fine-tuning the parser with pretrained BERT (Devlin et al., 2019) embedding, which enrich the context information to make accurate local decisions, and global learning for exact search. Together with the post-processing, we developed a unified pipeline for meaning representation parsing. Our contribution can be summarised as follows: In order to design the unified transition-based parser, we refer to the following frameworkspecific parsers: Wang et al. (2018b) for DM and PSD, Hershcovich et al. (2017) for UCCA, Buys and Blunsom (2017) for EDS, Liu et al. (2018) for AMR. Those parsers differ in the design of transition system to generate oracle action sequence, but similar in modeling the parsing state. A tuple (S, L, B, E, V ) is used to represent parsing state, where S is a stack holding processed words, L is a list holding words popped out of S that will be pushed back in the future, and B is a buffer holding unprocessed words. E is a set of labeled dependency arcs. V is a set of graph nodes include concept nodes and surface tokens. The initial"
K19-2007,K18-2001,0,0.0649206,"Missing"
L16-1180,bestgen-2008-building,0,0.785704,"(2004) used WordNet (Miller et al. 1995) to assign positive or negative polarity to words using synonyms and antonyms for a small set of seed words, however, such a method is limited by the set of seed words chosen. Esuli and Sebastini (2006) used semi-supervised learning to create SentiWordNet, where potentially every word in WordNet would be assigned a sentiment score, although many words actually may not be sentiment-bearing (cf. Taboada, 2011 for further discussion). A number of approaches use semantic proximity of words in variations of Latent Semantic Analysis (Turney and Littman, 2003; Bestgen, 2008; Bestgen and Vincze, 2012); however, their self-reported correlations of proposed expansions against human ratings are not sufficiently robust. Neilson (2011) created a new ANEW specifically geared towards detecting sentiment in microblog posts, but it only contains 2477 words scored manually on a scale of +2 (positive) to -2 (negative). 3. Approach Our expansion method follows that adopted by Liu et al. (2014) for their automated expansion of the MRC psycholinguistic database. We use WordNet (Miller, 1995), a large English lexical database with over 150,000 words, hierarchically organized in"
L16-1180,esuli-sebastiani-2006-sentiwordnet,0,0.112714,"Missing"
L16-1180,C04-1200,0,0.0155118,"e our method of automatically expanding an existing affective lexicon for English. Our approach is general enough to apply to any specialized lexicon in any language where a partial resource exists. We also describe a method of automatically creating lexicons for a new language where no prior resources exist or are very limited; specifically, for Spanish, Russian and Farsi. We also describe the validation procedures used to ensure validity of these lexicons1. 2. Related Work There has been prior work in the automatic construction and expansion of affective lexicons using different techniques. Kim and Hovy (2004) used WordNet (Miller et al. 1995) to assign positive or negative polarity to words using synonyms and antonyms for a small set of seed words, however, such a method is limited by the set of seed words chosen. Esuli and Sebastini (2006) used semi-supervised learning to create SentiWordNet, where potentially every word in WordNet would be assigned a sentiment score, although many words actually may not be sentiment-bearing (cf. Taboada, 2011 for further discussion). A number of approaches use semantic proximity of words in variations of Latent Semantic Analysis (Turney and Littman, 2003; Bestge"
L16-1180,J11-2001,0,0.0249183,"Missing"
L16-1180,1996.amta-1.36,0,0.492001,"Missing"
L16-1594,W13-0909,1,0.901166,"Missing"
L18-1431,C16-1167,1,0.868173,"Missing"
L18-1431,P17-1055,1,0.897104,"ed their evaluation systems for testing their reading comprehension models on our newly developed dataset, suggesting its potential impact. We hope the release of the 1 CMRC 2017 Public Datasets: https://github.com/ ymcui/cmrc2017. dataset to the public will accelerate the progress of Chinese research community on machine reading comprehension field. We also provide four official baselines for the evaluations, including two traditional baselines and two neural baselines. In this paper, we adopt two widely used neural reading comprehension model: AS Reader (Kadlec et al., 2016) and AoA Reader (Cui et al., 2017). The rest of the paper will be organized as follows. In Section 2, we will introduce the related works on the reading comprehension dataset, and then the proposed dataset as well as our competitions will be illustrated in Section 3. The baseline and participant system results will be given in Section 4 and we will made a brief conclusion at the end of this paper. 2. Related Works In this section, we will introduce several public cloze-style reading comprehension dataset. 2.1. CNN/Daily Mail Some news articles often come along with a short summary or brief introduction. Inspired by this, Herma"
L18-1431,P16-1086,0,0.457629,"ly there were 17 participants submitted their evaluation systems for testing their reading comprehension models on our newly developed dataset, suggesting its potential impact. We hope the release of the 1 CMRC 2017 Public Datasets: https://github.com/ ymcui/cmrc2017. dataset to the public will accelerate the progress of Chinese research community on machine reading comprehension field. We also provide four official baselines for the evaluations, including two traditional baselines and two neural baselines. In this paper, we adopt two widely used neural reading comprehension model: AS Reader (Kadlec et al., 2016) and AoA Reader (Cui et al., 2017). The rest of the paper will be organized as follows. In Section 2, we will introduce the related works on the reading comprehension dataset, and then the proposed dataset as well as our competitions will be illustrated in Section 3. The baseline and participant system results will be given in Section 4 and we will made a brief conclusion at the end of this paper. 2. Related Works In this section, we will introduce several public cloze-style reading comprehension dataset. 2.1. CNN/Daily Mail Some news articles often come along with a short summary or brief int"
L18-1431,P17-1010,1,0.847243,"ular framework for cloze-style reading comprehension. Apart from setting embedding and hidden layer size as 256, we did not change other hyperparameters and experimental setups as used in Kadlec et al. (2016), nor we tuned the system for further improvements. • AoA Reader: We also implemented Attention-overAttention Reader (AoA Reader) (Cui et al., 2017) which is the state-of-the-art model for cloze-style reading comprehension. We follow hyper-parameter settings in AS Reader baseline without further tuning. In the User Query Track, as there is a gap between training and validation, we follow (Liu et al., 2017) and regard this task as domain adaptation or transfer learning problem. The neural baselines are built by the following steps. Validation Test Baseline - Random Guess Baseline - Top Frequency Baseline - AS Reader Baseline - AoA Reader 1.50 10.65 - 1.47 8.73 49.03 51.53 ECNU (Ensemble) ECNU (single model) Shanxi University (Team-3) Zhengzhou University 90.45 85.55 47.80 31.10 69.53 65.77 49.07 32.53 Table 3: Results on User Query Track. Due to the using of validation data, we did not report its performance. • We first use the shared training data to build a general systems, and choose the best"
L18-1431,D16-1264,0,0.22547,"Missing"
L18-1431,C10-3004,1,0.83444,"X”. The detailed procedures can be illustrated as follows. 3.4.1. Cloze-style Reading Comprehension For the validation and test set in cloze data, we first randomly choose 5,000 paragraphs each for automatically generating questions using the techniques mentioned above. Then we invite our resource team to manually select 2,000 questions based on the following rules. • Whether the question is appropriate and correct • Whether the question is hard for LMs to answer • Pre-processing: For each sentence in the document, we do word segmentation, POS tagging and dependency parsing using LTP toolkit (Che et al., 2010). • Dependency Extraction: Extract following dependencies: COO, SBV, VOB, HED, FOB, IOB, POB5 , and only preserve the parts that have dependencies. • Further Filtering: Only preserve SBV, VOB and restrict the related words not to be pronouns and verbs. • Frequency Restriction: After calculating word frequencies, only word frequency that greater than 2 is valid for generating question. • Only select one question for each paragraph 3.4.2. User Query Reading Comprehension Unlike the cloze dataset, we have no automatic question generation procedure in this type. In the user query dataset, we asked"
L18-1431,P16-1223,0,0.0376137,"lable at http://cs.nyu.edu/˜kcho/DMQA/ 2721 Cloze Track Validation Test Train # Query Max # tokens in docs Max # tokens in query Avg # tokens in docs Avg # tokens in query Vocabulary 354,295 486 184 324 27 2,000 481 72 321 19 3,000 484 106 307 23 94,352 User Query Track Validation Test 2,000 481 21 310 8 3,000 486 29 290 8 Table 1: Statistics of the dataset for the 1st Evaluation on Chinese Machine Reading Comprehension (CMRC-2017). as we have known that world knowledge is very important when we do reading comprehension in reality, which makes this dataset much artificial than real situation. Chen et al. (2016) also showed that the proposed anonymization in CNN/Daily Mail dataset is less useful, and the current models (Kadlec et al., 2016; Chen et al., 2016) are nearly reaching ceiling performance with the automatically generated dataset which contains much errors, such as coreference errors, ambiguous questions etc. is accordance with the training data which is also automatically generated and they share many similar characteristics, which is not the case when it comes to human-annotated data. 2.2. 3.1. Children’s Book Test Another popular cloze-style reading comprehension dataset is the Children’s"
liu-etal-2012-extending,shaikh-etal-2010-mpc,1,\N,Missing
liu-etal-2012-extending,W09-3404,0,\N,Missing
liu-etal-2012-extending,song-etal-2010-enhanced,0,\N,Missing
liu-etal-2014-automatic-expansion,W13-0909,1,\N,Missing
N10-1030,J96-1002,0,0.0190661,"an Chapter of the ACL, pages 246–249, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics trees into dependence trees with an Constituent-toDependency Conversion Tool2 . In addition, we also convert the OntoNotes sense of each polysemant into WordNet sense using sense inventory file provided by OntoNotes 2.0. For an OntoNotes sense with more than one WordNet sense, we simply use the foremost (more popular) one. 3 Semantic Role Labeling System Our baseline is a state-of-the-art SRL system based on dependency syntactic tree (Che et al., 2009). A maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence to be each semantic role. A virtual role “NULL” (presenting none of roles is assigned) is added to the roles set, so it does not need semantic role identification stage anymore. For a predicate, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 1. problem. As for the POS, it represents the syntactic information, but is not enough to dist"
N10-1030,D07-1007,0,0.0482077,"a hot task in natural language processing. SRL aims at identifying the relations between the predicates in a sentence and their associated arguments. At present, the main stream researches are focusing on feature engineering or combination of multiple results. Word senses are important information for recognizing semantic roles. For example, if we know “cat” is an “agent” of the predicate “eat” in a sentence, we can guess that “dog” can also be an “agent” of “eat”. Word sense has been successfully used in many natural language processing tasks, such as machine translation (Chan et al., 2007; Carpuat and Wu, 2007). CoNLL 2008 shared task (Surdeanu et al., 2008) first introduced the predicate classification task, which can be regarded as the predicate sense disambiguation. Meza-Ruiz and Riedel (2009) has shown that the predicate sense can improve the final SRL performance. However, there is few discussion about the concrete influence of all word senses, i.e. the words besides predicates. The major reason is lacking the corpus, which is both annotated with all word senses and semantic roles. The release of OntoNotes corpus provides an opportunity for us to verify whether all word senses can help SRL. Ont"
N10-1030,P07-1005,0,0.0880655,"Missing"
N10-1030,W09-1207,1,0.868361,"Annual Conference of the North American Chapter of the ACL, pages 246–249, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics trees into dependence trees with an Constituent-toDependency Conversion Tool2 . In addition, we also convert the OntoNotes sense of each polysemant into WordNet sense using sense inventory file provided by OntoNotes 2.0. For an OntoNotes sense with more than one WordNet sense, we simply use the foremost (more popular) one. 3 Semantic Role Labeling System Our baseline is a state-of-the-art SRL system based on dependency syntactic tree (Che et al., 2009). A maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a word in the sentence to be each semantic role. A virtual role “NULL” (presenting none of roles is assigned) is added to the roles set, so it does not need semantic role identification stage anymore. For a predicate, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 1. problem. As for the POS, it represents the syntact"
N10-1030,N09-1037,0,0.0933089,"Missing"
N10-1030,N06-2015,0,0.181376,"Missing"
N10-1030,N09-1018,0,0.0452269,"arches are focusing on feature engineering or combination of multiple results. Word senses are important information for recognizing semantic roles. For example, if we know “cat” is an “agent” of the predicate “eat” in a sentence, we can guess that “dog” can also be an “agent” of “eat”. Word sense has been successfully used in many natural language processing tasks, such as machine translation (Chan et al., 2007; Carpuat and Wu, 2007). CoNLL 2008 shared task (Surdeanu et al., 2008) first introduced the predicate classification task, which can be regarded as the predicate sense disambiguation. Meza-Ruiz and Riedel (2009) has shown that the predicate sense can improve the final SRL performance. However, there is few discussion about the concrete influence of all word senses, i.e. the words besides predicates. The major reason is lacking the corpus, which is both annotated with all word senses and semantic roles. The release of OntoNotes corpus provides an opportunity for us to verify whether all word senses can help SRL. OntoNotes is a large corpus annotated with constituency trees (based on Penn Treebank), predicate argument structures (based on Penn PropBank) and word senses. It has been used in some natural"
N10-1030,W08-2121,0,0.032221,"Missing"
N10-1030,D08-1105,0,0.126758,"Missing"
N10-1059,H05-1043,0,0.0267018,"Missing"
N10-1059,H05-2017,0,\N,Missing
N13-1006,P11-1062,0,0.0227296,"e complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale OntoNotes Chinese-English parallel NER data showed that Soft-align method, which allows"
N13-1006,J92-4003,0,0.0240595,"rson), LOC (Location), Chinese NER Templates 00: 1 (class bias param) 01: wi+k , −1 ≤ k ≤ 1 02: wi+k−1 ◦ wi+k , 0 ≤ k ≤ 1 03: shape(wi+k ), −4 ≤ k ≤ 4 04: prefix(wi , k), 1 ≤ k ≤ 4 05: prefix(wi−1 , k), 1 ≤ k ≤ 4 06: suffix(wi , k), 1 ≤ k ≤ 4 07: suffix(wi−1 , k), 1 ≤ k ≤ 4 08: radical(wi , k), 1 ≤ k ≤ len(wi ) Unigram Features yi ◦ 00 – 08 Bigram Features yi−1 ◦ yi ◦ 00 – 08 NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). To our knowledge, this work is the first use of word clustering features for Chinese NER. A C++ implementation of the Brown word clustering algorithms (Brown et al., 1992) was used to obtain the word clusters (Liang, 2005).8 Raw text was obtained from the fifth edition of Chinese Gigaword (LDC2011T13). One million paragraphs from Xinhua news section were randomly selected, and the Stanford Word Segmenter with LDC standard was applied to segment Chinese text into words.9 About 46 million words were obtained which were clustered into 1,000 word classes. Table 1: Basic features of Chinese NER. 5 ORG (Organization) and GPE (Geo-Political Entities), and discarded the others. Since the bilingual corpus is only aligned at the document level, we performed sentence alig"
N13-1006,W10-2906,0,0.0980242,"ual cues can help to recognize errors a monolingual tagger would make, allowing us to produce more accurately tagged bitext. Each side of the tagged bitext can then be used to expand the original monolingual training dataset, which may lead to higher accuracy in the monolingual taggers. Previous work such as Li et al. (2012) and Kim et al. (2012) demonstrated that bilingual corpus annotated with NER labels can be used to improve monolingual tagger performance. But a major drawback of their approaches are the need for manual annotation efforts to create such corpora. To avoid this requirement, Burkett et al. (2010) suggested a “multi-view” learning scheme based on re-ranking. Noisy output of a “strong” tagger is used as training data to learn parameters of a log-linear re-ranking model with additional bilingual features, simulated by a “weak” tagger. The learned parameters are then reused with the “strong” tagger to re-rank its own outputs for unseen inputs. Designing good “weak” taggers so that they complement the “view” of bilingual features in the log-linear re-ranker is crucial to the success of this algorithm. Unfortunately there is no principled way of designing such “weak” taggers. In this paper,"
N13-1006,P10-1065,0,0.0271909,"d the difference discussed in Section 1, their re-ranking strategy may lose the correct named entity results if they are not included in the top-N outputs. Furthermore, we consider the word alignment probabilities in our method which can reduce the influence of word alignment errors. Finally, we test our method on a large standard publicly available corpus (8,249 sentences), while they used a much smaller (200 sentences) manually annotated bilingual NER corpus for results validation. In addition to bilingual corpora, bilingual dictionaries are also useful resources. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities"
N13-1006,P11-1061,0,0.0689837,"ults validation. In addition to bilingual corpora, bilingual dictionaries are also useful resources. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Depende"
N13-1006,1993.eamt-1.1,0,0.475415,"Missing"
N13-1006,P05-1045,1,0.0203067,"Geo-Political Entities), and discarded the others. Since the bilingual corpus is only aligned at the document level, we performed sentence alignment using the Champollion Tool Kit (CTK).4 After removing sentences with no aligned sentence, a total of 8,249 sentence pairs were retained. We used the BerkeleyAligner,5 to produce word alignments over the sentence-aligned datasets. BerkeleyAligner also gives posterior probabilities Pa for each aligned word pair. We used the CRF-based Stanford NER tagger (using Viterbi decoding) as our baseline monolingual NER tool.6 English features were taken from Finkel et al. (2005). Table 1 lists the basic features of Chinese NER, where ◦ means string concatenation and yi is the named entity tag of the ith word wi . Moreover, shape(wi ) is the shape of wi , such as date and number. prefix/suffix(wi , k) denotes the k-characters prefix/suffix of wi . radical(wi , k) denotes the radical of the k th Chinese character of wi .7 len(wi ) is the number of Chinese characters in wi . To make the baseline CRF taggers stronger, we added word clustering features to improve generalization over unseen data for both Chinese and English. Word clustering features have been successfully"
N13-1006,I11-1030,1,0.656986,"es. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and"
N13-1006,N06-2015,0,0.0588506,"al constraints. We can re-express Eq. (13) as follows: Y Y (λayc ye )Ia (18) λyac ye = X X X XX j zjy log Pjy y∈Y zayc ye Pa log λyac ye (19) a∈A yc ∈Y ye ∈Y We name the set of constraints above Soft-align, which has the same constraints as Soft-tag, i.e., Eqs. (8), (9), (15) and (16). 4 Experimental Setup We conduct experiments on the latest OntoNotes 4.0 corpus (LDC2011T03). OntoNotes is a large, manually annotated corpus that contains various text genres and annotations, such as part-of-speech tags, named entity labels, syntactic parse trees, predicateargument structures and co-references (Hovy et al., 2006). Aside from English, this corpus also contains several Chinese and Arabic corpora. Some of these corpora contain bilingual parallel documents. We used the Chinese-English parallel corpus with named entity labels as our development and test data. This corpus includes about 400 document pairs (chtb 0001-0325, ectb 1001-1078). We used oddnumbered documents as development data and evennumbered documents as test data. We used all other portions of the named entity annotated corpus as training data for the monolingual systems. There were a total of ∼660 Chinese documents (∼16k sentences) and ∼1,400"
N13-1006,D09-1127,0,0.013019,"Missing"
N13-1006,P12-1073,0,0.292443,"ut entities. For example, in Figure 1, the word “本 (Ben)” is common in Chinese but rarely appears as a translated foreign name. However, its aligned word on the English side (“Ben”) provides a strong clue that this is a person name. Judicious use of this type of bilingual cues can help to recognize errors a monolingual tagger would make, allowing us to produce more accurately tagged bitext. Each side of the tagged bitext can then be used to expand the original monolingual training dataset, which may lead to higher accuracy in the monolingual taggers. Previous work such as Li et al. (2012) and Kim et al. (2012) demonstrated that bilingual corpus annotated with NER labels can be used to improve monolingual tagger performance. But a major drawback of their approaches are the need for manual annotation efforts to create such corpora. To avoid this requirement, Burkett et al. (2010) suggested a “multi-view” learning scheme based on re-ranking. Noisy output of a “strong” tagger is used as training data to learn parameters of a log-linear re-ranking model with additional bilingual features, simulated by a “weak” tagger. The learned parameters are then reused with the “strong” tagger to re-rank its own out"
N13-1006,P08-1068,0,0.0356057,"es). OntoNotes annotates 18 named entity types, such as person, location, date and money. In this paper, we selected the four most common named entity types, i.e., PER (Person), LOC (Location), Chinese NER Templates 00: 1 (class bias param) 01: wi+k , −1 ≤ k ≤ 1 02: wi+k−1 ◦ wi+k , 0 ≤ k ≤ 1 03: shape(wi+k ), −4 ≤ k ≤ 4 04: prefix(wi , k), 1 ≤ k ≤ 4 05: prefix(wi−1 , k), 1 ≤ k ≤ 4 06: suffix(wi , k), 1 ≤ k ≤ 4 07: suffix(wi−1 , k), 1 ≤ k ≤ 4 08: radical(wi , k), 1 ≤ k ≤ len(wi ) Unigram Features yi ◦ 00 – 08 Bigram Features yi−1 ◦ yi ◦ 00 – 08 NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). To our knowledge, this work is the first use of word clustering features for Chinese NER. A C++ implementation of the Brown word clustering algorithms (Brown et al., 1992) was used to obtain the word clusters (Liang, 2005).8 Raw text was obtained from the fifth edition of Chinese Gigaword (LDC2011T13). One million paragraphs from Xinhua news section were randomly selected, and the Stanford Word Segmenter with LDC standard was applied to segment Chinese text into words.9 About 46 million words were obtained which were clustered into 1,000 word classes. Table 1: Basic features of Chinese NER."
N13-1006,P09-1039,0,0.0137924,"above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale OntoNotes Chinese-English parallel NER data"
N13-1006,J94-2001,0,0.580131,"Missing"
N13-1006,N04-1043,0,0.0626188,"s) and ∼1,400 English documents (∼39k sentences). OntoNotes annotates 18 named entity types, such as person, location, date and money. In this paper, we selected the four most common named entity types, i.e., PER (Person), LOC (Location), Chinese NER Templates 00: 1 (class bias param) 01: wi+k , −1 ≤ k ≤ 1 02: wi+k−1 ◦ wi+k , 0 ≤ k ≤ 1 03: shape(wi+k ), −4 ≤ k ≤ 4 04: prefix(wi , k), 1 ≤ k ≤ 4 05: prefix(wi−1 , k), 1 ≤ k ≤ 4 06: suffix(wi , k), 1 ≤ k ≤ 4 07: suffix(wi−1 , k), 1 ≤ k ≤ 4 08: radical(wi , k), 1 ≤ k ≤ len(wi ) Unigram Features yi ◦ 00 – 08 Bigram Features yi−1 ◦ yi ◦ 00 – 08 NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). To our knowledge, this work is the first use of word clustering features for Chinese NER. A C++ implementation of the Brown word clustering algorithms (Brown et al., 1992) was used to obtain the word clusters (Liang, 2005).8 Raw text was obtained from the fifth edition of Chinese Gigaword (LDC2011T13). One million paragraphs from Xinhua news section were randomly selected, and the Stanford Word Segmenter with LDC standard was applied to segment Chinese text into words.9 About 46 million words were obtained which were clustered into 1,000 word classes"
N13-1006,D10-1069,0,0.0403525,"Missing"
N13-1006,C04-1197,0,0.0188073,"alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale"
N13-1006,N01-1026,0,0.135301,"Finally, we test our method on a large standard publicly available corpus (8,249 sentences), while they used a much smaller (200 sentences) manually annotated bilingual NER corpus for results validation. In addition to bilingual corpora, bilingual dictionaries are also useful resources. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Const"
N13-1006,D10-1030,0,0.0481815,"al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale OntoNotes Chinese-English parallel NER data showed that Soft-align method, which allows inconsistent named entit"
N15-1115,J08-1001,0,0.310993,"parts of the text; therefore, it misses the contribution from semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft). In this paper, we capture such semantic relatedness by leveraging world knowledge (e.g., Gates is the person who created Microsoft), and use two existing evaluation frameworks. First, in the unsupervised framework, we introduce semantic relatedness as an enrichment to the original graph-based model of Guinaudeau and Strube (2013). In addition, we incorporate semantic relatedness as additional features into the popular entity-based model of Barzilay and Lapata (2008). Across both frameworks, our enriched model with semantic relatedness outperforms the original methods, especially on short documents. 1 Introduction In a well-written document, sentences are organized and presented in a logical and coherent form, which makes the text fluent and easily understood. Therefore, coherence is a fundamental aspect of high text quality, and the evaluation of coherence is a crucial component of many NLP applications, such as essay scoring (Miltsakaki and Kukich, 2004), story generation (McIntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗"
N15-1115,P08-2011,0,0.0781101,"Missing"
N15-1115,P11-2022,0,0.66887,"kaki and Kukich, 2004), story generation (McIntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗ This work was partly done while the first author was visiting University of Toronto. A particularly popular model for evaluating text coherence is the entity-based local coherence model of Barzilay and Lapata (2008) (B&L), which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention. Following this direction, a number of extensions have been proposed (Elsner and Charniak, 2008; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), the majority of which focus on enriching the original entity features. An exception is the unsupervised model of Guinaudeau and Strube (2013) (G&S), which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph. However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities. For example, the text in Figure 1a1 has no common e"
N15-1115,D11-1142,0,0.0124592,"trieve all world knowledge related to d. There are two major issues for this process: (1) knowledge sources: where can we obtain this knowledge?, and (2) knowledge selection: how do we pinpoint the most relevant ones? Knowledge sources There are two main kinds of knowledge sources: (1) manually edited knowledge 1089 bases, such as YAGO (Hoffart et al., 2013), which consists of about 4 million human-edited instances from on-line encyclopedias such as WikiPedia (Denoyer and Gallinari, 2007) and FreeBase (Bollacker et al., 2008), and (2) automatically constructed knowledge bases, such as Reverb (Fader et al., 2011), which covers about 20 million instances extracted from raw texts. Generally speaking, manually edited knowledge bases have better accuracy but lower coverage, while automatically extracted knowledge bases are the opposite. To seek a good balance, we use both YAGO and Reverb as our knowledge sources. In addition, the automatically constructed knowledge bases can be extracted from raw texts of any domain, which makes our method adaptable. Both sources are presented in triples, argument1 -predicate-argument2 , (e.g., Gates-create-Microsoft), where the two arguments are usually entities and the"
N15-1115,C14-1089,1,0.939925,"ntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗ This work was partly done while the first author was visiting University of Toronto. A particularly popular model for evaluating text coherence is the entity-based local coherence model of Barzilay and Lapata (2008) (B&L), which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention. Following this direction, a number of extensions have been proposed (Elsner and Charniak, 2008; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), the majority of which focus on enriching the original entity features. An exception is the unsupervised model of Guinaudeau and Strube (2013) (G&S), which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph. However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities. For example, the text in Figure 1a1 has no common entity in s2 and s3 . However, the tran"
N15-1115,W07-2321,0,0.0327156,"(d)), where pt (d) is the normalized frequency of the transition t in the entity grid, and m is the number of predefined transitions. pt (d) is computed as the number of occurrences of transition t among all entities in the entity grid, divided by the total number of transitions of the same length. Using this feature encoding, the model is then trained as a preference ranking problem between documents of different degrees of coherence. 2.2 Graph-based local coherence modeling As mentioned previously, most extensions to the entity-based local coherence model focus on enriching the feature set (Filippova and Strube, 2007; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), all of which follow a supervised learning framework. To the best of our knowledge, the only exception is the unsupervised method proposed by s1 s2 s3 G&S, which transforms the entity grid into a sentence graph and measures text coherence by computing the average out-degree of the graph. For a document d, its entity grid is constructed first, following the method described in Section 2.1. Then, a bipartite graph G = (V s , Ve , L, W) is constructed, where V s is the set of nodes representing sentences in the text; Ve is the set"
N15-1115,P13-1010,0,0.839476,"gh}@cs.toronto.edu Abstract Previous work on text coherence was primarily based on matching multiple mentions of the same entity in different parts of the text; therefore, it misses the contribution from semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft). In this paper, we capture such semantic relatedness by leveraging world knowledge (e.g., Gates is the person who created Microsoft), and use two existing evaluation frameworks. First, in the unsupervised framework, we introduce semantic relatedness as an enrichment to the original graph-based model of Guinaudeau and Strube (2013). In addition, we incorporate semantic relatedness as additional features into the popular entity-based model of Barzilay and Lapata (2008). Across both frameworks, our enriched model with semantic relatedness outperforms the original methods, especially on short documents. 1 Introduction In a well-written document, sentences are organized and presented in a logical and coherent form, which makes the text fluent and easily understood. Therefore, coherence is a fundamental aspect of high text quality, and the evaluation of coherence is a crucial component of many NLP applications, such as essay"
N15-1115,P11-1100,0,0.511546,"ry generation (McIntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗ This work was partly done while the first author was visiting University of Toronto. A particularly popular model for evaluating text coherence is the entity-based local coherence model of Barzilay and Lapata (2008) (B&L), which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention. Following this direction, a number of extensions have been proposed (Elsner and Charniak, 2008; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), the majority of which focus on enriching the original entity features. An exception is the unsupervised model of Guinaudeau and Strube (2013) (G&S), which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph. However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities. For example, the text in Figure 1a1 has no common entity in s2 and s3"
N15-1115,de-marneffe-etal-2006-generating,0,0.00708196,"train a ranker to prefer the(b)more coherent Sentence graph text over the less coherent one. Performance is therefore measured as the fraction of correct pairwise rankings as recognized by the ranker. We use SVMlight (Joachims, 2002) with the ranking configuration to train and evaluate our models, with all parameters set to default values. On both tasks, across two frameworks, supervised and unsupervised, we directly compare our modified model against the original one, i.e., B&L in the supervised framework and G&S in the unsupervised framework. In our experiments, we use the Stanford parser (Marneffe et al., 2006) to automatically extract the grammatical role for each entity mention. 5.1 Sentence ordering The task of sentence ordering attempts to simulate the situation where, given a predefined set of information-bearing items, we need to determine the best order to arrange those items. In this paper, we follow G&S and introduce CoNLL 20122 (Pradhan et al., 2012) as our dataset, which is composed of documents from multiple news sources. For each text, we randomly shuffle its sentences to generate 20 permutations with incorrect sentence order. For a fair comparison, we also evaluate our model on a filte"
N15-1115,P10-1158,0,0.026107,"Missing"
N15-1115,W12-4501,0,0.0383312,"across two frameworks, supervised and unsupervised, we directly compare our modified model against the original one, i.e., B&L in the supervised framework and G&S in the unsupervised framework. In our experiments, we use the Stanford parser (Marneffe et al., 2006) to automatically extract the grammatical role for each entity mention. 5.1 Sentence ordering The task of sentence ordering attempts to simulate the situation where, given a predefined set of information-bearing items, we need to determine the best order to arrange those items. In this paper, we follow G&S and introduce CoNLL 20122 (Pradhan et al., 2012) as our dataset, which is composed of documents from multiple news sources. For each text, we randomly shuffle its sentences to generate 20 permutations with incorrect sentence order. For a fair comparison, we also evaluate our model on a filtered subset of documents with an average length of 31.8 sentences. Therefore, our dataset contains 72 documents and 72 × 20 = 1440 permutations, among which the shortest one contains 25 sentences. For our enhanced graph-based model (introduced in Section 4.1), which is purely unsupervised, we evaluate our model over the entire dataset. For our enhanced en"
N15-1115,C14-1087,1,0.829171,"s extracted from raw texts. Generally speaking, manually edited knowledge bases have better accuracy but lower coverage, while automatically extracted knowledge bases are the opposite. To seek a good balance, we use both YAGO and Reverb as our knowledge sources. In addition, the automatically constructed knowledge bases can be extracted from raw texts of any domain, which makes our method adaptable. Both sources are presented in triples, argument1 -predicate-argument2 , (e.g., Gates-create-Microsoft), where the two arguments are usually entities and the predicate is the relation between them (Zhang et al., 2014). Knowledge selection For each document d, we then select the subset of relevant knowledge instances, in the sense that they represent relations between the entities in d. In particular, we extract all entities in d, and query the knowledge bases to obtain all the knowledge instances in which both of the two arguments, argument1 and argument2 , match some of the entities in d. One issue in knowledge selection is whether to retrieve knowledge instances using exact or partial matching. For a given pair of entities in the text, the chance is rather low to find instances in the knowledge bases whe"
N18-1051,P16-1139,0,0.0373964,"Missing"
N18-1051,C16-1303,0,0.179551,"anager or a potential customer may be interested in both food and service; however, the sentiment polarities towards the two targets are different. Hence, it would be beneficial for such tasks to tailor the sentence representations with respect to particular targets. Tree structures are promising for such tasks, as they can potentially capture long-distance dependencies between target words and their contexts (Li et al., 2015). Therefore, it is not surprising to find work that exploits the syntactically parsed trees for learning target-specific sentence representations. Dong et al. (2014) and Chang et al. (2016) adapted the word orders in a parsed tree, depending on their distances to the target entities. Nguyen et al. (2015) extended Dong et al. (2014) by combining the constituency tree and the dependency tree of a sentence. An important assumption of such work is that different tree structures lead to different semantic representations even for the same sentence. However, they all resort to exIntroduction We investigate target-dependent classification problem in this paper, with a special focus on the sentence level. Target-dependent classification aims to identify the fine-grained polarities of se"
N18-1051,P97-1003,0,0.05275,"ctic parse trees. Dong et al. (2014) are among the first to exploit tree structures, in which they adapt the parse trees based on the dependency relations between 5.2 Neural-based Syntactic Constituency Parsing Our work is related to syntactic constituency parsing as we build the tree structure in a transition 558 References manner. Syntactic constituency parsing is a fundamental task in natural language processing, which uses phrase structure to organize words into nested constituents. Early approaches rely on probabilistic context-free grammars or transition-based models with rich features (Collins, 1997; Klein and Manning, 2003). Recently, recursive neural network (Socher et al., 2013) and neural-based transition model (Liu and Zhang, 2009) are also applied, which achieve competitive or even better performances compared to traditional state-ofthe-art approaches that rely on hand-crafted features. Vinyals et al. (2015), from which we get inspirations, use the RNN Encoder-Decoder to encode the sentence and generate its corresponding full parse tree. Bowman et al. (2016) propose a Stack SPINN framework that integrates parsing and interpreting the sentence in a hybrid model. Yogatama et al. (201"
N18-1051,D14-1148,1,0.824719,"ntactic parser to obtain the dependency relations between the words. The parse tree are then adapted and binarized depending on their distances to targets in the dependency graph. A tree-structured Long Short-Term Memory Network (Tai et al., 2015) is then applied to learn a vector representation of the binarized tree structure. Method Sentiment-based Bi-LSTM Bi-LSTM + Attention TD-CTX-TLSTM Our Approach 4.1.3 Parameters & Metrics The hyper-parameters used in this paper are listed in Table 3. We pretrain word vectors with the Word2Vec (Mikolov et al., 2013) tool on the news dataset released by Ding et al. (2014), which are fine-tuned during training. The embeddings of target firms are obtained by averaging their words of constituents. We use macro-F1 to evaluate the performance on both positive and negative classes. Class +CAR3 -CAR3 Macro +CAR3 -CAR3 Macro +CAR3 -CAR3 Macro +CAR3 -CAR3 Macro +CAR3 -CAR3 Macro F1-score 0.597 0.476 0.536 0.557 0.490 0.523 0.575 0.523 0.549 0.552 0.570 0.561 0.572 0.592 0.582 Table 4: Results for cumulative abnormal return prediction task 4.1.5 Accuracy Versus Sentence Length Longer sentences are much more challenging for syntactic parsers. To gain insights on the perf"
N18-1051,C16-1201,1,0.821083,"Missing"
N18-1051,P14-2009,0,0.409388,"example, a restaurant manager or a potential customer may be interested in both food and service; however, the sentiment polarities towards the two targets are different. Hence, it would be beneficial for such tasks to tailor the sentence representations with respect to particular targets. Tree structures are promising for such tasks, as they can potentially capture long-distance dependencies between target words and their contexts (Li et al., 2015). Therefore, it is not surprising to find work that exploits the syntactically parsed trees for learning target-specific sentence representations. Dong et al. (2014) and Chang et al. (2016) adapted the word orders in a parsed tree, depending on their distances to the target entities. Nguyen et al. (2015) extended Dong et al. (2014) by combining the constituency tree and the dependency tree of a sentence. An important assumption of such work is that different tree structures lead to different semantic representations even for the same sentence. However, they all resort to exIntroduction We investigate target-dependent classification problem in this paper, with a special focus on the sentence level. Target-dependent classification aims to identify the fine-"
N18-1051,P82-1020,0,0.781921,"Missing"
N18-1051,P11-1016,0,0.346249,"mances. Aspect-level Sentiment Analysis To verify our proposed approach on informal social media texts, we apply it to aspect-level sentiment analysis on tweets. Aspect-level sentiment analysis aims to identify sentiment polarities towards specific targets mentioned in a sentence. Target-specific sentence representations can be naturally applied to this task. Dataset Training Testing #Target 6248 692 #Positive 1568 173 #Negative 1560 173 Model Jiang et al.(2011) Dong et al.(2014) Our Method #Neutral 3127 346 Dataset Baselines We compare our approach with feature-based and neural-based models. Jiang et al. (2011) They extract rich targetdependent and target-independent lexical and syntactic features for classification. Dong et al. (2014) They adapt the parse tree of a sentence concerning the target with predefined rules and use recursive neural network (Socher et al., 2013) to learn a target-specific sentence representation. 4.2.3 F1 63.3 65.9 66.3 4.2.4 Final Results The final results on aspect-level sentiment analysis task are shown in Table 7. Dong et al. (2014) are used as our main baseline, as they build targetspecific sentence representation over adapted tree structures. Neural-based models outp"
N18-1051,P03-1054,0,0.00784446,"s. Dong et al. (2014) are among the first to exploit tree structures, in which they adapt the parse trees based on the dependency relations between 5.2 Neural-based Syntactic Constituency Parsing Our work is related to syntactic constituency parsing as we build the tree structure in a transition 558 References manner. Syntactic constituency parsing is a fundamental task in natural language processing, which uses phrase structure to organize words into nested constituents. Early approaches rely on probabilistic context-free grammars or transition-based models with rich features (Collins, 1997; Klein and Manning, 2003). Recently, recursive neural network (Socher et al., 2013) and neural-based transition model (Liu and Zhang, 2009) are also applied, which achieve competitive or even better performances compared to traditional state-ofthe-art approaches that rely on hand-crafted features. Vinyals et al. (2015), from which we get inspirations, use the RNN Encoder-Decoder to encode the sentence and generate its corresponding full parse tree. Bowman et al. (2016) propose a Stack SPINN framework that integrates parsing and interpreting the sentence in a hybrid model. Yogatama et al. (2016) extend their model by u"
N18-1051,D13-1170,0,0.0098412,"ctures, in which they adapt the parse trees based on the dependency relations between 5.2 Neural-based Syntactic Constituency Parsing Our work is related to syntactic constituency parsing as we build the tree structure in a transition 558 References manner. Syntactic constituency parsing is a fundamental task in natural language processing, which uses phrase structure to organize words into nested constituents. Early approaches rely on probabilistic context-free grammars or transition-based models with rich features (Collins, 1997; Klein and Manning, 2003). Recently, recursive neural network (Socher et al., 2013) and neural-based transition model (Liu and Zhang, 2009) are also applied, which achieve competitive or even better performances compared to traditional state-ofthe-art approaches that rely on hand-crafted features. Vinyals et al. (2015), from which we get inspirations, use the RNN Encoder-Decoder to encode the sentence and generate its corresponding full parse tree. Bowman et al. (2016) propose a Stack SPINN framework that integrates parsing and interpreting the sentence in a hybrid model. Yogatama et al. (2016) extend their model by using reinforcement learning to build the tree structures t"
N18-1051,D14-1108,0,0.0206426,"t also important for deep text understanding. The definitions of polarity vary across different tasks, which can be positive or negative in ∗ * Corresponding author 551 Proceedings of NAACL-HLT 2018, pages 551–560 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics ternal syntactic resources, such as parse trees or Treebank annotations (Marcus et al., 1993), which limits their broader applications. On the one hand, annotated data are highly expensive to produce; and informal texts, such those on the social media, remain a challenge for syntactic parsers (Kong et al., 2014). On the other hand, the tree structures in their pipeline-style architecture are fixed during training, which cascade errors to later representation learning stage. A desirable solution would be to automatically and dynamically induce the tree structures for target-specific sentence representations. However, the challenge is that the absence of external supervisions makes it difficult to evaluate the quality of the tree structures and train the parameters. Inspired by Yogatama et al. (2016), we propose a reinforcement learning based approach that integrates target information and generates ta"
N18-1051,P15-1150,0,0.446407,"all the words in the sentence. The operations specified by SHIFT and REDUCE are as follows. • For a SHIFT transition, the buffer pops the topmost word out and pushes it to the top of the stack. • For a REDUCE transition, the topmost two elements of the stack are popped out and composed. Their compositions are then pushed back to the stack. M 1 X 5θG J(θG ) ≈ − [5θG log p(a)Rm (a)] M To produce a valid binary tree, we follow Yogatama et al. (2016) to disallow SHIFT transition when the buffer is empty and forbid REDUCE transition when the stack has no more than two elements. We use a tree-LSTM (Tai et al., 2015) to semantically compose the top two elements of the stack. Initially, the hidden state ht and the cell state st of m=1 (10) The 5θG log p(a) can be used to update θG . REINFORCE algorithm is non-biased but may have high variance. To reduce the variance, a widely used trick is to subtract a baseline from the reward. It has been theoretically proven that 554 any baselines that do not depend on the actions are applicable. In this paper, we follow Rennie et al. (2016) to apply a self-critical baseline to the rewards. Rather than estimating a baseline reward, the self-critical method uses the outp"
N18-1051,D15-1278,0,0.0145015,"n find that there can be multiple target mentions in the same text scope, which makes it challenging for generic sentence representation approaches. For the first example, a restaurant manager or a potential customer may be interested in both food and service; however, the sentiment polarities towards the two targets are different. Hence, it would be beneficial for such tasks to tailor the sentence representations with respect to particular targets. Tree structures are promising for such tasks, as they can potentially capture long-distance dependencies between target words and their contexts (Li et al., 2015). Therefore, it is not surprising to find work that exploits the syntactically parsed trees for learning target-specific sentence representations. Dong et al. (2014) and Chang et al. (2016) adapted the word orders in a parsed tree, depending on their distances to the target entities. Nguyen et al. (2015) extended Dong et al. (2014) by combining the constituency tree and the dependency tree of a sentence. An important assumption of such work is that different tree structures lead to different semantic representations even for the same sentence. However, they all resort to exIntroduction We inve"
N18-1051,C16-1311,1,0.870893,"Missing"
N18-1051,E17-2091,0,0.0210552,"resources. On the other hand, the transitions generated at each step are discrete, making it difficult to train and propagate errors to update the model parameters. We give details of the two components and how we address the challenges in this section. 3.1 uit = dt−1 hi exp(uit ) ait = P i0 i0 exp(ut ) n X ct = ait · hi , Transition Generator The basic idea of the transition generator is to generate different transition orders given different targets. We propose using the RNN encoderdecoder framework (Cho et al., 2014), which has shown capacity in shift-reduce parsing (Vinyals et al., 2015; Liu and Zhang, 2017b). A standard RNN encoder-decoder contains two recurrent neural networks, one for encoding a sequence of variable-length into a vector representation and the other for decoding the representation back into another variable-length sequence. (2) (3) (4) i=1 in which denotes element-wise dot product; ait is the normalized attention score and the context vector ct is a weighted sum of all the encoder hidden states. To enable the target of interest to influence the decoding process, we enrich the input of the decoder by concatenating the target entity. The hidden state of the decoder at time t is"
N18-1051,W17-6315,0,0.0197217,"resources. On the other hand, the transitions generated at each step are discrete, making it difficult to train and propagate errors to update the model parameters. We give details of the two components and how we address the challenges in this section. 3.1 uit = dt−1 hi exp(uit ) ait = P i0 i0 exp(ut ) n X ct = ait · hi , Transition Generator The basic idea of the transition generator is to generate different transition orders given different targets. We propose using the RNN encoderdecoder framework (Cho et al., 2014), which has shown capacity in shift-reduce parsing (Vinyals et al., 2015; Liu and Zhang, 2017b). A standard RNN encoder-decoder contains two recurrent neural networks, one for encoding a sequence of variable-length into a vector representation and the other for decoding the representation back into another variable-length sequence. (2) (3) (4) i=1 in which denotes element-wise dot product; ait is the normalized attention score and the context vector ct is a weighted sum of all the encoder hidden states. To enable the target of interest to influence the decoding process, we enrich the input of the decoder by concatenating the target entity. The hidden state of the decoder at time t is"
N18-1051,J93-2004,0,0.0644402,"classification problem in this paper, with a special focus on the sentence level. Target-dependent classification aims to identify the fine-grained polarities of sentences towards specific targets, which is challenging but also important for deep text understanding. The definitions of polarity vary across different tasks, which can be positive or negative in ∗ * Corresponding author 551 Proceedings of NAACL-HLT 2018, pages 551–560 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics ternal syntactic resources, such as parse trees or Treebank annotations (Marcus et al., 1993), which limits their broader applications. On the one hand, annotated data are highly expensive to produce; and informal texts, such those on the social media, remain a challenge for syntactic parsers (Kong et al., 2014). On the other hand, the tree structures in their pipeline-style architecture are fixed during training, which cascade errors to later representation learning stage. A desirable solution would be to automatically and dynamically induce the tree structures for target-specific sentence representations. However, the challenge is that the absence of external supervisions makes it d"
N18-1051,D15-1298,0,0.0379741,"Missing"
N18-1051,D14-1162,0,0.0812272,"ults on aspect-level sentiment analysis task Table 5: Statistics of aspect-level sentiment analysis datasets 4.2.1 Value 100 100 0.5 32 0.0005 4.3 Case Study To gain further insights on the induced structures, we inspect the shift-reduce trees our approach generated in this section. We present two examples that our model gives high confidences in Figure 4. For the sentence “Nike NKE.N has sued WalMart WMT.N, saying the world ’s largest retailer Parameters & Metrics The parameter settings are listed in Table 6. We use 100-dimension GloVe vectors which are pre-trained on a large Twitter Corpus (Pennington et al., 2014) and fine-tuned during training. 557 Figure 4: Two tree structures generated by our model. We removed stop words and punctuations. The upper tree structure is for the sentence “Nike NKE.N has sued Wal-Mart WMT.N, saying the world ’s largest retailer is selling athletic shoes that infringe on its design patents” and the bottom one is for the sentence “Walgreen WAG.N , which operates the largest U.S. drugstore chain , raised its dividend on Monday.” the words and the target, and then use a recursive neural network to learn the sentence representations. Similarly, Chang et al. (2016) explore a hy"
P00-1067,P93-1002,0,0.0604293,"Missing"
P00-1067,P91-1022,0,0.0208785,"Missing"
P00-1067,W93-0301,0,0.0648142,"Missing"
P00-1067,J93-2003,0,0.00789834,"Missing"
P00-1067,P93-1001,0,\N,Missing
P06-1058,P91-1034,0,0.166095,"s the value of EP for unsupervised WSD. 1 Introduction Word sense disambiguation (WSD) has been a hot topic in natural language processing, which is to determine the sense of an ambiguous word in a specific context. It is an important technique for applications such as information retrieval, text mining, machine translation, text classification, automatic text summarization, and so on. Statistical solutions to WSD acquire linguistic knowledge from the training corpus using machine learning technologies, and apply the knowledge to disambiguation. The first statistical model of WSD was built by Brown et al. (1991). Since then, most machine learning methods have been applied to WSD, including decision tree, Bayesian model, neural network, SVM, maximum entropy, genetic algorithms, and so on. For different learning methods, supervised methods usually achieve good performance at a cost of human tagging of training corpus. The precision improves with larger size of training corpus. Compared with supervised methods, unsupervised methods do not require tagged corpus, but the precision is usually lower than that of the supervised methods. Thus, knowledge acquisition is critical to WSD methods. This paper propo"
P06-1058,P04-1039,0,0.0364432,"Missing"
P06-1058,W04-1609,0,0.0455949,"Missing"
P06-1058,P02-1033,0,0.0295354,"further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment for WSD. Diab and Resnik (2002) investigated the feasibility of automatically annotating large amounts of data in parallel corpora using an unsupervised algorithm, making use of two languages simultaneously, only one of which has an available sense inventory. The results showed that wordlevel translation correspondences are a valuable source of information for sense disambiguation. The method by Li and Li (2002) does not require parallel corpus. It avoids the alignment work and takes advantage of bilingual corpus. In short, technology of automatic corpus tagging is based on the manually labeled corpus. That is to say, it st"
P06-1058,1992.tmi-1.9,0,0.418271,"of the ACL, pages 457–464, c Sydney, July 2006. 2006 Association for Computational Linguistics tagged corpus. Unsupervised method is an alternative, which often involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to"
P06-1058,P92-1032,0,0.0812357,"of the ACL, pages 457–464, c Sydney, July 2006. 2006 Association for Computational Linguistics tagged corpus. Unsupervised method is an alternative, which often involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to"
P06-1058,W02-0808,0,0.0785777,"Missing"
P06-1058,P02-1044,0,0.0136108,"l Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment for WSD. Diab and Resnik (2002) investigated the feasibility of automatically annotating large amounts of data in parallel corpora using an unsupervised algorithm, making use of two languages simultaneously, only one of which has an available sense inventory. The results showed that wordlevel translation correspondences are a valuable source of information for sense disambiguation. The method by Li and Li (2002) does not require parallel corpus. It avoids the alignment work and takes advantage of bilingual corpus. In short, technology of automatic corpus tagging is based on the manually labeled corpus. That is to say, it still need human intervention and is not a completely unsupervised method. Large-scale parallel corpus; especially wordaligned corpus is highly unobtainable, which has limited the WSD methods based on parallel corpus. 3 Equivalent Pseudoword This section describes how to obtain equivalent pseudowords without a seed corpus. Monosemous words are unambiguous priori knowledge. According"
P06-1058,mihalcea-2002-bootstrapping,0,0.0531623,"involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment"
P06-1058,N03-2023,0,0.154789,"n be used as a knowledge source for WSD. 3.1 Definition of Equivalent Pseudoword If the ambiguous words in the corpus are replaced with its synonymous monosemous word, then is it convenient to acquire knowledge from raw corpus? For example in table 1, the ambiguous word &quot;把握&quot; has three senses, whose synonymous monosemous words are listed on the right column. These synonyms contain some information for disambiguation task. An artificial ambiguous word can be coined with the monosemous words in table 1. This process is similar to the use of general pseudowords (Gale et al., 1992b; Gaustad, 2001; Nakov and Hearst, 2003), but has some essential differences. This artificial ambiguous word need to simulate the function of the real ambiguous word, and to acquire semantic knowledge as the real ambiguous word does. Thus, we call it an equivalent pseudoword (EP) for its equivalence with the real ambiguous word. It's apparent that the equivalent pseudoword has provided a new way to unsupervised WSD. S1 信心/自信心 把握(ba3 wo4) S2 握住/在握/把住/抓住/控制 S3 领会/理解/领悟/深谙/体会 Table 1. Synonymous Monosemous Words for the Ambiguous Word &quot;把握&quot; The equivalence of the EP with the real ambiguous word is a kind of semantic synonym or similarit"
P06-1058,P03-1058,0,0.0859216,"Missing"
P06-1058,P05-1049,0,0.0167964,"solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab (2003, 2004a, and 2004b) made research on the use of alignment for WSD. Diab and Resnik (2002) investigated the feasibility of automatically annotating large amounts of data in parallel corpora using an unsupervise"
P06-1058,J98-1004,0,0.0966695,"P-based WSD It is based on the following assumptions that EPs can substitute the original ambiguous word for knowledge acquisition in WSD model training. Assumption 1: Words of the same meaning play the same role in a language. The sense is an important attribute of a word. This plays as the basic assumption in this paper. Assumption 2: Words of the same meaning occur in similar context. This assumption is widely used in semantic analysis and plays as a basis for much related research. For example, some researchers cluster the contexts of ambiguous words for WSD, which shows good performance (Schutze, 1998). Because an EP has a higher similarity with the ambiguous word in syntax and semantics, it is a useful knowledge source for WSD. 3.4 Implementation of the EP-based Solution 3.3 Design and Construction of EPs Because of the special characteristics of EPs, it's more difficult to construct an EP than a general pseudo word. To ensure the maximum similarity between the EP and the original ambiguous word, the following principles should be followed. 1) Every EP should map to one and only one original ambiguous word. 2) The morphemes of an EP should map one by one to those of the original ambiguous"
P06-1058,P94-1013,0,0.0628952,"d corpus. Unsupervised method is an alternative, which often involves automatic generation of tagged corpus, bilingual corpus alignment, etc. The value of unsupervised methods lies in the knowledge acquisition solutions they adopt. 2.1 Automatic Generation of Training Corpus Automatic corpus tagging is a solution to WSD, which generates large-scale corpus from a small seed corpus. This is a weakly supervised learning or semi-supervised learning method. This reinforcement algorithm dates back to Gale et al. (1992a). Their investigation was based on a 6word test set with 2 senses for each word. Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. A semi-supervised method proposed by Niu et al. (2005) clustered untagged instances with tagged ones starting from a small seed corpus, which assumes that similar instances should have similar tags. Clustering was used instead of bootstrapping and was proved more efficient. 2.2 Method Based on Parallel Corpus Parallel corpus is a solution to the bottleneck of knowledge acquisition. Ide et al. (2001 and 2002), Ng et al. (2003), and Diab ("
P06-1058,P95-1026,0,0.70339,"Missing"
P06-1058,J04-1001,0,\N,Missing
P06-2010,W05-0627,1,0.831541,"based method use a large number of hand-craft diverse features, from word, POS, syntax and semantics, NER, etc. The standard features with polynomial kernel gets the best performance. The reason is that the arbitrary binary combination among features implicated by the polynomial kernel is useful to SRL. We believe that combining the two methods can perform better. In order to make full use of the syntactic information and the standard flat features, we present a composite kernel between hybrid kernel (Khybrid ) and standard features with polynomial Stage 4: A rule-based post-processing stage (Liu et al., 2005) is used to handle some unmatched arguments with constituents, such as AM-MOD, AM-NEG. 5.1.4 Classifier We use the Voted Perceptron (Freund and Schapire, 1998) algorithm as the kernel machine. The performance of the Voted Perceptron is close to, but not as good as, the performance of SVM on the same problem, while saving computation time and programming effort significantly. SVM is too slow to finish our experiments for tuning parameters. The Voted Perceptron is a binary classifier. In order to handle multi-classification problems, we adopt the one vs. others strategy and select the one with t"
P06-2010,P98-1013,0,0.0108697,"l for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model th"
P06-2010,W04-2412,0,0.0431802,"Missing"
P06-2010,J93-2004,0,0.0306892,"hybrid convolution tree kernel, Khybrid . The aim of our experiments is to verify the effectiveness of our hybrid convolution tree kernel and and its combination with the standard flat features. Since the size of a parse tree is not constant, we normalize K(T1 , T2 ) by dividing it by p K(T1 , T1 ) · K(T2 , T2 ) 5.1 Experimental Setting 5.1.1 Corpus We use the benchmark corpus provided by CoNLL-2005 SRL shared task (Carreras and M`arquez, 2005) provided corpus as our training, development, and test sets. The data consist of sections of the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al., 2005). We followed the standard partition used in syntactic parsing: sections 02-21 for training, section 24 for development, and section 23 for test. In addition, the test set of the shared task includes three sections of the Brown corpus. Table 2 provides counts of sentences, tokens, annotated propositions, and arguments in the four data sets. 4.3 Comparison with Previous Work It would be interesting to investigate the differences between our method and the feature-based methods. The basic"
P06-2010,W05-0620,0,0.262008,"Missing"
P06-2010,P04-1043,0,0.365021,"and Duffy, 2001) provide an elegant kernel-based solution to implicitly explore tree structure features by directly computing the similarity between two trees. In addition, some machine learning algorithms with dual form, such as Perceptron and Support Vector Machines (SVM) (Cristianini and Shawe-Taylor, 2000), which do not need know the exact presentation of objects and only need compute their kernel functions during the process of learning and prediction. They can be well used as learning algorithms in the kernel-based methods. They are named kernel machines. In this paper, we decompose the Moschitti (2004)’s predicate-argument feature (PAF) kernel into a Path kernel and a Constituent Structure kerIntroduction In the last few years there has been increasing interest in Semantic Role Labeling (SRL). It is currently a well defined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows"
P06-2010,A00-2018,0,0.0820962,"Missing"
P06-2010,W03-0423,0,0.026918,"sentation in our feature space is more robust than the Parse Tree Path feature in the flat feature set since the Path feature is sensitive to small changes of the parse trees and it also does not maintain the hierarchical information of a parse tree. Sentences Tokens Propositions Arguments Train 39,832 950,028 90,750 239,858 Devel 1,346 32,853 3,248 8,346 tWSJ 2,416 56,684 5,267 14,077 tBrown 426 7,159 804 2,177 Table 2: Counts on the data set The preprocessing modules used in CONLL2005 include an SVM based POS tagger (Gim´enez and M`arquez, 2003), Charniak (2000)’s full syntactic parser, and Chieu and Ng (2003)’s Named Entity recognizer. 5.1.2 Evaluation The system is evaluated with respect to precision, recall, and Fβ=1 of the predicted arguments. P recision (p) is the proportion of arguments predicted by a system which are correct. Recall (r) is the proportion of correct arguments which are predicted by a system. Fβ=1 computes the harmonic mean of precision and recall, which is the final measure to evaluate the performances of systems. It is formulated as: Fβ=1 = 2pr/(p + r). srl-eval.pl2 is the official program of the CoNLL-2005 SRL shared task to evaluate a system performance. It is also worth c"
P06-2010,J05-1004,0,0.316121,"d a Constituent Structure kerIntroduction In the last few years there has been increasing interest in Semantic Role Labeling (SRL). It is currently a well defined task with a substantial body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in"
P06-2010,P05-1072,0,0.24175,"tive evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final composing kernel between hybrid convolution tree kernel and standard features’ polynomial kernel outperforms each of them individually."
P06-2010,P04-1054,0,0.121072,"hod. Section 5 shows the experimental results. We conclude our work in Section 6. 2 Many kernel functions have been proposed in machine learning community and have been applied to NLP study. In particular, Haussler (1999) and Watkins (1999) proposed the best-known convolution kernels for a discrete structure. In the context of convolution kernels, more and more kernels for restricted syntaxes or specific domains, such as string kernel for text categorization (Lodhi et al., 2002), tree kernel for syntactic parsing (Collins and Duffy, 2001), kernel for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004) are proposed and explored in NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features"
P06-2010,C04-1197,0,0.0455194,"ing was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorithms from generalizing unseen data well. As an alternative to the standard feature-based methods, kernel-based methods have been proposed to implicitly explore features in a highdimension space by directly calculating the simi"
P06-2010,W05-0639,0,0.113832,"Missing"
P06-2010,J02-3001,0,0.676474,"al body of work and comparative evaluation. Given a sentence, the task consists of analyzing the propositions expressed by some target verbs and some constituents of the sentence. In particular, for each target verb (predicate) all the constituents in the sentence which fill a semantic role (argument) of the verb have to be recognized. Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al., 2005). The PropBank defines 6 main arguments, Arg0 is the Agent, Arg1 is Patient, etc. ArgMmay indicate adjunct arguments, such as Locative, Temporal. Many researchers (Gildea and Jurafsky, 2002; Pradhan et al., 2005a) use feature-based methods 1 http://www.cs.unt.edu/∼rada/senseval/senseval3/ 73 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73–80, c Sydney, July 2006. 2006 Association for Computational Linguistics nel, and then compose them into a hybrid convolution tree kernel. Our hybrid kernel method using Voted Perceptron kernel machine outperforms the PAF kernel in the development sets of CoNLL-2005 SRL shared task. In addition, the final composing kernel between hybrid convolution tree kernel and standard features’ polynomial kernel outperforms each"
P06-2010,P02-1031,0,0.0644078,"cate-Constituent related features parse tree path from the predicate to the constituent the relative position of the constituent and the predicate, before or after the nodes number on the parse tree path some part on the parse tree path the clause layers from the constituent to the predicate Table 1: Standard flat features However, to find relevant features is, as usual, a complex task. In addition, according to the description of the standard features, we can see that the syntactic features, such as Path, Path Length, bulk large among all features. On the other hand, the previous researches (Gildea and Palmer, 2002; Punyakanok et al., 2005) have also recognized the 74 Firstly, a parse tree T can be represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): Φ(T ) = (# of sub-trees of type 1, . . . , # of sub-trees of type i, . . . , # of sub-trees of type n) This results in a very high dimension since the number of different subtrees is exponential to the tree’s size. Thus it is computationally infeasible to use the feature vector Φ(T ) directly. To solve this problem, we introduce the tree kernel function which is able to calculate the dot product between the above hi"
P06-2010,W04-3212,0,0.270004,"ent Feature (PAF) kernel under the framework of convolution tree kernel for SRL. In this paper, we follow the same framework and design a novel hybrid convolution kernel for SRL. Related Work Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). They used a linear interpolation method and extract features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) with syntactic parsing results. Here, the basic features include Phrase Type, Parse Tree Path, Position. Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other works paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art method for SRL and achieved much success. However, as we know, the standard flat features are less effective to model the syntactic structured information. It is sensitive to small changes of the syntactic structure features. This can give rise to a data sparseness problem and prevent the learning algorit"
P06-2010,W04-3211,0,\N,Missing
P06-2010,C98-1013,0,\N,Missing
P07-1026,P98-1013,0,0.0102809,"nvolution tree kernel on the data set of the CoNLL-2005 SRL shared task. The remainder of the paper is organized as follows: Section 2 reviews the previous work and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treeba"
P07-1026,W05-0620,0,0.371187,"classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argument or a non-argument while semantic role classification involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. Th"
P07-1026,A00-2018,0,0.0600695,"Missing"
P07-1026,J05-1004,0,0.0606769,"Missing"
P07-1026,W04-3212,0,0.110658,"ork and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004). These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “N"
P07-1026,P06-1104,1,0.842473,"hods for SRL: as an alternative, kernel methods are more effective in modeling structured objects. This is because a kernel can measure the similarity between two structured objects using the original representation of the objects instead of explicitly enumerating their features. Many kernels have been proposed and applied to the NLP study. In particular, Haussler (1999) proposed the well-known convolution kernels for a discrete structure. In the context of it, more and more kernels for restricted syntaxes or specific domains (Collins and Duffy, 2001; Lodhi et al., 2002; Zelenko et al., 2003; Zhang et al., 2006) are proposed and explored in the NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. He selected portions of syntactic parse trees as predicateargument feature spaces, which include salient substructures of predicate-arguments, to define convolution kernels for the task of semantic role classification. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kernel, which consists of two individual convolution kernels: a Path kernel and a Constituent Structure kern"
P07-1026,W04-3211,0,\N,Missing
P07-1026,J93-2004,0,\N,Missing
P07-1026,N06-2025,0,\N,Missing
P07-1026,W03-1012,0,\N,Missing
P07-1026,C04-1197,0,\N,Missing
P07-1026,P05-1072,0,\N,Missing
P07-1026,C98-1013,0,\N,Missing
P07-1026,P04-1043,0,\N,Missing
P07-1026,J02-3001,0,\N,Missing
P07-1026,P04-1016,0,\N,Missing
P07-1026,P06-2010,1,\N,Missing
P07-1026,W04-2412,0,\N,Missing
P08-1089,P05-1074,0,0.836647,"el, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora. Our method involves three steps: (1) corpus preprocessing, including English monolingual dependency 780 Proceedings of ACL-08: HLT, pages 780–788, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics parsing and English-foreign language word alignment, (2) aligned patterns induction, which produces English patterns along with the aligned pivot patterns in the foreign language, (3) paraphrase patterns extraction, in which paraphrase patterns are extracted based on a log-linear model. Our contri"
P08-1089,N06-1003,0,0.288784,"Missing"
P08-1089,W03-1608,0,0.229068,"or sentence) by filling the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora."
P08-1089,N06-1058,0,0.313374,"Missing"
P08-1089,N03-1017,0,0.0127798,"hm 1 denotes the POS tag of wk . 783 weight. In this paper, 4 feature functions are used in our log-linear model, which include: h1 (e1 , e2 , c) = scoreM LE (c|e1 ) h2 (e1 , e2 , c) = scoreM LE (e2 |c) h3 (e1 , e2 , c) = scoreLW (c|e1 ) h4 (e1 , e2 , c) = scoreLW (e2 |c) Feature functions h1 (e1 , e2 , c) and h2 (e1 , e2 , c) are based on MLE. scoreM LE (c|e) is computed as: scoreM LE (c|e) = log pM LE (c|e) (4) scoreM LE (e|c) is computed in the same way. h3 (e1 , e2 , c) and h4 (e1 , e2 , c) are based on LW. LW was originally used to validate the quality of a phrase translation pair in MT (Koehn et al., 2003). It checks how well the words of the phrases translate to each other. This paper uses LW to measure the quality of aligned patterns. We define scoreLW (c|e) as the logarithm of the lexical weight3 : scoreLW (c|e) = n X 1X 1 log( w(ci |ej )) n |{j|(i, j) ∈ a}| i=1 4 (5) ∀(i,j)∈a where a denotes the word alignment between c and e. n is the number of words in c. ci and ej are words of c and e. w(ci |ej ) is computed as follows: count(ci , ej ) w(ci |ej ) = P 0 c0 count(ci , ej ) (6) i where count(ci , ej ) is the frequency count of the aligned word pair (ci , ej ) in the corpus. scoreLW (e|c) is"
P08-1089,W06-2931,1,0.814779,"Missing"
P08-1089,P00-1056,0,0.00494939,"p(e2 |e1 ) = X pM LE (c|e1 )pM LE (e2 |c) (1) c In Equation (1), pM LE (c|e1 ) and pM LE (e2 |c) are the probabilities of translating e1 to c and c to e2 , which are computed based on MLE: count(c, e1 ) pM LE (c|e1 ) = P 0 c0 count(c , e1 ) Figure 2: Examples of a subtree and a partial subtree. 3 Proposed Method 3.1 In this paper, we use English paraphrase patterns extraction as a case study. An English-Chinese (EC) bilingual parallel corpus is employed for training. The Chinese part of the corpus is used as pivots to extract English paraphrase patterns. We conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag heuristic (Koehn et al., 2005) for symmetrization. Since the paraphrase patterns are extracted from dependency trees, we parse the English sentences in the corpus with MaltParser (Nivre et al., 2007). Let SE be an English sentence, TE the parse tree of SE , e a word of SE , we define the subtree and partial subtree following the definitions in (Ouangraoua et al., 2007). In detail, a subtree STE (e) is a particular connected subgraph of the tree TE , which is rooted at e and includes all the descendants of e. A partial subtree P STE (e) is a conn"
P08-1089,N03-1024,0,0.274941,"g the pattern slots with specific words. Paraphrase patterns are useful in both paraphrase recognition and generation. In paraphrase recognition, if two text units match a pair of paraphrase patterns and the corresponding slot-fillers are identical, they can be identified as paraphrases. In paraphrase generation, a text unit that matches a pattern P can be rewritten using the paraphrase patterns of P. A variety of methods have been proposed on paraphrase patterns extraction (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Ibrahim et al., 2003; Pang et al., 2003; Szpektor et al., 2004). However, these methods have some shortcomings. Especially, the precisions of the paraphrase patterns extracted with these methods are relatively low. In this paper, we extract paraphrase patterns from bilingual parallel corpora based on a pivot approach. We assume that if two English patterns are aligned with the same pattern in another language, they are likely to be paraphrase patterns. This assumption is an extension of the one presented in (Bannard and Callison-Burch, 2005), which was used for deriving phrasal paraphrases from bilingual corpora. Our method involve"
P08-1089,W04-3219,0,0.6299,"Missing"
P08-1089,P02-1006,0,0.183123,". Hence the precision can be enhanced. In this experiment, we also estimated a threshold T 0 for MLE-Model using the development set (T 0 = −5.1). The pattern pairs whose score based on Equation (1) exceed T 0 were extracted as paraphrase patterns. 6 785 It is necessary to compare our method with another paraphrase patterns extraction method. However, it is difficult to find methods that are suitable for comparison. Some methods only extract paraphrase patterns using news articles on certain topics (Shinyama et al., 2002; Barzilay and Lee, 2003), while some others need seeds as initial input (Ravichandran and Hovy, 2002). In this paper, we compare our method with DIRT (Lin and Pantel, 2001), which does not need to specify topics or input seeds. As mentioned in Section 2, DIRT learns paraphrase patterns from a parsed monolingual corpus based on an extended distributional hypothesis. In our experiment, we implemented DIRT and extracted paraphrase patterns from the English part of our bilingual parallel corpus. Our corpus is smaller than that reported in (Lin and Pantel, 2001). To alleviate the data sparseness problem, we only kept patterns appearing more than 10 times in the corpus for extracting paraphrase pat"
P08-1089,N03-1003,0,\N,Missing
P08-1089,2005.iwslt-1.8,0,\N,Missing
P08-1089,W04-3206,0,\N,Missing
P08-1096,P95-1017,0,0.282338,"plicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et"
P08-1096,N07-1011,0,0.682128,"n (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an entity and record their information in a single feature vector, as it would make the feature space too large. Even worse, the number of mentions in an entity is not fixed, which would result in variant-length feature vectors and make trouble for normal machine learning algorithms. A solution seen in previous work (Luo et al., 2004; Culotta et al., 2007) is to design a set of first-order features summarizing the information of the mentions in an entity, for example, “whether the entity has any mention that is a name alias of the active mention?” or “whether most of the mentions in the entity have the same head word as the active mention?” These features, nevertheless, are designed in an ad-hoc manner and lack the capability of describing each individual mention in an entity. In this paper, we present a more expressive entity843 Proceedings of ACL-08: HLT, pages 843–851, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Ling"
P08-1096,N07-1030,0,0.213821,"Missing"
P08-1096,P04-1018,0,0.879059,". Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all"
P08-1096,P02-1014,0,0.959589,"nd automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, th"
P08-1096,P05-1020,0,0.108939,"Missing"
P08-1096,P07-1068,0,0.0587945,"Missing"
P08-1096,J01-4004,0,0.901524,"ontained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional menti"
P08-1096,P07-1006,0,0.0425872,"ch other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to a partition (i.e., the probability that an active mention belongs to an entity) based on the sum of its distances to all the nodes in the partition (i.e., the sum of the probability of the active mention co-referring with the mentions in the entity). Inductive Logic Programming (ILP) has been applied to some natural language processing tasks, including parsing (Mooney, 1997), POS disambiguation (Cussens, 1996), lexicon construction (Claveau et al., 2003), WSD (Specia et al., 2007), and so on. However, to our knowledge, our work is the first effort to adopt this technique for the coreference resolution task. 3 Modelling Coreference Resolution Suppose we have a document containing n mentions {mj : 1 &lt; j &lt; n}, in which mj is the jth mention occurring in the document. Let ei be the ith entity in the document. We define P (L|ei , mj ), (1) the probability that a mention belongs to an entity. Here the random variable L takes a binary value and is 1 if mj is a mention of ei . By assuming that mentions occurring after mj have no influence on the decision of linking mj to an en"
P08-1096,M95-1005,0,0.818946,"Missing"
P08-1096,P07-1067,1,0.315086,"Missing"
P08-1096,P04-1017,1,0.929248,"(1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an"
P08-1096,C04-1033,1,0.933005,"(1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an"
P08-1096,W00-1309,1,0.821131,"Missing"
P08-1096,P02-1060,1,0.450274,"Missing"
P08-1116,P05-1074,0,0.349233,"paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (20"
P08-1116,N03-1003,0,0.929129,"es that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using biling"
P08-1116,P01-1008,0,0.869425,"e more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They 1022 exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphras"
P08-1116,J93-2003,0,0.00852245,"raphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Model The SMT-based paraphrasing model used by Quirk et al. (2004) was the noisy channel model of Brown et al. (1993), which identified the optimal paraphrase T ∗ of a sentence S by finding: T ∗ = arg max{P (T |S)} model feature. λT M i and λLM are the weights of the feature functions. hT M i (T, S) is defined as: hT M i (T, S) = log Ki Y Scorei (Tk , Sk ) (3) k=1 where Ki is the number of phrase substitutes from S to T based on P Ti . Tk in T and Sk in S are phrasal paraphrases in P Ti . Scorei (Tk , Sk ) is the paraphrase likelihood according to P Ti 2 . A 5-gram language model is used, therefore: hLM (T, S) = log J Y p(tj |tj−4 , ..., tj−1 ) (4) j=1 where J is the length of T , tj is the j-th word of T ."
P08-1116,N06-1003,0,0.198695,"f the method varies greatly on different test sets and it performs best on the test set of news sentences, which are from the same source as most of the training data. The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (19"
P08-1116,I05-5003,0,0.0339384,", 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They 1022 exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a pars"
P08-1116,N06-1058,0,0.367526,", which are from the same source as most of the training data. The rest of the paper is organized as follows: Section 2 reviews related work. Section 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resour"
P08-1116,koen-2004-pharaoh,0,0.038836,"ights of the feature functions. hT M i (T, S) is defined as: hT M i (T, S) = log Ki Y Scorei (Tk , Sk ) (3) k=1 where Ki is the number of phrase substitutes from S to T based on P Ti . Tk in T and Sk in S are phrasal paraphrases in P Ti . Scorei (Tk , Sk ) is the paraphrase likelihood according to P Ti 2 . A 5-gram language model is used, therefore: hLM (T, S) = log J Y p(tj |tj−4 , ..., tj−1 ) (4) j=1 where J is the length of T , tj is the j-th word of T . 4 Exploiting Multiple Resources This section describes the extraction of phrasal paraphrases using various resources. Similar to Pharaoh (Koehn, 2004), our decoder3 uses top 20 paraphrase options for each input phrase in the default setting. Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource. 1 - Thesaurus: The thesaurus4 used in this work was automatically constructed by Lin (1998). The similarity of two words e1 and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: T = arg max{P (S|T )P (T )} T (1) In contrast, we adopt a log-linear model (Och and Ney, 2002) in this work, since multiple paraphrase tables can be eas"
P08-1116,P98-2127,0,0.511715,"ts before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been widely used for extracting paraphrases. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Besides, the automatically constructed thesauri can also be used. Lin (1998) constructed a thesaurus by automatically clustering words based on context similarity. Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. They 1022 exploited a corpus of multiple English translations of the same source text written in a foreign language, from which phrases in aligned sentences that appear in similar contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especial"
P08-1116,W07-0716,0,0.260551,"rns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level paraphrases based on a SMT model. The advantage of the SMT-based method is that it achieves better coverage than the pattern-based method. The main difference between their methods and ours is that they only used bilingual parallel corpora as paraphrase resource, while we exploit and combine multiple resources. 3 SMT-based Paraphrasing Model The SMT-based paraphrasing model used by Quirk et al. (2004) was the noisy channel model of Brown et al. (1993), which identified the optimal paraphrase T ∗ of a sentence S by finding: T ∗ = arg max{P (T |S)} model feature. λ"
P08-1116,P00-1056,0,0.124901,"m the comparable corpus, parallel sentences are extracted. Let s1 and s2 be two sentences from comparable documents d1 and d2 , if their similarity based on word overlapping rate is above a threshold T h3 , s1 and s2 are identified as parallel sentences. In this way, 872,330 parallel sentence pairs are extracted. 5 http://people.csail.mit.edu/mcollins/code.html The context of a chunk is made up of 6 words around the chunk, 3 to the left and 3 to the right. 7 The similarity of two documents is computed using the vector space model and the word weights are based on tf·idf. 6 1024 We run Giza++ (Och and Ney, 2000) on the parallel sentences and then extract aligned phrases as described in (Koehn, 2004). The generated paraphrase table is pruned by keeping the top 20 paraphrases for each phrase. After pruning, 100,621 pairs of paraphrases are extracted. Given phrase p1 and its paraphrase p2 , we compute Score3 (p1 , p2 ) by relative frequency (Koehn et al., 2003): count(p2 , p1 ) Score3 (p1 , p2 ) = p(p2 |p1 ) = P 0 p0 count(p , p1 ) (7) People may wonder why we do not use the same method on the monolingual parallel and comparable corpora. This is mainly because the volumes of the two corpora differ a lot"
P08-1116,P02-1038,0,0.0245662,"hrasal paraphrases using various resources. Similar to Pharaoh (Koehn, 2004), our decoder3 uses top 20 paraphrase options for each input phrase in the default setting. Therefore, we keep at most 20 paraphrases for a phrase when extracting phrasal paraphrases using each resource. 1 - Thesaurus: The thesaurus4 used in this work was automatically constructed by Lin (1998). The similarity of two words e1 and e2 was calculated through the surrounding context words that have dependency relations with the investigated words: T = arg max{P (S|T )P (T )} T (1) In contrast, we adopt a log-linear model (Och and Ney, 2002) in this work, since multiple paraphrase tables can be easily combined in the loglinear model. Specifically, feature functions are derived from each paraphrase resource and then combined with the language model feature1 : ∗ T = arg max { T N X λT M i hT M i (T, S)+ i=1 λLM hLM (T, S)} (2) where N is the number of paraphrase tables. hT M i (T, S) is the feature function based on the ith paraphrase table P Ti . hLM (T, S) is the language 1 The reordering model is not considered in our model. 1023 Sim(e1 , e2 ) P = P (r,e)∈Tr (e1 )∩Tr (e2 ) (I(e1 , r, e) (r,e)∈Tr (e1 ) I(e1 , r, e) + P + I(e2 , r"
P08-1116,N03-1024,0,0.164374,"2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in paraphrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. Pang et al. (2003) built finite state automata (FSA) from semantically equivalent translation sets based on syntactic alignment and used the FSAs in paraphrase generation. The pattern-based methods can generate complex paraphrases that usually involve syntactic variation. However, the methods were demonstrated to be of limited generality (Quirk et al., 2004). Quirk et al. (2004) first recast paraphrase generation as monolingual SMT. They generated paraphrases using a SMT system trained on parallel sentences extracted from clustered news articles. In addition, Madnani et al. (2007) also generated sentence-level"
P08-1116,I05-1011,0,0.0283694,"allison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation, i.e., the pattern-based and SMT-based methods. Automatically learned patterns have been used in paraphrase generation. For example, Barzilay and Lee (2003) applied multiple-sequence alignment (MSA) to parallel news sentences and induced paraphrasing patterns for generating new sentences. Pang"
P08-1116,W04-3219,0,0.790425,"r contexts were extracted as paraphrases. In addition, Finch et al. (2005) applied MT evaluation methods (BLEU, NIST, WER and PER) to build classifiers for paraphrase identification. Monolingual parallel corpora are difficult to find, especially in non-literature domains. Alternatively, some researchers utilized monolingual comparable corpora for paraphrase extraction. Different news articles reporting on the same event are commonly used as monolingual comparable corpora, from which both paraphrase patterns and phrasal paraphrases can be derived (Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004). Lin and Pantel (2001) learned paraphrases from a parsed monolingual corpus based on an extended distributional hypothesis, where if two paths in dependency trees tend to occur in similar contexts it is hypothesized that the meanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora."
P08-1116,P02-1006,0,0.0452147,"ction 3 introduces the log-linear model for paraphrase generation. Section 4 describes the phrasal paraphrase extraction from different resources. Section 5 presents the parameter estimation method. Section 6 shows the experiments and results. Section 7 draws the conclusion. 2 Related Work Paraphrases have been used in many NLP applications. In MT, Callison-Burch et al. (2006) utilized paraphrases of unseen source phrases to alleviate data sparseness. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. In QA, Lin and Pantel (2001) and Ravichandran and Hovy (2002) paraphrased the answer patterns to enhance the recall of answer extraction. In IE, Shinyama et al. (2002) automatically learned paraphrases of IE patterns to reduce the cost of creating IE patterns by hand. In MDS, McKeown et al. (2002) identified paraphrase sentences across documents before generating summarizations. In NLG, Iordanskaja et al. (1991) used paraphrases to generate more varied and fluent texts. Previous work has examined various resources for acquiring paraphrases, including thesauri, monolingual corpora, bilingual corpora, and the web. Thesauri, such as WordNet, have been wide"
P08-1116,P03-1016,1,0.890346,"eanings of the paths are similar. The monolingual corpus used in their work is not necessarily parallel or comparable. Thus it is easy to obtain. However, since this resource is used to extract paraphrase patterns other than phrasal paraphrases, we do not use it in this paper. Bannard and Callison-Burch (2005) learned phrasal paraphrases using bilingual parallel corpora. The basic idea is that if two phrases are aligned to the same translation in a foreign language, they may be paraphrases. This method has been demonstrated effective in extracting large volume of phrasal paraphrases. Besides, Wu and Zhou (2003) exploited bilingual corpora and translation information in learning synonymous collocations. In addition, some researchers extracted paraphrases from the web. For example, Ravichandran and Hovy (2002) retrieved paraphrase patterns from the web using hand-crafted queries. Pasca and Dienes (2005) extracted sentence fragments occurring in identical contexts as paraphrases from one billion web documents. Since web mining is rather time consuming, we do not exploit the web to extract paraphrases in this paper. So far, two kinds of methods have been proposed for sentence-level paraphrase generation"
P08-1116,W97-0703,0,\N,Missing
P08-1116,N03-1017,0,\N,Missing
P08-1116,C98-2122,0,\N,Missing
P09-1091,C00-1007,0,0.287256,"2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree al"
P09-1091,P06-1130,0,0.0340316,"Missing"
P09-1091,W07-2303,0,0.0447939,"zation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it i"
P09-1091,P07-1041,0,0.0700134,"ose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it is easy to integrate a variety of features, and to tune feature weights to maximize the probability. A few papers have presented maximum entropy models for word or phrase ordering (Ratnaparkhi, 2000; Filippova and Strube, 2007). However, those attempts have been limited to specialized applications, such as air travel reservation or ordering constituents of a main clause in German. This paper presents a general-purpose realizer based on log-linear models for directly linearizing dependency relations given dependency structures. We reduce the generation space by 809 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809–816, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP two techniques: the first is dividing the entire dependency tree into one-depth sub-trees and solving"
P09-1091,C08-1038,1,0.870281,"Missing"
P09-1091,D07-1028,0,0.0241365,"Missing"
P09-1091,A00-2023,0,0.0669447,"2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word o"
P09-1091,W02-2103,0,0.141967,"he features used in the log-linear model. The examples listed in the table are features of the linearization <node 6, node 4, node 3, node 5>, extracted from the sub-tree in Figure 4. In this paper, all the feature functions used in the log-linear model are n-gram probabilities. However, the log-linear framework has great potential for including other types of features. 4.3 Parameter Estimation BLEU score, a method originally proposed to automatically evaluate machine translation quality (Papineni et al., 2002), has been widely used as a metric to evaluate general-purpose sentence generation (Langkilde, 2002; White et al., 2007; Guo et al. 2008, Wan et al. 2009). The BLEU measure computes the geometric mean of the precision of n-grams of various lengths between a sentence realization and a (set of) reference(s). To estimate the parameters (λ1 ,..., λ M ) for the feature functions (h1 ,..., hM ) , we use BLEU 3 as optimization objective function and adopt the approach of minimum error rate training 2 Here the term “headword” is used to describe the word that occurs at head nodes in dependency trees. 3 The BLEU scoring script is supplied by NIST Open Machine Translation Evaluation at ftp://jaguar.n"
P09-1091,W05-1510,0,0.0148029,"ningful, grammatically correct and fluent text of a particular language. Most previous general-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc. The grammar rules are either developed by hand, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or extracted automatically from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over differe"
P09-1091,P03-1021,0,0.0262535,"Missing"
P09-1091,P02-1040,0,0.0766672,"thus the headword model is more likely to relax the data sparseness. Table 2 gives some examples of all the features used in the log-linear model. The examples listed in the table are features of the linearization <node 6, node 4, node 3, node 5>, extracted from the sub-tree in Figure 4. In this paper, all the feature functions used in the log-linear model are n-gram probabilities. However, the log-linear framework has great potential for including other types of features. 4.3 Parameter Estimation BLEU score, a method originally proposed to automatically evaluate machine translation quality (Papineni et al., 2002), has been widely used as a metric to evaluate general-purpose sentence generation (Langkilde, 2002; White et al., 2007; Guo et al. 2008, Wan et al. 2009). The BLEU measure computes the geometric mean of the precision of n-grams of various lengths between a sentence realization and a (set of) reference(s). To estimate the parameters (λ1 ,..., λ M ) for the feature functions (h1 ,..., hM ) , we use BLEU 3 as optimization objective function and adopt the approach of minimum error rate training 2 Here the term “headword” is used to describe the word that occurs at head nodes in dependency trees."
P09-1091,A00-2026,0,0.0386295,"elop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it is easy to integrate a variety of features, and to tune feature weights to maximize the probability. A few papers have presented maximum entropy models for word or phrase ordering (Ratnaparkhi, 2000; Filippova and Strube, 2007). However, those attempts have been limited to specialized applications, such as air travel reservation or ordering constituents of a main clause in German. This paper presents a general-purpose realizer based on log-linear models for directly linearizing dependency relations given dependency structures. We reduce the generation space by 809 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809–816, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP two techniques: the first is dividing the entire dependency tree into on"
P09-1091,2005.mtsummit-papers.15,0,0.0230485,"gm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model,"
P09-1091,2007.mtsummit-ucnlg.4,0,0.470821,"eral-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc. The grammar rules are either developed by hand, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or extracted automatically from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langki"
P09-1091,E09-1097,0,\N,Missing
P09-1091,W06-2931,1,\N,Missing
P09-1094,N03-1003,0,0.101039,"el specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; 834 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834–842, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Kauchak"
P09-1094,W06-2932,0,0.0139591,"araphrase collocations (PT-5): Collocations4 can cover long distance dependencies in sentences. Thus paraphrase collocations are useful for SPG. We extract collocations from a monolingual 4 Experimental Setup Our SPG decoder is developed by remodeling Moses that is widely used in SMT (Hoang and Koehn, 2008). The POS tagger and dependency parser for sentence preprocessing are SVM4 A collocation is a lexically restricted word pair with a certain syntactic relation. This work only considers verbobject collocations, e.g., &lt;promote, OBJ, trades&gt;. 838 Tool (Gimenez and Marquez, 2004) and MSTParser (McDonald et al., 2006). The language model is trained using a 9 GB English corpus. last percentage is much lower than the first two is that, for sentence similarity computation, many sentences cannot find unit replacements from the PTs that improve the similarity to the reference sentences. For the other applications, only some very short sentences cannot be paraphrased. Further results show that the average number of unit replacements in each sentence is 5.36, 4.47, and 1.87 for sentence compression, simplification, and similarity computation. It also indicates that sentence similarity computation is more difficul"
P09-1094,W07-0718,0,0.0152141,". We compute the kappa statistic between the (E) raters. Kappa is defined as K = P (A)−P 1−P (E) (Carletta, 1996), where P (A) is the proportion of times that the labels agree, and P (E) is the proportion of times that they may agree by chance. We define P (E) = 31 , as the labeling is based on three point scales. The results show that the kappa statistics for adequacy and fluency are 0.6560 and 0.6500, which indicates a substantial agreement (K: 0.610.8) according to (Landis and Koch, 1977). The 4.2 Evaluation Metrics The evaluation metrics for SPG are similar to the human evaluation for MT (Callison-Burch et al., 2007). The generated paraphrases are manually evaluated based on three criteria, i.e., adequacy, fluency, and usability, each of which has three scales from 1 to 3. Here is a brief description of the different scales for the criteria: Adequacy 1: The meaning is evidently changed. 2: The meaning is generally preserved. 3: The meaning is completely preserved. Fluency 1: The paraphrase t is incomprehensible. 2: t is comprehensible. 3: t is a flawless sentence. Usability 1: t is opposite to the application purpose. 2: t does not achieve the application. 3: t achieves the application. 5 Results and Anal"
P09-1094,P79-1016,0,0.783926,"ummarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 200"
P09-1094,J96-2004,0,0.0854548,"Missing"
P09-1094,E99-1042,0,0.684738,"a conventional SMT-based PG method. Introduction 2 Related Work Paraphrases are alternative ways that convey the same meaning. There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application. PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either han"
P09-1094,P03-1021,0,0.00751664,"ed as the count of overlapping words. corpus and use a binary classifier to recognize if any two collocations are paraphrases. Due to the space limit, we cannot introduce the detail of the approach. We assign the score “1” for any pair of paraphrase collocations. PT-5 contains 238,882 pairs of paraphrase collocations. We combine the three sub-models based on a log-linear framework and get the SPG model: p(t|s) = K X (λk k=1 + λlm X 3.6 Parameter Estimation To estimate parameters λk (1 ≤ k ≤ K), λlm , and λum , we adopt the approach of minimum error rate training (MERT) that is popular in SMT (Och, 2003). In SMT, however, the optimization objective function in MERT is the MT evaluation criteria, such as BLEU. As we analyzed above, the BLEU-style criteria cannot be adapted in SPG. We therefore introduce a new optimization objective function in this paper. The basic assumption is that a paraphrase should contain as many correct unit replacements as possible. Accordingly, we design the following criteria: Replacement precision (rp): rp assesses the precision of the unit replacements, which is defined as rp = cdev (+r)/cdev (r), where cdev (r) is the total number of unit replacements in the gener"
P09-1094,C08-1018,0,0.0908022,"ble 2: The generated paraphrases of a source sentence for different applications. The target units after replacement are shown in blue and the pattern slot fillers are in cyan. [·]phr denotes that the unit is a phrase, while [·]pat denotes that the unit is a pattern. There is no collocation replacement in this example. reimplement the methods purposely designed for these applications. Thus here we just conduct an informal comparison with these methods. Sentence compression: Sentence compression is widely studied, which is mostly reviewed as a word deletion task. Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. Besides, they also used paraphrase patterns extracted from bilingual parallel corpora (like our PT-4) as a kind of rewriting resource. However, as most other sentence compression methods, their method allows information loss after compression, which means that the generated sentences are not necessarily paraphrases of the source sentences. Sentence Simplification: Carroll et"
P09-1094,P02-1040,0,0.0860896,"n SMT, words of an input sentence should be totally translated, whereas in SPG, not all words of an input sentence need to be paraphrased. Therefore, a SPG model should be able to decide which part of a sentence needs to be paraphrased. 3. The bilingual parallel data for SMT are easy to collect. In contrast, the monolingual parallel data for SPG are not so common (Quirk et al., 2004). Thus the SPG model should be able to easily combine different resources and thereby solve the data shortage problem (Zhao et al., 2008a). 4. Methods have been proposed for automatic evaluation in MT (e.g., BLEU (Papineni et al., 2002)). The basic idea is that a translation should be scored based on their similarity to the human references. However, they cannot be adopted in SPG. The main reason is that it is more difficult to provide human references in SPG. Lin and Pantel (2001) have demonstrated that the overlapping between the automatically acquired paraphrases and handcrafted ones is very small. Thus the human references cannot properly assess the quality of the generated paraphrases. 3.2 Method Overview The SPG method proposed in this work contains three components, i.e., sentence preprocessing, paraphrase planning, a"
P09-1094,N06-2009,0,0.0694119,"osed method is promising, which generates useful paraphrases for the given applications. In addition, comparison experiments show that our method outperforms a conventional SMT-based PG method. Introduction 2 Related Work Paraphrases are alternative ways that convey the same meaning. There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application. PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase g"
P09-1094,I05-5010,0,0.372716,"NLP Kauchak and Barzilay, 2006). This kind of method usually involves two phases, i.e., candidate extraction and paraphrase validation. In the first phase, it extracts all synonyms from a thesaurus, such as WordNet, for the words to be substituted. In the second phase, it selects an optimal substitute for each given word from the synonyms according to the context in s. This kind of method is simple, since the thesaurus synonyms are easy to access. However, it cannot generate other types of paraphrases but only synonym substitution. NLG-based methods: NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005) generally involve two stages. In the first one, the source sentence s is transformed into its semantic representation r by undertaking a series of NLP processing, including morphology analyzing, syntactic parsing, semantic role labeling, etc. In the second stage, a NLG system is employed to generate a sentence t from r. s and t are paraphrases as they are both derived from r. The NLG-based methods simulate human paraphrasing behavior, i.e., understanding a sentence and presenting the meaning in another way. However, deep analysis of sentences is a big challenge. Moreover, developing a NLG sys"
P09-1094,gimenez-marquez-2004-svmtool,0,0.0369816,"1,018,371 pairs of paraphrase patterns. Paraphrase collocations (PT-5): Collocations4 can cover long distance dependencies in sentences. Thus paraphrase collocations are useful for SPG. We extract collocations from a monolingual 4 Experimental Setup Our SPG decoder is developed by remodeling Moses that is widely used in SMT (Hoang and Koehn, 2008). The POS tagger and dependency parser for sentence preprocessing are SVM4 A collocation is a lexically restricted word pair with a certain syntactic relation. This work only considers verbobject collocations, e.g., &lt;promote, OBJ, trades&gt;. 838 Tool (Gimenez and Marquez, 2004) and MSTParser (McDonald et al., 2006). The language model is trained using a 9 GB English corpus. last percentage is much lower than the first two is that, for sentence similarity computation, many sentences cannot find unit replacements from the PTs that improve the similarity to the reference sentences. For the other applications, only some very short sentences cannot be paraphrased. Further results show that the average number of unit replacements in each sentence is 5.36, 4.47, and 1.87 for sentence compression, simplification, and similarity computation. It also indicates that sentence s"
P09-1094,W04-3219,0,0.928563,"a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; 834 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834–842, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Kauchak and Barzilay, 2006). This kind of method usually involves two phases, i.e., candidate extraction and paraphrase validation. In the first phase, it extracts all synonyms from a thesaurus, such as WordNet, for the words to be substituted. In the seco"
P09-1094,W08-0510,0,0.0131131,"pplied the approach proposed in (Zhao et al., 2008b). Its basic assumption is that if two English patterns e1 and e2 are aligned with the same foreign pattern f , then e1 and e2 are possible paraphrases. One can refer to (Zhao et al., 2008b) for the details. PT-4 contains 1,018,371 pairs of paraphrase patterns. Paraphrase collocations (PT-5): Collocations4 can cover long distance dependencies in sentences. Thus paraphrase collocations are useful for SPG. We extract collocations from a monolingual 4 Experimental Setup Our SPG decoder is developed by remodeling Moses that is widely used in SMT (Hoang and Koehn, 2008). The POS tagger and dependency parser for sentence preprocessing are SVM4 A collocation is a lexically restricted word pair with a certain syntactic relation. This work only considers verbobject collocations, e.g., &lt;promote, OBJ, trades&gt;. 838 Tool (Gimenez and Marquez, 2004) and MSTParser (McDonald et al., 2006). The language model is trained using a 9 GB English corpus. last percentage is much lower than the first two is that, for sentence similarity computation, many sentences cannot find unit replacements from the PTs that improve the similarity to the reference sentences. For the other ap"
P09-1094,P08-1116,1,0.712766,"r paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; 834 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834–842, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Kauchak and Barzilay, 2006"
P09-1094,P08-1089,1,0.908289,"r paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous. Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods. However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004). Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; 834 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 834–842, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Kauchak and Barzilay, 2006"
P09-1094,N06-1058,0,0.47651,"e meaning. There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application. PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually wri"
P09-1094,N06-1057,0,0.148086,"search of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application. PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006). This paper presents a method for statistical paraphrase generation (SPG). As far as we know, this is the first statistical model specially designed for paraphrase generation. It’s distinguishing feature is that it achieves various applications with a Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected. In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001"
P09-1094,N03-1017,0,0.00997098,"s, respectively1 . Paraphrase Model: Paraphrase generation is a decoding process. The input sentence s is first segmented into a sequence of I units s¯I1 , which are then paraphrased to a sequence of units t¯I1 . Let (¯ si , t¯i ) be a pair of paraphrase units, their paraphrase likelihood is computed using a score function φpm (¯ si , t¯i ). Thus the paraphrase score I I ¯ ppm (¯ s1 , t1 ) between s and t is decomposed into: ppm (¯ sI1 , t¯I1 ) = I Y φpm (¯ si , t¯i )λpm (1) i=1 where λpm is the weight of the paraphrase model. Actually, it is defined similarly to the translation model in SMT (Koehn et al., 2003). In practice, the units of a sentence may be paraphrased using different PTs. Suppose we have K PTs, (¯ ski , t¯ki ) is a pair of paraphrase units from the k-th PT with the score function φk (¯ ski , t¯ki ), then Equation (1) can be rewritten as: ppm (¯ sI1 , t¯I1 ) = K Y Y ( φk (¯ ski , t¯ki )λk ) (2) k=1 ki 3.4 Paraphrase Generation where λk is the weight for φk (¯ ski , t¯ki ). Equation (2) assumes that a pair of paraphrase units is from only one paraphrase table. However, Our SPG model contains three sub-models: a paraphrase model, a language model, and a usability model, which control th"
P09-1094,W03-1601,0,0.52605,"Missing"
P11-1104,P06-1067,0,0.0460858,"Missing"
P11-1104,J93-2003,0,0.0367397,"Missing"
P11-1104,P05-1033,0,0.640525,"EU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source"
P11-1104,N10-1127,0,0.0615614,"n IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA)"
P11-1104,W04-3250,0,0.0191744,"toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions that Reorder models Monotone model DBR model MSDR model (Baseline) DBR model SCBR Model 1 SCBR Model 2 MSDR+ SCBR Model 3 SCBR models (1+2) SCBR models (1+2+3) MT04 MT08 26.99 18.30 26.64"
P11-1104,N03-1017,0,0.0420923,"rce token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our metho"
P11-1104,2005.iwslt-1.8,0,0.0286277,"or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straig"
P11-1104,D09-1051,1,0.918596,"ence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing additional resources (Liu et al., 2009), and then the reordering model based on the detected collocations is learned from the word-aligned bilingual corpus. The source collocation based reordering model is integrated into SMT systems as an additional feature to softly constrain the translation orders of the source collocations in the sentence to be translated, so as to constrain the translation orders of those source phrases containing these collocated words. This method has two advantages: (1) it can automatically detect and leverage collocated words in a sentence, including long-distance collocated words; (2) such a reordering mo"
P11-1104,P08-1114,0,0.0864168,"r SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignm"
P11-1104,P03-1021,0,0.00926147,"ic function for estimating future score be determined according to the relative positions of the words and the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost i"
P11-1104,J03-1002,0,0.00360503,"in (10). L Input: Input sentence F  f 1 Initialization: Score = 0 for each uncovered word f i do e4 e3 for each word f j ( j  ci or r ( f i , f j )   ) do e2 if f j is covered then e1 f1 f2 f3 if i > j then Score+= r ( f i , f j ) log p (o  straight |f i , f j ) f4 f5 else Score+= r ( f i , f j ) log p (o  inverted |f i , f j ) Figure 2. An example for reordering 4 4.1 else Score += arg max o r ( f i , f j ) log p (o |f i , f j ) Evaluation of Our Method Output: Score Implementation We implemented our method in a phrase-based SMT system (Koehn et al., 2007). Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. Thus, given a sentence to be translated, we first identify the collocations in the sentence, and then estimate the reordering score according to the translation hypothesis. For a translation option to be expanded, the reordering score inside this source phrase is calculated according to their translation orders of the collocations in the corresponding target phrase. The reordering score crossing the current translation option and the covered parts can be calculated according to the relative position of the collocated words. If the source p"
P11-1104,P02-1040,0,0.0825739,"d the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan & Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions t"
P11-1104,N04-4026,0,0.0295724,"led as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimat"
P11-1104,P05-1069,0,0.0191896,"though the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by con"
P11-1104,C10-1126,0,0.0841098,"ls (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing add"
P11-1104,J97-3002,0,0.675665,"ng decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the"
P11-1104,P06-1066,0,0.120655,"ted in the same order as in the source language, or in the inverted order. We name the first case as straight, and the second inverted. Based on the observation that some collocations tend to have fixed translation orders such as “金融 jin-rong „financial‟ 危 机 wei-ji „crisis‟” (financial crisis) whose English translation order is usually straight, and “ 法 律 fa-lv „law‟ 范 围 fan-wei „scope‟” (scope of law) whose English translation order is generally inverted, some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al., 2006). We further notice that some words are translated in different orders when they are collocated with different words. For instance, when “潮流 chao-liu „trend‟” is collocated with “时代 shi-dai „times‟”, they are often translated into the “trend of times”; when collocated with “历史 li-shi „history‟”, the translation usually becomes the “historical trend”. Thus, if we can automatically detect the collocations in the sentence to be translated and their orders in the target language, the reordering information of the collocations could be used to constrain the reordering of phrases during decoding. Th"
P11-1104,P03-1019,0,0.0237065,"n quality. 6 Related Work Reordering was first proposed in the IBM models (Brown et al., 1993), later was named IBM constraint by Berger et al. (1996). This model treats the source word sequence as a coverage set that is processed sequentially and a source token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target"
P11-1104,W06-3108,0,0.0263897,"allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecu"
P11-1104,D07-1056,0,0.0844338,"uction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using"
P11-1104,P07-2045,0,\N,Missing
P12-1071,P11-1070,0,0.0244387,"Missing"
P12-1071,W09-1210,0,0.0236959,"Missing"
P12-1071,D08-1092,0,0.0423625,"ect comparison indicates that our approach also outperforms previous work based on treebank conversion. # of Words 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Grammar Phrase structure Phrase structure Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Corresp"
P12-1071,W10-2906,0,0.0128754,"so outperforms previous work based on treebank conversion. # of Words 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Grammar Phrase structure Phrase structure Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorpo"
P12-1071,P05-1022,0,0.228193,"Missing"
P12-1071,A00-2018,0,0.417901,"Missing"
P12-1071,W09-1207,1,0.900322,"Missing"
P12-1071,D09-1060,0,0.078901,"Missing"
P12-1071,P10-1003,0,0.0547958,"s work based on treebank conversion. # of Words 0.51 million 0.78 million 1.11 million 0.36 million about 1 million Grammar Phrase structure Phrase structure Dependency structure Phrase structure Phrase structure Table 1: Several publicly available Chinese treebanks. Therefore, studies have recently resorted to other resources for the enhancement of parsing models, such as large-scale unlabeled data (Koo et al., 2008; Chen et al., 2009; Bansal and Klein, 2011; Zhou et al., 2011), and bilingual texts or cross-lingual treebanks (Burkett and Klein, 2008; Huang et al., 2009; Burkett et al., 2010; Chen et al., 2010). 1 Introduction The scale of available labeled data significantly affects the performance of statistical data-driven models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased"
P12-1071,P99-1065,0,0.119653,"In contrast, we experiment with two real large-scale treebanks, and boost the stateof-the-art parsing accuracy using QG features. Second, we explore much richer QG features to fully exploit the knowledge of the source treebank. These features are tailored to the dependency parsing problem. In summary, the present work makes substantial progress in modeling structural annotation inconsistencies with QG features for parsing. Previous work on treebank conversion primarily focuses on converting one grammar formalism of a treebank into another and then conducting a study on the converted treebank (Collins et al., 1999; Xia et al., 2008). The work by Niu et al. (2009) is, to our knowledge, the only study to date that combines the converted treebank with the existing target treebank. They automatically convert the dependency-structure CDT into the phrase-structure style of CTB5 using a statistical constituency parser trained on CTB5. Their experiments show that the combined treebank can significantly improve the performance of constituency parsers. However, their method requires several sophisticated strategies, such as corpus weighting and score interpolation, to reduce the influence of conversion errors. I"
P12-1071,W02-1001,0,0.0341419,"Missing"
P12-1071,W09-1205,0,0.0613185,"Missing"
P12-1071,D11-1044,0,0.0868094,"ions, and the score of a target dependency tree becomes Source Treebank S={(xi, di)}i Train Target Treebank T={(xj, dj)}j Source Parser ParserS Parse Out Parsed Treebank TS={(xj, djS)}j Target Treebank with Source Annotations T+S={(xj, djS, dj)}j Train Target Parser ParserT Score(x, t, d′ , d) =Scorebs (x, t, d) Figure 3: Framework of our approach. +Scoreqg (x, t, d′ , d) portion of d′ , and the construction of d can be inspired by arbitrary substructures of d′ . To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem consid678 The first part corresponds to the baseline model, whereas the second part is affected by the source tree d′ and can be rewritten as Scoreqg (x, t, d′ , d) = wqg · fqg (x, t, d′ , d) where fqg (.) denotes the QG features. We expect the QG features"
P12-1071,D09-1127,0,0.176739,"Missing"
P12-1071,P09-1059,0,0.148157,". We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of parsing. Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. The first part of their work is closely connected with our work, but with a few important differences. First, they conduct simulated exper"
P12-1071,P10-1001,0,0.129047,"en models. As a structural classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). 1 The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which w"
P12-1071,P08-1068,0,0.10649,"Missing"
P12-1071,D11-1109,1,0.951214,"ral classification problem that is more challenging than binary classification and sequence labeling problems, syntactic parsing is more prone to suffer from the data sparseness problem. However, the heavy cost of treebanking typically limits one single treebank in both scale and genre. At present, learning from one single treebank seems inadequate for further boosting parsing accuracy.1 ∗ Correspondence author: tliu@ir.hit.edu.cn Incorporating an increased number of global features, such as third-order features in graph-based parsers, slightly affects parsing accuracy (Koo and Collins, 2010; Li et al., 2011). 1 The existence of multiple monolingual treebanks opens another door for this issue. For example, table 1 lists a few publicly available Chinese treebanks that are motivated by different linguistic theories or applications. In the current paper, we utilize the first three treebanks, i.e., the Chinese Penn Treebank 5.1 (CTB5) and 6.0 (CTB6) (Xue et al., 2005), and the Chinese Dependency Treebank (CDT) (Liu et al., 2006). The Sinica treebank (Chen et al., 2003) and the Tsinghua Chinese Treebank (TCT) (Qiang, 2004) can be similarly exploited with our proposed approach, which we leave as future"
P12-1071,D08-1017,0,0.185678,"ch as corpus weighting and score interpolation, to reduce the influence of conversion errors. Instead of using the noisy converted treebank as additional training data, our approach allows the QGenhanced parsing models to softly learn the systematic inconsistencies based on QG features, making our approach simpler and more robust. Our approach is also intuitively related to stacked learning (SL), a machine learning framework that has recently been applied to dependency parsing to integrate two main-stream parsing models, i.e., graph-based and transition-based models (Nivre and McDonald, 2008; Martins et al., 2008). However, the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies. 3 Dependency Parsing Given an input sentence x = w0 w1 ...wn and its POS tag sequence t = t0 t1 ...tn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m, l) : 0 ≤ h ≤ n, 0 < m ≤ n, l ∈ L}, where (h, m, l) indicates an directed arc from the head word (also called father) wh to the modifier (also called child or dependent) wm with a dependency label l, and L is the label set. We omit the l"
P12-1071,E06-1011,0,0.0258575,"the highest scoring tree from a directed graph. To guarantee the efficiency of the decoding algorithms, the score of a dependency tree is factored into the scores of some small parts (subtrees). Scorebs (x, t, d) = wbs · fbs (x, t, d) X = wpart · fpart (x, t, p) h g h h m s dependency m m grandparent sibling Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3 ) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3 ) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4 ).4 For the O2 model, the score function is rewritten as: X Scorebs (x, t, d) = wdep · fdep (x, t, h, m) {(h,m)}⊆d + X wsib · fsib (x, t, h, s, m) {(h,s),(h,m)}⊆d + X wgrd · fgrd (x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep (.), fsib (.) and fgrd (.) correspond to the features for the three kinds of scoring parts. We adopt the standard features following Li et al. (2011). For the O1 and O2sib models, the above formula"
P12-1071,P05-1012,0,0.141192,"earch, we adopt the graph-based parsing models for their state-of-the-art performance in a variety of languages.3 Graph-based models view the problem as finding the highest scoring tree from a directed graph. To guarantee the efficiency of the decoding algorithms, the score of a dependency tree is factored into the scores of some small parts (subtrees). Scorebs (x, t, d) = wbs · fbs (x, t, d) X = wpart · fpart (x, t, p) h g h h m s dependency m m grandparent sibling Figure 2: Scoring parts used in our graph-based parsing models. • The first-order model (O1) only incorporates dependency parts (McDonald et al., 2005), and requires O(n3 ) parsing time. • The second-order model using only sibling parts (O2sib) includes both dependency and sibling parts (McDonald and Pereira, 2006), and needs O(n3 ) parsing time. • The second-order model (O2) uses all the scoring parts in Figure 2 (Koo and Collins, 2010). The time complexity of the decoding algorithm is O(n4 ).4 For the O2 model, the score function is rewritten as: X Scorebs (x, t, d) = wdep · fdep (x, t, h, m) {(h,m)}⊆d + X wsib · fsib (x, t, h, s, m) {(h,s),(h,m)}⊆d + X wgrd · fgrd (x, t, g, h, m) {(g,h),(h,m)}⊆d where fdep (.), fsib (.) and fgrd (.) corre"
P12-1071,P09-1006,0,0.330964,"these treebanks contain rich human knowledge on the Chinese syntax, thereby having a great deal of common ground. Therefore, exploiting multiple treebanks is very attractive for boosting parsing accuracy. Figure 1 gives an example with different an675 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 675–684, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics OBJ NMOD NMOD ROOT w0 VV NN CC NN 促进1 贸易2 和3 工业4 promote v trade n and c industry n ROOT VOB proach also outperforms the treebank conversion approach of Niu et al. (2009). 2 Related Work LAD COO Figure 1: Example with annotations from CTB5 (upper) and CDT (under). notations from CTB5 and CDT.2 This example illustrates that the two treebanks annotate coordination constructions differently. In CTB5, the last noun is the head, whereas the first noun is the head in CDT. One natural idea for multiple treebank exploitation is treebank conversion. First, the annotations in the source treebank are converted into the style of the target treebank. Then, both the converted treebank and the target treebank are combined. Finally, the combined treebank are used to train a b"
P12-1071,P08-1108,0,0.119102,"phisticated strategies, such as corpus weighting and score interpolation, to reduce the influence of conversion errors. Instead of using the noisy converted treebank as additional training data, our approach allows the QGenhanced parsing models to softly learn the systematic inconsistencies based on QG features, making our approach simpler and more robust. Our approach is also intuitively related to stacked learning (SL), a machine learning framework that has recently been applied to dependency parsing to integrate two main-stream parsing models, i.e., graph-based and transition-based models (Nivre and McDonald, 2008; Martins et al., 2008). However, the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies. 3 Dependency Parsing Given an input sentence x = w0 w1 ...wn and its POS tag sequence t = t0 t1 ...tn , the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m, l) : 0 ≤ h ≤ n, 0 < m ≤ n, l ∈ L}, where (h, m, l) indicates an directed arc from the head word (also called father) wh to the modifier (also called child or dependent) wm with a dependency label l, and L is the l"
P12-1071,W03-3017,0,0.0228695,"2sib models, the above formula is modified by deactivating the extra parts. 4 Dependency Parsing with QG Features p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs (.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3 Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. 677 Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′ , a QG defines a monolingual grammar that generates translations of x′ , which can be denoted by p(x, d, a|x′ , d′ ), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4 We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decoding procedure ("
P12-1071,W06-3104,0,0.253692,"by deactivating the extra parts. 4 Dependency Parsing with QG Features p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs (.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3 Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. 677 Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′ , a QG defines a monolingual grammar that generates translations of x′ , which can be denoted by p(x, d, a|x′ , d′ ), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4 We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decoding procedure (Koo and Collins, 2010). Target Side h Syntactic Struct"
P12-1071,D09-1086,0,0.108437,"xperiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of parsing. Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. The first part of their work is closely connected with our work, but with a few important differences. First, they conduct simulated experiments on one treebank by ma"
P12-1071,D07-1003,0,0.0514962,"ee becomes Source Treebank S={(xi, di)}i Train Target Treebank T={(xj, dj)}j Source Parser ParserS Parse Out Parsed Treebank TS={(xj, djS)}j Target Treebank with Source Annotations T+S={(xj, djS, dj)}j Train Target Parser ParserT Score(x, t, d′ , d) =Scorebs (x, t, d) Figure 3: Framework of our approach. +Scoreqg (x, t, d′ , d) portion of d′ , and the construction of d can be inspired by arbitrary substructures of d′ . To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem consid678 The first part corresponds to the baseline model, whereas the second part is affected by the source tree d′ and can be rewritten as Scoreqg (x, t, d′ , d) = wqg · fqg (x, t, d′ , d) where fqg (.) denotes the QG features. We expect the QG features to encourage or penalize certain scorin"
P12-1071,D11-1038,0,0.0160561,"rget Treebank T={(xj, dj)}j Source Parser ParserS Parse Out Parsed Treebank TS={(xj, djS)}j Target Treebank with Source Annotations T+S={(xj, djS, dj)}j Train Target Parser ParserT Score(x, t, d′ , d) =Scorebs (x, t, d) Figure 3: Framework of our approach. +Scoreqg (x, t, d′ , d) portion of d′ , and the construction of d can be inspired by arbitrary substructures of d′ . To date, QGs have been successfully applied to various tasks, such as word alignment (Smith and Eisner, 2006), machine translation (Gimpel and Smith, 2011), question answering (Wang et al., 2007), and sentence simplification (Woodsend and Lapata, 2011). In the present work, we utilize the idea of the QG for the exploitation of multiple monolingual treebanks. The key idea is to let the parse tree of one style inspire the parsing process of another style. Different from a MT process, our problem consid678 The first part corresponds to the baseline model, whereas the second part is affected by the source tree d′ and can be rewritten as Scoreqg (x, t, d′ , d) = wqg · fqg (x, t, d′ , d) where fqg (.) denotes the QG features. We expect the QG features to encourage or penalize certain scoring parts in the target side according to the source tree d"
P12-1071,W03-3023,0,0.0296712,"al. (2011). For the O1 and O2sib models, the above formula is modified by deactivating the extra parts. 4 Dependency Parsing with QG Features p⊆d where p is a scoring part which contains one or more dependencies of d, and fbs (.) denotes the basic parsing features, as opposed to the QG features. Figure 2 lists the scoring parts used in our work, where g, h, m, and s, are word indices. We implement three parsing models of varying strengths in capturing features to better understand the effect of the proposed QG features. 3 Our approach can equally be applied to transition-based parsing models (Yamada and Matsumoto, 2003; Nivre, 2003) with minor modifications. 677 Smith and Eisner (2006) propose the QG for machine translation (MT) problems, allowing greater syntactic divergences between the two languages. Given a source sentence x′ and its syntactic tree d′ , a QG defines a monolingual grammar that generates translations of x′ , which can be denoted by p(x, d, a|x′ , d′ ), where x and d refer to a translation and its parse, and a is a cross-language alignment. Under a QG, any portion of d can be aligned to any 4 We use the coarse-to-fine strategy to prune the search space, which largely accelerates the decodi"
P12-1071,P08-1101,0,0.261243,"ling the annotation inconsistencies using transformation patterns (TP). The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees. We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of pa"
P12-1071,D08-1059,0,0.394174,"ling the annotation inconsistencies using transformation patterns (TP). The TPs are used to compose quasi-synchronous grammar (QG) features, such that the knowledge of the source treebank can inspire the target parser to build better trees. We conduct extensive experiments using CDT as the source treebank to enhance two target treebanks (CTB5 and CTB6). Results show that our approach can significantly boost state-of-the-art parsing accuracy. Moreover, an indirect comparison indicates that our ap2 CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). Notably, converting a phrase-structure tree into its dependency-structure counterpart is straightforward and can be performed by applying heuristic head-finding rules. 676 The present work is primarily inspired by Jiang et al. (2009) and Smith and Eisner (2009). Jiang et al. (2009) improve the performance of word segmentation and part-of-speech (POS) tagging on CTB5 using another large-scale corpus of different annotation standards (People’s Daily). Their framework is similar to ours. However, handling syntactic annotation inconsistencies is significantly more challenging in our case of pa"
P12-1071,P11-2033,0,0.0579417,"m) ◦ dist(h, m) to form new features. Corpus Train Dev Test PD 281,311 5,000 10,000 CDT 55,500 1,500 3,000 CTB5 16,091 803 1,910 352 348 CTB5X 18,104 CTB6 22,277 1,762 2,556 Models without QG with QG O2 86.13 86.44 (+0.31, p = 0.06) O2sib 85.63 86.17 (+0.54, p = 0.003) O1 83.16 84.40 (+1.24, p < 10−5 ) Li11 86.18 — Z&N11 86.00 — Table 3: Data used in this work (in sentence number). Table 4: Parsing accuracy (UAS) comparison on CTB5test with gold-standard POS tags. Li11 refers to the second-order graph-based model of Li et al. (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). We adopt unlabeled attachment score (UAS) as the primary evaluation metric. We also use Root accuracy (RA) and complete match rate (CM) to give more insights. All metrics exclude punctuation. We adopt Dan Bikel’s randomized parsing evaluation comparator for significance test (Noreen, 1989).7 For all models used in current work (POS tagging and parsing), we adopt averaged perceptron to train the feature weights (Collins, 2002). We train each model for 10 iterations and select the parameters that perform best on the development set. 5.1 Preliminaries This subsection describes how we project th"
P12-1071,P11-1156,0,0.0529369,"Missing"
P12-1071,W09-1201,0,\N,Missing
P12-1103,2008.iwslt-papers.2,0,0.110087,"ranslation models, either from the input side, which targets on rewriting the input sentences to the MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use"
P12-1103,N06-1003,0,0.0203314,"SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for d"
P12-1103,P05-1033,0,0.063803,"eel interest that N/A blue handbag 欢迎 乘坐 I to that N/A blue handbag have interest 我 对 那 只 蓝色 手提包 有 兴趣 。 我 很 感 兴趣 那 个 蓝色 手提包 I very feel interest that N/A blue handbag 乘坐 ride 。 Figure 2: Example for Word Alignment Filtration 2. Stop words (including some function words and punctuations) can only be aligned to either stop words or null. Figure 2 illustrates an example of using the heuristics to filter alignment. 3.4 Extracting Paraphrase Rules From the word-aligned sentence pairs, we then extract a set of rules that are consistent with the word alignments. We use the rule extracting methods of Chiang (2005). Take the sentence pair in Figure 2 as an example, two initial phrase pairs 感 兴趣 那 个 蓝色 手提包” are identified, and PP1 is contained by PP2, then we could form the rule: 对 X1 有 兴趣  很 感 兴趣 X1 to have interest very feel interest 4 Paraphrasing the Input Sentences The extracted paraphrase rules aim to rewrite the input sentences to an MT-favored form which may lead to a better translation. However, it is risky to directly replace the input sentence with a paraphrased sentence, since the errors in automatic paraphrase substitution may jeopardize the translation result seriously. To avoid such damag"
P12-1103,D10-1041,0,0.0787359,"), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For example, we can only substitute play with drama in a context related to stage or theatre. Phrasal paraphrase substitutions can hardly solve such kind of problems. In this paper, we propose a method that rewrites This work was done when the first author was visit"
P12-1103,I11-1090,1,0.511378,"entences and the translation models, either from the input side, which targets on rewriting the input sentences to the MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and D"
P12-1103,N03-1017,0,0.0378794,"Missing"
P12-1103,C10-1069,0,0.0127161,"e MT-favored expressions, or from the side of translation models, which tries to enrich the translation models to cover more expressions. In recent years, paraphrasing has been proven useful for improving SMT quality. The proposed methods can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods,"
P12-1103,D09-1040,0,0.0211672,"can be classified into two categories according to the paraphrase targets: (1) enrich translation models to cover more bilingual expressions; (2) paraphrase the input sentences to reduce OOVs or generate multiple inputs. In the first category, He et al. (2011), Bond et al. (2008) and Nakov (2008) enriched the SMT models via paraphrasing the training corpora. Kuhn et al. (2010) and Max (2010) used paraphrases to smooth translation models. For the second category, previous studies mainly focus on finding translations for unknown terms using phrasal paraphrases. Callison-Burch et al. (2006) and Marton et al. (2009) paraphrase unknown terms in the input sentences using phrasal paraphrases extracted from bilingual and monolingual corpora. Mirkin et al. (2009) rewrite OOVs with entailments and paraphrases acquired from WordNet. Onishi et al. (2010) and Du et al. (2010) use phrasal paraphrases to build a word lattice to get multiple input candidates. In the above methods, only word or phrasal paraphrases are used for input sentence rewriting. No structured paraphrases on the sentence level have been investigated. However, the information in the sentence level is very important for disambiguation. For exampl"
P12-1103,P00-1056,0,0.0382139,"e is: T1 = SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). Finally we compute BLEU (Papineni et al. 2002) score for every sentence in T2 and T1, using the corresponding sentence in T0 as reference. If the sentence in T2 has a higher BLEU score than the aligned sentence in T1, the corresponding sentences in S0 and S1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions. 3.3 Word Alignments Filtering We can construct word alignment between S0 and S1 through T0. On the initial corpus of (S0, T0), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005) for symmetrization. Because S1 is generated by feeding T0 into the PBMT system SYS_TS, the word alignment between T0 and S1 can be acquired from the verbose information of the decoder. The word alignments of S0 and S1 contain noises which are produced by either wrong alignment of GIZA++ or translation errors of SYS_TS. To ensure the alignment quality, we use some heuristics to filter the alignment between S0 and S1: 1. If two identical words are aligned in S0 and S1, then remove all the other links to the two"
P12-1103,P03-1021,0,0.0399546,". Taking α as an example, firstly, p – 1 nodes are created, and then p edges labeled with αj (1 ≤ j ≤ p) are generated to connect node θx-1, p-1 nodes and θy-1. Via step 2, word lattices are generated by adding new nodes and edges coming from paraphrases. 983 Experiments Experimental Data In our experiments, we used Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. The alignment was obtained using GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the growdiag-final heuristic. Parameters were tuned using Minimum Error Rate Training (Och, 2003). To comprehensively evaluate the proposed methods in different domains, two groups of experiments were carried out, namely, the oral group (Goral) and the news group (Gnews). The experiments were conducted in both Chinese-English and EnglishChinese directions for the oral group, and ChineseEnglish direction for the news group. The English sentences were all tokenized and lowercased, and the Chinese sentences were segmented into words by Language Technology Platform (LTP) 1 . We used SRILM2 for the training of language models (5-gram in all the experiments). The metrics for automatic evaluatio"
P12-1103,P02-1040,0,0.0861493,"n be paraphrased to RHS. Taking Chinese as a case 981 study, some examples of paraphrase rules are shown in Table 1. 3.2 Selecting Paraphrase Sentence Pairs Following the methods in Section 2, the initial bilingual corpus is (S0, T0). We train a source-totarget PBMT system (SYS_ST) and a target-tosource PBMT system (SYS_TS) on the parallel corpus. Then a Forward-Translation is performed on S0 using SYS_ST, and a Back-Translation is performed on T0 using SYS_TS and SYS_ST. As mentioned above, the detailed procedure is: T1 = SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). Finally we compute BLEU (Papineni et al. 2002) score for every sentence in T2 and T1, using the corresponding sentence in T0 as reference. If the sentence in T2 has a higher BLEU score than the aligned sentence in T1, the corresponding sentences in S0 and S1 are selected as candidate paraphrase sentence pairs, which are used in the following steps of paraphrase extractions. 3.3 Word Alignments Filtering We can construct word alignment between S0 and S1 through T0. On the initial corpus of (S0, T0), we conduct word alignment with Giza++ (Och and Ney, 2000) in both directions and then apply the grow-diag-final heuristic (Koehn et al., 2005)"
P12-1103,P09-2034,0,0.0583384,"Missing"
P12-1103,2010.eamt-1.34,0,0.0705043,"Missing"
P12-1103,D10-1064,0,\N,Missing
P12-1103,P07-2045,0,\N,Missing
P12-1103,P09-1089,0,\N,Missing
P12-1103,P10-2001,0,\N,Missing
P12-1103,W04-3250,0,\N,Missing
P12-1103,2005.iwslt-1.8,0,\N,Missing
P12-2003,W04-3224,0,0.0174745,"arsing algorithms specifically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2), with gold wor"
P12-2003,C10-1011,0,0.0646138,"eBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2), with gold word segmentation, following the guidelines suggested in documentation. 2.3 Settings Every parser was run with its own default options. However, since the default classifier used by MaltParser is libsvm (Chang and Lin, 2"
P12-2003,W06-2920,0,0.135444,"Missing"
P12-2003,cer-etal-2010-parsing,0,0.0769393,"Missing"
P12-2003,W09-2307,0,0.205061,"ndencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence. This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise. Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al., 2009), as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010; Banko et al., 2007) and sentiment analysis (Meena and Prabhakar, 2007). In addition to English, there is a Chinese version of Stanford dependencies (Chang et al., 2009), which is also useful for many applications, such as Chinese sentiment analysis (Wu et al., 2011; Wu et al., 2009; Zhuang et al., 2006) and relation extraction (Huang et al., 2008). Figure 1 shows a sample constituent parse tree and the corresponding Stanford dependencies for a sentence in Chinese. Although there are several variants of Stanford dependencies for English,1 so far only a basic version (i.e, dependency tree structures) is available for Chinese. Stanford dependencies were originally obtained from constituent trees, using rules (de Marneffe et al., 2006). But as dependency parsing"
P12-2003,P05-1022,0,0.271574,"Missing"
P12-2003,A00-2018,0,0.645992,"fically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2), with gold word segmentation, f"
P12-2003,W08-1301,0,0.169556,"Missing"
P12-2003,de-marneffe-etal-2006-generating,0,0.0727123,"Missing"
P12-2003,W09-1401,0,0.0111235,"ord dependencies. Figure 1: A sample Chinese constituent parse tree and its corresponding Stanford dependencies for the sentence China (中国) encourages (鼓励) private (民营) entrepreneurs (企业家) to invest (投资) in national (国家) infrastructure (基础) construction (建设). Introduction Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence. This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise. Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al., 2009), as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010; Banko et al., 2007) and sentiment analysis (Meena and Prabhakar, 2007). In addition to English, there is a Chinese version of Stanford dependencies (Chang et al., 2009), which is also useful for many applications, such as Chinese sentiment analysis (Wu et al., 2011; Wu et al., 2009; Zhuang et al., 2006) and relation extraction (Huang et al., 2008). Figure 1 shows a sample constituent parse tree and the corresponding Stanford dependencies for a sentence in Chinese. Although th"
P12-2003,P03-1054,0,0.13024,"ituent parse tree and the corresponding Stanford dependencies for a sentence in Chinese. Although there are several variants of Stanford dependencies for English,1 so far only a basic version (i.e, dependency tree structures) is available for Chinese. Stanford dependencies were originally obtained from constituent trees, using rules (de Marneffe et al., 2006). But as dependency parsing technologies mature (K¨ubler et al., 2009), they offer increasingly attractive alternatives that eliminate the need for an intermediate representation. Cer et al. (2010) reported that Stanford’s implementation (Klein and Manning, 2003) underperforms other constituent 1 nlp.stanford.edu/software/dependencies_manual.pdf 11 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 11–16, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Type Constituent Dependency Parser Version Algorithm URL Berkeley Bikel Charniak Stanford MaltParser Mate MSTParser 1.1 1.2 Nov. 2009 2.0 1.6.1 2.0 0.5 PCFG PCFG PCFG Factored Arc-Eager 2nd-order MST MST code.google.com/p/berkeleyparser www.cis.upenn.edu/˜dbikel/download.html www.cog.brown.edu/˜mj/Software.htm nlp.stan"
P12-2003,P10-1001,0,0.0421575,"mj/Software.htm nlp.stanford.edu/software/lex-parser.shtml maltparser.org code.google.com/p/mate-tools sourceforge.net/projects/mstparser Table 1: Basic information for the seven parsers included in our experiments. parsers, for English, on both accuracy and speed. Their thorough investigation also showed that constituent parsers systematically outperform parsing directly to Stanford dependencies. Nevertheless, relative standings could have changed in recent years: dependency parsers are now significantly more accurate, thanks to advances like the high-order maximum spanning tree (MST) model (Koo and Collins, 2010) for graph-based dependency parsing (McDonald and Pereira, 2006). Therefore, we deemed it important to re-evaluate the performance of constituent and dependency parsers. But the main purpose of our work is to apply the more sophisticated dependency parsing algorithms specifically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accurac"
P12-2003,P08-1068,0,0.156884,"Missing"
P12-2003,D11-1109,1,0.835978,"e difference between their accuracies is not statistically significant (p &gt; 0.05).9 Table 4 highlights performance (F1 scores) for the most frequent relation labels. Mate does better on most relations, noun compound modifiers (nn) and adjectival modifiers (amod) in particular; and the Berkeley parser is better at root and dep.10 Mate seems to excel at short-distance dependencies, possibly because it uses more local features (even with a second-order model) than the Berkeley parser, whose PCFG can capture longer-distance rules. Since POS-tags are especially informative of Chinese dependencies (Li et al., 2011), we harmonized training and test data, using 10-way jackknifing (see §2.4). This method is more robust than training a 7 One (small) factor contributing to the difference between the two languages is that in the Chinese setup we stop with basic Stanford dependencies — there is no penalty for further conversion; another is not using discriminative reranking for Chinese. 8 sites.google.com/site/sancl2012/home/shared-task 9 For LAS, p ≈ 0.11; and for UAS, p ≈ 0.25, according to www.cis.upenn.edu/˜dbikel/download/compare.pl 6 10 ilk.uvt.nl/conll/software/eval.pl 13 An unmatched (default) relation"
P12-2003,E06-1011,0,0.0748711,"maltparser.org code.google.com/p/mate-tools sourceforge.net/projects/mstparser Table 1: Basic information for the seven parsers included in our experiments. parsers, for English, on both accuracy and speed. Their thorough investigation also showed that constituent parsers systematically outperform parsing directly to Stanford dependencies. Nevertheless, relative standings could have changed in recent years: dependency parsers are now significantly more accurate, thanks to advances like the high-order maximum spanning tree (MST) model (Koo and Collins, 2010) for graph-based dependency parsing (McDonald and Pereira, 2006). Therefore, we deemed it important to re-evaluate the performance of constituent and dependency parsers. But the main purpose of our work is to apply the more sophisticated dependency parsing algorithms specifically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-use"
P12-2003,P08-1108,0,0.220944,"Missing"
P12-2003,nivre-etal-2006-maltparser,0,0.070012,"Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2), with gold word segmentation, following the guidelines suggested in documentation. 2.3 Settings Every parser was run with its own default options. However, since the default classifier used by MaltParser is lib"
P12-2003,P06-1055,0,0.13597,"isticated dependency parsing algorithms specifically to Chinese. Number of in files sentences tokens Train 2,083 46,572 1,039,942 Dev 160 2,079 59,955 Test 205 2,796 81,578 Total 2,448 51,447 1,181,475 Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data. 2 Methodology We compared seven popular open source constituent and dependency parsers, focusing on both accuracy and parsing speed. We hope that our analysis will help end-users select a suitable method for parsing to Stanford dependencies in their own applications. 2.1 Parsers We considered four constituent parsers. They are: Berkeley (Petrov et al., 2006), Bikel (2004), Charniak (2000) and Stanford (Klein and Manning, 2003) chineseFactored, which is also the default used by Stanford dependencies. The three dependency parsers are: MaltParser (Nivre et al., 2006), Mate (Bohnet, 2010)2 and MSTParser (McDonald and Pereira, 2006). Table 1 has more information. 2 A second-order MST parser (with the speed optimization). 12 2.2 Corpus We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions. We split the data into train/development/test sets (see Table 2),"
P12-2003,D10-1001,0,0.0423331,"open source parsers — four constituent and three dependency — for generating Stanford dependencies in Chinese. Mate, a high-order MST dependency parser, with lemmatization and jackknifed POS-tags, appears most accurate; but Berkeley’s faster constituent parser, with jointly-inferred tags, is statistically no worse. This outcome is different from English, where constituent parsers systematically outperform direct methods. Though Mate scored higher overall, Berkeley’s parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al., 2010, §4.2). Acknowledgments We thank Daniel Cer, for helping us replicate the English experimental setup and for suggesting that we explore jackknifing methods, and the anonymous reviewers, for valuable comments. Supported in part by the National Natural Science Foundation of China (NSFC) via grant 61133012, the National “863” Major Project grant 2011AA01A207, and the National “863” Leading Technology Research Project grant 2012AA011102. 13 w3.msi.vxu.se/˜nivre/research/Penn2Malt.html Second author gratefully acknowledges the continued help and support of his advisor, Dan Jurafsky, and of the Def"
P12-2003,N10-1091,0,0.0796641,"Missing"
P12-2003,N03-1033,0,0.011567,"its own default options. However, since the default classifier used by MaltParser is libsvm (Chang and Lin, 2011) with a polynomial kernel, it may be too slow for training models on all of CTB 7.0 training data in acceptable time. Therefore, we also tested this particular parser with the faster liblinear (Fan et al., 2008) classifier. All experiments were performed on a machine with Intel’s Xeon E5620 2.40GHz CPU and 24GB RAM. 2.4 Features Unlike constituent parsers, dependency models require exogenous part-of-speech (POS) tags, both in training and in inference. We used the Stanford tagger (Toutanova et al., 2003) v3.1, with the MEMM model,4 in combination with 10-way jackknifing.5 Word lemmas — which are generalizations of words — are another feature known to be useful for dependency parsing. Here we lemmatized each Chinese word down to its last character, since — in contrast to English — a Chinese word’s suffix often carries that word’s core sense (Tseng et al., 2005). For example, bicycle (自 行 车 ), car (汽 车 ) and train (火车 车) are all various kinds of vehicle (车). 3 www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2010T07 4 nlp.stanford.edu/software/tagger.shtml 5 Training sentences in each f"
P12-2003,I05-3005,0,0.0197428,"d on a machine with Intel’s Xeon E5620 2.40GHz CPU and 24GB RAM. 2.4 Features Unlike constituent parsers, dependency models require exogenous part-of-speech (POS) tags, both in training and in inference. We used the Stanford tagger (Toutanova et al., 2003) v3.1, with the MEMM model,4 in combination with 10-way jackknifing.5 Word lemmas — which are generalizations of words — are another feature known to be useful for dependency parsing. Here we lemmatized each Chinese word down to its last character, since — in contrast to English — a Chinese word’s suffix often carries that word’s core sense (Tseng et al., 2005). For example, bicycle (自 行 车 ), car (汽 车 ) and train (火车 车) are all various kinds of vehicle (车). 3 www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2010T07 4 nlp.stanford.edu/software/tagger.shtml 5 Training sentences in each fold were tagged using a model based on the other nine folds; development and test sentences were tagged using a model based on all ten of the training folds. Type Constituent Dependency Parser Dev UAS LAS Test UAS LAS 82.0 79.4 77.8 76.9 76.0 77.3 82.8 78.8 82.9 80.0 78.3 77.3 76.3 78.0 83.1 78.9 Berkeley Bikel Charniak Stanford MaltParser (liblinear) MaltParse"
P12-2003,P10-1013,0,0.0281581,"e sentence China (中国) encourages (鼓励) private (民营) entrepreneurs (企业家) to invest (投资) in national (国家) infrastructure (基础) construction (建设). Introduction Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence. This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise. Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al., 2009), as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010; Banko et al., 2007) and sentiment analysis (Meena and Prabhakar, 2007). In addition to English, there is a Chinese version of Stanford dependencies (Chang et al., 2009), which is also useful for many applications, such as Chinese sentiment analysis (Wu et al., 2011; Wu et al., 2009; Zhuang et al., 2006) and relation extraction (Huang et al., 2008). Figure 1 shows a sample constituent parse tree and the corresponding Stanford dependencies for a sentence in Chinese. Although there are several variants of Stanford dependencies for English,1 so far only a basic version (i.e, dependency tree stru"
P12-2003,D09-1159,0,0.0350361,"Missing"
P12-2003,D11-1123,0,0.0367332,"Missing"
P12-2003,P11-2033,0,0.190951,"Missing"
P12-2003,C10-2013,0,\N,Missing
P12-2003,W08-2121,0,\N,Missing
P12-2003,D07-1096,0,\N,Missing
P13-1013,P12-1110,0,0.289433,"syntax trees (Figure 1(b)). With richer information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-s"
P13-1013,P09-1058,0,0.179797,"Missing"
P13-1013,D12-1132,0,0.68711,"1(b)). With richer information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging"
P13-1013,P11-1141,0,0.689122,"c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (constituent) trees, adding recursive structures of characters for words. We manually annotate the structures of 37,382 words, which cover the entire CTB5. Using these annotations, we transform CTB-style constituent trees into character-level trees (Figure 1(b)). Our word structure corpus, together with a set of tools to transform CTB-style trees into character-level trees, is released at https://github.com/zhangmeishan/wordstructures. Our annotation work is in line with the work of Vadas and Curran (2007) and Li (2011), which provide extended annotations of Penn Treebank (PTB) noun phrases and CTB words (on the morphological level), respectively. NN-rNN-r NN-r NN-r NN-bNN-b NN-iNN-i NN-b NN-b NN-i NN-i NN-lNN-l NN-l NN-l VV-bVV-b VV-iVV-i VV-b VV-b VV-i VV-i 库 库 存 存 考 考 古 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (saving) (investigate) (investigate)(ancient) (ancient) (repository) (a) subject-predicate. (b) verb-object. NN-rNN-r NN-cNN-c NNc NN-r NN-r NN-c NN-bNN-b NN-iNN-i NN-bNN-b NN-iNN-i NN-b NN-b NN-b NN-i NN-i NN"
P13-1013,W03-1025,0,0.0140119,"for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging and parsing to be performed jointly, using an efficient CKY-style or shift-reduce algorithm. Luo (2003) exploited this advantage by adding flat word structures without manually annotation to CTB trees, and building a generative character-based parser. Compared to a pipeline system, the advantages of a joint system include reduction of error propagation, and the integration of segmentation, POS tagging and syntax features. With hierarchical structures and head character information, our annotated words are more informative than flat word structures, and hence can bring further improvements to phrase-structure parsing. 2 Word Structures and Syntax Trees The Chinese language is a character-based l"
P13-1013,W12-6304,0,0.0342964,"ore informative than flat word structures, and hence can bring further improvements to phrase-structure parsing. 2 Word Structures and Syntax Trees The Chinese language is a character-based language. Unlike alphabetical languages, Chinese characters convey meanings, and the meaning of most Chinese words takes roots in their character. For example, the word “计算机 (computer)” is composed of the characters “计 (count)”, “算 (calculate)” and “机 (machine)”. An informal name of “computer” is “电脑”, which is composed of “电 (electronic)” and “脑 (brain)”. Chinese words have internal structures (Xue, 2001; Ma et al., 2012). The way characters interact within words can be similar to the way words interact within phrases. Figure 2 shows the structures of the four words “库存 (repertory)”, “ 考古 To analyze word structures in addition to phrase structures, our character-based parser naturally performs joint word segmentation, POS tagging and parsing jointly. Our model is based on the discriminative shift-reduce parser of Zhang and Clark (2009; 2011), which is a state-of-the-art word-based phrase-structure parser for Chinese. We extend their shift-reduce framework, adding more transition actions for word segmentation a"
P13-1013,W04-3236,0,0.0796187,"and morphological-level word structures. 朋友 (friend) 们 (plural) 教育 (education) 界 (field) structures. For each word or subword, we specify its POS and head direction. We use “l”, “r” and “c” to indicate the “left”, “right” and “coordination” head directions, respectively. The “coordination” direction is mostly used in coordination structures, while a very small number of transliteration words, such as “奥巴马 (Obama)” and “洛 杉矶 (Los Angeles)”, have flat structures, and we use “coordination” for their left binarization. For leaf characters, we follow previous work on word segmentation (Xue, 2003; Ng and Low, 2004), and use “b” and “i” to indicate the beginning and nonbeginning characters of a word, respectively. The vast majority of words do not have structural ambiguities. However, the structures of some words may vary according to different POS. For example, “制 服” means “dominate” when it is tagged as a verb, of which the head is the left character; the same word means “uniform dress” when tagged as a noun, of which the head is the right character. Thus the input of the word structure annotation is a word together with its POS. The annotation work was conducted by three persons, with one person annot"
P13-1013,P04-1015,0,0.337377,"erform word segmentation, POS tagging and phrase-structure parsing. To our knowledge, this is the first work to develop a transition-based system that jointly performs the above three tasks. Trained using annotated word structures, our parser also analyzes the internal structures of Chinese words. Our character-based Chinese parsing model is based on the work of Zhang and Clark (2009), which is a transition-based model for lexicalized constituent parsing. They use a beam-search decoder so that the transition action sequence can be globally optimized. The averaged perceptron with early-update (Collins and Roark, 2004) is used to train the model parameters. Their transition system contains four kinds of actions: (1) SHIFT, (2) REDUCE-UNARY, (3) REDUCE-BINARY and (4) TERMINATE. The system can provide binarzied CFG trees in Chomsky Norm Form, and they present a reversible conversion procedure to map arbitrary CFG trees into binarized trees. In this work, we remain consistent with their work, using the head-finding rules of Zhang and Clark (2008), and the same binarization algorithm.1 We apply the same beam-search algorithm for decoding, and employ the averaged perceptron with early-update to train our model."
P13-1013,D10-1082,1,0.605411,"a word is in a tag dictionary, which is collected by extracting all multi-character subwords that occur more than five times in the training corpus. For string features, c0 , c−1 and c−2 represent the current character and its previous two characters, respectively; w−1 and w−2 represent the previous two words to the current character, respectively; t0 , t−1 and t−2 represent the POS tags of the current word and the previous two words, respectively. The string features are used for word segmentation and POS tagging, and are adapted from a state-of-the-art joint segmentation and tagging model (Zhang and Clark, 2010). In summary, our character-based parser contains the word-based features of constituent parser presented in Zhang and Clark (2009), the wordbased and shallow character-based features of joint word segmentation and POS tagging presented in Zhang and Clark (2010), and additionally the deep character-based features that encode word structure information, which are the first presented by this paper. 4 4.1 95 90 90 80 85 70 80 64b 16b 4b 1b 75 70 60 64b 16b 4b 1b 50 40 65 30 0 10 20 30 40 (a) Joint segmentation and POS tagging F-scores. 0 10 20 30 40 (b) Joint constituent parsing F-scores. Figure"
P13-1013,D12-1046,0,0.0993978,"information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging and parsing to be pe"
P13-1013,J11-1005,1,0.212902,"Missing"
P13-1013,P11-1139,0,0.0424243,"Missing"
P13-1013,E09-1100,0,0.443661,"Missing"
P13-1013,P07-1031,0,0.0309607,"Linguistics, pages 125–134, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (constituent) trees, adding recursive structures of characters for words. We manually annotate the structures of 37,382 words, which cover the entire CTB5. Using these annotations, we transform CTB-style constituent trees into character-level trees (Figure 1(b)). Our word structure corpus, together with a set of tools to transform CTB-style trees into character-level trees, is released at https://github.com/zhangmeishan/wordstructures. Our annotation work is in line with the work of Vadas and Curran (2007) and Li (2011), which provide extended annotations of Penn Treebank (PTB) noun phrases and CTB words (on the morphological level), respectively. NN-rNN-r NN-r NN-r NN-bNN-b NN-iNN-i NN-b NN-b NN-i NN-i NN-lNN-l NN-l NN-l VV-bVV-b VV-iVV-i VV-b VV-b VV-i VV-i 库 库 存 存 考 考 古 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (saving) (investigate) (investigate)(ancient) (ancient) (repository) (a) subject-predicate. (b) verb-object. NN-rNN-r NN-cNN-c NNc NN-r NN-r NN-c NN-bNN-b NN-iNN-i NN-bNN-b NN-iNN-i NN-b NN-b NN-"
P13-1013,I11-1035,0,0.0680725,"Missing"
P13-1013,O03-4002,0,0.0938348,"f NN-f NN-f and morphological-level word structures. 朋友 (friend) 们 (plural) 教育 (education) 界 (field) structures. For each word or subword, we specify its POS and head direction. We use “l”, “r” and “c” to indicate the “left”, “right” and “coordination” head directions, respectively. The “coordination” direction is mostly used in coordination structures, while a very small number of transliteration words, such as “奥巴马 (Obama)” and “洛 杉矶 (Los Angeles)”, have flat structures, and we use “coordination” for their left binarization. For leaf characters, we follow previous work on word segmentation (Xue, 2003; Ng and Low, 2004), and use “b” and “i” to indicate the beginning and nonbeginning characters of a word, respectively. The vast majority of words do not have structural ambiguities. However, the structures of some words may vary according to different POS. For example, “制 服” means “dominate” when it is tagged as a verb, of which the head is the left character; the same word means “uniform dress” when tagged as a noun, of which the head is the right character. Thus the input of the word structure annotation is a word together with its POS. The annotation work was conducted by three persons, wi"
P13-1013,D08-1059,1,0.600383,"zed constituent parsing. They use a beam-search decoder so that the transition action sequence can be globally optimized. The averaged perceptron with early-update (Collins and Roark, 2004) is used to train the model parameters. Their transition system contains four kinds of actions: (1) SHIFT, (2) REDUCE-UNARY, (3) REDUCE-BINARY and (4) TERMINATE. The system can provide binarzied CFG trees in Chomsky Norm Form, and they present a reversible conversion procedure to map arbitrary CFG trees into binarized trees. In this work, we remain consistent with their work, using the head-finding rules of Zhang and Clark (2008), and the same binarization algorithm.1 We apply the same beam-search algorithm for decoding, and employ the averaged perceptron with early-update to train our model. We make two extensions to their work to enable joint segmentation, POS tagging and phrasestructure parsing from the character level. First, we modify the actions of the transition system for • SHIFT-SEPARATE(t): remove the head character cj from Q, pushing a subword node S0 2 0 cj onto S, assigning S .t = t. Note that the parse tree S0 must correspond to a full-word or a phrase node, and the character cj is the first character of"
P13-1013,W09-3825,1,0.954764,"”, “算 (calculate)” and “机 (machine)”. An informal name of “computer” is “电脑”, which is composed of “电 (electronic)” and “脑 (brain)”. Chinese words have internal structures (Xue, 2001; Ma et al., 2012). The way characters interact within words can be similar to the way words interact within phrases. Figure 2 shows the structures of the four words “库存 (repertory)”, “ 考古 To analyze word structures in addition to phrase structures, our character-based parser naturally performs joint word segmentation, POS tagging and parsing jointly. Our model is based on the discriminative shift-reduce parser of Zhang and Clark (2009; 2011), which is a state-of-the-art word-based phrase-structure parser for Chinese. We extend their shift-reduce framework, adding more transition actions for word segmentation and POS tagging, and defining novel features that capture character information. Even when trained using character-level syntax trees with flat word structures, our joint parser outperforms a strong pipelined baseline that consists of a state-of-the126 VV-lV VV-l VV-bVV-b VV-b VV-b 烧 烧 烧 (burn) 烧(burn) (burn)(burn) AD-lA AD-l AD-bAD-b AD-b AD-b 徒 徒 徒 (vain) 徒(vain) (vain)(vain) V-l VV-b 横 (fiercely) 横 (fiercely) VV-l V"
P13-1013,N07-1051,0,\N,Missing
P14-1113,D07-1017,0,0.0489721,"of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chu"
P14-1113,C10-3004,1,0.465681,"uned on a development dataset. 3.3.3 Training Data To learn the projection matrices, we extract training data from a Chinese semantic thesaurus, Tongyi Cilin (Extended) (CilinE for short) which 1202 δ Root 物 object B 昆虫 : 动物 (insect : animal) 蜻蜓 : 动物 (dragonfly : animal) 动物 animal … Sense Code: Bi 昆虫 insect 18 -- … 蜻蜓 : 昆虫 Sense Code: Bi18A 蜻蜓 dragonfly Sense Code: Bi18A06@ A … … Level 3 x Figure 4: In this example, Φk x is located in the circle with center y and radius δ. So y is considered as a hypernym of x. Conversely, y is not a hypernym of x0 . Level 5 06@ CilinE contains 100,093 words (Che et al., 2010).3 CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym–hyponym relations (right panel, Figure 3). Each word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy. The senses of words in the first level, such as “物 (object)” and “时间 (time),” are very general. The fourth level only has sense codes without real words. Therefore, we extract words in the second, third and fifth levels to constitute hypernym– hyponym pairs (left panel, Figure 3). Note that mapping one hyponym to multiple hypernyms with t"
P14-1113,W09-0215,0,0.0162212,"ance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (T"
P14-1113,D13-1122,1,0.267748,"versely, “dog” is a hyponym of “canine.” As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications. However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming. Therefore, many researchers Email correspondence. 毛茛科 Ranunculaceae 乌头属 Aconitum Introduction ∗ 生物 organism 植物 plant have attempted to automatically extract semantic relations or to construct taxonomies. A major challenge for this task is the automatic discovery of hypernym-hyponym relations. Fu et al. (2013) propose a distant supervision method to extract hypernyms for entities from multiple sources. The output of their model is a list of hypernyms for a given enity (left panel, Figure 1). However, there usually also exists hypernym–hyponym relations among these hypernyms. For instance, “植 物 (plant)” and “毛 茛 科 (Ranunculaceae)” are both hypernyms of the entity “乌 头 (aconit),” and “植 物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some pre"
P14-1113,P05-1014,0,0.450387,"hod to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Soche"
P14-1113,C92-2082,0,0.775886,"物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some previous works extend and refine manually-built semantic hierarchies by using other resources (e.g., Wikipedia) (Suchanek et al., 2008). However, the coverage is limited by the scope of the resources. Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances (Hearst, 1992; Snow et al., 2005). 1 In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages. 1199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Besides, distributional similarity methods (Kotlerman et al., 2010; Lenci and Benotto, 2012) are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts whe"
P14-1113,S12-1012,0,0.493654,"icult to guarantee. Generally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns. The distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms. For distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts. Kotlerman et al. (2010) design a directional distributional measure to infer hypernym–hyponym relations based on the standard IR Average Precision evaluation measure. Lenci and Benotto (2012) propose another measure focusing on the contexts that hypernyms do not share with their hyponyms. However, broader semantics may not always infer broader contexts. For example, for terms “Obama’ and 1200 “American people”, it is hard to say whose contexts are broader. Our previous work (Fu et al., 2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources. We assume that the hypernyms of an entity co-occur with it frequently. It works well for named entities. But for class names (e.g., singers in Hong Kong, tropical fruits) with wider range of meanin"
P14-1113,I08-2112,0,0.0347144,". One possible solution may be adding more data of this kind to the training set. 6 Related Work In addition to the works mentioned in Section 2, we introduce another set of related studies in this section. Evans (2004), Ortega-Mendoza et al. (2007), and Sang (2007) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst (1992). However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (W"
P14-1113,N13-1090,0,0.480991,"ks well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics. Moreover, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for semantic hierarchy construction based on word embeddings. Word embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and realvalued vectors. Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al., 2013b). For example, v(king) − v(queen) ≈ v(man) − v(woman), where v(w) is the embedding of the word w. We observe that a similar property also applies to the hypernym–hyponym relationship (Section 3.3), which is the main inspiration of the present study. However, we further observe that hypernym– hyponym relations are more complicated than a single offset can represent. To address this challenge, we propose a more sophisticated and general method — learning a linear projection which maps words to their hypernyms (Section 3.3.1). Furthermore, we propose a piecewise linear projection method based o"
P14-1113,P07-2042,0,0.242856,"Missing"
P14-1113,P06-1101,0,0.241765,"tes that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the"
P14-1113,D11-1014,0,0.0421,"ethods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we im"
P14-1113,P07-1058,0,0.026901,"to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity"
P14-1113,P10-1040,0,0.084559,"). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym–hypony"
P14-1113,C04-1146,0,0.0870413,"Missing"
P14-1125,D12-1133,0,0.10455,"d of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhan"
P14-1125,W06-2925,0,0.0254782,"Missing"
P14-1125,W08-0336,0,0.0274826,"flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented as both “副 (deputy) |局 长 (director)” and “副 局 长 (deputy director)”, but not “副 (deputy) 局 (office) |长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotation"
P14-1125,W09-2307,0,0.0174117,"ting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 ("
P14-1125,D09-1127,0,0.015966,"Missing"
P14-1125,P10-1001,0,0.0106105,"ake speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows"
P14-1125,D12-1132,0,0.15249,"his work, we extend their formulation, making use of largescale annotations of Zhang et al. (2013), so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-tagging and dependency parsing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)’s work. With respect to word structures, real intra-word dependencies are often more complicated, while pseudo word structures cannot be used to correctly guide segmentation. Zhao (2009), Hatori et al. (2012) and our work all study character-level dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and"
P14-1125,P12-1071,1,0.880086,"(c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy directo"
P14-1125,P11-1141,0,0.0156388,"d increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level t"
P14-1125,P13-1104,0,0.0118312,"Missing"
P14-1125,P05-1012,0,0.0232684,"make a speech 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on"
P14-1125,P04-1015,0,0.0960443,"011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features fo"
P14-1125,P05-1013,0,0.00967053,"长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, bui"
P14-1125,W02-1001,0,0.090003,"ng and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to we"
P14-1125,J08-4003,0,0.437215,"ee by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author."
P14-1125,tsarfaty-goldberg-2008-word,0,0.311687,"el dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and inter-word structures, studying their influences on each other. Zhang et al. (2013) was the first to perform Chinese syntactic parsing over characters. They extended word-level constituent trees by annotated word structures, and proposed a transition-based approach to parse intra-word structures and wordlevel constituent structures jointly. For Hebrew, Tsarfaty and Goldberg (2008) investigated joint segmentation and parsing over characters using a graph-based method. Our work is similar in exploiting character-level syntax. We study the dependency grammar, another popular syntactic representation, and propose two novel transition systems for character-level dependency parsing. Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. We extend both algorithms to character-level joint word segmentation, POS-tagging and dependency parsing. To our knowl"
P14-1125,I05-3017,0,0.0666397,"ee Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the issue that no universal standard exists for Chinese word segmentation. In the well-known Chinese word segmentation bakeoff tasks, for example, different segmentation standards have been used by different data sets (Emerson, 2005). On the other hand, most disagreement on segmentation standards boils down to disagreement on segmentation granularity. As demonstrated by Zhao (2009), one can extract both finegrained and coarse-grained words from characterlevel dependency trees, and hence can adapt to flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented a"
P14-1125,I11-1035,0,0.0124878,"a-word dependencies and pseudo inter-word dependencies; same as those of the character-level arc-standard model, shown in Table 1. 4 • STD (pseudo, real): the arc-standard model with pseudo intra-word dependencies and real inter-word dependencies; Experiments 4.1 Experimental Settings We use the Chinese Penn Treebank 5.0, 6.0 and 7.0 to conduct the experiments, splitting the corpora into training, development and test sets according to previous work. Three different splitting methods are used, namely CTB50 by Zhang and Clark (2010), CTB60 by the official documentation of CTB 6.0, and CTB70 by Wang et al. (2011). The dataset statistics are shown in Table 2. We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. The intra-word dependencies are extracted from the annotations of Zhang et al. (2013)2 . The standard measures of word-level precision, recall and F1 score are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performance of predicting word structures. A word’s structure is correct only if all the i"
P14-1125,P12-1110,0,0.264853,"ency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features for word segmentation, POS-tagging and syntactic parsing. In this section, we introduce novel extensions to the arc-standard and the arc-eager transition systems, so that word-based and characterbased features can be used simultaneously for character-level dependency parsing. 3.1 The Arc-Standard Model The arc-standard model has been applied to joint segmentation, POS-tagging and dependency parsing (Hatori et al., 2012), but with pseudo word structures. For unified processing of annotated word structures and fair comparison between character-level arc-eager and arc-standard systems, we define a different arc-standard transition system, consistent with our character-level arceager system. In the word-based arc-standard model, the transition state includes a stack and a queue, where the stack contains a sequence of partially-parsed dependency trees, and the queue consists of unprocessed input words. Four actions are defined for state transition, including arc-left (AL, which creates a left arc between the top"
P14-1125,D08-1059,1,0.940802,"ter-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Correspo"
P14-1125,P10-1110,0,0.021232,"he smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be ei"
P14-1125,D10-1082,1,0.927638,"ows an example, where the smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the s"
P14-1125,J11-1005,1,0.727336,"for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotations and based on a transition-based parsing framework (Zhang and Clark, 2011). There are two dominant transitionbased dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). We study both algorithms for characterlevel dependency parsing in order to make a comprehensive investigation. For direct comparison with word-based parsers, we incorporate the traditional word segmentation, POS-tagging and dependency parsing stages in our joint parsing models. We make changes to the original transition systems, and arrive at two novel transition-based character-level parsers. We conduct experiments on three data sets, including CTB 5.0, CTB 6.0"
P14-1125,P11-2033,1,0.953288,"intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependenc"
P14-1125,W08-0335,0,0.0212139,"hence can adapt to flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented as both “副 (deputy) |局 长 (director)” and “副 局 长 (deputy director)”, but not “副 (deputy) 局 (office) |长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al"
P14-1125,P13-1013,1,0.528047,"ng interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the i"
P14-1125,E09-1100,0,0.097413,"over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 林业局 forestry administration 上 in 发言 make a speech 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohn"
P14-1125,P13-1043,1,0.251531,"ion is performed by the SHw action. The actions for intra-word dependencies include intra-word arc-left (ALc ), intra-word arcright (ARc ), pop-word (PW) and inter-word shift (SHc ). The definitions of ALc , ARc and SHc are the same as the word-based arc-standard model, while PW changes the top element on the stack into a full-word node, which can only take interword dependencies. One thing to note is that, due to variable word sizes in character-level parsing, the number of actions can vary between different sequences of actions corresponding to different analyses. We use the padding method (Zhu et al., 2013), adding an IDLE action to finished transition action sequences, for better alignments between states in the beam. 1328 In the character-level arc-standard transition step 0 1 2 3 4 5 6 7 ··· 12 13 ··· step 0 1 2 3 4 5 6 7 ··· 13 14 ··· action SHw (NR) SHc ALc SHc ALc PW SHw (NN) ··· PW ALw ··· (a) action SHc (NR) ALc SHc ALc SHc PW SHw ··· PW ALw ··· stack queue dependencies φ 林 业 ··· φ 林/NR 业 局 ··· φ 林/NR 业/NR 局 副 ··· φ x 业/NR 局 副 ··· A1 = {林 业} 业/NR 局/NR 副 局 ··· A1 S x 局/NR 副 局 ··· A2 = A1 {业 局} 林业局/NR 副 局 ··· A2 林业局/NR 副/NN 局 长 ··· A2 ··· ··· ··· 林业局/NR 副局长/NN 会 上 · · · Ai S x 副局长/NN 会 上 ·"
P14-1146,C10-2005,0,0.00563043,"aches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focu"
P14-1146,P07-1056,0,0.114896,"plement this system because the codes are not publicly available 3 . NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model. The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases. Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007). Results and Analysis. Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets. Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier. The results of bagof-ngram (uni/bi/tri-gram) features are not satisfied because the one-hot word representation cannot capture the latent connections between words. NBSVM and RAE perform comparably and have 3 For 3-class sentiment classification in SemEval 2013, our re-implementation"
P14-1146,C10-2028,0,0.00814251,"t classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter senti"
P14-1146,P11-2008,0,0.0123158,"Missing"
P14-1146,P13-1088,0,0.0113325,"Missing"
P14-1146,P11-1016,1,0.370167,"their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment cl"
P14-1146,P13-2087,0,0.792293,"utoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and"
P14-1146,P11-1015,0,0.646787,"(2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from sc"
P14-1146,D12-1110,0,0.368522,"Missing"
P14-1146,P13-1045,0,0.149339,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,S13-2053,0,0.231338,"on has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases"
P14-1146,S13-2052,0,0.0193943,"to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the w"
P14-1146,pak-paroubek-2010-twitter,0,0.047259,"low traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based me"
P14-1146,W02-1011,0,0.132502,"dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the lea"
P14-1146,D11-1014,0,0.926164,"Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We propose incorporat"
P14-1146,D13-1170,0,0.34155,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,J11-2001,0,0.172252,"ion, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy"
P14-1146,P02-1053,0,0.0615851,"asks. 2 Related Work In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011;"
P14-1146,P12-2018,0,0.321669,"ve/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013. Baseline Methods. We compare our method with the following sentiment classification algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available"
P14-1146,H05-1044,0,0.100893,"he sentiment lexicon, P#Lex PN j=1 β(wi , cij ) i=1 Accuracy = (10) #Lex × N where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, β(wi , cij ) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case. The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon. We set N as 100 in our experiment. Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding. For each lexicon, we remove the words that do not appear in the lookup table of word embedding. We only use unigram embedding in this section because these sentiment lexicons do not contain phrases. The distribution of the lexicons used in this paper is listed in Table 4. Lexicon HL MPQA Joint Positive 1,331 1,932 1,051 Negative 2,647 2,817 2,024 Total 3,978 4,749 3,075 Table 4: Statistics of the sentiment lexicons. Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity. Results. Table 5"
P14-1146,D11-1016,0,0.0249497,"the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors."
P14-1146,D13-1061,0,0.152584,"representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors. It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity. In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis. We encode the sentiment information in1555 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics to the continuous representation of words, so that it is able to separate good and bad to opposite ends"
P14-1146,P10-1040,0,\N,Missing
P14-6008,P14-1021,1,0.829043,"troduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks,"
P14-6008,P07-1106,1,0.91772,"l discriminative learning and beam-search decoding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010),"
P14-6008,P08-1101,1,0.827347,"coding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing ("
P14-6008,D08-1059,1,0.822846,"coding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing ("
P14-6008,W09-3825,1,0.756058,"tion to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentatio"
P14-6008,D10-1082,1,0.847244,"(Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks, the 13 framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on"
P14-6008,P11-1069,1,0.847476,"g, tliu}@ir.hit.edu.cn † Abstract This tutorial discusses a framework for incremental left-to-right structured predication, which makes use of global discriminative learning and beam-search decoding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has a"
P14-6008,J11-1005,1,0.848919,"g, tliu}@ir.hit.edu.cn † Abstract This tutorial discusses a framework for incremental left-to-right structured predication, which makes use of global discriminative learning and beam-search decoding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has a"
P14-6008,P11-2033,1,0.838007,"ange of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), j"
P14-6008,C12-2136,1,0.812736,"ch the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuracies of this framework are also enabled by direct interactions between learning and search (Daum´e III and Marcu, 2005; Huang et al., 2012; Zhang and Nivre, 2012). 2 Tutorial Overview In this tutorial, we make an introduction to the framework, illustrating how it can be applied to a range of NLP problems, giving theoretical discussions and demonstrating a software implementation. We start with a detailed introduction of the framework, describing the averaged perceptron algorithm (Collins, 2002) and its efficient implementation issues (Zhang and Clark, 2007), as well as beam-search and the early-update strategy (Collins and Roark, 2004). We then illustrate how the framework can be applied to NLP tasks, including word segmentation, joint segmentation & P"
P14-6008,P13-1013,1,0.83742,"tional categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks, the 13 framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuraci"
P14-6008,P14-1125,1,0.795356,"ammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks, the 13 framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuracies of this framework"
P14-6008,P13-1043,1,0.847813,"and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and"
P14-6008,E12-1009,0,\N,Missing
P14-6008,W02-1001,0,\N,Missing
P14-6008,P04-1015,0,\N,Missing
P14-6008,P10-1110,0,\N,Missing
P14-6008,P13-1001,0,\N,Missing
P14-6008,I11-1136,0,\N,Missing
P14-6008,N12-1015,0,\N,Missing
P15-1051,C10-3004,1,0.77884,"Missing"
P15-1051,D07-1074,0,0.205382,"Missing"
P15-1051,P13-2006,0,0.0290674,"Missing"
P15-1051,D11-1072,0,0.076523,"Missing"
P15-1051,C14-1087,1,0.243274,"ghdad hasChild CapitalQusay Hussein k2: SaddamkHussein Saddam Hussein hasChild Qusay Hussein k(b)2: Two omitted relevant pieces of background knowledge Figure 1: An example of document enrichment: A source document about a U.S. air strike omitting two important pieces of background knowledge which are acquired by our framework. triples from multiple sources, we also get better coverage. Therefore, one can expect that this representation is helpful for better document enrichment by incorporating both accuracy and coverage. In fact, there is already evidence that this representation is helpful. Zhang et al. (2014) proposed a triple-based document enrichment framework which uses triples of SPO as background knowledge. They first proposed a search engine– based method to evaluate the relatedness between every pair of triples, and then an iterative propagation algorithm was introduced to select the most relevant triples to a given source document (see Section 2), which achieved a good performance. However, to evaluate the semantic relatedness between two triples, Zhang et al. (2014) primarily relied on the text of triples and used search engines, which makes their method difficult to re-implement and in t"
P15-1051,J00-3005,0,0.0121463,"Missing"
P15-1051,nastase-etal-2010-wikinet,0,0.0788196,"Missing"
P15-1051,N10-1072,0,0.0604509,"Missing"
P15-1051,P11-1009,0,0.0688529,"Missing"
P15-1051,P09-1077,0,0.0248825,"Missing"
P15-1098,D10-1115,0,0.00705842,"rder to model user-text consistency, we represent each user as a continuous matrix Uk ∈ RdU ×d , which acts as an operator to modify the semantic meaning of a word. This is on the basis of vector based semantic composition (Mitchell and Lapata, 2010). They regard compositional modifier as a matrix X1 to modify another component x2 , and use matrix-vector multiplication y = X1 × x2 as the composition function. Multiplicative semantic composition is suitable for our need of user modifying word meaning, and it has been successfully utilized to model adjectivenoun composition (Clark et al., 2008; Baroni and Zamparelli, 2010) and adverb-adjective composition (Socher et al., 2012). Similarly, we model product-text consistency by encoding each product as a matrix Pj ∈ RdP ×d , where d is the dimension of word vector, dP is the output length of product-word multiplicative composition. After conducting user-word multiplication and productword multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product enhanced document representation. Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning fr"
P15-1098,I13-1156,0,0.266074,"lassifier and achieve better performance2 . In this paper, we propose a new model dubbed User Product Neural Network (UPNN) to capture user- and product-level information for sentiment classification of documents (e.g. reviews). UPNN takes as input a variable-sized document as well as the user who writes the review and the product which is evaluated. It outputs sentiment polarity label of a document. Users and products are encoded in continuous vector spaces, the representations of which capture important global clues such 2 One can manually design a small number of user and product features (Gao et al., 2013). However, we argue that they are not effective enough to capture sophisticated semantics of users and products. 1014 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1014–1023, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as user preferences and product qualities. These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification. We apply UPNN to three datasets der"
P15-1098,W06-3808,0,0.0209607,"earns tailored representation for each user and product. We evaluate classification accuracy on the extracted OOV test set. Experimental results are given in Figure 5. We can find that these two strategies perform slightly better than UPNN (no UP), but still worse than the full model. 5 5.1 Related Work Sentiment Classification Sentiment classification is a fundamental problem in sentiment analysis, which targets at inferring the sentiment label of a document. Pang and Lee (2002; 2005) cast this problem a classification task, and use machine learning method in a supervised learning framework. Goldberg and Zhu (2006) use unlabelled reviews in a graphbased semi-supervised learning method. Many studies design effective features, such as text topic (Ganu et al., 2009), bag-of-opinion (Qu et al., 2010) and sentiment lexicon features (Kiritchenko et al., 2014). User information is also used for sentiment classification. Gao et al. (2013) design user-specific features to capture user leniency. Li et al. (2014) incorporate textual topic and user-word factors with supervised topic modeling. Tan et al. (2011) and Hu et al. (2013) utilize usertext and user-user relations for Twitter sentiment analysis. Unlike most"
P15-1098,P14-1062,0,0.13981,"rd this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Socher et al., 2013; Kalchbrenner et al., 2014; Le and Mikolov, 2014). Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review “it works great” and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sentiment rating of a review. Product quality also has an impact on review sentiment rating. Reviews towards high-quality products (e.g. Macbook)"
P15-1098,D14-1181,0,0.0387917,"m distribution, regarded as a parameter and jointly trained with other parameters of neural networks. Alternatively, they can be pretrained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b), and applied as initial values of word embedding matrix. We adopt the latter strategy which better exploits the semantic and grammatical associations of words. To model semantic representations of sentences, convolutional neural network (CNN) and recursive neural network (Socher et al., 2013) are two state-of-the-art methods. We use CNN (Kim, 2014; Kalchbrenner et al., 2014) in this work as it does not rely on external parse tree. Specifically, we use multiple convolutional filters with different widths to produce sentence representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for sentiment classification. The convolutional filter with a width of 3 essentially captures the semantics of trigrams in a sentence. Accordingly, multiple convolutional filters with widths of 1, 2 and 3 encode the semantics of unigrams, bigrams and trigrams in a sentence. A"
P15-1098,P13-2087,0,0.0199465,"n that we encode four kinds of consistencies and use neural network approach. User representation is also leveraged for recommendation (Weston et al., 2013), web search (Song et al., 2014) and social media analytics (Perozzi et al., 2014). 5.2 Neural Network for Sentiment Classification Neural networks have achieved promising results for sentiment classification. Existing neural network methods can be divided into two groups: word embedding and semantic composition. For learning word embeddings, (Mikolov et al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent neural network on five NLP t"
P15-1098,D14-1162,0,0.111996,"sentations of user k and product j for capturing user-text and product-text consistencies. ous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is the size of word vocabulary. These word vectors can be randomly initialized from a uniform distribution, regarded as a parameter and jointly trained with other parameters of neural networks. Alternatively, they can be pretrained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b), and applied as initial values of word embedding matrix. We adopt the latter strategy which better exploits the semantic and grammatical associations of words. To model semantic representations of sentences, convolutional neural network (CNN) and recursive neural network (Socher et al., 2013) are two state-of-the-art methods. We use CNN (Kim, 2014; Kalchbrenner et al., 2014) in this work as it does not rely on external parse tree. Specifically, we use multiple convolutional filters with different widths to produce sentence representation. The reason is that they are capab"
P15-1098,C10-1103,0,0.102162,"ity or intensity (e.g. 1-5 or 1-10 stars on review sites) of a document. Dominating studies follow Pang et al. (2002; 2005) and regard this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Socher et al., 2013; Kalchbrenner et al., 2014; Le and Mikolov, 2014). Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review “it works great” and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sen"
P15-1098,D15-1278,0,0.0128866,"t al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent neural network on five NLP tasks including sentiment classification. (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2014) use convolutional neural networks. Le and Mikolov (2014) introduce 1021 Paragraph Vector. Unlike existing neural network approaches that only use the semantics of texts, we take consideration of user and product representations and leverage their connections with text semantics for sentiment classification. This work is an extension of our previous work (Tang et al., 2015), which only takes cons"
P15-1098,D12-1110,0,0.0623837,"a continuous matrix Uk ∈ RdU ×d , which acts as an operator to modify the semantic meaning of a word. This is on the basis of vector based semantic composition (Mitchell and Lapata, 2010). They regard compositional modifier as a matrix X1 to modify another component x2 , and use matrix-vector multiplication y = X1 × x2 as the composition function. Multiplicative semantic composition is suitable for our need of user modifying word meaning, and it has been successfully utilized to model adjectivenoun composition (Clark et al., 2008; Baroni and Zamparelli, 2010) and adverb-adjective composition (Socher et al., 2012). Similarly, we model product-text consistency by encoding each product as a matrix Pj ∈ RdP ×d , where d is the dimension of word vector, dP is the output length of product-word multiplicative composition. After conducting user-word multiplication and productword multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product enhanced document representation. Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning framework (Pang and Lee, 2005). Instead of using handcraf"
P15-1098,P11-1015,0,0.0913889,"Li et al. (2014) in that we encode four kinds of consistencies and use neural network approach. User representation is also leveraged for recommendation (Weston et al., 2013), web search (Song et al., 2014) and social media analytics (Perozzi et al., 2014). 5.2 Neural Network for Sentiment Classification Neural networks have achieved promising results for sentiment classification. Existing neural network methods can be divided into two groups: word embedding and semantic composition. For learning word embeddings, (Mikolov et al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent ne"
P15-1098,P14-5010,0,0.0030416,"atasets used for sentiment classification. The rating scale of IMDB dataset is 1-10. The rating scale of Yelp 2014 and Yelp 2013 datasets is 1-5. |V |is the vocabulary size of words in each dataset. #users is the number of users, #docs/user means the average number of documents per user posts in the corpus. et al., 2014) and Yelp Dataset Challenge4 in 2013 and 2014. Statistical information of the generated datasets are given in Table 1. We split each corpus into training, development and testing sets with a 80/10/10 split, and conduct tokenization and sentence splitting with Stanford CoreNLP (Manning et al., 2014). We use standard accuracy (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000) to measure the overall sentiment classification performance, and use M AE and RM SE to measure the divergences between predicted sentiment ratings (pr) and ground truth ratings (gd). P |gdi − pri | M AE = i (3) N rP 2 i (gdi − pri ) RM SE = (4) N 4.2 Baseline Methods We compare UPNN with the following baseline methods for document-level sentiment classification. (1) Majority is a heuristic baseline method, which assigns the majority sentiment category in training set to each review in the test dataset. (2) In Tr"
P15-1098,P05-1015,0,0.0327322,"rb-adjective composition (Socher et al., 2012). Similarly, we model product-text consistency by encoding each product as a matrix Pj ∈ RdP ×d , where d is the dimension of word vector, dP is the output length of product-word multiplicative composition. After conducting user-word multiplication and productword multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product enhanced document representation. Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning framework (Pang and Lee, 2005). Instead of using handcrafted features, we use continuous representation of documents, users and products as discriminative features. The sentiment classifier is built from documents with gold standard sentiment labels. As is shown in Figure 2, the feature representation for building rating predictor is the concatenation of three parts: continuous user representation uk , continuous product representation pj and continuous document representation vd , where vd encodes user-text consistency, product-text consistency and document level semantic composition. We use sof tmax to build the classifi"
P15-1098,W02-1011,0,0.0331478,"l evidence in turn facilitates embedding learning procedure at document level, yielding better text representations. By combining evidence at user-, product- and documentlevel in a unified neural framework, the proposed model achieves state-of-the-art performances on IMDB and Yelp datasets1 . 1 Introduction Document-level sentiment classification is a fundamental problem in the field of sentiment analysis and opinion mining (Pang and Lee, 2008; Liu, 2012). The task is to infer the sentiment polarity or intensity (e.g. 1-5 or 1-10 stars on review sites) of a document. Dominating studies follow Pang et al. (2002; 2005) and regard this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Soche"
P15-1098,D13-1170,0,0.476576,"(2002; 2005) and regard this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Socher et al., 2013; Kalchbrenner et al., 2014; Le and Mikolov, 2014). Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review “it works great” and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sentiment rating of a review. Product quality also has an impact on review sentiment rating. Reviews towards high-qual"
P15-1098,C14-1018,1,0.783389,"ernational Joint Conference on Natural Language Processing, pages 1014–1023, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as user preferences and product qualities. These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification. We apply UPNN to three datasets derived from IMDB and Yelp Dataset Challenge. We compare to several neural network models including recursive neural networks (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), sentimentspecific word embedding (Tang et al., 2014b), and a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) UPNN outperforms baseline methods for sentiment classification of documents; (2) incorporating representations of users and products significantly improves classification accuracy. The main contributions of this work are as follows: • We present a new neural network method (UPNN) by leveraging users and products for document-level sentiment classification. • We validate the influences of users and products in terms of sentiment and text on massive IMDB and Yelp reviews. • We repor"
P15-1098,P14-1146,1,0.652646,"ernational Joint Conference on Natural Language Processing, pages 1014–1023, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as user preferences and product qualities. These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification. We apply UPNN to three datasets derived from IMDB and Yelp Dataset Challenge. We compare to several neural network models including recursive neural networks (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), sentimentspecific word embedding (Tang et al., 2014b), and a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) UPNN outperforms baseline methods for sentiment classification of documents; (2) incorporating representations of users and products significantly improves classification accuracy. The main contributions of this work are as follows: • We present a new neural network method (UPNN) by leveraging users and products for document-level sentiment classification. • We validate the influences of users and products in terms of sentiment and text on massive IMDB and Yelp reviews. • We repor"
P15-1098,C14-1064,0,0.0191227,"ddings, (Mikolov et al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent neural network on five NLP tasks including sentiment classification. (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2014) use convolutional neural networks. Le and Mikolov (2014) introduce 1021 Paragraph Vector. Unlike existing neural network approaches that only use the semantics of texts, we take consideration of user and product representations and leverage their connections with text semantics for sentiment classification. This work is an extension of our previous work (Tang et al., 2015), whi"
P15-1119,J92-4003,0,0.140167,"n Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: c(wiT ) = arg max k ∑ (i,j)∈AT ∣S ci,j ⋅ 1[c(wjS ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et"
P15-1119,W06-2920,0,0.0171356,"e templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et al. (2013) (McD13), because we’re using Before this dataset was carried out, the CoNLL multilingual dependency treebanks (Buchholz and Marsi, 2006) were often used for evaluation. However, the major problem is that the dependency annotations vary for different languages (e.g. the choice of lexical versus functional head), which makes it impossible to evaluate the LAS. 1239 D ELEX P ROJ P ROJ+Cluster CCA CCA+Cluster Unlabeled Attachment Score (UAS) EN DE ES FR AVG 83.67 57.01 68.05 68.85 64.64 91.96 60.07 71.42 71.36 67.62 92.33 60.35 71.90 72.93 68.39 90.62† 59.42 68.87 69.58 65.96 92.03† 60.66 71.33 70.87 67.62 EN 79.42 90.48 90.91 88.88† 90.49† M C D13 83.33 58.50 68.07 70.14 65.57 78.54 48.11 56.86 58.20 54.39 84.44 90.21 57.30 60.55"
P15-1119,P14-1063,0,0.0232538,"ture templates used in our system are shown in Table 1. Then, feature compositions are performed at the hidden layer via a cube activation function: g(x) = x3 . The cube activation function can be viewed as a special case of low-rank tensor. Formally, g(x) can be expanded as: g(w1 x1 + ... + wm xm + b) = ∑ (wi wj wk )xi xj xk + ∑ b(wi wj )xi xj + ... i,j,k i,j If we treat the bias term as b × x0 where x0 = 1, then the weight corresponding to each feature combination xi xj xk is wi wj wk , which is exactly the same as a rank-1 component tensor in the lowrank form using CP tensor decomposition (Cao and Khudanpur, 2014). Consequently, the cube activation function implicitly derives full feature combinations. An advantage of the cube activation function is that it is flexible for adding extra features to the input. In fact, we can add as many features as possible to the input layer to improve the parsing accuracy. We will show in Section 5.2 that the Brown cluster features can be readily incorporated into our model. Cross-lingual Transfer. The idea of crosslingual transfer using the parser we examined above is straightforward. In contrast to traditional approaches that have to discard rich lexical features (d"
P15-1119,N13-1006,1,0.167482,"(Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao"
P15-1119,D14-1082,0,0.779303,"atures used in traditional dependency parsers, distributed representations map symbolic features into a continuous representation space, that can be shared across languages. Therefore, our model has the ability to utilize both lexical and non-lexical features naturally. Specifically, our framework contains two primary components: • A neural network-based dependency parser. We expect a non-linear model for dependency parsing in our study, because distributed feature representations are shown to be more effective in non-linear architectures than in linear architectures (Wang and Manning, 2013). Chen and Manning (2014) propose a transition-based dependency parser using a neural network architecture, which is simple but works well on several datasets. Briefly, this model simply replaces the predictor in transition-based dependency parser with a well-designed neural network classifier. We will provide explanations for the merits of this model in Section 3, as well as how we adapt it to the cross-lingual task. • Cross-lingual word representation learning. The key to filling the lexical feature gap is to project the representations of these features from different languages into a common vector space, preservin"
P15-1119,P11-1061,0,0.0110737,"ion method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49"
P15-1119,W08-1301,0,0.0204693,"Missing"
P15-1119,de-marneffe-etal-2006-generating,0,0.034227,"Missing"
P15-1119,D12-1001,0,0.0275469,"uding NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-"
P15-1119,P10-4002,0,0.0314634,"r-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units to 400. The dimension of embeddings for different features are shown in Table 2. Dim. Word 50 POS 50 Label 50 Dist. 5 Val. 5 Cluster 8 Table 2: Dimensions of feature embeddings. Adaptive stochastic gradient descent (AdaGrad) (Duchi et al., 2011) is used for optimization. For the CCA approach, we use the implementation of Faruqui and Dyer (2014). The dimensions of the monolin"
P15-1119,E14-1049,0,0.441226,"which is typically smaller than the monolingual datasets. Therefore, in order to improve the robustness of projection, we utilize a morphology-inspired mechanism, to propagate embeddings from in-vocabulary words to out-ofvocabulary (OOV) words. Specifically, for each T , we extract a list of candidate OOV word woov words that is similar to it in terms of edit distance, and then set the averaged vector as the embedding T of woov . Formally, T v(woov ) = Avg (v(w′ )) w′ ∈C T where C = {w∣EditDist(woov , w) ≤ τ } 4.3 ?1 Canonical Correlation Analysis The second approach we consider is similar to Faruqui and Dyer (2014), which use CCA to improve monolingual word embeddings with multilingual correlation. CCA is a way of measur?2 Σ ?1 Ω′ Ω ? ? ? ? ?2 ?2 Ω∗ CCA ?1 Σ∗ ? ? Figure 3: CCA for cross-lingual word representation learning. ing the linear relationship between multidimensional variables. For two multidimensional variables, CCA aims to find two projection matrices to map the original variables to a new basis (lowerdimensional), such that the correlation between the two variables is maximized. Let’s treat CCA as a blackbox here, and see how to apply CCA for inducing bilingual word embeddings. Suppose there"
P15-1119,P09-1042,0,0.0255939,"Missing"
P15-1119,P10-1156,0,0.0165341,"46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, whi"
P15-1119,D14-1012,1,0.642138,"major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Sutskever et al., 2014). 3 Transition actions In this paper, these two terms are used interchangeably. Stack Buffer ROOT has_VBZ good_JJ control_NN ._. nsubj He_PRP Configuration Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in Section 5.2. which typically consists of a stack S, a buffer B, and a partially"
P15-1119,P08-1088,0,0.0084253,"projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units"
P15-1119,P14-1006,0,0.0265905,"Missing"
P15-1119,D11-1110,0,0.0181767,"cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the b"
P15-1119,C10-1063,0,0.0172979,".). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingua"
P15-1119,P12-2010,0,0.021569,"oposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES U"
P15-1119,P12-1073,0,0.017508,"tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison"
P15-1119,P04-1061,0,0.0694958,"ng central problems. The majority of work on dependency parsing has been dedicated to resource-rich languages, such as English and Chinese. For these languages, there exist large-scale ∗ This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features."
P15-1119,C12-1089,0,0.351816,"Missing"
P15-1119,W02-0902,0,0.0119728,"dings for our cross-lingual task. Contrary to the projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network depe"
P15-1119,P13-1105,0,0.0787465,"Missing"
P15-1119,W14-1614,0,0.05557,"-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15 For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod"
P15-1119,P14-1126,0,0.19646,"pplied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15 For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod arcs are mostly leftdirected. ‡ FR LAS 58.78 46.66 58.08"
P15-1119,C14-1175,0,0.133083,"dency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel"
P15-1119,N01-1020,1,0.605674,"ual task. Contrary to the projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the"
P15-1119,D07-1013,0,0.00913989,"d cluster features can be effectively embedded into our model, leading to significant additive improvements. 2 2.1 Background Dependency Parsing Given an input sentence x = w0 w1 ...wn , the goal of dependency parsing is to build a dependency tree (Figure 1), which can be denoted by d = {(h, m, l) ∶ 0 ≤ h ≤ n; 0 &lt; m ≤ n, l ∈ L}. (h, m, l) indicates a directed arc from the head word wh to the modifier wm with a dependency label l, and L is the label set. The mainstream models that have been proposed for dependency parsing can be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy d"
P15-1119,D11-1006,0,0.219143,"uages, such as English and Chinese. For these languages, there exist large-scale ∗ This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features. A delexicalized parser makes sense in that POS tag features are significantly predictive for unlabele"
P15-1119,D10-1120,0,0.021069,"Missing"
P15-1119,P12-1066,0,0.0410584,"Missing"
P15-1119,W04-0308,0,0.0244311,"ROOT has_VBZ good_JJ control_NN ._. nsubj He_PRP Configuration Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in Section 5.2. which typically consists of a stack S, a buffer B, and a partially derived forest, i.e. a set of dependency arcs A. Given an input word sequence x = w1 w2 , ..., wn , the initial configuration can be represented as a tuple: ⟨[w0 ]S , [w1 w2 , ..., wn ]B , ∅⟩, and the terminal configuration is ⟨[w0 ]S , []B , A⟩, where w0 is a pseudo word indicating the root of the whole dependency tree. We consider the arc-standard algorithm (Nivre, 2004) in this paper, which defines three types of transition actions: L EFT-A RC(l), R IGHT-A RC(l), and S HIFT, l is the dependency label. The typical approach for greedy arc-standard parsing is to build a multi-class classifier (e.g., SVM, MaxEnt) of predicting the transition action given a feature vector extracted from a specific configuration. While conventional feature engineering suffers from the problem of sparsity, incompleteness and expensive feature computation (Chen and Manning, 2014), the neural network model provides a potential solution. The architecture of the neural network-based de"
P15-1119,petrov-etal-2012-universal,0,0.0336392,"Missing"
P15-1119,D09-1086,0,0.00730072,". 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized"
P15-1119,W15-1824,0,0.0443701,"be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Kle"
P15-1119,P12-1068,0,0.00760257,"UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser"
P15-1119,P10-1040,0,0.0337388,"in the NLP research community of learning distributed representations for different natural language units, from morphemes, words and phrases, to sentences and documents. Using distributed representations, these symbolic units are embedded into a lowdimensional and continuous space, thus it is often referred to as embeddings.1 In general, there are two major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Su"
P15-1119,P08-1086,0,0.0144334,"). We use the same alignment dictionary as described in Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: c(wiT ) = arg max k ∑ (i,j)∈AT ∣S ci,j ⋅ 1[c(wjS ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower p"
P15-1119,I13-1183,0,0.0132607,"Missing"
P15-1119,Q14-1005,0,0.032741,"om, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cr"
P15-1119,W14-1613,0,0.186749,"2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. T"
P15-1119,H01-1035,1,0.652706,"ks. It is worth noting that we don’t assume/require bilingual parallel data in CCA and P ROJ. What we need in practice is a bilingual lexicon for each paired languages. This is especially important for generalizing our approaches to lower-resource languages, where parallel texts are not available. 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorp"
P15-1119,P11-2033,0,0.022,"an be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy decoding and local training is not as accurate as graph-based parsers or transition-based parsers with beam-search and 1235 global training (Zhang and Clark, 2011). Recently, Chen and Manning (2014) show that greedy transition-based parsers can be greatly improved by using a well-designed neural network architecture. This approach can be considered as a new paradigm of parsing, in that it is based on pure distributed feature representations. In this study, we choose Chen and Manning’s architecture to build our basic depe"
P15-1119,P09-1007,0,0.00764621,"ting approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B"
P15-1119,D10-1030,0,0.0157323,"l., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings"
P15-1119,N12-1052,0,0.445863,"Missing"
P15-1119,J11-1005,0,\N,Missing
P15-1119,P13-2017,0,\N,Missing
P16-2011,P15-2047,1,0.12748,"as alternatives. 2.2 Output 2.4 Training In our model, the loss function is the cross-entropy error of event trigger identification and trigger classification. We initialize all parameters to form a uniform distribution U (−0.01, 0.01). We set the widths of convolutional filters as 2 and 3. The number of feature maps is 300 and the dimension of the PF is 5. Table 1 illustrates the setting parameters used for three languages in our experiments (Zeiler, 2012). Convolution Neural Network 3 As the convolutional neural network (CNN) is good at capturing salient features from a sequence of objects (Liu et al., 2015), we design a CNN to capture some local chunks. This approach has been used for event detection in previous studies (Nguyen and Grishman, 2015; Chen et al., 2015). Specifically, we use multiple convolutional filters with different widths to produce local context representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for event detection. In our work, multiple convolutional filters with widths of 2 and 3 encode the semantics of bigrams and trigrams in a sentence. This local information can also help our mode"
P16-2011,R15-1010,0,0.0363436,"and syntactic features, thus the precision is lower than neural network based methods. (4) RNN and LSTM perform slightly worse than Bi-LSTM. An obvious reason is that RNN and LSTM only consider the preceding sequence information of the trigger, which may miss some important following clues. Considering S1 again, when extracting the trigger “releases”, both models will miss the following sequence “20 million euros to Iraq”. This may seriously hinder the performance of RNN and LSTM for event detection. Table 2: Comparison of different methods on English event detection. (5) Pattern Recognition (Miao and Grishman, 2015), using a pattern expansion technique to extract event triggers. (6) Convolutional Neural Network (Chen et al., 2015), which exploits a dynamic multi-pooling convolutional neural network for event trigger detection. 3.2 Comparison On English Table 2 shows the overall performance of all methods on the ACE2005 English corpus. We can see that our approach significantly outperforms all previous methods. The better performance of HNN can be further explained by the following reasons: (1) Compared with feature based methods, such as MaxEnt, Cross-Event, Cross-Entity, and Joint Model, neural network"
P16-2011,C12-1033,0,0.0193323,"Missing"
P16-2011,P15-1017,0,0.563445,"t trigger or not. Specifically, we first use a Bi-LSTM to encode semantics of each word with its preceding and following information. Then, we add a convolutional neural network to capture structure information from local contexts. 2.1 Bi-LSTM In this section we describe a Bidirectional LSTM model for event detection. Bi-LSTM is a type of bidirectional recurrent neural networks (RNN), which can simultaneously model word representation with its preceding and following information. Word representations can be naturally considered as features to detect triggers and their event types. As show in (Chen et al., 2015), we take all the words of the whole sentence as the input and each token is transformed by looking up word embeddings. Specifically, we use the SkipGram model to pre-train the word embeddings to represent each word (Mikolov et al., 2013; Bahdanau et al., 2014). We present the details of Bi-LSTM for event trigger extraction in Figure 2. We can see that Bi-LSTM is composed of two LSTM neural networks, a forward LSTMF to model the preced67 vector with fixed length. C3 Max-Pooling Feature Map 1 Feature Map 2 Lookup 2.3 Feature Map n ... Convolution ... At the end, we concatenate the bidirectional"
P16-2011,P15-2060,0,0.548592,"d trigger classification. We initialize all parameters to form a uniform distribution U (−0.01, 0.01). We set the widths of convolutional filters as 2 and 3. The number of feature maps is 300 and the dimension of the PF is 5. Table 1 illustrates the setting parameters used for three languages in our experiments (Zeiler, 2012). Convolution Neural Network 3 As the convolutional neural network (CNN) is good at capturing salient features from a sequence of objects (Liu et al., 2015), we design a CNN to capture some local chunks. This approach has been used for event detection in previous studies (Nguyen and Grishman, 2015; Chen et al., 2015). Specifically, we use multiple convolutional filters with different widths to produce local context representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for event detection. In our work, multiple convolutional filters with widths of 2 and 3 encode the semantics of bigrams and trigrams in a sentence. This local information can also help our model fix some errors due to lexical ambiguity. An illustration of CNN with three convolutional filters is given in Figure 3. Let us denote a sent"
P16-2011,P11-1113,0,0.595906,"ection aims to extract event triggers (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”)"
P16-2011,P08-1030,1,0.898397,"Missing"
P16-2011,D15-1167,1,0.218601,"it will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are the output of Bi-LSTM and C2 , C3 are the output of CNN with convolutional filters with widths of 2 and 3. use of word embeddings to induce a more general representation for trigger candidates. Recently, deep learning techniques have been widely used in modeling complex structures and proven effective for many NLP tasks, such as machine translation (Bahdanau et al., 2014), relation extraction (Zeng et al., 2014) and sentiment analysis (Tang et al., 2015a). Bi-directional long short-term memory (Bi-LSTM) model (Schuster et al., 1997) is a two-way recurrent neural network (RNN) (Mikolov et al., 2010) which can capture both the preceding and following context information of each word. Convolutional neural network (CNN) (LeCun et al., 1995) is another effective model for extracting semantic representations and capturing salient features in a flat structure (Liu et al., 2015), such as chunks. In this work, we develop a hybrid neural network incorporating two types of neural networks: Bi-LSTM and CNN, to model both sequence and chunk information f"
P16-2011,P14-1038,1,0.0783646,"e (F). Table 1 shows the detailed description of the data sets used in our experiments. We abbreviate our model as HNN (Hybrid Neural Networks). 3.1 Baseline Methods We compare our approach with the following baseline methods. (1) MaxEnt, a basesline feature-based method, which trains a Maximum Entropy classifier with some lexical and syntactic features (Ji et al., 2008). (2) Cross-Event (Liao et al., 2010), using document-level information to improve the performance of ACE event extraction. (3) Cross-Entity (Hong et al., 2011), extracting events using cross-entity inference. (4) Joint Model (Li and Ji, 2014), a joint structured perception approach, incorporating multilevel linguistic features to extract event triggers and arguments at the same time so that local predictions can be mutually improved. 68 Language English Chinese Spanish Word Embedding corpus dim NYT 300 Gigaword 300 Gigaword 300 Gradient Learning Method method parameters SGD learning rate r = 0.03 Adadelta p = 0.95, δ = 1e−6 Adadelta p = 0.95, δ = 1e−6 Corpus ACE2005 ACE2005 ERE Data Sets Train Dev 529 30 513 60 93 12 Test 40 60 12 Table 1: Hyperparameters and # of documents used in our experiments on three languages. Model MaxEnt"
P16-2011,P13-1008,1,0.945252,"act event triggers (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are t"
P16-2011,D15-1278,0,0.0128362,"Missing"
P16-2011,C14-1220,0,0.0106635,"Missing"
P16-2011,P15-1107,0,0.0217978,"s (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are the output of Bi-L"
P16-2011,P10-1081,0,0.353297,"Missing"
P17-1010,P06-1079,0,0.634499,"he integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferr´andez and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution. Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011). Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution. In sum, most recent researches on ZP resolution are supervised approaches, which means that their performance highly relies on large-scale annotated data. Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs. Therefore, the advantage of our proposed approach is obvious. We are able to generate large-scale pseudo training data fo"
P17-1010,C10-3004,1,0.14054,"oaches that are based on feature engineering, we propose a neural network architecture, which is an attention-based neural network model, for zero pronoun resolution. Also we propose a two-step 102 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 102–111 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1010 we randomly choose an answer word A in the document. Note that, we restrict A to be either a noun or pronoun, where the part-of-speech is identified using LTP Toolkit (Che et al., 2010), as well as the answer word should appear at least twice in the document. Second, after the answer word A is chosen, the sentence that contains A is defined as a query Q, in which the answer word A is replaced by a specific symbol hblanki. In this way, given the query Q and document D, the target of the prediction is to recover the answer A. That is quite similar to the zero pronoun resolution task. Therefore, the automatically generated training samples is called pseudo training data. Figure 1 shows an example of a pseudo training sample. In this way, we can generate tremendous triples of hD"
P17-1010,D13-1135,0,0.670428,"Missing"
P17-1010,P11-1081,0,0.382207,"uns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferr´andez and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution. Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011). Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution. In sum, most recent researches on ZP resolution are supervised approaches, which means that their performance highly relies on large-scale annotated data. Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs. Therefore, the advantage of our proposed approach is obvious. We are able to generate large-scale pseudo training data for ZP resolution, and also we can benefit from the task-spec"
P17-1010,P15-2053,0,0.71283,"o pronoun resolution task. is the vocabulary, r = concat[hdoc att , hquery ] (10) P (A|D, Q) ∝ sof tmax(Wr · r) , s.t. A ∈ V (11) Figure 2 shows the proposed neural network architecture. Note that, for zero pronoun resolution task, antecedents of zero pronouns are always noun phrases (NPs), while our model generates only one word (a noun or a pronoun) as the result. To better adapt our model to zero pronoun resolution task, we further process the output result in the following procedure. First, for a given zero pronoun, we extract a set of NPs as its candidates utilizing the same strategy as (Chen and Ng, 2015). Then, we use our model to generate an answer (one word) for the zero pronoun. After that, we go through all the candidates from the nearest to the far-most. For an NP candidate, if the produced answer is its head word, we then regard this NP as the antecedent of the given zero pronoun. By doing so, for a given zero pronoun, we generate an NP as the prediction of its antecedent. 2.4 • Insert these unknown marks in the vocabulary. These marks may only take up dozens of slots, which is negligible to the size of shortlists (usually 30K ∼ 100K). (a) The weather today is not as pleasant as the wea"
P17-1010,W03-1024,0,0.772026,"ent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferr´andez and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution. Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011). Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution. In sum, most recent researches on ZP resolution are supervised approaches, which means that their performance highly relies on large-scale annotated data. Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs. Therefore, the advantage of our proposed approach is obvious. We are able to generate large-scale pseu"
P17-1010,P16-1074,0,0.450296,"Missing"
P17-1010,D10-1086,0,0.916559,"rate large-scale pseudo training data for pre-training our neural network model (pre-training step)1 . In the adaptation step, we used the official dataset OntoNotes Release 5.02 which is provided by CoNLL-2012 shared task, to carry out our experiments. The CoNLL2012 shared task dataset consists of three parts: a training set, a development set and a test set. The datasets are made up of 6 different domains, namely Broadcast News (BN), Newswires (NW), Broadcast Conversations (BC), Telephone Conversations (TC), Web Blogs (WB), and Magazines (MZ). We closely follow the experimental settings as (Kong and Zhou, 2010; Chen and Ng, 2014, 2015, 2016), where we treat the training set for training and the development set for testing, because only the training and development set are annotated with ZPs. The statistics of training and testing data is shown in Table 1 and 2 respectively. General Train Domain Train Validation Words • Embedding: We use randomly initialized embedding matrix with uniformed distribution in the interval [-0.1,0.1], and set units number as 256. No pre-trained word embeddings are used. Data Query # Sentences Table 2: Statistics of test set (OntoNotes 5.0 development data). Experiments S"
P17-1010,D14-1162,0,0.0998091,"Missing"
P17-1010,P00-1022,0,0.638612,"Missing"
P17-1010,I11-1085,0,0.784897,"ming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferr´andez and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution. Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011). Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution. In sum, most recent researches on ZP resolution are supervised approaches, which means that their performance highly relies on large-scale annotated data. Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs. Therefore, the advantage of our proposed approach is obvious. We are able to generate large-scale pseudo training data for ZP resolution, and also we can be"
P17-1010,D07-1057,0,0.886361,"e words with # are regarded as hunkis in our model. 5 5.1 φ 登上# 太平山# 顶 , 将 香港岛# 和 维多 利亚港# 的 美景 尽收眼底 。 φ Successfully climbed# the peak of [Taiping Mountain]# , to have a panoramic view of the beauty of [Hong Kong Island]# and [Victoria Harbour]# . Related Work Zero pronoun resolution For Chinese zero pronoun (ZP) resolution, early studies employed heuristic rules to Chinese ZP resolution. Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents. Then, supervised approaches to this task have been vastly explored. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs. Kong and Zhou (2010) develop a tree-kernel based approach for Chinese ZP resolution. More recently, unsupervised approaches In this case, the words “登上/climbed” and “太 平山/Taiping Mountain” that appears immediately after the ZP “φ” are all regarded as hunkis in our model. As we model the sequence of words by RNN, the hunkis make the model more difficult to capture the semantic information of the sentence, which in turn influence the overall performance. Especially for the words that are ne"
P17-1010,W12-4501,0,\N,Missing
P17-1010,S10-1001,0,\N,Missing
P17-1011,P16-1068,0,0.0615774,"sk aims to let computers have the ability to appreciate and criticize writing. It would be hugely beneficial for applications like automatic essay scoring (AES) and content recommendation. AES is the task of building a computer-aided scoring system, in order to reduce the involvement of human raters. Traditional approaches are based on supervised learning with designed feature templates (Larkey, 1998; Burstein, 2003; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015; Cummins et al., 2016). Recently, automatic feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016). Writing assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative essays (Persing et al., 2010) and the use of figurative language (Louis and Nenkova, 2013). This paper extends this line of work by taking discourse modes into"
P17-1011,J08-1001,0,0.0111342,"urstein, 2003; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015; Cummins et al., 2016). Recently, automatic feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016). Writing assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative essays (Persing et al., 2010) and the use of figurative language (Louis and Nenkova, 2013). This paper extends this line of work by taking discourse modes into account. • We demonstrate the effectiveness of taking discourse modes into account for automatic essay scoring. A higher ratio of description and emotion expressing can indicate essay quality to a certain extent. Discourse modes can be potentially used as features for other NLP applications. 2 Related Work 2.1 Discourse Analysis Discourse analysis is an important subfield of natural language processing (W"
P17-1011,P00-1037,0,0.141423,"e based on supervised learning with designed feature templates (Larkey, 1998; Burstein, 2003; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015; Cummins et al., 2016). Recently, automatic feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016). Writing assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative essays (Persing et al., 2010) and the use of figurative language (Louis and Nenkova, 2013). This paper extends this line of work by taking discourse modes into account. • We demonstrate the effectiveness of taking discourse modes into account for automatic essay scoring. A higher ratio of description and emotion expressing can indicate essay quality to a certain extent. Discourse modes can be potentially used as features for other NLP applications. 2 Related Work 2.1 Discourse Ana"
P17-1011,D13-1180,0,0.200348,"tification is feasible. 2.2 Automatic Writing Assessment Automatic writing assessment is an important application of natural language processing. The task aims to let computers have the ability to appreciate and criticize writing. It would be hugely beneficial for applications like automatic essay scoring (AES) and content recommendation. AES is the task of building a computer-aided scoring system, in order to reduce the involvement of human raters. Traditional approaches are based on supervised learning with designed feature templates (Larkey, 1998; Burstein, 2003; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015; Cummins et al., 2016). Recently, automatic feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016). Writing assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative e"
P17-1011,Q16-1026,0,0.0231849,"e ability of automatic representation learning. Representing words or relations with continuous vectors (Mikolov et al., 2013; Ji and Eisenstein, 2014) embeds semantics in the same space, which benefits alleviating the data sparseness problem 113 and enables end-to-end and multi-task learning. Recurrent neural networks (RNNs) (Graves, 2012) and the variants like Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent (GRU) (Cho et al., 2014) neural networks show good performance for capturing long distance dependencies on tasks like Named Entity Recognition (NER) (Chiu and Nichols, 2016; Ma and Hovy, 2016), dependency parsing (Dyer et al., 2015) and semantic composition of documents (Tang et al., 2015). This work describes a hierarchical neural architecture with multiple label outputs for modeling the discourse mode sequence of sentences. is added on the basis of four recognized discourse modes and Smith’s report mode is viewed as a subtype of description mode: dialogue description. In summary, we study the following discourse modes: • Narration introduces an event or series of events into the universe of discourse. The events are temporally related according to narrative ti"
P17-1011,P16-1075,0,0.240931,"Missing"
P17-1011,D16-1115,0,0.231833,"ave the ability to appreciate and criticize writing. It would be hugely beneficial for applications like automatic essay scoring (AES) and content recommendation. AES is the task of building a computer-aided scoring system, in order to reduce the involvement of human raters. Traditional approaches are based on supervised learning with designed feature templates (Larkey, 1998; Burstein, 2003; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015; Cummins et al., 2016). Recently, automatic feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016). Writing assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative essays (Persing et al., 2010) and the use of figurative language (Louis and Nenkova, 2013). This paper extends this line of work by taking discourse modes into account. • We demonstr"
P17-1011,P15-1033,0,0.0143212,"ds or relations with continuous vectors (Mikolov et al., 2013; Ji and Eisenstein, 2014) embeds semantics in the same space, which benefits alleviating the data sparseness problem 113 and enables end-to-end and multi-task learning. Recurrent neural networks (RNNs) (Graves, 2012) and the variants like Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent (GRU) (Cho et al., 2014) neural networks show good performance for capturing long distance dependencies on tasks like Named Entity Recognition (NER) (Chiu and Nichols, 2016; Ma and Hovy, 2016), dependency parsing (Dyer et al., 2015) and semantic composition of documents (Tang et al., 2015). This work describes a hierarchical neural architecture with multiple label outputs for modeling the discourse mode sequence of sentences. is added on the basis of four recognized discourse modes and Smith’s report mode is viewed as a subtype of description mode: dialogue description. In summary, we study the following discourse modes: • Narration introduces an event or series of events into the universe of discourse. The events are temporally related according to narrative time. E.g., Last year, we drove to San Francisco along the Sta"
P17-1011,P16-1166,0,0.0253019,"xt segmentation (Hearst, 1997). In some domains, discourse can be segmented according to specific discourse elements (Hutchins, 1977; Teufel and Moens, 2002; Burstein et al., 2003; Clerehan and Buchbinder, 2006; Song et al., 2015). This paper focuses on discourse modes influenced by Smith (2003). From the linguistic view of point, discourse modes are supposed to have different distributions of situation entity types such as event, state and generic (Smith, 2003; Mavridou et al., 2015). Therefore, there is work on automatically labeling clause level situation entity types (Palmer et al., 2007; Friedrich et al., 2016). Actually, situation entity type identification is also a challenging problem. It is even harder for processing Chinese language, 2.3 Neural Sequence Modeling A main challenge of discourse analysis is hard to collect large scale data due to its complexity, which may lead to data sparseness problem. Recently, neural networks become popular for natural language processing (Bengio et al., 2003; Collobert et al., 2011). One of the advantages is the ability of automatic representation learning. Representing words or relations with continuous vectors (Mikolov et al., 2013; Ji and Eisenstein, 2014)"
P17-1011,J95-2003,0,0.775721,"aking discourse modes into account for automatic essay scoring. A higher ratio of description and emotion expressing can indicate essay quality to a certain extent. Discourse modes can be potentially used as features for other NLP applications. 2 Related Work 2.1 Discourse Analysis Discourse analysis is an important subfield of natural language processing (Webber et al., 2011). Discourse is expected to be both cohesive and coherent. Many principles are proposed for discourse analysis, such as coherence relations (Hobbs, 1979; Mann and Thompson, 1988), the centering theory for local coherence (Grosz et al., 1995) and topic-based text segmentation (Hearst, 1997). In some domains, discourse can be segmented according to specific discourse elements (Hutchins, 1977; Teufel and Moens, 2002; Burstein et al., 2003; Clerehan and Buchbinder, 2006; Song et al., 2015). This paper focuses on discourse modes influenced by Smith (2003). From the linguistic view of point, discourse modes are supposed to have different distributions of situation entity types such as event, state and generic (Smith, 2003; Mavridou et al., 2015). Therefore, there is work on automatically labeling clause level situation entity types (Pa"
P17-1011,J97-1003,0,0.657491,"scoring. A higher ratio of description and emotion expressing can indicate essay quality to a certain extent. Discourse modes can be potentially used as features for other NLP applications. 2 Related Work 2.1 Discourse Analysis Discourse analysis is an important subfield of natural language processing (Webber et al., 2011). Discourse is expected to be both cohesive and coherent. Many principles are proposed for discourse analysis, such as coherence relations (Hobbs, 1979; Mann and Thompson, 1988), the centering theory for local coherence (Grosz et al., 1995) and topic-based text segmentation (Hearst, 1997). In some domains, discourse can be segmented according to specific discourse elements (Hutchins, 1977; Teufel and Moens, 2002; Burstein et al., 2003; Clerehan and Buchbinder, 2006; Song et al., 2015). This paper focuses on discourse modes influenced by Smith (2003). From the linguistic view of point, discourse modes are supposed to have different distributions of situation entity types such as event, state and generic (Smith, 2003; Mavridou et al., 2015). Therefore, there is work on automatically labeling clause level situation entity types (Palmer et al., 2007; Friedrich et al., 2016). Actua"
P17-1011,P14-1002,0,0.0235301,"; Friedrich et al., 2016). Actually, situation entity type identification is also a challenging problem. It is even harder for processing Chinese language, 2.3 Neural Sequence Modeling A main challenge of discourse analysis is hard to collect large scale data due to its complexity, which may lead to data sparseness problem. Recently, neural networks become popular for natural language processing (Bengio et al., 2003; Collobert et al., 2011). One of the advantages is the ability of automatic representation learning. Representing words or relations with continuous vectors (Mikolov et al., 2013; Ji and Eisenstein, 2014) embeds semantics in the same space, which benefits alleviating the data sparseness problem 113 and enables end-to-end and multi-task learning. Recurrent neural networks (RNNs) (Graves, 2012) and the variants like Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent (GRU) (Cho et al., 2014) neural networks show good performance for capturing long distance dependencies on tasks like Named Entity Recognition (NER) (Chiu and Nichols, 2016; Ma and Hovy, 2016), dependency parsing (Dyer et al., 2015) and semantic composition of documents (Tang et al., 2015). This work"
P17-1011,D14-1181,0,0.0111504,"Missing"
P17-1011,D15-1270,1,0.913314,"Missing"
P17-1011,D16-1193,0,0.408623,"reciate and criticize writing. It would be hugely beneficial for applications like automatic essay scoring (AES) and content recommendation. AES is the task of building a computer-aided scoring system, in order to reduce the involvement of human raters. Traditional approaches are based on supervised learning with designed feature templates (Larkey, 1998; Burstein, 2003; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015; Cummins et al., 2016). Recently, automatic feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016). Writing assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative essays (Persing et al., 2010) and the use of figurative language (Louis and Nenkova, 2013). This paper extends this line of work by taking discourse modes into account. • We demonstrate the effectiveness of"
P17-1011,D15-1167,1,0.894114,"013; Ji and Eisenstein, 2014) embeds semantics in the same space, which benefits alleviating the data sparseness problem 113 and enables end-to-end and multi-task learning. Recurrent neural networks (RNNs) (Graves, 2012) and the variants like Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent (GRU) (Cho et al., 2014) neural networks show good performance for capturing long distance dependencies on tasks like Named Entity Recognition (NER) (Chiu and Nichols, 2016; Ma and Hovy, 2016), dependency parsing (Dyer et al., 2015) and semantic composition of documents (Tang et al., 2015). This work describes a hierarchical neural architecture with multiple label outputs for modeling the discourse mode sequence of sentences. is added on the basis of four recognized discourse modes and Smith’s report mode is viewed as a subtype of description mode: dialogue description. In summary, we study the following discourse modes: • Narration introduces an event or series of events into the universe of discourse. The events are temporally related according to narrative time. E.g., Last year, we drove to San Francisco along the State Route 1 (SR 1). • Exposition has a function to explain"
P17-1011,Q13-1028,0,0.022051,"feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016). Writing assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative essays (Persing et al., 2010) and the use of figurative language (Louis and Nenkova, 2013). This paper extends this line of work by taking discourse modes into account. • We demonstrate the effectiveness of taking discourse modes into account for automatic essay scoring. A higher ratio of description and emotion expressing can indicate essay quality to a certain extent. Discourse modes can be potentially used as features for other NLP applications. 2 Related Work 2.1 Discourse Analysis Discourse analysis is an important subfield of natural language processing (Webber et al., 2011). Discourse is expected to be both cohesive and coherent. Many principles are proposed for discourse an"
P17-1011,J02-4002,0,0.137917,"se modes can be potentially used as features for other NLP applications. 2 Related Work 2.1 Discourse Analysis Discourse analysis is an important subfield of natural language processing (Webber et al., 2011). Discourse is expected to be both cohesive and coherent. Many principles are proposed for discourse analysis, such as coherence relations (Hobbs, 1979; Mann and Thompson, 1988), the centering theory for local coherence (Grosz et al., 1995) and topic-based text segmentation (Hearst, 1997). In some domains, discourse can be segmented according to specific discourse elements (Hutchins, 1977; Teufel and Moens, 2002; Burstein et al., 2003; Clerehan and Buchbinder, 2006; Song et al., 2015). This paper focuses on discourse modes influenced by Smith (2003). From the linguistic view of point, discourse modes are supposed to have different distributions of situation entity types such as event, state and generic (Smith, 2003; Mavridou et al., 2015). Therefore, there is work on automatically labeling clause level situation entity types (Palmer et al., 2007; Friedrich et al., 2016). Actually, situation entity type identification is also a challenging problem. It is even harder for processing Chinese language, 2."
P17-1011,P16-1101,0,0.0179067,"epresentation learning. Representing words or relations with continuous vectors (Mikolov et al., 2013; Ji and Eisenstein, 2014) embeds semantics in the same space, which benefits alleviating the data sparseness problem 113 and enables end-to-end and multi-task learning. Recurrent neural networks (RNNs) (Graves, 2012) and the variants like Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent (GRU) (Cho et al., 2014) neural networks show good performance for capturing long distance dependencies on tasks like Named Entity Recognition (NER) (Chiu and Nichols, 2016; Ma and Hovy, 2016), dependency parsing (Dyer et al., 2015) and semantic composition of documents (Tang et al., 2015). This work describes a hierarchical neural architecture with multiple label outputs for modeling the discourse mode sequence of sentences. is added on the basis of four recognized discourse modes and Smith’s report mode is viewed as a subtype of description mode: dialogue description. In summary, we study the following discourse modes: • Narration introduces an event or series of events into the universe of discourse. The events are temporally related according to narrative time. E.g., Last year,"
P17-1011,W15-2702,0,0.0264739,"ce relations (Hobbs, 1979; Mann and Thompson, 1988), the centering theory for local coherence (Grosz et al., 1995) and topic-based text segmentation (Hearst, 1997). In some domains, discourse can be segmented according to specific discourse elements (Hutchins, 1977; Teufel and Moens, 2002; Burstein et al., 2003; Clerehan and Buchbinder, 2006; Song et al., 2015). This paper focuses on discourse modes influenced by Smith (2003). From the linguistic view of point, discourse modes are supposed to have different distributions of situation entity types such as event, state and generic (Smith, 2003; Mavridou et al., 2015). Therefore, there is work on automatically labeling clause level situation entity types (Palmer et al., 2007; Friedrich et al., 2016). Actually, situation entity type identification is also a challenging problem. It is even harder for processing Chinese language, 2.3 Neural Sequence Modeling A main challenge of discourse analysis is hard to collect large scale data due to its complexity, which may lead to data sparseness problem. Recently, neural networks become popular for natural language processing (Bengio et al., 2003; Collobert et al., 2011). One of the advantages is the ability of autom"
P17-1011,P07-1113,0,0.0156557,"5) and topic-based text segmentation (Hearst, 1997). In some domains, discourse can be segmented according to specific discourse elements (Hutchins, 1977; Teufel and Moens, 2002; Burstein et al., 2003; Clerehan and Buchbinder, 2006; Song et al., 2015). This paper focuses on discourse modes influenced by Smith (2003). From the linguistic view of point, discourse modes are supposed to have different distributions of situation entity types such as event, state and generic (Smith, 2003; Mavridou et al., 2015). Therefore, there is work on automatically labeling clause level situation entity types (Palmer et al., 2007; Friedrich et al., 2016). Actually, situation entity type identification is also a challenging problem. It is even harder for processing Chinese language, 2.3 Neural Sequence Modeling A main challenge of discourse analysis is hard to collect large scale data due to its complexity, which may lead to data sparseness problem. Recently, neural networks become popular for natural language processing (Bengio et al., 2003; Collobert et al., 2011). One of the advantages is the ability of automatic representation learning. Representing words or relations with continuous vectors (Mikolov et al., 2013;"
P17-1011,D10-1023,0,0.31346,"Missing"
P17-1011,D15-1049,0,0.354957,"ble. 2.2 Automatic Writing Assessment Automatic writing assessment is an important application of natural language processing. The task aims to let computers have the ability to appreciate and criticize writing. It would be hugely beneficial for applications like automatic essay scoring (AES) and content recommendation. AES is the task of building a computer-aided scoring system, in order to reduce the involvement of human raters. Traditional approaches are based on supervised learning with designed feature templates (Larkey, 1998; Burstein, 2003; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015; Cummins et al., 2016). Recently, automatic feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016). Writing assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative essays (Persing et al."
P17-1011,D10-1094,0,0.0264923,"gned feature templates (Larkey, 1998; Burstein, 2003; Attali and Burstein, 2006; Chen and He, 2013; Phandi et al., 2015; Cummins et al., 2016). Recently, automatic feature learning based on neural networks starts to draw attentions (Alikaniotis et al., 2016; Dong and Zhang, 2016; Taghipour and Ng, 2016). Writing assessment involves highly technical aspects of language and discourse. In addition to give a score, it would be better to provide explainable feedbacks to learners at the same time. Some work has studied several aspects such as spelling errors (Brill and Moore, 2000), grammar errors (Rozovskaya and Roth, 2010), coherence (Barzilay and Lapata, 2008), organization of argumentative essays (Persing et al., 2010) and the use of figurative language (Louis and Nenkova, 2013). This paper extends this line of work by taking discourse modes into account. • We demonstrate the effectiveness of taking discourse modes into account for automatic essay scoring. A higher ratio of description and emotion expressing can indicate essay quality to a certain extent. Discourse modes can be potentially used as features for other NLP applications. 2 Related Work 2.1 Discourse Analysis Discourse analysis is an important sub"
P17-1011,xue-zhang-2014-buy,0,0.0190723,"e 55th Annual Meeting of the Association for Computational Linguistics, pages 112–122 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1011 • We build a corpus of narrative essays written by Chinese students in native language. Sentence level discourse modes are annotated with acceptable inter-annotator agreement. Corpus analysis reveals the characteristics of discourse modes in several aspects, including discourse mode distribution, co-occurrence and transition patterns. since Chinese doesn’t have grammatical tense (Xue and Zhang, 2014) and sentence components are often omitted. This increases the difficulties for situation entity type based discourse mode identification. In this paper, we investigate an end-to-end approach to directly model discourse modes without the necessity of identifying situation entity types first. • We describe a multi-label neural sequence labeling approach for discourse mode identification so that the co-occurrence and transition preferences can be captured. Experimental results show that discourse modes can be identified with an average F1-score of 0.7, indicating that automatic discourse mode id"
P17-1055,N12-1047,0,0.0132578,"To mimic the process of double-checking, we propose to use N-best re-ranking strategy after generating answers from our neural networks. The procedure can be illustrated as follows. • Word-class LM: Similar to global LM, the word-class LM is also trained on the document part of training data, but the words are converted to its word class ID. The word class can be obtained by using clustering methods. In this paper, we simply utilized the mkcls tool for generating 1000 word classes (Josef Och, 1999). • Weight Tuning To tune the weights among these features, we adopt the K-best MIRA algorithm (Cherry and Foster, 2012) to automatically optimize the weights on the validation set, which is widely used in statistical machine translation tuning procedure. • N-best Decoding Instead of only picking the candidate that has the highest possibility as answer, we can also extract follow-up candidates in the decoding process, which forms an N-best list. • Re-scoring and Re-ranking • Refill Candidate into Query As a characteristic of the cloze-style problem, each candidate can be refilled into the blank of the query to form a complete sentence. This allows us to check the candidate according to its context. After gettin"
P17-1055,D14-1179,0,0.0177841,"Missing"
P17-1055,C16-1167,1,0.486032,"Missing"
P17-1055,E99-1010,0,0.0455629,"blems in the candidate we choose, we will choose the second possible candidate and do some checking again. To mimic the process of double-checking, we propose to use N-best re-ranking strategy after generating answers from our neural networks. The procedure can be illustrated as follows. • Word-class LM: Similar to global LM, the word-class LM is also trained on the document part of training data, but the words are converted to its word class ID. The word class can be obtained by using clustering methods. In this paper, we simply utilized the mkcls tool for generating 1000 word classes (Josef Och, 1999). • Weight Tuning To tune the weights among these features, we adopt the K-best MIRA algorithm (Cherry and Foster, 2012) to automatically optimize the weights on the validation set, which is widely used in statistical machine translation tuning procedure. • N-best Decoding Instead of only picking the candidate that has the highest possibility as answer, we can also extract follow-up candidates in the decoding process, which forms an N-best list. • Re-scoring and Re-ranking • Refill Candidate into Query As a characteristic of the cloze-style problem, each candidate can be refilled into the blan"
P17-1055,P16-1086,0,0.112414,"ta is necessary for learning relationships between the given document and query. To create large-scale training data for neural networks, Hermann et al. (2015) released the CNN/Daily Mail news dataset, where the document is formed by the news articles and the queries are extracted from the summary of the news. Hill et al. (2015) released the Children’s Book Test dataset afterwards, where the training samples are generated from consecutive 20 sentences from books, and the query is formed by 21st sentence. Following these datasets, a vast variety of neural network approaches have been proposed (Kadlec et al., 2016; Cui et al., 2016; Chen et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Trischler et al., 2016; Seo et al., 2016; Xiong et al., 2016), and most of them stem from the attention-based neural network (Bahdanau et al., 2014), which has become a stereotype in most of the NLP tasks and is well-known by its capability of learning the “importance” distribution over the inputs. In this paper, we present a novel neural network architecture, called attention-over-attention model. As we can understand the meaning literally, our model aims to place another attention mechanism over the existing d"
P17-1055,P16-1223,0,0.292726,"Missing"
P17-1055,D16-1013,0,0.321203,"training data for neural networks, Hermann et al. (2015) released the CNN/Daily Mail news dataset, where the document is formed by the news articles and the queries are extracted from the summary of the news. Hill et al. (2015) released the Children’s Book Test dataset afterwards, where the training samples are generated from consecutive 20 sentences from books, and the query is formed by 21st sentence. Following these datasets, a vast variety of neural network approaches have been proposed (Kadlec et al., 2016; Cui et al., 2016; Chen et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Trischler et al., 2016; Seo et al., 2016; Xiong et al., 2016), and most of them stem from the attention-based neural network (Bahdanau et al., 2014), which has become a stereotype in most of the NLP tasks and is well-known by its capability of learning the “importance” distribution over the inputs. In this paper, we present a novel neural network architecture, called attention-over-attention model. As we can understand the meaning literally, our model aims to place another attention mechanism over the existing document-level attention. Unlike the previous works, that are using heuristic merging functions (Cui et al"
P17-1055,P17-1010,1,\N,Missing
P17-4003,D09-1026,0,0.0138392,"4: The framework of the proposed LTS model for response generation. versation state tracking as inputs and estimates the triggered domain distribution using a convolutional neural network. In Benben, we proposed a topic augmented convolutional neural network to integrate the continuous word representations and the discrete topic information into a unified framework for domain selection. Figure 3 shows the framework of the proposed topic augmented convolutional neural network for domain selection. The word embedding matrix and the topic matrix are obtained using the word2vec6 and Labeled LDA (Ramage et al., 2009), respectively. The two representations of the input conversation utterance are combined in the full connection layer and output the domain triggered distribution. At last, the domains whose triggered probabilities are larger than a threshold are selected to execute the following domain processing step. Note that after the domain selection step, there may be one or more triggered domains. If there is no domain to be triggered, the conversation state is updated and then sent to the response generation module. tracker records the historical content, the current domain and the historically trigge"
P17-4003,C10-3004,1,0.744957,"e seen, the architecture of Benben can be corresponded to the classic architecture of spoken dialogue systems (Young et al., 2013). Concretely, the natural language understanding, dialogue management and natural language generation in spoken dialogue systems are corresponding to the 1), 2) and 3), 4) components of the Benben architecture, respectively. We will next detail each component in the following sections. 2.1 Language Understanding The user input can be either text or speech. Therefore, the first step is to understand both the speech transcription and text. In Benben, the LTP toolkit (Che et al., 2010) is utilized to the basic language processing, including Chinese word segmentation, part-of-speech tagging, word sense disambiguation, named entity recognition, dependency parsing, semantic role labelling and semantic 5 2.2 Conversation State Tracking After the language understanding step, an input sentence is transferred to several feature representations. These feature representations are then taken as the inputs of the conversation state tracking and domain selection. The conversation state http://www.ltp-cloud.com 14 Word Embedding Matrix yt yt-1 y2 y1 st st-1 s2 s1 Word2Vec y0 =g(c,E) Ini"
P17-4003,P17-1010,1,\N,Missing
P18-1034,Q13-1005,0,0.0484716,"udies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic parsing over tables, which is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model paramet"
P18-1034,D13-1160,0,0.522838,"Missing"
P18-1034,D14-1179,0,0.0149909,"Missing"
P18-1034,H94-1010,0,0.425333,"he input, and outputs a SQL query y. We do not consider the join operation over multiple relational tables, which we leave in the future work. We use WikiSQL (Zhong et al., 2017), the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of S"
P18-1034,P17-1174,1,0.889119,"Missing"
P18-1034,N16-1024,0,0.0300555,"imum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect t"
P18-1034,P17-1089,0,0.106833,"mn names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti"
P18-1034,C12-2040,0,0.0387214,", the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL que"
P18-1034,P17-1167,0,0.0492084,"Missing"
P18-1034,P18-1168,0,0.060629,"Missing"
P18-1034,P16-1154,0,0.051477,"etworks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One advantage of this architecture is that it"
P18-1034,P16-1002,0,0.0341733,"al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. questi"
P18-1034,P16-1014,0,0.0177017,"y We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One"
P18-1034,P16-1086,0,0.028017,"s a probability distribution over the tokens from one of the three channels. X p(yt |y&lt;t , x) = pw (yt |zt , y&lt;t , x)pz (zt |y&lt;t , x) Methodology We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and ge"
P18-1034,P17-1097,0,0.143076,"is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu"
P18-1034,D16-1116,0,0.0360868,"Missing"
P18-1034,D17-1160,0,0.26447,"sign of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4.2 STAMP: Syntax- and Table- Aware seMantic Parser Figure 2 illustrates an overview of the prop"
P18-1034,Q13-1016,0,0.0505279,"tional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) th"
P18-1034,D11-1140,0,0.0801846,"Missing"
P18-1034,P15-1142,0,0.163748,"et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Z"
P18-1034,P16-1003,0,0.0875917,"s in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for le"
P18-1034,P17-1003,0,0.0189993,"tional databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus base"
P18-1034,D14-1162,0,0.0809332,"ells, and weighted average two cell distributions, which is calculated as follows. cell pcell pcell w (j) = λˆ w (j) + (1 − λ)αj 4.5 As the WikiSQL data contains rich supervision of question-SQL pairs, we use them to train model parameters. The model has two cross-entropy loss functions, as given below. One is for the switching gate classifier (pz ) and another is for the attentional probability distribution of a channel (pw ). l=− X X logpw (yt |zt , y&lt;t , x) logpz (zt |y&lt;t , x)− t t (6) Our parameter setting strictly follows Zhong et al. (2017). We represent each word using word embedding2 (Pennington et al., 2014) and the mean of the sub-word embeddings of all the n-grams in the word (Hashimoto et al., 2016)3 . The dimension of the concatenated word embedding is 400. We clamp the embedding values to avoid over-fitting. We set the dimension of encoder and decoder hidden state as 200. During training, we randomize model parameters from a uniform distribution with fan-in and fan-out, set batch size as 64, set the learning rate of SGD as 0.5, and update the model with stochastic gradient descent. Greedy search is used in the inference process. We use the model trained from question-SQL pairs as initializat"
P18-1034,P13-1092,0,0.0220016,"ses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic exec"
P18-1034,P11-1060,0,0.149695,"set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coup"
P18-1034,P17-1105,0,0.048093,"ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has n"
P18-1034,P17-2034,0,0.0222559,"to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic p"
P18-1034,D07-1071,0,0.600848,"Missing"
P18-1034,P07-1121,0,0.306045,"Missing"
P18-1034,D17-1125,0,0.635741,"Missing"
P18-1034,P17-1065,1,0.837395,"17; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4"
P18-1034,P16-1127,0,0.111172,"Missing"
P18-1034,P17-1041,0,0.227409,"the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input se"
P18-1034,N18-2093,0,0.393921,"Missing"
P18-1034,P16-1004,0,\N,Missing
P18-1034,P16-1138,0,\N,Missing
P18-1053,D13-1135,0,0.468683,"e important information for interpreting the zero pronoun are called the antecedents. In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated (Chen and Ng, 2016; Yin et al., 2017a,b). These solutions concentrate on anaphoric zero pronoun resolution, applying numerous neural network models to zero pronouncandidate antecedent prediction. Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents (Yin et al., 2017a,b), and substantially surpass classic models (Zhao and Ng, 2007; Chen and Ng, 2013, 2015), obtaining stateof-the-art results on the benchmark dataset. However, these models are heavily making local coreference decisions. They simply consider the coreference chain between the zero pronoun and one single candidate antecedent one link at a time while overlooking their impacts on future decisions. Intuitively, antecedents provide key linguistic cues for explaining the zero pronoun, it is therefore reasonable to leverage useful information provided by previously predicted antecedents as cues for predicting the later zero pronoun-candidate antecedent pairs. For instance, given a"
P18-1053,P15-2053,0,0.0423527,"atures. It is the first time that machine learning techniques are applied for this task. To better explore syntactics, Kong and Zhou (2010) employed the tree kernel technique in their model. Chen and Ng (2013) extended Zhao and Ng (2007)’s model further by integrating innovative features and coreference chains between zero pronoun as bridges to find antecedents. In contrast, unsupervised techniques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement lea"
P18-1053,P16-1074,0,0.516974,"our technique surpasses the state-of-the-art models. 1 A zero pronoun can be an anaphoric zero pronoun if it coreferes to one or more mentions in the associated text, or unanaphoric, if there are no such mentions. In this example, the second zero pronoun “ 2 ” is anaphoric and corefers to the mention “S ã ∫ N ö /Litigant Li Yading” while the zero pronoun “ 1 ” is unanaphoric. These mentions that contain the important information for interpreting the zero pronoun are called the antecedents. In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated (Chen and Ng, 2016; Yin et al., 2017a,b). These solutions concentrate on anaphoric zero pronoun resolution, applying numerous neural network models to zero pronouncandidate antecedent prediction. Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents (Yin et al., 2017a,b), and substantially surpass classic models (Zhao and Ng, 2007; Chen and Ng, 2013, 2015), obtaining stateof-the-art results on the benchmark dataset. However, these models are heavily making local coreference decisions. They simply consider the coreference chain between t"
P18-1053,D16-1245,0,0.0652603,"llconnected hidden layers and one sof tmax layer, the agent maps the state vector into the probability distribution over actions that indicates the coreference likelihood of the input zero pronouncandidate antecedent pair. 1 (st ) + bi ) where p(a|zp, npt ; ✓) indicates the probability of selecting action a. Intuitively, the estimation of the gradient might have very high variance. One commonly used remedy to reduce the variance is to subtract a baseline value b from the reward. Hence, we utilize the gradient estimate as follows: r✓ J(✓) = r✓ X log p(a|zp, npt ; ✓)(R(at ) bt ) t (6) Following Clark and Manning (2016), we intorduce the baseline b and get the value of bt at time t by Eat0 ⇠p R(a1 , ..., at0 , ..., aT ). 2.3 Pretraining Pretraining is crucial in reinforcement learning techniques (Clark and Manning, 2016; Xiong et al., 2017). In this work, we pretrain the model by using the loss function from Yin et al. (2017a): (2) where Wi and bi are the parameters of the ith hidden layer; si represents the state vector. After going through all the layers, we can get the representative vector for the zero pronoun-candidate antecedent pair (zp, npt ). We then feed it into a scoring-layer to get their corefer"
P18-1053,P06-1079,0,0.101425,"iques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to trai"
P18-1053,P11-1081,0,0.0509601,"ind antecedents. In contrast, unsupervised techniques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They ut"
P18-1053,I11-1085,0,0.0228345,"and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to train the model and achieves better re"
P18-1053,D15-1260,0,0.0238565,"Missing"
P18-1053,D17-1060,1,0.907609,"ormation of current candidate antecedent and 3) antecedent information generated by antecedents predicted in previous states. In particular, our reinforcement learning agent is designed as a policy network ⇡✓ (s, a) = p(a|s; ✓), where s represents the state; a indicates the action and ✓ represents the parameters of the model. The parameters ✓ are trained using stochastic gradient descent. Compared with Deep Q-Network (Mnih et al., 2013) that commonly learns a greedy policy, policy network is able to learn a stochastic policy that prevents the agent from getting stuck at an intermediate state (Xiong et al., 2017). Additionally, the learned policy is more explainable, comparing to learned value functions in Deep Q-Network. We here introduce the definitions of components of our reinforcement learning model, namely, state, action 571 and reward. 2.1.1 any function. To encourage the agent to find accurate antecedents, we regard the F-score for the selected antecedents as the reward for each action in a path. State Given a zero pronoun zp with its representation vzp and all of its candidate antecedents representations {vnp1 , vnp2 , ..., vnpn }, our model generate coreference decisions for zero pronoun-can"
P18-1053,D16-1132,0,0.0721111,"Missing"
P18-1053,D17-1135,1,0.393412,"sses the state-of-the-art models. 1 A zero pronoun can be an anaphoric zero pronoun if it coreferes to one or more mentions in the associated text, or unanaphoric, if there are no such mentions. In this example, the second zero pronoun “ 2 ” is anaphoric and corefers to the mention “S ã ∫ N ö /Litigant Li Yading” while the zero pronoun “ 1 ” is unanaphoric. These mentions that contain the important information for interpreting the zero pronoun are called the antecedents. In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated (Chen and Ng, 2016; Yin et al., 2017a,b). These solutions concentrate on anaphoric zero pronoun resolution, applying numerous neural network models to zero pronouncandidate antecedent prediction. Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents (Yin et al., 2017a,b), and substantially surpass classic models (Zhao and Ng, 2007; Chen and Ng, 2013, 2015), obtaining stateof-the-art results on the benchmark dataset. However, these models are heavily making local coreference decisions. They simply consider the coreference chain between the zero pronoun an"
P18-1053,W03-1024,0,0.0976102,"trast, unsupervised techniques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradien"
P18-1053,D10-1086,0,0.401465,"s, our model learns to predict potential antecedents incrementally, selecting global-optimal antecedents in a sequential manner. In the end, our model successfully predicts “y/She” as the result. 4 4.1 Related Work Zero Pronoun Resolution A wide variety of techniques for machine learning models for Chinese zero pronoun resolution have been proposed. Zhao and Ng (2007) utilized the decision tree to learn the anaphoric zero pronoun resolver by using syntactical and positional features. It is the first time that machine learning techniques are applied for this task. To better explore syntactics, Kong and Zhou (2010) employed the tree kernel technique in their model. Chen and Ng (2013) extended Zhao and Ng (2007)’s model further by integrating innovative features and coreference chains between zero pronoun as bridges to find antecedents. In contrast, unsupervised techniques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, ther"
P18-1053,D07-1057,0,0.897073,"ons that contain the important information for interpreting the zero pronoun are called the antecedents. In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated (Chen and Ng, 2016; Yin et al., 2017a,b). These solutions concentrate on anaphoric zero pronoun resolution, applying numerous neural network models to zero pronouncandidate antecedent prediction. Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents (Yin et al., 2017a,b), and substantially surpass classic models (Zhao and Ng, 2007; Chen and Ng, 2013, 2015), obtaining stateof-the-art results on the benchmark dataset. However, these models are heavily making local coreference decisions. They simply consider the coreference chain between the zero pronoun and one single candidate antecedent one link at a time while overlooking their impacts on future decisions. Intuitively, antecedents provide key linguistic cues for explaining the zero pronoun, it is therefore reasonable to leverage useful information provided by previously predicted antecedents as cues for predicting the later zero pronoun-candidate antecedent pairs. For"
P18-1053,D16-1127,0,0.0241677,"extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to train the model and achieves better results compared with the counterpart neural network model. Narasimhan et al. (2016) introduced a deep Q-learning based slot-filling technique, where the agent’s action is to retrieve or reconcile content from a new document. Xiong et al. (2017) proposed an innovative reinforcement learning framework for learning multi-"
P18-1053,P17-1010,1,0.821947,"hm and “Pre” presents the model without reinforcement learning. “dev” shows the performance of our reinforcement learning model on the development dataset. 3.3 Case Study Lastly, we show a case to illustrate the effectiveness of our proposed model, as is shown in Figure 4. In this case, we can see that our model correctly predict mentions “£✏W/The Xiaohui” 575 那 小穗 她 本来 就是 好 , , been extensively studied for zero pronoun resolution. Chen and Ng (2016) introduced a deep neural network resolver for this task. In their work, zero pronoun and candidates are encoded by a feedforward neural network. Liu et al. (2017) explored to produce pseudo dataset for anaphoric zero pronoun resolution. They trained their deep learning model by adopting a two-step learning method that overcomes the discrepancy between the generated pseudo dataset and the real one. To better utilize vector-space semantics, Yin et al. (2017b) employed recurrent neural network to encode zero pronoun and antecedents. In particular, a twolayer antecedent encoder was employed to generate the hierarchical representation of antecedents. Yin et al. (2017a) developed an innovative deep memory network resolver, where zero pronouns are encoded by"
P18-1053,D15-1001,0,0.0198445,"Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to train the model and achieves better results compared with the counterpart neural network model. Narasimhan et al. (2016) introduced a deep Q-learning based slot-filling technique, where the agent’s action is to retrieve or reconcile content from a new document. Xiong et al. (2017) proposed an innovative reinforcement learning framework f"
P18-1053,D16-1261,0,0.0376312,"nt years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to train the model and achieves better results compared with the counterpart neural network model. Narasimhan et al. (2016) introduced a deep Q-learning based slot-filling technique, where the agent’s action is to retrieve or reconcile content from a new document. Xiong et al. (2017) proposed an innovative reinforcement learning framework for learning multi-hop relational paths. Deep reinforcement learning is a natural choice for tasks that require making incremental decisions. By combin576 ing non-linear function approximations with reinforcement learning, the deep reinforcement learning paradigm can integrate vector-space semantic into a robust joint learning and reasoning process. Moreover, by optimizing the po"
P18-1053,D17-1035,0,0.0195247,"nforcement learning. We can see from the table that our model with reinforcement learning achieves better performance than the model without this all across the board. With the help of reinforcement learning, our model learns to choose effective actions in sequential decisions. It empowers the model to directly optimize the overall evaluation metrics, which brings a more effective and natural way of dealing with the task. Moreover, by seeing that the performance on development dataset stops increasing with iterations bigger than 70, we therefore set the pretraining iterations to 70. Following Reimers and Gurevych (2017), to illustrate the impact of randomness in our reinforcement learning model, we run our model with different random seed values. Table 4 shows the performance of our model with different random seeds on the test dataset. We report the minimum, the maximum, the median F-scores results and the standard deviation of F-scores. We run Moreover, on purpose of better illustrating the effectiveness of the proposed reinforcement learning model, we run a set of experiments with different settings. In particular, we compare the model with and without the proposed reinforcement learning process using dif"
P18-1129,D16-1211,0,0.0161719,"ation and the results are shown in Figure 3. Sharpen the distribution during the sampling process generally performs better on development set. Our distillation from exploration model gets almost the same performance as that from reference, but simply combing these two sets of data outperform both models by achieving an LAS of 92.14. We also compare our parser with the other parsers in Table 2. The second group shows the greedy transition-based parsers in previous literatures. Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding. (Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle. Our distillation parser outperforms all these greedy counterparts. The third group shows 92.14 92 89 BLEU on dev. set 4.2 BLEU 22.79 26.26 24.76 24.64 25.44 20.73 22.53 23.83 Baseline Ensemble (10) Distill (reference, α=0.8) Distill (exploration, T =0.1) Distill (both) MIXER BSO (local, B=1) BSO (global, B=1) LAS on dev. set Baseline Ensemble (20) Distill (reference, α=1.0) Distill (exploration, T =1.0) Distill (both) Ballesteros et al. (2016) (dyn. oracle) Andor et al. (2016) (local, B=1) Buckman et al. (2016) (local, B=8) Andor et al."
P18-1129,D16-1254,0,0.0321506,"Missing"
P18-1129,P04-1015,0,0.387277,"ch-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our"
P18-1129,P81-1022,0,0.63388,"Missing"
P18-1129,P15-1033,0,0.025449,"and exploration with the following manner: we use πR and πE to generate a set of training states. Then, we learn p(a |s) on the generated states. If one state was generated by the reference policy, we minimize the interpretation of distillation and NLL loss. Otherwise, we minimize the distillation loss only. 4 Experiments We perform experiments on two tasks: transitionbased dependency parsing and neural machine translation. Both these two tasks are converted to search-based structured prediction as Section 2.1. For the transition-based parsing, we use the stack-lstm parsing model proposed by Dyer et al. (2015) to parameterize the classifier.1 For the neural machine translation, we parameterize the classifier as an LSTM encoder-decoder model by following Luong et al. (2015).2 We encourage the reader of this paper to refer corresponding papers for more details. 4.1 Settings 4.1.1 Transition-based Dependency Parsing We perform experiments on Penn Treebank (PTB) dataset with standard data split (Section 2-21 for training, Section 22 for development, and Section 23 for testing). Stanford dependencies are converted from the original constituent trees using Stanford CoreNLP 3.3.03 by following Dyer et al."
P18-1129,P16-1231,0,0.0231506,".0, best performance on development set is achieved and the test LAS is 91.99. We tune the temperature T during exploration and the results are shown in Figure 3. Sharpen the distribution during the sampling process generally performs better on development set. Our distillation from exploration model gets almost the same performance as that from reference, but simply combing these two sets of data outperform both models by achieving an LAS of 92.14. We also compare our parser with the other parsers in Table 2. The second group shows the greedy transition-based parsers in previous literatures. Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding. (Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle. Our distillation parser outperforms all these greedy counterparts. The third group shows 92.14 92 89 BLEU on dev. set 4.2 BLEU 22.79 26.26 24.76 24.64 25.44 20.73 22.53 23.83 Baseline Ensemble (10) Distill (reference, α=0.8) Distill (exploration, T =0.1) Distill (both) MIXER BSO (local, B=1) BSO (global, B=1) LAS on dev. set Baseline Ensemble (20) Distill (reference, α=1.0) Distill (exploration, T =1.0) Distill (bo"
P18-1129,C12-1059,0,0.258443,"ructured prediction. The yellow bracket represents the ensemble of multiple models trained with different initialization. The dashed red line shows our distillation from reference (§3.2). The solid blue line shows our distillation from exploration (§3.3). Introduction ∗ Distilled model erence policy’s search action on the encountered states when performing the reference policy. Such imitation process can sometimes be problematic. One problem is the ambiguities of the reference policy, in which multiple actions lead to the optimal structure but usually, only one is chosen as training instance (Goldberg and Nivre, 2012). Another problem is the discrepancy between training and testing, in which during the test phase, the learned policy enters non-optimal states whose search action is never learned (Ross and Bagnell, 2010; Ross et al., 2011). All these problems harm the generalization ability of search-based structured prediction and lead to poor performance. Previous works tackle these problems from two directions. To overcome the ambiguities in data, techniques like ensemble are often adopted (Di1393 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages"
P18-1129,Q14-1010,0,0.0138696,"imal 89.59 90.90 91.38 Table 4: The ranking performance of parsers’ output distributions evaluated in MAP on “problematic” states. 4.3.1 Ensemble on “Problematic” States As mentioned in previous sections, “problematic” states which is either ambiguous or non-optimal harm structured prediciton’s performance. Ensemble shows to improve the performance in Section 4.2, which indicates it does better on these states. To empirically testify this, we use dependency parsing as a testbed and study the ensemble’s output distribution using the dynamic oracle. The dynamic oracle (Goldberg and Nivre, 2012; Goldberg et al., 2014) can be used to efficiently determine, given any state s, which transition action leads to the best achievable parse from s; if some errors may have already made, what is the best the parser can do, going forward? This allows us to analyze the accuracy of each parser’s individual decisions, in the “problematic” states. In this paper, we evaluate the output distributions of the baseline and ensemble parser against the reference actions suggested by the dynamic oracle. Since dynamic oracle yields more than one reference actions due to ambiguities and previous mistakes and the output distribution"
P18-1129,P16-1001,0,0.145512,"hat distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our knowledge distillation for search-based structured prediction. The yellow bracket represents the ensemble of m"
P18-1129,nilsson-nivre-2008-malteval,0,0.0401907,"Missing"
P18-1129,J08-4003,0,0.595658,"parsing (σ, β, A), where σ is a stack, β is a buffer, and A is the partially generated tree {S HIFT, L EFT, R IGHT} {([ ], [1, .., n], ∅)} {([ROOT], [ ], A)} • S HIFT: (σ, j|β) → (σ|j, β) • L EFT: (σ|i j, β) → (σ|j, β) A ← A ∪ {i ← j} • R IGHT: (σ|i j, β) → (σ|i, β) A ← A ∪ {i → j} Neural machine translation ($, y1 , y2 , ..., yt ), where $ is the start symbol. pick one word w from the target side vocabulary W. {($)} {($, y1 , y2 , ..., ym )} ($, y1 , y2 , ..., yt ) → ($, y1 , y2 , ..., yt , yt+1 = w) Table 1: The search-based structured prediction view of transition-based dependency parsing (Nivre, 2008) and neural machine translation (Sutskever et al., 2014). etterich, 2000). To mitigate the discrepancy, exploration is encouraged during the training process (Ross and Bagnell, 2010; Ross et al., 2011; Goldberg and Nivre, 2012; Bengio et al., 2015; Goodman et al., 2016). In this paper, we propose to consider these two problems in an integrated knowledge distillation manner (Hinton et al., 2015). We distill a single model from the ensemble of several baselines trained with different initialization by matching the ensemble’s output distribution on the reference states. We also let the ensemble r"
P18-1129,P02-1040,0,0.10334,"t n = 20. 4.1.2 Neural Machine Translation We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign. The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs. We use the same preprocessing as Ranzato et al. (2015), which leads to a German vocabulary of about 30K entries and an English vocabulary of 25K entries. One-layer LSTM for both encoder and decoder with 256 hidden units are used by following Wiseman and Rush (2016). BLEU (Papineni et al., 2002) was used to evaluate the translator’s performance.4 Like in the dependency parsing experiments, we run M = 10 differentlyseeded runs and report the averaged score. Optimizing the distillation loss in Equation 1 requires enumerating over the action space. It is expensive for machine translation since the size of the action space (vocabulary) is considerably large (25K in our experiments). In this paper, we use the K-most probable actions (translations on target side) on one state to approximate the P whole probability distribution of q(a |s) as a q(a | P s) · log p(a |s) ≈ K ak |s) · log p(ˆ a"
P18-1129,N12-1015,0,0.0257324,"arsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our knowledge distillation for search-based structured prediction."
P18-1129,D17-1035,0,0.0240206,"tion and α is determined on the development set. 1 The code for parsing experiments is available at: https://github.com/Oneplus/twpipe. 2 We based our NMT experiments on OpenNMT (Klein et al., 2017). The code for NMT experiments is available at: https://github.com/Oneplus/OpenNMT-py. 3 stanfordnlp.github.io/CoreNLP/ history.html BLEU score on dev. set 3.4 27.25 27.13 27.06 27.00 26.934 26.926 10 20 26.86 26.99 26.99 50 100 26.75 26.50 26.25 1 2 5 Figure 2: The effect of using different Ks when approximating distillation loss with K-most probable actions in the machine translation experiments. Reimers and Gurevych (2017) and others have pointed out that neural network training is nondeterministic and depends on the seed for the random number generator. To control for this effect, they suggest to report the average of M differentlyseeded runs. In all our dependency parsing, we set n = 20. 4.1.2 Neural Machine Translation We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign. The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs. We use the"
P18-1129,D16-1139,0,0.0931068,"ons are achieved by our distillation methods. As Keskar et al. (2016) pointed out that the generalmin max σ 90.45 92.00 91.14 92.37 0.17 0.09 21.63 24.22 23.67 25.65 0.55 0.12 Table 5: The minimal, maximum, and standard derivation values on differently-seeded runs. ization gap is not due to overfit, but due to the network converge to sharp minimizer which generalizes worse, we attribute the more stable training from our distillation model as the distillation loss presents less sharp minimizers. 5 Related Work Several works have been proposed to applying knowledge distillation to NLP problems. Kim and Rush (2016) presented a distillation model which focus on distilling the structured loss from a large model into a small one which works on sequencelevel. In contrast to their work, we pay more attention to action-level distillation and propose to do better action-level distillation by both from reference and exploration. Freitag et al. (2017) used an ensemble of 6translators to generate training reference. Exploration was tried in their work with beam-search. We differ their work by training the single model 1400 to match the distribution of the ensemble. Using ensemble in exploration was also studied i"
P18-1129,P17-4012,0,0.0724743,"Missing"
P18-1129,W04-3250,0,0.0171869,"a q(a | P s) · log p(a |s) ≈ K ak |s) · log p(ˆ ak |s), k q(ˆ where a ˆk is the k-th probable action. We fix α to 4 We use multi-bleu.perl to evaluate our model’s performance 1397 LAS 90.83 92.73 91.99 92.00 92.14 91.42 91.02 91.19 91.70 92.79 94.08 92.06 94.60 Table 2: The dependency parsing results. Significance test (Nilsson and Nivre, 2008) shows the improvement of our Distill (both) over Baseline is statistically significant with p < 0.01. Table 3: The machine translation results. MIXER denotes that of Ranzato et al. (2015), BSO denotes that of Wiseman and Rush (2016). Significance test (Koehn, 2004) shows the improvement of our Distill (both) over Baseline is statistically significant with p < 0.01. 92.11 1 and vary K and evaluate the distillation model’s performance. These results are shown in Figure 2 where there is no significant difference between different Ks and in speed consideration, we set K to 1 in the following experiments. 4.2.1 92.05 92.09 92.11 0.5 0.67 1 91.98 91.85 91 90 89.04 0.1 Results Transition-based Dependency Parsing Table 2 shows our PTB experimental results. From this result, we can see that the ensemble model outperforms the baseline model by 1.90 in LAS. For ou"
P18-1129,D17-1208,0,0.0231132,"transfer knowledge of cumbersome model into a simple one on the unlabeled data (Liang et al., 2008; Li et al., 2014). Their extensions to knowledge distillation call for further study. Kuncoro et al. (2016) proposed to compile the knowledge from an ensemble of 20 transitionbased parsers into a voting and distill the knowledge by introducing the voting results as a regularizer in learning a graph-based parser. Different from their work, we directly do the distillation on the classifier of the transition-based parser. Besides the attempts for directly using the knowledge distillation technique, Stahlberg and Byrne (2017) propose to first build the ensemble of several machine translators into one network by unfolding and then use SVD to shrink its parameters, which can be treated as another kind of knowledge distillation. 6 Conclusion In this paper, we study knowledge distillation for search-based structured prediction and propose to distill an ensemble into a single model both from reference and exploration states. Experiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single model’s performance. Comparison analysis gives empiric"
P18-1129,E17-1117,0,0.0735766,"Missing"
P18-1129,D16-1180,0,0.140531,"outperforms all these greedy counterparts. The third group shows 92.14 92 89 BLEU on dev. set 4.2 BLEU 22.79 26.26 24.76 24.64 25.44 20.73 22.53 23.83 Baseline Ensemble (10) Distill (reference, α=0.8) Distill (exploration, T =0.1) Distill (both) MIXER BSO (local, B=1) BSO (global, B=1) LAS on dev. set Baseline Ensemble (20) Distill (reference, α=1.0) Distill (exploration, T =1.0) Distill (both) Ballesteros et al. (2016) (dyn. oracle) Andor et al. (2016) (local, B=1) Buckman et al. (2016) (local, B=8) Andor et al. (2016) (local, B=32) Andor et al. (2016) (global, B=32) Dozat and Manning (2016) Kuncoro et al. (2016) Kuncoro et al. (2017) 27.00 26.99 0.2 26.97 1.5 2 5 26.93 26.7 26.75 26.50 26.24 26.25 0.1 0.2 0.5 0.67 1 Figure 3: The effect of T on PTB (above) and IWSLT 2014 (below) development set. parsers trained on different techniques including decoding with beam search (Buckman et al., 2016; Andor et al., 2016), training transitionbased parser with beam search (Andor et al., 2016), graph-based parsing (Dozat and Manning, 2016), distilling a graph-based parser from the output of 20 parsers (Kuncoro et al., 2016), and converting constituent parsing results to dependencies (Kuncoro et al., 2017). Our d"
P18-1129,P14-1043,0,0.0301015,"and exploration. Freitag et al. (2017) used an ensemble of 6translators to generate training reference. Exploration was tried in their work with beam-search. We differ their work by training the single model 1400 to match the distribution of the ensemble. Using ensemble in exploration was also studied in reinforcement learning community (Osband et al., 2016). In addition to distilling the ensemble on the labeled training data, a line of semisupervised learning works show that it’s effective to transfer knowledge of cumbersome model into a simple one on the unlabeled data (Liang et al., 2008; Li et al., 2014). Their extensions to knowledge distillation call for further study. Kuncoro et al. (2016) proposed to compile the knowledge from an ensemble of 20 transitionbased parsers into a voting and distill the knowledge by introducing the voting results as a regularizer in learning a graph-based parser. Different from their work, we directly do the distillation on the classifier of the transition-based parser. Besides the attempts for directly using the knowledge distillation technique, Stahlberg and Byrne (2017) propose to first build the ensemble of several machine translators into one network by un"
P18-1129,Q14-1042,0,0.0137791,"d model significantly improves over strong baselines and outperforms other greedy structured prediction (§4.2). Comprehensive analysis empirically shows the feasibility of our distillation method (§4.3). 2 2.1 Background Search-based Structured Prediction Structured prediction maps an input x = (x1 , x2 , ..., xn ) to its structural output y = (y1 , y2 , ..., ym ), where each component of y has some internal dependencies. Search-based structured prediction (Collins and Roark, 2004; Daum´e III et al., 2005; Daum´e III et al., 2009; Ross and Bagnell, 2010; Ross et al., 2011; Doppa et al., 2014; Vlachos and Clark, 2014; Chang et al., 2015) models the generation of the structure as a search problem and it can be formalized as a tuple (S, A, T (s, a), S0 , ST ), in which S is a set of states, A is a set of actions, T is a function that maps S × A → S, S0 is a set of initial states, and ST is a set of terminal states. Starting from an initial state s0 ∈ S0 , the structured prediction model repeatably chooses an action at ∈ A by following a policy π(s) and applies at to st and enter a new state st+1 as st+1 ← T (st , at ), until a final state sT ∈ ST is achieved. Several natural language structured prediction p"
P18-1129,D16-1137,0,0.0952854,"l our dependency parsing, we set n = 20. 4.1.2 Neural Machine Translation We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign. The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs. We use the same preprocessing as Ranzato et al. (2015), which leads to a German vocabulary of about 30K entries and an English vocabulary of 25K entries. One-layer LSTM for both encoder and decoder with 256 hidden units are used by following Wiseman and Rush (2016). BLEU (Papineni et al., 2002) was used to evaluate the translator’s performance.4 Like in the dependency parsing experiments, we run M = 10 differentlyseeded runs and report the averaged score. Optimizing the distillation loss in Equation 1 requires enumerating over the action space. It is expensive for machine translation since the size of the action space (vocabulary) is considerably large (25K in our experiments). In this paper, we use the K-most probable actions (translations on target side) on one state to approximate the P whole probability distribution of q(a |s) as a q(a | P s) · log"
P18-1129,D08-1059,0,0.0483245,"tion-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our knowledge distillation for search-based str"
P18-1129,P06-1096,0,0.162632,"Missing"
P18-1129,D15-1166,0,0.0503662,"Missing"
P19-1366,J05-3002,0,0.053984,"Missing"
P19-1366,C10-3004,1,0.580884,"nt retrieved is always the one whose message is exactly the training message and responses contain the ground-truth response. We thus remove the document from the retrieved result before re-ranking to make sure that the N-best response candidates are different from the groundtruth response. 4 Experiments 4.1 Data We use the NTCIR corpus4 in our experiments. Its data are collected from a Chinese microblogging service, Sina Weibo5 , where users can both post messages and make comments (responses) on other users’ messages. First, we tokenize each utterance using the Language Technology Platform (Che et al., 2010) and remove samples whose responses are shorter than 5, which is helpful in relieving the generic response problem (Li et al., 2017). Then, we randomly select 10,000 messages associated with responses to form a validation set and another 10,000 messages with responses as a test set. Table 2 shows some statistics of the datasets. 4.2 Baselines Rtr: The retrieval-based method searches the index for response candidates and subsequently re4 5 http://research.nii.ac.jp/ntcir/data/data-en.html https://weibo.com turns the one that best matches the message after re-ranking (see Sec. 3.4 for details)."
P19-1366,P18-1139,0,0.0947042,"roach is 500. The batch size is set to 64. The discriminator and the generator are trained alternately, where the discriminator is optimized for 10 batches, then switch to the generator for 20 batches. We use ADAM optimizer whose learning rate is initialized to 0.0001. In the inference process, we generate responses using beam search with beam size set to 5. 5 Results 5.1 Evaluation Metrics Human Evaluation We randomly sampled 200 messages from the test set to conduct the human evaluation as it is extremely time-consuming. Five annotators8 are recruited to judge a response from three aspects (Ke et al., 2018): 6 https://github.com/MarkWuNLP/ResponseEdit https://code.google.com/archive/p/word2vec/ 8 All annotators are well-educated students and have Bachelor or higher degree. 3768 7 Mean Rtr S2S MS2S Edit AL Ours 0.63 0.76 0.85 0.85 0.98 1.10 Appropriateness +2 +1 0 24.8 27.9 31.9 31.4 36.8 41.5 12.9 20.0 21.5 21.9 24.0 26.8 62.3 52.1 46.6 46.7 39.2 31.7  Mean 0.71 0.58 0.63 0.66 0.57 0.65 0.92 0.51 0.62 0.67 0.77 0.88 Informativeness +2 +1 0 41.1 10.2 14.1 15.9 21.8 31.2 10.1 30.5 33.8 34.9 33.6 25.9 48.8 59.3 52.1 49.2 44.6 42.9  Mean 0.67 0.69 0.73 0.68 0.66 0.72 1.93 1.74 1.74 1.92 1.88 1.87"
P19-1366,P17-4012,0,0.0344183,"tor representing lexical differences between retrieved contexts and the message. AL: The adversarial learning for neural response generation (Li et al., 2017) is also an adversarial method but is not retrieval-enhanced. Here, we do not employ the REGS (reward for every generation step) setting as the Monte-Carlo roll-out is quite time-consuming and the accuracy of the discriminator trained on partially decoded sequences is not as good as that on complete sequences. 4.3 Experiment Settings We use the published code6 for Edit and implement other approaches by an open source framework: Open-NMT (Klein et al., 2017). The vocabulary table consists of the most frequent 30,000 words, whose 300-dimensional word embeddings are pre-trained on the training set by Word2Vec 7 . The number of hidden units for all LSTM in our approach is 500. The batch size is set to 64. The discriminator and the generator are trained alternately, where the discriminator is optimized for 10 batches, then switch to the generator for 20 batches. We use ADAM optimizer whose learning rate is initialized to 0.0001. In the inference process, we generate responses using beam search with beam size set to 5. 5 Results 5.1 Evaluation Metrics"
P19-1366,W06-1303,0,0.542045,"systems, personal assistants, and chatbots. Early dialogue systems are often built using the rule-based method (Weizenbaum, 1966) or template-based method (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007), which are usually labor-intensive and difficult to scale up. Recently, with the rise of social networking, conversational data have accumulated to a considerable scale. This promoted the development of data-driven methods, including retrieval-based methods (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Wen et al., 2017) and generation-based methods (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016). Retrieval-based methods reply to users by searching and re-ranking response candidates ⇤ Corresponding author. from a pre-constructed response set. Written mainly by humans, these responses are always diverse and informative, but may be inappropriate to input messages due to their being prepared in advance and thus incapable of being customized (Shang et al., 2015). In contrast, generation-based methods can produce responses tailored to the messages. The most common method of this category in recent years is the sequence to sequence (Seq2Seq) model (Sutske"
P19-1366,N16-1014,0,0.387164,"set. Written mainly by humans, these responses are always diverse and informative, but may be inappropriate to input messages due to their being prepared in advance and thus incapable of being customized (Shang et al., 2015). In contrast, generation-based methods can produce responses tailored to the messages. The most common method of this category in recent years is the sequence to sequence (Seq2Seq) model (Sutskever et al., 2014; Shang et al., 2015; Vinyals and Le, 2015). In practice, it usually suffers from the problem of generating generic responses, such as “I don’t know” and “Me, too” (Li et al., 2016a; Serban et al., 2016). While the contents of retrieved responses, apart from the irrelevant parts, are of great diversity, making it a potential resource for tailoring appropriate and informative responses. Therefore, it is natural to enhance the response generation approach with retrieved responses. Previous work has been proposed to extend the input of a Seq2Seq model with N-best response candidates (or their contexts) (Song et al., 2018; Pandey et al., 2018). On one hand, these approaches are trained using MLE objective, which correlates weakly with true quality of responses thus limits t"
P19-1366,D16-1127,0,0.613432,"set. Written mainly by humans, these responses are always diverse and informative, but may be inappropriate to input messages due to their being prepared in advance and thus incapable of being customized (Shang et al., 2015). In contrast, generation-based methods can produce responses tailored to the messages. The most common method of this category in recent years is the sequence to sequence (Seq2Seq) model (Sutskever et al., 2014; Shang et al., 2015; Vinyals and Le, 2015). In practice, it usually suffers from the problem of generating generic responses, such as “I don’t know” and “Me, too” (Li et al., 2016a; Serban et al., 2016). While the contents of retrieved responses, apart from the irrelevant parts, are of great diversity, making it a potential resource for tailoring appropriate and informative responses. Therefore, it is natural to enhance the response generation approach with retrieved responses. Previous work has been proposed to extend the input of a Seq2Seq model with N-best response candidates (or their contexts) (Song et al., 2018; Pandey et al., 2018). On one hand, these approaches are trained using MLE objective, which correlates weakly with true quality of responses thus limits t"
P19-1366,D17-1230,0,0.430029,"sponses, such as keywords (Mou et al., 2016; Serban et al., 2017a), topic information (Xing et al., 2017) and knowledge triples (Zhou et al., 2018). Another way is to improve the Seq2Seq architecture. Li et al.(2016b) introduce the Maximum Mutual Information as the objective function. Serban et al.(2017b) add a latent variable to inject variability. The training of Seq2Seq can be formulated as a reinforcement learning problem (Li et al., 2016b; Zhang et al., 2017). To avoid manually defining reward functions, a discriminator can be introduced and trained synchronously by adversarial learning (Li et al., 2017). After that, Xu et al. (2018) propose a language model based discriminator to better distinguish novel responses from repeated responses. In a similar adversarial setting, Zhang et al. (2018) optimize a Variational Information Maximization Objective to improve informativeness. Our approach is also an adversarial model, the difference is that we employ the N-best response candidates to enhance the generation. Taking advantages of the two methods, retrieval-enhanced response generation approaches make use of the informative content in retrieved results to generate new responses. Typically, gene"
P19-1366,W00-0304,0,0.00841286,"y favorite dessert. It’s so delicious. Table 1: An example of a message (MSG), a groundtruth response (GT), a generated response (RSP) and N-best response candidates (C#1 and C#2) during the training process. Similar contents in the response and candidates are in boldface. Introduction Dialogue systems intend to converse with humans with a coherent structure. They have been widely used in real-world applications, including customer service systems, personal assistants, and chatbots. Early dialogue systems are often built using the rule-based method (Weizenbaum, 1966) or template-based method (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007), which are usually labor-intensive and difficult to scale up. Recently, with the rise of social networking, conversational data have accumulated to a considerable scale. This promoted the development of data-driven methods, including retrieval-based methods (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Wen et al., 2017) and generation-based methods (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016). Retrieval-based methods reply to users by searching and re-ranking response candidates ⇤ Corresponding author. from a p"
P19-1366,W05-1612,0,0.0246684,". (2018) optimize a Variational Information Maximization Objective to improve informativeness. Our approach is also an adversarial model, the difference is that we employ the N-best response candidates to enhance the generation. Taking advantages of the two methods, retrieval-enhanced response generation approaches make use of the informative content in retrieved results to generate new responses. Typically, generating responses from retrieved candidates can be seen as a text-to-text system, which produces meaningful text from meaningful text rather than from abstract meaning representations (Marsi and Krahmer, 2005). Barzilay 3764 Retrieval-based Method Policy Gradient Generator Prob ( human-generated ) Response How did you make it? It looks delicious. Encoder Training Set MLP Message: I made strawberry shortcake. Candidate LSTM Candidate#1 Could you tell me how this thing is cooked? Candidate#2Tiramisu is my favorite dessert. It’s so delicious. … N-best Response Candidates Zx Concatenation Response LSTM Message LSTM Index Decoder Average 1 Zc 2 Zc … Discriminator Figure 1: An overview of our proposed approach. The discriminator is enhanced by the N-best response candidates returned by a retrieval-"
P19-1366,C16-1316,0,0.129093,"information retrieval techniques to rank response candidates. In addition, the matching and ranking methods can also be implemented using neural networks (Yan et al., 2016; Qiu et al., 2017; Wu et al., 2017). Based on that, Yang et al. (2018) propose a deep matching network which could model external knowledge. Generation-based methods can be cast as a sequence to sequence (Seq2Seq) process (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015) but suffers from generating generic responses. One way to address the problem is to introduce new content into responses, such as keywords (Mou et al., 2016; Serban et al., 2017a), topic information (Xing et al., 2017) and knowledge triples (Zhou et al., 2018). Another way is to improve the Seq2Seq architecture. Li et al.(2016b) introduce the Maximum Mutual Information as the objective function. Serban et al.(2017b) add a latent variable to inject variability. The training of Seq2Seq can be formulated as a reinforcement learning problem (Li et al., 2016b; Zhang et al., 2017). To avoid manually defining reward functions, a discriminator can be introduced and trained synchronously by adversarial learning (Li et al., 2017). After that, Xu et al. (20"
P19-1366,P18-1123,0,0.0372555,"ls and Le, 2015). In practice, it usually suffers from the problem of generating generic responses, such as “I don’t know” and “Me, too” (Li et al., 2016a; Serban et al., 2016). While the contents of retrieved responses, apart from the irrelevant parts, are of great diversity, making it a potential resource for tailoring appropriate and informative responses. Therefore, it is natural to enhance the response generation approach with retrieved responses. Previous work has been proposed to extend the input of a Seq2Seq model with N-best response candidates (or their contexts) (Song et al., 2018; Pandey et al., 2018). On one hand, these approaches are trained using MLE objective, which correlates weakly with true quality of responses thus limits the effectiveness of the candidates in producing the responses. Table 1 shows an exam3763 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3763–3773 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ple during the training process. Related contents of the candidates are appropriately integrated into the response, but the model is discouraged as the response is different from th"
P19-1366,P17-2079,0,0.013461,"aselines in both automatic and human evaluations. 2 Related Work Data-driven dialogue systems can be roughly divided into two categories: retrieval-based and generation-based. Retrieval-based methods respond to users by selecting the response that best matches an input message from a pre-constructed response set. Leuski et al. (2006) match a response with a message using a statistical language model. Ji et al.(2014) employ information retrieval techniques to rank response candidates. In addition, the matching and ranking methods can also be implemented using neural networks (Yan et al., 2016; Qiu et al., 2017; Wu et al., 2017). Based on that, Yang et al. (2018) propose a deep matching network which could model external knowledge. Generation-based methods can be cast as a sequence to sequence (Seq2Seq) process (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015) but suffers from generating generic responses. One way to address the problem is to introduce new content into responses, such as keywords (Mou et al., 2016; Serban et al., 2017a), topic information (Xing et al., 2017) and knowledge triples (Zhou et al., 2018). Another way is to improve the Seq2Seq architecture. Li et al.(2016b)"
P19-1366,P15-1152,0,0.0615908,"Missing"
P19-1366,N15-1020,0,0.0779876,"Missing"
P19-1366,E17-1042,0,0.0278622,"Missing"
P19-1366,D18-1428,0,0.0597753,"et al., 2016; Serban et al., 2017a), topic information (Xing et al., 2017) and knowledge triples (Zhou et al., 2018). Another way is to improve the Seq2Seq architecture. Li et al.(2016b) introduce the Maximum Mutual Information as the objective function. Serban et al.(2017b) add a latent variable to inject variability. The training of Seq2Seq can be formulated as a reinforcement learning problem (Li et al., 2016b; Zhang et al., 2017). To avoid manually defining reward functions, a discriminator can be introduced and trained synchronously by adversarial learning (Li et al., 2017). After that, Xu et al. (2018) propose a language model based discriminator to better distinguish novel responses from repeated responses. In a similar adversarial setting, Zhang et al. (2018) optimize a Variational Information Maximization Objective to improve informativeness. Our approach is also an adversarial model, the difference is that we employ the N-best response candidates to enhance the generation. Taking advantages of the two methods, retrieval-enhanced response generation approaches make use of the informative content in retrieved results to generate new responses. Typically, generating responses from retrieve"
P19-1415,D14-1179,0,0.0545077,"Missing"
P19-1415,P18-1078,0,0.0736195,"eparate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for humans to the paragraph. Clark and Gardner (2018) and Tan et al. (2018) use questions to retrieve paragraphs that do not contain the answer as adversarial examples. Rajpurkar et al. (2018) create unanswerable questions through rigid rules, which swap entities, numbers and antonyms of answerable questions. It has been shown that adversarial examples generated by rule-based systems are much easier to detect than ones in the SQuAD 2.0 dataset. Data Augmentation for MRC Several attempts have been made to augment training data for machine reading comprehension. We catego4239 SEQ2SEQ tween inputs. The decoder of two models generates unanswerable q"
P19-1415,N19-1423,0,0.183228,") = Interac+on |˜ q| Y P (˜ qt |˜ q&lt;t , q, p, a) (1) t=1 Encoder W W W C … C C P A P W Encoder W W W W C … C C … C C C … C C C Q P A P Q Q Q Paragraph + Ques+on W Encoder P Paragraph W P W W W … where q˜&lt;t = q˜1 . . . q˜t−1 . W C C Q Q 3.1 Ques+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and it"
P19-1415,P18-1177,0,0.049005,"Missing"
P19-1415,P17-1123,0,0.0430977,"l., 2018; Yu et al., 2018; Wang et al., 2018) have outperformed humans on this task in terms of automatic metrics. The SQuAD 2.0 dataset (Rajpurkar et al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. I"
P19-1415,P16-1154,0,0.0669924,"Missing"
P19-1415,N10-1086,0,0.0764351,"t al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable quest"
P19-1415,P82-1020,0,0.820425,"Missing"
P19-1415,D17-1215,0,0.184369,"xtra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for humans to the paragraph. Clark and Gardner (2018) and Tan et al. (2018) use questions to retrieve paragraphs that do not contain the answer as adversarial examples. Rajpurkar et al. (2018) create unanswerable questions through rigid rules, which swap entities, numbers and antonyms of answerable questions. It has been shown that adversarial examples generated by rule-based systems are much easier to detect than ones in the SQuAD 2.0 dataset. Data Augmentation for MRC Several attempts hav"
P19-1415,P17-1147,0,0.0420112,"W W W C … C C … C C C … C C C Q P A P Q Q Q Paragraph + Ques+on W Encoder P Paragraph W P W W W … where q˜&lt;t = q˜1 . . . q˜t−1 . W C C Q Q 3.1 Ques+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and its corresponding paragraph p that contains the answer a, we aim to generate unanswerable questio"
P19-1415,P17-4012,0,0.0800098,"Missing"
P19-1415,Q18-1023,0,0.0474809,"Missing"
P19-1415,K17-1034,0,0.0257042,"ast example shows that inserting negation words in different positions (“n’t public” versus “not in victoria”) can express different meanings. Such cases are critical for generated questions’ answerability, which is hard to handle in a rule-based system. 4.2 4.2.1 Data Augmentation for Machine Reading Comprehension Question Answering Models We apply our automatically generated unanswerable questions as augmentation data to the following reading comprehension models: BiDAF-No-Answer (BNA) BiDAF (Seo et al., 2017) is a benchmark model on extractive machine reading comprehension. Based on BiDAF, Levy et al. (2017) propose the BiDAF-No-Answer model to predict the distribution of answer candidates and the probability of a question being unanswerable at the same time. DocQA Clark and Gardner (2018) propose the DocQA model to address document-level reading comprehension. The no-answer probability is also predicted jointly. BERT Fine-Tuning It is the state-of-the-art model on unanswerable machine reading comprehension. We adopt the uncased version of BERT (Devlin et al., 2019) for fine-tuning. The batch sizes of BERT-base and BERT-large are set to 12 and 24 respectively. The rest hyperparameters are kept un"
P19-1415,W04-1013,0,0.0112197,"d embeddings. The hidden state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation task. We also conduc"
P19-1415,P18-1157,0,0.0277041,"Missing"
P19-1415,D15-1166,0,0.0405388,"rent neural networks with long short-term memory units (LSTM; Hochreiter and Schmidhuber, 1997) to produce encoder hidden states hi = fBiLSTM (hi−1 , ei ). On each decoding step t, the hidden states of decoder (a single-layer unidirectional LSTM network) are computed by st = fLSTM (st−1 , [yt−1 ; ct−1 ]), where yt−1 is the word embedding of previously predicted token and ct−1 is the encoder context vector of previous step. Besides, we use an attention mechanism to summarize the encoder-side information into ct for current step. The attention distribution γt over source words is computed as in Luong et al. (2015): score(hi , st ) = hT i Wγ s t γi,t = exp(score(hi , st ))/Zt ct = |x| X γi,t hi (2) (3) (4) i P|x| where Zt = k exp(score(hk , st )), Wγ in score function is a learnable parameter. Next, st is concatenated with ct to produce the vocabulary distribution Pv : 4240 Pv = softmax(Wv [st ; ct ] + bv ) (5) where Wv and bv are learnable parameters. Copy mechanism (See et al., 2017) is incorporated to directly copy words from inputs, because words in paragraphs or source questions are of great value for unanswerable question generation. Specifically, we use st and ct to produce a gating probability g"
P19-1415,P15-2097,0,0.0231868,"den state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation task. We also conduct human evaluation on 100 sample"
P19-1415,P02-1040,0,0.104033,"are the same vocabulary and word embeddings. The hidden state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation t"
P19-1415,D14-1162,0,0.0826805,"Missing"
P19-1415,P18-2124,0,0.555636,"aset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during internship at Microsoft Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve question answering models. The generated unanswerable questions should not be too easy for the question answering model so that data augmentation can better help the model. For example, a simple baseline method is randomly choosing a question asked for an"
P19-1415,D16-1264,0,0.0603667,"ion on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model. 1 Ans. Question: What organization runs the public schools in Victoria? UnAns. Question: What organization runs the waste management in Victoria? (Plausible) Answer: Victoria Department of Education Figure 1: An example taken from the SQuAD 2.0 dataset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during int"
P19-1415,P17-1099,0,0.0281374,"s the encoder context vector of previous step. Besides, we use an attention mechanism to summarize the encoder-side information into ct for current step. The attention distribution γt over source words is computed as in Luong et al. (2015): score(hi , st ) = hT i Wγ s t γi,t = exp(score(hi , st ))/Zt ct = |x| X γi,t hi (2) (3) (4) i P|x| where Zt = k exp(score(hk , st )), Wγ in score function is a learnable parameter. Next, st is concatenated with ct to produce the vocabulary distribution Pv : 4240 Pv = softmax(Wv [st ; ct ] + bv ) (5) where Wv and bv are learnable parameters. Copy mechanism (See et al., 2017) is incorporated to directly copy words from inputs, because words in paragraphs or source questions are of great value for unanswerable question generation. Specifically, we use st and ct to produce a gating probability gt : gt = sigmoid(Wg [st ; ct ] + bg ) (6) where Wg and bg are learnable parameters. The gate gt determines whether generating a word from the vocabulary or copying a word from inputs. Finally, we obtain the probability of generating q˜t by: X P (˜ qt |˜ q&lt;t , q, p, a) = gt Pv (˜ qt ) + (1 − gt ) γˆi,t i∈ζq˜t where ζq˜t denotes all the occurrence of q˜t in inputs, and the copy"
P19-1415,N18-2090,0,0.0319965,"document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for h"
P19-1415,N19-1270,0,0.0353075,"Missing"
P19-1415,D18-1427,0,0.0242532,"answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do"
P19-1415,P18-1158,0,0.0133866,"ith BERT-large model. 2 Related Work Machine Reading Comprehension (MRC) Various large-scale datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2018; Kocisky et al., 2018) have spurred rapid progress on machine reading comprehension in recent years. SQuAD (Rajpurkar et al., 2016) is an extractive benchmark whose questions and answers spans are annotated by humans. Neural reading comprehension systems (Wang and Jiang, 2017; Seo et al., 2017; Wang et al., 2017; Hu et al., 2018; Huang et al., 2018; Liu et al., 2018; Yu et al., 2018; Wang et al., 2018) have outperformed humans on this task in terms of automatic metrics. The SQuAD 2.0 dataset (Rajpurkar et al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models base"
P19-1415,P17-1018,1,0.86445,"1 Ans. Question: What organization runs the public schools in Victoria? UnAns. Question: What organization runs the waste management in Victoria? (Plausible) Answer: Victoria Department of Education Figure 1: An example taken from the SQuAD 2.0 dataset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during internship at Microsoft Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve"
P19-1415,P17-1096,0,0.0389431,"ues+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and its corresponding paragraph p that contains the answer a, we aim to generate unanswerable questions q˜ that fulfills certain requirements. First, it cannot be answered by paragraph p. Second, it must be relevant to both answerable question q"
P19-1415,P13-1171,0,0.0355318,"Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve question answering models. The generated unanswerable questions should not be too easy for the question answering model so that data augmentation can better help the model. For example, a simple baseline method is randomly choosing a question asked for another paragraph, and using it as an unanswerable question. However, it would be trivial to determine whether the retrieved question is answerable by using wordoverlap heuristics, because the question is irrelevant to the context (Yih et al., 2013). In this work, we propose to generate unanswerable questions by editing an answerable question and conditioning on the corresponding paragraph that contains the answer. So the generated unanswerable questions are more lexically similar and relevant to the context. Moreover, by using the answerable question as a prototype and its answer span as a plausible answer, the generated examples can provide more discriminative training signal to the question answering model. 4238 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4238–4248 c Florence, Italy,"
P19-1444,D13-1160,0,0.440424,"Missing"
P19-1444,P18-1068,0,0.218902,"enotes the embedding of columns in memory and Ecs denotes the embedding of columns that are never selected. wm is trainable parameter. When it comes to S ELECT TABLE, the decoder selects a table t from the schema via a pointer network: p(ai = S ELECT TABLE[t]|x, s, a&lt;i ) ∝ exp(viT Et ). As shown in Figure 4, the decoder first predicts a column and then predicts the table that it belongs to. To this end, we can leverage the relations between columns and tables to prune the irrelevant tables. Coarse-to-fine. We further adopt a coarse-to-fine framework (Solar-Lezama, 2008; Bornholt et al., 2016; Dong and Lapata, 2018), decomposing the decoding process of a SemQL query into two stages. In the first stage, a skeleton decoder outputs a skeleton of the SemQL query. Then, a detail decoder fills in the missing details in the skeleton by selecting columns and tables. Supplementary materials provide a detailed description of the skeleton of a SemQL query and the coarse-to-fine framework. 3 Experiment In this section, we evaluate the effectiveness of IRNet by comparing it to the state-of-the-art approaches and ablating several design choices in IRNet to understand their contributions. 3.1 Experiment Setup Dataset."
P19-1444,P18-1033,0,0.290803,"hallenging to accommodate cross-domain settings. Later work focus on building a system that can be reused for multiple databases with minimal human efforts (Grosz et al., 1987; Androutsopoulos et al., 1993; Tang and Mooney, 2000). Recently, with the development of advanced neural approaches on Semantic Parsing and the release of large-scale, cross-domain Text-to-SQL benchmarks such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c), there is a renewed interest in the task (Xu et al., 2017; Iyer et al., 2017; Sun et al., 2018; Gur et al., 2018; Yu et al., 2018a,b; Wang et al., 2018; Finegan-Dollak et al., 2018; Hwang et al., 2019). Unlike these neural approaches that end-to-end synthesize a SQL query, IRNet first synthesizes a SemQL query and then infers a SQL query from it. Intermediate Representations in NLIDB. Early proposed systems like as LUNAR (Woods, 1986) and MASQUE (Androutsopoulos et al., 1993) also propose intermediate representations (IR) to represent the meaning of questions and then translate it into SQL queries. The predicates in these IRs are designed for a specific database, which sets SemQL apart. SemQL targets a wide adoption and no human effort is needed when it is used in a new"
P19-1444,P16-1154,0,0.0942492,"Missing"
P19-1444,P18-1124,0,0.0835598,"ra, 1982; Woods, 1986; Hendrix et al., 1978), making it challenging to accommodate cross-domain settings. Later work focus on building a system that can be reused for multiple databases with minimal human efforts (Grosz et al., 1987; Androutsopoulos et al., 1993; Tang and Mooney, 2000). Recently, with the development of advanced neural approaches on Semantic Parsing and the release of large-scale, cross-domain Text-to-SQL benchmarks such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c), there is a renewed interest in the task (Xu et al., 2017; Iyer et al., 2017; Sun et al., 2018; Gur et al., 2018; Yu et al., 2018a,b; Wang et al., 2018; Finegan-Dollak et al., 2018; Hwang et al., 2019). Unlike these neural approaches that end-to-end synthesize a SQL query, IRNet first synthesizes a SemQL query and then infers a SQL query from it. Intermediate Representations in NLIDB. Early proposed systems like as LUNAR (Woods, 1986) and MASQUE (Androutsopoulos et al., 1993) also propose intermediate representations (IR) to represent the meaning of questions and then translate it into SQL queries. The predicates in these IRs are designed for a specific database, which sets SemQL apart. SemQL targets a"
P19-1444,W06-1414,0,0.0979616,"Missing"
P19-1444,D18-1190,0,0.0594342,"n these IRs are designed for a specific database, which sets SemQL apart. SemQL targets a wide adoption and no human effort is needed when it is used in a new domain. Li and Jagadish (2014) propose a query tree in their NLIDB system to represent the meaning of a question and it mainly serves as an interaction medium between users and their system. Entity Linking. The insight behind performing schema linking is partly inspired by the success of incorporating entity linking in knowledge base question answering and semantic parsing (Yih et al., 2016; Krishnamurthy et al., 2017; Yu et al., 2018a; Herzig and Berant, 2018; Kolitsas et al., 2018). In the context of semantic parsing, Krishnamurthy et al. (2017) propose a neural entity linking module for answering compositional questions on semi-structured tables. TypeSQL (Yu et al., 2018a) proposes to utilize type information to better understand rare entities and numbers in questions. Similar to TypeSQL, IRNet also recognizes the columns and tables mentioned in a question. What sets IRNet apart is that IRNet assigns different types to the columns based on how they are mentioned in the question. 6 Conclusion We present a neural approach SemQL for complex and cro"
P19-1444,P17-1089,0,0.209091,"a specific database (Warren and Pereira, 1982; Woods, 1986; Hendrix et al., 1978), making it challenging to accommodate cross-domain settings. Later work focus on building a system that can be reused for multiple databases with minimal human efforts (Grosz et al., 1987; Androutsopoulos et al., 1993; Tang and Mooney, 2000). Recently, with the development of advanced neural approaches on Semantic Parsing and the release of large-scale, cross-domain Text-to-SQL benchmarks such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c), there is a renewed interest in the task (Xu et al., 2017; Iyer et al., 2017; Sun et al., 2018; Gur et al., 2018; Yu et al., 2018a,b; Wang et al., 2018; Finegan-Dollak et al., 2018; Hwang et al., 2019). Unlike these neural approaches that end-to-end synthesize a SQL query, IRNet first synthesizes a SemQL query and then infers a SQL query from it. Intermediate Representations in NLIDB. Early proposed systems like as LUNAR (Woods, 1986) and MASQUE (Androutsopoulos et al., 1993) also propose intermediate representations (IR) to represent the meaning of questions and then translate it into SQL queries. The predicates in these IRs are designed for a specific database, whic"
P19-1444,W08-2105,0,0.064658,"Missing"
P19-1444,K18-1050,0,0.0167548,"for a specific database, which sets SemQL apart. SemQL targets a wide adoption and no human effort is needed when it is used in a new domain. Li and Jagadish (2014) propose a query tree in their NLIDB system to represent the meaning of a question and it mainly serves as an interaction medium between users and their system. Entity Linking. The insight behind performing schema linking is partly inspired by the success of incorporating entity linking in knowledge base question answering and semantic parsing (Yih et al., 2016; Krishnamurthy et al., 2017; Yu et al., 2018a; Herzig and Berant, 2018; Kolitsas et al., 2018). In the context of semantic parsing, Krishnamurthy et al. (2017) propose a neural entity linking module for answering compositional questions on semi-structured tables. TypeSQL (Yu et al., 2018a) proposes to utilize type information to better understand rare entities and numbers in questions. Similar to TypeSQL, IRNet also recognizes the columns and tables mentioned in a question. What sets IRNet apart is that IRNet assigns different types to the columns based on how they are mentioned in the question. 6 Conclusion We present a neural approach SemQL for complex and cross-domain Text-to-SQL, a"
P19-1444,D17-1160,0,0.15068,"ranslate it into SQL queries. The predicates in these IRs are designed for a specific database, which sets SemQL apart. SemQL targets a wide adoption and no human effort is needed when it is used in a new domain. Li and Jagadish (2014) propose a query tree in their NLIDB system to represent the meaning of a question and it mainly serves as an interaction medium between users and their system. Entity Linking. The insight behind performing schema linking is partly inspired by the success of incorporating entity linking in knowledge base question answering and semantic parsing (Yih et al., 2016; Krishnamurthy et al., 2017; Yu et al., 2018a; Herzig and Berant, 2018; Kolitsas et al., 2018). In the context of semantic parsing, Krishnamurthy et al. (2017) propose a neural entity linking module for answering compositional questions on semi-structured tables. TypeSQL (Yu et al., 2018a) proposes to utilize type information to better understand rare entities and numbers in questions. Similar to TypeSQL, IRNet also recognizes the columns and tables mentioned in a question. What sets IRNet apart is that IRNet assigns different types to the columns based on how they are mentioned in the question. 6 Conclusion We present"
P19-1444,P17-1003,0,0.052506,"luding A P PLY RULE , S ELECT C OLUMN and S ELECT TABLE . A PPLY RULE(r) applies a production rule r to the current derivation tree of a SemQL query. S E LECT C OLUMN (c) and S ELECT TABLE (t) selects a column c and a table t from the schema, respectively. Here, we detail the action S ELECT C OL UMN and S ELECT TABLE . Interested readers can refer to Yin and Neubig (2017) for details of the action A PPLY RULE. We design a memory augmented pointer network to implement the action S ELECT C OLUMN. The memory is used to record the selected columns, which is similar to the memory mechanism used in Liang et al. (2017). When the decoder is going to select a column, it first makes a decision on whether to select from the memory or not, and then selects a column from the memory or the schema based on the decision. Once a column is selected, it will be removed from the schema where S represents selecting from schema, M EM represents selecting from memory, vi denotes the context vector that is obtained by performing an attention over Hx , Ecm denotes the embedding of columns in memory and Ecs denotes the embedding of columns that are never selected. wm is trainable parameter. When it comes to S ELECT TABLE, the"
P19-1444,J13-2005,0,0.0408879,"Missing"
P19-1444,P11-1060,0,0.0838098,"Missing"
P19-1444,P15-1142,0,0.206279,"Missing"
P19-1444,D14-1162,0,0.0814326,"Missing"
P19-1444,C04-1021,0,0.509961,"Missing"
P19-1444,speer-havasi-2012-representing,0,0.0123867,"As shown in the figure, the column ‘book title’ is selected from the schema, while the second column ‘year’ is selected from the memory. span a type according to its entity. For example, if a span is recognized as column, we will assign it a type C OLUMN. Figure 4 depicts the schema linking results of a question. For those spans recognized as column, if they exactly match the column names in the schema, we assign these columns a type E XACT M ATCH, otherwise a type PARTIAL M ATCH. To link the cell value with its corresponding column in the schema, we first query the value span in ConceptNet (Speer and Havasi, 2012) which is an open, large-scale knowledge graph and search the results returned by ConceptNet over the schema. We only consider the query results in two categories of ConceptNet, namely, ‘is a type of’ and ‘related terms’, as we observe that the column that a cell value belongs to usually occurs in these two categories. If there exists a result exactly or partially matches a column name in the schema, we assign the column a type VALUE E XACT M ATCH or VALUE PARTIAL M ATCH. 2.3 Model We present the neural model to synthesize SemQL queries, which takes a question, a database schema and the schema"
P19-1444,P18-1034,1,0.868125,"(Warren and Pereira, 1982; Woods, 1986; Hendrix et al., 1978), making it challenging to accommodate cross-domain settings. Later work focus on building a system that can be reused for multiple databases with minimal human efforts (Grosz et al., 1987; Androutsopoulos et al., 1993; Tang and Mooney, 2000). Recently, with the development of advanced neural approaches on Semantic Parsing and the release of large-scale, cross-domain Text-to-SQL benchmarks such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c), there is a renewed interest in the task (Xu et al., 2017; Iyer et al., 2017; Sun et al., 2018; Gur et al., 2018; Yu et al., 2018a,b; Wang et al., 2018; Finegan-Dollak et al., 2018; Hwang et al., 2019). Unlike these neural approaches that end-to-end synthesize a SQL query, IRNet first synthesizes a SemQL query and then infers a SQL query from it. Intermediate Representations in NLIDB. Early proposed systems like as LUNAR (Woods, 1986) and MASQUE (Androutsopoulos et al., 1993) also propose intermediate representations (IR) to represent the meaning of questions and then translate it into SQL queries. The predicates in these IRs are designed for a specific database, which sets SemQL apart"
P19-1444,W00-1317,0,0.746065,"se. The task of Natural Language Interface to Database (NLIDB) has received significant attention since the 1970s (Warren and Pereira, 1981; Androutsopoulos et al., 1995; Popescu et al., 2004; Hallett, 4531 2006; Giordani and Moschitti, 2012). Most of the early proposed systems are hand-crafted to a specific database (Warren and Pereira, 1982; Woods, 1986; Hendrix et al., 1978), making it challenging to accommodate cross-domain settings. Later work focus on building a system that can be reused for multiple databases with minimal human efforts (Grosz et al., 1987; Androutsopoulos et al., 1993; Tang and Mooney, 2000). Recently, with the development of advanced neural approaches on Semantic Parsing and the release of large-scale, cross-domain Text-to-SQL benchmarks such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c), there is a renewed interest in the task (Xu et al., 2017; Iyer et al., 2017; Sun et al., 2018; Gur et al., 2018; Yu et al., 2018a,b; Wang et al., 2018; Finegan-Dollak et al., 2018; Hwang et al., 2019). Unlike these neural approaches that end-to-end synthesize a SQL query, IRNet first synthesizes a SemQL query and then infers a SQL query from it. Intermediate Representations in N"
P19-1444,J82-3002,0,0.740671,"Missing"
P19-1444,D18-1197,0,0.114903,"Missing"
P19-1444,P16-2033,0,0.0192249,"estions and then translate it into SQL queries. The predicates in these IRs are designed for a specific database, which sets SemQL apart. SemQL targets a wide adoption and no human effort is needed when it is used in a new domain. Li and Jagadish (2014) propose a query tree in their NLIDB system to represent the meaning of a question and it mainly serves as an interaction medium between users and their system. Entity Linking. The insight behind performing schema linking is partly inspired by the success of incorporating entity linking in knowledge base question answering and semantic parsing (Yih et al., 2016; Krishnamurthy et al., 2017; Yu et al., 2018a; Herzig and Berant, 2018; Kolitsas et al., 2018). In the context of semantic parsing, Krishnamurthy et al. (2017) propose a neural entity linking module for answering compositional questions on semi-structured tables. TypeSQL (Yu et al., 2018a) proposes to utilize type information to better understand rare entities and numbers in questions. Similar to TypeSQL, IRNet also recognizes the columns and tables mentioned in a question. What sets IRNet apart is that IRNet assigns different types to the columns based on how they are mentioned in the questi"
P19-1444,P17-1041,0,0.144865,"mn ci is as follows. and be recorded in the memory. The probability of selecting a column c is calculated as follows. p(ai = S ELECT C OLUMN[c]|x, s, a&lt;i ) = p(M EM|x, s, a&lt;i )p(c|x, s, a&lt;i , M EM) + p(S|x, s, a&lt;i )p(c|x, s, a&lt;i , S) T vi ) p(M EM|x, s, a&lt;i ) = sigmod(wm p(S|x, s, a&lt;i ) = 1 − p(M EM|x, s, a&lt;i ) gki = cic = eic = (ˆ eic )T ekx kˆ eic kkekx k L X gki ekx k=1 eˆic + cic + ϕi , p(c|x, s, a&lt;i , M EM) ∝ exp(viT Ecm ) p(c|x, s, a&lt;i , S) ∝ exp(viT Ecs ), Decoder. The goal of the decoder is to synthesize SemQL queries. Given the tree structure of SemQL, we use a grammar-based decoder (Yin and Neubig, 2017, 2018) which leverages a LSTM to model the generation process of a SemQL query via sequential applications of actions. Formally, the generation process of a SemQL query y can be formalized as follows. p(y|x, s) = T Y p(ai |x, s, a&lt;i ), i=1 where ai is an action taken at time step i, a&lt;i is the sequence of actions before i, and T is the number of total time steps of the whole action sequence. The decoder interacts with three types of actions to generate a SemQL query, including A P PLY RULE , S ELECT C OLUMN and S ELECT TABLE . A PPLY RULE(r) applies a production rule r to the current derivati"
P19-1444,D18-2002,0,0.221175,"Missing"
P19-1444,N18-2093,0,0.643104,"thesize SemQL queries. 4525 2.1 Intermediate Representation To eliminate the mismatch, we design a domain specific language, called SemQL, which serves as an intermediate representation between NL and SQL. Figure 2 presents the context-free grammar of SemQL. An illustrative SemQL query is shown in Figure 3. We elaborate on the design of SemQL in the following. Inspired by lambda DCS (Liang, 2013), SemQL is designed to be tree-structured. This structure, on the one hand, can effectively constrain the search space during synthesis. On the other hand, in view of the tree-structure nature of SQL (Yu et al., 2018b; Yin and Neubig, 2018), following the same structure also makes it easier to translate to SQL intuitively. The mismatch problem is mainly caused by the implementation details in SQL queries and missing specification in questions as discussed in Section 1. Therefore, it is natural to hide the implementation details in the intermediate representation, which forms the basic idea of SemQL. Considering the example in Figure 3, the GROUPBY, HAVING and FROM clauses in the SQL query are eliminated in the SemQL query, and the conditions in WHERE and HAVING are uniformly expressed in the subtree of Fi"
P19-1444,D18-1193,0,0.625958,"thesize SemQL queries. 4525 2.1 Intermediate Representation To eliminate the mismatch, we design a domain specific language, called SemQL, which serves as an intermediate representation between NL and SQL. Figure 2 presents the context-free grammar of SemQL. An illustrative SemQL query is shown in Figure 3. We elaborate on the design of SemQL in the following. Inspired by lambda DCS (Liang, 2013), SemQL is designed to be tree-structured. This structure, on the one hand, can effectively constrain the search space during synthesis. On the other hand, in view of the tree-structure nature of SQL (Yu et al., 2018b; Yin and Neubig, 2018), following the same structure also makes it easier to translate to SQL intuitively. The mismatch problem is mainly caused by the implementation details in SQL queries and missing specification in questions as discussed in Section 1. Therefore, it is natural to hide the implementation details in the intermediate representation, which forms the basic idea of SemQL. Considering the example in Figure 3, the GROUPBY, HAVING and FROM clauses in the SQL query are eliminated in the SemQL query, and the conditions in WHERE and HAVING are uniformly expressed in the subtree of Fi"
P19-1444,D18-1425,0,0.333479,"Missing"
S07-1034,W02-1006,0,0.0879164,"by their morphological root forms. The implementation for getting the morphological root forms is WordNet (morph). 4.2 We used 4 kinds of features of the target word and its context as shown in Table 1. Part of the original text of an example is “… This is the <head>age</head> of new media , the era of …”. Name MaltParser4 this_0, be_0, the_0, age_t, of_1, new_1, medium_1, ,_1, the_1 SYN_HEAD_is SYN_HEADPOS_VBZ SYN_RELATION_PRD SYN_HEADRIGHT Table 1: Features the system extracted The next 4 subsections elaborate these features. Learning Algorithm SVM is an effective learning algorithm to WSD (Lee and Ng, 2002). The SVM tries to find a hyperplane with the largest margin separating the training samples into two classes. The instances in the same side of the hyperplane have the same class label. A test instance’s feature decides the position where the sample is in the feature space and which side of the hyperplane it is. In this way, it leads to get a prediction. SVM could be extended to tackle multi-classes problems by using oneagainst-one or one-against-rest strategy. In the WSD problem, input of SVM is the feature vector of the instance. Features that appear in all the training samples are arranged"
S07-1034,gimenez-marquez-2004-svmtool,0,0.0141843,"; the instance is “x2 x6 x5 x7”. The feature vector of this sample should be <0, 1, 0, 0, 1, 1, 1>. The implementation of SVM here is libsvm 1 (Chang and Lin, 2001) for multi-classes. 4 Collocation Part-of-Speechs of Neighboring Words As mentioned above, the data sparseness is a serious problem in WSD. Besides changing tokens to their morphological root forms, part-of-speech is a good choice too. The size of POS tag set is much smaller than the size of surrounding words set. And the neighboring words’ part-of-speeches also contain useful information for WSD. In this part, we use a POS tagger (Giménez and Márquez, 2004) to assign POS tags to those tokens. We get the left and right 3 words’ POS tags together with their position information in the target words’ sentence. For example, the word age is to be disambiguated in the sentence of “… This is the 2 4 166 http://w3.msi.vxu.se/~nivre/research/MaltParser.html <head>age</head> of new media , the era of …”. The features then will be added to the feature vector are “DT_0, VBZ_0, DT_0, NN_t, IN_1, JJ_1, NNS_1”, in which _0/_1 stands for the word with current POS tag is in the left/right side of the target word. The POS tag set in use here is Penn Treebank Tagse"
S07-1034,W06-2933,0,0.0267094,"removed. Take the same instance last subsection has mentioned as example. The features we extracted are “this_0, be_0, the_0, age_t, of_1, new_1, medium_1”. Like POS, _0/_1 stands for the word is in the left/right side of the target word. Then the features were added to the feature vector space. 4.4 Syntactic Relations Many effective context words are not in a short distance to the target word, but we shouldn’t enlarge the window size too much in case of including too many noises. A solution to this problem is to use the syntactic relations of the target word and its parent head word. We use Nivre et al., (2006)’s dependency parser. In this part, we get 4 features from every instance: head word of the target word, the head word’s POS, the head word’s dependency relation with the target word and the relative position of the head word to the target word. Still take the same instance which has been mentioned in the las subsection as example. The features we extracted are “SYN_HEAD_is, SYN_HEADPOS_VBZ, SYN_RELATION_PRD, SYN_HEADRIGHT”, in which SYN_HEAD_is stands for is is the head word of age; SYN_HEADPOS_VBZ stands for the POS of the 5 6 5 http://www.lsi.upc.es/~nlp/SVMTool/PennTreebank.html 167 Data S"
S07-1036,P01-1008,0,0.120893,"Missing"
S07-1036,P97-1067,0,0.0747512,"Missing"
S07-1036,P98-1116,0,0.00851545,"the given sentence. Then we construct queries by replacing the target word in the fragments with the candidate substitute. Finally, we search Google using the constructed queries and score each candidate based on the counts of retrieved snippets. The rest of this paper is organized as follows: Section 2 reviews some related work on lexical substitution. Section 3 describes our system, especially the web-based scoring method. Section 4 presents the results and analysis. 2 Related Work Synonyms defined in WordNet have been widely used in lexical substitution and expansion (Smeaton et al., 1994; Langkilde and Knight, 1998; Bol173 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173–176, c Prague, June 2007. 2007 Association for Computational Linguistics shakov and Gelbukh, 2004). In addition, a lot of methods have been proposed to automatically construct thesauri of synonyms. For example, Lin (1998) clustered words with similar meanings by calculating the dependency similarity. Barzilay and McKeown (2001) extracted paraphrases using multiple translations of literature works. Wu and Zhou (2003) extracted synonyms with multiple resources, including a monolingual diction"
S07-1036,P98-2127,0,0.0457621,"Missing"
S07-1036,C98-1112,0,\N,Missing
S07-1036,W03-1610,0,\N,Missing
S07-1036,W06-2907,0,\N,Missing
S07-1036,N06-1058,0,\N,Missing
S07-1036,P06-1057,0,\N,Missing
S07-1036,C98-2122,0,\N,Missing
S10-1091,P05-1074,0,0.0367893,"e can be translated to columna, columna vertebral, pilar and convicciones etc. in Spanish, and these words also have other relevant translations in English, such as vertebral column, column, pillar and convictions etc., which are semantically related to the target word backbone. We use a statistical machine translation system to calculate the translation probability from English to another language (called as pivot language) as well as the translation probability from that language to English. By multiplying these two probabilities, we get a paraphrase probability. This method was defined in (Bannard and Callison-Burch, 2005). In our system, we choose the top k paraphrases rs(w, nj ) = ∑ p(f |w)p(nj |f ), (4) f where f is the pivot language word. We use the English-Spanish parallel text from Europarl (Koehn, 2005). We choose Spanish as the pivot language because in the both directions the BLEU score of the translation between English and Spanish is relatively higher than other English and other languages (Koehn, 2005). 4 Data set and System Settings The organizers of the SemEval-2 specific domain WSD task provide no training data but raw background data in the environmental domain. The English background data is o"
S10-1091,2005.mtsummit-papers.11,0,0.0436287,"Missing"
S10-1091,H05-1053,0,0.0496578,"Missing"
S10-1091,J07-4005,0,0.106253,"ems when the training data is inadequate. In past evaluations, MFS from WordNet performed even better than most of the unsupervised systems (Snyder and Palmer, 2004; Navigli et al., 2007). MFS is usually obtained from a large scale sense tagged corpus, such as SemCor (Miller et al., 1994). However, some polysemous words have different MFS in different domains. For example, in the Koeling et al. (2005) corpus, target word coach means “manager” mostly in the S PORTS domain but means “bus” mostly in the F INANCE domain. So when the MFS is applied to specific domains, it needs to be re-estimated. McCarthy et al. (2007) proposed an unsupervised predominant word sense acquisition method which obtains domain specific MFS without sense tagged corpus. In their method, a thesaurus, in which words are connected with their distributional similarity, is constructed from the domain raw text. Word senses are ranked by their prevalence score which is calculated using the thesaurus and the sense inventory. In this paper, we propose another way to construct the thesaurus. We use statistical machine Figure 1: The architecture of HIT-CIR translation (SMT) techniques to extract paraphrase pairs from bilingual parallel text."
S10-1091,S07-1006,0,0.0286945,"Missing"
S10-1091,P07-1096,0,0.0354263,"Missing"
S10-1091,W04-0811,0,0.0259013,"Missing"
S10-1091,S10-1013,0,\N,Missing
S10-1091,H93-1061,0,\N,Missing
S12-1050,W06-2920,0,0.0898414,"the three sections. Data Set CTB files 1-10; 36-65;81-121; Training 1001-1078; 1100-1119; 1126-1140 Devel 66-80; 1120-1125 Test 11-35; 1141-1151 Total 1-121; 1001-1078 1100-1151 # sent. # words. 8301 250311 534 15329 1233 34311 10068 299951 Table 3: Statistics of training, development and test data. 3.2 Data Format The data format is identical to that of a syntactic dependency parsing shared task. All the sentences are in one text file, with each sentence separated by a blank line. Each sentence consists of one or more tokens, and each token is represented on one line consisting of 10 fields. Buchholz and Marsi (2006) provide more detailed information on the format. Fields are separated from each other by a tab. Only five of the 10 fields are used: token id, form, pos tagger, head, and deprel. Head denotes the semantic dependency of each word, and deprel denotes the corresponding semantic relations of the dependency. In the data, the lemma column is filled with the form and the cpostag column with the postag. Figure 2 shows an example. 3.3 Evaluation Method LAS, which is a method widely used in syntactic dependency parsing, is used to evaluate the performance of the semantic dependency parsing system. LAS"
S12-1050,P10-1110,0,0.0245126,"vel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the secondary metric. Table 4 shows the results for these two indicators in all participating systems. As shown in Table 4, the Zhou Qiaoli-3 system achieved the best results with LAS of 61.84. The LAS val"
S12-1050,P10-1001,0,0.0410518,"semicolon should be used to split the sentence. Second, the last character in a Chinese word is extracted as the lemma, since it usually contains the main sense or semantic class. Third, the multilevel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the se"
S12-1050,W03-1712,0,0.174219,"lization, easy comprehension, high efficiency, and so on. Dependency parsing has been studied intensively in recent decades, with most related work focusing on syntactic structure. Many research papers on Chinese linguistics demonstrate the remarkable difference between semantics and syntax (Jin, 2001; Zhou and Zhang, 2003). Chinese is a meaning-combined language with very flexible syntax, and semantics are more stable than syntax. The word is the basic unit of semantics, and the structure and meaning of a sentence consists mainly of a series of semantic dependencies between individual words (Li et al., 2003). Thus, a reasonable endeavor is to exploit dependency parsing for semantic analysis of Chinese languages. Figure 1 shows an example of Chinese semantic dependency parsing. 378 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 378–384, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics root d-genetive d-restrictive content d-restrictive d-genetive 国际 货币 International Monetary 基金 Fund d-restrictive prep-depend agent 组织 调低 organization turn down 对 for 全球 global d-domain 经济 economy aux-depend 增长 increasing 的 of 预测 prediction Figure 1: An"
S12-1050,D11-1109,1,0.841365,"h dependency. The system-combining strategy involves three steps: 1. Zhou Qiaoli-1, Zhou Qiaoli-2, Zhou Qiaoli-3 These three systems propose a divide-andconquer strategy for semantic dependency parsing. The Semantic Role (SR) phrases are identified (Cai et al., 2011) and then replaced by their head or the SR of the head. The original sentence is thus divided into two types of parts that can be parsed separately. The first type is SR phrase parsing, and the second involves the replacement of SR phrases with either their head or the SR of the head. Finally, the paper takes a graph-based parser (Li et al., 2011) as the semantic dependency parser for all parts. These three systems differ in their phrase identification strategies. 2. NJU-Parser-1, NJU-Parser-2 The NJU-Parser is based on the state-of-theart MSTParser (McDonald, 2006). NJU-Parser applies three methods to enhance semantic dependency parsing. First, sentences are split into sub-sentences using commas and semicolons: (a) sentences are split using only commas and semicolons, as in the primary system, and (b) classifiers are used to determine whether a comma or semicolon should be used to split the sentence. Second, the last character in a Ch"
S12-1050,P05-1013,0,0.0275768,"usually contains the main sense or semantic class. Third, the multilevel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the secondary metric. Table 4 shows the results for these two indicators in all participating systems. As shown in Table 4, the Zhou Qia"
S12-1050,J08-4003,0,0.0232911,"n sense or semantic class. Third, the multilevel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the secondary metric. Table 4 shows the results for these two indicators in all participating systems. As shown in Table 4, the Zhou Qiaoli-3 system a"
S12-1050,W03-1707,0,0.242389,"Missing"
S12-1050,J03-4003,0,\N,Missing
S14-2033,S13-2053,0,0.595629,"utomatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants o"
S14-2033,P14-2009,1,0.80825,"afted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creat"
S14-2033,S13-2052,0,0.118118,"Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants owning larger training data than us. The performance of only using SSWE as features is comparable to th"
S14-2033,W02-1011,0,0.0200027,"nction 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 http://alt.qcri.org/semeval20"
S14-2033,P11-2008,0,0.291901,"Missing"
S14-2033,P14-1146,1,0.727439,"uru Wei‡ , Bing Qin† , Ting Liu† , Ming Zhou‡ Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment c"
S14-2033,P11-1016,1,0.797284,"res with the state-of-the-art hand-crafted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the orga"
S14-2033,H05-1044,0,0.204982,"Missing"
S14-2033,P14-1062,0,0.00858127,"omputing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To le"
S16-1167,W06-2920,0,0.0405957,"XT #sent #word #sent #word Train 8,301 250,249 10,817 128,095 Dev 534 15,325 1,546 18,257 Test 1,233 34,305 3,096 36,097 Table 2: Statics of the corpus. 3.2 • Labeled precision (LP), recall (LR), F1 (LR) and recall for non-local dependencies (NLR); Data Format All data provided for the task uses a columnbased file format, similar to the one of the 2006 CoNLL Shared Task (Table 3). Each training/developing/testing set is a text file, containing sentences separated by a blank line. Each sentence consists of more than one tokens, and each token is represented on one line consisting of 10 fields. Buchholz and Marsi (2006) provide more detailed information on the format. It’s worth noting that if one word has more than one heads, it will appear in more than one lines in the training/developing/testing files continuously. Fields are separated from each other by a tab. Only five of the 10 fields are used: token id, form, pos tagger, head, and deprel. Head denotes the semantic dependency of each word, and deprel denotes the corresponding semantic relations of the dependency. In the data, the lemma column is filled with the form and the cpostag column with the postag. 3.3 files containing sentences in two domains w"
S16-1167,S12-1050,1,0.655272,"aphs that can capture richer latent semantics, and the goal of this task is to identify such semantic structures from a corpus of Chinese sentences. We provide two distinguished corpora in the NEWS domain with 10,068 sentences and the TEXTBOOKS domain with 14,793 sentences respectively. We will first introduce the motivation for this task, and then present the task in detail including data preparation, data format, task evaluation and so on. At last, we briefly describe the submitted systems and analyze these results. 1 Yu Ding† Introduction This task is a rerun of the task 5 at SemEval 2012 (Che et al., 2012), named Chinese semantic dependency parsing (SDP). In the previous task, we aimed at investigating “deep” semantic relations within sentences through tree-structured dependencies. As traditionally defined, syntactic dependency parsing results are connected trees defined over all words of a sentence and language-specific grammatical functions. On the contrary, in semantic dependency parsing, each head-dependent arc instead bears a semantic relation, rather than grammatical relation. In this way, semantic dependency parsing results can be used to answer questions directly, like who did what to w"
S16-1167,S14-2008,0,0.0393196,"head-dependent arc instead bears a semantic relation, rather than grammatical relation. In this way, semantic dependency parsing results can be used to answer questions directly, like who did what to whom when and where. Figure 1 illustrates an example of semantic dependency graph. Here, “她 (she)” is the argument of “脸色 (face)” and at the same time it is an argument of “病 (disease)”. Researchers in dependency parsing community realized dependency parsing restricted in a tree structure is still too shallow, so they explored semantic information beyond tree structure in task 8 at SemEval 2014 (Oepen et al., 2014) and task 18 at SemEval 2015 (Oepen et al., 2015). They provided data in similar structure with what we are going to provide, but in distinct semantic representation systems. Once again we propose this task to promote research that will lead to deeper understanding of Chinese sentences, and we believe that freely available and well annotated corpora which can be used as common testbed is necessary to promote research in data-driven statistical dependency parsing. 1074 Proceedings of SemEval-2016, pages 1074–1080, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Lin"
S16-1167,S15-2153,0,0.0880504,"tion, rather than grammatical relation. In this way, semantic dependency parsing results can be used to answer questions directly, like who did what to whom when and where. Figure 1 illustrates an example of semantic dependency graph. Here, “她 (she)” is the argument of “脸色 (face)” and at the same time it is an argument of “病 (disease)”. Researchers in dependency parsing community realized dependency parsing restricted in a tree structure is still too shallow, so they explored semantic information beyond tree structure in task 8 at SemEval 2014 (Oepen et al., 2014) and task 18 at SemEval 2015 (Oepen et al., 2015). They provided data in similar structure with what we are going to provide, but in distinct semantic representation systems. Once again we propose this task to promote research that will lead to deeper understanding of Chinese sentences, and we believe that freely available and well annotated corpora which can be used as common testbed is necessary to promote research in data-driven statistical dependency parsing. 1074 Proceedings of SemEval-2016, pages 1074–1080, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Exp ROOT time Poss ROOT 现在 now 她 she 脸色"
S16-1167,P14-1042,0,0.0275411,"ins will be released separately. The final rankings will refer to the average results of the two testing files (taking training data size into consideration). We compare predicted dependencies (predicate-roleargument triples, and some of them contain roots of the whole sentences) with our human-annotated ones, which are regarded as gold dependencies. Our evaluate measures are on two granularity, dependency arc and the complete sentence. Labeled and unlabeled precision and recall with respect to predicted dependencies will be used as evaluation measures. Since non-local dependencies (following Sun et al. (2014), we call these dependency arcs making dependency trees collapsed non-local ones) discovery is extremely difficult, we will evaluate non-local dependencies separately. For sentences level, we will use labeled and unlabeled exact match to measure sentence parsing accuracy. Following Task 8 at SemEval 2014, below and in other taskrelated contexts, we abbreviate these metrics as: Evaluation During the phase of evaluation, each system should propose parsing results on the previously unseen testing data. Similar with training phase, testing 1078 • Unlabeled precision (UP), recall (UR), F1 (UF) and"
S16-1167,W03-1707,0,0.0686364,"he closed track encourages participates to focus on building dependency 1077 3.1 Corpus Statistics Since texts in rich categories have different linguistic properties with different communication purpose. This task provides two distinguished corpora in appreciable quantity respectively in the domain of NEWS and TEXTBOOKS (from primary school textbooks). Each corpus contains particular linguistic phenomena. We provide 10,068 sentences of NEWS and 14,793 sentence of TEXTBOOKS. The sentences of news keep the same with the data in task 5 at SemEval 2012, which come from the Chinese PropBank 6.01 (Xue and Palmer, 2003) as the raw corpus to create the Chinese semantic dependency corpus. Sentences were selected by index: 1-121, 1001-1078, 1100-1151. TEXTBOOKS refer to shorter sentences with various ways of expressions, i.e., colloquial sentences (3,000), primary school texts (11,793). Detailed statics are described in Table 2. NEWS TEXT #sent #word #sent #word Train 8,301 250,249 10,817 128,095 Dev 534 15,325 1,546 18,257 Test 1,233 34,305 3,096 36,097 Table 2: Statics of the corpus. 3.2 • Labeled precision (LP), recall (LR), F1 (LR) and recall for non-local dependencies (NLR); Data Format All data provided f"
S17-2049,D14-1181,0,0.00922578,"d n are the length of keywords of S and T. Second, for judging the similarity between two sentences, we check whether each keyword in one sentence can be covered by the other sentence. For a sentence pair S and T, we first calculate a similarity matrix A(m×n) , where each element a(i,j) ∈ A(m×n) computes cosine similarity between words si and tj as a(i,j) = sTi tj ||si ||· ||tj || ∀si ∈ S, ∀tj ∈ T + tˆj = αsi sˆi − = (1 − α)si j=k−w fmatch (si , T ) = Pk+w ai,j tj j=k−w where ai,j (7) Finally, due to the dissimilar and similar components have strong connections, we use a twochannel CNN model (Kim, 2014) to compose them together. In the CNN model, we have three layers. The first is a convolution layer. We define a list of filters wo . The shape of each filter is d h, where d is the dimension of word vectors and h is the window size. Each filter is applied to two patches (a window size h of vectors) from both similar and dissimilar channels, and generates a feature. Eq.(8) expresses this process: (5) + − co,i = f (wo ∗ S[i:i+h] + wo ∗ S[i:i+h] + bo ) (8) We calculate a semantic matching vector sˆi for each word si by composing part of word vectors in the other sentence T. In this way, we can m"
S17-2049,S17-2003,0,0.0415334,"Missing"
S17-2049,D14-1162,0,0.0924751,"o based on its semantic matching vector sˆi , our model further decomposes word si into two components: similar component sˆi + and dissimilar component sˆi − . Then we choose a linear decomposition method. The motivation for the linear decomposition is that the more similar between si and sˆi , the higher proportion of si should be assigned to the similar component. First, we calculate the cosine similarity between si and sˆi . Then, we decompose si linearly based on α. Eq.(7) gives the corresponding definition: a(i,j) = First, with word embedding pre-trained by Stanford using GloVe’s model (Pennington et al., 2014), we transform keywords of question S and T into matrix S = [s1 , s2 , ..., sm ] and T = [t1 , t2 , ..., tn ], where si and tj are 300-dimention vectors of corresponding keywords, and m and n are the length of keywords of S and T. Second, for judging the similarity between two sentences, we check whether each keyword in one sentence can be covered by the other sentence. For a sentence pair S and T, we first calculate a similarity matrix A(m×n) , where each element a(i,j) ∈ A(m×n) computes cosine similarity between words si and tj as a(i,j) = sTi tj ||si ||· ||tj || ∀si ∈ S, ∀tj ∈ T + tˆj = αsi"
S17-2049,C16-1127,0,0.0218118,"of vertices, E is the set of edges, and C(vi ) is the set of vertices connected to the vertex vi . The score of the vertex vi is calculated from the Equation(4), where weight(wi , wj ) is calculated from the Equation(3), d is the damping coefficient. ws(vi ) = (1−d)+d∗ X vj ∈C(vi ) weight(vi , vj ) vk ∈C(vj ) weight(vj , vk ) P (4) Then we select the t words with the highest score as the keywords. 2.2 CNN model based on similar and dissimilar information We use a CNN model based on similar parts and dissimilar parts between two sentences to get sentence similarity. This model is proposed by (Wang et al., 2016), now we will introduce the model briefly. Figure 2 shows the structure of the model. Given a sentence pair, the model represents each keyword as a vector, and calculates a semantic matching vector for each keyword based on part of keywords in the other sentence. Then each word vector is decomposed into two components based on the semantic matching vector: a similar component and a dissimilar component. After this, we use a two-channel CNN to compose the similar and dissimilar components into a feature vector. Finally, a fully connected neural network is used to predict the sentence similarity"
S17-2049,C12-1190,1,0.765082,"gous to the tree structure. If we remove its root node, and ignore the arc of the point, we can get an undirected dependency diagram G0 = (V 0 , E 0 ), V 0 = w1 , w2 , ..., wn , E 0 = e1 , e2 , ..., em , where wi denotes a word and ej denotes an undirected relationship between two words. The undirected dependency graph guarantees that there is a dependency path between any two words in the question, and the length of the dependency path reflects the intensity of the dependency relationship. Therefore, we introduce the concept of dependency degree according to the length of the dependent path (Zhang et al., 2012), as shown in Equation(1), where dr path len(wi , wj ) represents the dependency path length between words wi and wj , b is the superparameter. Dep(wi , wj ) = 1 bdr path len(wi ,wj ) f req(wi ) ∗ f req(wj ) d2 (3) Graph ranking: We use the weighted TextRank algorithm to sort the graph. In the undirected graph G = (V, E), V is the set of vertices, E is the set of edges, and C(vi ) is the set of vertices connected to the vertex vi . The score of the vertex vi is calculated from the Equation(4), where weight(wi , wj ) is calculated from the Equation(3), d is the damping coefficient. ws(vi ) = (1"
W03-1206,C00-1043,0,0.0155195,"mation piece needed to fulfill the statement. Recent automated systems for answering factual questions deduct this expected answer type from the form of the question and a finite list of possible answer types. For example, “Who was the first man in space” expects a “person” as the answer, while “How long was the Titanic?” expects some length measure as an answer, probably in yards and feet, or meters. This is generally a very good strategy, that has been exploited successfully in a number of automated QA systems that appeared in recent years, especially in the context of TREC QA1 evaluations (Harabagiu et al., 2000; Hovy et al., 2000; Prager at al., 2001). This process is not easily applied to analytical questions. This is because the type of an answer for analytical questions cannot always be anticipated due to their inherently exploratory character. In contrast to a factual question, an analytical question has an unlimited variety of syntactic forms with only a loose connection between their syntax and the expected answer. Given the unlimited potential of the formation of analytical questions, it would be counter-productive to restrict them to a limited number of question/answer types. Even finding a"
W03-1206,M98-1007,0,0.0373134,"locating and extracting instances of this attribute in the running text. The extractors are implemented using information extraction utilities which form the kernel of Sheffield’s GATE2 system. We have modified GATE to separate organizations into companies and other organizations, and we have also expanded by adding new concepts such as industries. Therefore, the framing process resembles strongly the template filling task in information extraction (cf. MUC3 evaluations), with one significant exception: while the MUC task was to fill in a template using potentially any amount of source text (Humphreys et al., 1998), the framing is essentially an inverse process. In framing, potentially multiple frames can be associated with a small chunk of text (a passage or a short paragraph). Furthermore, this chunk of text is part of a cluster of very similar text chunks that further reinforce some of the most salient features of these texts. This makes the frame filling a significantly less error-prone task – our experience has been far more positive than the MUC evaluation results may indicate. This is because, rather than trying to find the most appropriate values for attributes from among many potential candidat"
W03-1206,P02-1048,0,0.0221625,"Missing"
W03-1206,H01-1006,0,0.0530186,"Missing"
W03-1206,W00-0303,0,0.0953278,"Missing"
W03-1206,W00-1501,0,\N,Missing
W03-1206,W02-0209,0,\N,Missing
W03-1206,A00-1005,1,\N,Missing
W04-1108,J96-1002,0,0.0274857,"wsky, 1992). These methods draw the support from the high-powered computers, get the statistics of large real-world corpus, find and acquire knowledge of linguistics automatically. They deal with all change by invariability, thus it is easy to trace the evaluation and development of natural language. So the statistic methods of NLP has attracted the attention of professional researchers and become the mainstream bit by bit. Corpus-based Statistical approaches are Decision Tree (Pedersen, 2001), Decision List, Genetic Algorithm, Naive-Bayesian Classifier (Escudero, 2000)、Maximum Entropy Model (Adam, 1996; Li, 1999), and so on. Corpus-based statistical approaches can be divided into supervised and unsupervised according to whether training corpus is sense-labeled text. Supervised learning methods have the good learning ability and can get better accuracy in WSD experiments (Schütze, 1998). Obviously the data sparseness problem is a bottleneck for supervised learning algorithm. If you want to get better learning and disambiguating effect, you can enlarge the size and smooth the data of training corpus. According to practical demand, it would spend much more time and manpower to enlarge the size"
W04-1108,J98-1004,0,0.0314309,"atural language. So the statistic methods of NLP has attracted the attention of professional researchers and become the mainstream bit by bit. Corpus-based Statistical approaches are Decision Tree (Pedersen, 2001), Decision List, Genetic Algorithm, Naive-Bayesian Classifier (Escudero, 2000)、Maximum Entropy Model (Adam, 1996; Li, 1999), and so on. Corpus-based statistical approaches can be divided into supervised and unsupervised according to whether training corpus is sense-labeled text. Supervised learning methods have the good learning ability and can get better accuracy in WSD experiments (Schütze, 1998). Obviously the data sparseness problem is a bottleneck for supervised learning algorithm. If you want to get better learning and disambiguating effect, you can enlarge the size and smooth the data of training corpus. According to practical demand, it would spend much more time and manpower to enlarge the size of training corpus. Smoothing data is merely a subsidiary measure. The sufficient large size of training corpus is still the foundation to get a satisfied effect in WSD experiment. Unsupervised WSD never depend on tagged corpus and could realize the training of large real corpus coming f"
W04-1120,C94-1032,0,0.0235379,"ayer components; on the other hand the incorrect analysis of lower layers must reduce the accuracy of higher layers. In Chinese Word-Seg component, many segmentation ambiguities which cannot be solved using only lexical information. In order to improve the performance of Word-Seg, we have to use some syntax and even semantic information. Without correct Word-Seg results, however the syntax and semantic parser cannot obtain a correct analysis. It is a chain debts problem. People have tried to solve the error-multiplied problem by integrating multi-layers into a uniform model (Gao et al., 2001; Nagata, 1994). But with the increasing number of integrated layers, the model becomes too complex to build or solve. The feedback mechanism (Wu and Jiang, 1998) helps to use the information of high layers to control the final result. If the analysis at feedback point cannot be passed, the whole analysis will be denied. This mechanism places too much burden on the function of feedback point. This leads to the problems that a correct lower layer result may be rejected or an error result may be accepted. We propose a new Multilayer Search Mechanism (MSM) to solve the problems mentioned above. Based on the mec"
W04-1121,P94-1012,0,0.501998,"Missing"
W04-1121,P93-1001,0,0.0925226,"Missing"
W04-1121,J93-1004,0,\N,Missing
W04-1121,P91-1022,0,\N,Missing
W04-1121,P96-1018,0,\N,Missing
W04-1121,J93-1006,0,\N,Missing
W04-1121,P93-1002,0,\N,Missing
W04-2507,M98-1007,0,0.13033,"Missing"
W05-0627,J96-1002,0,0.0317627,"ntic interpretation is needed, such as question and answering, information extraction, machine translation, paraphrasing, and so on. ∗ This research was supported by National Natural Science Foundation of China via grant 60435020 Last year, CoNLL-2004 hold a semantic role labeling shared task (Carreras and M`arquez, 2004) to test the participant systems’ performance based on shallow syntactic parser results. In 2005, SRL shared task is continued (Carreras and M`arquez, 2005), because it is a complex task and now it is far from desired performance. In our SRL system, we select maximum entropy (Berger et al., 1996) as a classifier to implement the semantic role labeling system. Different from the best classifier reported in literatures (Pradhan et al., 2005) – support vector machines (SVMs) (Vapnik, 1995), it is much easier for maximum entropy classifier to handle the multi-class classification problem without additional post-processing steps. The classifier is much faster than training SVMs classifiers. In addition, maximum entropy classifier can be tuned to minimize over-fitting by adjusting gaussian prior. Xue and Palmer (2004; 2005) and Kwon et al. (2004) have applied the maximum entropy classifier"
W05-0627,W04-2412,0,0.0947866,"Missing"
W05-0627,W05-0620,0,0.112708,"Missing"
W05-0627,C04-1179,0,0.0311086,"Missing"
W05-0627,W04-3212,0,0.147832,"Missing"
W06-2931,I05-2044,0,0.0122772,"A classifier is often used to choose the most probable action to assemble the dependency tree. (Yamada and Matsumoto, 2003) defined three actions and used a SVM classifier to choose one of them in a bottom-up way. The algorithm in (Nivre et al., 2004) is a blend of bottom-up and top-down processing. Its classifier is trained by memory-based learning. Deterministic parsing derives an analysis without redundancy or backtracking, and linear time can be achieved. But when searching the local optimum in the order of left-to-right, some wrong reduce may prevent next analysis with more possibility. (Jin et al., 2005) used a two-phase shift-reduce to decrease such errors, and improved the accuracy of long distance dependencies. In this paper a deterministic parsing based on dynamic local optimization is proposed. According to the probabilities of dependency arcs, the algorithm dynamically finds the one with the highest probabilities instead of dealing with the sentence in order. A procedure of constraint which can integrate more structure information is made to check the rationality of the reduce. Finally our results and error analysis are presented. 2 Dependency Probabilities An example of Chinese depende"
W06-2931,W04-2407,0,0.0657967,"lity because their GDs are stable and helpful to syntactic analysis. Other FTags with small probabilities are unstable in GDs and cannot provide efficient information for syntactic analysis. If their probabilities are less than 0.65 they will be ignored in our dependency parsing. 3 Dynamic local optimization Many previous methods are based on history-based models. Despite many obvious advantages, these methods can be awkward to encode some constrains within their framework (Collins, 2000). Classifiers are good at encoding more features in the deterministic parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004). However, such algorithm often make more probable dependencies be prevented by preceding errors. An example is showed in Figure 2. Arc a is a frequent dependency and b is an arc with more probability. Arc b will be prevented by a if the reduce is carried out in order. Figure 2: A common error in deterministic parsing 3.1 Our algorithm Our deterministic parsing is based on dynamic local optimization. The algorithm calculates the arc probabilities of two continuous nodes, and then reduces the most probable arc. The construction of dependency tree includes four actions: Check, Reduce, Delete, an"
W06-2931,C96-1058,0,0.0935554,"been achieved in some dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004). With the availability of many dependency treebanks (van der Beek et al., 2002; Hajiˇc et al., 2004; B¨ohmov´a et al., 2003; Kromann, 2003; Dˇzeroski et al., 2006) and more other treebanks which can be converted to dependency annotation (Brants et al., 2002; Nilsson et al., 2005; Chen et al., 2003; Kawata and Bartels, 2000), multi-lingual dependency parsing is proposed in CoNLL shared task (Buchholz et al., 2006). Many previous works focus on unlabeled parsing, in which exhaustive methods are often used (Eisner, 1996). Their global searching performs well in the unlabeled dependency parsing. But with the increase of parameters, efficiency has to be considered in labeled dependency parsing. Thus deterministic parsing was proposed as a robust and efficient method in recent years. Such method breaks the construction of dependency tree into a series of actions. A classifier is often used to choose the most probable action to assemble the dependency tree. (Yamada and Matsumoto, 2003) defined three actions and used a SVM classifier to choose one of them in a bottom-up way. The algorithm in (Nivre et al., 2004) i"
W06-2931,W03-3023,0,\N,Missing
W06-2931,W06-2920,0,\N,Missing
W06-2931,dzeroski-etal-2006-towards,0,\N,Missing
W06-2931,W03-2405,0,\N,Missing
W06-2931,afonso-etal-2002-floresta,0,\N,Missing
W08-2134,W05-0627,1,0.833091,"o the predicate (negative for being left to the predicate and positive for right), another feature is formed, namely “Bag of POS (Numbered)”. WIND5 BIGRAM (b3): 5 closest words from both left and right plus the predicate itself, in total 11 words form a “window”, within which bigrams are enumerated. The final optimized feature set for the task of predicate classification is (a1, a21, a23, a71, a72, a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3, a71+a9). 3.4 Semantic Role Classification In our system, the identification and classification of semantic roles are achieved in a single stage (Liu et al., 2005) through one single classifier (actually two, one for noun predicates, and the other for verb predicates). Each word in a sentence is given probabilities to be each semantic role (including none of the these roles) for a predicate. Features introduced in addition to those of the previous subsections are the following: POS PATH (c11), REL PATH (c12): The “POS Path” feature consists of POS tags of the words along the path from a word to the predicate. Other than “Up” and “Down”, the “Left” and “Right” direction of the path is added. Similarly, the “Relation Path” feature consists of the relation"
W08-2134,C04-1197,0,0.0272563,"ost Inference. During the Predicate Identification stage we examine each word in a sentence to discover target predicates, including both noun predicates (from NomBank) and verb predicates (from PropBank). In the Predicate Classification stage, each predicate is assigned a certain sense number. For each predicate, the probabilities of a word in the sentence to be each semantic role are predicted in the Semantic Role Classification stage. Maximum entropy model is selected as our classifiers in these stages. Finally an ILP (Integer Linear Programming) based method is adopted for post inference (Punyakanok et al., 2004). 3.2 Predicate Identification The predicate identification is treated as a binary classification problem. Each word in a sentence is predicted to be a predicate or not to be. A set of features are extracted for each word, and an optimized subset of them are adopted in our final system. The following is a full list of the features: DEPREL (a1): Type of relation to the parent. WORD (a21), POS (a22), LEMMA (a23), HEAD (a31), HEAD POS (a32), HEAD LEMMA (a33): The forms, POS tags and lemmas of a word and it’s headword (parent) . FIRST WORD (a41), FIRST POS (a42), FIRST LEMMA (a43), LAST WORD (a51)"
W08-2134,W08-2121,0,0.0848569,"Missing"
W09-1207,W09-1201,0,0.118581,"Missing"
W09-1207,D08-1008,0,0.0936805,"Missing"
W09-1207,kawahara-etal-2002-construction,0,0.0239566,"ntropy classifier is implemented with Maximum Entropy Modeling Toolkit1 . The classifier parameters are tuned with the development data for different languages respectively. lp solve 5.52 is chosen as our ILP problem solver. 1 2 http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html http://sourceforge.net/projects/lpsolve 5 Experiments 5.1 Experimental Setup We participate in the CoNLL 2009 shared task with all 7 languages: Catalan (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002), and Spanish (Taul´e et al., 2008). Besides the closed challenge, we also submitted the open challenge results. Our open challenge strategy is very simple. We add the SRL development data of each language into their training data. The purpose is to examine the effect of the additional data, especially for out-of-domain (ood) data. Three machines (with 2.5GHz Xeon CPU and 16G memory) were used to train our models. During the peak time, Amazon’s EC2 (Elastic Compute Cloud)3 was used, too. Our system requires 15G memory at most and the longest training time is about 36 hours. During training the"
W09-1207,W02-1006,0,0.0162974,"dicate Classification The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Support Vector Machine (SVM) classifier is used to classify its sense. As reported by Lee and Ng (2002) and Guo et al. (2007), SVM shows good performance on the WSD task. Here libsvm (Chang and Lin, 2001) is used. The linear kernel function is used and the trade off parameter C is 1. 4. Post processing: for each predicate in the test data which does not appear in the training data, its first sense in the frame files is used. 4 Semantic Role Labeling The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a"
W09-1207,P05-1013,0,0.0387666,"role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (McDonald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 49 For each arc, we firstly use unigram features to choose the K1 -best labels. The second parameter of f1lbl (·) indicates whether the node is the head of the arc, and the third parameter indicates the direction. L denotes the whole label set. Then we re-rank the labels by combining the bigram features, and choose K2 -best labels. During decoding, we only use the K2 labels chosen for each arc (K2 ¿ K1 < |L|). 2.2 High-order Model and Algorithm Following the Eisner (2000) algorithm, we use spans as the basic unit. A s"
W09-1207,C04-1197,0,0.029135,"tage anymore. For a predicate of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 4. The probability of each word to be a semantic role for a predicate is given by the SRC stage. The results generated by selecting the roles with the largest probabilities, however, do not satisfy some constrains. As we did in the last year’s system (Che et al., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al., 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label “NULL”). C2: Roles with a small probability should never be labeled (except for the virtual role “NULL”). The threshold we use in our system is 0.3. C3: Statistics show that some roles (except for the virtual role “NULL”) usually appear once for a predicate. We impose a no-duplicate-roles constraint with a no-duplicate-roles list, which is constructed according to the times of semantic roles’ duplication for each single predicate. T"
W09-1207,W08-2121,0,0.165854,"Missing"
W09-1207,taule-etal-2008-ancora,0,0.0539562,"Missing"
W09-1207,burchardt-etal-2006-salsa,0,\N,Missing
W09-1207,S07-1034,1,\N,Missing
W09-1207,J96-1002,0,\N,Missing
W09-1207,W08-2134,1,\N,Missing
W09-1207,D07-1101,0,\N,Missing
W12-6316,C10-3004,1,0.878455,"Missing"
W12-6316,D11-1090,0,0.0789906,"ow. Information of unlabeled data can be easily computed and benefit the word segmentation model. When integrated into machine learning framework, it will help reduce sparsity issue caused by the out of vocabulary words. 2.3.1 Mutual Information In probability theory, mutual information measures the mutual dependency of two random variables. Empirical study shows that observation of high mutual information between two characters may indicates real association of these two characters in a word, while low mutual information usually means they belongs to different words. In this paper, we follow Sun and Xu (2011)’s definition of mutual information. For a character bigram ci ci+1 , their mutual information is computed as follow: • character unigram: cs (i − 2 ≤ s ≤ i + 2) • character bigram: cs cs+1 (i − 2 ≤ s ≤ i + 1), cs cs+2 (i − 2 ≤ s ≤ i) • character trigram: cs−1 cs cs+1 (s = i) • repetition of characters: is cs equals cs+1 (i− 1 ≤ s ≤ i), is cs equals cs+2 (i − 2 ≤ s ≤ i) • character type: is ci an alphabet, digit, punctuation or others M I(ci ci+1 ) = log 2.2 Rule Detection Features p(ci ci+1 ) p(ci )p(ci+1 ) For each character ci , M I(ci ci+1 ) and M I(ci−1 ci ) are computed and rounded down"
W12-6316,I11-1035,0,0.0608607,"Missing"
W12-6330,N06-2033,0,0.0369784,"s been investigated by many researchers. Most methods of PSG parsing exploited some manly annotated corpus and proposed a single statistical model (Petrov and Klein, 2007; Zhang and Clark, 2009) based on the corpus. For Chinese, Tsinghua Chinese Treebank (TCT) (Qiang, 2004) and Penn Chinese TreeBank (CTB) (Xue et al., 2005) are two most popular manly annotated corpus. In this paper, we are especially interested in parser combination. Many past works have suggest a number of methods for parser combination. These methods concern on combing different parsers which are trained on the same corpus. Sagae and Lavie (2006) proposed a constituent reparsing method for multiple parsers combina2. The grammars of TCT corpus are very different that of CTB corpus. We should transform CTB grammars into TCT grammars before final combination. If these two issues have been done already, we can apply CKY reparsing algorithm and get the final parsing result. The rest of the paper is organized as follows. Section 2 introduces the overall system architecture. And then we introduce our method in detail. In section 3 we present the binarization algorithm used in the system. Section 4 describes the CKY reparsing algorithm. Secti"
W12-6330,D08-1018,0,0.0588326,"Missing"
W12-6330,W09-3825,0,0.0614032,"Missing"
W12-6330,D09-1161,0,0.0291345,"Missing"
W12-6330,P06-1055,0,0.0839699,"Missing"
W12-6330,N07-1051,0,\N,Missing
W12-6330,J03-4003,0,\N,Missing
W13-0909,W10-0303,0,0.588845,"Missing"
W13-0909,J91-1003,0,0.409834,"aphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method reli"
W13-0909,W06-3506,0,0.850487,"Missing"
W13-0909,P03-1054,0,0.00653924,"g is presented in a separate, future publication. Words that have an imageability rating lower than an experimentally determined threshold are further excluded from consideration. In the exam70 ple shown in Figure 1, words that have sufficiently high imageability scores are “labyrinthine”, “port”, “rail” and “airline”. We shall consider them as candidate relations, to be further investigated, as explained in the dependency parsing step described next. 3.3 Relation Extraction Dependency parsing reveals the syntactic structure of the sentence with the Target concept. We use the Stanford parser (Klein and Manning, 2003) for English language data. We identify candidate metaphorical relations to be any verbs that have the Target concept in direct dependency path (other than auxiliary and modal verbs). We exclude verbs of attitude (“think”, “say”, “consider”), since these have been found to be more indicative of metonymy than of metaphor. This list of attitude verbs is automatically derived from WordNet. From the example shown in Figure 1, one of the candidate relations extracted would be the verb “navigate”. In addition, we have a list of candidate relations from Step 3.2, which are the highly imageable nouns"
W13-0909,W07-0103,0,0.160031,"ig, 1995) who have focused on the 68 way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on refugees (1992), see metaphor as a tool to help outsiders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Political Science underpinning (Musolff, 2008; Lakoff, 2001). In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Naraya"
W13-0909,P80-1004,0,0.840031,"ed. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data,"
W13-0909,shutova-teufel-2010-metaphor,0,0.033737,"Net (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities"
W13-0909,C12-2109,0,0.305578,"Missing"
W13-0909,C10-1117,1,\N,Missing
W13-0909,D11-1063,0,\N,Missing
W14-2306,W10-0303,0,0.0169498,"age. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van der Sluis and Mellish 2008). It is expressed through multiple means, many"
W14-2306,P80-1004,0,0.702834,"rishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to"
W14-2306,J91-1003,0,0.690231,"Missing"
W14-2306,W06-3506,0,0.0751735,"Missing"
W14-2306,C04-1200,0,0.115008,"metimes referred to as intensity. Our approach to affect in metaphor has been vetted not only by our core linguistic team but also by an independent team of linguist-analysts with whom we work to understand metaphor across several language-culture groups. Our research continues to show no difficulties in comprehension or disagreement across languages concerning the concept of linguistic affect, of its application to metaphor, and of its having both polarity and intensity. 5 Related Research: sentiment and affect There is a relatively large volume of research on sentiment analysis in language (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; inter alia) that aim at detecting polarity of text, but is not specifically concerned with metaphors. A number of systems were developed to automatically extract writer’s senti1 The First Workshop on Metaphor in NLP. http://aclweb.org/anthology//W/W13/W13-09.pdf 43 ment towards specific products or services such as movies or hotels, from online reviews (e.g., Turney, 2002; Pang and Lee, 2008) or social media messages (e.g., Thelwall et al., 2010). None of these techniques has been applied specifically to metaphorical language, and it is"
W14-2306,P13-1067,0,0.356958,"egative categories. However, the presence of largely negative concepts such as “poverty” in a given sentence overwhelms the sentiment for the sentence in their method. Other relevant efforts in sentence level sentiment analysis include Sem-Eval Task2. While presence of affect in metaphorical language is well documented in linguistic and psycholinguistic literature (e.g., Osgood, 1980; Pavio and Walsh, 1993; Caffi and Janney, 1994; Steen, 1994), relatively little work was done to detect affect automatically. Some notable recent efforts include Zhang and Barnden (2010), Veale and Li (2012), and Kozareva (2013), who proposed various models of metaphor affect classification based primarily on lexical features of the surrounding text: specifically the word polarity information. In these and other similar approaches, which are closely related to sentiment analysis, affect is attributed to the entire text fragment: a sentence or utterance containing a metaphor, or in some cases the immediate textual context around it. In contrast, our objective is to isolate affect due to the metaphor itself, independently of its particular context, and also to determine how various elements of the metaphoric expression"
W14-2306,W07-0103,0,0.0258652,"loran, 2007) that attempt to correlate metaphor semantics with their usage in naturally occurring text but generally lack robust tools to do so; and (3) social science approaches, particularly in psychology and anthropology that seek to explain how people produce and understand metaphors in interaction, but which lack the necessary computational tools to work with anything other than relatively isolated examples. In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & N"
W14-2306,S13-2053,0,0.216251,"m a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O’Halloran, 2007) that attempt to correlate metaphor semantics with their usag"
W14-2306,W13-0904,0,0.0153035,"interest are those that operate between the concepts within a Source domain and can be “borrowed” to link concepts within the Target domain, e.g., “Crime(TARGET) spread to(RELATION) previously safe areas” may be borrowing from a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors a"
W14-2306,shutova-teufel-2010-metaphor,0,0.0617997,"Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van d"
W14-2306,P10-1071,0,0.0174283,"between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van der Sluis and Mellish 2008). It i"
W14-2306,C12-2109,0,0.0359731,"Missing"
W14-2306,S07-1013,0,0.0701286,"Missing"
W14-2306,W13-0909,1,0.73941,"may be borrowing from a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O’Halloran, 2007) that attempt to correlate metaphor semantics with their usag"
W14-2306,P02-1053,0,0.0132888,"Missing"
W14-2306,P12-3002,0,0.0922262,"sentences into positive/negative categories. However, the presence of largely negative concepts such as “poverty” in a given sentence overwhelms the sentiment for the sentence in their method. Other relevant efforts in sentence level sentiment analysis include Sem-Eval Task2. While presence of affect in metaphorical language is well documented in linguistic and psycholinguistic literature (e.g., Osgood, 1980; Pavio and Walsh, 1993; Caffi and Janney, 1994; Steen, 1994), relatively little work was done to detect affect automatically. Some notable recent efforts include Zhang and Barnden (2010), Veale and Li (2012), and Kozareva (2013), who proposed various models of metaphor affect classification based primarily on lexical features of the surrounding text: specifically the word polarity information. In these and other similar approaches, which are closely related to sentiment analysis, affect is attributed to the entire text fragment: a sentence or utterance containing a metaphor, or in some cases the immediate textual context around it. In contrast, our objective is to isolate affect due to the metaphor itself, independently of its particular context, and also to determine how various elements of the"
W14-2306,W13-0905,0,0.020242,"at operate between the concepts within a Source domain and can be “borrowed” to link concepts within the Target domain, e.g., “Crime(TARGET) spread to(RELATION) previously safe areas” may be borrowing from a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language"
W14-4725,P80-1004,0,0.355181,"ed. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data,"
W14-4725,J91-1003,0,0.171713,"Missing"
W14-4725,W06-3506,0,0.0458493,"Missing"
W14-4725,W07-0103,0,0.0290505,"errig, 1995) who have focused on the way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on refugees (1992), see metaphor as a tool to help outsiders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Political Science underpinning (Musolff, 2008; Lakoff, 2001). In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Naraya"
W14-4725,shutova-teufel-2010-metaphor,0,0.0214182,"Net (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantitie"
W14-4725,P10-1071,0,0.014226,"tiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generate"
W14-4725,C12-2109,0,0.0320502,"Missing"
W14-4725,W13-0905,0,0.016931,"initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. More recently, several important approaches to metaphor extraction have emerged from the IARPA Metaphor program, including Broadwell et al (2013), Strzalkowski et al. (2014), Wilks et al (2013), Hovy et al (2013) inter alia. These papers concentrate on the algorithms for detection and classification of individual linguistic metaphors in text rather than formation of conceptual metaphors in a broader cultural context. Taylor et al (2014) outlines the rationale why conceptual level metaphors may provide important insights into cross-cultural contrasts. Our work described here is a first attempt at automatic discovery of conceptual metaphors operating within a culture directly from the linguistic evidence in language. 3 Our Approach The process of discovering conceptual metaphors is ne"
W14-4725,W13-0907,0,\N,Missing
W14-4725,shaikh-etal-2014-multi,1,\N,Missing
W14-4725,W13-0909,1,\N,Missing
W15-1408,W10-0303,0,0.0227642,"pages 67–76, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Understanding conflicts in this manner may allow policy-makers facilitate negotiations and discussions across different communities and help bridge contrasting viewpoints and cultural values. 2 Relevant Research The underlying core of our research is automated, large-scale metaphor extraction. Computational approaches to metaphor to date have yielded only limited scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our"
W15-1408,C12-2016,0,0.0515284,"Missing"
W15-1408,J91-1003,0,0.629185,"Missing"
W15-1408,W13-1105,1,0.941007,"ssessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant work includes Nguyen et al. (2013), who look at nonparametric topic modeling as a measure of influence; and Bracewell et al. (2012), who look at a category of social acts to determine measures of leadership; among others. Analysis of positions held by discourse participants has been studied in the realm of political science and computational sociolinguistics (Laver, Benoit & Garry, 2003; Slapin & Proksch, 2008; Lin et al., 2013; Pang & Lee, 2008) and our approach draws parallels from such prior work. Our topical positioning approach is a departure from existing approaches to sentiment analysis (Wiebe, Wilson and Cardie, 2005; Strapparava and Mihalcea, 2008) in looking at a larger context of discourse rather than individual utterances. 68 3 The Conflict – U.S. Gun Debate The main hypothesis, and an open research question, is then: can this new technology be effectively applied to understanding of a broad cultural conflict such as may arise in any society where potentially divisive issues exist? To answer this questio"
W15-1408,W13-0904,0,0.020107,"ave yielded only limited scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our approach builds upon their work. The use of metaphor, along with sociocultural aspects of language to understand cross-cultural conflict is novel to our approach. Recent research in computational sociolinguistic has developed methods for automatic assessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant"
W15-1408,shutova-teufel-2010-metaphor,0,0.0163255,"e Third Workshop on Metaphor in NLP, pages 67–76, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Understanding conflicts in this manner may allow policy-makers facilitate negotiations and discussions across different communities and help bridge contrasting viewpoints and cultural values. 2 Relevant Research The underlying core of our research is automated, large-scale metaphor extraction. Computational approaches to metaphor to date have yielded only limited scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extract"
W15-1408,C10-1117,1,0.822159,"approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our approach builds upon their work. The use of metaphor, along with sociocultural aspects of language to understand cross-cultural conflict is novel to our approach. Recent research in computational sociolinguistic has developed methods for automatic assessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant work includes Nguyen et al. (2013), who look at nonparametric topic modeling as a measure of influence; and Bracewell et al. (2012), who look at a category of social acts to determine measures of leadership; among others. Analysis of positions held by discourse participants has been studied in the realm of political science and computational sociolinguistics (Laver, Benoit & Garry, 2003; Slapin & Proksch, 2008; Lin et al., 2013; Pang & Lee, 2008) and our approach draws parallels from such prior work. Our topical positioning approach is a depa"
W15-1408,W13-0909,1,0.870627,"the listener (Perloff, 2014). Metaphors, which are mapping systems that allow the semantics of a familiar Source domain to be applied to a Target domain so that new frameworks of reasoning can emerge in the Target domain, are pervasive in discourse. Metaphorically rich language is considered highly influential. Persuasion and influence literature (Soppory and Dillard, 2002) indicates messages containing metaphorical language produce somewhat greater attitude change than messages that do not. Metaphors embody a number of elements of persuasive language, including concreteness and imageability (Strzalkowski et al., 2013, Broadwell et al., 2013, Charteris-Black, 2005). Using this line of investigation, we aim to understand the motivations of a group or of a political faction through their discourse, as part of the answer to such questions as: What are the key differences in protagonists’ positions? How extensive is a protagonists’ influence? Who dominates the discourse? Where is the core of the groups’ support? Our goal is to provide a basis for the analysis of cross-cultural conflicts by viewing the conflict as an ongoing debate or a “dialogue” between protagonists or participants. In this interpretation, ea"
W15-1408,W13-0905,0,0.027816,"ted scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our approach builds upon their work. The use of metaphor, along with sociocultural aspects of language to understand cross-cultural conflict is novel to our approach. Recent research in computational sociolinguistic has developed methods for automatic assessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant work includes Nguyen"
W16-4907,N16-1030,0,0.0275254,"e last word “损害(damage)” shows up. Traditional models with features extracted from a limited context window may not be able to handle these situations. Neural network-based models have been extensively used in natural language processing (NLP) during recent years, due to their strong capability of automatical feature learning. In particular, the long shortterm memory (LSTM) (Hochreiter and Schmidhuber, 1997) based recurrent neural networks (RNN) have been proved to be highly effective in various applications that involves sequence modeling, such as language modeling, named entity recognition (Lample et al., 2016) and parsing (Vinyals et al., 2015), etc. Therefore, in this paper, we propose to use LSTM-based RNNs to solve the CGED problem. In order to leverage both the merits of CRF models and LSTM models, we further present an ensemble model using Stacking (Nivre and McDonald, 2008). Evaluations on the NLP-TEA-3 shared task for CGED show that our models achieve the best F1-scores in all levels and the best recall in two levels. The rest of the paper is organized as follows: Section 2 gives the definition of the CEGD task. Section 3 describes how LSTM network is used to predict errors and what other wo"
W16-4907,C14-2015,0,0.147888,"n traditional Chinese texts rather than simplified Chinese, and one sentence includes one error at most in last two years. There have been several studies focused on Chinese grammatical error detection. Wu et al. (2010) proposed a method using both Relative Position Language Model and Parse Template Language Model to detect Chinese errors written by US learner. Yu and Chen (2012) proposed a classifier to detect wordordering errors in Chinese sentences from the HSK dynamic composition corpus. Lee et al. (2013) proposed linguistic rule based Chinese error detection for second language learning. Lee et al. (2014) developed a sentence judgment system using both rule-based and n-gram statistical methods to detect grammatical errors in Chinese sentences. However, all of these previous works used hand-crafted features which may be incomplete and cause the loss of some important information. Comparatively, our neural network approaches have strong capability of automatical feature learning and are completely data-driven. 6 Conclusion This paper describes our system in the NLP-TEA-3 task for CGED-HSK. We explored the CRF-based model, the LSTM-based model and further used stacking to combine the two models."
W16-4907,W15-4401,0,0.10525,"m presents the best F1 scores in all three levels and also the best recall rates in the last two levels on evaluation dataset. However, the results of this task are not that credible because there are many ways to correct a wrong Chinese sentence. For example, deleting some redundant words may replace errors of missing words. 5 Related Works In NLP-TEA-1 (Yu et al., 2014) shared task for CGED, there were four types of errors, which were the same as the task of this year. The evaluation was only based on detection of error occurrence, disregarding the recognization of boundaries. In NLP-TEA-2 (Lee et al., 2015) shared task for CGED, the participating systems are required to not only detect the errors but also locate them. Evaluations were focused on traditional Chinese texts rather than simplified Chinese, and one sentence includes one error at most in last two years. There have been several studies focused on Chinese grammatical error detection. Wu et al. (2010) proposed a method using both Relative Position Language Model and Parse Template Language Model to detect Chinese errors written by US learner. Yu and Chen (2012) proposed a classifier to detect wordordering errors in Chinese sentences from"
W16-4907,P08-1108,0,0.167278,"o their strong capability of automatical feature learning. In particular, the long shortterm memory (LSTM) (Hochreiter and Schmidhuber, 1997) based recurrent neural networks (RNN) have been proved to be highly effective in various applications that involves sequence modeling, such as language modeling, named entity recognition (Lample et al., 2016) and parsing (Vinyals et al., 2015), etc. Therefore, in this paper, we propose to use LSTM-based RNNs to solve the CGED problem. In order to leverage both the merits of CRF models and LSTM models, we further present an ensemble model using Stacking (Nivre and McDonald, 2008). Evaluations on the NLP-TEA-3 shared task for CGED show that our models achieve the best F1-scores in all levels and the best recall in two levels. The rest of the paper is organized as follows: Section 2 gives the definition of the CEGD task. Section 3 describes how LSTM network is used to predict errors and what other works we have done. Section 4 ∗ Email correspondence. 49 Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications, pages 49–56, Osaka, Japan, December 12 2016. shows the evaluation results. Section 5 gives some related works. Secti"
W16-4907,C12-1184,0,0.199883,"error occurrence, disregarding the recognization of boundaries. In NLP-TEA-2 (Lee et al., 2015) shared task for CGED, the participating systems are required to not only detect the errors but also locate them. Evaluations were focused on traditional Chinese texts rather than simplified Chinese, and one sentence includes one error at most in last two years. There have been several studies focused on Chinese grammatical error detection. Wu et al. (2010) proposed a method using both Relative Position Language Model and Parse Template Language Model to detect Chinese errors written by US learner. Yu and Chen (2012) proposed a classifier to detect wordordering errors in Chinese sentences from the HSK dynamic composition corpus. Lee et al. (2013) proposed linguistic rule based Chinese error detection for second language learning. Lee et al. (2014) developed a sentence judgment system using both rule-based and n-gram statistical methods to detect grammatical errors in Chinese sentences. However, all of these previous works used hand-crafted features which may be incomplete and cause the loss of some important information. Comparatively, our neural network approaches have strong capability of automatical fe"
W18-3707,J90-1003,0,0.23071,"Missing"
W18-3707,W16-4906,0,0.103894,"Missing"
W18-3707,I17-4006,0,0.569554,"rmance. For a target word, we compute ePMI together with neighbor words and map them to discrete value internals as features. Gaussian ePMI. we use trainable weighted Gaussian distribution to leverage words’ distance. (4) Novel Features The task heavily depends on the prior knowledge that can be represented by the selection of features. In practice, feature selection is straightforward phase to affects the model’s performance. Better task-specific features simplify the complexity of a model, whereby improve the performance in all levels. Besides the feature engineering introduced by ALI team (Yang et al., 2017), we design several additional features that will be discussed next. Word Segmentation. we found that sentences in segments are essential to solving the grammatical task due to Chinese’s words being combined without segmented spaces that help to indicate the exact meaning of the sentence without ambiCombination of POS and PMI. our intuition is that the efficiency of PMI-score (Church and Hanks, 1 54 http://www.ltp-cloud.com/ 1990) between words is more relevant to what their POSs (Ferraro et al., 2014) exactly are; PMI-scores for different POS-pairs have different meaning, even though the POS-"
W18-3707,C12-1184,0,0.0221177,"hat are correctly identified; FP (False Positive) is the number of error-sentences that are incorrectly identified as correct sentences; TN (True Negative) is the number of correctsentences that are correctly identified; FN (False Negative) is the number of correct-sentences that are incorrectly identified as containing grammatical errors. The metrics that are used to measure a system’s performance has three levels: detection, identification, and position. Each level is evaluated with the help of the confusion matrix based on these metrics (Lee et al.,2016):      dom field (CRF) network (Yu and Chen, 2012) to form a BiLSTM-CRF model can efficiently use past and future information via a Bi-LSTM layer and connecting consecutive output layers from Bi-LSTM via a CRF layer such that the sequence tagging problems can be solved better. Two kinds of potentials are defined in the BiLSTM-CRF model (Huang et al.,2015): emission and transition potentials. The emission potential P is the matrix of scores output by the BiLSTM network, of size , where k in the size of distinct tags. Specifically, represents the emission score of the word to the tag in an input sequence. The transition potential A is the matri"
W18-3707,W16-4907,1,0.823817,"Missing"
wacholder-etal-2004-designing,W03-1206,1,\N,Missing
webb-etal-2008-cross,W04-2319,0,\N,Missing
webb-etal-2008-cross,J93-3003,0,\N,Missing
webb-etal-2008-cross,N06-1036,0,\N,Missing
webb-etal-2008-cross,J00-3003,0,\N,Missing
webb-etal-2008-cross,J96-2004,0,\N,Missing
webb-etal-2008-cross,J86-3001,0,\N,Missing
webb-etal-2008-cross,P98-2188,0,\N,Missing
webb-etal-2008-cross,C98-2183,0,\N,Missing
webb-etal-2008-cross,H01-1073,0,\N,Missing
