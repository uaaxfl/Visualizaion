2020.acl-demos.9,N19-1371,1,0.935324,"stics particular intervention, comparator, and outcome measure (which we describe as an ICO triplet). Because the end-to-end task combines NLP subtasks that are supported by different datasets, we collected new development and test sets — 160 abstracts in all, exhaustively annotated — in order to evaluate the overall performance of our system. Two medical doctors3 annotated these documents with the all of the expressed entities, their mentions in the text, the relations between them, the conclusions reported for each ICO triplet and the sentence that contains the supporting evidence for this (Lehman et al., 2019). We were unable to obtain normalized concept labels for the ICO triplets due to the excessive difficulty of the task for the annotators. Modeling decisions were informed by the 60 document development set, and we present evaluations of the first four information extraction modules with regard to the 100 documents in the unseen test set. ploration of the primary literature. In this work we describe an open-source prototype that enables evidence mapping, using NLP to generate interactive overviews and visualizations of all RCT reports indexed by MEDLINE (and accessible via PubMed). When mapping"
2020.acl-demos.9,P18-1019,1,0.882233,"articular intervention for a given condition? In the remainder of this paper we describe a prototype system that facilitates interactive exploration and mapping of the evidence base, with an emphasis on answering the above questions. The Trialstreamer mapping interface allows structured search over study populations, interventions/comparators, and outcomes — collectively referred to as PICO elements (Huang et al., 2006). It then displays key clinical attributes automatically extracted from the set of retrieved trials. This is made possible via NLP modules trained on recently released corpora (Nye et al., 2018; Lehman et al., 2019), described below. 2 2.1 Preprocessing Enabling search over RCT reports requires first compiling and indexing all such studies. This is, perhaps surprisingly, non-trivial. One may rely on “Publication Type” (PT) tags that codify study designs of articles, but these are manually applied by staff at the National Library of Medicine. Consequently, there is a lag between when a new study is published and when a PT tag is applied. Relying on these tags may thus hinder access to the most up-to-date evidence available. Therefore, we instead use an automated tagging system that u"
2021.eacl-main.225,W19-3504,0,0.0126027,"of toxicity in the lower ranges of the toxicity scale. Waseem and Hovy (2016) found that the most common words in sexist and racist tweets in their corpus are “not, sexist, #mkr, women, kat” and 2621 “islam, muslims, muslim, not, mohammed”. Any system trained on this dataset would likely learn these correlations with the frequent words as well, and computational methods to prevent this are yet to be developed. Studies have shown that African American English is more likely to be labeled as toxic by both annotators and systems due to differences in dialect (Sap et al., 2019; Xia et al., 2020; Davidson et al., 2019). Our work does not involve AAE but does reveal how words quite common in news reporting can be erroneously associated with incivility. 3 Show FOX MSNBC PBS Overall U C R Overall Avg Len Vocab 81 12 23 116 201.0 3960 13 23 8 44 209.6 1763 11 31 17 59 216.4 2627 105 66 48 219 206.9 5962 Table 1: Number of Pass II snippets, separated by show and Pass I class: U(ncivil), C(ivil) and R(andom), along with average length of the snippets in words and the size of the vocabulary (unique words). Data Collection for Incivility in News We study the following American programs: PBS NewsHour, MSNBC’s The Ra"
2021.eacl-main.225,2020.acl-main.487,0,0.0555416,"e word with the dominant usage in the training data. To mitigate this bias, Jigsaw’s Perspective API was updated, and model cards (Mitchell et al., 2019) were released for the system to show how well the system is able to predict toxicity when certain identity words are mentioned in a text. Simple templates such as “I am <IDENTITY&gt;” were used to measure the toxicity associated with identity words. More recently, many more incorrect associations with toxicity were discovered. Prabhakaran et al. (2019) found that Perspective returned a higher toxicity score when certain names are mentioned, and Hutchinson et al. (2020) found that this was also the case for words/phrases representing disability. “I am a blind person” had a significantly higher toxicity score than “I am a tall person”. We show that when measured with different templates, the bias that was mitigated in Perspective still manifests. Further, we propose a way to establish a reference set of words and then find words associated with markedly higher toxicity than the reference. This approach reveals a larger set of words which do not lead to errors but trigger uncommonly elevated predictions of toxicity in the lower ranges of the toxicity scale. Wa"
2021.eacl-main.225,D19-1578,0,0.018001,"tems are not capable of using context to disambiguate between civil and uncivil uses of the word, instead associating the word with the dominant usage in the training data. To mitigate this bias, Jigsaw’s Perspective API was updated, and model cards (Mitchell et al., 2019) were released for the system to show how well the system is able to predict toxicity when certain identity words are mentioned in a text. Simple templates such as “I am <IDENTITY&gt;” were used to measure the toxicity associated with identity words. More recently, many more incorrect associations with toxicity were discovered. Prabhakaran et al. (2019) found that Perspective returned a higher toxicity score when certain names are mentioned, and Hutchinson et al. (2020) found that this was also the case for words/phrases representing disability. “I am a blind person” had a significantly higher toxicity score than “I am a tall person”. We show that when measured with different templates, the bias that was mitigated in Perspective still manifests. Further, we propose a way to establish a reference set of words and then find words associated with markedly higher toxicity than the reference. This approach reveals a larger set of words which do n"
2021.eacl-main.225,W19-3621,0,0.0235929,"Computational Linguistics We discover notable anomalies, where words quite typical in neutral news reporting are confounded with incivility in the news domain. We also discover that the mention of many identities, such as Black, gay, Muslim, feminist, etc., triggers high incivility predictions. This occurs despite the fact that Perspective has been modified specifically to minimize such associations (Dixon et al., 2018a). Our findings echo results from gender debiasing of word representations, where bias is removed as measured by a fixed definition but remains present when probed differently (Gonen and Goldberg, 2019). This common error—treating the mention of identity as evidence for incivility—is problematic when the goal is to analyze American political discourse, which is very much marked by us-vs-them identity framing of discussions. These findings will serve as a basis for future work in debiasing systems for incivility prediction, while the dataset of incivility in American news will support computational work on this new task. Our work has implications for researchers of language technology and political science alike. For those developing automated methods for quantifying incivility, we pinpoint t"
2021.eacl-main.225,P19-1163,0,0.0268229,"gger uncommonly elevated predictions of toxicity in the lower ranges of the toxicity scale. Waseem and Hovy (2016) found that the most common words in sexist and racist tweets in their corpus are “not, sexist, #mkr, women, kat” and 2621 “islam, muslims, muslim, not, mohammed”. Any system trained on this dataset would likely learn these correlations with the frequent words as well, and computational methods to prevent this are yet to be developed. Studies have shown that African American English is more likely to be labeled as toxic by both annotators and systems due to differences in dialect (Sap et al., 2019; Xia et al., 2020; Davidson et al., 2019). Our work does not involve AAE but does reveal how words quite common in news reporting can be erroneously associated with incivility. 3 Show FOX MSNBC PBS Overall U C R Overall Avg Len Vocab 81 12 23 116 201.0 3960 13 23 8 44 209.6 1763 11 31 17 59 216.4 2627 105 66 48 219 206.9 5962 Table 1: Number of Pass II snippets, separated by show and Pass I class: U(ncivil), C(ivil) and R(andom), along with average length of the snippets in words and the size of the vocabulary (unique words). Data Collection for Incivility in News We study the following Ameri"
2021.eacl-main.225,D18-1305,0,0.0133998,"ch, defined as speech that targets social groups with the intent to cause harm, is arguably the most widely studied form of incivility detection, largely due to the practical need to moderate online discussions. Many Twitter datasets have been collected, of racist and sexist tweets (Waseem and Hovy, 2016), of hateful and offensive tweets 2 Available at https://github.com/ anushreehede/incivility_in_news (Davidson et al., 2017), and of hateful, abusive, and spam tweets (Founta et al., 2018). Another category of incivility detection that more closely aligns with our work is toxicity prediction. Hua et al. (2018) collected a dataset for toxicity identification in online comments on Wikipedia talk pages. They defined toxicity as comments that are rude, disrespectful, or otherwise likely to make someone leave a discussion. All these datasets are built for social media platforms using either online comments or tweets. We work with American news. To verify if Perspective can reproduce human judgments of civility in this domain, we collect a corpus of news segments annotated for civility. 2.2 Bias in Incivility Detection Models trained on the datasets described above associate the presence of certain descr"
2021.eacl-main.225,N16-2013,0,0.159811,"ivility ratings would be of use to both groups as test data for future models for civility prediction2 . 2 2.1 Related Work Datasets for Incivility Detection Incivility detection is a well-established task, though it is not well-standardized, with the degree and type of incivility varying across datasets. Hate speech, defined as speech that targets social groups with the intent to cause harm, is arguably the most widely studied form of incivility detection, largely due to the practical need to moderate online discussions. Many Twitter datasets have been collected, of racist and sexist tweets (Waseem and Hovy, 2016), of hateful and offensive tweets 2 Available at https://github.com/ anushreehede/incivility_in_news (Davidson et al., 2017), and of hateful, abusive, and spam tweets (Founta et al., 2018). Another category of incivility detection that more closely aligns with our work is toxicity prediction. Hua et al. (2018) collected a dataset for toxicity identification in online comments on Wikipedia talk pages. They defined toxicity as comments that are rude, disrespectful, or otherwise likely to make someone leave a discussion. All these datasets are built for social media platforms using either online"
2021.eacl-main.225,2020.socialnlp-1.2,0,0.0115664,"evated predictions of toxicity in the lower ranges of the toxicity scale. Waseem and Hovy (2016) found that the most common words in sexist and racist tweets in their corpus are “not, sexist, #mkr, women, kat” and 2621 “islam, muslims, muslim, not, mohammed”. Any system trained on this dataset would likely learn these correlations with the frequent words as well, and computational methods to prevent this are yet to be developed. Studies have shown that African American English is more likely to be labeled as toxic by both annotators and systems due to differences in dialect (Sap et al., 2019; Xia et al., 2020; Davidson et al., 2019). Our work does not involve AAE but does reveal how words quite common in news reporting can be erroneously associated with incivility. 3 Show FOX MSNBC PBS Overall U C R Overall Avg Len Vocab 81 12 23 116 201.0 3960 13 23 8 44 209.6 1763 11 31 17 59 216.4 2627 105 66 48 219 206.9 5962 Table 1: Number of Pass II snippets, separated by show and Pass I class: U(ncivil), C(ivil) and R(andom), along with average length of the snippets in words and the size of the vocabulary (unique words). Data Collection for Incivility in News We study the following American programs: PBS"
2021.findings-acl.349,N18-1127,0,0.0228765,"Missing"
2021.findings-acl.349,W17-4419,0,0.0223239,"Missing"
2021.findings-acl.349,S17-2091,0,0.248183,"segmentation: finding spans of text that refer to named entities, and typing: assigning a type to the identified span. Recent efforts have also incorporated entity segmentation explicitly in neural models, with goal of finding longer entities better (Xiao et al., 2019; Ye and Ling, 2018). Such work can be divided into two categories—Multitask learning and semi-Markov CRFs. 6.1 Multi-task Learning Multi-task learning (MTL) involves jointly training multiple related tasks using the same representation such that the auxiliary tasks can help with the performance of the target task. Aguilar et al. (2017) use MTL with hard parameter sharing to add two auxiliary tasks—binary classification to identify entity spans (segmentation) and multi-class classification to type them without CRF. Both tasks use the same biLSTM representation as the target task. The output of the additional tasks is not used in the NER task; they only act as regularizers for NER. Others (Stratos, 2017; Aguilar et al., 2018) also use auxiliary tasks as regularizers but instead of binary classification, the additional tasks performs multiclass classification into B (first word of entity), I (remaining entity words) and O (non"
2021.findings-acl.349,P17-2054,0,0.0131578,"alculated with the multiplicative attention function over the word representation and each of these two vectors. The final word representation is concatenated with the biLSTM representation for NER and the attention weights are used as proxy for probabilities of the word being an entity or not. The loss of both tasks is jointly optimized, with less weight given to entity segmentation over NER. Yu et al. (2018) also add extra supervision, but with a two-step approach. They learn two character-level language models for entity and non-entity words and use their output as a binary feature in NER. Augenstein and Søgaard (2017) use MTL for NER in scientific texts, adding five auxiliary tasks. They add syntactic chunking, hyperlink prediction, multi-word expression identification, frame target annotation, and semantic super-sense tagging; the first three target segmentation. The auxiliary tasks are used one at a time with the target task. Since the datasets for NER and the auxiliary tasks are 3993 Dataset Genre Labels CoNLL news PER, LOC, ORG, MISC Ontonotes convers- PERSON, ORG, LOC, EVENT, NORP, ations LANGUAGE, LAW, MONEY, DATE, TIME, WORK OF ART, PERCENT, QUANTITY, CARDINAL, ORDINAL BTC twitter PER, LOC, ORG TTC"
2021.findings-acl.349,N15-1075,0,0.0144282,"data is low (Augenstein et al., 2017b; Fu et al., 2020a,b). Both traditional models with hand-crafted features (Finkel et al., 2005; Okazaki, 2007) and more recent neural network approaches (Collobert et al., 2011; Huang et al., 2015; Peters et al., 2018; Devlin et al., 2019) achieve lower performance on entities unseen in the training data. Methods that make use of large pretrained language representations, neural (Collobert et al., 2011) or not (Miller et al., 2004), can ameliorate the problem of coverage to some extent. We do not yet know enough about how pretraining data should be chosen (Cherry and Guo, 2015), though there is some evidence that performance on downstream tasks correlates with the vocabulary coverage in the pre-training data (Dai et al., 2019). Prior work has reported that performance is lowest for words that appear neither in the training nor the pretraining vocabulary (Ma and Hovy, 2016). Moreover, the deteriorated performance on out of vocabulary words is not necessarily a failure of the models: many contexts simply do not provide sufficient knowledge to predict the type of an entity, even for people (Agarwal et al., 2021). Models need to expand their knowledge of entities and ga"
2021.findings-acl.349,Q16-1026,0,0.0118341,"s in the dataset and indicate (with a binary 1/0 value) if the word is part of a gazetteer entry of the given type. Many neural network approaches continue to incorporate gazetteers as discrete indicator features concatenated to the pre-trained word embeddings as the input (Collobert et al., 2011; Huang et al., 2015). Adding the features in later stages does not work as well (Magnolini et al., 2019). Both Collobert et al. (2011) and Huang et al. (2015) pre-process datasets to match the gazetteer entries to sentences, using both exact matches and multiword partial matches to gazetteer entries. Chiu and Nichols (2016) perform a similar matching but use four binary values for each label, indicating whether the given word matches the gazetteer entity exactly (S), at the beginning (B), end (E) or the any of the words in between (I). 5.2 Continuous Gazetteer Features The approach above does not use gazetteers very effectively. Gazetteers contain many more entities of each type than are available in even the largest training set (Table 3). One insight is to use the gazetteers as a additional source of training examples. A simple way is to add the gazetteer entries to the labeled data, without any context. Liu e"
2021.findings-acl.349,W99-0613,0,0.01766,"ich the entity types are used. The same entity may appear in multiple gazetteers. Given that current methods heavily rely on entity memorization and little on context, this is possibly acceptable. For completeness however, we ought to mention that the link structure of Wikipedia can be used to derive dense representations for entity types directly (Long et al., 2016; Ganea and Hofmann, 2017; Mengge et al., 2020; Ghaddar and Langlais, 2018). Comparing gazetteer representations with and without context would be a direction for exploration in future work. 6 Entity Segmentation in NER Early work (Collins and Singer, 1999; Downey et al., 2007; Ritter et al., 2011) treated NER as two subtasks, i.e. entity segmentation: finding spans of text that refer to named entities, and typing: assigning a type to the identified span. Recent efforts have also incorporated entity segmentation explicitly in neural models, with goal of finding longer entities better (Xiao et al., 2019; Ye and Ling, 2018). Such work can be divided into two categories—Multitask learning and semi-Markov CRFs. 6.1 Multi-task Learning Multi-task learning (MTL) involves jointly training multiple related tasks using the same representation such that"
2021.findings-acl.349,N19-1149,0,0.0120347,"recent neural network approaches (Collobert et al., 2011; Huang et al., 2015; Peters et al., 2018; Devlin et al., 2019) achieve lower performance on entities unseen in the training data. Methods that make use of large pretrained language representations, neural (Collobert et al., 2011) or not (Miller et al., 2004), can ameliorate the problem of coverage to some extent. We do not yet know enough about how pretraining data should be chosen (Cherry and Guo, 2015), though there is some evidence that performance on downstream tasks correlates with the vocabulary coverage in the pre-training data (Dai et al., 2019). Prior work has reported that performance is lowest for words that appear neither in the training nor the pretraining vocabulary (Ma and Hovy, 2016). Moreover, the deteriorated performance on out of vocabulary words is not necessarily a failure of the models: many contexts simply do not provide sufficient knowledge to predict the type of an entity, even for people (Agarwal et al., 2021). Models need to expand their knowledge of entities and gazetteers are a natural way for doing that. 4 Where Do Gazetteers Come From? Existing tables, lists, directories, databases and knowledge bases are widel"
2021.findings-acl.349,C16-1111,0,0.0257051,"Missing"
2021.findings-acl.349,N19-1423,0,0.0460204,"020). An alternative to retraining, not yet explored in literature, is to develop methods that can make use of gazetteers that possibly could be updated more quickly and cheaply compared to continuously annotating new training data. Even in stable domains such as newswire, the ability of models to generalize to words not seen in the training data is low (Augenstein et al., 2017b; Fu et al., 2020a,b). Both traditional models with hand-crafted features (Finkel et al., 2005; Okazaki, 2007) and more recent neural network approaches (Collobert et al., 2011; Huang et al., 2015; Peters et al., 2018; Devlin et al., 2019) achieve lower performance on entities unseen in the training data. Methods that make use of large pretrained language representations, neural (Collobert et al., 2011) or not (Miller et al., 2004), can ameliorate the problem of coverage to some extent. We do not yet know enough about how pretraining data should be chosen (Cherry and Guo, 2015), though there is some evidence that performance on downstream tasks correlates with the vocabulary coverage in the pre-training data (Dai et al., 2019). Prior work has reported that performance is lowest for words that appear neither in the training nor"
2021.findings-acl.349,W03-0420,0,0.122307,"teer to not only improve coverage but also improve performance on longer entities for which segmentation methods are developed. Regardless of the distribution of entity lengths, the total number of entities in gazetteers is much higher than that in NER datasets so a higher percentage of longer entities does not equate to a small number of short entities. 5 Gazetteer Features for NER Here, we overview the ways gazetteers have been integrated in NER models. 5.1 Discrete Gazetteer Lookup Features Feature-based CRF models for NER used gazetteers to generate indicators for each word in a sentence (Bender et al., 2003; Minkov et al., 2005; Ratinov and Roth, 2009; Ritter et al., 2011; Yang et al., 2016; Seyler et al., 2018). The number of indicators equals the number of entity types in the dataset and indicate (with a binary 1/0 value) if the word is part of a gazetteer entry of the given type. Many neural network approaches continue to incorporate gazetteers as discrete indicator features concatenated to the pre-trained word embeddings as the input (Collobert et al., 2011; Huang et al., 2015). Adding the features in later stages does not work as well (Magnolini et al., 2019). Both Collobert et al. (2011) a"
2021.findings-acl.349,N13-1037,0,0.012995,"ded for better generalization through improved entity coverage, to predict the type for words that have not been encountered in training and possibly even pre-training. This means the need will be more acute for practical deployment of NER and will be less pronounced on fixed datasets in which train and test data are sampled from overlapping or adjacent time periods, with high overlap of entities across both. The most compelling example for the need to handle unseen language comes from work on NER on Twitter. Language on Twitter changes rapidly, much more rapidly than for other types of text (Eisenstein, 2013). This change requires models to be retrained periodically to maintain optimal performance for the current time period (Rijhwani and Preotiuc-Pietro, 2020). An alternative to retraining, not yet explored in literature, is to develop methods that can make use of gazetteers that possibly could be updated more quickly and cheaply compared to continuously annotating new training data. Even in stable domains such as newswire, the ability of models to generalize to words not seen in the training data is low (Augenstein et al., 2017b; Fu et al., 2020a,b). Both traditional models with hand-crafted fea"
2021.findings-acl.349,P05-1045,0,0.048016,"change requires models to be retrained periodically to maintain optimal performance for the current time period (Rijhwani and Preotiuc-Pietro, 2020). An alternative to retraining, not yet explored in literature, is to develop methods that can make use of gazetteers that possibly could be updated more quickly and cheaply compared to continuously annotating new training data. Even in stable domains such as newswire, the ability of models to generalize to words not seen in the training data is low (Augenstein et al., 2017b; Fu et al., 2020a,b). Both traditional models with hand-crafted features (Finkel et al., 2005; Okazaki, 2007) and more recent neural network approaches (Collobert et al., 2011; Huang et al., 2015; Peters et al., 2018; Devlin et al., 2019) achieve lower performance on entities unseen in the training data. Methods that make use of large pretrained language representations, neural (Collobert et al., 2011) or not (Miller et al., 2004), can ameliorate the problem of coverage to some extent. We do not yet know enough about how pretraining data should be chosen (Cherry and Guo, 2015), though there is some evidence that performance on downstream tasks correlates with the vocabulary coverage i"
2021.findings-acl.349,2020.emnlp-main.489,0,0.017712,"much more rapidly than for other types of text (Eisenstein, 2013). This change requires models to be retrained periodically to maintain optimal performance for the current time period (Rijhwani and Preotiuc-Pietro, 2020). An alternative to retraining, not yet explored in literature, is to develop methods that can make use of gazetteers that possibly could be updated more quickly and cheaply compared to continuously annotating new training data. Even in stable domains such as newswire, the ability of models to generalize to words not seen in the training data is low (Augenstein et al., 2017b; Fu et al., 2020a,b). Both traditional models with hand-crafted features (Finkel et al., 2005; Okazaki, 2007) and more recent neural network approaches (Collobert et al., 2011; Huang et al., 2015; Peters et al., 2018; Devlin et al., 2019) achieve lower performance on entities unseen in the training data. Methods that make use of large pretrained language representations, neural (Collobert et al., 2011) or not (Miller et al., 2004), can ameliorate the problem of coverage to some extent. We do not yet know enough about how pretraining data should be chosen (Cherry and Guo, 2015), though there is some evidence t"
2021.findings-acl.349,D17-1277,0,0.0427126,"Missing"
2021.findings-acl.349,C18-1161,0,0.0117967,"ngent on the input representation. 5.3 Contextual Gazetteers Learning from just the gazetteer has the drawback that the representations do not include any clues about the context in which the entity types are used. The same entity may appear in multiple gazetteers. Given that current methods heavily rely on entity memorization and little on context, this is possibly acceptable. For completeness however, we ought to mention that the link structure of Wikipedia can be used to derive dense representations for entity types directly (Long et al., 2016; Ganea and Hofmann, 2017; Mengge et al., 2020; Ghaddar and Langlais, 2018). Comparing gazetteer representations with and without context would be a direction for exploration in future work. 6 Entity Segmentation in NER Early work (Collins and Singer, 1999; Downey et al., 2007; Ritter et al., 2011) treated NER as two subtasks, i.e. entity segmentation: finding spans of text that refer to named entities, and typing: assigning a type to the identified span. Recent efforts have also incorporated entity segmentation explicitly in neural models, with goal of finding longer entities better (Xiao et al., 2019; Ye and Ling, 2018). Such work can be divided into two categories"
2021.findings-acl.349,C96-1079,0,0.371598,"hat gazetteers improve entity segmentation and not just entity typing. Hence, we explore their utility in recognizing long entities, a problem for which entity segmentation techniques were developed. Our work explains the mechanisms via which gazetteers improve the performance of neural NER models. 1 Introduction Named Entity Recognition (NER) has the unique property of being a task appealing to researchers and at the same time being fairly robust for immediate practical applications. In many domains, it is of interest to identify segments of text conveying a concept of a given type—a person (Grishman and Sundheim, 1996), an event (Hovy et al., 2006), a disease (Do˘gan et al., 2014), a gene (Kim et al., 2003), a chemical (Krallinger et al., 2015), a food (Magnolini et al., 2019), an item of clothing (Putthividhya and Hu, 2011), a research technique (Augenstein et al., 2017a), etc. Approaches to NER are typically not domainspecific, treating the problem as a sequence labelling task regardless of the categories of interest. Yet, researchers also widely agree that named entity recognition is a knowledge intensive task (Ratinov and Roth, 2009; Seyler et al., 2018): the availability of external knowledge resources"
2021.findings-acl.349,N06-2015,0,0.339181,"on and not just entity typing. Hence, we explore their utility in recognizing long entities, a problem for which entity segmentation techniques were developed. Our work explains the mechanisms via which gazetteers improve the performance of neural NER models. 1 Introduction Named Entity Recognition (NER) has the unique property of being a task appealing to researchers and at the same time being fairly robust for immediate practical applications. In many domains, it is of interest to identify segments of text conveying a concept of a given type—a person (Grishman and Sundheim, 1996), an event (Hovy et al., 2006), a disease (Do˘gan et al., 2014), a gene (Kim et al., 2003), a chemical (Krallinger et al., 2015), a food (Magnolini et al., 2019), an item of clothing (Putthividhya and Hu, 2011), a research technique (Augenstein et al., 2017a), etc. Approaches to NER are typically not domainspecific, treating the problem as a sequence labelling task regardless of the categories of interest. Yet, researchers also widely agree that named entity recognition is a knowledge intensive task (Ratinov and Roth, 2009; Seyler et al., 2018): the availability of external knowledge resources in the form of lists of examp"
2021.findings-acl.349,N16-1030,0,0.0175322,"ve methods. We use two input word representations: the 300-d GloVe vectors trained on Common Crawl (Pennington et al., 2014), which is the dominant representation in NER, and the 1024-d contextual ELMo (Peters et al., 2018) representations trained on the 1B Word Benchmark (Chelba et al., 2014). In each case, character-based word representations learned with CNNs (Ma and Hovy, 2016) are also concatenated. The final concatenated representation is used as input to bidirectional LSTMs (Hochreiter and Schmidhuber, 1997), followed by a CRF (Lafferty et al., 2001) layer. We use the implementation by Lample et al. (2016) for most experiments. 3 Why Use Gazetteers? Gazetteers are large dictionaries consisting of lists of entities of a particular type. For example, a person gazetteer may consist of full names and parts of names such as the first names of people. Before we start our discussion of methods for incorporating gazetteers, it is worth considering if we have sufficient evidence that they are needed at all. Gazetteers are needed for better generalization through improved entity coverage, to predict the type for words that have not been encountered in training and possibly even pre-training. This means t"
2021.findings-acl.349,P19-1524,0,0.0504975,"2016) perform a similar matching but use four binary values for each label, indicating whether the given word matches the gazetteer entity exactly (S), at the beginning (B), end (E) or the any of the words in between (I). 5.2 Continuous Gazetteer Features The approach above does not use gazetteers very effectively. Gazetteers contain many more entities of each type than are available in even the largest training set (Table 3). One insight is to use the gazetteers as a additional source of training examples. A simple way is to add the gazetteer entries to the labeled data, without any context. Liu et al. (2019a) report that this data augmentation approach led to much worse overall results, presumably because of the great shift in label distributions. Another approach is to augment the training data by replacing entities in place by other entities from gazetteers. Song et al. (2020) reported no improvement with such a random entity replacement, likely due to the need for manual intervention for replacement of entities of some types to maintain coherence of text (Agarwal et al., 2020). A much more successful alternative is to learn a separate (or sub-) module, trained to predict types for text spans,"
2021.findings-acl.349,P19-1233,0,0.0556792,"2016) perform a similar matching but use four binary values for each label, indicating whether the given word matches the gazetteer entity exactly (S), at the beginning (B), end (E) or the any of the words in between (I). 5.2 Continuous Gazetteer Features The approach above does not use gazetteers very effectively. Gazetteers contain many more entities of each type than are available in even the largest training set (Table 3). One insight is to use the gazetteers as a additional source of training examples. A simple way is to add the gazetteer entries to the labeled data, without any context. Liu et al. (2019a) report that this data augmentation approach led to much worse overall results, presumably because of the great shift in label distributions. Another approach is to augment the training data by replacing entities in place by other entities from gazetteers. Song et al. (2020) reported no improvement with such a random entity replacement, likely due to the need for manual intervention for replacement of entities of some types to maintain coherence of text (Agarwal et al., 2020). A much more successful alternative is to learn a separate (or sub-) module, trained to predict types for text spans,"
2021.findings-acl.349,P16-2019,0,0.0197892,"data. We observed some improvement on almost all datasets, contingent on the input representation. 5.3 Contextual Gazetteers Learning from just the gazetteer has the drawback that the representations do not include any clues about the context in which the entity types are used. The same entity may appear in multiple gazetteers. Given that current methods heavily rely on entity memorization and little on context, this is possibly acceptable. For completeness however, we ought to mention that the link structure of Wikipedia can be used to derive dense representations for entity types directly (Long et al., 2016; Ganea and Hofmann, 2017; Mengge et al., 2020; Ghaddar and Langlais, 2018). Comparing gazetteer representations with and without context would be a direction for exploration in future work. 6 Entity Segmentation in NER Early work (Collins and Singer, 1999; Downey et al., 2007; Ritter et al., 2011) treated NER as two subtasks, i.e. entity segmentation: finding spans of text that refer to named entities, and typing: assigning a type to the identified span. Recent efforts have also incorporated entity segmentation explicitly in neural models, with goal of finding longer entities better (Xiao et"
2021.findings-acl.349,P16-1101,0,0.450668,"NER We explore variants of the now classic biLSTMCRF architecture for NER (Huang et al., 2015). We overview how gazetteer features and segmentation can be integrated in this paradigm and carry out a comparison of several representative methods. We use two input word representations: the 300-d GloVe vectors trained on Common Crawl (Pennington et al., 2014), which is the dominant representation in NER, and the 1024-d contextual ELMo (Peters et al., 2018) representations trained on the 1B Word Benchmark (Chelba et al., 2014). In each case, character-based word representations learned with CNNs (Ma and Hovy, 2016) are also concatenated. The final concatenated representation is used as input to bidirectional LSTMs (Hochreiter and Schmidhuber, 1997), followed by a CRF (Lafferty et al., 2001) layer. We use the implementation by Lample et al. (2016) for most experiments. 3 Why Use Gazetteers? Gazetteers are large dictionaries consisting of lists of entities of a particular type. For example, a person gazetteer may consist of full names and parts of names such as the first names of people. Before we start our discussion of methods for incorporating gazetteers, it is worth considering if we have sufficient e"
2021.findings-acl.349,W19-5807,0,0.167174,"tation techniques were developed. Our work explains the mechanisms via which gazetteers improve the performance of neural NER models. 1 Introduction Named Entity Recognition (NER) has the unique property of being a task appealing to researchers and at the same time being fairly robust for immediate practical applications. In many domains, it is of interest to identify segments of text conveying a concept of a given type—a person (Grishman and Sundheim, 1996), an event (Hovy et al., 2006), a disease (Do˘gan et al., 2014), a gene (Kim et al., 2003), a chemical (Krallinger et al., 2015), a food (Magnolini et al., 2019), an item of clothing (Putthividhya and Hu, 2011), a research technique (Augenstein et al., 2017a), etc. Approaches to NER are typically not domainspecific, treating the problem as a sequence labelling task regardless of the categories of interest. Yet, researchers also widely agree that named entity recognition is a knowledge intensive task (Ratinov and Roth, 2009; Seyler et al., 2018): the availability of external knowledge resources in the form of lists of example entities of a given type, or gazetteers, improve performance almost universally. Since gazetteers are readily available, from kn"
2021.findings-acl.349,2020.emnlp-main.514,0,0.0122216,"t all datasets, contingent on the input representation. 5.3 Contextual Gazetteers Learning from just the gazetteer has the drawback that the representations do not include any clues about the context in which the entity types are used. The same entity may appear in multiple gazetteers. Given that current methods heavily rely on entity memorization and little on context, this is possibly acceptable. For completeness however, we ought to mention that the link structure of Wikipedia can be used to derive dense representations for entity types directly (Long et al., 2016; Ganea and Hofmann, 2017; Mengge et al., 2020; Ghaddar and Langlais, 2018). Comparing gazetteer representations with and without context would be a direction for exploration in future work. 6 Entity Segmentation in NER Early work (Collins and Singer, 1999; Downey et al., 2007; Ritter et al., 2011) treated NER as two subtasks, i.e. entity segmentation: finding spans of text that refer to named entities, and typing: assigning a type to the identified span. Recent efforts have also incorporated entity segmentation explicitly in neural models, with goal of finding longer entities better (Xiao et al., 2019; Ye and Ling, 2018). Such work can b"
2021.findings-acl.349,E99-1001,0,0.102545,"Missing"
2021.findings-acl.349,D11-1141,0,0.127011,"longer entities for which segmentation methods are developed. Regardless of the distribution of entity lengths, the total number of entities in gazetteers is much higher than that in NER datasets so a higher percentage of longer entities does not equate to a small number of short entities. 5 Gazetteer Features for NER Here, we overview the ways gazetteers have been integrated in NER models. 5.1 Discrete Gazetteer Lookup Features Feature-based CRF models for NER used gazetteers to generate indicators for each word in a sentence (Bender et al., 2003; Minkov et al., 2005; Ratinov and Roth, 2009; Ritter et al., 2011; Yang et al., 2016; Seyler et al., 2018). The number of indicators equals the number of entity types in the dataset and indicate (with a binary 1/0 value) if the word is part of a gazetteer entry of the given type. Many neural network approaches continue to incorporate gazetteers as discrete indicator features concatenated to the pre-trained word embeddings as the input (Collobert et al., 2011; Huang et al., 2015). Adding the features in later stages does not work as well (Magnolini et al., 2019). Both Collobert et al. (2011) and Huang et al. (2015) pre-process datasets to match the gazetteer"
2021.findings-acl.349,N04-1043,0,0.137196,"uously annotating new training data. Even in stable domains such as newswire, the ability of models to generalize to words not seen in the training data is low (Augenstein et al., 2017b; Fu et al., 2020a,b). Both traditional models with hand-crafted features (Finkel et al., 2005; Okazaki, 2007) and more recent neural network approaches (Collobert et al., 2011; Huang et al., 2015; Peters et al., 2018; Devlin et al., 2019) achieve lower performance on entities unseen in the training data. Methods that make use of large pretrained language representations, neural (Collobert et al., 2011) or not (Miller et al., 2004), can ameliorate the problem of coverage to some extent. We do not yet know enough about how pretraining data should be chosen (Cherry and Guo, 2015), though there is some evidence that performance on downstream tasks correlates with the vocabulary coverage in the pre-training data (Dai et al., 2019). Prior work has reported that performance is lowest for words that appear neither in the training nor the pretraining vocabulary (Ma and Hovy, 2016). Moreover, the deteriorated performance on out of vocabulary words is not necessarily a failure of the models: many contexts simply do not provide su"
2021.findings-acl.349,H05-1056,0,0.0484396,"ove coverage but also improve performance on longer entities for which segmentation methods are developed. Regardless of the distribution of entity lengths, the total number of entities in gazetteers is much higher than that in NER datasets so a higher percentage of longer entities does not equate to a small number of short entities. 5 Gazetteer Features for NER Here, we overview the ways gazetteers have been integrated in NER models. 5.1 Discrete Gazetteer Lookup Features Feature-based CRF models for NER used gazetteers to generate indicators for each word in a sentence (Bender et al., 2003; Minkov et al., 2005; Ratinov and Roth, 2009; Ritter et al., 2011; Yang et al., 2016; Seyler et al., 2018). The number of indicators equals the number of entity types in the dataset and indicate (with a binary 1/0 value) if the word is part of a gazetteer entry of the given type. Many neural network approaches continue to incorporate gazetteers as discrete indicator features concatenated to the pre-trained word embeddings as the input (Collobert et al., 2011; Huang et al., 2015). Adding the features in later stages does not work as well (Magnolini et al., 2019). Both Collobert et al. (2011) and Huang et al. (2015"
2021.findings-acl.349,D14-1162,0,0.0850686,"ed comparison of representative approaches for each aspect, and (iii) novel findings and analyses of the interplay between gazetteers and segmentation. Our findings can inform both future researchers and practitioners interested in NER. 2 biLSTM-CRF architecture for NER We explore variants of the now classic biLSTMCRF architecture for NER (Huang et al., 2015). We overview how gazetteer features and segmentation can be integrated in this paradigm and carry out a comparison of several representative methods. We use two input word representations: the 300-d GloVe vectors trained on Common Crawl (Pennington et al., 2014), which is the dominant representation in NER, and the 1024-d contextual ELMo (Peters et al., 2018) representations trained on the 1B Word Benchmark (Chelba et al., 2014). In each case, character-based word representations learned with CNNs (Ma and Hovy, 2016) are also concatenated. The final concatenated representation is used as input to bidirectional LSTMs (Hochreiter and Schmidhuber, 1997), followed by a CRF (Lafferty et al., 2001) layer. We use the implementation by Lample et al. (2016) for most experiments. 3 Why Use Gazetteers? Gazetteers are large dictionaries consisting of lists of en"
2021.findings-acl.349,N18-1202,0,0.197237,"e interplay between gazetteers and segmentation. Our findings can inform both future researchers and practitioners interested in NER. 2 biLSTM-CRF architecture for NER We explore variants of the now classic biLSTMCRF architecture for NER (Huang et al., 2015). We overview how gazetteer features and segmentation can be integrated in this paradigm and carry out a comparison of several representative methods. We use two input word representations: the 300-d GloVe vectors trained on Common Crawl (Pennington et al., 2014), which is the dominant representation in NER, and the 1024-d contextual ELMo (Peters et al., 2018) representations trained on the 1B Word Benchmark (Chelba et al., 2014). In each case, character-based word representations learned with CNNs (Ma and Hovy, 2016) are also concatenated. The final concatenated representation is used as input to bidirectional LSTMs (Hochreiter and Schmidhuber, 1997), followed by a CRF (Lafferty et al., 2001) layer. We use the implementation by Lample et al. (2016) for most experiments. 3 Why Use Gazetteers? Gazetteers are large dictionaries consisting of lists of entities of a particular type. For example, a person gazetteer may consist of full names and parts of"
2021.findings-acl.349,D11-1144,0,0.0141259,"lains the mechanisms via which gazetteers improve the performance of neural NER models. 1 Introduction Named Entity Recognition (NER) has the unique property of being a task appealing to researchers and at the same time being fairly robust for immediate practical applications. In many domains, it is of interest to identify segments of text conveying a concept of a given type—a person (Grishman and Sundheim, 1996), an event (Hovy et al., 2006), a disease (Do˘gan et al., 2014), a gene (Kim et al., 2003), a chemical (Krallinger et al., 2015), a food (Magnolini et al., 2019), an item of clothing (Putthividhya and Hu, 2011), a research technique (Augenstein et al., 2017a), etc. Approaches to NER are typically not domainspecific, treating the problem as a sequence labelling task regardless of the categories of interest. Yet, researchers also widely agree that named entity recognition is a knowledge intensive task (Ratinov and Roth, 2009; Seyler et al., 2018): the availability of external knowledge resources in the form of lists of example entities of a given type, or gazetteers, improve performance almost universally. Since gazetteers are readily available, from knowledge bases, databases of products and speciali"
2021.findings-acl.349,W09-1119,0,0.514839,"fy segments of text conveying a concept of a given type—a person (Grishman and Sundheim, 1996), an event (Hovy et al., 2006), a disease (Do˘gan et al., 2014), a gene (Kim et al., 2003), a chemical (Krallinger et al., 2015), a food (Magnolini et al., 2019), an item of clothing (Putthividhya and Hu, 2011), a research technique (Augenstein et al., 2017a), etc. Approaches to NER are typically not domainspecific, treating the problem as a sequence labelling task regardless of the categories of interest. Yet, researchers also widely agree that named entity recognition is a knowledge intensive task (Ratinov and Roth, 2009; Seyler et al., 2018): the availability of external knowledge resources in the form of lists of example entities of a given type, or gazetteers, improve performance almost universally. Since gazetteers are readily available, from knowledge bases, databases of products and specialized ontologies, having practical guidance on how to handle gazetteers in NER would be valuable. In this paper, we provide a survey of how gazetteers have been used in neural approaches to NER in English and compare key approaches with the popular biLSTM-CRF architecture. To ensure that our conclusions accurately char"
2021.findings-acl.349,2020.acl-main.680,0,0.102008,"and possibly even pre-training. This means the need will be more acute for practical deployment of NER and will be less pronounced on fixed datasets in which train and test data are sampled from overlapping or adjacent time periods, with high overlap of entities across both. The most compelling example for the need to handle unseen language comes from work on NER on Twitter. Language on Twitter changes rapidly, much more rapidly than for other types of text (Eisenstein, 2013). This change requires models to be retrained periodically to maintain optimal performance for the current time period (Rijhwani and Preotiuc-Pietro, 2020). An alternative to retraining, not yet explored in literature, is to develop methods that can make use of gazetteers that possibly could be updated more quickly and cheaply compared to continuously annotating new training data. Even in stable domains such as newswire, the ability of models to generalize to words not seen in the training data is low (Augenstein et al., 2017b; Fu et al., 2020a,b). Both traditional models with hand-crafted features (Finkel et al., 2005; Okazaki, 2007) and more recent neural network approaches (Collobert et al., 2011; Huang et al., 2015; Peters et al., 2018; Devl"
2021.findings-acl.349,I17-2017,0,0.0150447,"ve semi-markov CRF (Zhuo et al., 2016) creates a pyramid-shaped feature extractor for spans. The bottom-most layer consists of word representations and hence length one spans. Representation of adjacent words are combined to form length two spans for the next layer and so on. The top layer consists of a single span with the full sentence. Hybrid semi-Markov CRF or HSCRF (Ye and Ling, 2018) do not use an explicit span representation. Instead they consider the span-level score as sum of the word-level CRF scores of constituent words. Both the word-level and span-level CRF are jointly optimized. Sato et al. (2017) use a two step process to reduce the search space of the spans. They first generate possible spans from a separate model using a score cutoff and then find the best possible labelling over these spans instead of all spans upto a maximum specified length. 7 Datasets We evaluate several of the above models on four datasets, to compare their performance. Table 2 shows the entity types in each dataset. 1. CoNLL is the English portion of the CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003), extracted from the Reuters 1996 newswire corpus. 4. TTC or Temporal Twitter Corpus (Rijhwani and Preotiuc"
2021.findings-acl.349,P18-2039,0,0.0853544,"eying a concept of a given type—a person (Grishman and Sundheim, 1996), an event (Hovy et al., 2006), a disease (Do˘gan et al., 2014), a gene (Kim et al., 2003), a chemical (Krallinger et al., 2015), a food (Magnolini et al., 2019), an item of clothing (Putthividhya and Hu, 2011), a research technique (Augenstein et al., 2017a), etc. Approaches to NER are typically not domainspecific, treating the problem as a sequence labelling task regardless of the categories of interest. Yet, researchers also widely agree that named entity recognition is a knowledge intensive task (Ratinov and Roth, 2009; Seyler et al., 2018): the availability of external knowledge resources in the form of lists of example entities of a given type, or gazetteers, improve performance almost universally. Since gazetteers are readily available, from knowledge bases, databases of products and specialized ontologies, having practical guidance on how to handle gazetteers in NER would be valuable. In this paper, we provide a survey of how gazetteers have been used in neural approaches to NER in English and compare key approaches with the popular biLSTM-CRF architecture. To ensure that our conclusions accurately characterize the utility o"
2021.findings-acl.349,W17-4302,0,0.024182,"kov CRFs. 6.1 Multi-task Learning Multi-task learning (MTL) involves jointly training multiple related tasks using the same representation such that the auxiliary tasks can help with the performance of the target task. Aguilar et al. (2017) use MTL with hard parameter sharing to add two auxiliary tasks—binary classification to identify entity spans (segmentation) and multi-class classification to type them without CRF. Both tasks use the same biLSTM representation as the target task. The output of the additional tasks is not used in the NER task; they only act as regularizers for NER. Others (Stratos, 2017; Aguilar et al., 2018) also use auxiliary tasks as regularizers but instead of binary classification, the additional tasks performs multiclass classification into B (first word of entity), I (remaining entity words) and O (non-entity). The auxiliary tasks in MTL can also be used for extra supervision by concatenating their output label distribution to the representation used by NER (Xiao et al., 2019). Unlike prior work, Xiao et al. (2019) do not use the same representation for the target and auxiliary task. Instead, they build a submodule called similarity-based auxiliary classifier (SAC). S"
2021.findings-acl.349,D19-1105,0,0.071007,"l., 2016; Ganea and Hofmann, 2017; Mengge et al., 2020; Ghaddar and Langlais, 2018). Comparing gazetteer representations with and without context would be a direction for exploration in future work. 6 Entity Segmentation in NER Early work (Collins and Singer, 1999; Downey et al., 2007; Ritter et al., 2011) treated NER as two subtasks, i.e. entity segmentation: finding spans of text that refer to named entities, and typing: assigning a type to the identified span. Recent efforts have also incorporated entity segmentation explicitly in neural models, with goal of finding longer entities better (Xiao et al., 2019; Ye and Ling, 2018). Such work can be divided into two categories—Multitask learning and semi-Markov CRFs. 6.1 Multi-task Learning Multi-task learning (MTL) involves jointly training multiple related tasks using the same representation such that the auxiliary tasks can help with the performance of the target task. Aguilar et al. (2017) use MTL with hard parameter sharing to add two auxiliary tasks—binary classification to identify entity spans (segmentation) and multi-class classification to type them without CRF. Both tasks use the same biLSTM representation as the target task. The output of"
2021.findings-acl.349,N16-1032,0,0.0211444,"hich segmentation methods are developed. Regardless of the distribution of entity lengths, the total number of entities in gazetteers is much higher than that in NER datasets so a higher percentage of longer entities does not equate to a small number of short entities. 5 Gazetteer Features for NER Here, we overview the ways gazetteers have been integrated in NER models. 5.1 Discrete Gazetteer Lookup Features Feature-based CRF models for NER used gazetteers to generate indicators for each word in a sentence (Bender et al., 2003; Minkov et al., 2005; Ratinov and Roth, 2009; Ritter et al., 2011; Yang et al., 2016; Seyler et al., 2018). The number of indicators equals the number of entity types in the dataset and indicate (with a binary 1/0 value) if the word is part of a gazetteer entry of the given type. Many neural network approaches continue to incorporate gazetteers as discrete indicator features concatenated to the pre-trained word embeddings as the input (Collobert et al., 2011; Huang et al., 2015). Adding the features in later stages does not work as well (Magnolini et al., 2019). Both Collobert et al. (2011) and Huang et al. (2015) pre-process datasets to match the gazetteer entries to sentenc"
2021.findings-acl.349,P18-2038,0,0.150216,"with such a random entity replacement, likely due to the need for manual intervention for replacement of entities of some types to maintain coherence of text (Agarwal et al., 2020). A much more successful alternative is to learn a separate (or sub-) module, trained to predict types for text spans, using the gazetteer entries and synthetic negative examples sampled from a NER training set or even the gazetteer. We will refer to the separate module as a gazetteer network. It is straightforward to integrate the label distribution scores from this model in a semi-Markov CRF for sequence labeling (Ye and Ling, 2018) that operates at the span level (which we describe in greater detail later). The resulting combination is far more effective than discrete indicator gazetteer features. Magnolini et al. (2019) and Liu et al. (2019b) propose a similar approach. They learn a gazetteer network but instead of using the label score distribution, intermediate word representations (gazetteer embeddings henceforth) are incorporated in the NER model. Liu et al. (2019b) use a semi-Markov CRF operating at the span level and generate the 3992 gazetteer embeddings for each potential span. They follow the evaluation approa"
2021.findings-acl.349,D18-1345,0,0.0161563,"aintains two randomly initialized vectors representing entity and non-entity classes. These vectors are combined with an attention layer to get the final word representations. The attention weights are calculated with the multiplicative attention function over the word representation and each of these two vectors. The final word representation is concatenated with the biLSTM representation for NER and the attention weights are used as proxy for probabilities of the word being an entity or not. The loss of both tasks is jointly optimized, with less weight given to entity segmentation over NER. Yu et al. (2018) also add extra supervision, but with a two-step approach. They learn two character-level language models for entity and non-entity words and use their output as a binary feature in NER. Augenstein and Søgaard (2017) use MTL for NER in scientific texts, adding five auxiliary tasks. They add syntactic chunking, hyperlink prediction, multi-word expression identification, frame target annotation, and semantic super-sense tagging; the first three target segmentation. The auxiliary tasks are used one at a time with the target task. Since the datasets for NER and the auxiliary tasks are 3993 Dataset"
2021.findings-acl.349,P16-1134,0,0.0207998,"ng instance. 6.2 Semi-Markov CRFs Semi-Markov CRFs (Sarawagi and Cohen, 2004) are a variant of linear chain CRFs that capture dependencies between adjacent spans of text instead of adjacent words. The Markov assumption still holds across spans but not within the span. The goal is to find the best possible segmentation into spans using scores at span-level. The maximum length of spans is bound to reduce computation cost. Sarawagi and Cohen (2004) use hand-crafted features for span representations but recent work has explored other techniques to represent spans. Gated recursive semi-markov CRF (Zhuo et al., 2016) creates a pyramid-shaped feature extractor for spans. The bottom-most layer consists of word representations and hence length one spans. Representation of adjacent words are combined to form length two spans for the next layer and so on. The top layer consists of a single span with the full sentence. Hybrid semi-Markov CRF or HSCRF (Ye and Ling, 2018) do not use an explicit span representation. Instead they consider the span-level score as sum of the word-level CRF scores of constituent words. Both the word-level and span-level CRF are jointly optimized. Sato et al. (2017) use a two step proc"
C04-1129,E99-1042,0,0.0328042,"our evaluations on the test sets from the 2003 and 2004 Document Understanding Conference and report that simplifying parentheticals results in significant improvement on the automated evaluation metric Rouge. 1 Introduction Syntactic simplification is an NLP task, the goal of which is to rewrite sentences to reduce their grammatical complexity while preserving their meaning and information content. Text simplification is a useful task for varied reasons. Chandrasekar et al. (1996) viewed text simplification as a preprocessing tool to improve the performance of their parser. The PSET project (Carroll et al., 1999), on the other hand, focused its research on simplifying newspaper text for aphasics, who have trouble with long sentences and complicated grammatical constructs. We have previously (Siddharthan, 2002; Siddharthan, 2003) developed a shallow and robust syntactic simplification system for news reports, that simplifies relative clauses, apposition and conjunction. In this paper, we explore the use of syntactic simplification in multi-document summarization. 1.1 Sentence Shortening for Summarization It is interesting to survey the literature in sentence shortening, a task related to syntactic simp"
C04-1129,C96-2183,0,0.412753,"summary is a reference-generation task rather than a content-selection one, and implement a baseline reference rewriting module. We perform our evaluations on the test sets from the 2003 and 2004 Document Understanding Conference and report that simplifying parentheticals results in significant improvement on the automated evaluation metric Rouge. 1 Introduction Syntactic simplification is an NLP task, the goal of which is to rewrite sentences to reduce their grammatical complexity while preserving their meaning and information content. Text simplification is a useful task for varied reasons. Chandrasekar et al. (1996) viewed text simplification as a preprocessing tool to improve the performance of their parser. The PSET project (Carroll et al., 1999), on the other hand, focused its research on simplifying newspaper text for aphasics, who have trouble with long sentences and complicated grammatical constructs. We have previously (Siddharthan, 2002; Siddharthan, 2003) developed a shallow and robust syntactic simplification system for news reports, that simplifies relative clauses, apposition and conjunction. In this paper, we explore the use of syntactic simplification in multi-document summarization. 1.1 Se"
C04-1129,grover-etal-2000-lt,0,0.0107854,"o improved content selection in summaries. We therefore also need to evaluate our summarizer. We do this in $ 3, but first we describe the summarizer in more detail. 2.2 Description of our Summarizer Our summarizer has four stages—preprocessing of original documents to remove parentheticals, clustering of the simplified sentences, selecting of one representative sentence from each cluster and deciding which of these selected sentences to incorporate in the summary. We use our syntactic simplification software (Siddharthan, 2002; Siddharthan, 2003) to remove parentheticals. It uses the LT TTT (Grover et al., 2000) for POS-tagging and simple noun-chunking. It then performs apposition and relative clause identification and attachment using shallow techniques based on local context and animacy information obtained from WordNet (Miller et al., 1993). We then cluster the simplified sentences with SimFinder (Hatzivassiloglou et al., 1999). To further tighten the clusters and ensure that their size is representative of their importance, we post-process them as follows. SimFinder implements an incremental approach to clustering. At each incremental step, the similarity of a new sentence to an existing cluster"
C04-1129,W99-0625,0,0.0695217,"at the clustering is not always accurate. Clusters can contain spurious sentences, and a cluster’s size might then exaggerate its importance. Improving the quality of the clustering can thus be expected to improve the content of the summary. We now describe our experiments on syntactic simplification and sentence clustering. Our hypothesis is that simplifying parenthetical units (relative clauses and appositives) will improve the performance of our clustering algorithm, by preventing it from clustering on the basis of background information. 2.1 Simplification and Clustering We use SimFinder (Hatzivassiloglou et al., 1999) for sentence clustering and its similarity metric to evaluate cluster quality; SimFinder outputs similarity values (simvals) between 0 and 1 for pairs of sentences, based on word overlap, synonymy and n-gram matches. We use the average of the simvals for each pair of sentences in a cluster to evaluate a quality-score for the cluster. Table 1 below shows the quality-scores averaged over all clusters when the original document set is and is not preprocessed using our syntactic simplification software (described in $ 2.2). We use 30 document sets from the 2003 Document Understanding Conference ("
C04-1129,A00-1043,0,0.0157875,", the sentence: Former Democratic National Committee finance director Richard Sullivan faced more pointed questioning from Republicans during his second day on the witness stand in the Senate’s fund-raising investigation. got shortened (with different levels of reduction) to: # Richard Sullivan Republicans Senate. # Richard Sullivan faced pointed questioning. # Richard Sullivan faced pointed questioning from Republicans during day on stand in Senate fundraising investigation. Grefenstette (1998) provided a rule based approach to telegraphic reduction of the kind illustrated above. Since then, Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. These sentence-shortening approaches have been evaluated by comparison with human-shortened sentences and have been shown to compare favorably. However, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in a summary. Recently, Lin (2003) showed that"
C04-1129,N03-1020,0,0.045946,"evaluation methods and also started providing participants with multiple human-written models needed for reliable evaluation. Participating generic multidocument summarizers were tested on 30 eventbased sets in 2003 and 50 sets in 2004, all 80 containing roughly 10 newswire articles each. There were four human-written summaries for each set, created for evaluation purposes. In DUC’03, the task was to generate 100 word summaries, while in DUC’04, the limit was changed to 665 bytes. 3.2 Evaluation Metric We evaluated our summarizer on the DUC test sets using the Rouge automatic scoring metric (Lin and Hovy, 2003). The experiments in Lin and Hovy (2003) show that among n-gram approaches to scoring, Rouge-1 (based on unigrams) has the highest correlation with human scores. In 2004, an additional automatic metric based on longest common subsequence was included (Rouge-L), that aims to overcome some deficiencies of Rouge-1, such as its susceptibility to ungrammatical keyword packing by dishonest summarizers2 . For our evaluations, we use the Rouge settings from DUC’04: stop words are included, words are Porter-stemmed, and all four human model summaries are used. 3.3 DUC’04 Evaluation We entered our syste"
C04-1129,W03-1101,0,0.0123843,"ince then, Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. These sentence-shortening approaches have been evaluated by comparison with human-shortened sentences and have been shown to compare favorably. However, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in a summary. Recently, Lin (2003) showed that statistical sentence-shortening approaches like Knight and Marcu (2000) do not improve content selection in summaries. Indeed he reported that syntax-based sentence-shortening resulted in significantly worse content selection by their extractive summarizer NeATS. Lin (2003) concluded that pure syntaxbased compression does not improve overall summarizer performance, even though the compression algorithm performs well at the sentence level. 1.2 Simplifying Syntax for Summarization A problem with using statistical sentenceshortening for summarization is that syntactic form does not a"
C04-1129,N03-1026,0,0.014123,"e: Former Democratic National Committee finance director Richard Sullivan faced more pointed questioning from Republicans during his second day on the witness stand in the Senate’s fund-raising investigation. got shortened (with different levels of reduction) to: # Richard Sullivan Republicans Senate. # Richard Sullivan faced pointed questioning. # Richard Sullivan faced pointed questioning from Republicans during day on stand in Senate fundraising investigation. Grefenstette (1998) provided a rule based approach to telegraphic reduction of the kind illustrated above. Since then, Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. These sentence-shortening approaches have been evaluated by comparison with human-shortened sentences and have been shown to compare favorably. However, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in a summary. Recently, Lin (2003) showed that statistical sentence-s"
C04-1129,N03-2024,1,\N,Missing
C04-1129,A97-1030,0,\N,Missing
C04-1129,E99-1029,0,\N,Missing
C08-2022,prasad-etal-2008-penn,1,0.783332,"Missing"
C08-2022,W06-1317,0,0.358334,"d, the overall accuracy of identifying contingency and expansion relations is lower, 5 N-gram discourse relation models We have shown above that some relations, such as comparison, can be easily identified because they are often explicit and are expressed by an unambiguous connective. However, one must build a more subtle automatic classifier to find the implicit relations. We now look at the frequencies in which various relations are adjacent in the PDTB. Results from previous studies of discourse relations suggest that the context of a relation can be helpful in disambiguating the relation (Wellner et al., 2006). Here we identify specific dependencies that exist between sequences of relations. We computed χ2 statistics to test the independence of each pair of relations. The question is: do relations A and B occur adjacent to each other more than they would simply due to chance? The 89 First Relation E. Comparison E. Comparison E. Comparison I. Temporal I. Contingency I. Expansion E. Expansion I. Contingency Second Relation I. Contingency E. Comparison I. Expansion E. Temporal E. Contingency E. Expansion I. Expansion E. Comparison χ2 20.1 17.4 9.91 9.42 9.29 6.34 5.50 4.95 p-value .000007 .000030 .001"
C08-2022,N07-1054,0,\N,Missing
C08-2022,J93-2004,0,\N,Missing
C08-2022,J08-1001,0,\N,Missing
C08-2022,P02-1047,0,\N,Missing
C08-2022,N04-1020,0,\N,Missing
C14-1055,al-saif-markert-2010-leeds,0,0.0200724,"lied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugu"
C14-1055,P11-1061,0,0.0245884,"otated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and pr"
C14-1055,P97-1011,0,0.238511,"Missing"
C14-1055,D11-1027,0,0.0282846,"dies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we ca"
C14-1055,P09-1042,0,0.0306296,"i (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License detail"
C14-1055,W04-1101,0,0.0901396,"Missing"
C14-1055,P06-2057,0,0.0221681,"and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 577 Proceedings of COLING 2014,"
C14-1055,P03-1056,0,0.0521517,"n; multiway classification labels one of the five possible classes: non-discourse use, TEMPORAL, COMPARISON, CONTINGENCY and EXPANSION. This series of classifiers results in a system that can assign the same labels as the classifiers trained for English. To complete our presentation of the approach, we now turn to describe the features used to represent instances of potential discourse connectives. 5.2 Features The following set of features for each expression we need to classify are extracted solely from the Chinese part of the corpus4 . The syntactic parse trees were obtained automatically (Levy and Manning, 2003). Connective The connective expressions themselves. The vast majority of connectives (at least in English) are unambiguous, so using the identity of the connective is a hard-to-beat baseline for sense prediction (Pitler et al., 2008). Categories The syntactic category of the expression itself, as well as that of its parents, and its left and right siblings (if any). These features are adapted from Pitler and Nenkova (2009). Depth Depth of the expressions’s syntactic category in the parse tree for the sentence. POS bigram Bigram of part-of-speech tags of the entire sentence. Production pairs Pa"
C14-1055,li-etal-2010-enriching,0,0.0205623,"Missing"
C14-1055,D09-1036,0,0.435052,"labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been devel"
C14-1055,P11-1100,0,0.0134467,"urse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource"
C14-1055,P97-1013,0,0.177418,"divergence, and discourse connective ambiguities. Second, we introduce a novel approach to learning to recognize discourse relations, using the parallel corpus instead of discourse annotation in the language of interest. Our resulting semi-supervised system reaches state-of-art performance on the task of discourse relation detection, and outperforms a supervised system on discourse relation classification. 1 Introduction The analysis of the way spans of text semantically connect with each other to create a coherent text has a rich theoretical and empirical tradition (Mann and Thompson, 1988; Marcu, 1997; Di Eugenio et al., 1997; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relation"
C14-1055,W12-0117,0,0.146065,"he use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another"
C14-1055,W11-2022,0,0.188077,"t available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 577 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 577–587, Dublin, Ireland, August 23-29 2014. The goal of our work is not only to measure the accura"
C14-1055,W09-3029,0,0.0231369,"2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009"
C14-1055,H05-1108,0,0.0116767,"2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 577"
C14-1055,W12-1614,0,0.256277,"small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish ("
C14-1055,D08-1020,1,0.814766,"esource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-"
C14-1055,P09-2004,1,0.953807,"ause of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated w"
C14-1055,C08-2022,1,0.929939,"Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, s"
C14-1055,P09-1077,1,0.828179,"annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relatio"
C14-1055,prasad-etal-2008-penn,0,0.146439,"lting semi-supervised system reaches state-of-art performance on the task of discourse relation detection, and outperforms a supervised system on discourse relation classification. 1 Introduction The analysis of the way spans of text semantically connect with each other to create a coherent text has a rich theoretical and empirical tradition (Mann and Thompson, 1988; Marcu, 1997; Di Eugenio et al., 1997; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and c"
C14-1055,C10-2118,0,0.0144162,"were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza e"
C14-1055,P11-2052,0,0.0372337,"Missing"
C14-1055,D07-1010,0,0.0748826,"97; Allbritton and Moore, 1999; Schilder, 2002). Because of the difficulty in annotation, however, labelled datasets were rare and rather small. The release of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) brought about a new sense of maturity in discourse analysis, finally providing a high-quality large-scale resource for training discourse parsers for English. Based on the PDTB, a number of studies have provided insightful analysis of the use of discourse connectives in English news text and have developed methods for the identification of discourse relations and their arguments (Wellner and Pustejovsky, 2007; Pitler et al., 2008; Pitler and Nenkova, 2009; Pitler et al., 2009; Lin et al., 2009; Prasad et al., 2010; Park and Cardie, 2012; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the r"
C14-1055,P11-2111,0,0.0598275,"Missing"
C14-1055,H01-1035,0,0.0189898,"TB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations, prior work has shown that a parallel corpus is helpful for disambiguating certain explicit discourse connectives (Meyer et al., 2011). To the best of our knowledge, the work we present here is the first study that directly infers discourse relations using resources only available in another language. This work is licenced under a Creative Commons Attribution 4.0 International Licen"
C14-1055,I08-7009,0,0.0315716,"; Lin et al., 2014). Some have applied the insights and classifiers to standard natural language processing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado"
C14-1055,P12-1008,0,0.28866,"sing tasks such as assessing text coherence and text quality (Pitler and Nenkova, 2008; Lin et al., 2011), detecting causal dependencies of events (Do et al., 2011), and machine translation (Meyer and Popescu-Belis, 2012). A resource like the PDTB is extremely valuable, and it would be desirable to have a similar resource in other languages as well. Following the release of the PDTB, smaller corpora annotated with discourse relations have been developed for Hindi (Oza et al., 2009), Turkish (Zeyrek and Webber, 2008), Arabic (Al-Saif and Markert, 2010), and the effort is on-going with Chinese (Zhou and Xue, 2012). On the other hand, for the vast majority of languages, such well-annotated resource for discourse relations is not available. In our work we carry the valuable annotations in the PDTB over to another language—Chinese—using parallel corpora. Projecting information available in one language onto another has been explored in areas such as part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011), grammar induction (Hwa et al., 2005; Ganchev et al., 2009) and semantic role labeling (Pado and Lapata, 2005; Johansson and Nugues, 2006; van der Plas et al., 2011). For discourse relations,"
C14-1055,L08-1000,0,\N,Missing
D08-1020,J08-1001,0,0.789564,"ormance of various formulations of centering. Their results were somewhat unexpected, showing that while centering transition preferences were useful, the most successful strategy for information ordering was based on avoiding rough shifts, that is, sequences of sentences that share no entities in common. This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non-discourse features (Miltsakaki and Kukich, 2000). In a more powerful generalization of centering, Barzilay and Lapata (2008) developed a novel approach which doesn’t postulate a preference for any type of transition but rather computes a set of features that capture transitions of all kinds in the text and their relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al"
D08-1020,W01-1605,0,0.0797459,"ntion—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives. 3.1 Discourse annotation The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal 188 portion of the Penn Treebank (Marcus et al., 1994). All explicit relations (those marked with a discourse connective) are annotated. In addition, each adja"
D08-1020,N04-1025,0,0.797072,"Missing"
D08-1020,P08-2011,0,0.0318063,"reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives. 3"
D08-1020,J95-2003,0,0.808588,"language user) understands a text. Coherent text is characterized by various types of cohesive links that facilitate text comprehension (Halliday and Hasan, 1976). In recent work, considerable attention has been devoted to entity coherence in text quality, especially in relation to information ordering. In many applications such as text generation and summarization, systems need to decide the order in which selected sentences or generated clauses should be presented to the user. Most models attempting to capture local coherence between sentences were based on or inspired by centering theory (Grosz et al., 1995), which postulated strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of reference. In a detailed study of information ordering in three very different corpora, (Karamanis et al., to appear) assessed the performance of various formulations of centering. Their results were somewhat unexpected, showing that while centering transition preferences were useful, the most successful strategy for information ordering was based on avoiding rough shifts, that is, sequences of sentences that share no entities in common. This supports previo"
D08-1020,N04-1024,0,0.0202401,"Missing"
D08-1020,J06-4002,0,0.0289214,"texts and perform the ratings.2 Subjects were asked the following questions: • How well-written is this article? • How well does the text fit together? • How easy was it to understand? • How interesting is this article? For each question, they provided a rating between 1 and 5, with 5 being the best and 1 being the worst. 1 One of the selected articles was missing from the Penn Treebank. Thus, results that do not require syntactic information (Tables 1, 2, 4, and 6) are over all thirty articles, while Tables 3, 5, and 7 report results for the twenty-nine articles with Treebank parse trees. 2 (Lapata, 2006) found that human ratings are significantly correlated with self-paced reading times, a more direct measure of processing effort which we plan to explore in future work. After collecting the data, it turned out that most of the time subjects gave the same rating to all questions. For competent language users, we view text readability and text coherence as equivalent properties, measuring the extent to which a text is well written. Thus for all subsequent analysis, we will use only the first question (“On a scale of 1 to 5, how well written is this text?”). The score of an article was then the"
D08-1020,P00-1052,0,0.0206574,"three very different corpora, (Karamanis et al., to appear) assessed the performance of various formulations of centering. Their results were somewhat unexpected, showing that while centering transition preferences were useful, the most successful strategy for information ordering was based on avoiding rough shifts, that is, sequences of sentences that share no entities in common. This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non-discourse features (Miltsakaki and Kukich, 2000). In a more powerful generalization of centering, Barzilay and Lapata (2008) developed a novel approach which doesn’t postulate a preference for any type of transition but rather computes a set of features that capture transitions of all kinds in the text and their relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is per"
D08-1020,N03-2024,1,0.624302,"heir relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the larges"
D08-1020,C08-2022,1,0.631069,".0068 r = -.2729, p = .1445 r = .5409, p = .0020 r = .3819, p = .0373 r = .1528, p = .4203 r = .2403, p = .2009 Table 6: Discourse features The likelihood of discourse relations in the text under a multinomial model is very highly and significantly correlated with readability ratings, especially after text length is taken into account. Cor192 relations are 0.48 and 0.54 respectively. The probability of the explicit relations alone is not a sufficiently strong indicator of readability. This fact is disappointing as the explicit relations can be identified much more easily in unannotated text (Pitler et al., 2008). Note that the sequence of just the implicit relations is also not sufficient. This observation implies that the proportion of explicit and implicit relations may be meaningful but we leave the exploration of this issue for later work. 4.7 Summary of findings So far, we introduced six classes of factors that have been discussed in the literature as readability correlates. Through statistical tests of associations we identified the individual factors significantly correlated with readability ratings. These are, in decreasing order of association strength: LogL of Discourse Relations (r = .4835"
D08-1020,J98-2001,0,0.0232693,"l kinds in the text and their relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse"
D08-1020,prasad-etal-2008-penn,0,0.0301531,"ious readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives. 3.1 Discourse annotation The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal 188 portion of the Penn Treebank (Marcus et al., 1994). All explicit relations (those marked with a discourse connective) are annotated. In addition, each adjacent pair of sentences within a paragraph is annotated. If there is a discourse relation, then it is marked implicit and annotated with one or more connectives. If there is a relation between the sentences but adding a connective would be inappropriate, it is marked AltLex. If the consecutive sentences are only related by entity-based"
D08-1020,P05-1065,0,0.910292,"e that discourse relations are the one class of features that exhibits robustness across these two tasks. 1 Ani Nenkova Computer and Information Science University of Pennsylvania Philadelphia, PA 19104, USA nenkova@seas.upenn.edu Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readability for a given intended audience. The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007). Syntactic complexity is associated with delayed processing time in understanding (Gibson, 1998) and is another factor that can decrease readability. Text organization (discourse structure), topic development (entity coherence) and the form of referring expressions also determine readability. But we know little about the relative importance of each factor and how they combine in determining perceived text quality. Introduction The quest for a precise definition of text quality— pinpointing the factors that make text flow and easy to read—has a long history and trad"
D08-1020,J05-2005,0,0.0275687,"s first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives. 3.1 Discourse annotation The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal 188 portion of the Penn Treebank (Marcus et al., 1994). All explicit relations (those marked with a discourse connective) are annotated. In addition, each adjacent pair of sentences w"
D08-1020,N07-1058,0,\N,Missing
D08-1020,J93-2004,0,\N,Missing
D08-1020,W07-1007,0,\N,Missing
D08-1020,J09-1003,0,\N,Missing
D09-1032,C08-1019,0,0.0843761,"Missing"
D09-1032,P06-2020,0,0.231088,"Missing"
D09-1032,W00-0408,0,0.0887345,"nk documents according to their relevance to a given query. The summaries for each document were also ranked for relevance with respect to the same query. For good summarization systems, the relevance ranking of summaries is expected to be similar to that of the full documents. Based on this intuition, the correlation between relevance rankings of summaries and original documents was used to compare the different systems. The approach was motivated by the assumption that the distribution of terms in a good summary is similar to the distribution of terms in the original document. Even earlier, Donaway et al. (2000) suggested that there are considerable benefits to be had in adopting model-free methods of evaluation involving direct comparisons between the original document and its summary. The motivation for their work was the considerable variation in content selection choices in model summaries (Rath et al., 1961). The identity of the model writer significantly affects summary evaluations (also noted by McKeown et al. (2001), Jing et al. (1998)) and evaluations of the same systems can be rather different when different models are used. In their experiments, Donaway et al. (2000) demonstrated that the"
D09-1032,C00-1072,0,0.15036,"rs contain only topic signatures from the input and all words of the summary based on two different human models is about the same as the difference between system ranking based on one model summary and the ranking produced using input-summary similarity. Inputs and summaries were compared using only one metric: cosine similarity. Kullback Leibler (KL) divergence: The KL divergence between two probability distributions P and Q is given by D(P ||Q) = X pP (w) log2 w pP (w) pQ (w) Topic signatures are words highly descriptive of the input, as determined by the application of loglikelihood test (Lin and Hovy, 2000). Using only topic signatures from the input to represent text is expected to be more accurate because the reduced vector has fewer dimensions compared with using all the words from the input. (1) 4.2 Summary likelihood The likelihood of a word appearing in the summary is approximated as being equal to its probability in the input. We compute both a summary’s unigram probability as well as its probability under a multinomial model. Unigram summary probability: It is defined as the average number of bits wasted by coding samples belonging to P using another distribution Q, an approximate of P ."
D09-1032,N03-1020,0,0.24138,"an be used during system development when human model summaries are not available. Our results provide validation of several features that can be optimized in the development of new summarization systems when the objective is to improve content selection on average, over a collection of test inputs. However, none of the features is consistently predictive of good summary content for individual inputs. 1 Introduction The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric ROUGE (Lin, 2004; Lin and Hovy, 2003). ROUGE compares system summaries against one or more model summaries by computing n-gram word overlaps between the two. The wide adoption of such automatic measures is understandable because they are convenient and greatly reduce the complexity of evaluations. ROUGE scores also correlate well with manual evaluations of content based on comparison with a single model summary, as used in the early editions of the Document Understanding Conferences (Over et al., 2007). In our work, we take the idea of automatic evaluation to an extreme and explore the feasibility of developing a fully automatic"
D09-1032,N06-1059,0,0.1432,"and summary words were stopword filtered and stemmed before computing the features. 4.1 Distributional Similarity Measures of similarity between two probability distributions are a natural choice for the task at hand. One would expect good summaries to be characterized by low divergence between probability distributions of words in the input and summary, and by high similarity with the input. We experimented with three common measures: KL and Jensen Shannon divergence and cosine similarity. These three metrics have already been applied for summary evaluation, albeit in different contexts. In Lin et al. (2006), KL and JS divergences between human and machine summary distributions were used to evaluate content selection. The study found that JS divergence always outperformed KL divergence. Moreover, the performance of JS divergence was better than standard ROUGE scores for multi-document summarization when multiple human models were used for the comparison. The use of cosine similarity in Donaway et al. (2000) is more directly related to our work. They show that the difference between evaluations 3 The scores were computed after stemming but stop words were retained in the summaries. 308 2. Vectors"
D09-1032,W04-1013,0,0.348336,"measures can be used during system development when human model summaries are not available. Our results provide validation of several features that can be optimized in the development of new summarization systems when the objective is to improve content selection on average, over a collection of test inputs. However, none of the features is consistently predictive of good summary content for individual inputs. 1 Introduction The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric ROUGE (Lin, 2004; Lin and Hovy, 2003). ROUGE compares system summaries against one or more model summaries by computing n-gram word overlaps between the two. The wide adoption of such automatic measures is understandable because they are convenient and greatly reduce the complexity of evaluations. ROUGE scores also correlate well with manual evaluations of content based on comparison with a single model summary, as used in the early editions of the Document Understanding Conferences (Over et al., 2007). In our work, we take the idea of automatic evaluation to an extreme and explore the feasibility of developi"
D09-1032,N04-1019,1,0.864716,"herence. We do not make use of any of the linguistic quality evaluations. Our work focuses on fully automatic evaluation of content selection, so manual pyramid and responsiveness scores are used for comparison with our automatic method. The pyramid metric measures content selection exclusively, while responsiveness incorporates at least some aspects of linguistic quality. Table 1: Spearman correlation between manual scores and ROUGE-1 and ROUGE-2 recall. All correlations are highly significant with p-value &lt; 0.00001. produced by the systems. Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al., 2007). It uses multiple human models from which annotators identify semantically defined Summary Content Units (SCU). Each SCU is assigned a weight equal to the number of human model summaries that express that SCU. An ideal maximally informative summary would express a subset of the most highly weighted SCUs, with multiple maximally informative summaries being possible. The pyramid score for a system summary is equal to"
D09-1032,P03-1048,0,0.0510705,"Missing"
D09-1032,H05-1014,0,0.00941913,"he 2008 Text Analysis Conference (TAC)2 . Query-focused summaries were produced from input documents in response to a stated user information need. The update summaries require more sophistication: two sets of articles on the same topic are provided. The first set of articles represents the background of a story and users are assumed to be already familiar with the information contained in them. The update task is to produce a multi-document summary from the second set of articles that can serve as an update to the user. This task is reminiscent of the novelty detection task explored at TREC (Soboroff and Harman, 2005). 3.2 Data The test set for the TAC 2008 summarization task contains 48 inputs. Each input consists of two sets of 10 documents each, called docsets A and B. Both A and B are on the same general topic but B contains documents published later than those in A. In addition, the user’s information need associated with each input is given by a query statement consisting of a title and narrative. An example query statement is shown below. Title: Airbus A380 Narrative: Describe developments in the production and launch of the Airbus A380. A system must produce two summaries: (1) a query-focused summa"
D12-1004,N09-1006,0,0.27846,"Missing"
D12-1004,W10-4346,0,0.237075,"Missing"
D12-1004,W11-0610,0,0.180764,"Missing"
D12-1004,N03-1033,0,0.00626245,"Missing"
D12-1106,J08-1001,0,0.712522,"Missing"
D12-1106,N04-1015,0,0.327173,"ference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intenti"
D12-1106,W08-2102,0,0.011,"ause in p2 . Here the intentional structure is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3 Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are part of speech tags. Productions: In this representation we view each sentence as the set of grammatica"
D12-1106,P05-1022,0,0.0181966,"organization. The next sentence has a quote from that person, where the quotation forms the topicalized clause in p2 . Here the intentional structure is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3 Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are p"
D12-1106,D10-1003,0,0.0137417,"e for syntactic coherence We first present a pilot study that confirms that adjacent sentences in discourse exhibit stable patterns of syntactic co-occurrence. This study validates our second assumption relating the syntax of adjacent sentences. Later in Section 6, we examine syntactic patterns in individual sentences (assumption 1) using a corpus of academic articles where sentences were manually annotated with communicative goals. Prior work has reported that certain grammatical productions are repeated in adjacent sentences more often than would be expected by chance (Reitter et al., 2006; Cheung and Penn, 2010). We analyze all co-occurrence patterns rather than just repetitions. We use the gold standard parse trees from the Penn Treebank (Marcus et al., 1994). Our unit of analysis is a pair of adjacent sentences (S1 , S2 ) and we choose to use Section 0 of the corpus which has 99 documents and 1727 sentence pairs. We enumerate all productions that appear in the syntactic parse of any sentence and exclude those that appear less than 25 times, resulting in a list of 197 unique productions. Then all ordered pairs2 (p1 , p2 ) of productions are formed. For each pair, we compute 2 (p1 , p2 ) and (p2 , p1"
D12-1106,R11-1059,0,0.038123,"Missing"
D12-1106,J05-1003,0,0.0193848,"nce has a quote from that person, where the quotation forms the topicalized clause in p2 . Here the intentional structure is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3 Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are part of speech tags. Prod"
D12-1106,councill-etal-2008-parscit,0,0.0187609,"rvation, Experiment, Motivation, Model, Hypothesis. For our study, we use the annotation of the introduction and the abstract sections. We divide the data into training, development and test sets. For abstracts, we have 75, 50 and 100 for these sets respectively. For introductions, this split is 75, 31, 82.6 ACL Anthology Network (AAN) Corpus: Radev et al. (2009) provides the full text of publications from ACL venues. These articles do not have any zone annotations. The AAN corpus is produced from OCR analysis and no section marking is available. To recreate these, we use the Parscit tagger7 (Councill et al., 2008). We use articles from years 1999 to 2011. For training, we randomly choose 70 articles from ACL and NAACL main conferences. Similarly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles in which all three sections—abstract, introduction and related work— 6 Some articles did not have labelled ‘introduction’ sections resulting in fewer examples for this setup. 7 http://aye.comp.nus.edu.sg/parsCit/ 1164 could be successfully identified using"
D12-1106,P08-2011,0,0.653865,"Missing"
D12-1106,P11-2022,0,0.567307,"Missing"
D12-1106,N07-1055,0,0.866753,"here wi and wj are syntactic items and c(wi , wj ) is the number of sentences that contain the item wi immediately followed by a sentence that contains wj . |V |is the vocabulary size for syntactic items. 3.2.2 Global structure Now we turn to a global coherence approach that implements the assumption that sentences with similar syntax have the same communicative goal as well as captures the patterns in communicative goals in the discourse. This approach uses a Hidden Markov Model (HMM) which has been a popular implementation for modeling coherence (Barzilay and Lee, 2004; Fung and Ngai, 2006; Elsner et al., 2007). The hidden states in our model depict communicative goals by encoding a probability distribution over syntactic items. This distribution gives higher weight to syntactic items that are more likely for that communicative goal. Transitions between states record the common patterns in intentional structure for the domain. In this syntax-HMM, states hk are created by clustering the sentences from the documents in the training set by syntactic similarity. For the productions representation of syntax, the features for clustering are the number of times a given production appeared in the parse of t"
D12-1106,J86-3001,0,0.514503,"of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on. As a result each sentence in the article has a communicative goal and the sequence of goals helps t"
D12-1106,D11-1025,0,0.0217339,"er et al., 2007) and using discriminative training with different objectives (Soricut and Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labels: Result, Conclusion, Objective, Method, Goal, Background, Observation, Experiment, Motivation, Model, Hy"
D12-1106,P08-1067,0,0.0125096,"is INTRODUCE X / STATEMENT BY X. In the remainder of the paper we formalize our representation of syntax and the derived model of coherence and test its efficacy in three domains. 3 Coherence models using syntax We first describe the two representations of sentence structure we adopted for our analysis.3 Next, we 3 Our representations are similar to features used for reranking in parsing. Our first representation corresponds to “rules” features (Charniak and Johnson, 2005; Collins and Koo, 2005), and our second representation is related to “spines” (Carreras et al., 2008) and edge annotation(Huang, 2008). present two coherence models: a local model which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax. 3.1 Representing syntax Our models rely exclusively on syntactic cues. We derive representations from constituent parses of the sentences, and terminals (words) are removed from the parse tree before any processing is done. The leaf nodes in our parse trees are part of speech tags. Productions: In this representation we view each sentence as the set of grammatical productions, LHS → RHS, which a"
D12-1106,J09-1003,0,0.018057,"Missing"
D12-1106,P03-1054,0,0.00570919,"arly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles in which all three sections—abstract, introduction and related work— 6 Some articles did not have labelled ‘introduction’ sections resulting in fewer examples for this setup. 7 http://aye.comp.nus.edu.sg/parsCit/ 1164 could be successfully identified using Parscit.8 This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the Stanford Parser (Klein and Manning, 2003). For each corpus and each section, we train all our syntactic models: the two local coherence models using the production and d-sequence representations and the HMM models with the two representations. These models are tuned on the respective development data, on the task of differentiating the original from a permuted section. For this purpose, we created a maximum of 30 permutations per article. 6.1 Comparison with ART Corpus zones We perform this analysis using the ART corpus. The zone annotations present in this corpus allow us to directly test our first assumption in this work, that sent"
D12-1106,P03-1069,0,0.260847,"d work sections of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a"
D12-1106,liakata-etal-2010-corpora,0,0.0113564,"we tested for combination. In prior work, content and entity grid methods have been combined generatively (Elsner et al., 2007) and using discriminative training with different objectives (Soricut and Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labe"
D12-1106,D09-1036,0,0.0591354,"Missing"
D12-1106,P11-1100,0,0.416454,"l automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on. As a result each sentence in the ar"
D12-1106,D08-1020,1,0.432459,"have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and so on. As a result each"
D12-1106,A97-1004,0,0.0332239,"domly choose 70 articles from ACL and NAACL main conferences. Similarly, we obtain a development corpus of 36 ACL-NAACL articles. We create two test sets: one has 500 ACL-NAACL conference articles and another has 500 articles from ACL-sponsored workshops. We only choose articles in which all three sections—abstract, introduction and related work— 6 Some articles did not have labelled ‘introduction’ sections resulting in fewer examples for this setup. 7 http://aye.comp.nus.edu.sg/parsCit/ 1164 could be successfully identified using Parscit.8 This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the Stanford Parser (Klein and Manning, 2003). For each corpus and each section, we train all our syntactic models: the two local coherence models using the production and d-sequence representations and the HMM models with the two representations. These models are tuned on the respective development data, on the task of differentiating the original from a permuted section. For this purpose, we created a maximum of 30 permutations per article. 6.1 Comparison with ART Corpus zones We perform this analysis using the ART corpus. The zone annotations present in this corpus allow us"
D12-1106,P06-2103,0,0.761263,"s of these articles. 1 Introduction Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2008) and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items (Barzilay and Lee, 2004; Fung and Ngai, 2006). Other work has shown that co-occurrence of words (Lapata, 2003; Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008; Lin et al., 2011) also predict coherence. Early theories (Grosz and Sidner, 1986) posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments. The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected. Every discourse has a purpose: explaining a concept, narrating an eve"
D12-1106,W00-1302,0,0.147745,"bined generatively (Elsner et al., 2007) and using discriminative training with different objectives (Soricut and Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assigned one of 11 zone labels: Result, Conclusion, Objective, Method, Goal, Background, Observation, Experiment, Mo"
D12-1106,E99-1015,0,0.0194253,"simple approach that we tested for combination. In prior work, content and entity grid methods have been combined generatively (Elsner et al., 2007) and using discriminative training with different objectives (Soricut and Marcu, 2006). Such approaches might bring out the complementary strengths of the different aspects better and we leave such analysis for future work. 6 Predictions on academic articles The distinctive intentional structure of academic articles has motivated several proposals to define and annotate the communicative purpose (argumentative zone) of each sentence (Swales, 1990; Teufel et al., 1999; Liakata et al., 2010). Supervised classifiers were also built to identify these zones (Teufel and Moens, 2000; Guo et al., 2011). So we expect that these articles form a good testbed for our models. In the remainder of the paper, we examine how unsupervised patterns discovered by our approach relate to zones and how well our models predict coherence for articles from this genre. We employ two corpora of scientific articles. ART Corpus: contains a set of 225 Chemistry journal articles that were manually annotated for intentional structure (Liakata and Soldatova, 2008). Each sentence was assig"
D12-1106,J93-2004,0,\N,Missing
D15-1011,N09-1041,0,0.172063,"Missing"
D15-1011,D10-1047,0,0.505499,"cessing, pages 107–117, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. systems and system identity information is utilized during the re-ranking process. The signal from the original input is ignored. Our method handles these limitations. Our method derives an overall informativeness score for each candidate summary, then selects the one with the highest score. This is related to the growing body of research in global optimization, which selects the most informative subset of sentences towards a global objective (McDonald, 2007; Gillick et al., 2009; Aker et al., 2010). Some work uses integer linear programming to find the exact solution (Gillick et al., 2009; Li et al., 2015), other work employs supervised methods to optimize the ROUGE scores of a summary (Lin and Bilmes, 2011; Kulesza and Taskar, 2012). Here we use the ROUGE scores of the candidate summaries as labels while training our model. In our work, we propose novel features that encode the content quality of the entire summary. Though prior work has extensively investigated features that are indicative of important words (Yih et al., 2007; Hong and Nenkova, 2014) or sentences (Litvak et al., 2010;"
D15-1011,W99-0623,0,0.0606257,"sed on different sources. We verify the effectiveness of these features. • Our method outperforms the basic systems and several competitive baselines. Our model achieves competitive performance on six DUC/TAC datasets, which is on par with the state-of-the-art on most of these datasets. • Our method can be used to combine summaries generated by any systems. 2 Related Work System combination has enjoyed great success in many domains, such as automatic speech recognition (Fiscus, 1997; Mangu et al., 2000), machine translation (Frederking and Nirenburg, 1994; Bangalore et al., 2001) and parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006). However, only a handful of papers have leveraged this idea for summarization. Mohamed and Rajasekaran (2005) present a method that relies on a document graph (DG), which includes concepts connected by relations. This method selects among the outputs of the basic systems, based on their overlaps with the input in terms of DG. Thapar et al. (2006) propose to iteratively include sentences, based on the overlap of DG between the current sentence and (1) the original input, or (2) the basic summaries. However, in both papers, the machine summaries are not compared against"
D15-1011,P13-1020,0,0.0205117,"and Taskar, 2012), RegSum (Hong and Nenkova, 2014)) on the DUC 04 data. On the DUC 01 and 02 data, the top performing systems we find are R2N2 ILP (Cao et al., 2015a) and PriorSum (Cao et al., 2015b); both of them utilize neural networks. Comparing to these two, SumCombine achieves a lower performance on the DUC 01 data and a higher performance on the DUC 02 data. It also has a slightly lower R-1 and a higher R-2 compared to ClusterCMRW (Wan and Yang, 2008), a graph-based system that achieves the highest R-1 on the DUC 02 data. On the TAC 08 data, the top performing systems (Li et al., 2013; Almeida and Martins, 2013) achieve the state-of-the-art performance by sentence compression. Our model performs extractive summarization, but still has similar R-2 compared to theirs.8 On the TAC 09 data, the best system uses a supervised method that weighs bigrams in the ILP framework by leveraging external resources (Li et al., 2015). This system is better than ours on the TAC 09 data and is inferior to ours on the TAC 08 data. Overall, our combination model achieves very competitive performance, comparable to the state-of-the-art on multiple benchmarks. At last, we compare SumCombine to SSA (Pei et al., 2012) and WC"
D15-1011,E14-1075,1,0.899265,"(McDonald, 2007; Gillick et al., 2009; Aker et al., 2010). Some work uses integer linear programming to find the exact solution (Gillick et al., 2009; Li et al., 2015), other work employs supervised methods to optimize the ROUGE scores of a summary (Lin and Bilmes, 2011; Kulesza and Taskar, 2012). Here we use the ROUGE scores of the candidate summaries as labels while training our model. In our work, we propose novel features that encode the content quality of the entire summary. Though prior work has extensively investigated features that are indicative of important words (Yih et al., 2007; Hong and Nenkova, 2014) or sentences (Litvak et al., 2010; Ouyang et al., 2011), little work has focused on designing global features defined over the summary. Indeed, even for the papers that employ supervised methods to conduct global inference, the features are defined on the sentence level (Aker et al., 2010; Kulesza and Taskar, 2012). The most closely related papers are the ones that investigated automatic evaluation of summarization without human references (Louis and Nenkova, 2009; Saggion et al., 2010), where the effectiveness of several summary-input similarity metrics are examined. In our work, we propose"
D15-1011,hong-etal-2014-repository,1,0.583638,"tems. We choose these systems, because: First, these systems are either off-the-shelf or easy-to-implement. Second, even though many systems have been proposed for multi-document summarization, the output of them are often available only on one dataset or even unavailable. Third, compared to more sophisticated supervised methods (Kulesza and Taskar, 2012; Cao et al., 2015a), simple unsupervised methods perform unexpectedly well. Many of them achieved the state-of-the-art performance when they were proposed (Erkan and Radev, 2004; Gillick et al., 2009) and still serve as competitive baselines (Hong et al., 2014). After the summarizers have been chosen, we present a two-step pipeline that combines the basic summaries. In the first step, we generate combined candidate summaries (Section 4). We investigate two methods to do this: one uses entire basic summaries directly, the other combines these summaries on the sentence level. We show that the latter method has a much higher oracle performance. The second step includes a new supervised model that selects among the candidate summaries (Section 5). Our contributions are: We present a novel framework of system combination for multi-document summarization."
D15-1011,P15-2136,0,0.11498,"successfully generating better summaries by combining the summaries from different systems (i.e., basic summaries). This paper focuses on practical system combination, where we combine the summaries generated by four portable unsupervised systems. We choose these systems, because: First, these systems are either off-the-shelf or easy-to-implement. Second, even though many systems have been proposed for multi-document summarization, the output of them are often available only on one dataset or even unavailable. Third, compared to more sophisticated supervised methods (Kulesza and Taskar, 2012; Cao et al., 2015a), simple unsupervised methods perform unexpectedly well. Many of them achieved the state-of-the-art performance when they were proposed (Erkan and Radev, 2004; Gillick et al., 2009) and still serve as competitive baselines (Hong et al., 2014). After the summarizers have been chosen, we present a two-step pipeline that combines the basic summaries. In the first step, we generate combined candidate summaries (Section 4). We investigate two methods to do this: one uses entire basic summaries directly, the other combines these summaries on the sentence level. We show that the latter method has a"
D15-1011,N10-1133,0,0.210859,"y There does not exist a system that always outperforms the others for all problems. Based on this fact, we directly use the summary outputs (i.e., basic summaries) as the candidate summaries. 4.2.2 Sentence Level Combination Different systems provide different pieces of the correct answer. Based on this fact, the combined summary should include sentences that appear in the summaries produced by different systems. Here we exhaustively enumerate sentences so that to form the candidate summaries. A similar approach has been used to generate candidate summaries for single-document summarization (Ceylan et al., 2010). Let D = s1 , . . . , sn denote the sequence of unique sentences that appear in the basic summaries. We enumerate all subsequences Ai = si1 , . . . , sik of D in lexicographical order. Ai can P be used as a candidate summary iff kj=1 l(sij ) ≥ Pk−1 L and j=1 l(sij ) < L, where l(s) is the number of words in s and L is the predefined summary length. Table 2 shows the average number of (unique) sentences and summaries that are generated per input. Four Basic Unsupervised Systems ICSISumm: This system (Gillick et al., 2009) optimizes the coverage of bigrams weighted by their document frequency w"
D15-1011,P06-2020,0,0.0134752,"Missing"
D15-1011,D13-1047,0,0.0253216,"Missing"
D15-1011,N15-1079,0,0.0797814,". systems and system identity information is utilized during the re-ranking process. The signal from the original input is ignored. Our method handles these limitations. Our method derives an overall informativeness score for each candidate summary, then selects the one with the highest score. This is related to the growing body of research in global optimization, which selects the most informative subset of sentences towards a global objective (McDonald, 2007; Gillick et al., 2009; Aker et al., 2010). Some work uses integer linear programming to find the exact solution (Gillick et al., 2009; Li et al., 2015), other work employs supervised methods to optimize the ROUGE scores of a summary (Lin and Bilmes, 2011; Kulesza and Taskar, 2012). Here we use the ROUGE scores of the candidate summaries as labels while training our model. In our work, we propose novel features that encode the content quality of the entire summary. Though prior work has extensively investigated features that are indicative of important words (Yih et al., 2007; Hong and Nenkova, 2014) or sentences (Litvak et al., 2010; Ouyang et al., 2011), little work has focused on designing global features defined over the summary. Indeed,"
D15-1011,P11-1052,0,0.376765,"m the original input is ignored. Our method handles these limitations. Our method derives an overall informativeness score for each candidate summary, then selects the one with the highest score. This is related to the growing body of research in global optimization, which selects the most informative subset of sentences towards a global objective (McDonald, 2007; Gillick et al., 2009; Aker et al., 2010). Some work uses integer linear programming to find the exact solution (Gillick et al., 2009; Li et al., 2015), other work employs supervised methods to optimize the ROUGE scores of a summary (Lin and Bilmes, 2011; Kulesza and Taskar, 2012). Here we use the ROUGE scores of the candidate summaries as labels while training our model. In our work, we propose novel features that encode the content quality of the entire summary. Though prior work has extensively investigated features that are indicative of important words (Yih et al., 2007; Hong and Nenkova, 2014) or sentences (Litvak et al., 2010; Ouyang et al., 2011), little work has focused on designing global features defined over the summary. Indeed, even for the papers that employ supervised methods to conduct global inference, the features are define"
D15-1011,C00-1072,0,0.0352466,"al., 2012). R-1 and R-2 are the most widely used metrics in summarization literature. 4 ProbSum: This system (Nenkova et al., 2006) scores a sentence by taking the average of word probabilities over the words in the sentence, with stopwords assigned zero weights. Compared to Nenkova et al. (2006), we slightly change the way of handling redundancy: we iteratively include a sentence into the summary if its cosine similarity with any sentence in the summary does not exceed 0.5.3 LLRSum: This system (Conroy et al., 2006) employs a log-likelihood ratio (LLR) test to select topic words of an input (Lin and Hovy, 2000). The LLR test compares the distribution of words in the input to a large background corpus. Similar to Conroy et al. (2006), we consider words as topic words if their χ-square statistic derived by LLR exceeds 10. The sentence importance score is equal to the number of topic words divided by the number of words in the sentence. Redundancy is handled in the same way as in ProbSum. Generating Candidate Summaries We first introduce the four basic unsupervised systems, then describe our approach of generating candidate summaries. The four systems all perform extractive summarization, which directl"
D15-1011,A94-1016,0,0.451945,"om the New York Times (NYT) corpus (Sandhaus, 2008). perspectives, based on different sources. We verify the effectiveness of these features. • Our method outperforms the basic systems and several competitive baselines. Our model achieves competitive performance on six DUC/TAC datasets, which is on par with the state-of-the-art on most of these datasets. • Our method can be used to combine summaries generated by any systems. 2 Related Work System combination has enjoyed great success in many domains, such as automatic speech recognition (Fiscus, 1997; Mangu et al., 2000), machine translation (Frederking and Nirenburg, 1994; Bangalore et al., 2001) and parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006). However, only a handful of papers have leveraged this idea for summarization. Mohamed and Rajasekaran (2005) present a method that relies on a document graph (DG), which includes concepts connected by relations. This method selects among the outputs of the basic systems, based on their overlaps with the input in terms of DG. Thapar et al. (2006) propose to iteratively include sentences, based on the overlap of DG between the current sentence and (1) the original input, or (2) the basic summaries. However,"
D15-1011,W04-1013,0,0.0184016,"h both methods show advantages over the basic systems, they have two limitations. Most importantly, only summarizers that assign importance scores to each sentence can be used as the input summarizers. Second, only the sentence scores (ranks) from the basic 3 Data and Evaluation We conduct a large scale experiment on six datasets from the Document Understanding Conference (DUC) and the Text Analysis Conference (TAC). The tasks include generic (DUC 2001–2004) and query-focused (TAC 2008, 2009) multi-document summarization. We evaluate on the task of generating 100-word summaries. We use ROUGE (Lin, 2004) for automatic 108 evaluation, which compares the machine summaries to the human references. We report ROUGE-1 (unigram recall) and ROUGE-2 (bigram recall), with stemming and stopwords included.1 Among automatic evaluation metrics, ROUGE-1 (R-1) can predict that one system performs significantly better than the other with the highest recall (Rankel et al., 2013). ROUGE-2 (R-2) provides the best agreement with manual evaluations (Owczarzak et al., 2012). R-1 and R-2 are the most widely used metrics in summarization literature. 4 ProbSum: This system (Nenkova et al., 2006) scores a sentence by t"
D15-1011,P07-2049,1,0.354959,"lso derive features based on summary-article pairs from the NYT corpus. Frequency related features: For each n-gram t, we compute its probability, TF*IDF4 , document frequency (DF) and χ-square statistic from LLR test. Another feature is set to be equal to DF normalized by the number of input documents. A binary feature is set to determine whether DF is at least three, inspired by the observation that document specific words should not be regarded as informative (Mason and Charniak, 2011). It has been shown that unimportant words of an input should not be considered while scoring the summary (Gupta et al., 2007; Mason and Charniak, 2011). The features below are designed (1) (2) where A is the average of P and Q. Noticing that KL divergence is not symmetric, both KL(P k Q) and KL(Q k P ) are computed. In particular, smoothing is performed while computing KL(Q k P ), where we use the same setting as in Louis and Nenkova (2013). Topic words: Good summaries tend to include more topic words (TWs). We derive TWs using the method described in the LLRSum system in Section 4.1. For each summary S, we compute: (1) the ratio of the words that are TWs to all words in S; (2) the recall of TWs in S. Sentence loca"
D15-1011,P10-1095,0,0.0386964,"; Aker et al., 2010). Some work uses integer linear programming to find the exact solution (Gillick et al., 2009; Li et al., 2015), other work employs supervised methods to optimize the ROUGE scores of a summary (Lin and Bilmes, 2011; Kulesza and Taskar, 2012). Here we use the ROUGE scores of the candidate summaries as labels while training our model. In our work, we propose novel features that encode the content quality of the entire summary. Though prior work has extensively investigated features that are indicative of important words (Yih et al., 2007; Hong and Nenkova, 2014) or sentences (Litvak et al., 2010; Ouyang et al., 2011), little work has focused on designing global features defined over the summary. Indeed, even for the papers that employ supervised methods to conduct global inference, the features are defined on the sentence level (Aker et al., 2010; Kulesza and Taskar, 2012). The most closely related papers are the ones that investigated automatic evaluation of summarization without human references (Louis and Nenkova, 2009; Saggion et al., 2010), where the effectiveness of several summary-input similarity metrics are examined. In our work, we propose a wide range of features. These fe"
D15-1011,P13-2024,1,0.890841,"rstanding Conference (DUC) and the Text Analysis Conference (TAC). The tasks include generic (DUC 2001–2004) and query-focused (TAC 2008, 2009) multi-document summarization. We evaluate on the task of generating 100-word summaries. We use ROUGE (Lin, 2004) for automatic 108 evaluation, which compares the machine summaries to the human references. We report ROUGE-1 (unigram recall) and ROUGE-2 (bigram recall), with stemming and stopwords included.1 Among automatic evaluation metrics, ROUGE-1 (R-1) can predict that one system performs significantly better than the other with the highest recall (Rankel et al., 2013). ROUGE-2 (R-2) provides the best agreement with manual evaluations (Owczarzak et al., 2012). R-1 and R-2 are the most widely used metrics in summarization literature. 4 ProbSum: This system (Nenkova et al., 2006) scores a sentence by taking the average of word probabilities over the words in the sentence, with stopwords assigned zero weights. Compared to Nenkova et al. (2006), we slightly change the way of handling redundancy: we iteratively include a sentence into the summary if its cosine similarity with any sentence in the summary does not exceed 0.5.3 LLRSum: This system (Conroy et al., 2"
D15-1011,D09-1032,1,0.659008,"entire summary. Though prior work has extensively investigated features that are indicative of important words (Yih et al., 2007; Hong and Nenkova, 2014) or sentences (Litvak et al., 2010; Ouyang et al., 2011), little work has focused on designing global features defined over the summary. Indeed, even for the papers that employ supervised methods to conduct global inference, the features are defined on the sentence level (Aker et al., 2010; Kulesza and Taskar, 2012). The most closely related papers are the ones that investigated automatic evaluation of summarization without human references (Louis and Nenkova, 2009; Saggion et al., 2010), where the effectiveness of several summary-input similarity metrics are examined. In our work, we propose a wide range of features. These features are derived not only based on the input, but also based on the basic summaries and the summary-input pairs from the New York Times (NYT) corpus (Sandhaus, 2008). perspectives, based on different sources. We verify the effectiveness of these features. • Our method outperforms the basic systems and several competitive baselines. Our model achieves competitive performance on six DUC/TAC datasets, which is on par with the state-"
D15-1011,J13-2002,1,0.928868,"multiple times, the earliest position is used. Features are then set on the summary level, which equal to the mean of their corresponding features on the sentence level over all sentences in the summary S. Redundancy: Redundancy correlates negatively with content quality (Pitler et al., 2010). To indicate redundancy, we compute the maximum and average cosine similarity of all pairs of sentences in the summaries. Summaries with higher redundancy are expected to score higher. Summary level features directly encode the informativeness of the entire summary. Some of them are initially proposed in Louis and Nenkova (2013) that evaluates the summary content without human models. Different from them, the features in our work use not only I, but also H as the “input” (except for the redundancy features). “Input” refers to I or H in the rest of Section 5. Distributional Similarity: These features compute the distributional similarity (divergence) between the n-gram (n = 1, 2) probability distribution of the summary and that of the input (I or H). Good summaries tend to have high similarity and low divergence. We use three measures: Kullback-Leibler (KL) divergence, Jenson-Shannon (JS) divergence and cosine similar"
D15-1011,N06-2033,0,0.0238748,"e verify the effectiveness of these features. • Our method outperforms the basic systems and several competitive baselines. Our model achieves competitive performance on six DUC/TAC datasets, which is on par with the state-of-the-art on most of these datasets. • Our method can be used to combine summaries generated by any systems. 2 Related Work System combination has enjoyed great success in many domains, such as automatic speech recognition (Fiscus, 1997; Mangu et al., 2000), machine translation (Frederking and Nirenburg, 1994; Bangalore et al., 2001) and parsing (Henderson and Brill, 1999; Sagae and Lavie, 2006). However, only a handful of papers have leveraged this idea for summarization. Mohamed and Rajasekaran (2005) present a method that relies on a document graph (DG), which includes concepts connected by relations. This method selects among the outputs of the basic systems, based on their overlaps with the input in terms of DG. Thapar et al. (2006) propose to iteratively include sentences, based on the overlap of DG between the current sentence and (1) the original input, or (2) the basic summaries. However, in both papers, the machine summaries are not compared against human references. Rather"
D15-1011,W11-0507,0,0.0152191,"ms and bigrams. Below we describe the features in vt . Similar to Section 5.1, the features are computed based on both I and H. We also derive features based on summary-article pairs from the NYT corpus. Frequency related features: For each n-gram t, we compute its probability, TF*IDF4 , document frequency (DF) and χ-square statistic from LLR test. Another feature is set to be equal to DF normalized by the number of input documents. A binary feature is set to determine whether DF is at least three, inspired by the observation that document specific words should not be regarded as informative (Mason and Charniak, 2011). It has been shown that unimportant words of an input should not be considered while scoring the summary (Gupta et al., 2007; Mason and Charniak, 2011). The features below are designed (1) (2) where A is the average of P and Q. Noticing that KL divergence is not symmetric, both KL(P k Q) and KL(Q k P ) are computed. In particular, smoothing is performed while computing KL(Q k P ), where we use the same setting as in Louis and Nenkova (2013). Topic words: Good summaries tend to include more topic words (TWs). We derive TWs using the method described in the LLRSum system in Section 4.1. For eac"
D15-1011,E12-1023,0,0.0125797,"Summ and better than the other basic systems on R-2 (see Figure 1 (b) and Table 4). Apart from automatic evaluation, we also manually evaluate the summaries using the Pyramid method (Nenkova et al., 2007). This method solicits annotators to score a summary based on its coverage of summary content units, which are identified from human references. Here we evaluate the Pyramid scores of four systems: our system, two best basic systems and the oracle 5 Recent methods that performs global optimization for summarization mostly use R-1 while training (Lin and Bilmes, 2011; Kulesza and Taskar, 2012; Sipos et al., 2012). 6 We use the SVR model in SVMLight (Joachims, 1999) with linear kernel and default parameter settings when trained on R-1. When trained on R-2, we tune  in loss function on the developmenet set, because the default setting assigns the same value to all data points. 7 We use the SVM-Rank toolkit (Joachims, 2006) with default parameter settings. 113 Dataset DUC 03 DUC 04 DUC 01 DUC 02 TAC 08 TAC 09 System ICSISumm SumCombine ICSISumm SumCombine DPP RegSum ICSISumm SumCombine R2N2 ILP PriorSum ICSISumm SumCombine R2N2 ILP PriorSum ClusterCMRW ICSISumm SumCombine Li et al. (2013) A & M (2013) L"
D15-1011,W12-2601,1,0.882911,"ric (DUC 2001–2004) and query-focused (TAC 2008, 2009) multi-document summarization. We evaluate on the task of generating 100-word summaries. We use ROUGE (Lin, 2004) for automatic 108 evaluation, which compares the machine summaries to the human references. We report ROUGE-1 (unigram recall) and ROUGE-2 (bigram recall), with stemming and stopwords included.1 Among automatic evaluation metrics, ROUGE-1 (R-1) can predict that one system performs significantly better than the other with the highest recall (Rankel et al., 2013). ROUGE-2 (R-2) provides the best agreement with manual evaluations (Owczarzak et al., 2012). R-1 and R-2 are the most widely used metrics in summarization literature. 4 ProbSum: This system (Nenkova et al., 2006) scores a sentence by taking the average of word probabilities over the words in the sentence, with stopwords assigned zero weights. Compared to Nenkova et al. (2006), we slightly change the way of handling redundancy: we iteratively include a sentence into the summary if its cosine similarity with any sentence in the summary does not exceed 0.5.3 LLRSum: This system (Conroy et al., 2006) employs a log-likelihood ratio (LLR) test to select topic words of an input (Lin and Ho"
D15-1011,C12-1136,0,0.323793,"AC datasets, comparable to the state-of-the-art on most datasets. 1 Introduction Recent work shows that state-of-the-art summarization systems generate very different summaries, despite the fact that they have similar performance (Hong et al., 2014). This suggests that combining summaries from different systems might be helpful in improving content quality. A handful of papers have studied system combination for summarization. Based on the ranks of the input sentences assigned by different systems (i.e., basic systems), methods have been proposed to re-rank these sentences (Wang and Li, 2012; Pei et al., 2012). However, these methods require the basic systems to assign importance scores to all input sentences. Thapar et al. (2006) combine the summaries from different systems, based on a graph-based measure that computes summary-input or summary-summary similarity. However, their method does not show • We show that by combining summaries on the sentence level, the best possible (oracle) performance is very high. • In the second step of our pipeline, we propose a supervised model that includes a rich set of new features. These features capture content importance from different 107 Proceedings of the"
D15-1011,P10-1056,1,0.816166,"hest ROUGE-1 (R-1) and ROUGE-2 (R-2) (denoted as SumOracle R-1 and SumOracle R-2). For the second method, we design two oracle systems that pick the best summary in terms of R-1 and R-2 among the summary candidates (denoted as SentOracle R-1 and SentOracle R-2). As shown in 110 5.1 Summary Level Features one sentence appears multiple times, the earliest position is used. Features are then set on the summary level, which equal to the mean of their corresponding features on the sentence level over all sentences in the summary S. Redundancy: Redundancy correlates negatively with content quality (Pitler et al., 2010). To indicate redundancy, we compute the maximum and average cosine similarity of all pairs of sentences in the summaries. Summaries with higher redundancy are expected to score higher. Summary level features directly encode the informativeness of the entire summary. Some of them are initially proposed in Louis and Nenkova (2013) that evaluates the summary content without human models. Different from them, the features in our work use not only I, but also H as the “input” (except for the redundancy features). “Input” refers to I or H in the rest of Section 5. Distributional Similarity: These f"
D15-1011,C10-2122,0,\N,Missing
D15-1148,W14-3338,0,0.0498256,"Missing"
D15-1148,C96-2183,0,0.0999206,"having different rankers for expressing the content in one or multiple sentences. In that case the ranker will need to capture only sentence-level information and the discourse-level decision to use multiple sentences will be treated separately. Text simplification. “Text simplification, defined narrowly, is the process of reducing the linguistic complexity of a text, while still retaining the original information content and meaning” (Siddharthan, 2014). An important aspect of simplification is syntactic transformation in which sentences deemed difficult are re-written as multiple sentences (Chandrasekar et al., 1996; Alu´ısio et al., 2008). Our task may be viewed as identifying sentences in one language that will require simplification when translated, for the benefit of the speakers of the target language. In rule-based simplification systems, splitting is performed al1272 ways when a given syntactic construction such as relative clause, apposition or discourse connective are detected (Chandrasekar et al., 1996; Siddharthan, 2006; De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Cost"
D15-1148,W09-0436,0,0.0614197,"Missing"
D15-1148,W09-2307,0,0.020688,"gth can be an indication of too much content that needs to be 1276 VP VP PU VP Figure 2: Multiple VP structures repackaged into multiple sentences. Therefore as our baseline we train a decision tree using the number of words7 in a Chinese sentence. Sentence structure cues. We collect potential signals for structural complexity: punctuation, conjunctions, prepositional phrases and relative clauses. As features we count the number of commas, conjunction, preposition and postposition part-of-speech tags. In Chinese “DE” often marks prepositional phrases or relative clauses among other functions (Chang et al., 2009a). Here we include a simple count the number of “DEG” tags in the sentence. Dependencies. Dependency grammar captures both syntactic and semantic relationship between words and are shown to improve reordering in MT (Chang et al., 2009b). To account for such relational information we include two feature classes: the percentage of each dependency type and the typed dependency pairs themselves. For the latter we use the universal part-of-speech tags (Petrov et al., 2012) for each word rather than the word itself to avoid too detailed and sparse representations. For example, the relation dobj(处理/"
D15-1148,W11-1601,0,0.0274352,"1996; Alu´ısio et al., 2008). Our task may be viewed as identifying sentences in one language that will require simplification when translated, for the benefit of the speakers of the target language. In rule-based simplification systems, splitting is performed al1272 ways when a given syntactic construction such as relative clause, apposition or discourse connective are detected (Chandrasekar et al., 1996; Siddharthan, 2006; De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012). However simplification by repackaging the content into multiple sentences is not naturally compatible with the standard view of statistical MT in which a system is expected to produce a single output sentence for a single input sentence. Some of the recent systems using MT techniques separately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in mac"
D15-1148,E14-1063,0,0.0322252,"Missing"
D15-1148,W04-1101,0,0.0357726,"s by examining multiple reference translations for each Chinese sentence. We define heavy sentences based on agreement of translator choices and reader preferences. Commas in Chinese. Often a comma in a sentence can be felicitously replaced by a full stop. Such commas offer a straightforward way to split a long sentence into multiple shorter ones by replacing the comma with a full stop. Monolingual text simplification systems often try to identify such commas. They are particularly common in Chinese and replacing them with full stops leads to improvements in the accuracy of syntactic parsing (Jin et al., 2004; Li et al., 2005). Moreover, existing syntactically parsed corpora conveniently provide numerous examples of these full-stop commas, and thus training data for systems to identify them (Xue and Yang, 2011; Yang and Xue, 2012). In this paper, we systematically study the relationship between the presence of full-stop commas in the sentence and whether it is content-heavy for Chinese to English translation. 3 Data In this work we use three news datasets: the newswire portion of the NIST 2012 Open Machine Translation Evaluation (OpenMT) (Group, 2013), Multiple-Translation Chinese (MTC) parts 1-4"
D15-1148,P13-1151,0,0.0257441,"Missing"
D15-1148,P03-1056,0,0.02862,"Missing"
D15-1148,I05-2002,0,0.0419792,"tiple reference translations for each Chinese sentence. We define heavy sentences based on agreement of translator choices and reader preferences. Commas in Chinese. Often a comma in a sentence can be felicitously replaced by a full stop. Such commas offer a straightforward way to split a long sentence into multiple shorter ones by replacing the comma with a full stop. Monolingual text simplification systems often try to identify such commas. They are particularly common in Chinese and replacing them with full stops leads to improvements in the accuracy of syntactic parsing (Jin et al., 2004; Li et al., 2005). Moreover, existing syntactically parsed corpora conveniently provide numerous examples of these full-stop commas, and thus training data for systems to identify them (Xue and Yang, 2011; Yang and Xue, 2012). In this paper, we systematically study the relationship between the presence of full-stop commas in the sentence and whether it is content-heavy for Chinese to English translation. 3 Data In this work we use three news datasets: the newswire portion of the NIST 2012 Open Machine Translation Evaluation (OpenMT) (Group, 2013), Multiple-Translation Chinese (MTC) parts 1-4 (Huang et al., 200"
D15-1148,P14-2047,1,0.837372,"tences in Hindi that need simplification prior to translation. In each of these approaches, the identified sentences are segmented into smaller units. Similar to work in text simplification, the simplification rules are applied to all sentences meeting certain criteria, normally to all sentences longer than a predefined threshold or where certain conjunctions or coordinations are present. In contrast, the model we propose here can be used to predict when segmentation is at all necessary. Our approach to the problem is more compatible with the empirical evidence we presented in our prior work (Li et al., 2014) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality. Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences. This prior work was carried over a dataset containing a single reference translation for each Chinese sentence. In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence. We define heavy sent"
D15-1148,P14-5010,0,0.0062043,"Missing"
D15-1148,W14-5603,0,0.0931268,"al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et al. (2014) used rules involving similar factors to distinguish sentences in Hindi that need simplification prior to translation. In each of these approaches, the identified sentences are segmented into smaller units. Similar to work in text simplification, the simplification rules are applied to all sentences meeting certain criteria, normally to all sentences longer than a predefined threshold or where certain conjunctions or coordinations are present. In contrast, the model we propose here can be used to predict when segmentation is at all necessary. Our approach to the problem is more compatible with"
D15-1148,P14-1041,0,0.0982972,"De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012). However simplification by repackaging the content into multiple sentences is not naturally compatible with the standard view of statistical MT in which a system is expected to produce a single output sentence for a single input sentence. Some of the recent systems using MT techniques separately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et al. (2014) used rules involving similar factors to dis"
D15-1148,P05-1070,0,0.0108427,"anslation, text simplification and Chinese language processing but it is usually addressed in an implicit or application specific way. In contrast, we focus on identifying heavy sentences as a standalone task, providing a unifying view of the seemingly disparate strands of prior work. We now overview the literature which motivated our work. Sentence planning. In text generation, a sentence planner produces linguistic realizations of a list of propositions (Rambow and Korelsky, 1992). One subtask is to decide whether to package the same content into one or more sentences. In the example below (Pan and Shaw, 2005), the multisentence expression B is much easier to process: [A] This is a 1 million dollar 3 bedroom, 2 bathroom, 2000 square foot colonial with 2 acre of land, 2 car garage, annual taxes 8000 dollars in Armonk and in the Byram Hills school district. [B] This is a 3 bedroom, 2 bathroom, 2000 square foot colonial located in Armonk with 2 acres of land. The asking price is 1 million dollar and the annual taxes are 8000 dollars. The house is located in the Byram Hills School District. Identifying sentence [A] as heavy would be useful in selecting the best realization. A crucial difference between"
D15-1148,petrov-etal-2012-universal,0,0.015959,"d postposition part-of-speech tags. In Chinese “DE” often marks prepositional phrases or relative clauses among other functions (Chang et al., 2009a). Here we include a simple count the number of “DEG” tags in the sentence. Dependencies. Dependency grammar captures both syntactic and semantic relationship between words and are shown to improve reordering in MT (Chang et al., 2009b). To account for such relational information we include two feature classes: the percentage of each dependency type and the typed dependency pairs themselves. For the latter we use the universal part-of-speech tags (Petrov et al., 2012) for each word rather than the word itself to avoid too detailed and sparse representations. For example, the relation dobj(处理/handle, 事 情/matter) becomes feature dobj(verb, noun). Furthermore, we use dependency trees to extract four features for potentially complex constructions. First, we indicate the presence of noun phrases with heavy modifiers on the left. These are frequently used in Chinese and would require a relative clause or an additional sentence in English. Specifically we record the maximum number of dependents for the nouns in the sentence. The second type of construction is the"
D15-1148,A92-1006,0,0.110643,"). 2 Related work The need for identifying content-heavy sentences arises in many specialized domains, including dialog systems, machine translation, text simplification and Chinese language processing but it is usually addressed in an implicit or application specific way. In contrast, we focus on identifying heavy sentences as a standalone task, providing a unifying view of the seemingly disparate strands of prior work. We now overview the literature which motivated our work. Sentence planning. In text generation, a sentence planner produces linguistic realizations of a list of propositions (Rambow and Korelsky, 1992). One subtask is to decide whether to package the same content into one or more sentences. In the example below (Pan and Shaw, 2005), the multisentence expression B is much easier to process: [A] This is a 1 million dollar 3 bedroom, 2 bathroom, 2000 square foot colonial with 2 acre of land, 2 car garage, annual taxes 8000 dollars in Armonk and in the Byram Hills school district. [B] This is a 3 bedroom, 2 bathroom, 2000 square foot colonial located in Armonk with 2 acres of land. The asking price is 1 million dollar and the annual taxes are 8000 dollars. The house is located in the Byram Hill"
D15-1148,P04-1011,0,0.0111649,"c information about the type of propositions the system needs to convey, while in our task we have access only to Chinese text. In some dialog systems, content selection is treated as an optimization problem, balancing the placement of full-stops and the insertion or deletion of propositions with the similarity of the resulting output and an existing corpus of acceptable productions (Pan and Shaw, 2005). Others formulate the problem as a supervised ranking task, in which different possible content realizations are generated, including variation in the number of sentences (Walker et al., 2001; Stent et al., 2004). With the introduction of the concept of content-heavy sentences, we can envision dialog systems addressing the sentence realization task in two steps, first predicting if the semantic content will require multiple sentences, then having different rankers for expressing the content in one or multiple sentences. In that case the ranker will need to capture only sentence-level information and the discourse-level decision to use multiple sentences will be treated separately. Text simplification. “Text simplification, defined narrowly, is the process of reducing the linguistic complexity of a tex"
D15-1148,P12-1083,0,0.0144124,"citously replaced by a full stop. Such commas offer a straightforward way to split a long sentence into multiple shorter ones by replacing the comma with a full stop. Monolingual text simplification systems often try to identify such commas. They are particularly common in Chinese and replacing them with full stops leads to improvements in the accuracy of syntactic parsing (Jin et al., 2004; Li et al., 2005). Moreover, existing syntactically parsed corpora conveniently provide numerous examples of these full-stop commas, and thus training data for systems to identify them (Xue and Yang, 2011; Yang and Xue, 2012). In this paper, we systematically study the relationship between the presence of full-stop commas in the sentence and whether it is content-heavy for Chinese to English translation. 3 Data In this work we use three news datasets: the newswire portion of the NIST 2012 Open Machine Translation Evaluation (OpenMT) (Group, 2013), Multiple-Translation Chinese (MTC) parts 1-4 (Huang et al., 2002; Huang et al., 2003; Ma, 2004; Ma, 2006), and the Chinese Treebank (Xue et al., 2005). In OpenMT and MTC, multiple reference translations in English are available for each Chinese segment (sentence). To stu"
D15-1148,C10-1152,0,0.0832827,"Chandrasekar et al., 1996; Siddharthan, 2006; De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012). However simplification by repackaging the content into multiple sentences is not naturally compatible with the standard view of statistical MT in which a system is expected to produce a single output sentence for a single input sentence. Some of the recent systems using MT techniques separately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et"
D15-1148,N01-1003,0,0.0504752,"ccess to rich semantic information about the type of propositions the system needs to convey, while in our task we have access only to Chinese text. In some dialog systems, content selection is treated as an optimization problem, balancing the placement of full-stops and the insertion or deletion of propositions with the similarity of the resulting output and an existing corpus of acceptable productions (Pan and Shaw, 2005). Others formulate the problem as a supervised ranking task, in which different possible content realizations are generated, including variation in the number of sentences (Walker et al., 2001; Stent et al., 2004). With the introduction of the concept of content-heavy sentences, we can envision dialog systems addressing the sentence realization task in two steps, first predicting if the semantic content will require multiple sentences, then having different rankers for expressing the content in one or multiple sentences. In that case the ranker will need to capture only sentence-level information and the discourse-level decision to use multiple sentences will be treated separately. Text simplification. “Text simplification, defined narrowly, is the process of reducing the linguisti"
D15-1148,D11-1038,0,0.120773,"., 1996; Siddharthan, 2006; De Belder and Moens, 2010). Most recently, text simplification has been addressed as a monolingual machine translation task from complex to simple language (Specia, 2010; Coster and Kauchak, 2011; Wubben et al., 2012). However simplification by repackaging the content into multiple sentences is not naturally compatible with the standard view of statistical MT in which a system is expected to produce a single output sentence for a single input sentence. Some of the recent systems using MT techniques separately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et al. (2014) used rules invo"
D15-1148,P12-1107,0,0.0937459,"Missing"
D15-1148,2005.eamt-1.37,0,0.0430194,"parately model the need for sentence splitting (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Identifying heavy sentences in simplification is equivalent to identifying sentences that require syntactic simplification. Sentence structure and MT. Prior work in machine translation has discussed the existence of sentences in Chinese which would result in a poor translation if translated in one sentence in English. The main factors proposed to characterize such problematic sentences are sentence length (Xu and Tan, 1996) and the presence of given syntactic constructions (Xu et al., 2005; Yin et al., 2007; Jin and Liu, 2010). Mishra et al. (2014) used rules involving similar factors to distinguish sentences in Hindi that need simplification prior to translation. In each of these approaches, the identified sentences are segmented into smaller units. Similar to work in text simplification, the simplification rules are applied to all sentences meeting certain criteria, normally to all sentences longer than a predefined threshold or where certain conjunctions or coordinations are present. In contrast, the model we propose here can be used to predict when segmentation is at all ne"
D15-1148,P11-2111,0,0.25413,"sentence can be felicitously replaced by a full stop. Such commas offer a straightforward way to split a long sentence into multiple shorter ones by replacing the comma with a full stop. Monolingual text simplification systems often try to identify such commas. They are particularly common in Chinese and replacing them with full stops leads to improvements in the accuracy of syntactic parsing (Jin et al., 2004; Li et al., 2005). Moreover, existing syntactically parsed corpora conveniently provide numerous examples of these full-stop commas, and thus training data for systems to identify them (Xue and Yang, 2011; Yang and Xue, 2012). In this paper, we systematically study the relationship between the presence of full-stop commas in the sentence and whether it is content-heavy for Chinese to English translation. 3 Data In this work we use three news datasets: the newswire portion of the NIST 2012 Open Machine Translation Evaluation (OpenMT) (Group, 2013), Multiple-Translation Chinese (MTC) parts 1-4 (Huang et al., 2002; Huang et al., 2003; Ma, 2004; Ma, 2006), and the Chinese Treebank (Xue et al., 2005). In OpenMT and MTC, multiple reference translations in English are available for each Chinese segme"
D18-1087,J13-2002,1,0.802593,"development and evaluation, thanks to its speed and low cost. Specifically, we use several variants of the ROUGE metric (Lin, 2004), which is almost exclusively utilized as an automatic evaluation metric class for summarization. ROUGE variants are based on word sequence overlap between a system summary and a reference summary, where each variant measures a different aspect of text comparison. Despite its pitfalls, ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods (Lin, 2004; Over and James, 2004; Over et al., 2007; Nenkova et al., 2007; Louis and Nenkova, 2013; Peyrard et al., 2017), such as SEE (Lin, 2001), responsiveness (NIST, 2006) and Pyramid (Nenkova et al., 2007). Table 1: DUC 2001 and 2002. Number of reference summaries per length for each text cluster, reference lengths, number of clusters and number of evaluated systems. terms of correlation with human judgment. Our promising results suggest repeating the assessment methodology presented here in future work, to test our question over more recent and broader summarization datasets and human evaluation schemes. This, in turn, would allow the community to feasibly resume proper evaluation an"
D18-1087,K16-1028,0,0.271393,"is paper, we propose that the summarization community should consider resuming evaluating summarization systems over multiple length outputs, as it would allow better assessment of length-related performance within and across systems (illustrated in Section 3). To avoid the need in multiple-length reference summaries we raise the following research question: can reference summaries of a single length be used to evaluate system summaries of multiple lengths, as reliably as when using references of multiple lengths, with respect to different standard evaluation metrics? Recently, Kikuchi et al. (2016) evaluated system summaries of three different lengths against reference summaries of a single length. Yet, their evaluation methodology was not assessed through correlation to human judgment, as has been commonly done for other automatic evaluation protocols. Here, we provide a closer look into this methodology, given its potential value. As a first accessible case study, we test our research question over the DUC 2001 and 2002 data (Section 2). To the best of our knowledge, these are the only two datasets that include multiple length reference and submitted system summaries, as well as manua"
D18-1087,W18-2706,0,0.0649941,"Missing"
D18-1087,W17-4912,0,0.0134345,"ation protocol in question is indeed competitive. This result paves the way to practically evaluating varying-length summaries with simple, possibly existing, summarization benchmarks. 1 Introduction Automated summarization systems typically produce a text that mimics a manual summary. In these systems, an important aspect is the output summary length, which may vary according to user needs. Consequently, output length has been a common tunable parameter in pre-neural summarization systems and has been incorporated recently in few neural models as well (Kikuchi et al., 2016; Fan et al., 2017; Ficler and Goldberg, 2017). It was originally assumed that summarization systems should be assessed across multiple summary lengths. For that, the earliest Document Understand Conference (DUC) (NIST, 2011) benchmarks, in 2001 and 2002, defined several target summary lengths and evaluated each summary against (manually written) reference summaries of the same length. However, due to the high cost incurred, subsequent DUC and TAC (NIST, 2018) benchmarks 774 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 774–778 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Associa"
D18-1087,W17-4510,0,0.0192369,"n, thanks to its speed and low cost. Specifically, we use several variants of the ROUGE metric (Lin, 2004), which is almost exclusively utilized as an automatic evaluation metric class for summarization. ROUGE variants are based on word sequence overlap between a system summary and a reference summary, where each variant measures a different aspect of text comparison. Despite its pitfalls, ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods (Lin, 2004; Over and James, 2004; Over et al., 2007; Nenkova et al., 2007; Louis and Nenkova, 2013; Peyrard et al., 2017), such as SEE (Lin, 2001), responsiveness (NIST, 2006) and Pyramid (Nenkova et al., 2007). Table 1: DUC 2001 and 2002. Number of reference summaries per length for each text cluster, reference lengths, number of clusters and number of evaluated systems. terms of correlation with human judgment. Our promising results suggest repeating the assessment methodology presented here in future work, to test our question over more recent and broader summarization datasets and human evaluation schemes. This, in turn, would allow the community to feasibly resume proper evaluation and deliberate developmen"
D18-1087,D11-1043,0,0.0623214,"Missing"
D18-1087,D16-1140,0,0.478568,"limited. In this paper, we propose that the summarization community should consider resuming evaluating summarization systems over multiple length outputs, as it would allow better assessment of length-related performance within and across systems (illustrated in Section 3). To avoid the need in multiple-length reference summaries we raise the following research question: can reference summaries of a single length be used to evaluate system summaries of multiple lengths, as reliably as when using references of multiple lengths, with respect to different standard evaluation metrics? Recently, Kikuchi et al. (2016) evaluated system summaries of three different lengths against reference summaries of a single length. Yet, their evaluation methodology was not assessed through correlation to human judgment, as has been commonly done for other automatic evaluation protocols. Here, we provide a closer look into this methodology, given its potential value. As a first accessible case study, we test our research question over the DUC 2001 and 2002 data (Section 2). To the best of our knowledge, these are the only two datasets that include multiple length reference and submitted system summaries, as well as manua"
D18-1087,P13-2024,1,0.851931,"C 01; ICSISumm is a later competitive system (Gillick et al., 2008). lated by Only50, at each system summary length. We find very high correlations (above 0.95 for all system summary lengths, in both datasets) when using multiple references and slightly lower (0.85 to 0.9) with one reference summary. These figures show that the Only50 configuration ranks systems very similarly to Standard. To further verify our results, we computed correlations in two additional settings. First, we conducted the same analysis, excluding 2-3 of the worst systems, which might artificially boost the correlation (Rankel et al., 2013). Second, we computed score differences between all pairs of systems, for both human and ROUGE scores, and computed the correlation between these two sets of differences (Rankel et al., 2011). In both cases we observed rather consistent results, assessing that a single set of short reference summaries evaluates system summaries of different lengths just as well as the standard configuration. 3 4 Discussion We proposed the potential value of evaluating summarization systems at different summary lengths. Such evaluations would allow proper evaluation of systems’ “length knob”, tracking how their"
D19-1116,P15-2025,0,0.0206961,"sentations outperform ROUGE in recent corpora for abstractive news summarization but are less good on older test data and systems. 1 Introduction The widely used ROUGE (Lin, 2004) automatic evaluation for summarization relies on token overlap between reference and system summary. This limited view of meaning has motivated numerous studies on summarization evaluation (Zhou et al., 2006; Ganesan, 2018; ShafieiBavani et al., 2018), and the related areas of translation and dialog, to explore more compelling semantic matching (Kauchak and Barzilay, 2006; Lavie and Denkowski, 2009; Lo and Wu, 2011; Chen and Guo, 2015; Liu et al., 2016; Tao et al., 2018). Most recently, incorporating word embeddings in ROUGE pairwise comparison of n-grams has proven beneficial (Ng and Abrecht, 2015), as well as representing sentences using universal sentence representation to predict the quality of translation (Shimanaka et al., 2018). We build upon this line of work and show that cosine similarity between the reference and 2 Embeddings To get a dense low-dimenional representation of texts, we test seven representations covering sentence embedding, variants of un-contextualized word embedding and variants of contextualized"
D19-1116,D17-1070,0,0.0310963,"462 0.876∗ / 0.446 0.748∗ / 0.420 0.878∗ / 0.486 0.762∗ / 0.503 0.881∗ / 0.693∗ 0.790∗ / 0.692∗ 0.710∗ / 0.678∗ 0.643∗ / 0.650∗ 0.888∗ / 0.763∗ 0.748∗ / 0.685∗ Table 1: Correlation results on DUC2001/2002. P is shorthand for Pearson correlation, S for Spearman correlation. Detailed description of encoders can be found in §2. (ref / doc) shows the correlation between summary embedding with either reference embedding or document embedding. Entries with p-value lower than 0.05 are marked with ∗ . ROUGE F1 scores, the commonly reported metric, are underlined for better comparison. (iv) InferSent (Conneau et al., 2017) (InferSent), a BiLSTM encoder producing representation of 4,096 dimensions. We compute cosine similarity between summary and reference embedding to capture semantic similarity. To test the robustness of this evaluation approach, we check correlations on old single document summarization evaluations of somewhat obsolete systems and modern corpora for summarization with a mix of extractive and neural abstractive systems. 3 We find that there is no single optimal representation that gives the best correlation on both data sets. There is a clear increase of performance from DUC’01 to DUC’02. In D"
D19-1116,N18-1150,0,0.0219489,"ewly proposed automatic evaluations, we follow the conventional methodology of computing correlation between the automatic metric and human evaluations of summary content. The results are shown in Table 11 . R1-F correlates better with human ratings on DUC’01, while R2-R works extremely well on DUC’02. Both uni- and bi-gram ROUGE F-measure also correlate well with human evaluation, which is an important finding given that ROUGE-F has become the de facto standard for evaluation of neural summarization systems (Nallapati et al., 2016; See et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Celikyilmaz et al., 2018). 1 There are total 14 systems in DUC’02. We discard two poorly performing systems, 17 and 30. Including them in the analysis results in high correlation (&gt; 0.9) for both ROUGE and embedding similarity but the results we present are more convincing without the presence of clearly inferior systems. DUC2001 12 116.595 114.598 841.000 0.320 0.453 0.048 148.000 DUC2002 12 113.607 114.184 650.053 0.346 0.4975 0.049 293.250 Table 2: DUC2001 and DUC2002 dataset statistics To understand what causes the low performance on DUC’01, we examined statistics for both datasets, shown in Table 2. Systems in DU"
D19-1116,D18-2029,0,0.0504167,"Missing"
D19-1116,D18-1443,0,0.0182943,"ary (Lin and Hovy, 2003). To evaluate the newly proposed automatic evaluations, we follow the conventional methodology of computing correlation between the automatic metric and human evaluations of summary content. The results are shown in Table 11 . R1-F correlates better with human ratings on DUC’01, while R2-R works extremely well on DUC’02. Both uni- and bi-gram ROUGE F-measure also correlate well with human evaluation, which is an important finding given that ROUGE-F has become the de facto standard for evaluation of neural summarization systems (Nallapati et al., 2016; See et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Celikyilmaz et al., 2018). 1 There are total 14 systems in DUC’02. We discard two poorly performing systems, 17 and 30. Including them in the analysis results in high correlation (&gt; 0.9) for both ROUGE and embedding similarity but the results we present are more convincing without the presence of clearly inferior systems. DUC2001 12 116.595 114.598 841.000 0.320 0.453 0.048 148.000 DUC2002 12 113.607 114.184 650.053 0.346 0.4975 0.049 293.250 Table 2: DUC2001 and DUC2002 dataset statistics To understand what causes the low performance on DUC’01, we examined statistics for"
D19-1116,N18-1065,0,0.0640946,"Missing"
D19-1116,P15-1162,0,0.0412501,"Missing"
D19-1116,N06-1058,0,0.0805115,"tion with manual scores in reference-free setting. The distributed representations outperform ROUGE in recent corpora for abstractive news summarization but are less good on older test data and systems. 1 Introduction The widely used ROUGE (Lin, 2004) automatic evaluation for summarization relies on token overlap between reference and system summary. This limited view of meaning has motivated numerous studies on summarization evaluation (Zhou et al., 2006; Ganesan, 2018; ShafieiBavani et al., 2018), and the related areas of translation and dialog, to explore more compelling semantic matching (Kauchak and Barzilay, 2006; Lavie and Denkowski, 2009; Lo and Wu, 2011; Chen and Guo, 2015; Liu et al., 2016; Tao et al., 2018). Most recently, incorporating word embeddings in ROUGE pairwise comparison of n-grams has proven beneficial (Ng and Abrecht, 2015), as well as representing sentences using universal sentence representation to predict the quality of translation (Shimanaka et al., 2018). We build upon this line of work and show that cosine similarity between the reference and 2 Embeddings To get a dense low-dimenional representation of texts, we test seven representations covering sentence embedding, variants of"
D19-1116,W04-1013,0,0.202184,"representations for evaluating summarizers, both in reference-based and in reference-free setting. Our experimental results show that the max value over each dimension of the summary ELMo word embeddings is a good representation that results in high correlation with human ratings. Averaging the cosine similarity of all encoders we tested yields high correlation with manual scores in reference-free setting. The distributed representations outperform ROUGE in recent corpora for abstractive news summarization but are less good on older test data and systems. 1 Introduction The widely used ROUGE (Lin, 2004) automatic evaluation for summarization relies on token overlap between reference and system summary. This limited view of meaning has motivated numerous studies on summarization evaluation (Zhou et al., 2006; Ganesan, 2018; ShafieiBavani et al., 2018), and the related areas of translation and dialog, to explore more compelling semantic matching (Kauchak and Barzilay, 2006; Lavie and Denkowski, 2009; Lo and Wu, 2011; Chen and Guo, 2015; Liu et al., 2016; Tao et al., 2018). Most recently, incorporating word embeddings in ROUGE pairwise comparison of n-grams has proven beneficial (Ng and Abrecht"
D19-1116,N03-1020,0,0.389874,"Missing"
D19-1116,D16-1230,0,0.0324336,"Missing"
D19-1116,P11-1023,0,0.0374434,"distributed representations outperform ROUGE in recent corpora for abstractive news summarization but are less good on older test data and systems. 1 Introduction The widely used ROUGE (Lin, 2004) automatic evaluation for summarization relies on token overlap between reference and system summary. This limited view of meaning has motivated numerous studies on summarization evaluation (Zhou et al., 2006; Ganesan, 2018; ShafieiBavani et al., 2018), and the related areas of translation and dialog, to explore more compelling semantic matching (Kauchak and Barzilay, 2006; Lavie and Denkowski, 2009; Lo and Wu, 2011; Chen and Guo, 2015; Liu et al., 2016; Tao et al., 2018). Most recently, incorporating word embeddings in ROUGE pairwise comparison of n-grams has proven beneficial (Ng and Abrecht, 2015), as well as representing sentences using universal sentence representation to predict the quality of translation (Shimanaka et al., 2018). We build upon this line of work and show that cosine similarity between the reference and 2 Embeddings To get a dense low-dimenional representation of texts, we test seven representations covering sentence embedding, variants of un-contextualized word embedding and varian"
D19-1116,J13-2002,1,0.850447,"t summarization systems. Unlike prior work (Ng and Abrecht, 2015), we thoroughly abandon ROUGE and n-gram co-occurrences in the computation of semantic similarity. To give a sense of the generalizability of our findings, we validate the method on three different test sets with human evaluation. We compare several popular representation including sentence embedding, un-contextualized word embedding and contextualized word embedding. Finally, we present experiments on evaluating single document summaries without reference summaries which was originally proposed for multi-document summarization (Louis and Nenkova, 2013) and explored a variety of word-string similarity techniques. Here we study reference-free evaluations via embedding similarity between the full document to be summarized and the system summaries. ROUGE is widely used to automatically evaluate summarization systems. However, ROUGE measures semantic overlap between a system summary and a human reference on word-string level, much at odds with the contemporary treatment of semantic meaning. Here we present a suite of experiments on using distributed representations for evaluating summarizers, both in reference-based and in reference-free setting"
D19-1116,K16-1028,0,0.0911903,"e summary are expressed in the system summary (Lin and Hovy, 2003). To evaluate the newly proposed automatic evaluations, we follow the conventional methodology of computing correlation between the automatic metric and human evaluations of summary content. The results are shown in Table 11 . R1-F correlates better with human ratings on DUC’01, while R2-R works extremely well on DUC’02. Both uni- and bi-gram ROUGE F-measure also correlate well with human evaluation, which is an important finding given that ROUGE-F has become the de facto standard for evaluation of neural summarization systems (Nallapati et al., 2016; See et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Celikyilmaz et al., 2018). 1 There are total 14 systems in DUC’02. We discard two poorly performing systems, 17 and 30. Including them in the analysis results in high correlation (&gt; 0.9) for both ROUGE and embedding similarity but the results we present are more convincing without the presence of clearly inferior systems. DUC2001 12 116.595 114.598 841.000 0.320 0.453 0.048 148.000 DUC2002 12 113.607 114.184 650.053 0.346 0.4975 0.049 293.250 Table 2: DUC2001 and DUC2002 dataset statistics To understand what causes the low performa"
D19-1116,D15-1222,0,0.0333655,"GE (Lin, 2004) automatic evaluation for summarization relies on token overlap between reference and system summary. This limited view of meaning has motivated numerous studies on summarization evaluation (Zhou et al., 2006; Ganesan, 2018; ShafieiBavani et al., 2018), and the related areas of translation and dialog, to explore more compelling semantic matching (Kauchak and Barzilay, 2006; Lavie and Denkowski, 2009; Lo and Wu, 2011; Chen and Guo, 2015; Liu et al., 2016; Tao et al., 2018). Most recently, incorporating word embeddings in ROUGE pairwise comparison of n-grams has proven beneficial (Ng and Abrecht, 2015), as well as representing sentences using universal sentence representation to predict the quality of translation (Shimanaka et al., 2018). We build upon this line of work and show that cosine similarity between the reference and 2 Embeddings To get a dense low-dimenional representation of texts, we test seven representations covering sentence embedding, variants of un-contextualized word embedding and variants of contextualized word embedding. Specifically: (i) Two Google universal sentence encoders: (Cer et al., 2018), an encoder (enc-2) based on deep averaging net (Iyyer et al., 2015) and a"
D19-1116,N18-1202,0,0.0179469,"imilarity between the reference and 2 Embeddings To get a dense low-dimenional representation of texts, we test seven representations covering sentence embedding, variants of un-contextualized word embedding and variants of contextualized word embedding. Specifically: (i) Two Google universal sentence encoders: (Cer et al., 2018), an encoder (enc-2) based on deep averaging net (Iyyer et al., 2015) and an encoder (enc-3) based on transformer (Vaswani et al., 2017). Both encoders encode input text to 512-dimensional vector. (ii) Average (ELMo-a) and max (ELMo-m) over each dimension of all ELMo (Peters et al., 2018) word embeddings of an input text. For each token in the input, three layers of 1,024dimensional vectors were concatenated to form a 3,072-dimensional vector. (iii) Average (avg) and max (max) over GoogleNews 300-d word2vec. 1216 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1216–1221, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ROUGE R1-R R1-P R1-F R2-R R2-P R2-F - DUC2001 P S 0.683∗ 0.650∗ 0.315 0.441 0.835∗ 0.825∗ 0.810∗ 0.811∗"
D19-1116,D18-1085,0,0.165533,"esults in high correlation with human ratings. Averaging the cosine similarity of all encoders we tested yields high correlation with manual scores in reference-free setting. The distributed representations outperform ROUGE in recent corpora for abstractive news summarization but are less good on older test data and systems. 1 Introduction The widely used ROUGE (Lin, 2004) automatic evaluation for summarization relies on token overlap between reference and system summary. This limited view of meaning has motivated numerous studies on summarization evaluation (Zhou et al., 2006; Ganesan, 2018; ShafieiBavani et al., 2018), and the related areas of translation and dialog, to explore more compelling semantic matching (Kauchak and Barzilay, 2006; Lavie and Denkowski, 2009; Lo and Wu, 2011; Chen and Guo, 2015; Liu et al., 2016; Tao et al., 2018). Most recently, incorporating word embeddings in ROUGE pairwise comparison of n-grams has proven beneficial (Ng and Abrecht, 2015), as well as representing sentences using universal sentence representation to predict the quality of translation (Shimanaka et al., 2018). We build upon this line of work and show that cosine similarity between the reference and 2 Embeddings To"
D19-1116,N18-4015,0,0.0210982,"meaning has motivated numerous studies on summarization evaluation (Zhou et al., 2006; Ganesan, 2018; ShafieiBavani et al., 2018), and the related areas of translation and dialog, to explore more compelling semantic matching (Kauchak and Barzilay, 2006; Lavie and Denkowski, 2009; Lo and Wu, 2011; Chen and Guo, 2015; Liu et al., 2016; Tao et al., 2018). Most recently, incorporating word embeddings in ROUGE pairwise comparison of n-grams has proven beneficial (Ng and Abrecht, 2015), as well as representing sentences using universal sentence representation to predict the quality of translation (Shimanaka et al., 2018). We build upon this line of work and show that cosine similarity between the reference and 2 Embeddings To get a dense low-dimenional representation of texts, we test seven representations covering sentence embedding, variants of un-contextualized word embedding and variants of contextualized word embedding. Specifically: (i) Two Google universal sentence encoders: (Cer et al., 2018), an encoder (enc-2) based on deep averaging net (Iyyer et al., 2015) and an encoder (enc-3) based on transformer (Vaswani et al., 2017). Both encoders encode input text to 512-dimensional vector. (ii) Average (EL"
D19-1116,D18-1088,0,0.0187346,"). To evaluate the newly proposed automatic evaluations, we follow the conventional methodology of computing correlation between the automatic metric and human evaluations of summary content. The results are shown in Table 11 . R1-F correlates better with human ratings on DUC’01, while R2-R works extremely well on DUC’02. Both uni- and bi-gram ROUGE F-measure also correlate well with human evaluation, which is an important finding given that ROUGE-F has become the de facto standard for evaluation of neural summarization systems (Nallapati et al., 2016; See et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Celikyilmaz et al., 2018). 1 There are total 14 systems in DUC’02. We discard two poorly performing systems, 17 and 30. Including them in the analysis results in high correlation (&gt; 0.9) for both ROUGE and embedding similarity but the results we present are more convincing without the presence of clearly inferior systems. DUC2001 12 116.595 114.598 841.000 0.320 0.453 0.048 148.000 DUC2002 12 113.607 114.184 650.053 0.346 0.4975 0.049 293.250 Table 2: DUC2001 and DUC2002 dataset statistics To understand what causes the low performance on DUC’01, we examined statistics for both datasets, show"
D19-1116,N06-1057,0,0.0551147,"gs is a good representation that results in high correlation with human ratings. Averaging the cosine similarity of all encoders we tested yields high correlation with manual scores in reference-free setting. The distributed representations outperform ROUGE in recent corpora for abstractive news summarization but are less good on older test data and systems. 1 Introduction The widely used ROUGE (Lin, 2004) automatic evaluation for summarization relies on token overlap between reference and system summary. This limited view of meaning has motivated numerous studies on summarization evaluation (Zhou et al., 2006; Ganesan, 2018; ShafieiBavani et al., 2018), and the related areas of translation and dialog, to explore more compelling semantic matching (Kauchak and Barzilay, 2006; Lavie and Denkowski, 2009; Lo and Wu, 2011; Chen and Guo, 2015; Liu et al., 2016; Tao et al., 2018). Most recently, incorporating word embeddings in ROUGE pairwise comparison of n-grams has proven beneficial (Ng and Abrecht, 2015), as well as representing sentences using universal sentence representation to predict the quality of translation (Shimanaka et al., 2018). We build upon this line of work and show that cosine similari"
D19-1116,D15-1044,0,0.00978136,"01. The baseline system is shortened to ‘b’. 4 Evaluation on Newsroom 60 In this section, we use contemporary data and systems and explore other factors that embedding similarity could potentially capture. We employ the human evaluation set from newsroom data introduced in (Grusky et al., 2018). The evaluation data includes 7 systems, each producing summaries for 60 articles. The 7 systems are: (1) lead3 sentences of the article (2) textrank with word limit of 50 (3) extractive oracle ‘fragments’ system, representing the best possible performance of an extractive system (4) abstractive model (Rush et al., 2015) trained on Newsroom training data (5) Pointer-Generator (See et al., 2017) trained on CNN/DailyMail data set (Nallapati et al., 2016), on complete and a subset of Newsroom training set respectively. We collected crowdsourced evaluations of relevance (RL) and informativeness (IN) as introduced in the original paper, closely reproducing the earlier findings. We introduce four more dimensions: verbosity (VE), unnecessary content (UC), perfect surrogate (SR) and continue reading (CN). Higher rating corresponds to assessment that the summary is not unnecessarily verbose, it has no unnecessary cont"
D19-1116,P17-1099,0,0.274615,"in the system summary (Lin and Hovy, 2003). To evaluate the newly proposed automatic evaluations, we follow the conventional methodology of computing correlation between the automatic metric and human evaluations of summary content. The results are shown in Table 11 . R1-F correlates better with human ratings on DUC’01, while R2-R works extremely well on DUC’02. Both uni- and bi-gram ROUGE F-measure also correlate well with human evaluation, which is an important finding given that ROUGE-F has become the de facto standard for evaluation of neural summarization systems (Nallapati et al., 2016; See et al., 2017; Gehrmann et al., 2018; Zhang et al., 2018; Celikyilmaz et al., 2018). 1 There are total 14 systems in DUC’02. We discard two poorly performing systems, 17 and 30. Including them in the analysis results in high correlation (&gt; 0.9) for both ROUGE and embedding similarity but the results we present are more convincing without the presence of clearly inferior systems. DUC2001 12 116.595 114.598 841.000 0.320 0.453 0.048 148.000 DUC2002 12 113.607 114.184 650.053 0.346 0.4975 0.049 293.250 Table 2: DUC2001 and DUC2002 dataset statistics To understand what causes the low performance on DUC’01, we"
E09-1017,W00-1401,0,0.0369984,"Missing"
E09-1017,P00-1041,0,0.058244,"Missing"
E09-1017,J08-1001,0,0.156842,"s.upenn.edu Abstract and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge overall “goodness”, with higher n-grams meant to capture fluency considerations. More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semantic overlap between a model a"
E09-1017,J05-3002,0,0.0345384,"Missing"
E09-1017,P05-1022,0,0.0168625,"Missing"
E09-1017,A00-2018,0,0.103022,"Missing"
E09-1017,P06-1048,0,0.0265729,"Missing"
E09-1017,J05-1003,0,0.0226914,"Missing"
E09-1017,N04-1025,0,0.060636,"Missing"
E09-1017,P01-1020,0,0.0482745,"Missing"
E09-1017,W04-1016,0,0.0517144,"Missing"
E09-1017,W05-1628,0,0.0435628,"Missing"
E09-1017,N07-1023,0,0.0200433,"Missing"
E09-1017,P08-1067,0,0.0134644,"Missing"
E09-1017,C08-1145,0,0.0875213,"Missing"
E09-1017,A00-1043,0,0.0851484,"Missing"
E09-1017,P98-1116,0,0.0699588,"Missing"
E09-1017,P03-1069,0,0.0571968,"Missing"
E09-1017,E06-1038,0,0.0165076,"Missing"
E09-1017,P07-1044,0,0.0689023,"Missing"
E09-1017,P02-1040,0,0.0912371,"t al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007). Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth. Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). In many applications fluency is assessed in combination with other qualities. For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge overall “goodness”, with higher n-grams meant to capture fluency considerations. More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semantic overlap between a model and an output. Similarly, the metrics proposed for text generation by (Bangalore et al., 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. In contrast, the"
E09-1017,D08-1020,1,0.930599,"tences, “any pair” contains comparisons of sentences with different fluency over the entire data set. indeed structural features can enable us to perform this task very accurately in the context of machine translation. But will the models conveniently trained on data from MT evaluation be at all capable to identify sentences in human-written text that are not fluent and are difficult to understand? To answer this question, we performed an additional experiment on 30 Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). The articles were chosen at random and comprised a total of 290 sentences. One human assessor was asked to read each sentence and mark the ones that seemed disfluent because they were hard to comprehend. These were sentences that needed to be read more than once in order to fully understand the information conveyed in them. There were 52 such sentences. The assessments served as a goldstandard against which the predictions of the fluency models were compared. Two models trained on machine translation data were used to predict the status of each sentence in the WSJ articles. One of the models"
E09-1017,P05-1065,0,0.018038,"slations. Finally, we test the models on human written text in order to verify if the classifiers trained on data coming from machine translation evaluations can be used for general predictions of fluency and readability. 2 Features Perceived sentence fluency is influenced by many factors. The way the sentence fits in the context of surrounding sentences is one obvious factor (Barzilay and Lapata, 2008). Another well-known factor is vocabulary use: the presence of uncommon difficult words are known to pose problems to readers and to render text less readable (CollinsThompson and Callan, 2004; Schwarm and Ostendorf, 2005). But these discourse- and vocabularylevel features measure properties at granularities different from the sentence level. Syntactic sentence level features have not been investigated as a stand-alone class, as has been For our experiments we use the evaluations of Chinese to English translations distributed by LDC (catalog number LDC2003T17), for which both machine and human translations are available. Machine translations have been assessed 140 erage phrase length is computed for PP, NP and VP and is equal to the average phrase length of given type divided by the sentence length. These were"
E09-1017,P05-1036,0,0.0158073,"Missing"
E09-1017,C00-1007,0,\N,Missing
E09-1017,J09-1003,0,\N,Missing
E09-1017,C98-1112,0,\N,Missing
E09-1062,J08-1001,0,0.0809535,"Missing"
E09-1062,W02-1033,0,0.101391,"Missing"
E09-1062,P06-2020,0,0.130376,"Missing"
E09-1062,P08-1080,0,0.0605695,"Missing"
E09-1062,C00-1072,0,0.136372,"Missing"
E09-1062,W07-0737,0,0.018197,"on answering for example, a system may be configured not to answer questions for which the confidence of producing a correct answer is low, and in this way increase the overall accuracy of the system whenever it does produce an answer (Brill et al., 2002; Dredze and Czuba, 2007). 1 Introduction Similarly in machine translation, some sentences might contain difficult to translate phrases, that is, portions of the input are likely to lead to garbled output if automatic translation is attempted. Automatically identifying such phrases has the potential of improving MT as shown by an oracle study (Mohit and Hwa, 2007). More recent work (Birch et al., 2008) has shown that properties of reordering, source and target language complexity and relatedness can be used to predict translation quality. In information retrieval, the problem of predicting system performance has generated considerable interest and has led to notably good results (Cronen-Townsend et al., 2002; Yom-Tov et al., 2005; Carmel et al., 2006). The input to a summarization system significantly affects the quality of the summary that can be produced for it, by either a person or an automatic method. Some inputs are difficult and summaries produc"
E09-1062,P08-1094,1,0.852784,"d utilizing more representative examples for good and bad performance. We also extend the analysis to single document summarization, for which predicting system performance turns out to be much more accurate than for multi-document summarization. We address three key questions. In summarization, researchers have recognized that some inputs might be more successfully handled by a particular subsystem (McKeown et al., 2001), but little work has been done to qualify the general characteristics of inputs that lead to suboptimal performance of systems. Only recently the issue has drawn attention: (Nenkova and Louis, 2008) present an initial analysis of the factors that influence system performance in content selection. This study was based on results from the Document Understanding Conference (DUC) evaluations (Over et al., 2007) of multi-document summarization of news. They showed that input, system identity and length of the target summary were all significant factors affecting summary quality. Longer summaries were consistently better than shorter ones for the same input, so improvements can be easy in applications where varying target size is possible. Indeed, varying summary size is desirable in many situ"
E09-1062,N01-1003,0,0.0191257,"Missing"
E09-1062,D08-1078,0,\N,Missing
E14-1067,Q13-1028,1,0.862056,"tor (Chen and Mooney, 2008) and question answering with different compression rates for different types of questions (Kaisser et al., 2008). Predicting the type of content appropriate for the given length alone would be highly desirable, for example in automatic essay grading, summarization and even in information retrieval, in which verbose writing is particularly undesirable. In this respect, our work supplements recent computational methods to predict varied aspects of writing quality, such as popular writing style and phrasing in novels (Ganjigunte Ashok et al., 2013), science journalism (Louis and Nenkova, 2013), and social media content (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013). Our work is the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it hard to distinguish between individual aspect"
E14-1067,E06-1038,0,0.0400983,"versus specific sentences. We use a data set of general and specific sentences and features described in Louis and Nenkova (2011) to implement a sentence specificity model. The classifier produces a binary prediction and also a graded score for specificity. We add two features—the percentage of specific sentences and the average specificity score of words. Compression likelihood (29 features). These features use an external source of information about content importance. Specifically, we use data commonly employed to develop statistical models for sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007). It consists of pairs of sentences in an original text and a professional summary of that text. In every pair, one of the sentences (source) appeared in the original text and the other is a shorter version with the superfluous details deleted. Both sentences were produced by people. We use the dataset created by Galley and McKeown (2007). The sentences are taken from the Ziff Davis Corpus which contains articles about technology products. This data also contains alignment between the constituency parse nodes of the source and summary sentence pair. Through the align"
E14-1067,C08-1019,0,0.0335038,"l., 2013). Our work is the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it hard to distinguish between individual aspects of quality and often the ratings for different aspects are highly correlated (Conroy and Dang, 2008; Pitler et al., 2010) b) Moreover, for verbosity in particular, the most appropriate data for annotation would be concise and verbose versions of the same text (possibly of similar lengths). It is more likely that people can distinguish between verbosity of these controlled pairs compared to ratings on an individual article. Such writing samples are not easily available. So we have avoided the uncertainties in annotation in this first work by adopting a simpler approach based on three key ideas. (i) We define a concise article of length l as “an article that has the appropriate types of conte"
E14-1067,P12-1094,0,0.0205826,"ith different compression rates for different types of questions (Kaisser et al., 2008). Predicting the type of content appropriate for the given length alone would be highly desirable, for example in automatic essay grading, summarization and even in information retrieval, in which verbose writing is particularly undesirable. In this respect, our work supplements recent computational methods to predict varied aspects of writing quality, such as popular writing style and phrasing in novels (Ganjigunte Ashok et al., 2013), science journalism (Louis and Nenkova, 2013), and social media content (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013). Our work is the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it hard to distinguish between individual aspects of quality and often the ratings for different aspects are hig"
E14-1067,P09-2004,1,0.755621,"tion of entities, i.e the LHS (lefthand side) of the production is a noun phrase. The count of each of these productions is added as a feature allowing us to track what type of information about the entities is conveyed in the snippet. We also add features for the most frequent 15 productions whose LHS is not a noun phrase. Discourse relations (5 features). These features are based on the hypothesis that different discourse relations would vary in their appropriateness for articles of different lengths. For example causal information may be included only in more detailed texts. We use a tool (Pitler and Nenkova, 2009) to identify all explicit discourse connectives in our snippets, along with the general semantic class of the connective (temporal, comparison, contingency and expansion). We use the number of discourse connectives of each of the four types as features, as well as the total number of connectives. Continuity (6 features). These features capture the degree to which adjacent sentences in the snippet are related and continue the topic. The amount of continuity for subtopics is likely to vary for long and short texts. We add the number of pronouns and determiners as two features. Another feature is"
E14-1067,D08-1035,0,0.0294122,"source would be of high quality overall. 5.1 Data We obtain the text of the articles from the NYT Annotated Corpus (Sandhaus, 2008). We choose the articles from the opinion section of the newspaper since they are likely to have good topic continuity and related content compared to general news which often contain lists of facts. We further use only the editorial articles to ensure that the articles are of high quality. We collect 10,724 opinion articles from years 2000 to 2007 of the NYT. We divide each article into topic segments using the unsupervised topic segmentation method developed by Eisenstein and Barzilay (2008). We use the following heuristic to decide on the number of topic segments for each article. If the article has fewer than 50 sentences, we create segments such that the expected length Classification results The task is to predict the length of the summary from which the fixed length snippet was taken, i.e. 4-way classification—50, 100, 200 or a 400 word summary. We trained an SVM classifier with a radial basis kernel on the 2001 data. The regularization and kernel parameters were tuned using 10fold cross validation on the training set. The accuracies of classification on the 2002 data are sh"
E14-1067,P10-1056,1,0.856027,"the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it hard to distinguish between individual aspects of quality and often the ratings for different aspects are highly correlated (Conroy and Dang, 2008; Pitler et al., 2010) b) Moreover, for verbosity in particular, the most appropriate data for annotation would be concise and verbose versions of the same text (possibly of similar lengths). It is more likely that people can distinguish between verbosity of these controlled pairs compared to ratings on an individual article. Such writing samples are not easily available. So we have avoided the uncertainties in annotation in this first work by adopting a simpler approach based on three key ideas. (i) We define a concise article of length l as “an article that has the appropriate types of content expected in an arti"
E14-1067,P05-1045,0,0.00936916,"n the Stanford Coreference tool (Raghunathan et al., 2010) to identity pronoun and entity coreference links within the snippet. The number of total coreference links, and the number of intra- and inter-sentence links are added as three separate features. Amount of detail (7 features). To indicate descriptive words, we compute the number of adjectives and adverbs (two features). We also include the total number of named entities (NEs), average length of NEs in words and the number of sentences that do not have any NEs. The named entities were identified using the Stanford NER recognition tool (Finkel et al., 2005). We also use the predictions of a classifier trained to identify general versus specific sentences. We use a data set of general and specific sentences and features described in Louis and Nenkova (2011) to implement a sentence specificity model. The classifier produces a binary prediction and also a graded score for specificity. We add two features—the percentage of specific sentences and the average specificity score of words. Compression likelihood (29 features). These features use an external source of information about content importance. Specifically, we use data commonly employed to dev"
E14-1067,D10-1048,0,0.0169272,"gives the likelihood of the text being deleted. We also add the perplexity value based on this likelihood, P −1/n where P is the likelihood and n is the number of productions from the snippet for which we have deletion information in our data.2 For training a model, we need texts which we can assume are written in a concise manner. We use two sources of data—summaries written by people and high quality news articles. the vectors of adjacent sentences and the average value of the similarity across all pairs of adjacent sentences is the feature value. We also run the Stanford Coreference tool (Raghunathan et al., 2010) to identity pronoun and entity coreference links within the snippet. The number of total coreference links, and the number of intra- and inter-sentence links are added as three separate features. Amount of detail (7 features). To indicate descriptive words, we compute the number of adjectives and adverbs (two features). We also include the total number of named entities (NEs), average length of NEs in words and the number of sentences that do not have any NEs. The named entities were identified using the Stanford NER recognition tool (Finkel et al., 2005). We also use the predictions of a cla"
E14-1067,N07-1023,0,0.0201974,"sentences. We use a data set of general and specific sentences and features described in Louis and Nenkova (2011) to implement a sentence specificity model. The classifier produces a binary prediction and also a graded score for specificity. We add two features—the percentage of specific sentences and the average specificity score of words. Compression likelihood (29 features). These features use an external source of information about content importance. Specifically, we use data commonly employed to develop statistical models for sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007). It consists of pairs of sentences in an original text and a professional summary of that text. In every pair, one of the sentences (source) appeared in the original text and the other is a shorter version with the superfluous details deleted. Both sentences were produced by people. We use the dataset created by Galley and McKeown (2007). The sentences are taken from the Ziff Davis Corpus which contains articles about technology products. This data also contains alignment between the constituency parse nodes of the source and summary sentence pair. Through the alignment it is possible to trac"
E14-1067,D13-1181,0,0.155135,"ration (O’Donnell, 1997), soccer commentator (Chen and Mooney, 2008) and question answering with different compression rates for different types of questions (Kaisser et al., 2008). Predicting the type of content appropriate for the given length alone would be highly desirable, for example in automatic essay grading, summarization and even in information retrieval, in which verbose writing is particularly undesirable. In this respect, our work supplements recent computational methods to predict varied aspects of writing quality, such as popular writing style and phrasing in novels (Ganjigunte Ashok et al., 2013), science journalism (Louis and Nenkova, 2013), and social media content (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013). Our work is the first to explore text verbosity. We introduce a simple application-oriented definition of verbosity and a model to automatically predict verbosity scores. We start with a brief overview of our approach in the next section. 2 Text length and content appropriateness In this first model of verbosity, we do not carry out an elaborate annotation experiment to create labels for verbosity. There are two main reasons for this choice: a) People find it"
E14-1067,P08-1080,0,0.0333736,"and k &lt; mintj l(tj ). The mapping f is learned based on the constant length snippet only and the aim is to predict the original text length. are interspersed with the original shorter summary. The performance of a range of human-machine applications can be enhanced if they had the ability to predict the appropriate length of a system contribution and the type of content appropriate for that length. Such applications include document generation (O’Donnell, 1997), soccer commentator (Chen and Mooney, 2008) and question answering with different compression rates for different types of questions (Kaisser et al., 2008). Predicting the type of content appropriate for the given length alone would be highly desirable, for example in automatic essay grading, summarization and even in information retrieval, in which verbose writing is particularly undesirable. In this respect, our work supplements recent computational methods to predict varied aspects of writing quality, such as popular writing style and phrasing in novels (Ganjigunte Ashok et al., 2013), science journalism (Louis and Nenkova, 2013), and social media content (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013). Our work is the first to"
E14-1067,P03-1054,0,0.0273392,"easily available. So we have avoided the uncertainties in annotation in this first work by adopting a simpler approach based on three key ideas. (i) We define a concise article of length l as “an article that has the appropriate types of content expected in an article of length l”. Note that length is not equal to verbosity in our model. Our defif (sti ) → ˆl(ti ) In our work we choose to work with topical segments from documents rather than the complete documents themselves. 637 article. All the syntax based features are computed from the constituency trees produced from the Stanford Parser (Klein and Manning, 2003). Once the model is trained, we identify the verbosity for a test article as follows: Let us consider a new topic segment tx during test time. Let the length of the segment be l. We obtain a snippet stx of size k from tx . Now assume that our model predicts f (stx ) = ˆl. Case 1: ˆl ' l, the content type in tx matches the content types generally present in articles of length l. We consider such articles as concise. Case 2: ˆl  l, the type of content included in tx is really suitable for longer and detailed topic segments. Thus tx is likely conveying too much detail given its length i.e. it is"
E14-1067,I11-1068,1,0.860989,"er-sentence links are added as three separate features. Amount of detail (7 features). To indicate descriptive words, we compute the number of adjectives and adverbs (two features). We also include the total number of named entities (NEs), average length of NEs in words and the number of sentences that do not have any NEs. The named entities were identified using the Stanford NER recognition tool (Finkel et al., 2005). We also use the predictions of a classifier trained to identify general versus specific sentences. We use a data set of general and specific sentences and features described in Louis and Nenkova (2011) to implement a sentence specificity model. The classifier produces a binary prediction and also a graded score for specificity. We add two features—the percentage of specific sentences and the average specificity score of words. Compression likelihood (29 features). These features use an external source of information about content importance. Specifically, we use data commonly employed to develop statistical models for sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007). It consists of pairs of sentences in an original text and a professional summary of th"
E14-1075,P11-1049,0,0.0325075,"y et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). Introduction In automatic extractive summarization, sentence importance is calculated by taking into account, among possibly other features, the importance of words that appear in the sentence. In this paper, we describe experiments on identifying words from the input that are also included in human summaries; we call such words summary keywords. We review several unsupervised approaches for summary keyword identification and further combine these, along with features including position, part-of-speech, subjectivity, topic categories, context and intrinsic importance, in a superior supervise"
E14-1075,P10-1084,0,0.0186174,"so been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 i Mean |Gi | 1 102 2 32 3 15 4 6 Table 1: Average number of words in Gi For the summarization task, we compare results using ROUGE (Lin, 2004). We report ROUGE-1, -2, -4 recall, with stemming and without removing stopwo"
E14-1075,P11-1050,0,0.0172792,"Missing"
E14-1075,P06-2020,0,0.0215213,"Missing"
E14-1075,C00-1072,0,0.335149,"n automatic summarization (Luhn, 1958). There keywords were identified based on the number of times they appeared in the input, and words that appeared most and least often were excluded. Then the sentences in which keywords appeared near each other, presumably better conveying the relationship between the keywords, were selected to form a summary. Many successful recent systems also estimate word importance. The simplest but competitive way to do this task is to estimate the word probability from the input (Nenkova and Vanderwende, 2005). Another powerful method is log-likelihood ratio test (Lin and Hovy, 2000), which identifies the set of words that appear in the input more often than in a background corpus (Conroy et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches,"
E14-1075,P06-1039,0,0.0164541,"Missing"
E14-1075,W04-1013,0,0.0624636,"he formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 i Mean |Gi | 1 102 2 32 3 15 4 6 Table 1: Average number of words in Gi For the summarization task, we compare results using ROUGE (Lin, 2004). We report ROUGE-1, -2, -4 recall, with stemming and without removing stopwords. We consider ROUGE-2 recall as the main metric for this comparison due to its effectiveness in comparing machine summaries (Owczarzak et al., 2012). All of the summaries were truncated to the first 100 words by ROUGE4 . We use Wilcoxon signed-rank test to examine the statistical significance as advocated by Rankel et al. (2011) for both tasks, and consider differences to be significant if the p-value is less than 0.05. 4 Unsupervised Word Weighting Data and Planned Experiments In this section we describe three uns"
E14-1075,P10-1095,0,0.00950818,"roaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 i Mean |Gi | 1 102 2 32 3 15 4 6 Table 1: Average number of words in Gi For the summarization task, we compare results using ROUGE (Lin, 2004). We report ROUGE-1, -2, -4 recall, with stemming and without removing stopwords. We consider ROUGE"
E14-1075,C04-1057,0,0.0400853,"his task is to estimate the word probability from the input (Nenkova and Vanderwende, 2005). Another powerful method is log-likelihood ratio test (Lin and Hovy, 2000), which identifies the set of words that appear in the input more often than in a background corpus (Conroy et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). Introduction In automatic extractive summarization, sentence importance is calculated by taking into account, among possibly other features, the importance of words that appear in the sentence. In this paper, we describe experiments on identifying words from the input that are also included in human summaries; we call such words s"
E14-1075,P05-1045,0,0.0137775,"MRW scores as features. Since prior work has demonstrated that for LLR weights in 5 10, 15, 20, 30, 40, · · · , 190, 200, 220, 240, 260, 280, 300, 350, 400, 450, 500, 600, 700 (in total 33 values) 716 first sentence and the number of times it appears in a first sentence among documents in one input. There are 6 features in this group. All of them are very significant, ranked within the top 100. Word type: These features include Part of Speech (POS) tags, Name Entity (NE) labels and capitalization information. We use the Stanford POS-Tagger (Toutanova et al., 2003) and Name Entity Recognizer (Finkel et al., 2005). We have one feature corresponding to each possible POS and NE tag. The value of this feature is the proportion of occurrences of the word with this tag; in most cases only one feature gets a non-zero value. We have two features which indicate if one word has been capitalized and the ratio of its capitalized occurrences. Most of the NE features (6 out of 8) are significant: there are more Organizations and Locations but fewer Time and Date words in the human summaries. Of the POS tags, 11 out of 41 are significant: there are more nouns (NN, NNS, NNPS); fewer verbs (VBG, VBP, VB) and fewer car"
E14-1075,de-marneffe-etal-2006-generating,0,0.0129757,"aph methods have been successfully applied to weighting sentences for generic (Wan and Yang, 2008; Mihalcea and Tarau, 2004; Erkan and Radev, 2004) and query-focused summarization (Otterbacher et al., 2009). Here instead of constructing a graph with sentences as nodes and edges weighted by sentence similarity, we treat the words as vertices, similar to Mihalcea and Tarau (2004). The difference in our approach is that the edges between the words are defined by syntactic dependencies rather than depending on the co-occurrence of words within a window of k. We use the Stanford dependency parser (Marneffe et al., 2006). In our approach, we consider a word w more likely to be included in a human summary when it is syntactically related to other (important) words, even if w itself is not mentioned often. The edge weight between two vertices is equal to the number of syntactic dependencies of any type between two words within the same sentence in the input. The weights are then normalized by summing up the weights of edges linked to one node. We apply the Pagerank algorithm (Lawrence et al., 1998) on the resulting graph. We set the probability of performing random jump between nodes λ=0.15. The algorithm termi"
E14-1075,P07-2049,1,0.584137,"on Sunday because he wasn’t feeling well, his spokesman said. Doctors ordered Russian President Boris Yeltsin to cut short his Central Asian trip because of a respiratory infection and he agreed to return home Monday, a day earlier than planned, officials said. Table 4: Summary comparison by Random, Blind Extraction and First Sentence systems of NYT abstract-original pairs encodes highly relevant information about important content independent of the actual text of the input. 7 particular, it is useful to identify a small set of important words and ignore all other words in summary selection (Gupta et al., 2007), we use a number of keyword indicators as features. For these indicators, the value of feature is 1 if the word is ranked within top ki , 0 otherwise. Here ki are preset cutoffs5 . These cutoffs capture different possibilities for defining the keywords in the input. We also add the number of input documents that contain the word as a feature. There are a total of 100 features in this group, all of which are highly significant, ranked among the top 200. Regression-Based Keyword Extraction Here we introduce a logistic regression model for assigning importance weights to words in the input. Cruc"
E14-1075,W11-0507,0,0.0313794,"ty of performing random jump between nodes λ=0.15. The algorithm terminates when the change of node weight between iterations is smaller than 10−4 for all nodes. Word importance is equal to the final weight of its corresponding node in the graph. 5.2 KL Divergence Summarizer The KLSUM summarizer (Haghighi and Vanderwende, 2009) aims at minimizing the KL divergence between the probability distribution over words estimated from the summary and the input respectively. This summarizer is a component of the popular topic model approaches (Daum´e and Marcu, 2006; Celikyilmaz and Hakkani-T¨ur, 2011; Mason and Charniak, 2011) and achieves competitive performance with minimal differences compared to a full-blown topic model system. 6 Global Indicators from NYT Some words evoke topics that are of intrinsic interest to people. Here we search for global indicators of word importance regardless of particular input. 5 Summary Generation Process 6.1 Global Indicators of Word Importance In this section, we outline how summaries are generated by a greedy optimization system which selects the sentence with highest weight iteratively. This is the main process we use in all our summarization systems. For comparison we also us"
E14-1075,N09-1041,0,0.304105,"equal to the number of syntactic dependencies of any type between two words within the same sentence in the input. The weights are then normalized by summing up the weights of edges linked to one node. We apply the Pagerank algorithm (Lawrence et al., 1998) on the resulting graph. We set the probability of performing random jump between nodes λ=0.15. The algorithm terminates when the change of node weight between iterations is smaller than 10−4 for all nodes. Word importance is equal to the final weight of its corresponding node in the graph. 5.2 KL Divergence Summarizer The KLSUM summarizer (Haghighi and Vanderwende, 2009) aims at minimizing the KL divergence between the probability distribution over words estimated from the summary and the input respectively. This summarizer is a component of the popular topic model approaches (Daum´e and Marcu, 2006; Celikyilmaz and Hakkani-T¨ur, 2011; Mason and Charniak, 2011) and achieves competitive performance with minimal differences compared to a full-blown topic model system. 6 Global Indicators from NYT Some words evoke topics that are of intrinsic interest to people. Here we search for global indicators of word importance regardless of particular input. 5 Summary Gen"
E14-1075,W04-3252,0,0.0651402,"summaries were truncated to the first 100 words by ROUGE4 . We use Wilcoxon signed-rank test to examine the statistical significance as advocated by Rankel et al. (2011) for both tasks, and consider differences to be significant if the p-value is less than 0.05. 4 Unsupervised Word Weighting Data and Planned Experiments In this section we describe three unsupervised approaches of assigning importance weights to words. The first two are probability and log-likelihood ratio, which have been extensively used in prior work. We also apply a markov random walk model for keyword ranking, similar to Mihalcea and Tarau (2004). In the next section we describe a summarizer that uses these weights to form a summary and then describe our regression approach to combine these and other predictors in order to achieve more accurate predictions for the word importance in Section 7. The task is to assign a score to each word in the input. The keywords extracted are thus the content words with highest scores. We carry out our experiments on two datasets from the Document Understanding Conference (DUC) (Over et al., 2007). DUC 2003 is used for training and development, DUC 2004 is used for testing. These are the last two year"
E14-1075,hong-etal-2014-repository,1,0.20795,"Missing"
E14-1075,N03-1033,0,0.0387221,"Missing"
E14-1075,P07-1070,0,0.287137,", April 26-30 2014. 2014 Association for Computational Linguistics excluded3 . Table 1 shows the average number of unique content words for the respective keyword gold-standard. Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 i Me"
E14-1075,W12-2601,1,0.653737,"man summaries. Also there has been only a few investigations of suitable features for estimating word importance and identifying keywords in summaries; we address this issue by exploring a range of possible indicators of word importance in our model. 3 i Mean |Gi | 1 102 2 32 3 15 4 6 Table 1: Average number of words in Gi For the summarization task, we compare results using ROUGE (Lin, 2004). We report ROUGE-1, -2, -4 recall, with stemming and without removing stopwords. We consider ROUGE-2 recall as the main metric for this comparison due to its effectiveness in comparing machine summaries (Owczarzak et al., 2012). All of the summaries were truncated to the first 100 words by ROUGE4 . We use Wilcoxon signed-rank test to examine the statistical significance as advocated by Rankel et al. (2011) for both tasks, and consider differences to be significant if the p-value is less than 0.05. 4 Unsupervised Word Weighting Data and Planned Experiments In this section we describe three unsupervised approaches of assigning importance weights to words. The first two are probability and log-likelihood ratio, which have been extensively used in prior work. We also apply a markov random walk model for keyword ranking,"
E14-1075,D11-1043,0,0.170566,"Missing"
E14-1075,C10-1111,0,0.0242701,"round corpus (Conroy et al., 2006; Harabagiu and Lacatusu, 2005). In contrast to selecting a set of keywords, weights are assigned to all words in the input in the majority of summarization methods. Approaches based on (approximately) optimizing the coverage of these words have become widely popular. Earliest such work relied on TF*IDF weights (Filatova and Hatzivassiloglou, 2004), later approaches included heuristics to identify summary-worthy bigrams (Riedhammer et al., 2010). Most optimization approaches, however, use TF*IDF or word probability in the input as word weights (McDonald, 2007; Shen and Li, 2010; Berg-Kirkpatrick et al., 2011). Introduction In automatic extractive summarization, sentence importance is calculated by taking into account, among possibly other features, the importance of words that appear in the sentence. In this paper, we describe experiments on identifying words from the input that are also included in human summaries; we call such words summary keywords. We review several unsupervised approaches for summary keyword identification and further combine these, along with features including position, part-of-speech, subjectivity, topic categories, context and intrinsic imp"
E14-1075,E12-1023,0,0.376681,"ivalent in quality to the standard baseline of extracting the first 100 words from the latest 712 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics excluded3 . Table 1 shows the average number of unique content words for the respective keyword gold-standard. Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in human summaries. Als"
E14-1075,E09-1089,0,0.428392,"omputation on the input, equivalent in quality to the standard baseline of extracting the first 100 words from the latest 712 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 712–721, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics excluded3 . Table 1 shows the average number of unique content words for the respective keyword gold-standard. Word weights have also been estimated by supervised approaches, with word probability and location of occurrence as typical features (Yih et al., 2007; Takamura and Okumura, 2009; Sipos et al., 2012). A handful of investigations have productively explored the mutually reinforcing relationship between word and sentence importance, iteratively re-estimating each in either supervised or unsupervised framework (Zha, 2002; Wan et al., 2007; Wei et al., 2008; Liu et al., 2011). Most existing work directly focuses on predicting sentence importance, with emphasis on the formalization of the problem (Kupiec et al., 1995; Celikyilmaz and Hakkani-Tur, 2010; Litvak et al., 2010). There has been little work directly focused on predicting keywords from the input that will appear in"
E17-2112,Q13-1009,0,0.0293114,"Missing"
E17-2112,W04-1013,0,0.0235056,"ts. Consistent with the observation above, the DUC model is predicting intrinsic importance more aggressively. Only for a handful of sentences the NYT model predicts positive (important) while the DUC model predicts negative (not important). 5.1 Results on Automatic Evaluation The model trained on the NYT corpus is used in the experiments here. Business and politics articles (100 each) with human-generated summaries from NYT2007 are used for evaluation. Summaries generated by summarizers are restricted to 100 words. Summarizer performance is measured by ROUGE-1 (R-1) and ROUGE-2 (R-2) scores (Lin, 2004). Several summarization systems are used for comparison here, including LeadWords, which picks the first 100 words as the summary; RandomRank, which ranks the sentences randomly and then picks the most highly ranked sentences to form a 100-word summary; and Icsisumm (Gillick et al., 2009), a state-of-the-art multi-document summarizer (Hong et al., 2014). Table 4 shows the ROUGE scores for all summarizers. InfoRank significantly outperforms Icsisumm on R-1 score and is on par with it on R-2 score. Both InfoRank and Icsisumm outperform RandomRank by a large margin. These results show that the se"
E17-2112,P11-1049,0,0.253206,"Missing"
E17-2112,P16-1046,0,0.0471687,"izer and the beginning-ofarticle baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline. 1 Ani Nenkova University of Pennsylvania Philadelphia, PA 19104 Introduction 2 To summarize a text, one has to decide what content is important and what can be omitted. With a handful of exceptions (Svore et al., 2007; BergKirkpatrick et al., 2011; Kulesza and Taskar, 2011; Cao et al., 2015; Cheng and Lapata, 2016), modern summarization methods are unsupervised, relying on on-the-fly analysis of the input text to generate the summary, without using indicators of intrinsic importance learned from previously seen document-summary pairs. This state of the art is highly unintuitive, as it stands to reason that some aspects of importance are learnable. Recent work has demonstrated that indeed supervised systems can perform well without sophisticated features when sufficient training data is available (Cheng and Lapata, 2016). In this paper we demonstrate that in the context of news it is possible to learn an"
E17-2112,D07-1047,0,0.039352,"ach, combined with the “beginning of document” heuristic, outperforms a state-ofthe-art summarizer and the beginning-ofarticle baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline. 1 Ani Nenkova University of Pennsylvania Philadelphia, PA 19104 Introduction 2 To summarize a text, one has to decide what content is important and what can be omitted. With a handful of exceptions (Svore et al., 2007; BergKirkpatrick et al., 2011; Kulesza and Taskar, 2011; Cao et al., 2015; Cheng and Lapata, 2016), modern summarization methods are unsupervised, relying on on-the-fly analysis of the input text to generate the summary, without using indicators of intrinsic importance learned from previously seen document-summary pairs. This state of the art is highly unintuitive, as it stands to reason that some aspects of importance are learnable. Recent work has demonstrated that indeed supervised systems can perform well without sophisticated features when sufficient training data is available (Cheng and"
E17-2112,hong-etal-2014-repository,1,0.850304,"Business and politics articles (100 each) with human-generated summaries from NYT2007 are used for evaluation. Summaries generated by summarizers are restricted to 100 words. Summarizer performance is measured by ROUGE-1 (R-1) and ROUGE-2 (R-2) scores (Lin, 2004). Several summarization systems are used for comparison here, including LeadWords, which picks the first 100 words as the summary; RandomRank, which ranks the sentences randomly and then picks the most highly ranked sentences to form a 100-word summary; and Icsisumm (Gillick et al., 2009), a state-of-the-art multi-document summarizer (Hong et al., 2014). Table 4 shows the ROUGE scores for all summarizers. InfoRank significantly outperforms Icsisumm on R-1 score and is on par with it on R-2 score. Both InfoRank and Icsisumm outperform RandomRank by a large margin. These results show that the sentence importance detector 710 Table 3: Example of unimportant content in the opening paragraph of an article. The detected unimportant sentences are italicized. The third panel shows a new summary, with unimportant content skipped. Human Summary: Pres Bush and his aides insist United States is committed to diplomatic path in efforts to stop Iran’s susp"
E17-2112,P13-2123,0,0.0636677,"Missing"
E17-2112,N03-1037,0,0.0795862,"ophisticated features when sufficient training data is available (Cheng and Lapata, 2016). In this paper we demonstrate that in the context of news it is possible to learn an accurate predictor to decide if a sentence contains content that is summary-worthy. We show that the predictors built in our approach are remarkably consistent, providing almost identical predictions on a Corpora One of the most cited difficulties in using supervised methods for summarization has been the lack of suitable corpora of document-summary pairs where each sentence is clearly labeled as either important or not (Zhou and Hovy, 2003). We take advantage of two currently available resources: archival data from the Document Understanding Conferences (DUC) (Over et al., 2007) and the New York Times (NYT) corpus (https://catalog.ldc.upenn.edu/ LDC2008T19). The DUC data contains document-summary pairs in which the summaries were produced for research purposes during the preparation of a shared task for summarization. The NYT dataset contains thousands such pairs and the summaries were written by information scientists working for the newspaper. DUC2002 is the latest dataset from the DUC series in which annotators produced extra"
H05-1031,J98-3005,1,\N,Missing
H05-1031,J98-2001,0,\N,Missing
H05-1031,N03-2024,1,\N,Missing
H05-1031,C04-1129,1,\N,Missing
H05-1031,J95-2003,0,\N,Missing
H05-1031,grover-etal-2000-lt,0,\N,Missing
H05-1031,J96-2004,0,\N,Missing
H05-1031,J86-3001,0,\N,Missing
hong-etal-2014-repository,W12-2601,1,\N,Missing
hong-etal-2014-repository,W04-3252,0,\N,Missing
hong-etal-2014-repository,N09-1041,0,\N,Missing
hong-etal-2014-repository,C00-1072,0,\N,Missing
hong-etal-2014-repository,P06-2020,1,\N,Missing
hong-etal-2014-repository,J13-2002,1,\N,Missing
hong-etal-2014-repository,D11-1043,1,\N,Missing
hong-etal-2014-repository,W04-1013,0,\N,Missing
hong-etal-2014-repository,E14-1075,1,\N,Missing
hong-etal-2014-repository,C12-2078,0,\N,Missing
hong-etal-2014-repository,P13-2024,1,\N,Missing
hong-etal-2014-repository,P08-1094,1,\N,Missing
hong-etal-2014-repository,radev-etal-2004-mead,0,\N,Missing
hong-etal-2014-repository,E12-1023,0,\N,Missing
hong-etal-2014-repository,P11-1052,1,\N,Missing
hong-etal-2014-repository,W09-1802,1,\N,Missing
hong-etal-2014-repository,P13-1100,0,\N,Missing
hong-etal-2014-repository,P13-1101,0,\N,Missing
hong-etal-2014-repository,W13-3108,1,\N,Missing
I08-1016,J05-3002,0,0.151788,"e key components of effective summarizations are the ability to identify important points in the text and to adequately reword the original text in order to convey these points. Automatic text summarization approaches have offered reasonably well-performing approximations for identifiying important sentences (Lin and Hovy, 2002; Schiffman et al., 2002; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Daum´e III and Marcu, 2006) but, not surprisingly, text (re)generation has been a major challange despite some work on sub-sentential modification (Jing and McKeown, 2000; Knight and Marcu, 2000; Barzilay and McKeown, 2005). An additional drawback of extractive approaches is that estimates for the importance of larger text units such as sentences depend on the length of the sentence (Nenkova et al., 2006). Sentence simplification or compaction algorithms are driven mainly by grammaticality considerations. Whether approaches for estimating importance can be applied to units smaller than sentences and used in text rewrite in the summary production is a question that remains unanswered. The option to operate on smaller units, which can be mixed and matched from the input to give novel combinations in the summary, o"
I08-1016,A00-2018,0,0.0364547,"s well as postmodifiers such as prepositional phrases, appositions, and relative clauses. This means that maximum NPs can be rather complex, covering a wide range of production rules in a context-free grammar. The dependency tree definition of maximum noun phrase makes it easy to see why these are a good unit for subsentential rewrite: the subtree that has the head of the NP as a root contains only modifiers of the head, and by rewriting the noun phrase, the amount of information expressed about the head entity can be varied. In our implementation, a context free grammar probabilistic parser (Charniak, 2000) was used to parse the input. The maximum noun phrases were identified by finding sequences of &lt;np&gt;...&lt;/np&gt; tags in the parse such that the number of opening and closing tags is equal. Each NP identified by such tag spans was considered as a candidate for rewrite. Coreference classes A coreference class CRm is the class of all maximum noun phrases in the input that refer to the same entity Em . The general problem of coreference resolution is hard, and is even more complicated for the multi-document summarization case, in which cross-document resolution needs to be performed. Here we make a si"
I08-1016,P06-2020,0,0.0760866,"Missing"
I08-1016,P06-1039,0,0.0542729,"Missing"
I08-1016,A00-2024,0,0.28606,"amid method evaluation. 1 Introduction Two of the key components of effective summarizations are the ability to identify important points in the text and to adequately reword the original text in order to convey these points. Automatic text summarization approaches have offered reasonably well-performing approximations for identifiying important sentences (Lin and Hovy, 2002; Schiffman et al., 2002; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Daum´e III and Marcu, 2006) but, not surprisingly, text (re)generation has been a major challange despite some work on sub-sentential modification (Jing and McKeown, 2000; Knight and Marcu, 2000; Barzilay and McKeown, 2005). An additional drawback of extractive approaches is that estimates for the importance of larger text units such as sentences depend on the length of the sentence (Nenkova et al., 2006). Sentence simplification or compaction algorithms are driven mainly by grammaticality considerations. Whether approaches for estimating importance can be applied to units smaller than sentences and used in text rewrite in the summary production is a question that remains unanswered. The option to operate on smaller units, which can be mixed and matched from t"
I08-1016,C00-1072,0,0.00890335,"e arrest took place, it will be more appropriate to rewrite sentence 2 into sentence 1 for inclusion in the summary. This example shows the potential power of noun phrase rewrite. It also suggests that context will play a role in the rewrite process, since different noun phrase realizations will be most appropriate depending on what has been said in the summary up to the point at which rewrite takes place. 2 NP-rewrite enhanced frequency summarizer Frequency and frequency-related measures of importance have been traditionally used in text summarization as indicators of importance (Luhn, 1958; Lin and Hovy, 2000; Conroy et al., 2006). Notably, a greedy frequency-driven approach leads to very good results in content selection (Nenkova et al., 2006). In this approach sentence importance is measured as a function of the frequency in the input of the content words in that sentence. The most important sentence is selected, the weight of words in it are adjusted, and sentence weights are recomputed for the new weights beofre selecting the next sentence. This conceptually simple summarization approach can readily be extended to include NP rewrite and allow us to examine the effect of rewrite capabilities on"
I08-1016,W04-3252,0,\N,Missing
I11-1068,N09-1041,0,0.020246,"tails about the topic. We show that our classifier successfully distinguishes these two types of summaries. Summarization is one task where the distinction between general and specific content is relevant. The space available for summary content is limited. So authors include some specific detail but at the same time have to generalize other content to stay within the space limit. Early work in Jing and McKeown (2000) report that when people create summaries, they generalize some of the sentences from the source text, others are made more specific. From the point of view of automatic systems, Haghighi and Vanderwende (2009) developed a topic model-based summarization system which learns the topics of the input at both overall document level as well as specific subtopics. Sentences are assumed to be generated by a combination of the general and specific topics in the input 611 texts. However, since the preference of such sentences is not known, only heuristics were applied to choose the proportions. We expect our classifier to be useful in such cases. 4.1 Data We use summaries and source texts from the Document Understanding Conference (DUC) organized by NIST in 2005.5 The task in 2005 was to create summaries tha"
I11-1068,H05-1044,0,0.00700482,"he specific ones. So we introduced two features—the number of words in the sentence and the number of nouns. Polarity. Sentences with strong opinion are typical in the general category in our examples in Table 2. For instance, the phrases “publishing sensation”, and “very slowly–if at all” are evaluative while the specific sentences in these relations present evidence which justify the general statements. So, we record for each sentence the number of positive, negative and polar (not neutral) words using two lexicons—The General Inquirer (Stone et al., 1966) and the MPQA Subjectivity Lexicon (Wilson et al., 2005). We also add another set of features where each of these counts is normalized by the sentence length. Specificity. Specific sentences are more likely to contain specific words and details. We use two sets of features to capture specificity of words in the sentence. The first of these is based on WordNet (Miller et al., 1990) and is motivated by prior work by Resnik (1995) where hypernym relations from WordNet were used to compute specificity. For each noun and verb in a sentence, we record the length of the path from the word to the root of 606 Instantiations [1] The 40-year-old Mr. Murakami"
I11-1068,A00-2024,0,0.0529215,"luation on news summaries. Here people were asked to write general or specific summaries for a set of articles, in the first type conveying only the general ideas and in the second providing specific details about the topic. We show that our classifier successfully distinguishes these two types of summaries. Summarization is one task where the distinction between general and specific content is relevant. The space available for summary content is limited. So authors include some specific detail but at the same time have to generalize other content to stay within the space limit. Early work in Jing and McKeown (2000) report that when people create summaries, they generalize some of the sentences from the source text, others are made more specific. From the point of view of automatic systems, Haghighi and Vanderwende (2009) developed a topic model-based summarization system which learns the topics of the input at both overall document level as well as specific subtopics. Sentences are assumed to be generated by a combination of the general and specific topics in the input 611 texts. However, since the preference of such sentences is not known, only heuristics were applied to choose the proportions. We expe"
I11-1068,P03-1054,0,0.00812179,"Missing"
I11-1068,prasad-etal-2008-penn,0,0.0599945,"relations generalizes without noticeable compromise in accuracy. We describe our classifier based on discourse relations here, the annotation study is detailed in the next section. 2 2.1 Features A general vs. specific sentence classifier based on discourse relations The task of differentiating general and specific content has not been addressed in prior work, so there is no existing corpus annotated for specificity. For this reason, we first exploit indirect annotations of these distinctions in the form of certain types of discourse relations annotated in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). The discourse relations we consider are Specification and Instantiation. They are defined to hold between adjacent sentences. The definitions of the relations do not talk directly about the specificity of sentences, but they seem to indirectly indicate that the first one is general and the second is specific. The exact definitions of these two relations in the PDTB are given in (Prasad et al., 2007). Some examples are shown in Table 2. The PDTB annotations cover 1 million words from Wall Street Journal (WSJ) articles. Instantiations and Specifications are fairly frequent (1403 and 2370 respe"
I11-1068,P10-1005,0,0.256259,"Missing"
I11-1068,J93-2004,0,\N,Missing
I11-1068,P02-1047,0,\N,Missing
J11-4007,W08-1107,0,0.0255227,"Missing"
J11-4007,J05-3002,1,0.864994,"eport an evaluation of the effect of reference rewriting on summary quality in Section 6, including a discussion of its scope and limitations in Section 6.2. 812 Siddharthan, Nenkova, and McKeown Information Status and References to People 2. Related Work Related research into summarization, information status distinctions, and generating referring expressions is reviewed here. 2.1 Extractive and Abstractive Summarization Multi-document summarization has been an active area of research over the past two decades and yet, barring a few exceptions (Radev and McKeown 1998; Daum´e III et al. 2002; Barzilay and McKeown 2005), most systems still use shallow features to produce an extractive summary, an age-old technique (Luhn 1958) that has well-known problems. Extractive summaries may contain phrases that the reader cannot understand out of context (Paice 1990) and irrelevant phrases that happen to occur in a relevant sentence (Knight and Marcu 2000; Barzilay 2003). Referring expressions in extractive summaries illustrate this, as sentences compiled from different documents might contain too little, too much, or repeated information about the referent. In a study of how summary revisions could be used to improve"
J11-4007,W09-2817,0,0.0534453,"Missing"
J11-4007,W07-2302,0,0.0297396,"stance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky 2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, and Kruijff 2005) have not been used for generation or summarization tasks. Current efforts in the language generation community aim at providing a corpus and evaluation task (the GREC challenge) to address just this issue (Belz and Varges 2007; Belz, Kow, and Viethen 2009). The GREC-2.0 corpus, extracted from Wikipedia articles and annotated for the task of referring expression generation for both ﬁrst and subsequent mentions of the main subject of the article, consists of 2,000 texts in ﬁve different domains (cities, countries, rivers, people, and mountains). The more recent GREC-People corpus consists of 1,000 texts in just one domain (people) but references to all people mentioned in a text have been annotated. The GREC challenges require systems to pick the most appropriate reference in context from a list of all references in"
J11-4007,J96-2004,0,0.191249,"Missing"
J11-4007,A00-2018,0,0.0190877,"containing 651,000 words and coming from 876 news reports from six different news agencies. The variety of sources is important because working with text from a single source could lead to the learning of paper-speciﬁc editorial rules. The reference characteristics we were interested in were number of pre-modiﬁers, presence and type of post-modiﬁers, and the form of name used to refer to people. The corpus was automatically annotated for person name occurrence and co-reference using Nominator (Wacholder, Ravin, and Choi 1997). Syntactic form of references was obtained using Charniak’s parser (Charniak 2000). This automatically annotated corpus contains references to 6,240 distinct people. The distribution of forms for discourse-new and discourse-old references are shown in Table 1. For discourse-old references, computing the probability of a syntactic realization is not as straightforward as for discourse-new references, because the form of the reference is inﬂuenced by the form of previous references, among other factors. To capture this relationship, we used the data from discourse-old mentions to form a Markov chain, which captures exactly the probability of transitioning from one form of ref"
J11-4007,W09-0609,0,0.0146935,"uations. There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts. Recent evaluation exercises such as the TUNA challenge (Gatt, Belz, and Kow 2008) therefore consider metrics other than length of a reference, such as humanness and the time taken by hearers to identify the referent. In a similar vein, Viethen and Dale (2006) examine how similar references produced by well-known algorithms are to human-produced references, and Dale and Viethen (2009) examine differences in human behavior when generating referring expressions. There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky 2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, an"
J11-4007,P02-1057,0,0.0118805,"rse-old is the only information status distinction 815 Computational Linguistics Volume 37, Number 4 that participating systems model, with other features derived from lexical and syntactic context; for instance, Greenbacker and McCoy (2009) consider subjecthood, parallelism, recency, and ambiguity. 2.2.4 Applications of Information Status Distinctions to GRE. The main application of theories of information status has been in anaphora resolution. Information status distinctions are not normally used in work on generating referring expressions, with a few notable exceptions. Krahmer and Theune (2002) show that the relative salience of discourse entities can be taken into account to produce less-informative descriptions (including fewer attributes than those necessary to uniquely identify the referent using a discourse model that does not incorporate salience). In contrast, Jordan and Walker (2005) show that, in task-oriented dialogs, over-speciﬁed references (including more attributes than needed to uniquely identify the intended referent) are more likely for certain dialog states and communicative goals. Some participating teams in the GREC challenges use the discourse-new vs. discourse-"
J11-4007,W10-4203,0,0.0181468,"tractors are added to the referring expression until its interpretation contains only the intended referent. Subsequent work on referring expression generation has (a) expanded the logical framework to allow reference by negation (the dog that is not black) and references to multiple entities (the brown or black dogs) (van Deemter 2002; Gatt and Van Deemter 2007), (b) explored different search algorithms for ﬁnding a minimal description (e.g., Horacek 2003), and (c) offered different representation frameworks such as graph theory (Krahmer, van Erk, and Verleg 2003) or reference domain theory (Denis 2010) as alternatives for representing referring characteristics. This body of research assumes a limited domain where the semantics of attributes and their allowed values can be formalized, though semantic representations and inference mechanisms are getting increasingly sophisticated (e.g., the use of description logic: Areces, Koller, and Striegnitz 2008; Ren, van Deemter, and Pan 2010). In contrast, Siddharthan and Copestake (2004) consider open-domain generation of referring expressions in a regeneration task (text simpliﬁcation); they take a different approach, approximating the hand-coded do"
J11-4007,D08-1019,0,0.0261054,"linguistic rules (e.g., Zajic et al. 2007) often combined with corpus-based information (Jing and McKeown 2000), whereas other approaches use statistical compression applied to news (Knight and Marcu 2000; Daum´e III and Marcu 2002) and to spoken dialogue (Galley and McKeown 2007). Other researchers addressed the problem of generating new sentences to include in a summary. Information fusion, which uses bottom–up multi-sequence alignment of the parse trees of similar sentences, generates new summary sentences from phrases extracted from different document sentences (Barzilay and McKeown 2005; Filippova and Strube 2008). 2.1.2 Summary Revision. Research in single-document summarization on improving summaries through revision (Mani, Gates, and Bloedorn 1999) is closer to our work. Three types of ad hoc revision rules are deﬁned—elimination (removing parentheticals, sentence initial prepositional phrases, and adverbial phrases), aggregation (combining constituents from two sentences), and smoothing. The smoothing operators cover some reference editing operations. They include substitution of a proper name with a name alias if the name is mentioned earlier, expansion of a pronoun with co-referential proper name"
J11-4007,N07-1023,1,0.491252,"h that unclear references in summaries pose serious problems for users (Paice 1990). 2.1.1 Sentence Compression and Fusion. Research in abstractive summarization has largely focused on the problem of compression, developing techniques to edit sentences by removing information that is not salient from extracted sentences. Some approaches use linguistic rules (e.g., Zajic et al. 2007) often combined with corpus-based information (Jing and McKeown 2000), whereas other approaches use statistical compression applied to news (Knight and Marcu 2000; Daum´e III and Marcu 2002) and to spoken dialogue (Galley and McKeown 2007). Other researchers addressed the problem of generating new sentences to include in a summary. Information fusion, which uses bottom–up multi-sequence alignment of the parse trees of similar sentences, generates new summary sentences from phrases extracted from different document sentences (Barzilay and McKeown 2005; Filippova and Strube 2008). 2.1.2 Summary Revision. Research in single-document summarization on improving summaries through revision (Mani, Gates, and Bloedorn 1999) is closer to our work. Three types of ad hoc revision rules are deﬁned—elimination (removing parentheticals, sente"
J11-4007,W08-1131,0,0.160759,"Missing"
J11-4007,J95-2003,0,0.277775,"Missing"
J11-4007,J86-3001,0,0.385105,"48, with W EKA parameters: “J48 -U -M 4”). We now discuss what features we used for our two classiﬁcation tasks (see the list of features in Table 2). Our hypothesis is that features capturing the frequency and syntactic and lexical forms of references are sufﬁcient to infer the desired distinctions. The frequency features are likely to give a good indication of the global salience of a person in the document set. Pronominalization indicates that an entity was particularly salient at a speciﬁc point of the discourse, as has been widely discussed in attentional status and centering literature (Grosz and Sidner 1986; Gordon, Grosz, and Gilliom 1993). Modiﬁed noun phrases (with apposition, relative clauses, or pre-modiﬁcation) can also signal different information status; for instance, we expect post-modiﬁcation to be more prevalent for characters who are less familiar. For our lexical features, we used two months worth of news articles collected over the Web (and independent of the DUC collection) to collect unigram and bigram lexical models of discourse-new references of people. The names themselves were removed from the discourse-new reference noun phrases and the counts were collected over the pre-mod"
J11-4007,C00-1045,0,0.728214,"Missing"
J11-4007,A00-2024,1,0.290386,"Missing"
J11-4007,P03-1054,0,0.0123544,"cision Section Prediction Accuracy Discourse-new references Include Name Section 5.1 Include Role & temporal mods Include Afﬁliation Include Post-Modiﬁcation Section 5.3.1 Section 5.3.2 Section 5.2 .74 (rising to .92 when there is unanimity among human summarizers) .79 .75 to .79 (depending on rule) .72 (rising to 1.00 when there is unanimity among human summarizers) Discourse-old references Include Only Surname Section 3 .70 833 Computational Linguistics Volume 37, Number 4 Implementation of Algorithm 4. Our reference rewrite module operates on parse trees obtained using the Stanford Parser (Klein and Manning 2003). For each person automatically identiﬁed using the techniques described in Section 4.1.1, we matched every mention of their surname in the parse trees of MEAD summary sentences. We then replaced the enclosing NP (includes all pre- and post-modifying constituents) with a new NP generated using Algorithm 4. The regenerated summary was produced automatically, without any manual correction of parses, semantic analyses, or information status classiﬁcations. We now enumerate implementation details not covered in Section 4.1.1: 1. Although our algorithm determines when to include role and afﬁliation"
J11-4007,J03-1003,0,0.0161679,"Missing"
J11-4007,P99-1072,0,0.248696,"Missing"
J11-4007,W99-0108,0,0.237814,"identify the referent using a discourse model that does not incorporate salience). In contrast, Jordan and Walker (2005) show that, in task-oriented dialogs, over-speciﬁed references (including more attributes than needed to uniquely identify the intended referent) are more likely for certain dialog states and communicative goals. Some participating teams in the GREC challenges use the discourse-new vs. discourse-old distinction as a feature to help select the most likely reference in context. Different interpretations of Centering Theory have also been used to generate pronominal references (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000). Our research is substantially different in that we model a much richer set of information status distinctions. Also, our choice of the news genre makes our studies complementary to the GREC challenges, which use Wikipedia articles about people or other named entities. News stories tend to be about events, not people, and the choice of initial references to participants is particularly important to help the reader understand the news. Our research is thus largely focused on the generation of initial references. Due to their short length, summaries do not gen"
J11-4007,I08-1016,1,0.86528,"f pronoun replacement), is meant to work for all entities, not just mentions to people, and does not incorporate distinctions inferred from the input to the summarizer. Although the rules and the overall approach are based on reasonable intuitions, in practice entity rewrites for summarization do introduce errors, some due to the rewrite rules themselves, others due to problems with co-reference resolution and parsing. 813 Computational Linguistics Volume 37, Number 4 Readers are very sensitive to these errors and prefer extractive summaries to summaries where all references have been edited (Nenkova 2008). Automatic anaphora resolution for all entities mentioned in the input and summary text is also errorful, with about one third of all substitutions in the summary being incorrect (Steinberger et al. 2007). In contrast, when editing references is restricted to references to people alone, as we do in the work presented here, there are fewer edits per summary but the overall result is perceived as better than the original by readers (Nenkova and McKeown 2003). 2.1.3 Reference in Summaries. There has been little investigation of the phenomenon of reference in news summaries. In addition to the re"
J11-4007,N03-2024,1,0.898914,"Missing"
J11-4007,H05-1031,1,0.891249,"Missing"
J11-4007,W06-1612,0,0.0862228,"r vein, Viethen and Dale (2006) examine how similar references produced by well-known algorithms are to human-produced references, and Dale and Viethen (2009) examine differences in human behavior when generating referring expressions. There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky 2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, and Kruijff 2005) have not been used for generation or summarization tasks. Current efforts in the language generation community aim at providing a corpus and evaluation task (the GREC challenge) to address just this issue (Belz and Varges 2007; Belz, Kow, and Viethen 2009). The GREC-2.0 corpus, extracted from Wikipedia articles and annotated for the task of referring expression generation for both ﬁrst and subsequent mentions of the main subject of t"
J11-4007,W02-0404,0,0.147936,"Missing"
J11-4007,H05-1002,0,0.0614896,"Missing"
J11-4007,A97-1033,1,0.647061,"o errorful, with about one third of all substitutions in the summary being incorrect (Steinberger et al. 2007). In contrast, when editing references is restricted to references to people alone, as we do in the work presented here, there are fewer edits per summary but the overall result is perceived as better than the original by readers (Nenkova and McKeown 2003). 2.1.3 Reference in Summaries. There has been little investigation of the phenomenon of reference in news summaries. In addition to the revision of subsequent references described in Mani, Gates, and Bloedorn (1999), we are aware of Radev and McKeown (1997), who built a prototype system called PROFILE that extracted references to people from news, merging and recording information about people mentioned in various news articles. The idea behind the system was that the rich proﬁles collected for people could be used in summaries of later news in order to generate informative descriptions. However, the collection of information about entities from different contexts and different points in time leads to complications in description generation; for example, past news can refer to Bill Clinton as Clinton, an Arkansas native, the democratic president"
J11-4007,J98-3005,1,0.379455,"m was that the rich proﬁles collected for people could be used in summaries of later news in order to generate informative descriptions. However, the collection of information about entities from different contexts and different points in time leads to complications in description generation; for example, past news can refer to Bill Clinton as Clinton, an Arkansas native, the democratic presidential candidate Bill Clinton, U.S. President Clinton, or former president Clinton and it is not clear which of these descriptions are appropriate to use in a summary of a novel news item. In later work, Radev and McKeown (1998) developed an approach to learn correlations between linguistic indicators and semantic constraints to address such problems, but this line of research has not been pursued further. Next, we review related work on reference outside the ﬁeld of summarization. 2.2 Information Status and Generating Referring Expressions Research on information status distinctions closely relates to work on generating referring expressions. We now overview the two ﬁelds and how they interact. 2.2.1 Information Status Distinctions. Information status distinctions depend on two parameters related to the referent’s p"
J11-4007,W10-4212,0,0.079416,"Missing"
J11-4007,W03-2602,1,0.897915,"Missing"
J11-4007,P04-1052,1,0.822018,"nding a minimal description (e.g., Horacek 2003), and (c) offered different representation frameworks such as graph theory (Krahmer, van Erk, and Verleg 2003) or reference domain theory (Denis 2010) as alternatives for representing referring characteristics. This body of research assumes a limited domain where the semantics of attributes and their allowed values can be formalized, though semantic representations and inference mechanisms are getting increasingly sophisticated (e.g., the use of description logic: Areces, Koller, and Striegnitz 2008; Ren, van Deemter, and Pan 2010). In contrast, Siddharthan and Copestake (2004) consider open-domain generation of referring expressions in a regeneration task (text simpliﬁcation); they take a different approach, approximating the hand-coded domainknowledge of earlier systems with a measure of relatedness for attribute-values that is derived from WordNet synonym and antonym links. 2.2.3 Recent Trends: Data Collection and Evaluations. There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts. Recent evalu"
J11-4007,C04-1129,1,0.871401,"Missing"
J11-4007,P98-2204,0,0.0630138,"hearer-old O R the person’s organization (country/ state/ afﬁliation) has been already mentioned A ND is the most salient organization in the discourse at the point where the reference needs to be generated T HEN the afﬁliation of a person can be omitted in the discourse-new reference. Algorithm 2: Omitting the afﬁliation in a discourse-new reference. Based on our intuitions about discourse salience and information status, we initially postulated the decision procedure in Algorithm 2. We described how we make the hearer-new/hearer-old judgment in Section 4.2. We used a salience-list (S-List) (Strube 1998) to determine the salience of organizations. This is a shallow attentional-state model and works as follows: 1. Within a sentence, entities are added to the salience-list from left to right. 2. Within the discourse, sentences are considered from right to left. In other words, entities in more recent sentences are more salient than those in previous ones, and within a sentence, earlier references are more salient than later ones. Results. To make the evaluation meaningful, we only considered examples where there was an afﬁliation mentioned for the person in the input documents, ruling out the t"
J11-4007,J02-1003,0,0.0174239,"Missing"
J11-4007,W03-0508,0,0.0286631,"Missing"
J11-4007,W06-1410,0,0.0243741,"ness for attribute-values that is derived from WordNet synonym and antonym links. 2.2.3 Recent Trends: Data Collection and Evaluations. There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts. Recent evaluation exercises such as the TUNA challenge (Gatt, Belz, and Kow 2008) therefore consider metrics other than length of a reference, such as humanness and the time taken by hearers to identify the referent. In a similar vein, Viethen and Dale (2006) examine how similar references produced by well-known algorithms are to human-produced references, and Dale and Viethen (2009) examine differences in human behavior when generating referring expressions. There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 200"
J11-4007,A97-1030,0,\N,Missing
J11-4007,W09-0629,0,\N,Missing
J11-4007,grover-etal-2000-lt,0,\N,Missing
J11-4007,C98-2199,0,\N,Missing
J11-4007,radev-etal-2004-mead,0,\N,Missing
J11-4007,E03-1017,0,\N,Missing
J13-2002,P07-1038,0,0.263603,"mmaries. These system summaries or “pseudomodels” are chosen to be the ones which receive high scores based on the one available model summary. We expect that the beneﬁt of pseudomodels will be noticeable in micro-level correlations with pyramid scores. At the macro level, even with multiple human models there is no improvement in correlations compared with a single model, and the addition of less-ideal system summaries is not likely to be better than adding human summaries. 5.1 Related Work The idea of using system output for evaluation was introduced in the context of machine translation by Albrecht and Hwa (2007, 2008). In their method, Albrecht and Hwa (2007) designate some systems to act as pseudoreferences. Then, every candidate translation to be evaluated is compared to the translations produced by the pseudoreferences using a variety of similarity metrics. Each similarity value is then used as a feature and trained to predict the human assigned score for that candidate translation. They show that the scores produced by their regression metric using only system-based references correlates with human judgments to the same extent as scores produced using multiple human reference translations. Also,"
J13-2002,W08-0330,0,0.0753241,"Missing"
J13-2002,W10-1703,0,0.0554496,"Missing"
J13-2002,W11-2103,0,0.0396829,"Missing"
J13-2002,P06-2020,0,0.181305,"Missing"
J13-2002,W00-0408,0,0.268032,"Missing"
J13-2002,W04-3247,0,0.0243617,"for generic summarization (i.e., the set of documents given as input must be summarized to reﬂect the sources as best as possible; in contrast, TAC 2009 tasks can be considered as focused summarization where either a query is provided or an update is required). This method ﬁrst computes a set of topic words from the input using a loglikelihood ratio. The sentences are ranked using the score introduced by Conroy, Schlesinger, and O’Leary (2006): the ratio of the number of unique topic words in the sentence to the unique content words in the sentence. Graph centrality: This approach (Erkan and Radev 2004; Mihalcea and Tarau 2005) performs selection over a graph representation of the input sentences. Each sentence is represented in vector space using unigram word counts. Two sentences are linked when their vectors have a cosine similarity of at least 0.1. When this graph is converted into a Markov chain, we can compute the stationary distribution of the transition matrix deﬁned by the graph’s edges. This stationary distribution gives the probability of visiting each node during repeated random walks through the graph. The high probability nodes are the ones that are most visited and these corr"
J13-2002,W09-1802,0,0.0838456,"ry similarity features are computed using the input, they can be useful features to incorporate in a summarization system. The combination of systems to perform evaluation also provides a way to build a better system. The concern would be how the usefulness of these metrics will change if systems were also optimizing for them. To optimize a metric such as JS divergence exactly would be difﬁcult because the JS divergence score cannot be factored or divided among individual sentences, a necessary condition if the problem should be solved using an Integer Linear Program as in McDonald (2007) and Gillick and Favre (2009). Therefore only greedy methods are possible. In fact, KL divergence was greedily optimized in Haghighi and Vanderwende (2009) to obtain a high performance summarizer. Gaming the evaluation should carry little concern, however, as these metrics are proposed with a view to tuning systems. The metrics we presented are developed for evaluation in a new setting where model summaries are not available and to aid system development and tuning. Further, notice from the micro-level evaluation that a single metric such as JS divergence does not predict content selection performance well for all inputs."
J13-2002,W10-0722,0,0.35282,"ider the ROUGE results as the upper bound of performance for the model-free evaluations that we propose because ROUGE involves direct comparison with the gold-standard summaries. Our metrics are designed to be used when model summaries are not available. 2.5 Automatic Evaluation Without Gold-Standard Summaries All of these methods require signiﬁcant human involvement. In evaluations where goldstandard summaries are needed, assessors ﬁrst read the input documents (10 or more per input) and write a summary. Then manual comparison of system and gold standard is done, which takes additional time. Gillick and Liu (2010) hypothesize that at least 17.5 hours are needed to evaluate two systems under this set up on a standard test set. Moreover, multiple gold-standard summaries are needed for the same input, so different assessors have to read and create summaries. The more reliable evaluation 4 The scores were computed after stemming but stop words were retained in the summaries. 271 Computational Linguistics Volume 39, Number 2 Table 1 Spearman correlation between manual scores and ROUGE metrics on TAC 2009 data (53 systems). All correlations are highly signiﬁcant with p-value < 10−10 . ROUGE variant ROUGE-1 R"
J13-2002,N09-1041,0,0.202792,"em. The combination of systems to perform evaluation also provides a way to build a better system. The concern would be how the usefulness of these metrics will change if systems were also optimizing for them. To optimize a metric such as JS divergence exactly would be difﬁcult because the JS divergence score cannot be factored or divided among individual sentences, a necessary condition if the problem should be solved using an Integer Linear Program as in McDonald (2007) and Gillick and Favre (2009). Therefore only greedy methods are possible. In fact, KL divergence was greedily optimized in Haghighi and Vanderwende (2009) to obtain a high performance summarizer. Gaming the evaluation should carry little concern, however, as these metrics are proposed with a view to tuning systems. The metrics we presented are developed for evaluation in a new setting where model summaries are not available and to aid system development and tuning. Further, notice from the micro-level evaluation that a single metric such as JS divergence does not predict content selection performance well for all inputs. System developers should therefore involve other specialized features as well. Regression of similarity metrics is a better p"
J13-2002,W04-1003,0,0.0129822,"expert evaluation is the quality of the model summaries. Evaluations based on model summaries assume that the gold standards are of high quality. Through the years at TAC, considerable effort has been invested to ensure that the evaluation scores do not vary depending on the particular gold standard. In the early years of TAC only one gold-standard summary was used. During this time, papers reported ANOVA tests examining the factors that most inﬂuenced summary scores from the evaluations and found that the identity of the judge turned out to be the most signiﬁcant factor (McKeown et al. 2001; Harman and Over 2004). But it is desirable that a model summary or a human judgment be representative of important content in general and does not depict the individual biases of the person who created the summary or made the judgment. So the evaluation methodology was reﬁned to remove the inﬂuence of the assessor identity on the evaluation. The pyramid evaluation was also developed with this goal of smoothing out the variation between judges. Gillick and Liu (2010) point out that Mechanical Turk evaluations have this undesirable outcome: The identity 272 Louis and Nenkova Automatic Content Evaluation of the judge"
J13-2002,N04-1022,0,0.00886725,"nt, we ﬁnd that consensus among system summaries is indicative of important content. This result suggests that by combining the content selected by multiple systems, one might be able to build a summary that is better than each of them individually. In fact, this idea of system consensus has been utilized in the development of MT systems for quite some time. One approach in MT is rescoring the n-best list from an individual system’s decoder, and picking the (consensus) translation that is close on average to all translations. Such rescoring is implemented using a minimum Bayes risk technique (Kumar and Byrne 2004; Tromble et al. 2008). The other approach is system combination where the output from multiple systems is combined to produce a new translation. Several techniques including minimum Bayes risk have been applied to perform system combination in machine translation. Shared tasks on system combination have also been organized in recent years to encourage the development of such methods (Callison-Burch et al. 2010, 2011). Such strategies could be a useful direction to explore for summarization as well. 7. Discussion In this article, we have discussed metrics for summary evaluation when human summ"
J13-2002,W04-1013,0,0.48583,"Missing"
J13-2002,N06-1059,0,0.386894,"ting the features. 4.2.1 Distribution Similarity. Measures of similarity between two probability distributions are a natural choice for our task. One would expect good summaries to be characterized by low divergence between probability distributions of words in the input and summary, and by high similarity with the input. We experimented with three common measures: Kullback Leibler (KL) divergence, Jensen Shannon (JS) divergence, and cosine similarity. These three metrics have already been applied for summary evaluation, albeit in a different context. In their study of model-based evaluation, Lin et al. (2006) used KL and JS divergences to measure the similarity between human and machine summaries. They found that JS divergence always outperformed KL divergence. Moreover, the performance of JS divergence was better than standard ROUGE scores for multi-document summarization when multiple human models were used for the comparison. The use of input–summary similarity in Donaway, Drummey, and Mather (2000), which we described in the previous section, is more directly related to our work. But here, inputs and summaries were compared using only one metric: cosine similarity. Kullback Leibler (KL) diverg"
J13-2002,C00-1072,0,0.0493824,"the JS distance is symmetric and always deﬁned. We compute both smoothed and unsmoothed versions of the divergence as summary scores. Vector space similarity: The third metric is cosine overlap between the tf ∗ idf vector representations of input and summary contents. cosθ = vinp .vsumm ||vinp |vsumm || (5) We compute two variants: 1. Vectors contain all words from input and summary. 2. Vectors contain only topic signature words from the input and all words of the summary. Topic signatures are words highly descriptive of the input, as determined by the application of the log-likelihood test (Lin and Hovy 2000). Using only topic signatures from the input to represent text is expected to be more accurate because the reduced vector has fewer dimensions compared with using all the words from the input. 4.2.2 Summary Likelihood. For this approach, we view summaries as being generated according to word distributions in the input. Then the probability of a word in the input would be indicative of how likely it is to be emitted into a summary. Under this generative model, the likelihood of a summary’s content can be computed using different methods and we expect the likelihood to be higher for better quali"
J13-2002,N03-1020,0,0.505299,"nly the one available model. Finally, we explore the feasibility of another measure—similarity between a system summary and the pool of all other system summaries for the same input. This method of comparison with the consensus of systems produces impressively accurate rankings of system summaries, achieving correlation with human rankings above 0.9. 1. Introduction In this work, we present evaluation metrics for summary content which make use of little or no human involvement. Evaluation methods such as manual pyramid scores (Nenkova, Passonneau, and McKeown 2007) and automatic ROUGE scores (Lin and Hovy 2003) rely on multiple human summaries as a gold standard (model) against which they compare a summary to assess how informative the candidate summary is. It is desirable that evaluation of similar quality be done quickly and cheaply ∗ E-mail: lannie@seas.upenn.edu. ∗∗ University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut St., Philadelphia, PA 19104. E-mail: nenkova@seas.upenn.edu. Submission received: 18 June 2011; revised submission received: 23 March 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00123 © 2013 Association for Computational Ling"
J13-2002,D09-1032,1,0.877548,"y, Drummey, and Mather demonstrated that the correlations between manual evaluation using a gold-standard summary and a) manual evaluation using a different gold-standard summary 275 Computational Linguistics Volume 39, Number 2 b) automatic evaluation by directly comparing input and summary6 are the same. Their conclusion was that such automatic methods should be seriously considered as an alternative to evaluation protocols built around the need to compare with a gold standard. These studies, however, do not directly assess the performance of input–summary similarity for ranking systems. In Louis and Nenkova (2009a), we provided the ﬁrst study of several metrics for measuring similarity for this task and presented correlations of these metrics with human produced rankings of systems. We have released a tool, SIMetrix (Summary-Input Similarity Metrics), which computes all the similarity metrics that we explored.7 4.2 Metrics for Computing Similarity In this section, we describe a suite of similarity metrics for comparing the input and summary content. We use cosine similarity, which is standard for many applications. The other metrics fall under three main classes: distribution similarity, summary likel"
J13-2002,E09-1062,1,0.910439,"y, Drummey, and Mather demonstrated that the correlations between manual evaluation using a gold-standard summary and a) manual evaluation using a different gold-standard summary 275 Computational Linguistics Volume 39, Number 2 b) automatic evaluation by directly comparing input and summary6 are the same. Their conclusion was that such automatic methods should be seriously considered as an alternative to evaluation protocols built around the need to compare with a gold standard. These studies, however, do not directly assess the performance of input–summary similarity for ranking systems. In Louis and Nenkova (2009a), we provided the ﬁrst study of several metrics for measuring similarity for this task and presented correlations of these metrics with human produced rankings of systems. We have released a tool, SIMetrix (Summary-Input Similarity Metrics), which computes all the similarity metrics that we explored.7 4.2 Metrics for Computing Similarity In this section, we describe a suite of similarity metrics for comparing the input and summary content. We use cosine similarity, which is standard for many applications. The other metrics fall under three main classes: distribution similarity, summary likel"
J13-2002,W07-0716,0,0.0305552,"however, pseudoreferences of different quality are chosen in an oracle manner (using the human-assigned scores). This setting is not practical because it depends on the actual system scores. In later work, Albrecht and Hwa (2008) use off-the-self machine translation systems as pseudoreferences and show that they can contribute to good results. This later work is a more realistic set-up and here regression is important because we have no guarantees as to the quality of the off-the-shelf systems on the test data. A similar idea of augmenting machine output to human gold standard was explored in Madnani et al. (2007) in the context of machine translation (MT). For tuning MT systems, often multiple reference translations are required. Madnani et al. augmented reference translations of a sentence with automatically generated paraphrases of the reference. They found in the experiments that such augmentation helped in MT tuning—the number of reference translations needed could be cut in half and compensated with automatic paraphrases. 5.2 Choice of Pseudoreference Systems For this evaluation, the choice of the pseudoreference system is an important step. In this section, we detail some development experiments"
J13-2002,P08-1094,1,0.891924,"ndard. Saggion et al. (2010) report that trends can be observed in the JSD metric performance although it does not provide good evaluations for opinion and biographical type inputs. Automatic evaluations in different genres therefore have different requirements and exploring these is an avenue for future work. Input–summary similarity based only on word distribution works well for evaluating summaries of cohesive-type inputs. We can also envision a situation where we will be able to predict whether the JS divergence evaluation will be accurate or not on a particular test set. In prior work in Nenkova and Louis (2008) and Louis and Nenkova (2009b), we have explored properties of summarization inputs and provided a characterization of inputs into cohesive and less cohesive based on automatic features. The less cohesive inputs were found to be the ones where automatic systems in general performed poorly. In that work, we proposed features to predict if an input is cohesive or not. We now apply these features to the TAC’09 data with the intention of automatically identifying inputs suitable for JS divergence evaluation (the cohesive ones). The features were trained on data from previous years of TAC evaluatio"
J13-2002,N04-1019,1,0.739737,"y’s content is called coverage. These coverage scores were taken as indicators of content quality for the system summaries. Different people include very different content in their summaries, however, and so the coverage scores can vary depending on which model is used (Rath, Resnick, and Savage 1961). This problem of bias in evaluation was later addressed by the pyramid technique, which combines information from multiple model summaries to compose the reference for evaluation. Since 2005, the pyramid evaluation method has become standard. 2.2 Pyramid Evaluation The pyramid evaluation method (Nenkova and Passonneau 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova, Passonneau, and McKeown 2007). It uses multiple human models from which annotators identify semantically deﬁned Summary Content Units (SCUs). Each SCU is assigned a weight equal to the number of human model summaries that express that SCU. An ideal maximally informative summary would express a subset of the most highly weighted SCUs, with multiple maximally informative summaries being possible. The pyramid score for a system sum"
J13-2002,W09-2806,0,0.0334818,"Missing"
J13-2002,radev-etal-2004-mead,0,0.169143,"Missing"
J13-2002,D08-1065,0,0.0124538,"sus among system summaries is indicative of important content. This result suggests that by combining the content selected by multiple systems, one might be able to build a summary that is better than each of them individually. In fact, this idea of system consensus has been utilized in the development of MT systems for quite some time. One approach in MT is rescoring the n-best list from an individual system’s decoder, and picking the (consensus) translation that is close on average to all translations. Such rescoring is implemented using a minimum Bayes risk technique (Kumar and Byrne 2004; Tromble et al. 2008). The other approach is system combination where the output from multiple systems is combined to produce a new translation. Several techniques including minimum Bayes risk have been applied to perform system combination in machine translation. Shared tasks on system combination have also been organized in recent years to encourage the development of such methods (Callison-Burch et al. 2010, 2011). Such strategies could be a useful direction to explore for summarization as well. 7. Discussion In this article, we have discussed metrics for summary evaluation when human summaries are not present."
J13-2002,W03-0508,0,0.0903101,"Missing"
J13-2002,C10-2122,0,\N,Missing
L16-1620,P10-1018,0,0.0626835,"Missing"
L16-1620,Q14-1037,0,0.0139044,"specificity and they have high consensus as which text segments within the sentence are 2 Appendix: examples Formatting. Each example includes the sentence to be rated in italic as well as the two consecutive sentences immediately before (i.e., immediate context). The ratings are shown in annotator:rating format. For questions, they are formatted as: “underspecified text” — question body (context status) A Entity co-reference 7. underspecified. We plan to release our dataset and further expand it to enable more sophisticated linguistic analysis. We used the Berkeley Entity Resolution System (Durrett and Klein, 2014). High agreement [Ex1: general] Those forces are made up of about 150,000 troops from the United States and upward of 25,000 from other nations. But Dr. Allawi raised the tantalizing prospect of an eventual American withdrawal while giving little away, insisting that a pullout could not be tied to a fixed timetable, but rather to the Iraqi forces’ progress toward standing on their own. That formula is similar to what President Bush and other senior administration officials have spoken about. Ratings: A1:5, A2:5, A3:5 Questions: Q1: “That formula” — What is the formula? (immediate context) Q2:"
L16-1620,I11-1068,1,0.863716,"the the prior context are more likely to trigger questions about the reason behind events, “why” and “how”. Our data is accessible at http://www.cis.upenn.edu/%7Enlp/corpora/lrec16spec.html Keywords: specificity rating, underspecification, discourse 1. Introduction Louis and Nenkova (2012) introduced a corpus of sentences annotated as general or specific. Their definition of sentence specificity relied mostly on examples and intuition, related to the amount of detail contained by the sentence. They used the corpus of general and specific sentences to evaluate a classifier for the binary task (Louis and Nenkova, 2011a) and showed that changes in sentence and overall text specificity are strongly associated with perceptions of text quality. Science writing of the best quality in the New York Times is overall more general than regular science pieces in NYT and contain fewer stretches of specific content (Louis and Nenkova, 2013). Automatic summaries, which are often judged to be incoherent, are significantly more specific than same length human-written summaries for the same events (Louis and Nenkova, 2011b). Sentence specificity is also more robust than sentence length as indicator of which sentences may p"
L16-1620,W11-1605,1,0.776555,"the the prior context are more likely to trigger questions about the reason behind events, “why” and “how”. Our data is accessible at http://www.cis.upenn.edu/%7Enlp/corpora/lrec16spec.html Keywords: specificity rating, underspecification, discourse 1. Introduction Louis and Nenkova (2012) introduced a corpus of sentences annotated as general or specific. Their definition of sentence specificity relied mostly on examples and intuition, related to the amount of detail contained by the sentence. They used the corpus of general and specific sentences to evaluate a classifier for the binary task (Louis and Nenkova, 2011a) and showed that changes in sentence and overall text specificity are strongly associated with perceptions of text quality. Science writing of the best quality in the New York Times is overall more general than regular science pieces in NYT and contain fewer stretches of specific content (Louis and Nenkova, 2013). Automatic summaries, which are often judged to be incoherent, are significantly more specific than same length human-written summaries for the same events (Louis and Nenkova, 2011b). Sentence specificity is also more robust than sentence length as indicator of which sentences may p"
L16-1620,louis-nenkova-2012-corpus,1,0.868603,"form of free text questions. We present results from a pilot annotation with this new scheme and demonstrate good inter-annotator agreement. We found that the lack of specificity distributes evenly among immediate prior context, long distance prior context and no prior context. We find that missing details that are not resolved in the the prior context are more likely to trigger questions about the reason behind events, “why” and “how”. Our data is accessible at http://www.cis.upenn.edu/%7Enlp/corpora/lrec16spec.html Keywords: specificity rating, underspecification, discourse 1. Introduction Louis and Nenkova (2012) introduced a corpus of sentences annotated as general or specific. Their definition of sentence specificity relied mostly on examples and intuition, related to the amount of detail contained by the sentence. They used the corpus of general and specific sentences to evaluate a classifier for the binary task (Louis and Nenkova, 2011a) and showed that changes in sentence and overall text specificity are strongly associated with perceptions of text quality. Science writing of the best quality in the New York Times is overall more general than regular science pieces in NYT and contain fewer stretc"
L16-1620,R11-1037,0,0.133671,"selling records are sensational or what constitutes a valuable member may differ radically. Similarly when a typical argument of a verb is missing from a sentence (Palmer et al., 2005), the reader may have difficulty understanding the full event that is being described. Word choice can also determine the overall specificity of a sentence, by making more explicit the manner in which an action is performed or the identity of the discourse entity, as shown by the contrast of sentence pairs like “The worker cleaned the floor” vs. “The maid swept the floor” (Stinson and Tracy, 1983; Resnik, 1995; McKinlay and Markert, 2011; Nastase et al., 2012). The annotation we propose indirectly provides mechanisms to analyze which of the above intricate linguistic and semantic phenomena trigger the need for clarification of naive readers interested in gaining good understanding of a text. It is developed with the flexibility and intention to enable further analysis such as the classification of triggers and future refinement of annotation, to provide a practical connection between language-related applications and linguistic phenomena. 3. Methodology and corpus summary The annotation is carried out on news articles. Each a"
L16-1620,D12-1017,0,0.0674223,"Missing"
L16-1620,W13-2313,0,0.0271448,"he cause of underspecification in the form of free text questions, and identify if these questions may be answered by information given in previous context. If the annotator chose not to ask any question, she is asked to distinguish if the sentence is most specific (i.e., no underspecified segments) or most general (i.e., the sentence conveys general information that needs no further specification). The latter types of sentences capture generics such as “Cats have four paws.” that do not refer to specific events or entities (Carlson, 2005). Agreement on annotating generic noun phrases is low (Nedoluzhko, 2013), so we adopt a more high-level annotation at the sentence level that can be done with less training and with higher agreement. There are four types of status concerning previous context: • In the immediate context: the answer to the question can be found in the two immediately preceding sentences, a distance shown to be the median length of pronoun chains in writing (Hindle, 1983). Here we use this as the effective context for pronoun resolution. • In some previous context: the answer to the question can be found in the article but it is in a sentence more than two sentences before the one cu"
L16-1620,J05-1004,0,0.0811825,"d for interpreting these properties, it is impossible to verify if a sentence has the same truth value for both the writer and reader. These issues of ability to verify the truth value of a statement are directly related to Wiebe (2000)’s original definition of adjective subjectivity. Sentences like “He is a publishing sensation” and “He is a valuable member of our team” are subjective because different people’s definitions of what selling records are sensational or what constitutes a valuable member may differ radically. Similarly when a typical argument of a verb is missing from a sentence (Palmer et al., 2005), the reader may have difficulty understanding the full event that is being described. Word choice can also determine the overall specificity of a sentence, by making more explicit the manner in which an action is performed or the identity of the discourse entity, as shown by the contrast of sentence pairs like “The worker cleaned the floor” vs. “The maid swept the floor” (Stinson and Tracy, 1983; Resnik, 1995; McKinlay and Markert, 2011; Nastase et al., 2012). The annotation we propose indirectly provides mechanisms to analyze which of the above intricate linguistic and semantic phenomena tri"
L16-1620,petrov-etal-2012-universal,0,0.0184035,"annotation to broader questions in computational linguistics and semantics. 1 All 1388 419 332 317 242 66 24 Table 5: Number of question interrogatives and percentages of associated context status. 0.4 Token fraction 1.0 1.2 Interrogative what who how why which where when Underspecified tokens and context Previously we observed that about a third of the lack of specificity cannot be resolved in prior context. Here we offer insight into the characteristics of the tokens associated with this category of underspecification. In Table 6, we lay out the percentage of universal part-of-speech tags (Petrov et al., 2012) of tokens in underspecified segments their percentage associated with the following: fully specified, resolved in immediate context, in previous context and no context. We also separated the definite determiner “the” from the main determiner category to distinguish between definite and indefinite references. Each token is counted 3924 once if marked by multiple annotators. These numbers clearly show that most of the underspecification comes from content words. Among them, most of the lack of specificity of pronouns and determiners can be resolved in prior context. The definite expression “the"
L16-1620,P10-1005,0,0.0276331,"based on the information in the sentence and commonly shared background knowledge, and key information about the participants and causes of an event are fully expressed in the sentence. These three requirements cover a broad range of linguistic and semantic phenomena. For example a reference to a discourse entity may not be readily interpretable when the reference is anaphoric, by either a pronoun or definite noun phrase, when the reference is by proper name 3921 with which the reader is not familiar or the reference is generic, not referring to a specific discourse entity at all (Dahl, 1975; Reiter and Frank, 2010). Similarly gradable adjectives (Frazier et al., 2008; de Marneffe et al., 2010) like “tall”, “smart” and “valuable” are interpreted according to an assumed standard. If the standard is unknown or if the writer and the reader do not share the same standard for interpreting these properties, it is impossible to verify if a sentence has the same truth value for both the writer and reader. These issues of ability to verify the truth value of a statement are directly related to Wiebe (2000)’s original definition of adjective subjectivity. Sentences like “He is a publishing sensation” and “He is a"
L16-1620,W15-4631,0,0.0418913,"verall more general than regular science pieces in NYT and contain fewer stretches of specific content (Louis and Nenkova, 2013). Automatic summaries, which are often judged to be incoherent, are significantly more specific than same length human-written summaries for the same events (Louis and Nenkova, 2011b). Sentence specificity is also more robust than sentence length as indicator of which sentences may pose comprehension problems and need to be simplified for given audiences (Li and Nenkova, 2015). It is also a stable predictor in identifying high-quality arguments in online discussions (Swanson et al., 2015). Given the demonstrated importance of sentence and text specificity in practical applications and the known shortcomings of the existing annotation, we set out to develop a more detailed framework for annotation of sentence specificity. In the brief annotation guidelines of Louis and Nenkova (2012), the general vs. specific distinction was defined in the following way: “General sentences are broad statements about a topic. Specific sentences contain details and can be used to support or explain the general sentences further. In other words, general sentences create expectations in the minds o"
louis-nenkova-2012-corpus,J93-2004,0,\N,Missing
louis-nenkova-2012-corpus,prasad-etal-2008-penn,0,\N,Missing
louis-nenkova-2012-corpus,I11-1068,1,\N,Missing
louis-nenkova-2012-corpus,W11-1605,1,\N,Missing
N03-2024,A00-2018,0,\N,Missing
N03-2024,A97-1030,0,\N,Missing
N04-1019,W03-0508,0,0.666528,"f the Lockerbie bombing A1 [two Libyans]1 [indicted]1 B1 [Two Libyans were indicted]1 C1 [Two Libyans,]1 [accused]1 D2 [Two Libyan suspects were indicted]1 W=4 W=4 SCU2 (w=3): the indictment of the two Lockerbie suspects was in 1991 A1 [in 1991]2 B1 [in 1991]2 D2 [in 1991.]2 Figure 2: Two of six optimal summaries with 4 SCUs The remaining parts of the four sentences above end up as contributors to nine different SCUs of different weight and granularity. Though we look at multidocument summaries rather than single document ones, SCU annotation otherwise resembles the annotation of factoids in (Halteren and Teufel, 2003); as they do with factoids, we find increasing numbers of SCUs as the pool of summaries grows. For our 100 word summaries, we find about 3440 distinct SCUs across four summaries; with ten summaries this number grows to about 60. A more complete comparison of the two approaches follows in section 4. An SCU consists of a set of contributors that, in their sentential contexts, express the same semantic content. An SCU has a unique index, a weight, and a natural language label. The label, which is subject to revision throughout the annotation process, has three functions. First, it frees the annot"
N04-1019,W02-0406,0,0.0701845,"than an obstacle. In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). To show that an automatic method is a reasonable approximation of human judgments, one needs to demonstrate that these can be reliably elicited. However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization (Rath et al., 1961) (Lin and Hovy, 2002). Our approach tailors the evaluation to observed distributions of content over a pool of human summaries, rather than to human judgments of summaries. Our method involves semantic matching of content units to which differential weights are assigned based on their frequency in a corpus of summaries. This can lead to more stable, more informative scores, and hence to a meaningful content evaluation. We create a weighted inventory of Summary Content Units–a pyramid–that is reliable, predictive and diagnostic, and which constitutes a resource for investigating alternate realizations of the same m"
N04-1019,N03-1020,0,0.94887,"erties of pyramid scores, can be found in (Passonneau and Nenkova, 2003). 2 Current Approach: the Document Understanding Conference 1 Introduction 2.1 DUC Evaluating content selection in summarization has proven to be a difficult problem. Our approach acknowledges the fact that no single best model summary exists, and takes this as a foundation rather than an obstacle. In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). To show that an automatic method is a reasonable approximation of human judgments, one needs to demonstrate that these can be reliably elicited. However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization (Rath et al., 1961) (Lin and Hovy, 2002). Our approach tailors the evaluation to observed distributions of content over a pool of human summaries, rather than to human judgments of summaries. Our method involves semantic matching of content units to which differential weights are assig"
N04-1019,P02-1040,0,0.127909,"our next step, the feasibility of automating our method. A more detailed account of the work described here, but not including the study of distributional properties of pyramid scores, can be found in (Passonneau and Nenkova, 2003). 2 Current Approach: the Document Understanding Conference 1 Introduction 2.1 DUC Evaluating content selection in summarization has proven to be a difficult problem. Our approach acknowledges the fact that no single best model summary exists, and takes this as a foundation rather than an obstacle. In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). To show that an automatic method is a reasonable approximation of human judgments, one needs to demonstrate that these can be reliably elicited. However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization (Rath et al., 1961) (Lin and Hovy, 2002). Our approach tailors the evaluation to observed distributions of content over a pool of"
N04-1019,passonneau-2004-computing,1,0.329784,"separate syntactic from semantic agreement, as in (Klavans et al., 2003). Because constituent structure is not relevant here, we normalize all contributors before computing reliability. We treat every word in a summary as a coding unit, and the SCU it was assigned to as the coding value. We require every surface word to be in exactly one contributor, and every contributor to be in exactly one SCU, thus an SCU annotation constitutes a set of equivalence classes. Computing reliability then becomes identical to comparing the equivalence classes constituting a set of coreference annotations. In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements. Applying the same data representation and reliability formula (Krippendorff’s Alpha) as in (Passonneau, 2004), and a distance metric that takes into account relative SCU size, to the two codings C1 and C2 yields α = 81. Values above .67 indicate good reliability (Krippendorff, 1980). 0.9 0.8 Summary score Max = C1 C2 Consensus 0.7 0.6 0.5 arv d30042.b min d30042.b max d30042.b arv d30042.q min d30042.q max d30042.q 0.4 0.3 1 (9) 2 (36) 3 (84) 4"
N04-1019,P03-1048,0,0.00835103,"Missing"
N07-1002,P04-1086,0,0.537501,"Missing"
N07-1002,nissim-etal-2004-annotation,0,0.211469,"nd contrast information) to investigate the relative usefulness of both linguistic and shallow features, as well as how well different features combine with each other. 9 Proceedings of NAACL HLT 2007, pages 9–16, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics 2 Data and features For our experiments we use 12 Switchboard (Godfrey et al., 1992) conversations, 14,555 tokens in total. Each word was manually labeled for presence or absence of pitch accent1 , as well as additional features including information status (or givenness), contrast and animacy distinctions, (Nissim et al., 2004; Calhoun et al., 2005; Zaenen et al., 2004), features that linguistic literature suggests are predictive of prominence (Bolinger, 1961; Chafe, 1976). All of the features described in detail below have been shown to have statistically significant correlation with prominence (Brenier et al., 2006). Information status The information status (IS), or givenness, of discourse entities is important for choosing appropriate reference form (Prince, 1992; Gundel et al., 1993) and possibly plays a role in prominence decisions as well (Brown, 1983). No previous studies have examined the usefulness of inf"
N07-1002,W06-1612,0,0.117177,"Missing"
N07-1002,P00-1030,0,0.0580454,"Missing"
N07-1002,W99-0619,0,0.0323707,"res. theyold have all the WATERnew theyold WANT. theyold can ACTUALLY PUMP waterold. 1 Introduction Being able to predict the prominence or pitch accent status of a word in conversational speech is important for implementing text-to-speech in dialog systems, as well as in detection of prosody in conversational speech recognition. Previous investigations of prominence prediction from text have primarily relied on robust surface features with some deeper information structure features. Surface features like a word’s part-of-speech (POS) (Hirschberg, 1993) and its unigram and bigram probability (Pan and McKeown, 1999; Pan and 0 Thanks to the Edinburgh-Stanford Link and ONR (MURI award N000140510388) for generous support. While previous models have attempted to capture global properties of words (via POS or unigram probability), they have not in general used word identity as a predictive feature, assuming either that current supervised training sets would be too small or that word identity would not be robust across genres (Pan et al., 2002). In this paper, we show a way to capture word identity in a feature, accent ratio, that works well with current small supervised training sets, and is robust to genre"
N07-1002,W05-0307,1,0.918816,"on) to investigate the relative usefulness of both linguistic and shallow features, as well as how well different features combine with each other. 9 Proceedings of NAACL HLT 2007, pages 9–16, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics 2 Data and features For our experiments we use 12 Switchboard (Godfrey et al., 1992) conversations, 14,555 tokens in total. Each word was manually labeled for presence or absence of pitch accent1 , as well as additional features including information status (or givenness), contrast and animacy distinctions, (Nissim et al., 2004; Calhoun et al., 2005; Zaenen et al., 2004), features that linguistic literature suggests are predictive of prominence (Bolinger, 1961; Chafe, 1976). All of the features described in detail below have been shown to have statistically significant correlation with prominence (Brenier et al., 2006). Information status The information status (IS), or givenness, of discourse entities is important for choosing appropriate reference form (Prince, 1992; Gundel et al., 1993) and possibly plays a role in prominence decisions as well (Brown, 1983). No previous studies have examined the usefulness of information status in nat"
N07-1002,W04-0216,0,\N,Missing
N10-1043,J95-2003,0,0.792196,"Missing"
N10-1043,N06-2015,0,0.0305724,"Missing"
N10-1043,J04-3003,0,0.203222,"Missing"
N10-1043,prasad-etal-2008-penn,0,0.237627,"Missing"
N12-1002,P11-2020,1,0.77456,"monstrated that mimicry of posture and behavior led to increased liking between the dialogue participants as well as a smoother interaction. They also found that naturally empathetic individuals exhibited a greater degree of mimicry than did others. Nenkova et al. (2008) found that entrainment on high-frequency words was correlated with naturalness, task success, and coordinated turn-taking behavior. Natale (1975) showed that an individual’s social desirability, or “propensity to act in a social manner,” can predict the degree to which that individual will match her partner’s vocal intensity. Levitan et al. (2011) showed that entrainment on backchannel-preceding cues is correlated with shorter latency between turns, fewer interruptions, and a higher degree of task success. In a study of married couples discussing problems in their relationships, Lee et al. (2010) found that entrainment measures derived from pitch features were significantly higher in positive interactions than in negative interactions and were predictive of the polarity of the participants’ attitudes. These studies have been motivated by theoretical models such as Giles’ Communication Accommodation Theory (Giles & Coupland, 1991), whic"
N12-1002,P08-2043,1,0.590389,"of spoken language, including speakers’ choice of referring expressions (Brennan & Clark, 1996); linguistic style (Niederhoffer & Pennebaker, 2002; Danescu-Niculescu-Mizil et al., 2011); syntactic Entrainment in many of these dimensions has also been associated with different measures of dialogue success. For example, Chartrand and Bargh (1999) demonstrated that mimicry of posture and behavior led to increased liking between the dialogue participants as well as a smoother interaction. They also found that naturally empathetic individuals exhibited a greater degree of mimicry than did others. Nenkova et al. (2008) found that entrainment on high-frequency words was correlated with naturalness, task success, and coordinated turn-taking behavior. Natale (1975) showed that an individual’s social desirability, or “propensity to act in a social manner,” can predict the degree to which that individual will match her partner’s vocal intensity. Levitan et al. (2011) showed that entrainment on backchannel-preceding cues is correlated with shorter latency between turns, fewer interruptions, and a higher degree of task success. In a study of married couples discussing problems in their relationships, Lee et al. (2"
N15-1023,I13-1010,0,0.0410107,"on et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/everybody burial/funeral rail/the train physicians/a doctor Table 1: Paraphrases with large style differences. Our method learns these distinctions au"
N15-1023,N13-1078,0,0.0418022,"on et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/everybody burial/funeral rail/the train physicians/a doctor Table 1: Paraphrases with large style differences. Our method learns these distinctions au"
N15-1023,C10-2011,0,0.200595,"wn et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/everybody burial/funeral rail/the train physicians/a doctor Table 1: Paraphrases with large style differences. Our method lear"
N15-1023,J92-4003,0,0.0950185,"ns to genre analysis and paraphrasing. 1 Introduction True language understanding requires comprehending not just what is said, but how it is said, yet only recently have computational approaches been applied to the subtleties of tone and style. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al.,"
N15-1023,N04-1025,0,0.0231148,"Missing"
N15-1023,P11-2117,0,0.0203954,"way consistent with human judgements. 2 Method We focus on two style dimensions: formality and complexity. We define formal language as the way one talks to a superior, whereas casual language is used with friends. We define simple language to be that used to talk to children or non-native English speakers, whereas more complex language is used by academics or domain experts. We use the Europarl corpus of parliamentary proceedings as an example of formal text and the Switchboard corpus of informal telephone conversations as casual text. We use articles from Wikipedia and simplified Wikipedia (Coster and Kauchak, 2011) as examples of complex and simple language respectively. For each style dimension, we subsample sentences from the larger corpus so that the two ends of the spectrum are roughly balanced. We end up with roughly 300K sentences each for formal/casual text and about 500K sentences each for simple/complex text.1 Given examples of language at each end of a style dimension, we score a phrase by the log ratio of the probability of observing the word in the reference corpus (REF) to observing it in the combined corpora (ALL). For formality the reference corpus is Europarl and the combined data is Eur"
N15-1023,P12-1094,0,0.0134308,"have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr jour"
N15-1023,N12-1094,0,0.00641177,"d search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple g"
N15-1023,P13-1174,0,0.0163253,"Introduction True language understanding requires comprehending not just what is said, but how it is said, yet only recently have computational approaches been applied to the subtleties of tone and style. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Ca"
N15-1023,N13-1092,0,0.118147,"Missing"
N15-1023,N09-1057,0,0.0136519,"performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/every"
N15-1023,P10-2013,0,0.0119118,"largest difference in log-ratio style score. Random guessing achieves an accuracy of 0.5. 4.2 annotations, we see some sentences for which the judgement seems unanimous among annotators and some sentences for which there is very little consensus (Table 7). We discuss this variation further in Section 5. Genre characterization Now we explore if the dimensions we learned at the sub-sentential level can be used to capture stylistic variation at the sentence and genre level. Sentence-level human judgements We gather human ratings of formality and complexity for 900 sentences from the MASC corpus (Ide et al., 2010): 20 sentences from each of 18 genres.2 Recently data from this corpus has been used to study genre difference in terms of pronoun, named entity, punctuation and part of speech usage (Passonneau et al., 2014). We use the data to test a specific hypothesis that automatically induced scores for lexical style are predictive of perceptions of sentence- and genre-level style. We average 7 independent human scores to get sentence-level style scores. To get genre-level style scores, we use the the average of the 20 sentencelevel scores for the sentences belonging to that genre. In human perception, t"
N15-1023,Y09-1024,0,0.0297932,"anguage understanding requires comprehending not just what is said, but how it is said, yet only recently have computational approaches been applied to the subtleties of tone and style. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh"
N15-1023,Q13-1028,1,0.817237,"rucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his fa"
N15-1023,N04-1043,0,0.00695759,"le. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch"
N15-1023,C14-1054,0,0.0140907,"ces for which there is very little consensus (Table 7). We discuss this variation further in Section 5. Genre characterization Now we explore if the dimensions we learned at the sub-sentential level can be used to capture stylistic variation at the sentence and genre level. Sentence-level human judgements We gather human ratings of formality and complexity for 900 sentences from the MASC corpus (Ide et al., 2010): 20 sentences from each of 18 genres.2 Recently data from this corpus has been used to study genre difference in terms of pronoun, named entity, punctuation and part of speech usage (Passonneau et al., 2014). We use the data to test a specific hypothesis that automatically induced scores for lexical style are predictive of perceptions of sentence- and genre-level style. We average 7 independent human scores to get sentence-level style scores. To get genre-level style scores, we use the the average of the 20 sentencelevel scores for the sentences belonging to that genre. In human perception, the formality and complexity dimensions are highly correlated (Spearman ρ = 0.7). However, we see many interesting examples of sentences which break this trend (Table 6). Overall, inter-annotator correlations"
N15-1023,P13-1162,0,0.0210492,"ncounters words for which there exist a range of paraphrases. Her lexical choice in these cases signals the style independent of the topic. Table 8 shows how well our two scoring methods correlate with the human judgements of sentences’ styles. The “all words” method performs very well, correlating with humans nearly as well as humans correlate with each other. Interestingly, when using paraphrases only we maintain significant correlations. This ability to differentiate stylistic variation without relying on cues from topic words could be especially important for tasks such as bias detection (Recasens et al., 2013) and readability (Callan, 2004; Kanungo and Orr, 2009). Inter-anno. All words PP only Formality Sent. Genre 0.47 – 0.44 0.77 0.18 0.63 Complexity Sent. Genre 0.48 – 0.43 0.80 0.23 0.45 6 Table 8: Spearman ρ of automatic rankings with human rankings. Genres are the concatenation of sentences from that genre. In “all words,” a text’s score is the average log-ratio style score of its words. In “PP only,” a text’s score is the proportion of times a formal term was chosen when more casual paraphrases existed, effectively capturing style independent of topic. 5 tasks, capturing style information in"
N15-1023,E14-1068,0,0.0109244,"ns on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing f"
N15-1023,N10-1119,0,0.0285426,"s grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/h"
N15-1023,H05-1044,0,0.00270404,"and paraphrasing. 1 Introduction True language understanding requires comprehending not just what is said, but how it is said, yet only recently have computational approaches been applied to the subtleties of tone and style. As the expectations on language technologies grow to include tailored search, context-aware inference, and analysis of author belief, an understanding of style becomes crucial. Lexical features have proven indispensable for the good performance of most applications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirs"
N15-1023,C12-1177,0,0.0250819,"ications dealing with language. Particularly, more generalized characterizations of the lexicon (Brown et al., 1992; Wilson et al., 2005; Feng et al., 2013; Ji and Lin, 2009; Resnik, 1995) have become key in overcoming issues with lexical sparseness and in providing practical semantic information for natural language processing systems (Miller et al., 2004; Rutherford and Xue, 2014; Velikovich et al., 2010; Dodge et al., 2012). Most work on stylistic variation, however, has focused on larger units of text (Louis and Nenkova, 2013; Danescu-Niculescu-Mizil et al., 2012; Greene and Resnik, 2009; Xu et al., 2012) and studies of style at the lexical level have been scant. The few recent efforts (Brooke et al., 2010; Brooke and Hirst, 2013b; Formal/Casual jesus/my gosh 18 years/eighteen respiratory/breathing yes/yeah decade/ten years 1970s/the seventies foremost/first of all megan/you there somewhere/some place this film/that movie full/a whole bunch otherwise/another thing father/my dad recreation/hobby Complex/Simple great/a lot cinema/a movie a large/a big music/the band much/many things exposure/the show relative/his family matters/the things april/apr journal/diary the world/everybody burial/funera"
N15-1023,J02-2001,0,\N,Missing
N15-1166,E14-4040,0,0.0150756,"ators of importance. We also constrain our analysis to a single domain, which allows us to examine the semantic aspects of the verbs that may contribute to their perceived importance. We leverage a dataset of human-written summaries of news articles to objectively ground the definition of word importance. Summaries are intended to convey important information while omitting the less important pieces, so words that are important in a newsworthy sense will occur more frequently in summaries. The same data and intuition was used recently to develop a large corpus for determining entity salience (Dunietz and Gillick, 2014). We derive a list of over one thousand verbs that have statistically significant bias to appear in the summaries (important verbs) and verbs with higher rate of occurrence in the original articles (unimportant). This resource of verbs and their domain-level importance may be fruitfully exploited in models of summarization that do not use pre-defined templates but are richer than approaches that rely solely on analysis of the article text. We furthermore seek to characterize the properties of words that are biased to occur more often 1440 Human Language Technologies: The 2015 Annual Conference"
N15-1166,E14-1075,1,0.835272,"analyzing the anchor text of links to a webpage, or comments on a blog post or citations to a scientific article (Nenkova and McKeown, 2012). Here we explore the feasibility of data-driven identification of important information in the world news domain. We specifically focus on the analysis of verbs, which is the first step of identifying event types of special interest. The goal is to collect evidence of verb importance globally, without regard to a particular input or its context. Such ideas have been explored in the past as subcomponents of extractive summarizers (Schiffman et al., 2002; Hong and Nenkova, 2014) or as features derived from small datasets for sentence compression (Woodsend and Lapata, 2012). In contrast, in our work we rely on large corpora and exclusively focus on the task of acquiring input independent indicators of importance. We also constrain our analysis to a single domain, which allows us to examine the semantic aspects of the verbs that may contribute to their perceived importance. We leverage a dataset of human-written summaries of news articles to objectively ground the definition of word importance. Summaries are intended to convey important information while omitting the l"
N15-1166,J98-3005,0,0.231857,". We develop a novel characterization of the association between verbs and personal story narratives, which is descriptive of verbs avoided in summaries for this domain. 1 Introduction Summarization, either by people or machine, calls for the ability to identify important content. Computational approaches to identifying important content fall into the two extremes of a possible spectrum. On one end, the types of important information for a given domain and topic are predefined as information extraction templates defined by experts, as in the earliest approaches to multidocument summarization (Radev and McKeown, 1998) and the recently introduced guided summarization (Owczarzak and Dang, 2011). On the other extreme, traditional systems work only with indicators of importance coming solely from the input to be summarized, or possibly also from the context of the input, i.e . analyzing the anchor text of links to a webpage, or comments on a blog post or citations to a scientific article (Nenkova and McKeown, 2012). Here we explore the feasibility of data-driven identification of important information in the world news domain. We specifically focus on the analysis of verbs, which is the first step of identifyi"
N15-1166,D12-1022,0,0.0590113,"a scientific article (Nenkova and McKeown, 2012). Here we explore the feasibility of data-driven identification of important information in the world news domain. We specifically focus on the analysis of verbs, which is the first step of identifying event types of special interest. The goal is to collect evidence of verb importance globally, without regard to a particular input or its context. Such ideas have been explored in the past as subcomponents of extractive summarizers (Schiffman et al., 2002; Hong and Nenkova, 2014) or as features derived from small datasets for sentence compression (Woodsend and Lapata, 2012). In contrast, in our work we rely on large corpora and exclusively focus on the task of acquiring input independent indicators of importance. We also constrain our analysis to a single domain, which allows us to examine the semantic aspects of the verbs that may contribute to their perceived importance. We leverage a dataset of human-written summaries of news articles to objectively ground the definition of word importance. Summaries are intended to convey important information while omitting the less important pieces, so words that are important in a newsworthy sense will occur more frequent"
N16-1141,P10-1018,0,0.0302197,"Missing"
N16-1141,J95-2003,0,0.487739,"ntences (Louis et al., 2010) and that being a first sentence in an INSTANTIATION relation is the most powerful indicator for content selection related to discourse relation sense. The sentences between which the relation holds also contain more sentiment expressions than other sentences (Trnavac and Taboada, 2013), making it a special target for sentiment analysis applications. Moreover, INSTANTIATION relations appear to play a special role in local coherence (Louis and Nenkova, 2010), as the flow between IN STANTIATION sentences is not explained by the major coherence theories (Kehler, 2004; Grosz et al., 1995). Many of the sentences in INSTANTIATION relation contain entity instantiations (complex examples of set-instance anaphora), such as “several EU countries”—“the UK”, “footballers”—“Wayne Rooney” and “most cosmetic purchase”—“lipstick” (McKinlay and Markert, 2011), raising further questions about the relationship between INSTANTIA TIONS and key discourse phenomena. Detecting an INSTANTIATION, however, is hard. In the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), I NSTANTIATION is one of the few relations that are more often implicit, i.e., expressed without a discourse marker such as “f"
N16-1141,C00-1044,0,0.419769,"Missing"
N16-1141,Q15-1024,0,0.0511403,"Missing"
N16-1141,W14-4320,1,0.851102,"Missing"
N16-1141,W14-4327,1,0.878277,"Missing"
N16-1141,D09-1036,0,0.0492481,"Missing"
N16-1141,N10-1043,1,0.798815,"ation revealed that the first sentences from INSTANTIATION pairs are included in human summaries significantly more often than other sentences (Louis et al., 2010) and that being a first sentence in an INSTANTIATION relation is the most powerful indicator for content selection related to discourse relation sense. The sentences between which the relation holds also contain more sentiment expressions than other sentences (Trnavac and Taboada, 2013), making it a special target for sentiment analysis applications. Moreover, INSTANTIATION relations appear to play a special role in local coherence (Louis and Nenkova, 2010), as the flow between IN STANTIATION sentences is not explained by the major coherence theories (Kehler, 2004; Grosz et al., 1995). Many of the sentences in INSTANTIATION relation contain entity instantiations (complex examples of set-instance anaphora), such as “several EU countries”—“the UK”, “footballers”—“Wayne Rooney” and “most cosmetic purchase”—“lipstick” (McKinlay and Markert, 2011), raising further questions about the relationship between INSTANTIA TIONS and key discourse phenomena. Detecting an INSTANTIATION, however, is hard. In the Penn Discourse Treebank (PDTB) (Prasad et al., 200"
N16-1141,W10-4327,1,0.87887,"Missing"
N16-1141,R11-1037,0,0.0186645,"expressions than other sentences (Trnavac and Taboada, 2013), making it a special target for sentiment analysis applications. Moreover, INSTANTIATION relations appear to play a special role in local coherence (Louis and Nenkova, 2010), as the flow between IN STANTIATION sentences is not explained by the major coherence theories (Kehler, 2004; Grosz et al., 1995). Many of the sentences in INSTANTIATION relation contain entity instantiations (complex examples of set-instance anaphora), such as “several EU countries”—“the UK”, “footballers”—“Wayne Rooney” and “most cosmetic purchase”—“lipstick” (McKinlay and Markert, 2011), raising further questions about the relationship between INSTANTIA TIONS and key discourse phenomena. Detecting an INSTANTIATION, however, is hard. In the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), I NSTANTIATION is one of the few relations that are more often implicit, i.e., expressed without a discourse marker such as “for exam1181 Proceedings of NAACL-HLT 2016, pages 1181–1186, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ple”. Identifying implicit discourse relation is an acknowledged difficult task (Braud and Denis, 2015; Ji and Ei"
N16-1141,W12-1614,0,0.0442688,"Missing"
N16-1141,D14-1162,0,0.0775346,"Missing"
N16-1141,D08-1020,1,0.575688,"tions (Pennington et al., 2014). Table 2 shows that s1 of INSTANTIATIONs contain significantly fewer out-of-vocabulary words compared to either s2 and non-INSTANTIATIONs. We also compare the difference in unigram probability2 of content word pairs across sentence pairs, i.e., (wi , wj ), wi ∈ s1 , wj ∈ s2 . Compared to non-INSTANTIATION, words across INSTANTIATION arguments show significantly larger average unigram log probability difference (1.24 vs. 1.22). These numbers show that the first sentences of INSTANTIATION do not involve many unfamiliar words — an indication of higher readability (Pitler and Nenkova, 2008). Gradable adjectives. The use of gradable adjectives (Frazier et al., 2008; de Marneffe et al., 2010)—popular, high, likely— may require further explanation to justify the appropriateness of their use. Here we compute the average percentage of gradable adjectives in a sentence. The list of adjectives is from Hatzivassiloglou and Wiebe (2000) and the respective percentages are shown in Table 3. Compared to other sentences, s1 of INSTANTIATION involves significantly more gradable adjectives. 2 We use a unigram language model on year 2006 of the New York Times Annotated Corpus (Sandhaus, 2008)."
N16-1141,P09-1077,1,0.782287,"Missing"
N16-1141,prasad-etal-2008-penn,0,0.0446494,"and Nenkova, 2010), as the flow between IN STANTIATION sentences is not explained by the major coherence theories (Kehler, 2004; Grosz et al., 1995). Many of the sentences in INSTANTIATION relation contain entity instantiations (complex examples of set-instance anaphora), such as “several EU countries”—“the UK”, “footballers”—“Wayne Rooney” and “most cosmetic purchase”—“lipstick” (McKinlay and Markert, 2011), raising further questions about the relationship between INSTANTIA TIONS and key discourse phenomena. Detecting an INSTANTIATION, however, is hard. In the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), I NSTANTIATION is one of the few relations that are more often implicit, i.e., expressed without a discourse marker such as “for exam1181 Proceedings of NAACL-HLT 2016, pages 1181–1186, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ple”. Identifying implicit discourse relation is an acknowledged difficult task (Braud and Denis, 2015; Ji and Eisenstein, 2015; Rutherford and Xue, 2014; Biran and McKeown, 2013; Park and Cardie, 2012; Lin et al., 2009; Pitler et al., 2009), but the challenge is exacerbated due to the lack of explicit IN STANTIATION s:"
N16-1141,E14-1068,0,0.0258267,"Missing"
N16-1141,N15-1081,0,0.0241629,"Missing"
N16-1141,P10-1040,0,0.0416154,"Missing"
N16-1141,P13-2013,0,\N,Missing
N16-1141,D15-1262,0,\N,Missing
N16-1141,W15-4612,0,\N,Missing
N18-2060,P14-5010,0,0.00454234,"Missing"
N18-2060,N10-1124,0,0.0218335,"Unfortunately, these aspects are not usually described in a structured way. Abstracts with explicit category headings (Nakayama et al., 2005) partially address this, but these are not standardized nor uniform. Automated solutions are thus emerging to better support medical search, including methods for: identifying sentences containing key pieces of clinical information (Wallace et al., 2016); summarization (Sarker et al., 2016); identifying contradictory claims in medical articles (Alamri and Stevenson, 2016); and information retrieval system prototypes that harness this type of information (Boudin et al., 2010a,b). (I) In Group I, the children were treated with prednisone ... (O) .. reported that Group 2 children underwent fewer isolated bone marrow relapses .. We explore three strategies for exploiting extracted patterns in a state-of-the-art LSTM-CRF sequence tagging model (Lample et al., 2016; Ma and Hovy, 2016): as additional features at the CRF layer; as one-hot indicators concatenated to distributed representations of words; and as individual units embedded in a semantic space shared with words. The second representation improves recall for two extraction tasks, and the third improves precisi"
N18-2060,J07-1005,0,0.138493,"Missing"
N18-2060,N16-1030,0,0.620846,"methods for: identifying sentences containing key pieces of clinical information (Wallace et al., 2016); summarization (Sarker et al., 2016); identifying contradictory claims in medical articles (Alamri and Stevenson, 2016); and information retrieval system prototypes that harness this type of information (Boudin et al., 2010a,b). (I) In Group I, the children were treated with prednisone ... (O) .. reported that Group 2 children underwent fewer isolated bone marrow relapses .. We explore three strategies for exploiting extracted patterns in a state-of-the-art LSTM-CRF sequence tagging model (Lample et al., 2016; Ma and Hovy, 2016): as additional features at the CRF layer; as one-hot indicators concatenated to distributed representations of words; and as individual units embedded in a semantic space shared with words. The second representation improves recall for two extraction tasks, and the third improves precision for all three tasks. We analyze the induced semantic space to show that patterns capture contextual information that is otherwise lost. ∗ * now at Google Inc. 371 Proceedings of NAACL-HLT 2018, pages 371–377 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational L"
N18-2060,P16-1101,0,0.0201047,"Missing"
N19-1150,S16-2018,0,0.0971038,"rovements in model performance. 1 Introduction Assembling training corpora of annotated natural language examples in specialized domains such as biomedicine poses considerable challenges. Experts with the requisite domain knowledge to perform high-quality annotation tend to be expensive, while lay annotators may not have the necessary knowledge to provide high-quality annotations. A practical approach for collecting a sufficiently large corpus would be to use crowdsourcing platforms like Amazon Mechanical Turk (MTurk). However, crowd workers in general are likely to provide noisy annotations (Abad and Moschitti, 2016; Plank et al., 2014; Alonso et al., 2015), an issue exacerbated by the technical nature of specialized content. Some of this noise may reflect worker quality and can be modeled (Abad and Moschitti, 2016; Plank et al., 2014; Cohn and Specia, 2013; Nguyen et al., 2017), but for some instances lay people may simply lack the domain knowledge to provide useful annotation. In this paper we report experiments on the EBM-NLP corpus comprising crowdsourced annotations of medical literature (Nye et al., 2018). We operationalize the concept of annotation difficulty and show how it can be exploited durin"
N19-1150,W15-2711,0,0.0378808,"Missing"
N19-1150,D18-2029,1,0.909448,"nnotations is better than using lay data alone. Does it matter what data is annotated by experts? We demonstrate that a system trained on combined data achieves better predictive performance when experts annotate difficult examples rather than instances selected at i.i.d. random. Our contributions in this work are summarized as follows. We define a task difficulty prediction task and show how this is related to, but distinct from, inter-worker agreement. We introduce a new model for difficulty prediction combining learned representations induced via a pre-trained ‘universal’ sentence encoder (Cer et al., 2018), and a sentence encoder learned from scratch for this task. We show that predicting annotation difficulty can be used to improve the task routing and model performance for a biomedical information extraction task. Our results open up a new direction for ensuring corpus quality. We believe that item difficulty prediction will likely be useful in other, nonspecialized tasks as well, and that the most effective data collection in specialized domains requires research addressing the fundamental questions we examine here. 2 Related Work Crowdsourcing annotation is now a well-studied problem (Snow"
N19-1150,P13-1004,0,0.140217,"uality annotation tend to be expensive, while lay annotators may not have the necessary knowledge to provide high-quality annotations. A practical approach for collecting a sufficiently large corpus would be to use crowdsourcing platforms like Amazon Mechanical Turk (MTurk). However, crowd workers in general are likely to provide noisy annotations (Abad and Moschitti, 2016; Plank et al., 2014; Alonso et al., 2015), an issue exacerbated by the technical nature of specialized content. Some of this noise may reflect worker quality and can be modeled (Abad and Moschitti, 2016; Plank et al., 2014; Cohn and Specia, 2013; Nguyen et al., 2017), but for some instances lay people may simply lack the domain knowledge to provide useful annotation. In this paper we report experiments on the EBM-NLP corpus comprising crowdsourced annotations of medical literature (Nye et al., 2018). We operationalize the concept of annotation difficulty and show how it can be exploited during training to improve information extraction models. We then obtain expert annotations for the abstracts predicted to be most difficult, as well as for a similar number of randomly selected abstracts. The annotation of highly specialized data and"
N19-1150,D14-1181,0,0.00470094,"pare the correlation between inter-annotator agreement and difficulty scores in the training data. Given that the majority of sentences do not contain a PICO span, we only include in these calculations those that contain a reference label. Pearson’s r are 0.34, 0.30 and 0.31 for P, I and O, respectively, confirming that inter-worker agreement and our proposed difficulty score are quite distinct. 6 Predicting Annotation Difficulty We treat difficulty prediction as a regression problem, and propose and evaluate neural model variants for the task. We first train RNN (Chung et al., 2014) and CNN (Kim, 2014) models. We also use the universal sentence encoder (USE) (Cer et al., 2018) to induce sentence representations, and train a model using these as features. Following (Cer et al., 2018), we then experiment with an ensemble model that combines the ‘universal’ and task-specific representations to predict annotation difficulty. We expect these universal embeddings to capture general, high-level semantics, and the task specific representations to capture more granular information. Figure 2 de1474 NGRAM+SVR RNN CNN USE USE+RNN P 0.455 0.521 0.470 0.492 0.550 I 0.311 0.555 0.522 0.518 0.604 O 0.541 0"
N19-1150,P17-1028,1,0.90712,"to be expensive, while lay annotators may not have the necessary knowledge to provide high-quality annotations. A practical approach for collecting a sufficiently large corpus would be to use crowdsourcing platforms like Amazon Mechanical Turk (MTurk). However, crowd workers in general are likely to provide noisy annotations (Abad and Moschitti, 2016; Plank et al., 2014; Alonso et al., 2015), an issue exacerbated by the technical nature of specialized content. Some of this noise may reflect worker quality and can be modeled (Abad and Moschitti, 2016; Plank et al., 2014; Cohn and Specia, 2013; Nguyen et al., 2017), but for some instances lay people may simply lack the domain knowledge to provide useful annotation. In this paper we report experiments on the EBM-NLP corpus comprising crowdsourced annotations of medical literature (Nye et al., 2018). We operationalize the concept of annotation difficulty and show how it can be exploited during training to improve information extraction models. We then obtain expert annotations for the abstracts predicted to be most difficult, as well as for a similar number of randomly selected abstracts. The annotation of highly specialized data and the use of lay and ex"
N19-1150,P18-1019,1,0.84498,"urk (MTurk). However, crowd workers in general are likely to provide noisy annotations (Abad and Moschitti, 2016; Plank et al., 2014; Alonso et al., 2015), an issue exacerbated by the technical nature of specialized content. Some of this noise may reflect worker quality and can be modeled (Abad and Moschitti, 2016; Plank et al., 2014; Cohn and Specia, 2013; Nguyen et al., 2017), but for some instances lay people may simply lack the domain knowledge to provide useful annotation. In this paper we report experiments on the EBM-NLP corpus comprising crowdsourced annotations of medical literature (Nye et al., 2018). We operationalize the concept of annotation difficulty and show how it can be exploited during training to improve information extraction models. We then obtain expert annotations for the abstracts predicted to be most difficult, as well as for a similar number of randomly selected abstracts. The annotation of highly specialized data and the use of lay and expert annotators allow us to examine the following key questions related to lay and expert annotations in specialized domains: Can we predict item difficulty? We define a training instance as difficult if a lay annotator or an automated m"
N19-1150,N18-2060,1,0.834411,"e use Spearmans’ correlation coefficient as a scoring function. Specifically, for each sentence we create two vectors comprising counts of how many times each token was annotated by crowd and expert workers, respectively, and calculate the correlation between these. Sentences with no labels are treated as maximally easy; those with only either crowd worker or expert label(s) are assumed maximally difficult. The training set contains only crowdsourced annotations. To label the training data, we use a 10fold validation like setting. We iteratively retrain the LSTM-CRF-Pattern sequence tagger of Patel et al. (2018) on 9 folds of the training data and use that trained model to predict labels for the 10th. In this way we obtain predictions on the full training set. We then use predicted spans as proxy ‘ground truth’ annotations to calculate the diffi1473 Probability Density of Difficulty Scores 4 Workers crowd workers domain experts Participants Mean: 0.500; Std: 0.296 3 2 1 P 0.52 0.74 I 0.43 0.68 O 0.41 0.57 0 4 Intervention Mean: 0.562; Std: 0.279 Table 2: Average inter-worker agreement. 3 2 1 0 4 spective label types as separate tasks. Outcome Mean: 0.576; Std: 0.279 3 2 5 1 0 0.0 0.2 0.4 Scores 0.6 0"
N19-1150,D08-1027,0,0.284085,"Missing"
N19-1150,E12-2021,0,0.10862,"Missing"
N19-1150,D14-1162,0,0.0823658,"USE USE+RNN P 0.455 0.521 0.470 0.492 0.550 I 0.311 0.555 0.522 0.518 0.604 O 0.541 0.601 0.550 0.580 0.622 Table 3: Pearson correlation coefficients of sentence difficulty predictions. Figure 2: Model architecture. picts the model architecture. Sentences are fed into both the universal sentence encoder and, separately, a task specific neural encoder, yielding two representations. We concatenate these and pass the combined vector to the regression layer. 6.1 Experimental Setup and Results We trained models for each label type separately. Word embeddings were initialized to 300d GloVe vectors (Pennington et al., 2014) trained on common crawl data;2 these are fine-tuned during training. We used the Adam optimizer (Kingma and Ba, 2014) with learning rate and decay set to 0.001 and 0.99, respectively. We used batch sizes of 16. We used the large version of the universal sentence encoder3 with a transformer (Vaswani et al., 2017). We did not update the pretrained sentence encoder parameters during training. All hyperparamaters for all models (including hidden layers, hidden sizes, and dropout) were tuned using Vizier (Golovin et al., 2017) via 10-fold cross validation on the training set maximizing for F1.4 As"
N19-1150,E14-1078,0,0.0766517,"Missing"
N19-3003,W17-1606,0,0.0317991,"erland et al., 2018) to child entertainment and education (Druga et al., 2017). As these conversational agents become commonplace, people are likely to express emotion during their interactions, either because of their perception of the agent or because of the emotioneliciting situations in which the agent is deployed. In this paper, we set out to study the extent to which emotional content in speech impacts speech recognition performance of commercial systems. Similar studies have been conducted in the past to study how recognition varies with gender and dialect (Adda-Decker and Lamel, 2005; Tatman, 2017), lexical content (Goldwater et al., 2010), topical domain (Traum et al., 16 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 16–21 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics 3 reasonable approximation and it can be used for broader studies on factors influencing automatic speech recognition. Here, we apply the method to analyze data from a spontaneous emotional speech corpus. 2 Datasets and APIs We use three acted Emotion datasets: CREMAD (Cao et a"
N19-3003,W15-4629,0,0.0733571,"Missing"
P07-2049,P06-2020,0,0.200415,"Missing"
P07-2049,C00-1072,0,0.710836,"st, when the summary is produced in response to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging. In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario. First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006). While both schemes have proved to be suitable for sum193 marization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ. Second, there are many little-understood aspects of the differences between generic and query-focused summarization. For example, we’d like to know if a particular word weighting scheme is more suitable for focused summarization than others. More significantly, previous studies show that generic and focused systems perform very similarly to each other in"
P07-2049,W04-1013,0,0.0342145,"nouns, verbs, adjectives and adverbs are considered and a short list of light verbs are excluded: “has, was, have, are, will, were, do, been, say, said, says”. For FOCUSED summarization, we modify this algorithm merely by running the sentence selection algorithm on only those sentences in the input that are relevent to the user query. In some previous DUC evaluations, relevant sentences are explicitly marked by annotators and given to systems. In our version here, a sentence in the input is considered relevant if it contains at least one word from the user query. For evaluation we use ROUGE (Lin, 2004) SU4 recall metric1 , which was among the official automatic evaluation metrics for DUC. 4 Results The results are shown in Table 1. The focused summarizer using LLR(CQ) is the best, and it significantly outperforms the focused summarizer based on frequency. Also, LLR (using log-likelihood ratio to assign weights to all words) perfroms significantly worse than LLR(C). We can observe some trends even from the results for which there is no significance. Both LLR and LLR(C) are sensitive to the introduction of topic relevance, producing somewhat better summaries in the FOCUSED scenario 1 -n 2 -x"
P08-1094,P99-1071,0,0.0853771,"Let X be a discrete random variable taking values from the finite set V = {w1 , ..., wn } where V is the vocabulary of the input set and wi are the words that appear in the input. The probability distribution p(w) = P r(X = w) can be easily calculated using frequency counts from the input. The entropy of the input set is equal to the entropy of X: H(X) = − i=n X p(wi ) log2 p(wi ) (1) i=1 Average, minimum and maximum cosine overlap between the news articles in the input. Repetition in the input is often exploited as an indicator of importance by different summarization approaches (Luhn, 1958; Barzilay et al., 1999; Radev et al., 2004; Nenkova et al., 2006). The more similar the different documents in the input are to each other, the more likely there is repetition across documents at various granularities. Cosine similarity between the document vector representations is probably the easiest and most commonly used among the various similarity measures. We use tf*idf weights in the vector representations, with term frequency (tf) normalized by the total number of words in the document in order to remove bias resulting from high frequencies by virtue of higher document length alone. 829 The cosine similar"
P08-1094,P06-2020,0,0.186573,"Missing"
P08-1094,P07-2049,1,0.778613,"rge background collection. Then the relative entropy between the input and the collection is given by KL divergence = X w∈I pinp (w) log 2 pinp (w) pcoll (w) (2) Low KL divergence from a random background collection may be characteristic of highly noncohesive inputs consisting of unrelated documents. Number of topic signature terms for the input set. The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy, 2000) in the context of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007). Lin and Hovy’s idea was to automatically identify words that are descriptive for a cluster of documents on the same topic, such as the input to a multidocument summarizer. We will call this cluster T . Since the goal is to find descriptive terms for the cluster, a comparison collection of documents not on the topic is also necessary (we will call this background collection N T ). Given T and N T , the likelihood ratio statistic (Dunning, 1994) is used to identify the topic signature terms. The probabilistic model of the data allows for statistical inference in order to decide which terms t a"
P08-1094,W05-0902,0,0.0273248,"th the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). 826 Type Human Automatic Baseline 50 1.00 0.50 0.41 100 1.17 0.55 0.46 200 1.38 0.70 0.52 400 1.29 0.76 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention has been paid to this fact in system development and no specific user studies are available to show what summary length might be most suitable for specific applications. In later editions of the DUC conference, only summaries of 100 words were produced, focusing development efforts on one of the more demanding length restrictions. The interaction between summary length and summarizer is small but significant (Table 1), with certain summarization strategies more successful at particular summary lengths than at others. Improved performance as measured by increase"
P08-1094,C00-1072,0,0.408407,"py, between the input (I) and collection language models. Let pinp (w) be the probability of the word w in the input and pcoll (w) be the probability of the word occurring in the large background collection. Then the relative entropy between the input and the collection is given by KL divergence = X w∈I pinp (w) log 2 pinp (w) pcoll (w) (2) Low KL divergence from a random background collection may be characteristic of highly noncohesive inputs consisting of unrelated documents. Number of topic signature terms for the input set. The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy, 2000) in the context of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007). Lin and Hovy’s idea was to automatically identify words that are descriptive for a cluster of documents on the same topic, such as the input to a multidocument summarizer. We will call this cluster T . Since the goal is to find descriptive terms for the cluster, a comparison collection of documents not on the topic is also necessary (we will call this background collection N T ). Given T and N T , the likelihood r"
P08-1094,N03-1020,0,0.28589,"cores than summarizer identity does, as indicated by the larger values of the F statistic. Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row). The general trend we observe is that on average systems are better at producing summaries when more space is available. The dif1 The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). 826 Type Human Automatic Baseline 50 1.00 0.50 0.41 100 1.17 0.55 0.46 200 1.38 0.70 0.52 400 1.29 0.76 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attenti"
P08-1094,W03-0510,0,0.0215943,"cores than summarizer identity does, as indicated by the larger values of the F statistic. Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row). The general trend we observe is that on average systems are better at producing summaries when more space is available. The dif1 The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). 826 Type Human Automatic Baseline 50 1.00 0.50 0.41 100 1.17 0.55 0.46 200 1.38 0.70 0.52 400 1.29 0.76 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attenti"
P08-1094,W04-1013,0,0.0199578,"identity does, as indicated by the larger values of the F statistic. Length The average automatic summarizer coverage scores increase steadily as length requirements are relaxed, going up from 0.50 for 50-word summaries to 0.76 for 400-word summaries as shown in Table 2 (second row). The general trend we observe is that on average systems are better at producing summaries when more space is available. The dif1 The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). 826 Type Human Automatic Baseline 50 1.00 0.50 0.41 100 1.17 0.55 0.46 200 1.38 0.70 0.52 400 1.29 0.76 0.57 Table 2: Average human, system and baseline coverage scores for different summary lengths of N words. N = 50, 100, 200, and 400. ferences are statistically significant2 only between 50-word and 200- and 400-word summaries and between 100-word and 400-word summaries. The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention has been p"
P08-2043,N06-2031,0,\N,Missing
P08-2043,P07-1102,0,\N,Missing
P09-1077,N07-1054,0,0.731783,"n to be skewed, with expansions occurring most frequently. Causal and comparison relations, which are most useful for applications, are less frequent. Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the absence of explicit markers, the most easily accessible features are the Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. 684 In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans. Verb pairs turned out to be one of the best features, but no useful information was obtained using nouns and adjectives. Blair-Goldensohn et al. (2007) proposed several refinements of the word pair model."
P09-1077,C08-2022,1,0.876339,". In addition, using equal numbers of examples of each type can be misleading because the distribution of relations is known to be skewed, with expansions occurring most frequently. Causal and comparison relations, which are most useful for applications, are less frequent. Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the absence of explicit markers, the most easily accessible features are the Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. 684 In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans. Verb pairs turned out to be one of the best features, but no useful info"
P09-1077,W01-1605,0,0.139711,"Missing"
P09-1077,N03-1030,0,0.0982978,"marked by explicit discourse connectives (also called cue words) such as “because” or “but”. It is not uncommon, though, for a discourse relation to hold between two text spans without an explicit discourse connective, as the example below demonstrates: 2 (1) The 101-year-old magazine has never had to woo advertisers with quite so much fervor before. [because] It largely rested on its hard-to-fault demographics. Related Work Experiments on implicit and explicit relations Previous work has dealt with the prediction of discourse relation sense, but often for explicits and at the sentence level. Soricut and Marcu (2003) address the task of In this paper we address the problem of automatic sense prediction for discourse relations 683 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683–691, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 3 parsing discourse structures within the same sentence. They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). Many of the useful features, syntax in particular, exploit the fact that both arguments of the co"
P09-1077,W03-1210,0,0.262437,"Missing"
P09-1077,W06-1317,0,0.209991,"d to proceed]” often contain rationales afterwards (signifying Contingency relations), while short verb phrases like “They proceed” might occur more often in Expansion or Temporal relations. Our final verb features were the part of speech tags (gold-standard from the Penn Treebank) of the main verb. One would expect that Expansion would link sentences with the same tense, whereas Contingency and Temporal relations would contain verbs with different tenses. First-Last, First3: The first and last words of a relation’s arguments have been found to be particularly useful for predicting its sense (Wellner et al., 2006). Wellner et al. (2006) suggest that these words are such predictive features because they are often explicit discourse connectives. In our experiments on implicits, the first and last words are not connectives. However, some implicits have been found to be related by connective-like expressions which often appear in the beginning of the second argument. In the PDTB, these are annotated as alternatively lexicalized relations (AltLexes). To capture such effects, we included the first and last words of Arg1 as features, the first word falls into according to the General Inquirer lexicon (Stone e"
P09-1077,H05-1044,0,0.0221037,"two text spans of a relation are taken from the getting anything but basic food supplies to peogold-standard annotations in the PDTB. ple remains difficult.] is created as an example of Polarity Tags: We define features that represent the Cause relation. Because of examples like this, the sentiment of the words in the two spans. Each “but-but” is a very useful word pair feature indiword’s polarity was assigned according to its encating Cause, as the but would have been removed try in the Multi-perspective Question Answering for the artifical Contrast examples. In fact, the top Opinion Corpus (Wilson et al., 2005). In this re17 features for classifying Contrast versus Other source, each sentiment word is annotated as posiall contain the word “but”, and are indications that tive, negative, both, or neutral. We use the number the relation is Other. of negated and non-negated positive, negative, and neutral sentiment words in the two text spans as These findings indicate an unexpected anomafeatures. If a writer refers to something as “nice” lous effect in the use of synthetic data. Since rein Arg1, that counts towards the positive sentiment lations are created by removing connectives, if an count (Arg1Pos"
P09-1077,J05-2005,0,0.0770325,"Missing"
P09-1077,N04-1020,0,0.0245116,"uccess, while the Marcu and Echihabi (2001) experiments report only accuracy. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the absence of explicit markers, the most easily accessible features are the Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. 684 In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans. Verb pairs turned out to be one of the best features, but no useful information was obtained using nouns and adjectives. Blair-Goldensohn et al. (2007) proposed several refinements of the word pair model. They show that (i) stemming, (ii) using a small fixed vocabulary size consisting of only the most frequent stems (which would tend to be dominated by function words) and (iii) a cutoff on the minimum frequency of a feature, all resu"
P09-1077,P02-1047,0,\N,Missing
P09-2004,P07-1062,0,0.0151581,"n the two clauses is compared or contrasted), Contingency (one clause expresses the cause of the other), and Temporal (information in two clauses are related because of their timing). These top-level discourse relation senses are general enough to be annotated with high inter-annotator agreement and are common to most theories of discourse. 2.2 Syntactic features Syntactic features have been extensively used for tasks such as argument identification: dividing sentences into elementary discourse units among which discourse relations hold (Soricut and Marcu, 2003; Wellner and Pustejovsky, 2007; Fisher and Roark, 2007; Elwell and Baldridge, (4) NASA won’t attempt a rescue; instead, it will try to predict whether any of the rubble will smash to the ground 14 Features (1) Connective Only (2) Syntax Only (3) Connective+Syntax (3)+Conn-Syn Interaction (3)+Conn-Syn+Syn-Syn Interaction and where. The syntactic category of “where” is SBAR, so the set of features above could not distinguish the single word “where” from a full embedded clause like “I went to the store”. In order to address this deficiency, we include two additional features about the contents of the right sibling, Right Sibling Contains a VP and Ri"
P09-2004,P07-1101,0,0.00702299,"discourse connectives have these desirable properties. Some linguistic expressions are ambiguous between DISCOURSE AND NON - DISCOURSE US AGE . Consider for example the following sentences containing and and once. 1 The discourse vs. non-discourse usage ambiguity is even more problematic in spoken dialogues because there the number of potential discourse markers is greater than that in written text, including common words such as now, well and okay. Prosodic and acoustic features are the most powerful indicators of discourse vs. non-discourse usage in that genre (Hirschberg and Litman, 1993; Gravano et al., 2007) ∗ This work was partially supported by NSF grants IIS0803159, IIS-0705671 and IGERT 0504487. 13 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13–16, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP 2008). Syntax has not been used for discourse vs. non-discourse disambiguation, but it is clear from the examples above that discourse connectives appear in specific syntactic contexts. The syntactic features we used were extracted from the gold standard Penn Treebank (Marcus et al., 1994) parses of the PDTB articles: Self Category The highest node in the tree which domina"
P09-2004,J93-3003,0,0.230378,"and phrases that can serve as discourse connectives have these desirable properties. Some linguistic expressions are ambiguous between DISCOURSE AND NON - DISCOURSE US AGE . Consider for example the following sentences containing and and once. 1 The discourse vs. non-discourse usage ambiguity is even more problematic in spoken dialogues because there the number of potential discourse markers is greater than that in written text, including common words such as now, well and okay. Prosodic and acoustic features are the most powerful indicators of discourse vs. non-discourse usage in that genre (Hirschberg and Litman, 1993; Gravano et al., 2007) ∗ This work was partially supported by NSF grants IIS0803159, IIS-0705671 and IGERT 0504487. 13 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13–16, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP 2008). Syntax has not been used for discourse vs. non-discourse disambiguation, but it is clear from the examples above that discourse connectives appear in specific syntactic contexts. The syntactic features we used were extracted from the gold standard Penn Treebank (Marcus et al., 1994) parses of the PDTB articles: Self Category The highest node i"
P09-2004,J00-3005,0,0.0699712,"ked by an elaboration/expansion relation; in sentence (1b), the occurrence of and is non-discourse. Similarly in sentence (2a), once is a discourse connective marking the temporal relation between the clauses “The asbestos fiber, crocidolite is unusually resilient” and “it enters the lungs”. In contrast, in sentence (2b), once occurs with a non-discourse sense, meaning “formerly” and modifying “used”. The only comprehensive study of discourse vs. non-discourse usage in written text1 was done in the context of developing a complete discourse parser for unrestricted text using surface features (Marcu, 2000). Based on the findings from a corpus study, Marcu’s parser “ignored both cue phrases that had a sentential role in a majority of the instances in the corpus and those that were too ambiguous to be explored in the context of a surface-based approach”. The other ambiguity that arises during discourse processing involves DISCOURSE RELA TION SENSE . The discourse connective since for Introduction Discourse connectives are often used to explicitly mark the presence of a discourse relation between two textual units. Some connectives are largely unambiguous, such as although and additionally, which"
P09-2004,C08-2022,1,0.857272,"Missing"
P09-2004,D07-1010,0,0.26031,"her), Comparison (information in the two clauses is compared or contrasted), Contingency (one clause expresses the cause of the other), and Temporal (information in two clauses are related because of their timing). These top-level discourse relation senses are general enough to be annotated with high inter-annotator agreement and are common to most theories of discourse. 2.2 Syntactic features Syntactic features have been extensively used for tasks such as argument identification: dividing sentences into elementary discourse units among which discourse relations hold (Soricut and Marcu, 2003; Wellner and Pustejovsky, 2007; Fisher and Roark, 2007; Elwell and Baldridge, (4) NASA won’t attempt a rescue; instead, it will try to predict whether any of the rubble will smash to the ground 14 Features (1) Connective Only (2) Syntax Only (3) Connective+Syntax (3)+Conn-Syn Interaction (3)+Conn-Syn+Syn-Syn Interaction and where. The syntactic category of “where” is SBAR, so the set of features above could not distinguish the single word “where” from a full embedded clause like “I went to the store”. In order to address this deficiency, we include two additional features about the contents of the right sibling, Right Sibl"
P09-2004,J93-2004,0,\N,Missing
P09-2004,N03-1030,0,\N,Missing
P09-2004,prasad-etal-2008-penn,0,\N,Missing
P10-1056,J08-1001,0,0.704322,"n validated on data from NIST evaluations. In their pioneering work on automatic evaluation of summary coherence, Lapata and Barzilay (2005) provide a correlation analysis between human coherence assessments and (1) semantic relatedness between adjacent sentences and (2) measures that characterize how mentions of the same entity in different syntactic positions are spread across adjacent sentences. Several of their models exhibit a statistically significant agreement with human ratings and complement each other, yielding an even higher correlation when combined. Lapata and Barzilay (2005) and Barzilay and Lapata (2008) both show the effectiveness of entity-based coherence in evaluating summaries. However, fewer than five automatic summarizers were used in these studies. Further, both sets of experiments perform evaluations of mixed sets of human-produced and machine-produced summaries, so the results may be influenced by the ease of discriminating between a human and machine written summary. Therefore, we believe it is an open question how well these features predict the quality of automatically generated summaries. In this work, we focus on linguistic quality evaluation for automatic systems only. We analy"
P10-1056,W08-0309,0,0.0746868,"Missing"
P10-1056,P03-1054,0,0.00266968,". 2010 Association for Computational Linguistics in Section 4). We test the performance of different sets of features separately and in combination with each other (Section 5). Results are presented in Section 6, showing the robustness of each class and their abilities to reproduce human rankings of systems and summaries with high accuracy. All of the features we investigate can be computed automatically directly from text, but some require considerable linguistic processing. Several of our features require a syntactic parse. To extract these, all summaries were parsed by the Stanford parser (Klein and Manning, 2003). 2 Aspects of linguistic quality 3.1 Word choice: language models We focus on the five aspects of linguistic quality that were used to evaluate summaries in DUC: grammaticality, non-redundancy, referential clarity, focus, and structure/coherence.3 For each of the questions, all summaries were manually rated on a scale from 1 to 5, in which 5 is the best. The exact definitions that were provided to the human assessors are reproduced below. Psycholinguistic studies have shown that people read frequent words and phrases more quickly (Haberlandt and Graesser, 1985; Just and Carpenter, 1987), so t"
P10-1056,E09-1017,1,0.930837,"he discourse and so must be informative and properly descriptive (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. 3 Indicators of linguistic quality Multiple factors influence the linguistic quality of text in general, including: word choice, the reference form of entities, and local coherence. We extract features which serve as proxies for each of the factors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from prior work (Sections 3.6 to 3.9). First mentions to people Feature exploration on our development set found that under-specified 3 http://www-nlpir.nist.gov/projects/ duc/duc2006/quality-questions.txt 545 3.4 Local coherence: Cohesive devices references to people are much more disruptive to a summary than short references to organizations or locations. In fact, prior work in Nenkova and McKeown (2003) found that summaries that have been rewritten so that first mentions of people are informative desc"
P10-1056,E09-1018,0,0.0329764,"ural features will be better at detecting ungrammatical sentences than the local language model features. Coreference Steinberger et al. (2007) compare the coreference chains in input documents and in summaries in order to locate potential problems. We instead define a set of more general features related to coreference that are not specific to summarization and are applicable for any text. Our features check the existence of proper antecedents for pronouns in the summary without reference to the text of the input documents. We use the publicly available pronoun resolution system described in Charniak and Elsner (2009) to mark possible antecedents for pronouns in the summary. We then compute as features the number of times an antecedent for a pronoun was found in the previous sentence, in the same sentence, or neither. In addition, we modified the pronoun resolution system to also output the probability of the most likely antecedent and include the average antecedent probability for the pronouns in the text. Automatic coreference systems are trained on human-produced texts and we expect their accuracies to drop when applied to automatically generated summaries. However, the predictions and confidence scores"
P10-1056,C08-1019,0,0.0391937,"system scores on the five linguistic quality questions Content Gram Non-redun Ref Focus Gram .02 Non-redun -.40 * .38 * Ref .29 .25 -.07 Focus .28 .24 -.09 .89 * Struct .09 .54 * .27 .76 * .80 * Table 1: Spearman correlations between the manual ratings for systems averaged over the 50 inputs in 2006; * p < .05 We use the summaries from DUC 2006 for training and feature development and DUC 2007 served as the test set. Validating the results on consecutive years of evaluation is important, as results that hold for the data in one year might not carry over to the next, as happened for example in Conroy and Dang (2008)’s work. Following Barzilay and Lapata (2008), we report summary ranking accuracy as the fraction of correct pairwise rankings in the test set. We use a Ranking SVM (SV M light (Joachims, 2002)) to score summaries using our features. The Ranking SVM seeks to minimize the number of discordant pairs (pairs in which the gold standard has x1 ranked strictly higher than x2 , but the learner ranks x2 strictly higher than x1 ). The output of the ranker is always a real valued score, so a global rank order is always obtained. The default regularization parameter was used. 5.1 Combining predictions To"
P10-1056,P03-1069,0,0.0305634,"ce resolution. Instead, noun phrases are considered to refer to the same entity if their heads are identical. Entity coherence features are the only ones that have been previously applied with success for predicting summary coherence. They can therefore be considered to be the state-of-the-art approach for automatic evaluation of linguistic quality. Word coherence: Soricut and Marcu (2006) Word co-occurrence patterns across adjacent sentences provide a way of measuring local coherence that is not linguistically informed but which can be easily computed using large amounts of unannotated text (Lapata, 2003; Soricut and Marcu, 2006). Word coherence can be considered as the analog of language models at the inter-sentence level. Specifically, we used the two features introduced by Soricut and Marcu (2006). Soricut and Marcu (2006) make an analogy to machine translation: two words are likely to be translations of each other if they often appear in parallel sentences; in texts, two words are likely to signal local coherence if they often appear in adjacent sentences. The two features we computed are forward likelihood, the likelihood of observing the words in sentence si conditioned on si−1 , and ba"
P10-1056,N03-1020,0,0.308836,"Missing"
P10-1056,P08-2011,0,0.0534132,"bout a topic. These five questions get at different aspects of what makes a well-written text. We therefore predict each aspect of linguistic quality separately. 3.2 Reference form: Named entities This set of features examines whether named entities have informative descriptions in the summary. We focus on named entities because they appear often in summaries of news documents and are often not known to the reader beforehand. In addition, first mentions of entities in text introduce the entity into the discourse and so must be informative and properly descriptive (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. 3 Indicators of linguistic quality Multiple factors influence the linguistic quality of text in general, including: word choice, the reference form of entities, and local coherence. We extract features which serve as proxies for each of the factors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from pr"
P10-1056,W04-1013,0,0.0525781,"Missing"
P10-1056,N07-1055,0,0.010597,"ity to compute the overlap of words in adjacent sentences si and si+1 as a measure of continuity. vsi .vsi+1 (1) cosθ = ||vsi |vsi+1 || The dimensions of the two vectors (vsi and vsi+1 ) are the total number of word types from both sentences si and si+1 . Stop words were retained. The value of each dimension for a sentence is the number of tokens of that word type in that sentence. We compute the min, max, and average value of cosine similarity over the entire summary. 5 547 http://cohmetrix.memphis.edu/ 3.8 would differ from those in incoherent sequences. We use the Brown Coherence Toolkit7 (Elsner et al., 2007) to construct the grids. The tool does not perform full coreference resolution. Instead, noun phrases are considered to refer to the same entity if their heads are identical. Entity coherence features are the only ones that have been previously applied with success for predicting summary coherence. They can therefore be considered to be the state-of-the-art approach for automatic evaluation of linguistic quality. Word coherence: Soricut and Marcu (2006) Word co-occurrence patterns across adjacent sentences provide a way of measuring local coherence that is not linguistically informed but which"
P10-1056,N03-2024,1,0.772875,"tors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from prior work (Sections 3.6 to 3.9). First mentions to people Feature exploration on our development set found that under-specified 3 http://www-nlpir.nist.gov/projects/ duc/duc2006/quality-questions.txt 545 3.4 Local coherence: Cohesive devices references to people are much more disruptive to a summary than short references to organizations or locations. In fact, prior work in Nenkova and McKeown (2003) found that summaries that have been rewritten so that first mentions of people are informative descriptions and subsequent mentions are replaced with more concise reference forms are overwhelmingly preferred to summaries whose entity references have not been rewritten. In coherent text, constituent clauses and sentences are related and depend on each other for their interpretation. Referring expressions such as pronouns link the current utterance to those where the entities were previously mentioned. In addition, discourse connectives such as “but” or “because” relate propositions or events e"
P10-1056,P05-1045,0,0.00216479,"a well-written text. We therefore predict each aspect of linguistic quality separately. 3.2 Reference form: Named entities This set of features examines whether named entities have informative descriptions in the summary. We focus on named entities because they appear often in summaries of news documents and are often not known to the reader beforehand. In addition, first mentions of entities in text introduce the entity into the discourse and so must be informative and properly descriptive (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. 3 Indicators of linguistic quality Multiple factors influence the linguistic quality of text in general, including: word choice, the reference form of entities, and local coherence. We extract features which serve as proxies for each of the factors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from prior work (Sections 3.6 to 3.9). First mentions to people Feature ex"
P10-1056,W02-0404,0,0.0614937,"Missing"
P10-1056,J95-2003,0,0.140656,"marized and 35 summarization systems which participated in the evaluation. This included 34 automatic systems submitted by participants, and a baseline system that simply extracted the leading sentences from the most recent article. In DUC 2007, there were 45 inputs and 32 different summarization systems. Apart from the leading sentences baseline, a high performance automatic summarizer from a previous year was also used as a baseline. All these automatic systems are included in our evaluation experiments. Entity coherence: Barzilay and Lapata (2008) Linguistic theories, and Centering theory (Grosz et al., 1995) in particular, have hypothesized that the properties of the transition of attention from entities in one sentence to those in the next, play a major role in the determination of local coherence. Barzilay and Lapata (2008), inspired by Centering, proposed a method to compute the local coherence of texts on the basis of the sequences of entity mentions appearing in them. In their Entity Grid model, a text is represented by a matrix with rows corresponding to each sentence in a text, and columns to each entity mentioned anywhere in the text. The value of a cell in the grid is the entity’s gramma"
P10-1056,W09-2807,0,0.02483,"Missing"
P10-1056,P06-2103,0,0.555556,"ve (Prince, 1981; Fraurud, 1990; Elsner and Charniak, 2008). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. 3 Indicators of linguistic quality Multiple factors influence the linguistic quality of text in general, including: word choice, the reference form of entities, and local coherence. We extract features which serve as proxies for each of the factors mentioned above (Sections 3.1 to 3.5). In addition, we investigate some models of grammaticality (Chae and Nenkova, 2009) and coherence (Graesser et al., 2004; Soricut and Marcu, 2006; Barzilay and Lapata, 2008) from prior work (Sections 3.6 to 3.9). First mentions to people Feature exploration on our development set found that under-specified 3 http://www-nlpir.nist.gov/projects/ duc/duc2006/quality-questions.txt 545 3.4 Local coherence: Cohesive devices references to people are much more disruptive to a summary than short references to organizations or locations. In fact, prior work in Nenkova and McKeown (2003) found that summaries that have been rewritten so that first mentions of people are informative descriptions and subsequent mentions are replaced with more concis"
P11-5003,D10-1047,0,0.0627991,"Missing"
P11-5003,P06-2020,0,0.0457541,"term t which occurs with probability p in a text consisting of N words is given by t  Estimate the probability of t in three ways    Input + background corpus combines Input only Background only 29 Testing which hypothesis is more likely: log-likelihood ratio test λ= -2 log λ Likelihood of the data given H1 Likelihood of the data given H2 has a known statistical distribution: chi-square At a given significance level, we can decide if a word is descriptive of the input or not. This feature is used in the best performing systems for multi-document summarization of news [Lin and Hovy, 2000; Conroy et al., 2006] 30 15 Motivation & Definition Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian] Topic Models Graph Based Methods Supervised Techniques Features, Discriminative Training Sampling, Data, Co-training Global Optimization Methods Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection Speech Summarization Segmentation, ASR Acoustic Information, Disfluency Evaluation Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic 31 The background corpus takes more central stage  Learn topics from the background corpus     topic ~ themes often discusses in the backgroun"
P11-5003,W03-1203,0,0.0765119,"Missing"
P11-5003,W06-1643,0,0.0464152,"Missing"
P11-5003,W09-1802,0,0.0487993,"Missing"
P11-5003,W00-0405,0,0.230027,"Missing"
P11-5003,W06-0701,0,0.0604512,"Missing"
P11-5003,N09-1041,0,0.0828029,"Missing"
P11-5003,C00-1072,0,0.0532023,"elihood of observing term t which occurs with probability p in a text consisting of N words is given by t  Estimate the probability of t in three ways    Input + background corpus combines Input only Background only 29 Testing which hypothesis is more likely: log-likelihood ratio test λ= -2 log λ Likelihood of the data given H1 Likelihood of the data given H2 has a known statistical distribution: chi-square At a given significance level, we can decide if a word is descriptive of the input or not. This feature is used in the best performing systems for multi-document summarization of news [Lin and Hovy, 2000; Conroy et al., 2006] 30 15 Motivation & Definition Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayesian] Topic Models Graph Based Methods Supervised Techniques Features, Discriminative Training Sampling, Data, Co-training Global Optimization Methods Iterative, Greedy, Dynamic Programming ILP, Sub-Modular Selection Speech Summarization Segmentation, ASR Acoustic Information, Disfluency Evaluation Manual (Pyramid), Automatic (Rouge, F-Measure) Fully Automatic 31 The background corpus takes more central stage  Learn topics from the background corpus     topic ~ themes often discu"
P11-5003,N10-1134,0,0.0420107,"Missing"
P11-5003,P09-2066,1,0.894416,"Missing"
P11-5003,D09-1032,1,0.898421,"Missing"
P11-5003,N06-2023,1,0.869977,"Missing"
P11-5003,W05-0905,0,0.0631504,"Missing"
P11-5003,N04-1019,1,0.385841,"Missing"
P11-5003,W02-0401,0,0.10109,"Missing"
P11-5003,P08-1054,0,0.0732559,"Missing"
P11-5003,C04-1129,1,0.86581,"Missing"
P11-5003,J02-4004,0,0.106324,"Missing"
P11-5003,C08-1124,0,0.0978198,"Missing"
P11-5003,N10-1006,1,0.89478,"Missing"
P11-5003,J02-4003,0,0.227966,"bout ASR errors?  Deliver summary using original speech    Can avoid showing recognition errors in the delivered text summary But still need to correctly identify summary sentences/segments Use recognition confidence measure and multiple candidates to help better summarize 127 Address problems due to ASR errors  Re-define summarization task: select sentences that are most informative, at the same time have high recognition accuracy   Important words tend to have high recognition accuracy Use ASR confidence measure or n-gram language model scores in summarization  Unsupervised methods [Zechner, 2002; Kikuchi et al., 2003; Maskey, 2008]  Use as a feature in supervised methods 128 64 Address problems due to ASR errors  Use multiple recognition candidates    n-best lists [Liu et al., 2010] Lattices [Lin et al., 2010] Confusion network [Xie and Liu, 2010]     Use in MMR framework Summarization segment/unit contains all the word candidates (or pruned ones based on probabilities) Term weights (TF, IDF) use candidate’s posteriors Improved performance over using 1-best recognition output 129 Motivation & Definition Topic Models Frequency, TF*IDF, Topic Words Topic Models [LSA, EM, Bayes"
P11-5003,A00-2025,0,0.0976678,"Missing"
P11-5003,N06-2050,0,0.0596245,"Missing"
P11-5003,P09-1062,0,0.0430067,"Missing"
P13-2024,W12-2605,0,0.0250438,"Missing"
P13-2024,C08-1019,1,0.879898,"Missing"
P13-2024,J11-1001,1,0.888307,"Missing"
P13-2024,hovy-etal-2006-automated,0,0.0234145,"keep only the best performing systems for the analysis because we are interested in studying how well automatic evaluation metrics can correctly compare very good systems. Year 2008 2009 2010 2011 Pyr A 82 146 165 39 Pyr B 109 190 139 83 Resp A 68 106 150 5 Resp B 105 92 128 11 Table 1: Number of pairs of significantly different systems among the top 30 across the years. There is a total of 435 pairs in each year. 3 Which ROUGE is best? In this section, we study the performance of several ROUGE variants, including ROUGE-n, for n = 1, 2, 3, 4, ROUGE-L, ROUGE-W-1.2, ROUGE-SU4, and ROUGE-BE-HM (Hovy et al., 2006). ROUGE-n measures the n-gram recall of the evaluated summary compared to the available reference summaries. ROUGE-L is the ratio of the number of words in the longest common subsequence between the reference and the evaluated summary and the number of words in the reference. ROUGE-W-1.2 is a weighted version of ROUGE-L. ROUGE-SU4 is a combination of skip bigrams and unigrams, where the skip bigrams are formed for all words that appear in the text with no more than four intervening words in between. ROUGE-BE-HM computes recall of dependency syntactic relations between the summary and the refer"
P13-2024,P08-1094,1,0.896158,"in each subtask, we compare the results of two Wilcoxon signedrank tests, one using the manual evaluation scores for each system and one using the automatic evaluation scores for each system (Rankel et al., 2011).2 The accuracy then is simply the percent agreement between the results of these two tests. 2 We use the Wilcoxon test as it was demonstrated by Rankel et al. (2011) to give more statistical power than unpaired tests. As reported by Yeh (2000), other tests such as randomized testing, may also be appropriate. There is considerable variation in system performance for different inputs (Nenkova and Louis, 2008) and paired tests remove the effect of the input. 132 Metric R1 R2 R3 R4 RL R-SU4 R-W-1.2 R-BE-HM Responsiveness Acc P R 0.58 (0.61) 0.24 0.64 0.64 (0.63) 0.28 0.60 0.70 (0.63) 0.31 0.48 0.73 (0.64) 0.33 0.40 0.50 (0.59) 0.20 0.56 0.61(0.62) 0.26 0.61 0.52(0.62) 0.21 0.54 0.70 (0.63) 0.30 0.49 BA 0.57 0.59 0.60 0.60 0.54 0.58 0.55 0.59 Acc 0.62 (0.66) 0.68 (0.69) 0.73 (0.68) 0.74 (0.65) 0.54 (0.63) 0.65 (0.68) 0.57(0.64) 0.74(0.68) Pyramid P R 0.37 0.67 0.43 0.63 0.49 0.53 0.50 0.45 0.29 0.60 0.40 0.65 0.32 0.62 0.49 0.56 BA 0.61 0.64 0.66 0.65 0.55 0.63 0.57 0.66 Table 2: Accuracy, Precision,"
P13-2024,W12-2601,1,0.663611,"ratio of the number of words in the longest common subsequence between the reference and the evaluated summary and the number of words in the reference. ROUGE-W-1.2 is a weighted version of ROUGE-L. ROUGE-SU4 is a combination of skip bigrams and unigrams, where the skip bigrams are formed for all words that appear in the text with no more than four intervening words in between. ROUGE-BE-HM computes recall of dependency syntactic relations between the summary and the reference. To evaluate how well an automatic evaluation metric reproduces human judgments, we use prediction accuracy similar to Owczarzak et al. (2012). For each pair of systems in each subtask, we compare the results of two Wilcoxon signedrank tests, one using the manual evaluation scores for each system and one using the automatic evaluation scores for each system (Rankel et al., 2011).2 The accuracy then is simply the percent agreement between the results of these two tests. 2 We use the Wilcoxon test as it was demonstrated by Rankel et al. (2011) to give more statistical power than unpaired tests. As reported by Yeh (2000), other tests such as randomized testing, may also be appropriate. There is considerable variation in system performa"
P13-2024,D11-1043,1,0.898954,"Missing"
P13-2024,C00-2137,0,0.0146646,"w well an automatic evaluation metric reproduces human judgments, we use prediction accuracy similar to Owczarzak et al. (2012). For each pair of systems in each subtask, we compare the results of two Wilcoxon signedrank tests, one using the manual evaluation scores for each system and one using the automatic evaluation scores for each system (Rankel et al., 2011).2 The accuracy then is simply the percent agreement between the results of these two tests. 2 We use the Wilcoxon test as it was demonstrated by Rankel et al. (2011) to give more statistical power than unpaired tests. As reported by Yeh (2000), other tests such as randomized testing, may also be appropriate. There is considerable variation in system performance for different inputs (Nenkova and Louis, 2008) and paired tests remove the effect of the input. 132 Metric R1 R2 R3 R4 RL R-SU4 R-W-1.2 R-BE-HM Responsiveness Acc P R 0.58 (0.61) 0.24 0.64 0.64 (0.63) 0.28 0.60 0.70 (0.63) 0.31 0.48 0.73 (0.64) 0.33 0.40 0.50 (0.59) 0.20 0.56 0.61(0.62) 0.26 0.61 0.52(0.62) 0.21 0.54 0.70 (0.63) 0.30 0.49 BA 0.57 0.59 0.60 0.60 0.54 0.58 0.55 0.59 Acc 0.62 (0.66) 0.68 (0.69) 0.73 (0.68) 0.74 (0.65) 0.54 (0.63) 0.65 (0.68) 0.57(0.64) 0.74(0.6"
P13-2024,W04-1013,0,0.0660389,"t Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations of ROUGE variants and find that they considerably improve the accuracy of automatic prediction. 1 Introduction ROUGE (Lin, 2004) is a suite of automatic evaluations for summarization and was introduced a decade ago as a reasonable substitute for costly and slow human evaluation. The scores it produces are based on n-gram or syntactic overlap between an automatic summary and a set of human reference summaries. However, the field does not have a good grasp of which of the many evaluation scores is most accurate in replicating human judgements. This state of uncertainty has led to problems in comparing published work, as different researchers choose to publish different variants of scores. In this paper we reassess the st"
P13-2024,J13-2002,1,0.730686,"asures for judging the performance of ROUGE variants has direct intuitive interpretation, unlike other opaque measures such as correlation coefficients and F-measure which have formal definitions which do not readily yield to intuitive understanding. 3 This is a somewhat surprising finding which may warrant further investigation. One possible explanation is that different systems generate similar summaries. Recent work has shown that this is unlikely to be the case because the collection of summaries from several systems indicates better what content is important than the single best summary (Louis and Nenkova, 2013). The short summary length for which the summarizers are compared may also contribute to the fact that there are few significant difference. In early NIST evaluations manual evaluations could not distinguish automatic and human summaries based on summaries of length 50 and 100 words and there were more significant differences between systems for 200-word summaries than for 100-word summaries (Nenkova, 2005). 4 More generally, one could define a utility function which gives costs associated with errors and benefits to correct prediction. Balanced accuracy weighs all errors as equally bad and al"
P14-2047,W13-3306,0,0.135469,"Missing"
P14-2047,D07-1007,1,0.794195,"ces, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides more reliable estimates of translation qu"
P14-2047,W13-3303,0,0.10424,"very long sentences which at times require the use of multiple English sentences to express the same content and preserve grammaticality. Similarly discourse connectives like because, but, since and while often relate information expressed in simple sentential clauses. There are a number of possible complications in translating these connectives: they may be ambiguous between possible senses, e.g., English while is ambiguous between COMPARISON and TEMPORAL; explicit discourse connectives may be translated into implicit discourse relations or translated in morphology rather than lexical items (Meyer and Webber, 2013; Meyer and Pol´akov´a, 2013). In our work, we quantify the relationship between information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004;"
P14-2047,P07-1005,0,0.0458452,"uality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides more reliable estimates of translation quality than using tra"
P14-2047,P07-1017,0,0.0288128,"y the relationship between information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited re"
P14-2047,C96-2183,0,0.0371132,"Missing"
P14-2047,P09-2004,1,0.922921,"Missing"
P14-2047,P05-1066,0,0.0606216,"Missing"
P14-2047,prasad-etal-2008-penn,0,0.490232,"Missing"
P14-2047,N04-1035,0,0.0469702,"eyer and Webber, 2013; Meyer and Pol´akov´a, 2013). In our work, we quantify the relationship between information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English tra"
P14-2047,2006.amta-papers.25,0,0.0432856,"tems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides more reliable estimates of translation quality than using translated references, especially at the segment level. The data for the analysis is drawn from an extended set of newswire reports in the 2008/2010 NIST Metrics for Machine Translation 283 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 283–288, c Baltimor"
P14-2047,N06-2013,0,0.0340844,"tween information packaging, discourse devices, and translation quality. Introduction In this study we examine how the use of discourse devices to organize information in a sentence — and the mismatch in their usage across languages — influence machine translation (MT) quality. The goal is to identify discourse processing tasks with high potential for improving translation systems. Historically MT researchers have focused their attention on the mismatch of linear realization of syntactic arguments (Galley et al., 2004; Collins et al., 2005), lexico-morphological mismatch (Minkov et al., 2007; Habash and Sadat, 2006) and word polysemy (Carpuat and Wu, 2007; Chan et al., 2007). Discourse structure has largely been considered irrelevant to MT, mostly due to the assumption that discourse analysis is needed to inter2 Data and experiment settings We examine the quality of translations to English from Chinese and Arabic using Human-targeted Translation Edit Rates (HTER) (Snover et al., 2006), which roughly captures the minimal number of edits necessary to transform the system output into an acceptable English translation of the source sentence. By comparing MT output with post-edited references, HTER provides m"
P14-2047,P11-2111,0,0.188979,"Missing"
P14-2047,W04-1101,0,0.249046,"Missing"
P14-2047,J93-2004,0,\N,Missing
P17-1028,C02-1025,0,0.0483385,"those from domain experts (Snow et al., 2008). It 1 Soure code and biomedical abstract data: www.github.com/thanhan/seqcrowd-acl17, www.byronwallace.com/EBM_abstracts_data 299 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 299–309 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1028 model from a hidden state to an observation and a transition model from a hidden state to the next hidden state. Later work focused on discriminative models such as Maximum Entropy Models (Chieu and Ng, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). These were able to achieve strong predictive performance by exploiting arbitrary features, but they may not be the best choice for label aggregation. Also, compared to the simple HMM model, discriminative sequentially structured models require more complex optimization and are generally more difficult to extend. Here we argue for the generative HMMs for our first task of aggregating crowd labels. The generative nature of HMMs is a good fit for existing crowd modeling techniques and also enables very efficient parameter estimation."
P17-1028,D15-1261,0,0.2839,"Missing"
P17-1028,D07-1031,0,0.477994,"sequences in unannotated text given a training set of crowd annotations (Task 2). As part of this work, we propose novel models for working with noisy sequence labels from the crowd. Reported experiments both benchmark existing state-of-the-art approaches (sequential and non-sequential) and show that our proposed models achieve best-in-class performance. As noted in the Abstract, we have also shared our sourcecode and data online for use by the community. In addition to the supervised setting, previous work has studied unsupervised HMMs, e.g., for PoS induction (Goldwater and Griffiths, 2007; Johnson, 2007). These works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels. For Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) (Collobert et al."
P17-1028,N15-1089,0,0.0163622,"al., 2016). Here we modify the LSTM model proposed by Lample et al. (2016) by augmenting the network with ‘crowd worker vectors’. We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations. Crowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improving aggregate label quality (Raykar et al., 2010; Felt et al., 2015; Kajino et al., 2012; Bi et al., 2014; Liu et al., 2012; Hovy et al., 2013). Sheshadri and Lease (2013) survey and benchmark methods. Sequence labeling. Early work on learning for sequential tasks used HMMs (Bikel et al., 1997). HMMs are a class of generative probabilistic models comprising two components: an emission However, these models are almost all in the binary or multiclass classification setting; only a few have considered sequence labeling. Dredze et al. (2009) proposed a method for learning a CRF 2 Related Work 300 model from multiple labels (although the identities of the annotato"
P17-1028,D14-1181,0,0.00550015,"works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels. For Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) (Collobert et al., 2011; Kim, 2014; Zhang and Wallace, 2015) and LSTMs (Lample et al., 2016). Here we modify the LSTM model proposed by Lample et al. (2016) by augmenting the network with ‘crowd worker vectors’. We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations. Crowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improvin"
P17-1028,W10-0713,0,0.039515,"Missing"
P17-1028,P07-1094,0,0.0246539,"ask 1) and how to best predict sequences in unannotated text given a training set of crowd annotations (Task 2). As part of this work, we propose novel models for working with noisy sequence labels from the crowd. Reported experiments both benchmark existing state-of-the-art approaches (sequential and non-sequential) and show that our proposed models achieve best-in-class performance. As noted in the Abstract, we have also shared our sourcecode and data online for use by the community. In addition to the supervised setting, previous work has studied unsupervised HMMs, e.g., for PoS induction (Goldwater and Griffiths, 2007; Johnson, 2007). These works are similar to our work in trying to infer the hidden states without labeled data. Our graphical model is different in incorporating signal from the crowd labels. For Task 2 (training predictive models), we consider CRFs and LSTMs. CRFs are undirected, conditional models that can exploit arbitrary features. They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features. Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) ("
P17-1028,N16-1030,0,0.434422,"beling of unannotated text is typically preferable, as it is more efficient, scalable, and cost-effective. Given a training set of crowd labels, how can we best predict sequences in unannotated text? Should we: (i) consider Task 1 as a pre-processing step and train the model using consensus labels; or (ii) instead directly train the model on all of the individual annotations, as done by Yang et al. (2010)? We investigate both directions in this work. Our approach is to augment existing sequence labeling models such as HMMs (Rabiner and Juang, 1986) and LSTMs (Hochreiter and Schmidhuber, 1997; Lample et al., 2016) by introducing an explicit ”crowd component”. For HMMs, we model this crowd component by including additional parameters for worker label quality and crowd label variables. For the LSTM, we introduce a vector representation for each annotator. In Despite sequences being core to NLP, scant work has considered how to handle noisy sequence labels from multiple annotators for the same text. Given such annotations, we consider two complementary tasks: (1) aggregating sequential crowd labels to infer a best single set of consensus annotations; and (2) using crowd annotations as training data for a"
P17-1028,N13-1132,0,0.76565,"by augmenting the network with ‘crowd worker vectors’. We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations. Crowdsourcing. Acquiring labeled data is critical for training supervised models. Snow et al. (2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality. Subsequent work has developed models for improving aggregate label quality (Raykar et al., 2010; Felt et al., 2015; Kajino et al., 2012; Bi et al., 2014; Liu et al., 2012; Hovy et al., 2013). Sheshadri and Lease (2013) survey and benchmark methods. Sequence labeling. Early work on learning for sequential tasks used HMMs (Bikel et al., 1997). HMMs are a class of generative probabilistic models comprising two components: an emission However, these models are almost all in the binary or multiclass classification setting; only a few have considered sequence labeling. Dredze et al. (2009) proposed a method for learning a CRF 2 Related Work 300 model from multiple labels (although the identities of the annotators or workers were not used). Rodrigues et al. (2014) extended this approach"
P17-1028,P14-2062,0,0.0816103,"ustin, 2 Northeastern University, 3 University of Pennsylvania, atn@cs.utexas.edu, byron@ccs.neu.edu, {ljunyi|nenkova}@seas.upenn.edu, ml@utexas.edu Abstract is therefore essential to model crowdsourced label quality, both to estimate individual annotator reliability and to aggregate individual annotations to induce a single set of “reference standard” consensus labels. While many models have been proposed for aggregating crowd labels for binary or multiclass classification problems (Sheshadri and Lease, 2013), far less work has explored crowdbased annotation of sequences (Finin et al., 2010; Hovy et al., 2014; Rodrigues et al., 2014). In this paper, we investigate two complementary challenges in using sequential crowd labels: how to best aggregate them (Task 1); and how to accurately predict sequences in unannotated text given training data from the crowd (Task 2). For aggregation, one might want to induce a single set of high-quality consensus annotations for various purposes: (i) for direct use at run-time (when a given application requires human-level accuracy in identifying sequences); (ii) for sharing with others; or (iii) for training a predictive model. When human-level accuracy in tagging"
P17-1028,D08-1027,0,0.39838,"Missing"
P17-1028,A97-1029,0,\N,Missing
P18-1019,W15-3817,0,0.0286551,"lability of only small corpora, which have typically provided on the order of a couple hundred annotated abstracts or articles for very complex information extraction tasks. For example, the ExaCT system (Kiritchenko et al., 2010) applies rules to extract 21 aspects of the reported trial. It was developed and validated on a dataset of 182 marked full-text articles. The ACRES system (Summerscales et al., 2011) produces summaries of several trial characteristic, and was trained on 263 annotated abstracts. Hinting at more challenging tasks that can build upon foundational information extraction, Alamri and Stevenson (2015) developed methods for detecting contradictory claims in biomedical papers. Their corpus of annotated claims contains 259 sentences (Alamri and Stevenson, 2016). Larger corpora for EBM tasks have been derived using (noisy) automated annotation approaches. This approach has been used to build, e.g., datasets to facilitate work on Information Retrieval (IR) models for biomedical texts (Scells et al., 2017; Chung, 2009; Boudin et al., 2010). Similar approaches have been used to ‘distantly supervise’ annotation of full-text articles describing clinical trials (Wallace et al., 2016). In contrast to"
P18-1019,N16-1030,0,0.0239175,"g medical literature generally and 204 CRF Participants Interventions Outcomes LSTM-CRF Participants Interventions Outcomes Precision 0.55 0.65 0.83 Precision 0.78 0.61 0.69 Recall 0.51 0.21 0.17 Recall 0.66 0.70 0.58 F-1 0.53 0.32 0.29 F-1 0.71 0.65 0.63 Participants Interventions Outcomes Precision 0.41 0.79 0.24 Precision 0.41 0.59 0.60 Recall 0.20 0.44 0.21 Recall 0.25 0.15 0.51 F-1 0.26 0.57 0.22 F-1 0.31 0.21 0.55 each token index, which is then passed to a CRF layer for prediction. We also exploit characterlevel information by passing a bi-LSTM over the characters comprising each word (Lample et al., 2016); these are appended to the word embedding representations before being passed through the bi-LSTM. 6 Conclusions We have presented EBM-NLP: a new, publicly available corpus comprising 5,000 richly annotated abstracts of articles describing clinical randomized controlled trials. This dataset fills a need for larger scale corpora to facilitate research on NLP methods for processing the biomedical literature, which have the potential to aid the conduct of EBM. The need for such technologies will only become more pressing as the literature continues its torrential growth. The EBM-NLP corpus, acco"
P18-1019,D10-1011,0,0.157081,"ty nye.b@husky.neu.edu jessy@austin.utexas.edu romapatel996@gmail.com Yinfei Yang∗ No affiliation Iain J. Marshall King’s College London Ani Nenkova UPenn yangyin7@gmail.com iain.marshall@kcl.ac.uk nenkova@seas.upenn.edu Byron C. Wallace Northeastern University b.wallace@northeastern.edu Abstract Computational methods could expedite biomedical evidence synthesis (Tsafnat et al., 2013; Wallace et al., 2013) and natural language processing (NLP) in particular can play a key role in the task. Prior work has explored the use of NLP methods to automate biomedical evidence extraction and synthesis (Boudin et al., 2010; Marshall et al., 2017; Ferracane et al., 2016; Verbeke et al., 2012).1 But the area has attracted less attention than it might from the NLP community, due primarily to a dearth of publicly available, annotated corpora with which to train and evaluate models. Here we address this gap by introducing EBMNLP, a new corpus to power NLP models in support of EBM. The corpus, accompanying documentation, baseline model implementations for the proposed tasks, and all code are publicly available.2 EBM-NLP comprises ∼5,000 medical abstracts describing clinical trials, multiply annotated in detail with r"
P18-1019,P16-1101,0,0.100901,"Missing"
P18-1019,P14-5010,0,0.00291024,"of MeSH terms with an instantiation rate above different thresholds for the respective PIO elements are shown in Table 11. 5 5.1 Identifying P, I and O Spans We consider two baseline models: a linear Conditional Random Field (CRF) (Lafferty et al., 2001) and a Long Short-Term Memory (LSTM) neural tagging model, an LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016). In both models, we treat tokens as being either Inside (I) or Outside (O) of spans. For the CRF, features include: indicators for the current, previous and next words; part of speech tags inferred using the Stanford CoreNLP tagger (Manning et al., 2014); and character information, e.g., whether a token contains digits, uppercase letters, symbols and so on. For the neural model, the model induces features via a bi-directional LSTM that consumes distributed vector representations of input tokens sequentially. The bi-LSTM yields a hidden vector at Tasks & Baselines We outline a few NLP tasks that are central to the aim of processing medical literature generally and 204 CRF Participants Interventions Outcomes LSTM-CRF Participants Interventions Outcomes Precision 0.55 0.65 0.83 Precision 0.78 0.61 0.69 Recall 0.51 0.21 0.17 Recall 0.66 0.70 0.58"
P18-1019,J07-1005,0,0.118292,"d, e.g., datasets to facilitate work on Information Retrieval (IR) models for biomedical texts (Scells et al., 2017; Chung, 2009; Boudin et al., 2010). Similar approaches have been used to ‘distantly supervise’ annotation of full-text articles describing clinical trials (Wallace et al., 2016). In contrast to the corpora discussed above, these automatically derived datasets tend to be relatively large, but they include only shallow annotations. Other work attempts to bypass basic extraction tasks and address more complex biomedical QA and (multi-document) summarization problems to support EBM (Demner-Fushman and Lin, 2007; Moll´a and Santiago-Martinez, 2011; Abacha and 3 Data Collection PubMed provides access to the MEDLINE database3 which indexes titles, abstracts and metadata for articles from selected medical journals dating back to the 1970s. MEDLINE indexes over 24 million abstracts; the majority of these have been manually assigned metadata which we used to retrieved a set of 5,000 articles describing RCTs with an emphasis on cardiovascular diseases, cancer, and autism. These particular topics were selected to cover a range of common conditions. We decomposed the annotation process into two steps, perfor"
P18-1019,W16-6112,1,0.840026,"du romapatel996@gmail.com Yinfei Yang∗ No affiliation Iain J. Marshall King’s College London Ani Nenkova UPenn yangyin7@gmail.com iain.marshall@kcl.ac.uk nenkova@seas.upenn.edu Byron C. Wallace Northeastern University b.wallace@northeastern.edu Abstract Computational methods could expedite biomedical evidence synthesis (Tsafnat et al., 2013; Wallace et al., 2013) and natural language processing (NLP) in particular can play a key role in the task. Prior work has explored the use of NLP methods to automate biomedical evidence extraction and synthesis (Boudin et al., 2010; Marshall et al., 2017; Ferracane et al., 2016; Verbeke et al., 2012).1 But the area has attracted less attention than it might from the NLP community, due primarily to a dearth of publicly available, annotated corpora with which to train and evaluate models. Here we address this gap by introducing EBMNLP, a new corpus to power NLP models in support of EBM. The corpus, accompanying documentation, baseline model implementations for the proposed tasks, and all code are publicly available.2 EBM-NLP comprises ∼5,000 medical abstracts describing clinical trials, multiply annotated in detail with respect to characteristics of the underlying tri"
P18-1019,P17-4002,1,0.894463,"Missing"
P18-1019,U11-1012,0,0.0658617,"Missing"
P18-1019,P14-2062,0,0.0712477,"indicating redundancy in the mentions of PICO elements. In addition, we outline several NLP tasks that would directly support the practice of EBM and that may be explored using the introduced resource. We present baseline models and associated results for these tasks. 2 2.2 Crowdsourcing, which we here define operationally as the use of distributed lay annotators, has shown encouraging results in NLP (Novotney and Callison-Burch, 2010; Sabou et al., 2012). Such annotations are typically imperfect, but methods that aggregate redundant annotations can mitigate this problem (Dalvi et al., 2013; Hovy et al., 2014; Nguyen et al., 2017). Medical articles contain relatively technical content, which intuitively may be difficult for persons without domain expertise to annotate. However, recent promising preliminary work has found that crowdsourced approaches can yield surprisingly high-quality annotations in the domain of EBM specifically (Mortensen et al., 2017; Thomas et al., 2017; Wallace et al., 2017). Related Work We briefly review two lines of research relevant to the current effort: work on NLP to facilitate EBM, and research in crowdsourcing for NLP. 2.1 Crowdsourcing NLP for EBM Prior work on NLP"
P18-1019,P17-1028,1,0.911942,"ncy in the mentions of PICO elements. In addition, we outline several NLP tasks that would directly support the practice of EBM and that may be explored using the introduced resource. We present baseline models and associated results for these tasks. 2 2.2 Crowdsourcing, which we here define operationally as the use of distributed lay annotators, has shown encouraging results in NLP (Novotney and Callison-Burch, 2010; Sabou et al., 2012). Such annotations are typically imperfect, but methods that aggregate redundant annotations can mitigate this problem (Dalvi et al., 2013; Hovy et al., 2014; Nguyen et al., 2017). Medical articles contain relatively technical content, which intuitively may be difficult for persons without domain expertise to annotate. However, recent promising preliminary work has found that crowdsourced approaches can yield surprisingly high-quality annotations in the domain of EBM specifically (Mortensen et al., 2017; Thomas et al., 2017; Wallace et al., 2017). Related Work We briefly review two lines of research relevant to the current effort: work on NLP to facilitate EBM, and research in crowdsourcing for NLP. 2.1 Crowdsourcing NLP for EBM Prior work on NLP for EBM has been limit"
P18-1019,N10-1024,0,0.0830566,"Missing"
P18-1019,E12-2021,0,0.134122,"Missing"
P18-1019,D12-1053,0,0.0489279,"Missing"
Q13-1028,P98-1013,0,0.0159989,"alth, crime, ethics, can provoke emotional reactions in readers as shown in the snippet below. Medicine is a constant trade-off, a struggle to cure the disease without killing the patient first. Chemotherapy, for example, involves purposely poisoning someone – but with the expectation that the short-term injury will be outweighed by the eventual benefits. We compute affect-related features using three lexicons. The MPQA (Wilson et al., 2005) and General Inquirer (Stone et al., 1966) give lists of positive and negative sentiment words. The third resource is emotion-related words from FrameNet (Baker et al., 1998). The sizes of these lexicon are 8,221, 5,395, and 653 words respectively. We compute the counts of positive, negative, polar, and emotion words, each normalized by the total number of content words in the article (POS PROP, NEG PROP, PO LAR PROP, EMOT PROP ). We also include the proportion of emotion and polar words taken together (POLAR EMOT PROP) and the ratio between count of positive and negative words (POS BY NEG). The features with higher values in the VERY GOOD class are NEG PROP, POLAR PROP, EMOT POLAR PROP . In TYPICAL articles, POS BY NEG , EMOT PROP have higher values. VERY GOOD ar"
Q13-1028,P11-1091,0,0.0293758,"Missing"
Q13-1028,J08-1001,0,0.0436698,"Missing"
Q13-1028,P00-1037,0,0.054098,"ting is clear and well-organized but the text also contains creative use of language and a clever story-like explanation of the scientific contribution. Such properties make science journalism an attractive genre for studying writing quality. Science journalism is also a highly relevant domain for information retrieval in the context of educational as well as entertaining applications. Article quality measures can hugely benefit such systems. Prior work indicates that three aspects of article quality can be successfully predicted: a) whether a text meets the acceptable standards for spelling (Brill and Moore, 2000), grammar (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010) and discourse organization (Barzilay et al., 2002; Lapata, 2003); b) has a topic that is interesting to a particular user. For example, content-based recommendation systems standardly represent user interest using frequent words from articles in a user’s history and retrieve other articles on the same topics (Paz341 Transactions of the Association for Computational Linguistics, 1 (2013) 341–352. Action Editor: Mirella Lapata. c Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. 2013 Association for Computational Ling"
Q13-1028,P96-1041,0,0.192558,"Missing"
Q13-1028,W04-3238,0,0.0199859,"Missing"
Q13-1028,W10-4236,0,0.0182046,"Missing"
Q13-1028,de-marneffe-etal-2006-generating,0,0.00631726,"Missing"
Q13-1028,N07-1055,0,0.0209746,"Missing"
Q13-1028,P05-1045,0,0.0107963,"Dr. Remington was born in Reedville, Va., in 1922, to Maud and P. Sheldon Remington, a school headmaster. Charles spent his boyhood chasing butterflies alongside his father, also a collector. During his graduate studies at Harvard, he founded the Lepidopterists’ Society with an equally butterfly-smitten undergraduate, Harry Clench. We approximate this facet by computing the number of explicit references to people, relying on three sources of information about animacy of words. The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005). We also created a list of personal pronouns such as ‘he’, ‘myself’ etc. which standardly indicate animate entities (animate pronouns). Our third resource contains the number of times different noun phrases (NP) were followed by each of the relative pronouns ‘who’, ‘where’ and ‘which’. These counts for 664,673 noun phrases were collected by Ji and Lin (2009) from the Google Ngram Corpus (Lin et al., 2010). We use a simple heuristic to obtain a list of animate (google animate) and inanimate nouns (google inanimate) from this list. The head of each NP is taken as a candidate noun. If the noun d"
Q13-1028,Y09-1024,0,0.0169442,"omputing the number of explicit references to people, relying on three sources of information about animacy of words. The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005). We also created a list of personal pronouns such as ‘he’, ‘myself’ etc. which standardly indicate animate entities (animate pronouns). Our third resource contains the number of times different noun phrases (NP) were followed by each of the relative pronouns ‘who’, ‘where’ and ‘which’. These counts for 664,673 noun phrases were collected by Ji and Lin (2009) from the Google Ngram Corpus (Lin et al., 2010). We use a simple heuristic to obtain a list of animate (google animate) and inanimate nouns (google inanimate) from this list. The head of each NP is taken as a candidate noun. If the noun does not occur with ‘who’ in any of the noun phrases where it is the head, then it is inanimate. In contrast, if it appears only with ‘who’ in all noun phrases, it is animate. Otherwise, for each NP where the noun is a head, we check whether the count of times the noun phrase appeared with ‘who’ is greater than each of the occurrences of ‘which’, ‘where’ and ‘"
Q13-1028,P03-1054,0,0.00685121,"n-gram model (AVR CHAR PERP ALL , AVR CHAR PERP 10, 20, 30). In phoneme features, we ignore words that do not have an entry in the CMU dictionary. Word pair measures: Next we attempt to detect unusual combinations of words. We do this calculation only for certain types of syntactic relations–a) nouns and their adjective modifiers, b) verbs with adverb modifiers, c) adjacent nouns in a noun phrase and d) verb and subject pairs. Counts for co-occurrence again come from NYT 1996 articles. The syntactic relations are obtained using the constituency and dependency parses from the Stanford parser (Klein and Manning, 2003; De Marneffe et al., 2006). To avoid the influence of proper names and named entities, we replace them with tags (NNP for proper names and PERSON, ORG, LOC for named entities). We treat the words for which the dependency holds as a (auxiliary word, main word) pair. For adjective-noun and adverb-verb pairs, the auxiliary is the adjective or adverb; for noun-noun pairs, it is the first noun; and for verb-subject pairs, the auxiliary is the subject. Our idea is to compute usualness scores based on frequency with which a particular pair of words appears in the background. Specifically, we compute"
Q13-1028,P03-1069,0,0.0213815,"ntribution. Such properties make science journalism an attractive genre for studying writing quality. Science journalism is also a highly relevant domain for information retrieval in the context of educational as well as entertaining applications. Article quality measures can hugely benefit such systems. Prior work indicates that three aspects of article quality can be successfully predicted: a) whether a text meets the acceptable standards for spelling (Brill and Moore, 2000), grammar (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010) and discourse organization (Barzilay et al., 2002; Lapata, 2003); b) has a topic that is interesting to a particular user. For example, content-based recommendation systems standardly represent user interest using frequent words from articles in a user’s history and retrieve other articles on the same topics (Paz341 Transactions of the Association for Computational Linguistics, 1 (2013) 341–352. Action Editor: Mirella Lapata. c Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. 2013 Association for Computational Linguistics. zani et al., 1996; Mooney and Roy, 2000); and c) is easy to read for a target readership. Shorter words (Flesch, 1948), les"
Q13-1028,C00-1072,0,0.0372494,"and TYPICAL categories created above allow us to study writing quality without regard to topic. However a typical information retrieval scenario would involve comparison between articles of the same topic, i.e. relevant to the same query. To investigate how quality differentiation can be done within topics, we created another corpus where we paired articles of VERY GOOD and TYPICAL quality. For each article in the VERY GOOD category, we compute similarity with all articles in the TYPICAL set. This similarity is computed by comparing the topic words (computed using a loglikelihood ratio test (Lin and Hovy, 2000)) of the two articles. We retain the most similar 10 TYPICAL articles for each VERY GOOD article. We enumerate all pairs of VERY GOOD with matched up TYPICAL ARTICLES (10 in number) giving a total of 35,300 pairs. There are two distinguishing aspects of our cor343 pus. First, the average quality of articles is high. They are unlikely to have spelling, grammar and basic organization problems allowing us to investigate article quality rather than the detection of errors. Second, our corpus contains more realistic samples of quality differences for IR or article recommendation compared to prior w"
Q13-1028,lin-etal-2010-new,0,0.0172851,"ople, relying on three sources of information about animacy of words. The first is named entity (NE) tags (PERSON, ORGANIZATION and LOCATION) returned by the Stanford NE recognition tool (Finkel et al., 2005). We also created a list of personal pronouns such as ‘he’, ‘myself’ etc. which standardly indicate animate entities (animate pronouns). Our third resource contains the number of times different noun phrases (NP) were followed by each of the relative pronouns ‘who’, ‘where’ and ‘which’. These counts for 664,673 noun phrases were collected by Ji and Lin (2009) from the Google Ngram Corpus (Lin et al., 2010). We use a simple heuristic to obtain a list of animate (google animate) and inanimate nouns (google inanimate) from this list. The head of each NP is taken as a candidate noun. If the noun does not occur with ‘who’ in any of the noun phrases where it is the head, then it is inanimate. In contrast, if it appears only with ‘who’ in all noun phrases, it is animate. Otherwise, for each NP where the noun is a head, we check whether the count of times the noun phrase appeared with ‘who’ is greater than each of the occurrences of ‘which’, ‘where’ and ‘when’ (taken individually) with that noun phrase"
Q13-1028,P09-1025,0,0.123395,"Missing"
Q13-1028,J88-2004,0,0.0160571,"TYPICAL class. All these trends indicate that unusual phrases are associated with the VERY GOOD category of articles. 3.4 Sub-genres There are several sub-genres within science writing (Stocking, 2010): short descriptions of discoveries, longer explanatory articles, narratives, stories about scientists, reports on meetings, review articles and blog posts. Naturally, some of these sub-genres will be more appealing to readers. To investigate this aspect, we compute scores for some sub-genres of interest—narrative, attribution and interview. Narrative texts typically have characters and events (Nakhimovsky, 1988), so we look for entities and past tense in the articles. We count the number of sentences where the first verb in surface order is in the past tense. Then among these sentences, we pick those which have either a personal pronoun or a proper noun before the target verb (again in surface order). The proportion of such sentences in the text is taken as the NARRATIVE score. We also developed a measure to identify the degree to which the article’s content is attributed to external sources as opposed to the author’s own statements. Attribution to other sources is frequent in the news domain since m"
Q13-1028,D08-1020,1,0.882749,"Missing"
Q13-1028,P09-2004,1,0.689097,"Missing"
Q13-1028,D09-1026,0,0.019944,"Missing"
Q13-1028,D10-1094,0,0.0170979,"ative use of language and a clever story-like explanation of the scientific contribution. Such properties make science journalism an attractive genre for studying writing quality. Science journalism is also a highly relevant domain for information retrieval in the context of educational as well as entertaining applications. Article quality measures can hugely benefit such systems. Prior work indicates that three aspects of article quality can be successfully predicted: a) whether a text meets the acceptable standards for spelling (Brill and Moore, 2000), grammar (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010) and discourse organization (Barzilay et al., 2002; Lapata, 2003); b) has a topic that is interesting to a particular user. For example, content-based recommendation systems standardly represent user interest using frequent words from articles in a user’s history and retrieve other articles on the same topics (Paz341 Transactions of the Association for Computational Linguistics, 1 (2013) 341–352. Action Editor: Mirella Lapata. c Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. 2013 Association for Computational Linguistics. zani et al., 1996; Mooney and Roy, 2000); and c) is easy t"
Q13-1028,P05-1065,0,0.0735558,"Missing"
Q13-1028,C08-1109,0,0.0125707,"but the text also contains creative use of language and a clever story-like explanation of the scientific contribution. Such properties make science journalism an attractive genre for studying writing quality. Science journalism is also a highly relevant domain for information retrieval in the context of educational as well as entertaining applications. Article quality measures can hugely benefit such systems. Prior work indicates that three aspects of article quality can be successfully predicted: a) whether a text meets the acceptable standards for spelling (Brill and Moore, 2000), grammar (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010) and discourse organization (Barzilay et al., 2002; Lapata, 2003); b) has a topic that is interesting to a particular user. For example, content-based recommendation systems standardly represent user interest using frequent words from articles in a user’s history and retrieve other articles on the same topics (Paz341 Transactions of the Association for Computational Linguistics, 1 (2013) 341–352. Action Editor: Mirella Lapata. c Submitted 12/2012; Revised 3/2013, 5/2013; Published 7/2013. 2013 Association for Computational Linguistics. zani et al., 1996; Mooney and"
Q13-1028,H05-1044,0,0.00189106,"ero, the score is set to zero. All three scores are significantly higher for the TYPICAL class. 3.5 Affective content Some articles, for example those detailing research on health, crime, ethics, can provoke emotional reactions in readers as shown in the snippet below. Medicine is a constant trade-off, a struggle to cure the disease without killing the patient first. Chemotherapy, for example, involves purposely poisoning someone – but with the expectation that the short-term injury will be outweighed by the eventual benefits. We compute affect-related features using three lexicons. The MPQA (Wilson et al., 2005) and General Inquirer (Stone et al., 1966) give lists of positive and negative sentiment words. The third resource is emotion-related words from FrameNet (Baker et al., 1998). The sizes of these lexicon are 8,221, 5,395, and 653 words respectively. We compute the counts of positive, negative, polar, and emotion words, each normalized by the total number of content words in the article (POS PROP, NEG PROP, PO LAR PROP, EMOT PROP ). We also include the proportion of emotion and polar words taken together (POLAR EMOT PROP) and the ratio between count of positive and negative words (POS BY NEG). T"
Q13-1028,C98-1013,0,\N,Missing
S19-1023,P07-1063,0,0.0604287,"the Ten Item Personality Inventory (TIPI) (Gosling et al., 2003). TIPI defines the extreme ends of each personality dimension by two simple descriptions: O conventional/uncreative ↔ open to new experiences/complex C disorganized/careless ↔ dependable/selfdisciplined E reserved/quiet ↔ extroverted/enthusiastic A critical/quarrelsome ↔ sympathetic/warm N calm/emotionally stable ↔ anxious/easily upset OCEAN personality traits have been used in a number of computational linguistics studies such as developing dialog systems whose generation components can be tuned to project specific personality (Mairesse and Walker, 2007), predicting perceived personality from social media posts (Celli et al., 2013; Kosinski et al., 2013), automatic personality detection from essays (Majumder et al., 2017) and predicting specific traits, such as neuroticism, strongly linked with risk for depression and anxiety (Resnik et al., 2013). 3 4 Analysis of Human Bias After excluding inconsistent participants, we had 30 judgments for the vast majority of nationalities and 25 judgments for the professions. We use the Wilcoxon signed-rank test to determine if the mean of the human judgments for each of the five personality traits is diff"
S19-1023,D14-1162,0,0.0884717,"ers for the trait. We build separate models for each of the five personality traits. Each of the models has descriptions of both nationalities and professions and we do not differentiate between the two. category of person descriptors, demonstrating that a long list of arbitrary person categories may trigger stereotypes in people and that these stereotypes are recoverable from text embeddings. We use off-the-shelf word representations to measure the (cosine) similarity between a list of personality descriptors and a target nationality or profession. We experimented with GloVe representations (Pennington et al., 2014) trained on Common crawl (6B tokens, 400K vocab, 300d) and symmetric pattern (SP) based representations (Schwartz et al., 2015). We used TIPI to collect human judgments but these descriptors of personality are likely too short for the noisy automatic creation of personality stereotypes. For this reason, we use a larger inventory of personality trait descriptors, Goldbergs Big Five markers (Goldberg, 1992). It has about ten descriptors associated with each of the positive and negative dimensions of a personality trait, all shown in Tables 3. Different words and phrases are present in the two ve"
S19-1023,D13-1133,0,0.0560655,"Missing"
S19-1023,N18-2002,0,0.0529723,"Missing"
S19-1023,W19-3621,0,0.0495629,"Missing"
S19-1023,K15-1026,0,0.0241834,"nationalities and professions and we do not differentiate between the two. category of person descriptors, demonstrating that a long list of arbitrary person categories may trigger stereotypes in people and that these stereotypes are recoverable from text embeddings. We use off-the-shelf word representations to measure the (cosine) similarity between a list of personality descriptors and a target nationality or profession. We experimented with GloVe representations (Pennington et al., 2014) trained on Common crawl (6B tokens, 400K vocab, 300d) and symmetric pattern (SP) based representations (Schwartz et al., 2015). We used TIPI to collect human judgments but these descriptors of personality are likely too short for the noisy automatic creation of personality stereotypes. For this reason, we use a larger inventory of personality trait descriptors, Goldbergs Big Five markers (Goldberg, 1992). It has about ten descriptors associated with each of the positive and negative dimensions of a personality trait, all shown in Tables 3. Different words and phrases are present in the two vector representations in our study. While multi-word expressions such as ‘drug dealer’ and ‘movie star’ are present in the SP em"
S19-1023,Y09-1024,0,0.0415559,"s in word representation and has proposed methods for debiasing, which take in a set of words to be debiased as an argument to the algorithm (Bolukbasi et al., 2016). Work further developing this line of analysis and debiasing has appeared in recent computational linguistics venues (Zhao et al., 2017, 2018; Rudinger et al., 2018). This line of work is in stark contrast with earlier work in the field, which treated human stereotypes encoded in text as common sense knowledge that could be helpful in automating tasks such as named entity tagging and coreference resolution (Bergsma and Lin, 2006; Ji and Lin, 2009). In this complex context, we set out to study how broad stereotypes are, both in terms of groups Word representations trained on text reproduce human implicit bias related to gender, race and age. Methods have been developed to remove such bias. Here, we present results that show that human stereotypes exist even for much more nuanced judgments such as personality, for a variety of person identities beyond the typically legally protected attributes and that these are similarly captured in word representations. Specifically, we collected human judgments about a person’s Big Five personality tr"
S19-1023,D17-1323,0,0.0524422,"lementary materials in (Caliskan et al., 2017), that Google Translate translates ‘doctor’ as male and ‘nurse’ as female, Google has indeed rolled out a new version of their systems for certain language pairs, in which both translation versions are displayed3 . Similarly, earlier work has zeroed in on the gender bias in word representation and has proposed methods for debiasing, which take in a set of words to be debiased as an argument to the algorithm (Bolukbasi et al., 2016). Work further developing this line of analysis and debiasing has appeared in recent computational linguistics venues (Zhao et al., 2017, 2018; Rudinger et al., 2018). This line of work is in stark contrast with earlier work in the field, which treated human stereotypes encoded in text as common sense knowledge that could be helpful in automating tasks such as named entity tagging and coreference resolution (Bergsma and Lin, 2006; Ji and Lin, 2009). In this complex context, we set out to study how broad stereotypes are, both in terms of groups Word representations trained on text reproduce human implicit bias related to gender, race and age. Methods have been developed to remove such bias. Here, we present results that show th"
S19-1023,N18-2003,0,0.0560116,"Missing"
W10-4310,N06-2015,0,0.01815,"ations. We use the part of speech (POS) tag associated with the head of the noun phrase to assign one of the following categories: pronoun, nominal, name or expletive. When the head does not belong to the above classes, we simply record its POS tag. We also mark whether the noun phrase is a definite description using the presence of the article ‘the’. Ex 2. Rolls-Royce Motor Cars Inc. said it expects its U.S sales to remain steady at about 1,200 cars in 1990. The luxury auto maker last year sold 1,214 cars in the U.S. We use the coreference annotations from the Ontonotes corpus (version 2.9) (Hovy et al., 2006) to compute our gold-standard entity features. The WSJ portion of this corpus contains 590 articles. Here, nominalizations and temporal expressions are also annotated for coreference but we use the links between noun phrases only. We expect these features computed on the gold-standard annotations to represent an upper bound on the performance of entity features. Finally, the Penn Treebank corpus (Marcus et al., 1994) is used to obtain gold-standard parse and grammatical role information. Only adjacent sentences within the same paragraph are used in our experiments. 3 Modification. We expected"
W10-4310,D09-1036,0,0.373089,"discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). 2 Data We use 590 Wall Street Journal (WSJ) articles with overlapping annotations for discourse, coreference and syntax from three corpora. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is the largest available resource of discourse relation annotations. In the PDTB, implicit relations are annotated between adjacent sentences in the same paragraph. They are assigned senses from a hierarchy containing four top level categories–Comparison, Contingency, Temporal and Expansion. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
W10-4310,P09-1077,1,0.332049,"expressions. We aim to investigate the association between discourse relations and the way in which references to entities are realized. In our work, we employ features related to entity realization to automatically identify discourse relations in text. We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as “because” or “but”. Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007) and the semantic orientation of words (Pitler et al., 2009), or on general syntactic regularities (Lin et al., 2009). 2 Data We use 590 Wall Street Journal (WSJ) articles with overlapping annotations for discourse, coreference and syntax from three corpora. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is the largest available resource of discourse relation annotations. In the PDTB, implicit relations are annotated between adjacent sentences in the same paragraph. They are assigned senses from a hierarchy containing four top level categories–Comparison, Contingency, Temporal and Expansion. Proceedings of SIGDIAL 2010: the 11th Annual Meetin"
W10-4310,prasad-etal-2008-penn,1,0.192545,"ract The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others (CorstonOliver, 1998; Sporleder and Lascarides, 2008). Corpus studies and reading time experiments performed by Wolf and Gibson (2006) have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them. However, the predictive power of entity-related features has not been studied independently of other factors. Further motivation for studying this type of features comes from new corpus evidence (Prasad et al., 2008), that about a quarter of all adjacent sentences are linked purely by entity coherence, solely because they talk about the same entity. Entity-related features would be expected to better separate out such relations. We present the first comprehensive study of the connection between entity features and discourse relations. We show that there are notable differences in properties of referring expressions across the different relations. Sense prediction can be done with results better than random baseline using only entity realization information. Their performance, however, is lower than a know"
W10-4310,miltsakaki-etal-2004-penn,1,\N,Missing
W10-4310,N07-1054,0,\N,Missing
W10-4310,J93-2004,0,\N,Missing
W10-4310,P02-1047,0,\N,Missing
W10-4327,C94-1056,0,0.0305994,"xical overlap provides a simple and cheap alternative to discourse for computing text structure with comparable performance for the task of content selection. 1 Introduction Discourse relations such as cause, contrast or elaboration are considered critical for text interpretation, as they signal in what way parts of a text relate to each other to form a coherent whole. For this reason, the discourse structure of a text can be seen as an intermediate representation, over which an automatic summarizer can perform computations in order to identify important spans of text to include in a summary (Ono et al., 1994; Marcu, 1998; Wolf and Gibson, 2004). In our work, we study the content selection performance of different types of discourse-based features. Discourse relations interconnect units of a text and discourse formalisms have proposed different Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 147–156, c The University of Tokyo, September 24-25, 2010. 2010 Association for Computational Linguistics 147 and compare discourse-based selection to simpler non-discourse methods. 2 ple parents appear frequently in texts and do not allow a t"
W10-4327,W01-1605,0,0.196426,"a subset of the overlapping documents for which we also have human summaries available. 2.1 lated expectation, Elaboration, Example, Generalization, Attribution, Temporal sequence, Similarity, Contrast and Same. The edge between two nodes representing a relation is directed in the case of asymmetric relations such as Cause and Condition and undirected for symmetric relations like Similarity and Contrast. RST corpus RST (Mann and Thompson, 1988) proposes that coherent text can be represented as a tree formed by the combination of text units via discourse relations. The RST corpus developed by Carlson et al. (2001) contains discourse tree annotations for 385 WSJ articles from the Penn Treebank corpus. The smallest annotation units in the RST corpus are sub-sentential clauses, also called elementary discourse units (EDUs). Adjacent EDUs combine through rhetorical relations into larger spans such as sentences. The larger units recursively participate in relations with others, yielding one hierarchical tree structure covering the entire text. The discourse units participating in a RST relation are assigned either nucleus or satellite status; a nucleus is considered to be more central, or important, in the"
W10-4327,W02-0404,0,0.0124147,"Missing"
W10-4327,prasad-etal-2008-penn,1,0.917385,"002; Conroy et al., 2006). Similarly, a graph representation of the text can be computed, in which vertices represent sentences, and the nodes are connected when the sentences are similar in terms of word overlap; properties of the graph would then determine the importance of the nodes (Erkan and Radev, 2004; Mihalcea and Tarau, 2005) and guide content selection. We compare the utility of discourse features for single-document text summarization from three frameworks: Rhetorical Structure Theory (Mann and Thompson, 1988), Graph Bank (Wolf and Gibson, 2005), and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a detailed analysis of the predictive power of different types of discourse features for content selection We present analyses aimed at eliciting which specific aspects of discourse provide the strongest indication for text importance. In the context of content selection for single document summarization of news, we examine the benefits of both the graph structure of text provided by discourse relations and the semantic sense of these relations. We find that structure information is the most robust indicator of importance. Semantic sense only provides constraints on content select"
W10-4327,N03-1030,0,0.854144,"Missing"
W10-4327,C00-1072,0,0.0904035,"Missing"
W10-4327,W02-0406,0,0.0101342,"rcu, 1998; Wolf and Gibson, 2004; Uzda et al., 2008), little is known about which aspects of discourse are actually correlated with content selection power. In our work, we separate out structural and semantic features and examine their usefulness. We also investigate whether simpler intermediate representations can be used in lieu of discourse. More parsimonious, easy to compute representations of text have been proposed for summarization. For example, a text can be reduced to a set of highly descriptive topical words, the presence of which is used to signal importance for content selection (Lin and Hovy, 2002; Conroy et al., 2006). Similarly, a graph representation of the text can be computed, in which vertices represent sentences, and the nodes are connected when the sentences are similar in terms of word overlap; properties of the graph would then determine the importance of the nodes (Erkan and Radev, 2004; Mihalcea and Tarau, 2005) and guide content selection. We compare the utility of discourse features for single-document text summarization from three frameworks: Rhetorical Structure Theory (Mann and Thompson, 1988), Graph Bank (Wolf and Gibson, 2005), and Penn Discourse Treebank (PDTB) (Pra"
W10-4327,N03-1020,0,0.0285085,"Missing"
W10-4327,W04-1013,0,0.0416163,"Missing"
W10-4327,W06-1317,0,0.092053,"Missing"
W10-4327,W04-1004,0,0.271132,"for the full text, i.e. tree (Mann and Thompson, 1988) and graph (Wolf and Gibson, 2005). This structure is one source of information from discourse which can be used to compute the importance of text units. The semantics of the discourse relations between sentences could be another indicator of content importance. For example, text units connected by “cause” and “contrast” relationships might be more important content for summaries compared to those conveying “elaboration”. While previous work have focused on developing content selection methods based upon individual frameworks (Marcu, 1998; Wolf and Gibson, 2004; Uzda et al., 2008), little is known about which aspects of discourse are actually correlated with content selection power. In our work, we separate out structural and semantic features and examine their usefulness. We also investigate whether simpler intermediate representations can be used in lieu of discourse. More parsimonious, easy to compute representations of text have been proposed for summarization. For example, a text can be reduced to a set of highly descriptive topical words, the presence of which is used to signal importance for content selection (Lin and Hovy, 2002; Conroy et al"
W10-4327,J05-2005,0,0.349654,"to signal importance for content selection (Lin and Hovy, 2002; Conroy et al., 2006). Similarly, a graph representation of the text can be computed, in which vertices represent sentences, and the nodes are connected when the sentences are similar in terms of word overlap; properties of the graph would then determine the importance of the nodes (Erkan and Radev, 2004; Mihalcea and Tarau, 2005) and guide content selection. We compare the utility of discourse features for single-document text summarization from three frameworks: Rhetorical Structure Theory (Mann and Thompson, 1988), Graph Bank (Wolf and Gibson, 2005), and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a detailed analysis of the predictive power of different types of discourse features for content selection We present analyses aimed at eliciting which specific aspects of discourse provide the strongest indication for text importance. In the context of content selection for single document summarization of news, we examine the benefits of both the graph structure of text provided by discourse relations and the semantic sense of these relations. We find that structure information is the most robust indicator of importance."
W10-4327,J00-3005,0,0.583013,"Missing"
W10-4327,J93-2004,0,\N,Missing
W10-4327,P06-2020,0,\N,Missing
W10-4327,P09-1077,1,\N,Missing
W10-4327,P04-1049,0,\N,Missing
W10-4327,I05-2004,0,\N,Missing
W11-1605,J05-3002,0,0.0340575,"more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics recently, Marsi et al. (2010) manually annotated the"
W11-1605,D08-1019,0,0.0147024,"nd Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics recently, Marsi et al. (2010) manually annotated the transformations between sour"
W11-1605,N07-1023,0,0.087651,"nt gets changed in the summary with respect to specificity. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Association for Computational"
W11-1605,N09-1041,0,0.178042,"techniques. General sentences are overview statements. Specific sentences supply details. An example general and specific sentence from different parts of a news article are shown in Table 1. Prior studies have advocated that the distinction between general and specific content is relevant for text summarization. Jing and McKeown (2000) studied what edits people use to create summaries from sentences in the source text. Two of the operations they identify are generalization and specification where the source content gets changed in the summary with respect to specificity. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube,"
W11-1605,A00-2024,0,0.0590099,"summary. In this work, we argue that the general and specific nature of the content is also taken into account by human summarizers; we show that this distinction is directly related to the quality of the summary and it also calls for the use and refinement of text-to-text generation techniques. General sentences are overview statements. Specific sentences supply details. An example general and specific sentence from different parts of a news article are shown in Table 1. Prior studies have advocated that the distinction between general and specific content is relevant for text summarization. Jing and McKeown (2000) studied what edits people use to create summaries from sentences in the source text. Two of the operations they identify are generalization and specification where the source content gets changed in the summary with respect to specificity. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two type"
W11-1605,N03-1020,0,0.139942,"of the importance of the content present. So we performed an analysis to check the contribution of generality to the content scores in addition to the importance factor. We combine a measure of content importance Predictor (Intercept) rouge2 avgspec Mean β 0.212 1.299 -0.166 Stdev. β 0.03 0.11 0.04 t value 6.87 11.74 -4.21 p-value 2.3e-11 * &lt; 2e-16 * 3.1e-05 * sums. 202 400 79 avg specificity 0.71 0.72 0.77 Table 4: Number of summaries at extreme levels of linguistic quality scores and their average specificity values Table 3: Results from regression test from the ROUGE automatic evaluation (Lin and Hovy, 2003; Lin, 2004) with generality to predict the coverage scores. We use the same reference as used for the official coverage score evaluation and compute ROUGE-2 which is the recall of bigrams of the human summary by the system summary. Next we train a regression model on our data using the ROUGE-2 score and specificity as predictors of the coverage score. We then inspected the weights learnt in the regression model to identify the influence of the predictors. Table 3 shows the mean values and standard deviation of the beta coefficients. We also report the results from a test to determine if the b"
W11-1605,W04-1013,0,0.012092,"f the content present. So we performed an analysis to check the contribution of generality to the content scores in addition to the importance factor. We combine a measure of content importance Predictor (Intercept) rouge2 avgspec Mean β 0.212 1.299 -0.166 Stdev. β 0.03 0.11 0.04 t value 6.87 11.74 -4.21 p-value 2.3e-11 * &lt; 2e-16 * 3.1e-05 * sums. 202 400 79 avg specificity 0.71 0.72 0.77 Table 4: Number of summaries at extreme levels of linguistic quality scores and their average specificity values Table 3: Results from regression test from the ROUGE automatic evaluation (Lin and Hovy, 2003; Lin, 2004) with generality to predict the coverage scores. We use the same reference as used for the official coverage score evaluation and compute ROUGE-2 which is the recall of bigrams of the human summary by the system summary. Next we train a regression model on our data using the ROUGE-2 score and specificity as predictors of the coverage score. We then inspected the weights learnt in the regression model to identify the influence of the predictors. Table 3 shows the mean values and standard deviation of the beta coefficients. We also report the results from a test to determine if the beta coeffici"
W11-1605,E06-1038,0,0.0699902,"the source content gets changed in the summary with respect to specificity. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Ass"
W11-1605,D08-1057,0,0.0135117,"l as well as those at specific subtopic levels were learnt. The underlying idea here is that summaries are generated by a combination of content from both these levels. But since the preference for these two types of content is not known, Haghighi and Vanderwende (2009) use some heuristic proportions. Many systems that deal with sentence compression (Knight and Marcu, 2002; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008) and fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), do not take into account the specificity of the original or desired sentence. However, Wan et al. (2008) introduce a generation task where a summary sentence is created by combining content from a key (general) sentence and its supporting sentences in the source. More 34 Workshop on Monolingual Text-To-Text Generation, pages 34–42, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 34–42, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics recently, Marsi et al. (2010) manually annotated the transformations between source and compressed phrases and observe that generalization is a frequent transformation. But it is not know"
W11-1605,W02-0406,0,\N,Missing
W12-2601,N03-1020,0,0.0858925,"te the accuracy of these metrics in reproducing human judgements about the relative content quality of pairs of systems and present an empirical assessment of the relationship between statistically significant differences between systems according to manual evaluations, and the difference according to automatic evaluations. Finally, we present a case study of how new metrics should be compared to the reference evaluation, as we search for even more accurate automatic measures. 1 Introduction Automatic evaluation of content selection in summarization, particularly the ROUGE evaluation toolkit (Lin and Hovy, 2003), has been enthusiastically adopted by researchers since its introduction in 2003. It is now standardly used to report results in publications; however we have a poor understanding of the accuracy of automatic evaluation. How often 1 do we publish papers where we report an improvement according to automatic evaluation, but nevertheless, a standard manual evaluation would have led us to different conclusions? In our work we directly address this question, and hope that our encouraging findings contribute to a better understanding of the strengths and shortcomings of automatic evaluation. The ai"
W12-2601,W06-1643,0,0.0516719,"an performance3 . As we will see, there is no single ROUGE variant that has both of these desirable properties. Finally, in Section 5, we discuss ways to compare other automatic evaluation protocols with the refer2 For now, automatic systems do not have the performance of humans, thus, the ability to distinguish between human and automatically generated summaries is an exemplar of the wider problem of distinguishing high quality summaries from others. 3 Such anomalous findings, when using automatic evaluation, have been reported for some summarization genres such as summarization of meetings (Galley, 2006). 2 ence ROUGE metrics we have established. We define standard tests for significance that would identify evaluations that are significantly more accurate than the current reference measures, thus warranting wider adoption for future system development and reporting of results. As a case study we apply these to the TAC AESOP (Automatically Evaluating Summaries of Peers) task which called for the development of novel evaluation techniques that are more accurate than ROUGE evaluations. 2 Manual evaluation Before automatic evaluation methods are developed, it is necessary to establish a desirable"
W12-2601,P10-1056,1,0.42566,"certain tasks (Lin, 2004; Liu and Liu, 2010). In our work, we compare automatic metrics with the manual methods used at TAC: Pyramid and Responsiveness. These manual metrics primarily aim to assess if the content of the summary is appropriately chosen to include only important information. They do not deal directly with the linguistic quality of the summary—how grammatical are the sentences or how well the information in the summary is organized. Subsequently, in the experiments that we present in later sections, we do not address the assessment of automatic evaluations of linguistic quality (Pitler et al., 2010), but instead analyze the performance of ROUGE and other related metrics that aim to score summary content. The Pyramid evaluation (Nenkova et al., 2007) relies on multiple human-written gold-standard summaries for the input. Annotators manually identify shared content across the gold-standards regardless of the specific phrasing used in each. The pyramid score is based on the “popularity” of information in the gold-standards. Information that is shared across several human gold-standards is given higher weight when a summary is evaluated relative to the gold-standard. Each evaluated summary i"
W12-2601,P08-1094,1,0.92368,"Missing"
W12-2601,D11-1043,1,0.828021,"Missing"
W14-4320,D09-1036,0,0.479635,"Missing"
W14-4320,P13-2013,0,0.542988,"icit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at discovering if a particular relation holds and keeping the full training set for multi-class prediction. To highlight the problem, in Table 1 we show the distribution of implicit relation classes in the entire PDTB. In our work, we aim to develop classifiers to"
W14-4320,W12-1614,0,0.227995,"he task 142 Proceedings of the SIGDIAL 2014 Conference, pages 142–150, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at disc"
W14-4320,C08-2022,1,0.902884,"Missing"
W14-4320,I11-1120,0,0.08792,"Missing"
W14-4320,P09-1077,1,0.903601,"l and linguistically rich features for the task 142 Proceedings of the SIGDIAL 2014 Conference, pages 142–150, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for on"
W14-4320,J95-2003,0,0.732462,"Missing"
W14-4320,D10-1039,0,0.0141057,"ns, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at discovering if a particular relation holds and keeping the full training set for multi-class prediction. To highlight the problem, in Table 1 we show the distribution of implicit relation classes in the entire PDTB. In our work, we aim"
W14-4320,prasad-etal-2008-penn,0,0.0745469,"tion Discourse relations holding between adjacent sentences in text play an essential role in establishing local coherence and contribute to the semantic interpretation of the text. For example, the causal relationship is helpful for textual entailment or question answering while restatement and exemplification are important for automatic summarization. Predicting the type of implicit relations, which are not signaled by any of the common explicit discourse connectives such as because, however, has proven to be a most challenging task in discourse analysis. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provided valuable annotations of implicit relations. Most research to date has focused on developing and refining lexical and linguistically rich features for the task 142 Proceedings of the SIGDIAL 2014 Conference, pages 142–150, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied"
W14-4327,N07-1054,0,0.0670673,"Missing"
W14-4327,C10-2172,0,0.238405,"Missing"
W14-4327,D10-1039,0,0.0325299,"Missing"
W14-4327,D09-1036,0,0.646066,"Missing"
W14-4327,P02-1047,0,0.656618,"Missing"
W14-4327,W12-1614,0,0.802014,"s. For example the Alternative class and Concession class have only 185 and 228 occurrences, respectively, in the 16,224 implicit relation annotations of the PDTB. 2 We use SVMLight (Joachims, 1999) with linear kernel. 200 prior work and downsampled the negative class so the number of positive and negative samples are equal in the training set.3 Our training set consists of PDTB sections 219. The testing set consists of sections 20-24. Like most studies, we do not include sections 0-1 in the training set. We expanded the test set (sections 23 or 23-24) used in previous work (Lin et al., 2014; Park and Cardie, 2012) to ensure the number of examples of the smaller relations, particularly of Temporal or Instantiation, are suitable for carrying out reliable tests for statistical significance. Some of the discourse relations are much larger than others, so we report our results in term of Fmeasure for each relation and average unweighted accuracy. Significance tests over F scores were carried out using a paired t-test. To do this, the test set is randomly partitioned into ten groups. In each group, the relation distribution was kept as close as possible to the overall test set. 4 word-pairs binary-lexical #"
W14-4327,P09-1077,1,0.917674,"orm those including the lexical items for 6 of the 7 relations. Notably, production rules without lexical items are among the three worst representations, outperforming only the pure lexical features in some cases. This is a strong indication that being both a sparse syntactic representation and lacking lexical information, these features are not favored in this task. Pure lexical features give the worst or second to worst F scores, significantly worse than the alternatives in most of the cases. In Table 7 we list the binary classification results from prior work: feature selected word pairs (Pitler et al., 2009), aggregated word pairs (Biran and McKeown, 2013), production rules only (Park and Cardie, 2012), and the best combination possible from a variety of features (Park and Cardie, 2012), all of which include production rules. We aim to compare the relative gains in performance with different representations. Note that the absolute results from prior work are not exactly comparable to ours for two reasons — the training Ex. Analyst estimate the value of the BellSouth proposal at about $115 to $125 a share. [Implicit=AND] They value McCaw’s bid at $112 to $118 a share . Here the contrast clearly ha"
W14-4327,prasad-etal-2008-penn,0,0.83214,"prior lexical and syntactic features, and improves significantly the classification of implicit discourse relations. Given these findings, we address the question if any lexical information at all should be preserved in discourse parsers. We find that purely syntactic representations show lower recognition Introduction Implicit discourse relations hold between adjacent sentences in the same paragraph, and are not signaled by any of the common explicit discourse connectives such as because, however, meanwhile, etc. Consider the two examples below, drawn from the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), of a causal and a contrast relation, respectively. The italic and bold fonts mark the arguments of the relation, i.e the portions of the text connected by the discourse relation. Ex1: Mrs Yeargin is lying. [Implicit = BECAUSE] They found students in an advanced class a year earlier who said she gave them similar help. 199 Proceedings of the SIGDIAL 2014 Conference, pages 199–207, c Philadelphia, U.S.A., 18-20 June 2014. 2014 Association for Computational Linguistics lustrated in the following example where the two arguments are marked in bold and italic: for most relations, indicating that l"
W14-4327,J93-2004,0,\N,Missing
W14-4327,P13-2013,0,\N,Missing
W16-1717,P11-1049,0,0.0129791,"one of the fairly wellunderstood ways for changing the amount of detail. Which sentences to remove can be decided in a system’s content selection module by a number of competitive approaches (Gillick and Favre, 2009; Lin and Bilmes, 2011; Kulesza and Taskar, 2011). Similarly, one can perform sentence compression, removing words or phrases from a sentence in the original text to form a summary sentence (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008) or perform sentence selection and compression jointly (Berg-Kirkpatrick et al., 2011). In this paper, we focus our attention on a much finer level to study the changes of specificity on the phrase level. The existence of these changes have been documented in prior work (Jing and McKeown, 2000; Marsi and Krahmer, 2010). Jing and McKeown (2000) analyzed 30 single document articles and their summaries and characterized the transformations performed on the original text to form a summary. They did not give statistics about the relative frequency of each transformation operation but list “add descriptions or names for people and organizations” and “substitute phrases with more gene"
W16-1717,C08-1018,0,0.0257536,"the amount of detail in the original news texts. Removing entire sentences is one of the fairly wellunderstood ways for changing the amount of detail. Which sentences to remove can be decided in a system’s content selection module by a number of competitive approaches (Gillick and Favre, 2009; Lin and Bilmes, 2011; Kulesza and Taskar, 2011). Similarly, one can perform sentence compression, removing words or phrases from a sentence in the original text to form a summary sentence (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008) or perform sentence selection and compression jointly (Berg-Kirkpatrick et al., 2011). In this paper, we focus our attention on a much finer level to study the changes of specificity on the phrase level. The existence of these changes have been documented in prior work (Jing and McKeown, 2000; Marsi and Krahmer, 2010). Jing and McKeown (2000) analyzed 30 single document articles and their summaries and characterized the transformations performed on the original text to form a summary. They did not give statistics about the relative frequency of each transformation operation but list “add desc"
W16-1717,E14-4040,0,0.0647081,"Missing"
W16-1717,N07-1023,0,0.0280455,"ransformations is changing the amount of detail in the original news texts. Removing entire sentences is one of the fairly wellunderstood ways for changing the amount of detail. Which sentences to remove can be decided in a system’s content selection module by a number of competitive approaches (Gillick and Favre, 2009; Lin and Bilmes, 2011; Kulesza and Taskar, 2011). Similarly, one can perform sentence compression, removing words or phrases from a sentence in the original text to form a summary sentence (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008) or perform sentence selection and compression jointly (Berg-Kirkpatrick et al., 2011). In this paper, we focus our attention on a much finer level to study the changes of specificity on the phrase level. The existence of these changes have been documented in prior work (Jing and McKeown, 2000; Marsi and Krahmer, 2010). Jing and McKeown (2000) analyzed 30 single document articles and their summaries and characterized the transformations performed on the original text to form a summary. They did not give statistics about the relative frequency of each transformation oper"
W16-1717,W09-1802,0,0.0137261,"a systematic analysis, elucidating the capabilities needed for automating the generation of more general or more specific references. 1 Introduction Summarization involves a number of complex transformations to condense the gist of a text into a short summary (Nenkova and McKeown, 2011). One of these transformations is changing the amount of detail in the original news texts. Removing entire sentences is one of the fairly wellunderstood ways for changing the amount of detail. Which sentences to remove can be decided in a system’s content selection module by a number of competitive approaches (Gillick and Favre, 2009; Lin and Bilmes, 2011; Kulesza and Taskar, 2011). Similarly, one can perform sentence compression, removing words or phrases from a sentence in the original text to form a summary sentence (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008) or perform sentence selection and compression jointly (Berg-Kirkpatrick et al., 2011). In this paper, we focus our attention on a much finer level to study the changes of specificity on the phrase level. The existence of these changes have been documented in prior work ("
W16-1717,N03-1026,0,0.0328278,"into a short summary (Nenkova and McKeown, 2011). One of these transformations is changing the amount of detail in the original news texts. Removing entire sentences is one of the fairly wellunderstood ways for changing the amount of detail. Which sentences to remove can be decided in a system’s content selection module by a number of competitive approaches (Gillick and Favre, 2009; Lin and Bilmes, 2011; Kulesza and Taskar, 2011). Similarly, one can perform sentence compression, removing words or phrases from a sentence in the original text to form a summary sentence (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008) or perform sentence selection and compression jointly (Berg-Kirkpatrick et al., 2011). In this paper, we focus our attention on a much finer level to study the changes of specificity on the phrase level. The existence of these changes have been documented in prior work (Jing and McKeown, 2000; Marsi and Krahmer, 2010). Jing and McKeown (2000) analyzed 30 single document articles and their summaries and characterized the transformations performed on the original text to form a summary. They did not giv"
W16-1717,J11-4007,1,0.886934,"Missing"
W16-1717,P05-1036,0,0.021728,"(Nenkova and McKeown, 2011). One of these transformations is changing the amount of detail in the original news texts. Removing entire sentences is one of the fairly wellunderstood ways for changing the amount of detail. Which sentences to remove can be decided in a system’s content selection module by a number of competitive approaches (Gillick and Favre, 2009; Lin and Bilmes, 2011; Kulesza and Taskar, 2011). Similarly, one can perform sentence compression, removing words or phrases from a sentence in the original text to form a summary sentence (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008) or perform sentence selection and compression jointly (Berg-Kirkpatrick et al., 2011). In this paper, we focus our attention on a much finer level to study the changes of specificity on the phrase level. The existence of these changes have been documented in prior work (Jing and McKeown, 2000; Marsi and Krahmer, 2010). Jing and McKeown (2000) analyzed 30 single document articles and their summaries and characterized the transformations performed on the original text to form a summary. They did not give statistics about the rela"
W16-1717,P11-1052,0,0.0212391,"ucidating the capabilities needed for automating the generation of more general or more specific references. 1 Introduction Summarization involves a number of complex transformations to condense the gist of a text into a short summary (Nenkova and McKeown, 2011). One of these transformations is changing the amount of detail in the original news texts. Removing entire sentences is one of the fairly wellunderstood ways for changing the amount of detail. Which sentences to remove can be decided in a system’s content selection module by a number of competitive approaches (Gillick and Favre, 2009; Lin and Bilmes, 2011; Kulesza and Taskar, 2011). Similarly, one can perform sentence compression, removing words or phrases from a sentence in the original text to form a summary sentence (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008) or perform sentence selection and compression jointly (Berg-Kirkpatrick et al., 2011). In this paper, we focus our attention on a much finer level to study the changes of specificity on the phrase level. The existence of these changes have been documented in prior work (Jing and McKeown, 2000"
W16-1717,E06-1038,0,0.0291963,". One of these transformations is changing the amount of detail in the original news texts. Removing entire sentences is one of the fairly wellunderstood ways for changing the amount of detail. Which sentences to remove can be decided in a system’s content selection module by a number of competitive approaches (Gillick and Favre, 2009; Lin and Bilmes, 2011; Kulesza and Taskar, 2011). Similarly, one can perform sentence compression, removing words or phrases from a sentence in the original text to form a summary sentence (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008) or perform sentence selection and compression jointly (Berg-Kirkpatrick et al., 2011). In this paper, we focus our attention on a much finer level to study the changes of specificity on the phrase level. The existence of these changes have been documented in prior work (Jing and McKeown, 2000; Marsi and Krahmer, 2010). Jing and McKeown (2000) analyzed 30 single document articles and their summaries and characterized the transformations performed on the original text to form a summary. They did not give statistics about the relative frequency o"
W16-1717,R11-1037,0,0.0259897,"Missing"
W16-1717,W01-0100,0,\N,Missing
W19-2303,D18-1443,0,0.261203,"s unwarranted given the non-linear shape of the F1 curve. Instead, we choose to normalize the F1 score of a 4 Evaluation on CNN/DailyMail Test Set We re-test 16 systems on the CNN/DailyMail test set: (1) Pointer-Generator (See et al., 2017) and its variants: a baseline sequence-to-sequence attentional model (baseline), a Pointer-Generator model with soft switch between generating from vocabulary and copying from input (pointer-gen) and the same Pointer-Generator with coverage loss (pointer-cov) for preventing repetitive generation. There are three other content-selection variants proposed in (Gehrmann et al., 2018) which are also based on Pointer-Generator: (i) aligning ref4 We could penalize summaries that are shorter or longer than the reference, similar to the brevity penalty in BLEU (Papineni et al., 2002). Such an approach however assumes that the reference summary length is ideal and deviations from that are clearly undesirable, a fairly strong assumption. 23 System latent cmpr baseline textrank 50 mask lo BU trans bottom up pointer-gen lead-pointer mask hi DiffMask lead-cov pointer-cov multitask textrank 70 latent ext lead3 Rank change Spearman Pearson erence with source article (mask-hi, mask-lo"
W19-2303,W00-0405,0,0.305847,"length, calling for the need of similar normalization in reporting human scores. 1 Introduction Algorithms for text summarization of news developed between 2000 and 2015, were evaluated with a requirement to produce a summary of a pre-specified length.1 This practice likely followed the DUC shared task, which called for summaries of length fixed in words or bytes (Over 1 Here is a list of the most cited ‘summarization’ papers of that period according to Google Scholar (Erkan and Radev, 2004; Radev et al., 2004; Gong and Liu, 2001; Conroy and O’leary, 2001; Lin and Hovy, 2000; Mihalcea, 2004; Goldstein et al., 2000). All of them present evaluations in which alternative systems produce summaries of the same length, with two of the papers fixing the number of sentences rather than number of words. 2 As a matter of fact, the established practice was to require human references of different lengths in order to evaluate system outputs of the respective length, a practice that has recently been shown unnecessary (Shapira et al., 2018). 21 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 21–29 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Ass"
W19-2303,D18-2029,0,0.0431248,"Missing"
W19-2303,N18-1065,0,0.0615502,"Missing"
W19-2303,D07-1001,0,0.0296884,"awa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead of recall, with the following justification “full-length recall favors longer summaries, so it may not be fair to use this metric to compare two systems that differ in summary lengths. Full-length F1 solves this pr"
W19-2303,hong-etal-2014-repository,1,0.841148,"ength summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead of recall, with the following justification “full-length recall favors longer summaries, so it may not be fair to use this metric to compare two systems that differ in summary lengths. Full-length F1 solves this problem since it can penalize longer summaries.”. The rest of the neural summarization literature adopted F1 evaluation without further discussion. In this paper we study how ROUGE F1 scores Until recently, summarization evaluations compared systems that produce summaries of the same target length. Neural approaches to summ"
W19-2303,P08-1080,0,0.0391558,"18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to"
W19-2303,W11-1611,0,0.139017,"Missing"
W19-2303,D16-1140,0,0.0280492,"ion of the Neural Summarization Literature Simeng Sun1 Ori Shapira2 Ido Dagan2 Ani Nenkova1 1 Department of Computer and Information Science, University of Pennsylvania 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel {simsun, nenkova}@seas.upenn.edu obspp18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for t"
W19-2303,P02-1040,0,0.105041,"(1) Pointer-Generator (See et al., 2017) and its variants: a baseline sequence-to-sequence attentional model (baseline), a Pointer-Generator model with soft switch between generating from vocabulary and copying from input (pointer-gen) and the same Pointer-Generator with coverage loss (pointer-cov) for preventing repetitive generation. There are three other content-selection variants proposed in (Gehrmann et al., 2018) which are also based on Pointer-Generator: (i) aligning ref4 We could penalize summaries that are shorter or longer than the reference, similar to the brevity penalty in BLEU (Papineni et al., 2002). Such an approach however assumes that the reference summary length is ideal and deviations from that are clearly undesirable, a fairly strong assumption. 23 System latent cmpr baseline textrank 50 mask lo BU trans bottom up pointer-gen lead-pointer mask hi DiffMask lead-cov pointer-cov multitask textrank 70 latent ext lead3 Rank change Spearman Pearson erence with source article (mask-hi, mask-lo) (ii) training tagger and summarizer at the same time (multitask), and (iii) a differentiable model with a soft mask predicted by selection probabilities (DiffMask). (2) Abstractive system with bott"
W19-2303,C00-1072,0,0.0251998,"ceived quality increase with summary length, calling for the need of similar normalization in reporting human scores. 1 Introduction Algorithms for text summarization of news developed between 2000 and 2015, were evaluated with a requirement to produce a summary of a pre-specified length.1 This practice likely followed the DUC shared task, which called for summaries of length fixed in words or bytes (Over 1 Here is a list of the most cited ‘summarization’ papers of that period according to Google Scholar (Erkan and Radev, 2004; Radev et al., 2004; Gong and Liu, 2001; Conroy and O’leary, 2001; Lin and Hovy, 2000; Mihalcea, 2004; Goldstein et al., 2000). All of them present evaluations in which alternative systems produce summaries of the same length, with two of the papers fixing the number of sentences rather than number of words. 2 As a matter of fact, the established practice was to require human references of different lengths in order to evaluate system outputs of the respective length, a practice that has recently been shown unnecessary (Shapira et al., 2018). 21 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 21–29 c Minneapoli"
W19-2303,D18-1444,0,0.037613,"arization Literature Simeng Sun1 Ori Shapira2 Ido Dagan2 Ani Nenkova1 1 Department of Computer and Information Science, University of Pennsylvania 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel {simsun, nenkova}@seas.upenn.edu obspp18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningf"
W19-2303,D15-1044,0,0.668831,"hapira2 Ido Dagan2 Ani Nenkova1 1 Department of Computer and Information Science, University of Pennsylvania 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel {simsun, nenkova}@seas.upenn.edu obspp18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take ou"
W19-2303,D13-1069,0,0.0245956,"lutions and Re-Examination of the Neural Summarization Literature Simeng Sun1 Ori Shapira2 Ido Dagan2 Ani Nenkova1 1 Department of Computer and Information Science, University of Pennsylvania 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel {simsun, nenkova}@seas.upenn.edu obspp18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke an"
W19-2303,P17-1099,0,0.494577,"ision and F1 scores for lead, random, textrank and Pointer-Generator on the CNN/DailyMail test set. comparing with the version of extracting the first three sentences of the article. Random Randomly and non-repetitively selects full sentences with a total number of tokens that is no more than the desired length. TextRank Sentences are scored by their centrality in the graph with sentences as the nodes (Erkan and Radev, 2004; Mihalcea, 2004). We use the Gensim.summarization package (Barrios et al., 2016) to produce these summaries. Pointer-gen: We use the pre-trained PointerGenerator model of (See et al., 2017) to get outputs with varying lengths by restricting both minimum and maximum decoding steps.3 The largest values for min and max decoding step are set to 130 and 150 respectively due to limited computing resources. Figure 1 shows that ROUGE recall keeps increasing as the summary becomes longer, while precision decreases. For recall, it is clear that even the random system produces better scoring summaries if it is allowed longer length. For all four systems, ROUGE F1 curves first rise steeply, then decline gradually. For summaries longer than 100 words, none of the systems produces a better sc"
W19-2303,E99-1011,0,0.357601,"length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead"
W19-2303,D18-1087,1,0.837137,"papers of that period according to Google Scholar (Erkan and Radev, 2004; Radev et al., 2004; Gong and Liu, 2001; Conroy and O’leary, 2001; Lin and Hovy, 2000; Mihalcea, 2004; Goldstein et al., 2000). All of them present evaluations in which alternative systems produce summaries of the same length, with two of the papers fixing the number of sentences rather than number of words. 2 As a matter of fact, the established practice was to require human references of different lengths in order to evaluate system outputs of the respective length, a practice that has recently been shown unnecessary (Shapira et al., 2018). 21 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 21–29 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics Recall Precision ROUGE-1 scores 0.8 0.7 0.40 0.6 0.35 0.300 0.25 50 100 150 200 250 lead random textrank pointer-gen 0.375 0.325 0.4 lead random textrank pointer-gen 0.400 0.350 0.30 0.5 0.3 ROUGE-2 scores F1 lead random textrank pointer-gen 0.45 0.275 0.20 0.250 0.15 300 50 0.40 0.20 0.35 0.18 100 150 200 250 300 lead random textrank pointer-gen 50 100 150 200 250 0.18 300 lead"
W19-2303,E06-1038,0,0.0251894,"ns (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead of recall, with the following justification “full-length recall favors longer summaries, so it may not be fair to use this metric to compare two systems that differ in summary lengths. Ful"
W19-2303,P04-3020,0,0.484802,"ase with summary length, calling for the need of similar normalization in reporting human scores. 1 Introduction Algorithms for text summarization of news developed between 2000 and 2015, were evaluated with a requirement to produce a summary of a pre-specified length.1 This practice likely followed the DUC shared task, which called for summaries of length fixed in words or bytes (Over 1 Here is a list of the most cited ‘summarization’ papers of that period according to Google Scholar (Erkan and Radev, 2004; Radev et al., 2004; Gong and Liu, 2001; Conroy and O’leary, 2001; Lin and Hovy, 2000; Mihalcea, 2004; Goldstein et al., 2000). All of them present evaluations in which alternative systems produce summaries of the same length, with two of the papers fixing the number of sentences rather than number of words. 2 As a matter of fact, the established practice was to require human references of different lengths in order to evaluate system outputs of the respective length, a practice that has recently been shown unnecessary (Shapira et al., 2018). 21 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 21–29 c Minneapolis, Minnesota, US"
W19-2303,D18-1088,0,0.0610359,"Missing"
W19-2303,K16-1028,0,0.300069,"e effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead of recall, with the following justification “full-length recall favors longer summaries, so it may not be fair to use this metric to compare two systems that differ in summary lengths. Full-length F1 solves this problem since it can penalize longer summaries.”. The rest of the neural summarization literature adopted F1 evaluation without further discussion. In this paper we study how ROUGE F1 scores Until recently, summarization evaluations compared systems that produce summaries of the same target length. Neural approaches to summarization however have done a"
W19-2606,N16-1030,0,0.0138707,"Missing"
W19-2606,P18-1019,1,0.832476,"owed better VAS and CSS patterns than the control group at 1-month follow-up (P < .05). No complications occurred in the study group. In the control group , there were 2 cases of arterial punctures and 3 cases of direct nerve injury with neurological deficit for 2 months. Ultrasonography-guided suprascapular nerve injection is a safe, accurate, and useful procedure compared to the blind technique. Figure 2: Example of a Human RCT abstract with the predicted spans for Participants (red), Intervention (blue) and Outcome (orange) Such granular spans were annotated in the original EBM-NLP corpus (Nye et al., 2018), along with a detailed types of interventions and outcomes. Performance for labeling these details and granular spans however is much lower than that for the original high-level spans that we examine here. An alternative would be to learn chunking rules to identify the condition, individual interventions and individual outcomes in an unsupervised manner, by collocation analysis of the thousands of extracted snippets from the MEDLINE corpus. In sum, progress on IE to aid browsing of the medical literature would require several modifications to track meaningful progress. Evaluation should be on"
W19-2801,P98-1012,0,0.626464,"he number of similar mentions shared by the gold and predicted clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |ki ∩ rj |, and in CEAFe, φ(ki , rj ) = |ki |+|r . j| LEA Recall is computed as the fraction of correctly resolved links betw"
W19-2801,I13-1193,0,0.017275,"any name or are linked to a wrong name, the results would not be useful for downstream tasks. Standard coreference metrics do not incorporate these aspects and hence give high performance for results unsuitable for further use. We also show that the existing metrics are not sensitive to finding any mention to an entity at all. They give higher performance for systems that do not find a large number of entities but do good coreference resolution on the subset of entities they find. This problem of coreference chains without any named mentions being unsuitable has previously been discussed in (Chen and Ng, 2013). The authors argued that a name is more informative than a nominal, which is more informative than a pronoun so they assign different weights to co-reference links (mention-antecedent pairs) in a chain depending on the type of mentions the link contains. They assign a higher weight to In many NLP applications like search and information extraction for named entities, it is necessary to find all the mentions of a named entity, some of which appear as pronouns (she, his, etc.) or nominals (the professor, the German chancellor, etc.). It is therefore important that coreference resolution systems"
W19-2801,D14-1221,0,0.0185875,"ntribution 1 Proceedings of the 2nd Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC 2019), pages 1–7, c Minneapolis, USA, June 7, 2019. 2019 Association for Computational Linguistics B-cubed B 3 works on the mention level. It iterates over all gold-standard mentions of an entity, averaging the recall of its gold cluster in its predicted cluster. It computes precision by reversing the role of gold and predicted clusters. a link having a name than one that doesn’t and also higher weight to a link having a nominal than a link that contains just pronouns. Similarly, (Martschat and Strube, 2014) perform an error analysis for co-reference by choosing an antecedent that is a name or a nominal in this order because they are more informative than a pronoun. However, we argue that we should view the coreference chains as a whole instead of individual links when evaluating systems for downstream application. If a chain contains even one named mention, it should be sufficient for using it in applications and we need not consider the mention type in each link within the chain. We introduce metrics focused on Named Entity Coreference (NEC) which separate the identification of entities and res"
W19-2801,P16-1061,0,0.050434,"Missing"
W19-2801,P16-1060,0,0.0187315,"ed clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |ki ∩ rj |, and in CEAFe, φ(ki , rj ) = |ki |+|r . j| LEA Recall is computed as the fraction of correctly resolved links between mentions. Results for each entity are weighted by its num"
W19-2801,P14-2006,0,0.102739,"ki ∈K |ki ∩rj |2 |ki | |ki | CEAF CEAF first maps each gold cluster to a predicted cluster. It then computes recall as the number of similar mentions shared by the gold and predicted clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |"
W19-2801,D15-1162,0,0.0265647,"eural end-to-end systems of (Lee et al., 2017) and (Lee et al., 2018) on traditional and NEC metrics. These general coreference systems find coreferring expressions of any type and produce coreference chains for all mentioned entities. In NEC, the goal is to find all mentions to an entity that has been referred to by name at least once in the document. The output of off-the-shelf coreference systems has to be filtered to keep only chains that contain at least one mention noun phrase with a syntactic head that is a entity’s name.4 For our evaluation, we use the spaCy dependency parsing system (Honnibal and Johnson, 2015) to detect whether a name is the head of a mention, by checking that no other word in the mention is an ancestor of the name in the dependency parse tree. In evaluation, we use gold NER tags to determine if the head is a name. Note that the dependency parsing and gold NER are not given to the systems but are used to process their output. Many system NEC chains did not have any Evaluation of Systems We make use of the relevant part of OntoNotes coreference corpus (Pradhan et al., 2007) and gold-standard annotations for named entities on the same data to quantify the patterns in coreference of d"
W19-2801,P11-1115,0,0.0821185,"Missing"
W19-2801,W11-1902,0,0.398855,"Missing"
W19-2801,D10-1048,0,0.104489,"Missing"
W19-2801,W09-1119,1,0.669316,"Missing"
W19-2801,D17-1018,0,0.0863558,"Missing"
W19-2801,P11-1138,1,0.814955,"Missing"
W19-2801,N18-2108,0,0.0883171,"Missing"
W19-2801,N13-1071,0,0.0599811,"Missing"
W19-2801,H05-1004,0,0.124694,"by the gold and predicted clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |ki ∩ rj |, and in CEAFe, φ(ki , rj ) = |ki |+|r . j| LEA Recall is computed as the fraction of correctly resolved links between mentions. Results"
W19-2801,P09-1074,0,0.0937083,"Missing"
W19-2801,W02-1111,0,0.170753,"ty Coreference Oshin Agarwal ∗ University of Pennsylvania oagarwal@seas.upenn.edu Sanjay Subramanian ∗ University of Pennsylvania subs@seas.upenn.edu Ani Nenkova University of Pennsylvania nenkova@seas.upenn.edu Dan Roth University of Pennsylvania danroth@seas.upenn.edu Abstract information extraction (Ji and Grishman, 2011), biography summarization (Zhou et al., 2004) and knowledge base completion tasks (West et al., 2014). More relevant information can be extracted for these tasks if we also know which pronouns and nominals refer to the entity. Similarly, creation of proper noun ontologies (Mann, 2002) can use patterns other than (proper noun–common noun) if other references to the entity are known. Recent work (Webster et al., 2018) has shown that standard coreference datasets are biased and high performance on these need not mean high performance in downstream tasks. We argue that the standard coreference metrics are not suitable either from the perspective of downstream applications. Since applications require information about entities and entities are usually identified by their names, the evaluation metrics should focus on the resolution of mentions to the correct name. If all the pro"
W19-2801,M95-1005,0,0.901397,"t then computes recall as the number of similar mentions shared by the gold and predicted clusters divided by the number of mentions in the gold cluster. Precision is equal to the number of similar mentions shared by the gold and predicted, divided by the number of mentions in the predicted cluster. Numbers are reported either per mention (CEAFm), or per entity (CEAFe). P ∗ ki ∈K ∗ φ(ki , g (ki )) Recall = P ki ∈K φ(ki , ki ) Coreference Evaluation Shared tasks on coreference (at CoNLL-2011 and 2012 (Pradhan et al., 2014) ) use the average of three F1 scores as their official evaluation: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFE (Luo, 2005). Prior work (Moosavi and Strube, 2016) discussed shortcoming of these metrics and introduced the improved link entity aware (LEA) score. Below we describe each score in the context of downstream tasks. Let K be the set of key (gold) clusters, and let R be the set of response clusters. where K ∗ is the set of key entities in the optimal one-to-one mapping and φ(·, ·) is a similarity measure for a pair of entities. In CEAFm, φ(ki , rj ) = 2|ki ∩rj | |ki ∩ rj |, and in CEAFe, φ(ki , rj ) = |ki |+|r . j| LEA Recall is computed as the fraction o"
W19-2801,A97-1030,0,0.644032,"Missing"
W19-2801,Q18-1042,0,0.0602823,"vania subs@seas.upenn.edu Ani Nenkova University of Pennsylvania nenkova@seas.upenn.edu Dan Roth University of Pennsylvania danroth@seas.upenn.edu Abstract information extraction (Ji and Grishman, 2011), biography summarization (Zhou et al., 2004) and knowledge base completion tasks (West et al., 2014). More relevant information can be extracted for these tasks if we also know which pronouns and nominals refer to the entity. Similarly, creation of proper noun ontologies (Mann, 2002) can use patterns other than (proper noun–common noun) if other references to the entity are known. Recent work (Webster et al., 2018) has shown that standard coreference datasets are biased and high performance on these need not mean high performance in downstream tasks. We argue that the standard coreference metrics are not suitable either from the perspective of downstream applications. Since applications require information about entities and entities are usually identified by their names, the evaluation metrics should focus on the resolution of mentions to the correct name. If all the pronouns referring to an entity are resolved correctly to each other but are not linked to any name or are linked to a wrong name, the re"
W19-2801,W04-3256,0,0.341417,"Missing"
