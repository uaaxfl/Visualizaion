2008.amta-papers.13,J07-2003,0,0.0414222,"oes not depend heavily on such overlap? Answering these questions will make it possible to characterize the utility of paraphrase-based optimization in real-world scenarios, and and how best to leverage it in those scenarios where it does prove useful. 3 Research Questions Paraphrasing Model We generate sentence-level paraphrases via Englishto-English translation using phrase table pivoting, following (Madnani et al., 2007). The translation system we use (for both paraphrase generation and translation) is based on a state-of-the-art hierarchical phrase-based translation model as described in (Chiang, 2007). English-to-English hierarchical phrases are induced using the pivot-based technique proposed in (Bannard and Callison-Burch, 2005) with primary features similar to those used by (Madnani et al., 2007): the joint probability p(e¯1 , e¯2 ), the two conditionals p(e¯1 |e¯2 ) & p(e¯2 |e¯1 ) and the target length. To limit noise during pivoting, we only keep the top 20 paraphrase pairs resulting from each pivot, as determined by the induced fractional counts. Furthermore, we pre-process the source to identify all named entities using BBN IdentiFinder (Bikel et al., 1999) and strongly bias our dec"
2008.amta-papers.13,C04-1051,0,0.191571,"Missing"
2008.amta-papers.13,W04-3250,0,0.0429334,"he paraphraser only on a subset—1 million sentences—instead of the full set. • We use a 1-3 split of the 4 reference translations from the NIST MT02 test set to tune the feature weights for the paraphraser similar to Madnani et al. (2007). • No changes are made to the number of references in any validation set. Only the tuning sets differed in the number of references across different experiments. • BLEU and TER are calculated on lowercased translation output. Brevity penalties for BLEU are indicated if not equal to 1. • For each experiment, BLEU scores shown in bold are significantly better (Koehn, 2004) than the appropriate baselines for that experiment (p &lt; 0.05). 4.1 Table 1: BLEU and TER scores are shown for MT04+05. 1H=Tuning with 1 human reference, 1H+1P=Tuning with the human reference and its paraphrase. Lower TER scores are better. BLEU TER 1H 37.65 56.39 1H+1P 39.32 54.39 Single Reference Datasets In this section, we attempt to gauge the utility of the paraphrase approach in a realistic scenario where only a single reference translation is available for the tuning set. We use the NIST MT03 data, which has four references per development item, to simulate a tuning set in which only a"
2008.amta-papers.13,A00-2023,0,0.0610845,"Missing"
2008.amta-papers.13,W07-0716,1,0.921324,"a for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using heldout test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases. 1 In ou"
2008.amta-papers.13,P08-1023,0,0.0319501,"Missing"
2008.amta-papers.13,P03-1021,0,0.0175466,"n Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al"
2008.amta-papers.13,P02-1040,0,0.0870838,"ference, Hawaii, 21-25 October 2008] Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking"
2008.amta-papers.13,2006.amta-papers.25,1,0.830776,"2008] Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that inv"
2008.amta-papers.13,strassel-etal-2006-integrated,0,0.0152588,"ns. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using heldout test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases"
2020.acl-main.697,P16-1068,0,0.0234579,"word choices than a writer who uses a more limited vocabulary. Attempts to capture topicality (Beigman Klebanov et al., 2016b) or development 7800 (Beigman Klebanov and Flor, 2013b; Somasundaran et al., 2016) through properties of vocabulary distribution without human annotation of topicality and development exemplify such approaches. 3.2.2 Model Interpretability Recent research has shown that more sophisticated machine learning models might perform better than simple regression-based models when it comes to predictive accuracy (Chen and He, 2013; Cummins et al., 2016; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Dong et al., 2017; Dasgupta et al., 2018; Jin et al., 2018). However, unlike linear regression where stakeholders can understand how much each feature used in the model contributed to the predicted score, many of the more complex models are essentially “black boxes” and do not really lend themselves to post-hoc interpretability (Lipton, 2016). Although interpretability is an active area of research in the machine learning literature (Ribeiro et al., 2016; Koh and Liang, 2017; Doshi-Velez and Kim, 2017), it currently lags behind the research on machine learning methods. For this reason, some"
2020.acl-main.697,E17-4010,0,0.0158167,"dialect may not work for another. This consideration does not appear in Page (1966), yet it is an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review. these true characteristics.” Page seems to do away rather quickly with trying to measure the actual thing – the set of all and only “true characteristics of essays”, or trins. Why is that? He explains: Notwithstanding the"
2020.acl-main.697,W17-5110,0,0.0886908,"Missing"
2020.acl-main.697,Q13-1028,0,0.0292127,"phor use (proportion of metaphorically used words in an essay) correlates with essay quality; Littlemore et al. (2014) likewise found that more skilled writers use metaphor more often. Song et al. (2016) observed a positive correlation between use of parallelism – syntactically similar and semantically related constructors, often used for emphasis or to enhance memorability – in student essays. Some pioneering work has been done on comparing writing that is recognized as outstanding (through receiving prestigious prizes) vs writing that is “merely” good in the domain of scientific journalism (Louis and Nenkova, 2013). Once various indicators of originality can be successfully measured, additional work may be necessary to incorporate these measurements into scoring ecosystems since such indicators may only occur infrequently. One way to achieve this would be to compute a “macro” feature that aggregates multiple such indicators, another would be to direct such essays to a human rater for review. 2.2.2 Gaming Won’t this grading system be easy to con? Can’t the shrewd student just put in the proxies which will get a good grade? (Page, 1966) Certainly, students can and do employ gaming strategies to discover a"
2020.acl-main.697,W19-4401,1,0.835194,"nt testtakers at test time. The educational measurement community has long been studying fairness in automated scoring (Williamson et al., 2012; Ramineni and Williamson, 2013; AERA, 2014) and recent progress made by the NLP community towards enhancing the usual accuracy-based evaluations with some of these psychometric analyses – from computing indicators of potential biases in automatic scores across various demographic sub-groups to computing new metrics that incorporate measurement theory to produce more reliable indicators of system performance – is quite promising (Madnani et al., 2017b; Loukina et al., 2019). 3.4 Pervasiveness of Technology Page’s gedankenexperiment on the potential of automated essay evaluation in a classroom context no doubt appeared audacious in 1966 but nothing back then could have prepared his readers to the pervasiveness of technology we are experiencing today. Today you can very literally carry your AWE system in your pocket; you can even carry several. You can use them (almost) at any time and at any place – not only in classrooms, but at home, at work, and even while texting with a friend. This is perhaps the biggest issue that Page’s vision did not address: the possibil"
2020.acl-main.697,W13-1722,1,0.818042,"Missing"
2020.acl-main.697,W16-0524,1,0.853421,"ent We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascer"
2020.acl-main.697,W17-5052,1,0.919248,"y about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascertain the “correctness&quot; of a respon"
2020.acl-main.697,W17-1605,1,0.884257,"Missing"
2020.acl-main.697,W19-4446,0,0.0293242,"with a new annual ACM conference on Fairness, Accountability, and Transparency having been inaugurated in 201817 and relevant research appearing at many impactful publication venues, such as Science (Caliskan et al., 2017), NIPS (Pleiss et al., 2017; Kim et al., 2018), ICML (Kearns et al., 2018), ACL (Hovy and Spruit, 2016; Sun et al., 2019; Sap et al., 2019), KDD (Speicher et al., 2018), AAAI (Zhang and Bareinboim, 2018), and others (Dwork et al., 2012; Hajian and Domingo-Ferrer, 2013). There is also recent work that examines fairness and ethical considerations when using AI in an education (Mayfield et al., 2019; Gardner et al., 2019). In the context of assessment, fairness considerations dictate that the test reflects the same construct(s) for the entire test taking population, that 17 https://facctconference.org/ scores from the test have the same meaning for all the test taking population, and that a fair test does not offer undue advantages (or disadvantages) to some individuals because of their characteristics – such as those associated with race, ethnicity, gender, age, socioeconomic status, or linguistic or cultural background – or the test characteristics itself, e.g., the different prompts s"
2020.acl-main.697,P11-1076,0,0.0122347,"ndardized testing (§3.2). This is one of the reasons standardized tests are often not scored solely by an AWE system but also by a human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., sci"
2020.acl-main.697,C16-1206,0,0.0117805,"ully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascertain the “co"
2020.acl-main.697,P13-1026,0,0.0240454,"rmation relevant to that particular aspect of essay quality. This would be a mix between what Page called a prox and a trin, in that a particular, intrinsically interesting, aspect of an essay can be identified reliably by humans, and an automated system can learn how to approximate that particular construct. Such approaches have been developed for organization (well-organized) (Burstein et al., 2003), coherence (well-focused, conveys ideas fluently) (Burstein et al., 2010; Somasundaran et al., 2014), grammaticality (facility with conventions) (Heilman et al., 2014), thesis clarity (clarity) (Persing and Ng, 2013) as well as aspects of scoring rubrics that are more task-specific, e.g., argumentation (clear position, with compelling reasons) (Stab and Gurevych, 2014; Ghosh et al., 2016; Beigman Klebanov et al., 2017; Stab and Gurevych, 2017; Carlile et al., 2018), use of evidence in the context of source-based writing (Rahimi et al., 2017). Finally, for some rubric items, it is not clear exactly how to reliably translate the relevant aspect of the writing construct into annotations guidelines, and so proxes might be employed. For example, consider Page’s argument for capturing “diction” (appropriate wor"
2020.acl-main.697,W15-0612,0,0.0253389,"not scored solely by an AWE system but also by a human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included i"
2020.acl-main.697,I17-4001,0,0.0281998,"sessing writing in multiple languages In order to advance the work on understanding and assessing writing quality, there is clearly a need for a multi-lingual perspective, since methods developed for one language or dialect may not work for another. This consideration does not appear in Page (1966), yet it is an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review"
2020.acl-main.697,N18-1012,0,0.0226197,"4 typically go beyond grammar 5 https://mentormywriting.org/ http://www.adaptiveliteracy.com/writing-pal 7 http://www.ets.org/criterion 8 https://www.grammarly.com/ 9 https://writeandimprove.com/ 10 https://www.gingersoftware.com/essay-checker 11 https://www.turnitin.com/products/ revision-assistant 12 http://www.vantagelearning.com/products/ my-access-school-edition/ 13 https://www.pearsonmylabandmastering.com 14 http://wtl.pearsonkt.com 6 and spelling.15 Such tools provide feedback on discourse structure (Criterion), topic development and coherence (Writing Mentor), tone (Writing Assistant, Rao and Tetreault (2018)), thesis relevance (Writing Pal), sentence “spicing” through suggestions of synonyms and idioms (Ginger’s Sentence Rephraser), and style & argumentationrelated feedback (Revision Assistant). Can we then put a green check-mark against Page’s agenda for automated feedback, which “may magnify and disseminate the best human capacities to criticize, evaluate, and correct”? Alas, not yet; research on effectiveness of automated feedback on writing is inconclusive (Englert et al., 2007; Shermis et al., 2008; Grimes and Warschauer, 2010; Choi, 2010; Roscoe and McNamara, 2013; Wilson and Czik, 2016; Wi"
2020.acl-main.697,D15-1050,0,0.0194271,"o the human and which part is due to the machine – perhaps the machine only corrected misspellings, or suggested improvements for the human to vet, or maybe the human only contributed the very first ideation, and the machine has done the rest. Perhaps all the human writer contributed was the thesis (‘I think school should start at 8 rather than 7’) and then clicked ‘submit’ to get back an essay making a cogent and convincing case in support of the thesis. Mining large textual databases for arguments and evaluating them are feasible today as recently demonstrated by IBM’s Debater technology18 (Rinott et al., 2015; Levy et al., 2017; Gretz et al., 2019); introduce some figuration to 18 https://www.research.ibm.com/ artificial-intelligence/project-debater/ make it more appealing (Veale et al., 2017; Veale, 2018) and storify it (Riegl and Veale, 2018; Radford et al., 2019), et voilà! This type of use is essentially a machine’s augmentation of human ability, and is hinted at, for example, in a customer testimonial for Grammarly: “Grammarly allows me to get those communications out and feel confident that I’m putting my best foot forward. Grammarly is like a little superpower, especially when I need to be"
2020.acl-main.697,W19-4411,0,0.0129014,"lem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascertain the “correctness&quot; of a response based on its similarity to other responses that humans have deemed to be correct or,"
2020.acl-main.697,W17-5017,0,0.012585,"matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content scoring systems ascertain the “correctness&quot; of a response based on its similar"
2020.acl-main.697,W10-1004,0,0.0270171,"sts, developers of a scoring engine often need to provide a construct validity argument in order to show that what the system is measuring is actually aligned with the “writing construct” – the actual set of writing skills that the test is supposed to measure. 16 https://www.ets.org/gre/revised_general/ prepare/analytical_writing/issue/scoring_guide Some of the items in a human-oriented scoring rubrics are amenable to reasonably direct implementation, often with the help of human-annotated gold standard data such as misspellings (Flor, 2012; Flor and Futagi, 2013) and specific grammar errors (Rozovskaya and Roth, 2010; Leacock et al., 2014). It might be the case that the system would miss some grammar errors and declare an error where there is none, but a grammar assessment system can be built for identifying specific, observable instances of errors that a human reader focused on Mechanics would likely pick upon. For other items in a rubric, one might need to drill down, articulate a reliable guideline for humans to assess that particular aspect of the essay, annotate a substantial enough number of essays using the guidelines to make machine learning possible, and then find automatically measurable propert"
2020.acl-main.697,N15-1111,1,0.825608,"human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles, trends in a graph, or details from a reading passage) is included in the response. Note that most current content"
2020.acl-main.697,P19-1163,0,0.0129424,"use some variant of least squares linear regression as the machine learning model to predict test taker scores. 3.3 Increased Attention to Fairness It would probably not be an overstatement to say that fairness in AI is quickly becoming its own sub-field, with a new annual ACM conference on Fairness, Accountability, and Transparency having been inaugurated in 201817 and relevant research appearing at many impactful publication venues, such as Science (Caliskan et al., 2017), NIPS (Pleiss et al., 2017; Kim et al., 2018), ICML (Kearns et al., 2018), ACL (Hovy and Spruit, 2016; Sun et al., 2019; Sap et al., 2019), KDD (Speicher et al., 2018), AAAI (Zhang and Bareinboim, 2018), and others (Dwork et al., 2012; Hajian and Domingo-Ferrer, 2013). There is also recent work that examines fairness and ethical considerations when using AI in an education (Mayfield et al., 2019; Gardner et al., 2019). In the context of assessment, fairness considerations dictate that the test reflects the same construct(s) for the entire test taking population, that 17 https://facctconference.org/ scores from the test have the same meaning for all the test taking population, and that a fair test does not offer undue advantages"
2020.acl-main.697,P17-2064,0,0.0220375,"multiple languages In order to advance the work on understanding and assessing writing quality, there is clearly a need for a multi-lingual perspective, since methods developed for one language or dialect may not work for another. This consideration does not appear in Page (1966), yet it is an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review. these true characte"
2020.acl-main.697,C14-1090,0,0.0296825,"es to make machine learning possible, and then find automatically measurable properties of essays that would provide information relevant to that particular aspect of essay quality. This would be a mix between what Page called a prox and a trin, in that a particular, intrinsically interesting, aspect of an essay can be identified reliably by humans, and an automated system can learn how to approximate that particular construct. Such approaches have been developed for organization (well-organized) (Burstein et al., 2003), coherence (well-focused, conveys ideas fluently) (Burstein et al., 2010; Somasundaran et al., 2014), grammaticality (facility with conventions) (Heilman et al., 2014), thesis clarity (clarity) (Persing and Ng, 2013) as well as aspects of scoring rubrics that are more task-specific, e.g., argumentation (clear position, with compelling reasons) (Stab and Gurevych, 2014; Ghosh et al., 2016; Beigman Klebanov et al., 2017; Stab and Gurevych, 2017; Carlile et al., 2018), use of evidence in the context of source-based writing (Rahimi et al., 2017). Finally, for some rubric items, it is not clear exactly how to reliably translate the relevant aspect of the writing construct into annotations guideli"
2020.acl-main.697,Q18-1007,0,0.0177326,"of a response based on its similarity to other responses that humans have deemed to be correct or, at least, high-scoring; they do not employ explicit fact-checking or reasoning for this purpose. Concerns about specific content extends to other cases where the scoring system needs to pay 7797 attention to details of genre and task – not all essays are five-paragraph persuasive essays; the specific task might require assessing whether the student has appropriately used specific source materials (Beigman Klebanov et al., 2014; Rahimi et al., 2017; Zhang and Litman, 2018) or assessing narrative (Somasundaran et al., 2018) or reflective (Beigman Klebanov et al., 2016a; Luo and Litman, 2016), rather than persuasive, writing. 2.2.4 Feedback Page emphasized the importance of feedback, and considered the following to be “the sort of feedback that can almost be programmed right now” (original italics): John [. . . ], please correct the following misspellings: believe, receive. Note the ie/ei problem. You overuse the words interesting, good, nice; then was repeated six times. Check trite expressions. All of your sentences are of the subject-verb variety and all are declarative. Reconstruct. Check subject-verb agreeme"
2020.acl-main.697,C16-1148,0,0.0259396,"for some rubric items, it is not clear exactly how to reliably translate the relevant aspect of the writing construct into annotations guidelines, and so proxes might be employed. For example, consider Page’s argument for capturing “diction” (appropriate word choice) through word frequency – a writer who can use many different words, including rarer and often semantically nuanced ones, is likelier to make precise word choices than a writer who uses a more limited vocabulary. Attempts to capture topicality (Beigman Klebanov et al., 2016b) or development 7800 (Beigman Klebanov and Flor, 2013b; Somasundaran et al., 2016) through properties of vocabulary distribution without human annotation of topicality and development exemplify such approaches. 3.2.2 Model Interpretability Recent research has shown that more sophisticated machine learning models might perform better than simple regression-based models when it comes to predictive accuracy (Chen and He, 2013; Cummins et al., 2016; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Dong et al., 2017; Dasgupta et al., 2018; Jin et al., 2018). However, unlike linear regression where stakeholders can understand how much each feature used in the model contributed t"
2020.acl-main.697,C16-1076,0,0.0995089,"system; finding such aspects, as well as measuring them, is still work in progress. While no current operational scoring system we are aware of is specifically looking for originality, research into aspects of writing that are often considered original is taking place. For example, using data from different tests, Beigman Klebanov and Flor (2013a) and Beigman Klebanov et al. (2018) found that the extent of metaphor use (proportion of metaphorically used words in an essay) correlates with essay quality; Littlemore et al. (2014) likewise found that more skilled writers use metaphor more often. Song et al. (2016) observed a positive correlation between use of parallelism – syntactically similar and semantically related constructors, often used for emphasis or to enhance memorability – in student essays. Some pioneering work has been done on comparing writing that is recognized as outstanding (through receiving prestigious prizes) vs writing that is “merely” good in the domain of scientific journalism (Louis and Nenkova, 2013). Once various indicators of originality can be successfully measured, additional work may be necessary to incorporate these measurements into scoring ecosystems since such indica"
2020.acl-main.697,E17-1092,0,0.0159877,"s, and an automated system can learn how to approximate that particular construct. Such approaches have been developed for organization (well-organized) (Burstein et al., 2003), coherence (well-focused, conveys ideas fluently) (Burstein et al., 2010; Somasundaran et al., 2014), grammaticality (facility with conventions) (Heilman et al., 2014), thesis clarity (clarity) (Persing and Ng, 2013) as well as aspects of scoring rubrics that are more task-specific, e.g., argumentation (clear position, with compelling reasons) (Stab and Gurevych, 2014; Ghosh et al., 2016; Beigman Klebanov et al., 2017; Stab and Gurevych, 2017; Carlile et al., 2018), use of evidence in the context of source-based writing (Rahimi et al., 2017). Finally, for some rubric items, it is not clear exactly how to reliably translate the relevant aspect of the writing construct into annotations guidelines, and so proxes might be employed. For example, consider Page’s argument for capturing “diction” (appropriate word choice) through word frequency – a writer who can use many different words, including rarer and often semantically nuanced ones, is likelier to make precise word choices than a writer who uses a more limited vocabulary. Attempts"
2020.acl-main.697,W17-0306,0,0.0211933,"onsideration does not appear in Page (1966), yet it is an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review. these true characteristics.” Page seems to do away rather quickly with trying to measure the actual thing – the set of all and only “true characteristics of essays”, or trins. Why is that? He explains: Notwithstanding the wonders of the computer, we have to"
2020.acl-main.697,P19-1159,0,0.0232578,"in, 2006) – still use some variant of least squares linear regression as the machine learning model to predict test taker scores. 3.3 Increased Attention to Fairness It would probably not be an overstatement to say that fairness in AI is quickly becoming its own sub-field, with a new annual ACM conference on Fairness, Accountability, and Transparency having been inaugurated in 201817 and relevant research appearing at many impactful publication venues, such as Science (Caliskan et al., 2017), NIPS (Pleiss et al., 2017; Kim et al., 2018), ICML (Kearns et al., 2018), ACL (Hovy and Spruit, 2016; Sun et al., 2019; Sap et al., 2019), KDD (Speicher et al., 2018), AAAI (Zhang and Bareinboim, 2018), and others (Dwork et al., 2012; Hajian and Domingo-Ferrer, 2013). There is also recent work that examines fairness and ethical considerations when using AI in an education (Mayfield et al., 2019; Gardner et al., 2019). In the context of assessment, fairness considerations dictate that the test reflects the same construct(s) for the entire test taking population, that 17 https://facctconference.org/ scores from the test have the same meaning for all the test taking population, and that a fair test does not offe"
2020.acl-main.697,D16-1193,0,0.0327825,"ikelier to make precise word choices than a writer who uses a more limited vocabulary. Attempts to capture topicality (Beigman Klebanov et al., 2016b) or development 7800 (Beigman Klebanov and Flor, 2013b; Somasundaran et al., 2016) through properties of vocabulary distribution without human annotation of topicality and development exemplify such approaches. 3.2.2 Model Interpretability Recent research has shown that more sophisticated machine learning models might perform better than simple regression-based models when it comes to predictive accuracy (Chen and He, 2013; Cummins et al., 2016; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Dong et al., 2017; Dasgupta et al., 2018; Jin et al., 2018). However, unlike linear regression where stakeholders can understand how much each feature used in the model contributed to the predicted score, many of the more complex models are essentially “black boxes” and do not really lend themselves to post-hoc interpretability (Lipton, 2016). Although interpretability is an active area of research in the machine learning literature (Ribeiro et al., 2016; Koh and Liang, 2017; Doshi-Velez and Kim, 2017), it currently lags behind the research on machine learning metho"
2020.acl-main.697,W19-4440,0,0.0135048,"s an active line of subsequent work. While most of the research we cited so far has been on English, various aspects of writing evaluation, e.g., annotation, detection of various types of errors, and building AWE systems, have been researched for a variety of languages: Song et al. (2016), Rao et al. (2017), Shiue et al. (2017) worked with data in Chinese, 15 Writing Pal does not provide specific grammar and spelling feedback. 7798 Lorenzen et al. (2019) in Danish, Berggren et al. (2019) in Norwegian, Amorim and Veloso (2017) in Portuguese, Stymne et al. (2017) in Swedish, Berkling (2018) and Weiss and Meurers (2019) in German, Mezher and Omar (2016) in Arabic, Kakkonen et al. (2005) in Finnish, Loraksa and Peachavanish (2007) in Thai, Lemaire and Dessus (2001) in French, and Ishioka and Kameda (2006) in Japanese. The list is by no means exhaustive; see Flor and Cahill (2020) for a recent review. these true characteristics.” Page seems to do away rather quickly with trying to measure the actual thing – the set of all and only “true characteristics of essays”, or trins. Why is that? He explains: Notwithstanding the wonders of the computer, we have to develop a strategy in order to tell the computer what to"
2020.acl-main.697,N18-3008,1,0.829014,"aphs over and over, varying sentence structure, replacing words with more sophisticated variants, re-using words from the prompt, using general academic words, plagiarizing from other responses or from material found on the Internet, inserting unnecessary shell language – linguistic scaffolding for organizing claims and arguments, and automated generation of essays (Powers et al., 2001; Bejar et al., 2013, 2014; Higgins and Heilman, 2014; Sobel et al., 2014). Such strategies are generally handled by building in filters or flags for aberrant responses (Higgins et al., 2006; Zhang et al., 2016; Yoon et al., 2018; Cahill et al., 2018). However, developers of AWE systems can never anticipate all possible strategies and may have to react quickly as new ones are discovered in use, by developing new AWE methods to identify them. This cat-andmouse game is particularly rampant in the context of standardized testing (§3.2). This is one of the reasons standardized tests are often not scored solely by an AWE system but also by a human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what"
2020.acl-main.697,W18-0549,0,0.0127282,"ent scoring systems ascertain the “correctness&quot; of a response based on its similarity to other responses that humans have deemed to be correct or, at least, high-scoring; they do not employ explicit fact-checking or reasoning for this purpose. Concerns about specific content extends to other cases where the scoring system needs to pay 7797 attention to details of genre and task – not all essays are five-paragraph persuasive essays; the specific task might require assessing whether the student has appropriately used specific source materials (Beigman Klebanov et al., 2014; Rahimi et al., 2017; Zhang and Litman, 2018) or assessing narrative (Somasundaran et al., 2018) or reflective (Beigman Klebanov et al., 2016a; Luo and Litman, 2016), rather than persuasive, writing. 2.2.4 Feedback Page emphasized the importance of feedback, and considered the following to be “the sort of feedback that can almost be programmed right now” (original italics): John [. . . ], please correct the following misspellings: believe, receive. Note the ie/ei problem. You overuse the words interesting, good, nice; then was repeated six times. Check trite expressions. All of your sentences are of the subject-verb variety and all are d"
2020.acl-main.697,W12-2022,0,0.0294318,".2). This is one of the reasons standardized tests are often not scored solely by an AWE system but also by a human rater. 2.2.3 Content We are talking awfully casually about grading subject matter like history. Isn’t this a wholly different sort of problem? Aren’t we supposed to see that what the students are saying makes sense, above and beyond their using commas in the right places? (Page, 1966) Indeed, work has been done over the last decade on automated evaluation of written responses for their content and not their general writing quality (Sukkarieh and Bolge, 2008; Mohler et al., 2011; Ziai et al., 2012; Basu et al., 2013; Madnani et al., 2013; Ramachandran et al., 2015; Burrows et al., 2015; Sakaguchi et al., 2015; Madnani et al., 2016; Padó, 2016; Madnani et al., 2017a; Riordan et al., 2017; Kumar et al., 2017; Horbach et al., 2018; Riordan et al., 2019). Scoring for content focuses primarily on what students know, have learned, or can do in a specific subject area such as Computer Science, Biology, or Music, with the fluency of the response being secondary. For example, some spelling or grammar errors are acceptable as long as the desired specific information (e.g., scientific principles,"
2020.bea-1.2,P11-1067,0,0.0375319,"onfirming previous findings that automated scoring systems are likely to be robust to random noise in the data. At the same time, the choice of evaluation set led to very different estimates of system performance regardless of what data was used to train the system. Related work The effect of noisy labels on machine learning algorithms has been extensively studied in terms of their effect on system training in both general machine learning literature (see, for example, Frénay and Verleysen (2014) for a comprehensive review), NLP (Reidsma and Carletta, 2008; Beigman Klebanov and Beigman, 2009; Schwartz et al., 2011; Plank et al., 2014; Martínez Alonso et al., 2015; Jamison and Gurevych, 2015) and automated scoring (Horbach et al., 2014; Zesch et al., 2015). One key insight that emerged from such work is that the nature of the noise is extremely important for the system performance. Machine learning algorithms are greatly affected by systematic noise but are less sensitive to random noise (Reidsma and Carletta, 2008; Reidsma and op den Akker, 2008). A typical case of random noise is when the labeling is done by multiple annotators which minimizes the individual bias introduced by any single annotator. Fo"
2020.bea-1.2,W18-0501,1,0.826751,"011) and the inter-rater agreement can vary substantially across prompts as well as across applications. For example, in the ASAP-AES data (Shermis, 2014), the agreement varies from Pearson’s r=0.63 to r=0.85 across “essay sets” (writing prompts) . In many automated scoring studies, the data for training and evaluating the system are randomly sampled from the same dataset, which means that the quality of human labels may affect both system training and evaluation. Notably, the effect of label quality on training and evaluation may not be the same. Previous studies (Reidsma and Carletta, 2008; Loukina et al., 2018) suggest that when annotation noise is relatively random, a system trained on noisier annotations may perform as well as a system trained on clean annotations. On the other hand, noise in the human labels used for evaluation 18 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 18–29 c July 10, 2020. 2020 Association for Computational Linguistics and machine-human agreement. We then show that one possible solution is to use proportional reduction in mean squared error (PRMSE) (Haberman, 2008), a metric developed in the educational measuremen"
2020.bea-1.2,D08-1027,0,0.339019,"Missing"
2020.bea-1.2,W17-5052,1,0.847526,"uman labels to adjust 19 performance estimates. We further explore how agreement between human raters affects the evaluation of automated scoring systems. We focus on a specific case where the human rating process is organized in such a way that annotator bias is minimized. In other words, the label noise can be considered random. We also assume that the scores produced by an automated scoring system are on a continuous scale. This is typical for many automated scoring contexts including essay scoring (Shermis, 2014), speech scoring (Zechner et al., 2009) and, to some extent, content scoring (Madnani et al., 2017a; Riordan et al., 2019) but, of course, not for all possible contexts: for example, some of the SemEval 2013 shared tasks on short answer scoring (Dzikovska et al., 2016) use a different scoring approach. 3 each rater, its score for a response was modeled as the gold-standard score for the response plus a random error. We model different groups of raters: with low (inter-rater correlation r=0.4), moderate (r=0.55), average (r=0.65) and high (r=0.8) agreement. The correlations for different categories were informed by correlations we have observed in empirical data from various studies. The er"
2020.bea-1.2,W17-1605,1,0.909232,"Missing"
2020.bea-1.2,W15-0625,0,0.0211173,"Reidsma and Carletta (2008) used simulated data to explore the effect of noisy labels on classifier performance. They showed that the performance of the model, measured using Cohen’s Kappa, when evaluated against the ‘real’ (or gold-standard) labels was higher than the performance when evaluated against the ‘observed’ labels with added random noise. This is because for some instances, the classifier’s predictions were correct, but the ‘observed’ Metrics such as Pearson’s correlation or quadratically-weighted kappa, commonly used to evaluate automated scoring systems (Williamson et al., 2012; Yannakoudakis and Cummins, 2015; Haberman, 2019), compare automated scores to observed human scores without correcting for any errors in human scores. In order to account for differences in human-human agreement, these are then compared to the same metrics computed for the human raters using measures such as “degradation”: the difference between human-human and humanmachine agreement (Williamson et al., 2012). In this paper, we build on findings from the educational measurement community to explore an alternative approach where estimates of system performance are corrected for measurement error in the human labels. Classica"
2020.bea-1.2,N15-1152,0,0.0210417,"ystems are likely to be robust to random noise in the data. At the same time, the choice of evaluation set led to very different estimates of system performance regardless of what data was used to train the system. Related work The effect of noisy labels on machine learning algorithms has been extensively studied in terms of their effect on system training in both general machine learning literature (see, for example, Frénay and Verleysen (2014) for a comprehensive review), NLP (Reidsma and Carletta, 2008; Beigman Klebanov and Beigman, 2009; Schwartz et al., 2011; Plank et al., 2014; Martínez Alonso et al., 2015; Jamison and Gurevych, 2015) and automated scoring (Horbach et al., 2014; Zesch et al., 2015). One key insight that emerged from such work is that the nature of the noise is extremely important for the system performance. Machine learning algorithms are greatly affected by systematic noise but are less sensitive to random noise (Reidsma and Carletta, 2008; Reidsma and op den Akker, 2008). A typical case of random noise is when the labeling is done by multiple annotators which minimizes the individual bias introduced by any single annotator. For example, in a study on crowdsourcing NLP tasks,"
2020.nlposs-1.20,2020.bea-1.2,1,0.736087,"ople’s lives and, therefore, requires a comprehensive evaluation to ensure its accuracy, validity and fairness (Ramineni and Williamson, 2013). The goal of RSMTool is to encourage comprehensive reporting of model performance and to make it easier for stakeholders to compare different models along all necessary dimensions before model deployment. This includes not only standard agreement metrics, but also metrics developed within the educational measurement community and not commonly found in existing Python packages, such as measures of system performance based on test theory (Haberman, 2008; Loukina et al., 2020) as well as measures to evaluate fairness of system scores (Williamson et al., 2012; Madnani et al., 2017; Loukina et al., 2019). In this respect, our approach is similar in spirit to “Model cards” proposed by Mitchell et al. (2019) or standardized data statements advocated by Bender and Friedman (2018). 3.2 Architecture RSMTool combines multiple analyses that are commonly conducted when building and evaluating automated scoring engines in a single package. In a typical use case, a user provides a file or a data frame with numeric system scores, gold-standard (human) scores, and metadata, if a"
2020.nlposs-1.20,W19-4401,1,0.8337,"liamson, 2013). The goal of RSMTool is to encourage comprehensive reporting of model performance and to make it easier for stakeholders to compare different models along all necessary dimensions before model deployment. This includes not only standard agreement metrics, but also metrics developed within the educational measurement community and not commonly found in existing Python packages, such as measures of system performance based on test theory (Haberman, 2008; Loukina et al., 2020) as well as measures to evaluate fairness of system scores (Williamson et al., 2012; Madnani et al., 2017; Loukina et al., 2019). In this respect, our approach is similar in spirit to “Model cards” proposed by Mitchell et al. (2019) or standardized data statements advocated by Bender and Friedman (2018). 3.2 Architecture RSMTool combines multiple analyses that are commonly conducted when building and evaluating automated scoring engines in a single package. In a typical use case, a user provides a file or a data frame with numeric system scores, gold-standard (human) scores, and metadata, if applicable. The tool processes the data and generates an HTML report containing a comprehensive evaluation including descriptive"
2020.nlposs-1.20,C18-1094,1,0.839897,"es the latest advances from these disciplines to provide a comprehensive evaluation of automated scoring systems, including model fairness and testtheory based measures. RSMTool was initially developed as a monolithic command-line tool that accepted input in a single format and generated a static model evaluation report as the only output. Over time, it became clear that this one-size-fits-all approach was not ideal. Operational development of an automated scoring system requires collaboration between many stakeholders including NLP researchers, engineers, psychometricians and business units (Madnani and Cahill, 2018). The interdisciplinary nature of this community led to the emergence of several distinct RSMTool user groups. Each of these groups had separate requirements in terms of entry points, inputs, outputs, and documentation. Only by addressing all of these diverse requirements were we able to achieve wider adoption of RSMTool for model evaluation (Rupp et al., 2019; Yoon and Lee, 2019; Kwong et al., 2020). The lessons we share are the salient ones we have learnt along the way: that different users have different needs and that going the extra mile on robustness – tests, documentation, and packaging"
2020.nlposs-1.20,W17-1605,1,0.806063,"Missing"
2020.nlposs-1.20,W19-4441,0,0.0200963,"its-all approach was not ideal. Operational development of an automated scoring system requires collaboration between many stakeholders including NLP researchers, engineers, psychometricians and business units (Madnani and Cahill, 2018). The interdisciplinary nature of this community led to the emergence of several distinct RSMTool user groups. Each of these groups had separate requirements in terms of entry points, inputs, outputs, and documentation. Only by addressing all of these diverse requirements were we able to achieve wider adoption of RSMTool for model evaluation (Rupp et al., 2019; Yoon and Lee, 2019; Kwong et al., 2020). The lessons we share are the salient ones we have learnt along the way: that different users have different needs and that going the extra mile on robustness – tests, documentation, and packaging – is essential to satisfy these needs. We believe that many of the points we discuss will be applicable to a range of NLP tools and, thus, could benefit the wider NLP OSS community. 3 3.1 RSMTool Motivation A single evaluation metric such as Pearson’s correlation coefficient or Quadratically-weighted Kappa represents only one aspect of system performance. An automated scoring sy"
C18-1094,Q13-1032,0,0.0260656,"ture and/or organization of the response, (iii) Relevance of the response to the question that was asked. 2 Motivation Over the last few years, there has been a significant increase in the number of NLP conference and workshop publications on the task of automated scoring of student responses (Burrows et al., 2015; Zesch et al., 2015; Sultan et al., 2016). Much of this increase in interest stems from the public availability of fairly large datasets containing scored human responses as part of shared tasks and public contests (the ASAP1 and ASAP22 Kaggle shared tasks, the Powergrading dataset (Basu et al., 2013), and the SemEval This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can"
C18-1094,W16-0522,0,0.0138923,"community, our goal is to bring them to the attention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be"
C18-1094,P98-1032,0,0.08571,"s NLP researchers to understand and incorporate these perspectives into our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful. 1 What is Automated Scoring? Automated scoring is an NLP application usually deployed in the educational domain. It involves automatically analyzing a student’s response to a question and generating either (a) a score in order to assess the student’s knowledge and/or other skills and/or (b) some actionable feedback on how the student can improve the response (Page, 1966; Burstein et al., 1998; Burstein et al., 2004; Zechner et al., 2009; Bernstein et al., 2010). It is considered an NLP application since typically the core technology behind the automated analysis of the student response enlists NLP techniques. The student responses can include essays, short answers, or spoken responses and the two most common kinds of automated scoring are the automated evaluation of writing quality and content knowledge. Both the scores and feedback are usually based on linguistic characteristics of the responses including but not limited to: (i) Lower-level errors in the response (e.g., pronuncia"
C18-1094,K17-1017,0,0.0135374,"Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are impacted by and contribute to automated scoring.3 To be clear, the issues we highlight are not simply a result of “operati"
C18-1094,P16-2089,0,0.0303005,"ons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are impacted by and contrib"
C18-1094,W17-5006,0,0.0225998,"ttention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be affected by automated scoring systems, and the"
C18-1094,P16-4014,1,0.785195,"king human scorers how they mentally translate the rubric into specific scoring decisions, humans are not as interpretable as one might think (Lipton, 2016). (d) vs. Teachers. To make sure that automated scoring (or feedback) systems behave as expected if deployed for in-classroom use, NLP researchers would like to conduct research studies with such systems in real classrooms in order to collect useful data, e.g., written or spoken responses, student behavior, and indicators of engagement which can then be used to improve the system further (Burstein et al., 2016; Burstein and Sabatini, 2016; Madnani et al., 2016). However, teachers want to ensure that such systems are sufficiently nuanced — and not too primitive — to handle interactions with students and do not lead to students being distracted instead of learning. Furthermore, it takes time to build up a level of trust between the teachers and NLP researchers as a system is being fine-tuned and developed. It is evident from the above discussion that trying to cater to everyone is akin to solving a difficult constraint satisfaction problem. For example, if NLP researchers want to build a more interpretable automated scoring system that can provide mor"
C18-1094,W17-1605,1,0.891136,"Missing"
C18-1094,W15-0629,0,0.0290828,"owever, since there are limited opportunities for that community to interact with the NLP community, our goal is to bring them to the attention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before"
C18-1094,W17-5001,0,0.0131632,"m NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be affected by automated scoring systems, and their correspon"
C18-1094,N16-3020,0,0.0238297,"ain before deployment. (b) vs. Score Users. As described above, one of the most important considerations for test-takers is a reasonably clear explanation of why they received the particular scores that they did. The NLP 1102 researchers optimizing for agreement with human scores might lean towards using more sophisticated machine learning models such as SVMs with non-linear kernels and deep neural networks. However, such models do not really lend themselves to post-hoc interpretability (Lipton, 2016). Although interpretability is an active area of research in the machine learning literature (Ribeiro et al., 2016; Koh and Liang, 2017; Doshi-Velez and Kim, 2017), it currently lags far behind the research on machine learning methods. Ensuring that there are no biases in automated scores – important for institutions using test scores to make decisions – is a topic that sees little discussion in the NLP literature. This is partly driven by a lack of demographic data available in publicly available datasets, as well as perhaps a focus on empirical accuracy. (c) vs. Subject-matter Experts. Subject-matter experts or assessment developers want to ensure that all the hard work that has been done on their end t"
C18-1094,W17-5017,1,0.852987,"27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are impacted by and contribute to automated scoring.3 To be clear, the issues we highlight are not simply a result of “operationalizing” NLP researc"
C18-1094,W15-0605,0,0.0225044,"tails: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are im"
C18-1094,N16-1123,0,0.0785039,"k are usually based on linguistic characteristics of the responses including but not limited to: (i) Lower-level errors in the response (e.g., pronunciation errors in spoken responses and grammatical/spelling errors in written responses), (ii) The discourse structure and/or organization of the response, (iii) Relevance of the response to the question that was asked. 2 Motivation Over the last few years, there has been a significant increase in the number of NLP conference and workshop publications on the task of automated scoring of student responses (Burrows et al., 2015; Zesch et al., 2015; Sultan et al., 2016). Much of this increase in interest stems from the public availability of fairly large datasets containing scored human responses as part of shared tasks and public contests (the ASAP1 and ASAP22 Kaggle shared tasks, the Powergrading dataset (Basu et al., 2013), and the SemEval This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 S"
C18-1094,D16-1193,0,0.0453309,"ggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are impacted by and contribute to automated scoring.3 To be clear, the issues we highlight are not simply a"
C18-1094,W15-0621,0,0.0201929,"portunities for that community to interact with the NLP community, our goal is to bring them to the attention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position,"
C18-1094,W17-5013,0,0.0213995,"ce as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be affected by automated scoring systems, and their corresponding perspectives on t"
C18-1094,W16-4916,0,0.0144614,"to bring them to the attention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be affected by automated"
C18-1094,W15-0626,0,0.0802712,"e scores and feedback are usually based on linguistic characteristics of the responses including but not limited to: (i) Lower-level errors in the response (e.g., pronunciation errors in spoken responses and grammatical/spelling errors in written responses), (ii) The discourse structure and/or organization of the response, (iii) Relevance of the response to the question that was asked. 2 Motivation Over the last few years, there has been a significant increase in the number of NLP conference and workshop publications on the task of automated scoring of student responses (Burrows et al., 2015; Zesch et al., 2015; Sultan et al., 2016). Much of this increase in interest stems from the public availability of fairly large datasets containing scored human responses as part of shared tasks and public contests (the ASAP1 and ASAP22 Kaggle shared tasks, the Powergrading dataset (Basu et al., 2013), and the SemEval This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguist"
C18-2025,P13-1113,1,0.833458,"feedback. Claims Sources Topic Development Flow of Ideas Transition Terms Long Sentences Headers Use of Anaphora Grammar, Usage, & Mechanic Errors Claim Verbs Word Choice Contractions Convincing Arguing expressions from a lexicon (Burstein et al., 1998) that contains discourse cue terms and relations (e.g., contrast, parallel, summary) and arguing expressions classified by stance (for/against) & type (hedges vs. boosters). Rule-based system that detects in-text formal citations consistent with MLA, APA and Chicago styles. Well-developed Detection of topics and their related word sets (Beigman Klebanov and Flor, 2013; Burstein et al., 2016a) Coherent Leverages terms in a document generated for the main topic (as identified by Topic Development above) and their related word sets. Identified using the same lexicon as in Claims above. Sentences with 1 independent clause & 1+ dependent clauses, identified using a syntactic chunker (Abney, 1996; Burstein and Chodorow, 1999) Rule-based system using regular expressions to identify title & section headers. Pronouns identified using a part-of-speech tagger (Ratnaparkhi, 1996). Well-Edited 9 automatically-detected grammar error feature types, 12 automatically-detec"
C18-2025,W99-0411,1,0.597332,"essions classified by stance (for/against) & type (hedges vs. boosters). Rule-based system that detects in-text formal citations consistent with MLA, APA and Chicago styles. Well-developed Detection of topics and their related word sets (Beigman Klebanov and Flor, 2013; Burstein et al., 2016a) Coherent Leverages terms in a document generated for the main topic (as identified by Topic Development above) and their related word sets. Identified using the same lexicon as in Claims above. Sentences with 1 independent clause & 1+ dependent clauses, identified using a syntactic chunker (Abney, 1996; Burstein and Chodorow, 1999) Rule-based system using regular expressions to identify title & section headers. Pronouns identified using a part-of-speech tagger (Ratnaparkhi, 1996). Well-Edited 9 automatically-detected grammar error feature types, 12 automatically-detected mechanics error feature types, and 10 automatically-detected word usage error feature types (Attali and Burstein, 2006). Verbs denoting claims from the lexicon used in Claims above. Rule-based system that detects words and expressions related to a set of 13 ‘unnecessary’ words and terms, e.g. very, literally, a total of. Identified using a part-of-speec"
C18-2025,P98-1032,1,0.719671,"plete College America, 2012). We describe Writing Mentor, an NLP-based solution to the literacy challenge that is designed to help struggling writers in 2- and 4-year colleges improve their writing at a self-regulated pace. Writing Mentor is a Google Docs add-on1 that provides automated instructional feedback focused on four key writing skills: credibility of claims, topic development, coherence, and editing. Writing mentor builds on a large body of research in the area of automated writing evaluation (AWE) which has so far primarily been used for scoring standardized assessments (Page, 1966; Burstein et al., 1998; Attali and Burstein, 2006; Zechner et al., 2009; Bernstein et al., 2010). Burstein et al. (2017) examined relationships between NLPderived linguistic features extracted from college writing samples and broader success indicators (such as, SAT and ACT composite and subject scores). Their findings suggested that AWE can also be useful for generating automated feedback that can help students with their writing. Writing Mentor has been developed to provide one-stop-shopping for writers looking for help with academic writing. Apps such as Grammarly and LanguageTool, cater to individual users but"
C18-2025,W17-5011,1,0.809701,"Missing"
C18-2025,W96-0213,0,0.470675,"cago styles. Well-developed Detection of topics and their related word sets (Beigman Klebanov and Flor, 2013; Burstein et al., 2016a) Coherent Leverages terms in a document generated for the main topic (as identified by Topic Development above) and their related word sets. Identified using the same lexicon as in Claims above. Sentences with 1 independent clause & 1+ dependent clauses, identified using a syntactic chunker (Abney, 1996; Burstein and Chodorow, 1999) Rule-based system using regular expressions to identify title & section headers. Pronouns identified using a part-of-speech tagger (Ratnaparkhi, 1996). Well-Edited 9 automatically-detected grammar error feature types, 12 automatically-detected mechanics error feature types, and 10 automatically-detected word usage error feature types (Attali and Burstein, 2006). Verbs denoting claims from the lexicon used in Claims above. Rule-based system that detects words and expressions related to a set of 13 ‘unnecessary’ words and terms, e.g. very, literally, a total of. Identified using a part-of-speech tagger (Ratnaparkhi, 1996). Table 1: Inventory of features provided by the NLP backend, grouped by Writing Mentor feedback types. 2 Description Writi"
H05-1098,P05-1033,1,0.428241,", and Analysis David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin Institute for Advanced Computer Studies (UMIACS) University of Maryland, College Park, MD 20742, USA {dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu Abstract Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of non"
H05-1098,W02-1039,0,0.35507,"evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a"
H05-1098,P02-1050,1,0.909542,"s. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG,"
H05-1098,P03-1040,0,0.00569351,"ase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any"
H05-1098,N03-1017,0,0.134136,"aper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based mo"
H05-1098,koen-2004-pharaoh,0,0.0221738,"e-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any commitment to a linguistically relevant analysis, and it does not require syntactically annotated training data. Chiang (2005) reported significant performance improvements in Chinese-English translation as compared with Pharaoh, a state-of-the-art phrase-based system (Koehn, 2004). In Section 2, we review the essential elements of Hiero. In Section 3 we describe extensions to this system, including new features involving named entities and numbers and support for a fourfold scale-up in training set size. Section 4 presents new evaluation results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochasti"
H05-1098,P04-1077,0,0.0151475,"on the FBIS data; Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded"
H05-1098,H05-2007,1,0.821428,"Missing"
H05-1098,W02-1018,0,0.0259841,"ew hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident w"
H05-1098,N03-2021,0,0.177666,"Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded again using an HMM wh"
H05-1098,P00-1056,0,0.0589312,"igure 1: Example synchronous CFG • the lexical weights Pw (γ |α) and Pw (α |γ) (Koehn et al., 2003);1 2.1 Grammar • a phrase penalty exp(1); A synchronous CFG or syntax-directed transduction grammar (Lewis and Stearns, 1968) consists of pairs of CFG rules with aligned nonterminal symbols. We denote this alignment by coindexation with boxed numbers (Figure 1). A derivation starts with a pair of aligned start symbols, and proceeds by rewriting pairs of aligned nonterminal symbols using the paired rules (Figure 2). Training begins with phrase pairs, obtained as by Och, Koehn, and others: GIZA++ (Och and Ney, 2000) is used to obtain one-to-many word alignments in both directions, which are combined into a single set of refined alignments using the “final-and” method of Koehn et al. (2003); then those pairs of substrings that are exclusively aligned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not alread"
H05-1098,P02-1038,0,0.00566047,"onous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w(X → hγ, αi) = φi (X → hγ, αi)λi S → hX 1 , X 1 i X → hyu X 1 you X 2 , have X 2 with X 1 i X → hX 1 de X 2 , the X 2 that X 1 i X → hX 1 zhiyi, one of X 1 i X → hAozhou, Australiai X → hshi, isi i X → hshaoshu guojia, few countriesi where the φi are features defined on rules. The basic model uses the following features, analogous to Pharaoh’s default featu"
H05-1098,J04-4002,0,0.0105751,"luations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absenc"
H05-1098,P03-1021,0,0.00789852,"verage. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up to a much larger training set. 3.1 New features supplementary ph"
H05-1098,P02-1040,0,0.109612,"nment, Koehn et al. take the maximum lexical weight; Hiero uses a weighted average. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up"
H05-1098,W96-0213,0,0.00943678,"t work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of every possible tag sequence ti . . . t j in the reference corpus. Third, we computed the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al., 2003). This algorithm provides a list of runs or contiguous sequences of words ei . . . e j in the reference that are also present in the hypothesis. (Note that runs"
H05-1098,2003.mtsummit-papers.53,0,0.0144904,"ed Chinese number/date translator, and created a new model feature for it. Again, the weight given to this module was optimized during minimum-error-rate training. In some cases we wrote the rules to provide multiple uniformlyweighted English translations for a Chinese phrase (for example, kå (bari) could become “the 8th” or “on the 8th”), allowing the language model to decide between the options. 3.2 The LDC Chinese-English named entity lists (900k entries) are a potentially valuable resource, but previous experiments have suggested that simply adding them to the training data does not help (Vogel et al., 2003). Instead, we placed them in a supplementary phrase-translation table, giving greater weight to phrases that occurred less frequently in the primary training data. For each entry h f, {e1 , . . . , en }i, we counted the number of times c( f ) that f appeared in the primary training data, and assigned the entry the weight c( f1)+1 , which was then distributed evenly among the supplementary phrase pairs {h f, ei i}. We then created a new model feature for named entities. When one of these 781 Scaling up training Chiang (2005) reports on experiments in ChineseEnglish translation using a model tra"
H05-1098,P96-1021,0,0.157884,"tion results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochastic synchronous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w"
H05-1098,J97-3002,0,0.094693,"ned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not already used. Various filters are also applied to reduce the number of extracted rules. Since one of these filters restricts the number of nonterminal symbols to two, our extracted grammar is equivalent to an inversion transduction grammar (Wu, 1997). 780 • a word penalty exp(l), where l is the number of terminals in α. The exceptions to the above are the two “glue” rules, which are the rules with left-hand side S in Figure 1. The second has weight one, and the first has weight w(S → hS 1 X 2 , S 1 X 2 i) = exp(−λg ), the idea being that parameter λg controls the model’s preference for hierarchical phrases over serial combination of phrases. Phrase translation probabilities are estimated by relative-frequency estimation. Since the extraction process does not generate a unique derivation for each training sentence pair, a distribution over"
H05-1098,P02-1039,0,0.0166923,"pect to syntax, although there remains a high degree of flexibility (Fox, 2002; Hwa et al., 2002). This suggests that in a comparative analysis of two MT systems, it may be useful to look for syntactic patterns that one system captures well in the target language and the other does not, using a syntax based metric. We propose to summarize reordering patterns using part-of-speech sequences. Unfortunately, recent work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of ever"
H05-1098,N04-1021,0,\N,Missing
J10-3003,P06-1002,1,0.223176,"rrespondences from the shown sentence pairs. (i1 , j1 ) × (i2 , j2 ) denotes the correspondence  fi1 . . . fj1 , ei2 . . . ej2 . Not all extracted correspondences are shown. paraphrases obtained via manually constructed word alignments is signiﬁcantly better than that of the paraphrases obtained from automatic alignments. It has been widely reported that the existing bilingual word alignment techniques are not ideal for use in translation and, furthermore, improving these techniques does not always lead to an improvement in translation performance. (Callison-Burch, Talbot, and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). More details on the relationship between word alignment and SMT can be found in the comprehensive SMT survey recently published by Lopez (2008) (particularly Section 4.2). Paraphrasing done via bilingual corpora relies on the word alignments in the same way as a translation system would and, therefore, would be equally susceptible to the shortcomings of the word alignment techniques. To determine how noisy automatic word alignments affect paraphrasing done via bilingual corpora, we inspected a sample of paraphrase pairs that were extracted when"
J10-3003,P00-1059,0,0.00822974,"ujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases"
J10-3003,P05-1074,0,0.455238,"th such a corpus exploits both its parallel and bilingual natures: Align phrases across the two languages and consider all co-aligned phrases in the intended language to be paraphrases. The bilingual phrasal alignments can simply be generated by using the automatic techniques developed for the same task in the SMT literature. Therefore, arguably the most important factor affecting the performance of these techniques is usually the quality of the automatic bilingual phrasal (or word) alignment techniques. One of the most popular methods leveraging bilingual parallel corpora is that proposed by Bannard and Callison-Burch (2005). This technique operates exactly as described above by attempting to infer semantic equivalence between phrases in the same language indirectly with the second language as a bridge. Their approach builds on one of the initial steps used to train a phrase-based statistical machine translation system (Koehn, Och, and Marcu 2003). Such systems rely on phrase tables—a tabulation of correspondences between phrases in the source language and phrases in the target language. These tables are usually extracted by inducing word alignments between sentence pairs in a training corpus and then incremental"
J10-3003,W02-1022,0,0.00836419,"les or empty slots. The clustering is done so as to bring together sentences pertaining to the same topics and having similar structure. The word lattice is the product of an algorithm that computes a multiple-sequence alignment (MSA) for a cluster of sentences (Step 6). A very brief outline of such an algorithm, originally developed to compute an alignment for a set of three or more protein or DNA sequences, is as follows:9 1. Find the most similar pair of sentences in the cluster according to a similarity scoring function. For this approach, a simpliﬁed version of the edit-distance measure (Barzilay and Lee 2002) is used. 2. Align this sentence pair and replace the pair with this single alignment. 3. Repeat until all sentences have been aligned together. The word lattice so generated now needs to be converted into a slotted lattice to allow its use as a paraphrase template. Slotting is performed based on the following intuition: Areas of high variability between backbone nodes, that is, several distinct parallel paths, may correspond to template arguments and can be collapsed into one slot that can be ﬁlled by these arguments. However, multiple parallel paths may also appear in the lattice because of"
J10-3003,N03-1003,0,0.520334,"Missing"
J10-3003,P01-1008,0,0.675717,"ences from a set of sentences that represent the same (or similar) semantic content. We present four techniques in this section that generate paraphrases by ﬁnding such correspondences. The ﬁrst two techniques attempt to do so by relying, again, on the paradigm of distributional similarity: one by positing a bootstrapping distributional similarity algorithm and the other by simply adapting the previously described dependency path similarity algorithm to work with a parallel corpus. The next two techniques rely on more direct, non-distributional methods to compute the required correspondences. Barzilay and McKeown (2001) align phrasal correspondences by attempting to move beyond a single-pass distributional similarity method. They propose a bootstrapping algorithm that allows for the gradual reﬁnement of the features used to determine similarity and yields improved paraphrase pairs. As their input corpus, they use multiple human-written English translations of literary texts such as Madame Bovary and Twenty Thousand Leagues Under the Sea that are expected to be rich in paraphrastic expressions because different translators would use their own words while still preserving the meaning of the original text. The"
J10-3003,J05-3002,0,0.0450185,"el or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extraction. Text-to-text generation applications rely heavily on paraphrase recognition. For a multi-document summarization system, detecting redundancy is a very important concern because two sentences from different documents may convey the same semantic content and it is important not to repeat the same information in the summary. On this note, Barzilay and McKeown (2005) exploit the redundancy present in a given set of sentences by detecting paraphrastic parts and fusing them into a single coherent sentence. Recognizing similar semantic content is also critical for text simpliﬁcation systems (Marsi and Krahmer 2005b). Information extraction enables the detection of regularities of information structure—events which are reported many times, about different individuals and in different forms—and making them explicit so that they can be processed and used in other ways. Sekine (2006) shows how to use paraphrase recognition to cluster together extraction patterns"
J10-3003,P08-1077,0,0.534592,"query (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query how to live with cat allergies, they may generate the following two paraphrases. The novel words in the two paraphrases are highlighted in bold and are used to expand the original query: P1 : ways to live with feline allergy P2 : how to deal with cat allergens Finally, paraphrases have also been used to improve the task of relation extraction (Romano et al. 2006). Most recently, Bhagat and Ravichandran (2008) collect paraphrastic patterns for relation extraction by applying semi-supervised paraphrase induction to a very large monolingual corpus. For example, for the relation of “acquisition,” they collect: Original : X agreed to buy Y Variant 1 : X completed its acquisition of Y Variant 2 : X purchased Y 1.3.2 Expanding Sparse Human Reference Data for Evaluation. A large percentage of NLP applications are evaluated by having human annotators or subjects carry out the same 344 Madnani and Dorr Generating Phrasal and Sentential Paraphrases task for a given set of data and using the output so created"
J10-3003,I05-5001,0,0.00624394,"to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generati"
J10-3003,J93-2003,0,0.0580672,"Missing"
J10-3003,W06-2920,0,0.00560944,"multiple annual community-wide evaluations using Figure 9 An example of syntactically motivated paraphrastic patterns that can be extracted from the paraphrase corpus constructed by Cohn, Callison-Burch, and Lapata (2008). 377 Computational Linguistics Volume 36, Number 3 standard test sets and manual as well as automated metrics, the task of automated paraphrasing does not. An obvious reason for this disparity could be that paraphrasing is not an application in and of itself. However, the existence of similar evaluations for other tasks that are not applications, such as dependency parsing (Buchholz and Marsi 2006; Nivre et al. 2007) and word sense disambiguation (Senseval), suggests otherwise. We believe that the primary reason is that, over the years, paraphrasing has been employed in an extremely fragmented fashion. Paraphrase extraction and generation are used in different forms and with different names in the context of different applications (for example: synonymous collocation extraction, query expansion). This usage pattern does not allow researchers in one community to share the lessons learned with those from other communities. In fact, it may even lead to research being duplicated across com"
J10-3003,D08-1021,0,0.594373,"c syntactic types that occur very rarely can be ignored and a less noisy paraphrase probability estimate can be computed, which may prove more useful in a downstream application than its counterpart computed via the unconstrained approach. We must also note that requiring syntactic constraints for pivot-based paraphrase extraction restricts the approach to those languages where a reasonably good parser is available. An obvious extension of the Callison-Burch style approach is to use the collection of pivoted English-to-English phrase pairs to generate sentential paraphrases for new sentences. Madnani et al. (2008a) combine the pivot-based approach to paraphrase acquisition with a well-deﬁned English-to-English translation model that is then used in an (unmodiﬁed) SMT system, yielding sentential paraphrases by means of “translating” input English sentences. However, instead of fully lexicalized phrasal correspondences as in (Bannard and Callison-Burch 2005), the fundamental units of translation (and paraphrasing) are hierarchical phrase pairs. The latter can be extracted from the former by replacing aligned sub-phrases with non-terminal symbols. For example, given the , growth rate has been effectively"
J10-3003,C08-1013,0,0.038034,"Missing"
J10-3003,N06-1003,0,0.42874,"Missing"
J10-3003,P04-1023,0,0.00930114,"Missing"
J10-3003,J07-2003,0,0.0572709,"Missing"
J10-3003,J08-4005,0,0.455743,"Missing"
J10-3003,D07-1008,0,0.00872769,"task is formulated differently. Overall, such a paraphrase corpus with detailed paraphrase annotations is much more informative than a corpus containing binary judgments at the sentence level such as the MSRP corpus. As an example, because the corpus contains paraphrase annotations at the word as well as phrasal levels, it can be used to build systems that can learn from these annotations and generate not only fully lexicalized phrasal paraphrases but also syntactically motivated paraphrastic patterns. To demonstrate the viability of the corpus for this purpose, a grammar induction algorithm (Cohn and Lapata 2007) is applied—originally developed for sentence compression—to the parsed version of their paraphrase corpus and the authors show that they can learn paraphrastic patterns such as those shown in Figure 9. In general, building paraphrase corpora, whether it is done at the sentence level or at the sub-sentential level, is extremely useful for the fostering of further research and development in the area of paraphrase generation. 5. Evaluation of Paraphrase Generation Whereas other language processing tasks such as machine translation and document summarization usually have multiple annual communit"
J10-3003,W05-1203,0,0.0195888,"nd cost implications of such a process, most such data sets usually have only a single reference translation. Madnani et al. (2007, 2008b) generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3"
J10-3003,P09-1053,0,0.0546388,"stem can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extraction. Text-to-text generation applications rely heavily on paraph"
J10-3003,W09-3102,0,0.0857492,"Missing"
J10-3003,C04-1051,0,0.912229,"Missing"
J10-3003,I05-5002,0,0.0129453,"ate paraphrase generation methods, it is important to examine some recent work that has been done on constructing paraphrase corpora. As part of this work, human subjects are generally asked to judge whether two given sentences are paraphrases of each other. We believe that a detailed examination of this manual evaluation task provides an illuminating insight into the nature of a paraphrase in a practical, rather than a theoretical, context. In addition, it has obvious implications for any method, whether manual or automatic, that is used to evaluate the performance of a paraphrase generator. Dolan and Brockett (2005) were the ﬁrst to attempt to build a paraphrase corpus on a large scale. The Microsoft Research Paraphrase (MSRP) Corpus is a collection of 5, 801 sentence pairs, each manually labeled with a binary judgment as to whether it constitutes a paraphrase or not. As a ﬁrst step, the corpus was created using a heuristic extraction method in conjunction with an SVM-based classiﬁer that was trained to select likely sentential paraphrases from a large monolingual corpus containing news article clusters. However, the more interesting aspects of the task were the subsequent evaluation of these extracted s"
J10-3003,P99-1011,0,0.0380591,"on of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More speciﬁcally, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murak"
J10-3003,J02-2001,0,0.0210905,"hata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note that we only discuss those paraphrasing techniques that can generate the types of paraphrases under examination in this survey: phrasal and sententi"
J10-3003,W07-1007,0,0.0191401,"adyto-use building blocks and by necessitating development of methods to effectively use such components for the unintended task of paraphrase generation. Domain-Speciﬁc Paraphrasing. Recently, work has been done to generate phrasal paraphrases in specialized domains. For example, in the ﬁeld of health literacy, it is well known that documents for health consumers are not very well-targeted to their purported audience. Recent research has shown how to generate a lexicon of semantically equivalent phrasal (and lexical) pairs of technical and lay medical terms from monolingual parallel corpora (Elhadad and Sutaria 2007) as well as monolingual comparable corpora (Del´eger and Zweigenbaum 2009). Examples include pairs such as myocardial infarction, heart attack and leucospermia, increased white cells in the sperm. In another domain, Max (2008) proposes an adaptation of the pivot-based method to generate rephrasings of short text spans that could help a writer revise a text. Because the goal is to assist a writer in making revisions, the rephrasings do not always need to bear a perfect paraphrastic relationship to the original, a scenario suited for the pivot-based method. Several variants of such adaptatio"
J10-3003,J07-3002,0,0.00594634,". (i1 , j1 ) × (i2 , j2 ) denotes the correspondence  fi1 . . . fj1 , ei2 . . . ej2 . Not all extracted correspondences are shown. paraphrases obtained via manually constructed word alignments is signiﬁcantly better than that of the paraphrases obtained from automatic alignments. It has been widely reported that the existing bilingual word alignment techniques are not ideal for use in translation and, furthermore, improving these techniques does not always lead to an improvement in translation performance. (Callison-Burch, Talbot, and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). More details on the relationship between word alignment and SMT can be found in the comprehensive SMT survey recently published by Lopez (2008) (particularly Section 4.2). Paraphrasing done via bilingual corpora relies on the word alignments in the same way as a translation system would and, therefore, would be equally susceptible to the shortcomings of the word alignment techniques. To determine how noisy automatic word alignments affect paraphrasing done via bilingual corpora, we inspected a sample of paraphrase pairs that were extracted when using Arabic—a language signiﬁcantly different"
J10-3003,W04-0402,0,0.129018,"ur discussion. In this survey, we will be restricting our discussion to only automatic acquisition of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More speciﬁcally, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applicatio"
J10-3003,I05-5004,0,0.0371368,"Missing"
J10-3003,W07-1425,0,0.0336995,"e restricting our discussion to only automatic acquisition of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More speciﬁcally, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Ramb"
J10-3003,C08-1029,0,0.0607718,"Computational Linguistics Volume 36, Number 3 the time and cost implications of such a process, most such data sets usually have only a single reference translation. Madnani et al. (2007, 2008b) generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monol"
J10-3003,I08-1070,0,0.0671576,"Computational Linguistics Volume 36, Number 3 the time and cost implications of such a process, most such data sets usually have only a single reference translation. Madnani et al. (2007, 2008b) generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monol"
J10-3003,P91-1023,0,0.264302,"e-pass distributional similarity method. They propose a bootstrapping algorithm that allows for the gradual reﬁnement of the features used to determine similarity and yields improved paraphrase pairs. As their input corpus, they use multiple human-written English translations of literary texts such as Madame Bovary and Twenty Thousand Leagues Under the Sea that are expected to be rich in paraphrastic expressions because different translators would use their own words while still preserving the meaning of the original text. The parallel components are obtained by performing sentence alignment (Gale and Church 1991) on the corpora to obtain sets of parallel sentences that are then lemmatized, part-of-speech tagged and chunked in order to identify all the verb and noun phrases. The bootstrapping algorithm is then employed to incrementally learn better and better contextual features that are then leveraged to generate semantically similar phrasal correspondences. Figure 4 shows the basic steps of the algorithm. To seed the algorithm, some fake paraphrase examples are extracted by using identical words from either side of the aligned sentence pair. For example, given the following sentence pair: S1 : Emma b"
J10-3003,W04-0910,0,0.0447354,"Missing"
J10-3003,I05-5005,0,0.00974313,"g 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note that we only discuss those"
J10-3003,P91-1026,0,0.147964,"r the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui a"
J10-3003,W01-0814,0,0.0234691,"e 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note tha"
J10-3003,P99-1044,0,0.0558834,"ssed in more detail in Section 3.1. 343 Computational Linguistics Volume 36, Number 3 In fact, in recent years, the information retrieval community has extensively explored the task of query expansion by applying paraphrasing techniques to generate similar or related queries (Beeferman and Berger 2000; Jones et al. 2006; Sahami and Hellman 2006; Metzler, Dumais, and Meek 2007; Shi and Yang 2007). The generation of paraphrases in these techniques is usually effected by utilizing the query log (a log containing the record of all queries submitted to the system) to determine semantic similarity. Jacquemin (1999) generates morphological, syntactic, and semantic variants for phrases in the agricultural domain. For example: Original : simultaneous measurements Variant : concurrent measures Original : development area Variant : area of growth Ravichandran and Hovy (2002) use semi-supervised learning to induce several paraphrastic patterns for each question type and use them in an open-domain question answering system. For example, for the INVENTOR question type, they generate: Original : X was invented by Y Variant 1 : Y’s invention of X Variant 2 : Y, inventor of X Riezler et al. (2007) expand a query b"
J10-3003,N06-1058,0,0.0120224,"acquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the ma"
J10-3003,N03-1017,0,0.0359916,"Missing"
J10-3003,N10-1017,0,0.258199,"oth treat paraphrasing as monolingual translation. However, as outlined in the discussion of that work, Quirk, Brockett, and Dolan use a relatively simplistic translation model and decoder which leads to paraphrases with little or no lexical variety. In contrast, Madnani et al. use a more complex translation model and an unmodiﬁed state-of-the-art SMT decoder to produce paraphrases that are much more diverse. Of course, the reliance of the latter approach on automatic word alignments does inevitably lead to much noisier sentential paraphrases than those produced by Quirk, Brockett, and Dolan. Kok and Brockett (2010) present a novel take on generating phrasal paraphrases with bilingual corpora. As with most approaches based on parallel corpora, they also start with phrase tables extracted from such corpora along with the corresponding phrasal translation probabilities. However, instead of performing the usual pivoting step with the bilingual phrases in the table, they take a graphical approach and represent each phrase in the table as a node, leading to a bipartite graph. Two nodes in the graph are connected to each other if they are aligned to each other. In order to extract paraphrases, they sample rand"
J10-3003,P98-2127,0,0.156252,"in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do"
J10-3003,2006.amta-papers.11,0,0.0050325,"he shown sentence pairs. (i1 , j1 ) × (i2 , j2 ) denotes the correspondence  fi1 . . . fj1 , ei2 . . . ej2 . Not all extracted correspondences are shown. paraphrases obtained via manually constructed word alignments is signiﬁcantly better than that of the paraphrases obtained from automatic alignments. It has been widely reported that the existing bilingual word alignment techniques are not ideal for use in translation and, furthermore, improving these techniques does not always lead to an improvement in translation performance. (Callison-Burch, Talbot, and Osborne 2004; Ayan and Dorr 2006; Lopez and Resnik 2006; Fraser and Marcu 2007). More details on the relationship between word alignment and SMT can be found in the comprehensive SMT survey recently published by Lopez (2008) (particularly Section 4.2). Paraphrasing done via bilingual corpora relies on the word alignments in the same way as a translation system would and, therefore, would be equally susceptible to the shortcomings of the word alignment techniques. To determine how noisy automatic word alignments affect paraphrasing done via bilingual corpora, we inspected a sample of paraphrase pairs that were extracted when using Arabic—a language"
J10-3003,W07-0716,1,0.805066,"iven source sentence can often be translated into the target language in many valid ways. Because there can be many “correct answers,” almost all models employed by SMT systems require, in addition to a large bitext, a held-out development set comprising multiple high-quality, human-authored reference translations in the target language in order to tune their parameters relative to a translation quality metric. However, given 345 Computational Linguistics Volume 36, Number 3 the time and cost implications of such a process, most such data sets usually have only a single reference translation. Madnani et al. (2007, 2008b) generate sentential paraphrases and use them to expand the available reference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sent"
J10-3003,2008.amta-papers.13,1,0.729331,"c syntactic types that occur very rarely can be ignored and a less noisy paraphrase probability estimate can be computed, which may prove more useful in a downstream application than its counterpart computed via the unconstrained approach. We must also note that requiring syntactic constraints for pivot-based paraphrase extraction restricts the approach to those languages where a reasonably good parser is available. An obvious extension of the Callison-Burch style approach is to use the collection of pivoted English-to-English phrase pairs to generate sentential paraphrases for new sentences. Madnani et al. (2008a) combine the pivot-based approach to paraphrase acquisition with a well-deﬁned English-to-English translation model that is then used in an (unmodiﬁed) SMT system, yielding sentential paraphrases by means of “translating” input English sentences. However, instead of fully lexicalized phrasal correspondences as in (Bannard and Callison-Burch 2005), the fundamental units of translation (and paraphrasing) are hierarchical phrase pairs. The latter can be extracted from the former by replacing aligned sub-phrases with non-terminal symbols. For example, given the , growth rate has been effectively"
J10-3003,P09-3004,0,0.0392249,"ter set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extraction. Text-to-text generation applications rely heavily on paraphrase recognition. Fo"
J10-3003,W05-1201,0,0.00805159,"eference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extra"
J10-3003,W05-1612,0,0.00688867,"eference translations for such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extra"
J10-3003,P79-1016,0,0.290719,"atic acquisition of phrasal paraphrases (including paraphrastic patterns) and on generation of sentential paraphrases. More speciﬁcally, this entails the exclusion of certain categories of paraphrasing work. However, as a compromise for the interested reader, we do include a relatively comprehensive list of references in this section for the work that is excluded from the survey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin"
J10-3003,P97-1063,0,0.0169929,"Missing"
J10-3003,C04-1116,0,0.0150918,"1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the"
J10-3003,W06-3112,0,0.012125,"Missing"
J10-3003,N03-1024,0,0.086167,"Missing"
J10-3003,N07-1071,0,0.00738155,"on the root of the extracted path. For example, whereas verbs frequently tend to have several modiﬁers, nouns tend to have no more than one. However, if a word has any fewer than two modiﬁers, no path can go through it as the root. Therefore, the algorithm tends to perform better for paths with verbal roots. Another issue is that this algorithm, despite the use of more informative distributional features, can generate several incorrect or implausible paraphrase patterns (inference rules). Recent work has shown how to ﬁlter out incorrect inferences when using them in a downstream application (Pantel et al. 2007). Finally, there is no reason for the distributional features to be in the same language as the one in which the paraphrases are desired. Wu and Zhou (2003) describe a 5 A demo of the algorithm is available online at http://demo.patrickpantel.com/Content/LexSem/ paraphrase.htm. 352 Madnani and Dorr Generating Phrasal and Sentential Paraphrases Algorithm 2 (Lin and Pantel 2001). Produce inference rules from a parsed corpus. Summary. Adapt Harris’s (1954) hypothesis of distributional similarity for paths in dependency trees: If two tree paths have similar distributions such that they tend to lin"
J10-3003,P02-1040,0,0.0947883,"ubjects carry out the same 344 Madnani and Dorr Generating Phrasal and Sentential Paraphrases task for a given set of data and using the output so created as a reference against which to measure the performance of the system. The two applications where comparison against human-authored reference output has become the norm are machine translation and document summarization. In machine translation evaluation, the translation hypotheses output by a machine translation system are evaluated against reference translations created by human translators by measuring the n-gram overlap between the two (Papineni et al. 2002). However, it is impossible for a single reference translation to capture all possible verbalizations that can convey the same semantic content. This may unfairly penalize translation hypotheses that have the same meaning but use n-grams that are not present in the reference. For example, the given system output S will not have a high score against the reference R even though it conveys precisely the same semantic content: S: We must consider the entire community. R: We must bear in mind the community as a whole. One solution is to use multiple reference translations, which is expensive. An al"
J10-3003,I05-1011,0,0.656018,"Missing"
J10-3003,I05-5010,0,0.026154,"and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 2002). 1.3 Applications of Paraphrase Generation Before describing the techniques used for paraphrasing, it is essential to examine the broader context of the application of paraphrases. For some of the applications we discuss subsequently, the use of paraphrases in the manner described may not yet be the norm. However, wherever applicable, we cite recent research that promises gains in performance by using paraphrases for these applications. Also note that we only discuss those paraphrasing techniques"
J10-3003,W04-3219,0,0.504647,"Missing"
J10-3003,W95-0107,0,0.0135345,"en computing the alignment similarity score, two lexically matched words across a sentence pair are not considered to fully match unless their score on syntactic features also exceeds a preset threshold. The syntactic features constituting the additional constraints are deﬁned in terms of the output of a chunk parser. Such a parser takes as input the syntactic trees of the sentences in a topic cluster and provides the following information for each word: r r r Part-of-speech tag IOB tag. This is a notation denoting the constituent covering a word and its relative position in that constituent (Ramshaw and Marcus 1995). If a word has the tag I-NP, we can infer that the word is covered by an NP and located inside that NP. Similarly, B denotes that the word is at the beginning and O denotes that the word is not covered by any constituent. IOB chain. A concatenation of all IOB tags going from the root of the tree to the word under consideration. With this information and a heuristic to compute the similarity between two words in terms of their POS and IOB tags, the alignment similarity score can be calculated as the sum of the heuristic similarity value for the given two words and the heuristic similarity valu"
J10-3003,P02-1006,0,0.0166785,"similar or related queries (Beeferman and Berger 2000; Jones et al. 2006; Sahami and Hellman 2006; Metzler, Dumais, and Meek 2007; Shi and Yang 2007). The generation of paraphrases in these techniques is usually effected by utilizing the query log (a log containing the record of all queries submitted to the system) to determine semantic similarity. Jacquemin (1999) generates morphological, syntactic, and semantic variants for phrases in the agricultural domain. For example: Original : simultaneous measurements Variant : concurrent measures Original : development area Variant : area of growth Ravichandran and Hovy (2002) use semi-supervised learning to induce several paraphrastic patterns for each question type and use them in an open-domain question answering system. For example, for the INVENTOR question type, they generate: Original : X was invented by Y Variant 1 : Y’s invention of X Variant 2 : Y, inventor of X Riezler et al. (2007) expand a query by generating n-best paraphrases for the query (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query ho"
J10-3003,P05-1077,0,0.060094,"Missing"
J10-3003,P07-1059,0,0.0988993,"semantic similarity. Jacquemin (1999) generates morphological, syntactic, and semantic variants for phrases in the agricultural domain. For example: Original : simultaneous measurements Variant : concurrent measures Original : development area Variant : area of growth Ravichandran and Hovy (2002) use semi-supervised learning to induce several paraphrastic patterns for each question type and use them in an open-domain question answering system. For example, for the INVENTOR question type, they generate: Original : X was invented by Y Variant 1 : Y’s invention of X Variant 2 : Y, inventor of X Riezler et al. (2007) expand a query by generating n-best paraphrases for the query (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query how to live with cat allergies, they may generate the following two paraphrases. The novel words in the two paraphrases are highlighted in bold and are used to expand the original query: P1 : ways to live with feline allergy P2 : how to deal with cat allergens Finally, paraphrases have also been used to improve the task of"
J10-3003,E06-1052,0,0.0204548,"nerating n-best paraphrases for the query (via a pivot-based sentential paraphrasing model employing bilingual parallel corpora, detailed in Section 3) and then using any new words introduced therein as additional query terms. For example, for the query how to live with cat allergies, they may generate the following two paraphrases. The novel words in the two paraphrases are highlighted in bold and are used to expand the original query: P1 : ways to live with feline allergy P2 : how to deal with cat allergens Finally, paraphrases have also been used to improve the task of relation extraction (Romano et al. 2006). Most recently, Bhagat and Ravichandran (2008) collect paraphrastic patterns for relation extraction by applying semi-supervised paraphrase induction to a very large monolingual corpus. For example, for the relation of “acquisition,” they collect: Original : X agreed to buy Y Variant 1 : X completed its acquisition of Y Variant 2 : X purchased Y 1.3.2 Expanding Sparse Human Reference Data for Evaluation. A large percentage of NLP applications are evaluated by having human annotators or subjects carry out the same 344 Madnani and Dorr Generating Phrasal and Sentential Paraphrases task for a gi"
J10-3003,I05-5011,0,0.0157859,"old), again based on the named entities they share. 5. For each pair of similar sentences, compare their respective attached patterns. If the variables in the patterns link to the same or comparable named entities (based on the entity text and type), then consider the patterns to be paraphrases of each other. At the end, the output is a list of generalized paraphrase patterns with named entity types as variables. For example, the algorithm may generate the following two patterns as paraphrases: PERSON is promoted to POST the promotion of PERSON to POST is decided As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords. The idea of enlisting named entities as proxies for detecting semantic equivalence is interesting and has certainly been explored before (see the discussion regarding Pas¸ca and Dienes [2005] in Section 3.2). However, it has some obvious disadvantages. The authors manually evaluate the technique by generating paraphrases for two speciﬁc 8 Although the authors provide motivating examples in Japanese"
J10-3003,P06-2094,0,0.00885352,"not to repeat the same information in the summary. On this note, Barzilay and McKeown (2005) exploit the redundancy present in a given set of sentences by detecting paraphrastic parts and fusing them into a single coherent sentence. Recognizing similar semantic content is also critical for text simpliﬁcation systems (Marsi and Krahmer 2005b). Information extraction enables the detection of regularities of information structure—events which are reported many times, about different individuals and in different forms—and making them explicit so that they can be processed and used in other ways. Sekine (2006) shows how to use paraphrase recognition to cluster together extraction patterns to improve the cohesion of the extracted information. Another recently proposed natural language processing task is that of recognizing textual entailment: A piece of text T is said to entail a hypothesis H if humans reading T will infer that H is most likely true. The observant reader will notice that, in this sense, the task of paraphrase recognition can simply be formulated as bidirectional entailment recognition. The task of recognizing entailment is an application-independent task and has important ramiﬁcatio"
J10-3003,P06-2096,0,0.530022,"gram similarities (sets of shared overlapping word sequences) between a large number of sentences. However, the two approaches are also different in that Pang, Knight, and Marcu use the parse trees of all sentences in a cluster to compute the alignment (and build the lattice), whereas Barzilay and Lee use only surface level information. Furthermore, Barzilay and Lee can use their slotted lattice pairs to generate paraphrases for novel and unseen sentences, whereas Pang, Knight, and Marcu cannot paraphrase new sentences at all. 366 Madnani and Dorr Generating Phrasal and Sentential Paraphrases Shen et al. (2006) attempt to improve Barzilay and Lee’s technique by trying to include syntactic constraints in the cluster alignment algorithm. In that way, it is doing something similar to what Pang, Knight, and Marcu do but with a comparable corpus instead of a parallel one. More precisely, whereas Barzilay and Lee use a relatively simple alignment scoring function based on purely lexical features, Shen et al. try to bring syntactic features into the mix. The motivation is to constrain the relatively free nature of the alignment generated by the MSA algorithm—which may lead to the generation of grammaticall"
J10-3003,I05-1021,0,0.0134205,"urvey. For one, we do not discuss paraphrasing techniques that rely primarily on knowledge-based resources such as dictionaries (Wallis 1993; Fujita et al. 2004), handwritten rules (Fujita et al. 2007), and formal grammars (McKeown 1979; Dras 1999; Gardent, Amoia, and Jacquey 2004; Gardent and Kow 2005). We also refrain from discussing work on purely lexical paraphrasing which usually comprises various ways to cluster words occurring in similar contexts (Inoue 1991; Crouch and Yang 1992; Pereira, Tishby, and Lee 1993; Grefenstette 1994; Lin 1998; Gasperin et al. 2001; Glickman and Dagan 2003; Shimohata and Sumita 2005).1 Exclusion of general lexical paraphrasing methods obviously implies that other lexical methods developed just for speciﬁc applications are also excluded (Bangalore and Rambow 2000; Duclaye, Yvon, and Collin 2003; Murakami and Nasukawa 2004; Kauchak and Barzilay 2006). Methods at the other end of the spectrum that paraphrase supra-sentential units such as paragraphs and entire documents are also omitted from discussion (Hovy 1988; Inui and Nogami 2001; Hallett and Scott 2005; Power and Scott 2005). Finally, we also do not discuss the notion of near-synonymy (Hirst 1995; Edmonds and Hirst 200"
J10-3003,H90-1004,0,0.239484,"sj and a constant u. This is necessary to handle words from the sentence that do not occur anywhere in the set of paraphrases. Figure 6 shows an example lattice. Once the lattice has been constructed, it is straightforward to extract the 1-best paraphrase by using a dynamic programming algorithm such as Viterbi decoding and extracting the optimal path from the lattice as scored by the product of an n-gram language model and the replacement model. In addition, as with SMT decoding, it is also possible to extract a list of n-best paraphrases from the lattice by using the appropriate algorithms (Soong and Huang 1990; Mohri and Riley 2002). Quirk, Brockett, and Dolan (2004) borrow from the statistical machine translation literature so as to align phrasal equivalences as well as to utilize the aligned phrasal equivalences to rewrite new sentences. The biggest advantage of this method is its SMT inheritance: It is possible to produce multiple sentential paraphrases for any new Figure 6 A paraphrase generation lattice for the sentence He ate lunch at a cafe near Paris. Alternate paths between various nodes represent phrasal replacements. The probability values associated with each edge are not shown for the"
J10-3003,P07-1058,0,0.0112734,"Missing"
J10-3003,W05-1205,0,0.0191222,"such sets so that the machine translation system can learn a better set of system parameters. 2. Paraphrase Recognition and Textual Entailment A problem closely related to, and as important as, generating paraphrases is that of assigning a quantitative measurement to the semantic similarity of two phrases (Fujita and Sato 2008a) or even two given pieces of text (Corley and Mihalcea 2005; Uzuner and Katz 2005). A more complex formulation of the task would be to detect or recognize which sentences in the two texts are paraphrases of each other (Brockett and Dolan 2005; Marsi and Krahmer 2005a; Wu 2005; Jo`ao, Das, and Pavel 2007a, 2007b; Das and Smith 2009; Malakasiotis 2009). Both of these task formulations fall under the category of paraphrase detection or recognition. The latter formulation of the task has become popular in recent years (Dolan and Dagan 2005) and paraphrase generation techniques that require monolingual parallel or comparable corpora (discussed in Section 3) can beneﬁt immensely from this task. In general, paraphrase recognition can be very helpful for several NLP applications. Two examples of such applications are text-to-text generation and information extraction. Tex"
J10-3003,P03-1016,0,0.079514,"ord has any fewer than two modiﬁers, no path can go through it as the root. Therefore, the algorithm tends to perform better for paths with verbal roots. Another issue is that this algorithm, despite the use of more informative distributional features, can generate several incorrect or implausible paraphrase patterns (inference rules). Recent work has shown how to ﬁlter out incorrect inferences when using them in a downstream application (Pantel et al. 2007). Finally, there is no reason for the distributional features to be in the same language as the one in which the paraphrases are desired. Wu and Zhou (2003) describe a 5 A demo of the algorithm is available online at http://demo.patrickpantel.com/Content/LexSem/ paraphrase.htm. 352 Madnani and Dorr Generating Phrasal and Sentential Paraphrases Algorithm 2 (Lin and Pantel 2001). Produce inference rules from a parsed corpus. Summary. Adapt Harris’s (1954) hypothesis of distributional similarity for paths in dependency trees: If two tree paths have similar distributions such that they tend to link the same set of words, then they likely mean the same thing and together generate an inference rule. 1: Extract paths of the form described above from the"
J10-3003,P09-1094,0,0.474492,"eries similar to the phrase, (2) deﬁnitions from the Encarta dictionary, (3) a monolingual parallel corpus, (4) a monolingual comparable corpus, and (5) an automatically constructed thesaurus. Phrasal paraphrase pairs are extracted separately from all six models and then combined in a log-linear paraphrasing-as-translation model proposed by Madnani et al. (2007). A manual inspection reveals that using multiple sources of information yields paraphrases with much higher accuracy. We believe that such exploitation of multiple types of resources and their combinations is an important development. Zhao et al. (2009) further increase the utility of this combination approach by incorporating application speciﬁc constraints on the pivoted paraphrases. For example, if the output paraphrases need to be simpliﬁed versions of the input sentences, then only those phrasal paraphrase pairs are used where the output is shorter than the input. 380 Madnani and Dorr Generating Phrasal and Sentential Paraphrases Use of SMT Machinery. In theory, statistical machine translation is very closely related to paraphrase generation since the former also relies on ﬁnding semantic equivalence, albeit in a second language. Hence,"
J10-3003,P08-1116,0,0.123234,"nﬂuence of the Web will extend to other avenues of paraphrase generation such as the aforementioned extrinsic evaluation or lack thereof. For example, Fujita and Sato (2008b) propose evaluating phrasal paraphrase pairs, automatically generated from a monolingual corpus, by querying the Web for snippets related to the pairs and using them as features to compute the pair’s paraphrasability. Combining Multiple Sources of Information. Another important trend in paraphrase generation is that of leveraging multiple sources of information to determine whether two units are paraphrastic. For example, Zhao et al. (2008) improve the sentential paraphrases that can be generated via the pivot method by leveraging ﬁve other sources in addition to the bilingual parallel corpus itself: (1) a corpus of Web queries similar to the phrase, (2) deﬁnitions from the Encarta dictionary, (3) a monolingual parallel corpus, (4) a monolingual comparable corpus, and (5) an automatically constructed thesaurus. Phrasal paraphrase pairs are extracted separately from all six models and then combined in a log-linear paraphrasing-as-translation model proposed by Madnani et al. (2007). A manual inspection reveals that using multiple"
J10-3003,W06-1610,0,0.00596023,"olution is to use multiple reference translations, which is expensive. An alternative solution, tried in a number of recent approaches, is to address this issue by allowing the evaluation process to take into account paraphrases of phrases in the reference translation so as to award credit to parts of the translation hypothesis that are semantically, even if not lexically, correct (Owczarzak et al. 2006; Zhou, Lin, and Hovy 2006). In evaluation of document summarization, automatically generated summaries (peers) are also evaluated against reference summaries created by human authors (models). Zhou et al. (2006) propose a new metric called ParaEval that leverages an automatically extracted database of phrasal paraphrases to inform the computation of n-gram overlap between peer summaries and multiple model summaries. 1.3.3 Machine Translation. Besides being used in evaluation of machine translation systems, paraphrasing has also been applied to directly improve the translation process. Callison-Burch, Koehn, and Osborne (2006) use automatically induced paraphrases to improve a statistical phrase-based machine translation system. Such a system works by dividing the given sentence into phrases and trans"
J10-3003,N06-1057,0,0.0316833,"olution is to use multiple reference translations, which is expensive. An alternative solution, tried in a number of recent approaches, is to address this issue by allowing the evaluation process to take into account paraphrases of phrases in the reference translation so as to award credit to parts of the translation hypothesis that are semantically, even if not lexically, correct (Owczarzak et al. 2006; Zhou, Lin, and Hovy 2006). In evaluation of document summarization, automatically generated summaries (peers) are also evaluated against reference summaries created by human authors (models). Zhou et al. (2006) propose a new metric called ParaEval that leverages an automatically extracted database of phrasal paraphrases to inform the computation of n-gram overlap between peer summaries and multiple model summaries. 1.3.3 Machine Translation. Besides being used in evaluation of machine translation systems, paraphrasing has also been applied to directly improve the translation process. Callison-Burch, Koehn, and Osborne (2006) use automatically induced paraphrases to improve a statistical phrase-based machine translation system. Such a system works by dividing the given sentence into phrases and trans"
J10-3003,J93-1004,0,\N,Missing
J10-3003,W03-1608,0,\N,Missing
J10-3003,J09-1008,0,\N,Missing
J10-3003,J90-2002,0,\N,Missing
J10-3003,W07-1401,0,\N,Missing
J10-3003,C98-2122,0,\N,Missing
J10-3003,N10-4008,0,\N,Missing
J10-3003,W07-1400,0,\N,Missing
J10-3003,D07-1096,0,\N,Missing
J10-3003,W05-1605,0,\N,Missing
J10-3003,W04-0400,0,\N,Missing
J10-3003,P93-1024,0,\N,Missing
N10-1041,N06-1049,1,0.83045,"a pre-arranged period of time shortly thereafter, each assessor was given five minutes to interact with the participant’s system, live over the web. After this interaction period, participants submitted a final run, which had presumably gained the benefit of user interaction. By comparing initial and final runs, it was possible to quantify the effect of the interaction. The target corpus was AQUAINT-2, which consists of around 970k documents totaling 2.5 GB. System responses consisted of multi-line answers and were evaluated using the “nugget” methodology with the “nugget pyramid” extension (Lin and Demner-Fushman, 2006). 3 Experiment Design This section describes our experiments for the TREC 2007 ciQA task. In summary: the initial run was generated automatically using standard MMR. The web-based interactions consisted of iterations of interactive MMR, where the user selected the best candidate extract at each step. The final run consisted of the output of interactive MMR padded with automatically-generated output. Sentence extracts were used as the basic response unit. For each topic, the top 100 documents were retrieved from the AQUAINT-2 collection with Lucene, using the topic template verbatim as the quer"
N10-1041,N07-1027,1,0.902114,"Missing"
N12-1003,W01-1605,0,0.103125,"nent’s claims (e.g., “The argument states that”) • to evaluate an opponent’s claims (e.g., “It may seem reasonable at first glance, but actually, there are some logical mistakes in it”) • to present evidence and relate it to specific claims (e.g., “To illustrate my point, I will now give the example of”) There are many ways of analyzing discourse. The most relevant is perhaps rhetorical structure theory (RST) (Mann and Thompson, 1988). To our knowledge, the RST parser from Marcu (2000) is the only RST parser readily available for experimentation. The parser is trained to model the RST corpus (Carlson et al., 2001), which treats complete clauses (i.e., clauses with their obligatory complements) as the elementary units of analysis. Thus, the parser treats the first sentence in example 1 as a single unit and does not differentiate between the main and subordinate clauses. In contrast, our approach distinguishes 21 the sequence “The argument states that . . . ” as shell (which is used here to restate the external claim). Furthermore, we identify the entire second sentence as shell (here, used to evaluate the external claim), whereas the RST parser splits the sentence into two clauses, “It may seem . . .” a"
N12-1003,1993.eamt-1.1,0,0.348313,"Missing"
N12-1003,P11-1099,0,0.0677393,"mber of relationships. On the other hand, shell can capture longer sequences that express more complex relationships between the components of an argumentative discourse (e.g., “But let’s get back to the core issue here” signals that the following point is more important than the previous one). Acknowledgments There are also various other approaches to analyzing arguments. Notably, much recent theoretical research on argumentation has focused on argumentation schemes (Walton et al., 2008), which are high-level strategies for constructing arguments (e.g., argument from consequences). Recently, Feng and Hirst (2011) developed automated methods for classifying texts by argumentation scheme. In similar work, Anand et al. (2011) use argumentation schemes to identify tactics in blog posts (e.g., moral appeal, social generalization, appeals to external authorities etc.). Although shell language can certainly be found in persuasive writing, it is used to organize the persuader’s tactics and claims rather than to express them. For example, consider the following sentence: “It must be the case that this diet works since it was recommended by someone who lost 20 pounds on it.” In shell detection, we focus on the"
N12-1003,P04-1087,0,0.0886083,"e opponents claims, connect ones own claims, etc., may be seen as determining what Grosz and Sidener call “discourse segment purposes” (i.e., the intentions underlying the segments containing the shell spans). We can also view shell detection as the task of identifying phrases that indicate certain types of speech acts (Searle, 1975). In particular, we aim to identify markers of assertive speech acts, which declare that the speaker believes a certain proposition, and expressive speech acts, which express attitudes toward propositions. Shell also overlaps with the concept of discourse markers (Hutchinson, 2004), such as “however” or TP L INCOLN (L) — D OUGLAS (D) DEBATES L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is actually crazy. L: The first thing I see fit to notice is the fact that . . . FP D: He became noted as the author of the scheme to . . . D: . . . such amendments were to be made to it as would render it useless and inefficient . . . FN D: I wish to impress it upon you, that every man who voted for those resolutions . . . L: That statement he makes, too, in the teeth of the knowledge that I had made the stipulation to come down here"
N12-1003,J86-3001,0,\N,Missing
N12-1019,P08-1007,0,0.058236,"a syntactically-aware metric designed to focus on structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires a length penalty to minimize the effects of length. 7. BADGER (Parker, 2008) is a language independent metric based on compression and information theory. It computes a compression distance between the two sentences that utilizes the Burrows Wheeler Transformation (BWT). The BWT enables taking into account common sentence contexts with no limit on the size of these contexts. 8. MAXSIM (Chan and Ng, 2008) treats the problem as one of bipartite graph matching and maps each word in one sentence to at most one word in the other sentence. It allows the use of arbitrary similarity functions between words.2 Our choice of metrics was based on their popularity in the MT community, their performance in open competitions such as the NIST MetricsMATR challenge (NIST, 2008) and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity. 3.3 Datasets In this section, we describe the two datasets that we used to evaluate our approach. 3.3.1 Microsoft"
N12-1019,P09-1053,0,0.648384,"based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for paraphrase identification. Fernando and Stevenson (2008) construct a matrix of word similarities between all pairs of words in both sentences instead of relying only on the maximal similarities. Das and Smith (2009) use an explicit model of alignment between the corresponding parts of two paraphrastic sentences and combine it with a logistic regression classifier built from n-gram overlap features. Most recently, Socher et al. (2011) employ a joint model that incorporates the similarities between both single word features as well as multi-word phrases extracted from the parse trees of the two sentences. We compare our results to those from all the approaches described in this section later in §3.4. 3 Classifying with MT Metrics In this section, we first describe our overall approach to paraphrase identif"
N12-1019,N10-1031,0,0.034254,"ording to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2009) builds upon the core TER algorithm by providing additional edit operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and paraphrase (via a lookup table). 6. SEPIA (Habash and El Kholy, 2008) is a syntactically-aware metric designed to focus on structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires a length penalty to minimize the effects of length. 7. BADGER (Parker, 2008) is a language independent metric based on compression and information theory. It com"
N12-1019,C04-1051,0,0.850577,"anslation hypothesis produced by a system is semantically equivalent to the source sentence that was translated. However, cross-lingual semantic equivalence is even harder to assess than monolingual, therefore, most MT metrics instead try to measure whether the hypothesis is semantically equivalent to a human-authored reference translation of the same source sentence. Using such automated metrics as This paper describes such a re-examination. We employ 8 different MT metrics for identifying paraphrases across two different datasets - the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al., 2010). We include both MSRP and PAN in our study because they represent two very different sources of paraphrased text. The creation of MSRP relied on the massive redundancy of news articles on the web and extracted sentential paraphrases from different stories written about the same topic. In the case of PAN, humans consciously paraphrased existing text to generate new, 182 2012 Conference of the North American Chapter of the Association for Compu"
N12-1019,I05-5003,0,0.777794,"olely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community. 1 In the last 5 years, several shared tasks and competitions have led to the development of increasingly sophisticated metrics that go beyond the computation of n-gram overlaps (BLEU, NIST) or edit distances (TER, WER, PER etc.). Note that the task of an MT metric is essentially one of identifying whether the translation produced by a system is a paraphrase of the reference translation. Although the notion of using MT metrics for the task of paraphrase identification is not novel (Finch et al., 2005; Wan et al., 2006), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask “How much better, if at all, do these newer metrics perform for the task of paraphrase identification?” Introduction One of the most important reasons for the recent advances made in Statistical Machine Translation (SMT) has been the development of automated metrics for evaluation of translation quality. The goal of any such metric is to assess whether the translation hypothesis produced by a system is semantically equivalent to the source sentence that was translated."
N12-1019,P11-2089,1,0.745416,"1 indicating the degree to which two pairs are paraphrastic is more suitable for most approaches. However, rather than asking annotators to rate pairs on a scale, a better idea might be to show the sentence pairs to a large number of Turkers (≥ 20) on Amazon Mechanical Turk and ask them to classify it as either a paraphrase or a non-paraphrase. A simple estimate of the degree of semantic equivalence of the pair is simply the proportion of the Turkers who classified the pair as paraphrastic. An example of such an approach, as applied to the task of grammatical error detection, can be found in (Madnani et al., 2011).3 Second, we believe that the PAN corpus— with Turker simulated plagiarism—contains much more realistic examples of paraphrase and should be incorporated into future evaluations of paraphrase identification. In order to encourage this, we are releasing our PAN dataset containing 13,000 sentence pairs. We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they attempted to accurately identify all error"
N12-1019,P02-1040,0,0.0953774,"r BLEU(1-4)). Our best system utilized a classifier combination approach. We used a simple meta-classifier that uses the average of the unweighted probability estimates from the constituent classifiers to make its final decision. We used three constituent classifiers: Logistic regression, the SMO implementation of a support vector machine (Platt, 1999; Keerthi et al., 2001) and a lazy, instance-based classifier that extends the nearest neighbor algorithm (Aha et al., 1991). We used the WEKA machine learning toolkit to perform our experiments (Hall et al., 2009). 1 3.2 MT metrics used 1. BLEU (Papineni et al., 2002) is the most commonly used metric for MT evaluation. It is computed as the amount of n-gram overlap— for different values of n—between the system output and the reference translation, tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. We use BLEU1 through BLEU4 as 4 different fea1 These constituent classifiers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metaclassifier was chosen similarly once the constituent cl"
N12-1019,C10-2115,0,0.0351299,"Missing"
N12-1019,W06-1603,0,0.304937,"rase and can prove useful to the 183 community for future evaluations of paraphrase identification. BLEU-based features were also employed by Wan et al. (2006) who use them in combination with several other features based on dependency relations and tree edit-distance inside an SVM. There are several other supervised approaches to paraphrase identification that do not use any features based on MT metrics. Mihalcea et al. (2006) combine pointwise mutual information, latent semantic analysis and WordNet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric. Qiu et al. (2006) build a framework that detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. Kozareva and Montoyo (2006) use features based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for par"
N12-1019,2006.amta-papers.25,0,0.0825834,"s 4 different fea1 These constituent classifiers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metaclassifier was chosen similarly once the constituent classifiers had been chosen. 184 2. NIST (Doddington, 2002) is a variant of BLEU that uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. It also weights each n-gram according to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2009) builds upon the core TER algorithm by providing additional edit operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and p"
N12-1019,U06-1019,0,0.930952,"We release both the new dataset and the error analysis annotations for use by the community. 1 In the last 5 years, several shared tasks and competitions have led to the development of increasingly sophisticated metrics that go beyond the computation of n-gram overlaps (BLEU, NIST) or edit distances (TER, WER, PER etc.). Note that the task of an MT metric is essentially one of identifying whether the translation produced by a system is a paraphrase of the reference translation. Although the notion of using MT metrics for the task of paraphrase identification is not novel (Finch et al., 2005; Wan et al., 2006), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask “How much better, if at all, do these newer metrics perform for the task of paraphrase identification?” Introduction One of the most important reasons for the recent advances made in Statistical Machine Translation (SMT) has been the development of automated metrics for evaluation of translation quality. The goal of any such metric is to assess whether the translation hypothesis produced by a system is semantically equivalent to the source sentence that was translated. However, cross-lin"
N12-1019,O97-1002,0,\N,Missing
N13-1055,C12-1038,1,0.818668,"ion corrections Total number of corrections suggested Number of correct preposition corrections Total number of corrections in test set Note that due to the high volume of unchanged prepositions in the test corpus, we obtain very high accuracies, which are not indicative of true performance, and are not included in our results. The results of our experiments are presented in Table 2.12 The first part of the table shows the fscores of preposition error correction systems that 10 We use liblinear (Fan et al., 2008) with the L1-regularized logistic regression solver and default parameters. 11 As Chodorow et al. (2012) note, it is not clear how to handle cases where the system predicts a preposition that is neither the same as the writer preposition nor the correct preposition. We count these cases as false positives. 12 No thresholds were used in the systems that were trained on well-edited text. Traditionally, thresholds are applied so as to only predict a correction when the system is highly confident. This has the effect of increasing precision at the cost of recall, and sometimes leads to an overall improved f-score. Here we take the prediction of the system, regardless of the confidence, reflecting a"
N13-1055,P11-1092,0,0.466589,"he usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and"
N13-1055,W11-2838,0,0.0132963,"on either side of the preposition (∼clean). The third contains all corrections regardless of any other changes in the surrounding context (all). Lang-8 The Lang-8 website contains journals written by language learners, where native speakers highlight and correct errors on a sentence-bysentence basis. As a result, it contains typical grammatical mistakes made by language learners, which can be easily downloaded. We automatically extract 75,622 sentences with preposition errors and corrections from the first million journal entries.7 HOO 2011 We take the test set from the HOO 2011 shared task (Dale and Kilgarriff, 2011) and extract all examples of preposition selection errors. The texts are fragments of ACL papers that have been manually annotated for grammatical errors.8 It is important to note that the three test sets we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault"
N13-1055,W12-2006,0,0.0438256,"and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction"
N13-1055,P11-4017,0,0.0264218,"al., 2012). To our knowledge, no one has previously extracted data for training a grammatical error detection system from Wikipedia revisions. 3.2 Extracting Preposition Correction Data from Wikipedia Revisions As the source of our Wikipedia revisions, we used an XML snapshot of Wikipedia generated in July 2011 containing 8,735,890 articles and 288,583,063 revisions.1 We then used the following process to extract preposition errors and their corresponding corrections from this snapshot: Step 1: Extract the plain text versions of all revisions of all articles using the Java Wikipedia Library (Ferschke et al., 2011). Step 2: For each Wikipedia article, compare each revision with the revision immediately preceding it using an efficient diff algorithm.2 Step 3: Compute all 1-word edit chains for the article, i.e., sequences of related edits derived from all revisions of the same article. For example, say revision 10 of an article inserts the preposition of into a sentence and revision 12 changes that preposition to on. Assuming that no other revisions change this sentence, the corresponding edit chain would contain the following 3 elements: →of→on. The extracted chains contain the full context on either s"
N13-1055,W09-2112,0,0.633063,"Missing"
N13-1055,I08-1059,0,0.0273622,"n welledited text? 3. What is the impact of having a mismatch in the error distributions of the training and test sets? 2 Related Work In this section, we only review work in preposition error correction in terms of the three training paradigms and refer the reader to Leacock et al. (2010) for a more comprehensive review of the field. 2.1 Training on Well-Edited Text Early approaches to error detection and correction did not have access to large amounts of errorannotated data to train statistical models and thus, systems were trained on millions of well-edited examples from news text instead (Gamon et al., 2008; Tetreault and Chodorow, 2008; De Felice and Pulman, 2009). Feature sets usually consisted of ngrams around the preposition, POS sequences, syntactic features and semantic information. Since the model only had knowledge of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for ma"
N13-1055,N10-1019,0,0.0565523,"dressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of correct usage) outperformed a model trained only on 5 million examples of correct usage. Gamon (2010) and Dahlmeier and Ng (2011) showed that combining models trained separately on examples of correct and incorrect usage could also improve the performance of a preposition error correction system. 3 3.1 Mining Wikipedia Revisions for Grammatical Error Corrections Related Work Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types"
N13-1055,P12-2076,0,0.0554385,"rror distributions derived from the error-annotated learner corpora available at the time. Izumi et al. (2003) was the first to evaluate a model trained on incorrect usage as well as artificial errors for the task of correcting several different 508 error types, including prepositions. However, with limited training data, system performance was quite poor. Rozovskaya and Roth (2010) evaluated different ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and addressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of"
N13-1055,P03-2026,0,0.440635,"of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for many years, such error-annotated corpora were not available. Instead, several researchers generated artificial errors based on the error distributions derived from the error-annotated learner corpora available at the time. Izumi et al. (2003) was the first to evaluate a model trained on incorrect usage as well as artificial errors for the task of correcting several different 508 error types, including prepositions. However, with limited training data, system performance was quite poor. Rozovskaya and Roth (2010) evaluated different ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and a"
N13-1055,max-wisniewski-2010-mining,0,0.390095,"ect usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction (Zesch, 2012), this resource has not been used for the task of grammatical error correction. To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one). We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data. We also generate artificial errors using the confusion distributio"
N13-1055,D10-1094,0,0.513155,"using rules and regular expressions (Leacock et al., 2010). On the other hand, errors involving the usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP ta"
N13-1055,P12-2039,0,0.0232089,"ish speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7 Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8 The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9 Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. 511 and De Felice and Pulman (2009). In short, the method models the problem o"
N13-1055,C08-1109,1,0.960607,". What is the impact of having a mismatch in the error distributions of the training and test sets? 2 Related Work In this section, we only review work in preposition error correction in terms of the three training paradigms and refer the reader to Leacock et al. (2010) for a more comprehensive review of the field. 2.1 Training on Well-Edited Text Early approaches to error detection and correction did not have access to large amounts of errorannotated data to train statistical models and thus, systems were trained on millions of well-edited examples from news text instead (Gamon et al., 2008; Tetreault and Chodorow, 2008; De Felice and Pulman, 2009). Feature sets usually consisted of ngrams around the preposition, POS sequences, syntactic features and semantic information. Since the model only had knowledge of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for many years, such error-annotated"
N13-1055,P10-2065,1,0.572162,"s we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7 Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8 The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9 Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. 51"
N13-1055,N03-1033,0,0.00614282,"here a preposition is replaced with another preposition. If the preposition edit is the only edit in the sentence, we convert the chain into a sentence pair and label it clean. If there are other 1-word edits but not within 5 words of the preposition edit on either side, we label the sentence somewhat clean. Otherwise, we label it dirty. The motivation is that the presence of other nearby edits make the preposition correction less reliable when used in isolation, due to the possible dependencies between corrections. All extracted sentences were part-of-speech tagged using the Stanford Tagger (Toutanova et al., 2003). Using the above process, we are able to extract approximately 2 million sentences containing prepositions errors and their corrections. Some examples of the sentences we extracted are given in Figure 1. Example (4) shows an example of a bad correction. 4 Corpora We use several corpora for training and testing our preposition error correction system. The properties of each are outlined in Table 1, organized by paradigm. For each corpus we report the total number of prepositions used for training, as well as the number and percentage of preposition corrections. 4.1 Well-edited Text We train ou"
N13-1055,P08-2035,0,0.019275,"Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at → in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of → on) those dates supports Ranneft’s claims. (3) [Wiki dirty] . . . cirque has a permanent production (to → at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French touris"
N13-1055,P11-1019,0,0.0895178,"hanged. 4.3 Naturally Occurring Errors We have a number of corpora that contain annotated preposition errors. Note that we are only considering incorrectly selected prepositions, we do not consider missing or extraneous. NUCLE The NUS Corpus of Learner English (NUCLE)5 contains one million words of learner essay text, manually annotated with error tags and corrections. We use the same training, dev and test splits as Dahlmeier and Ng (2011). FCE The CLC FCE Dataset6 is a collection of 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English (Yannakoudakis et al., 2011). It includes demographic metadata about the candidate, a grade for each essay and manuallyannotated error corrections. Wikipedia We use three versions of the preposition errors extracted from the Wikipedia revisions as described in Section 3.2. The first includes corrections where the preposition was the only word corrected in the entire sentence 5 6 http://bit.ly/nuclecorpus http://ilexir.co.uk/applications/clc-fce-dataset/ (clean). The second contains all clean corrections, as well as all corrections where there were no other edits within a five-word span on either side of the preposition ("
N13-1055,N10-1056,0,0.0707676,"ine a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at → in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of → on) those dates supports Ranneft’s claims. (3) [Wiki dirty] . . . cirque has a permanent production (to → at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French tourists (in → to) Petersburg. Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second"
N13-1055,E12-1054,0,0.192817,"The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction (Zesch, 2012), this resource has not been used for the task of grammatical error correction. To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one). We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data. We also generate artificial errors using the confusion distributions derived from this data. We compare both of these approaches to"
N13-1055,han-etal-2010-using,1,\N,Missing
N13-1055,W10-3504,0,\N,Missing
N15-1111,S13-1004,0,0.0302943,"be cheap to acquire since it is often generated as part of the assessment development process. Generally speaking, most work on short answer scoring takes one of the following approaches: • A response-based approach uses detailed features extracted from the student response itself (e.g., word n-grams, etc.) and learns a scoring function from human-scored responses. • A reference-based approach compares the student response to reference texts, such as exemplars for each score level, or specifications of required content from the assessment’s scoring guidelines. Various text similarity methods (Agirre et al., 2013) can be used. These two approaches can, of course, be combined. However, to our knowledge, the issues of how to combine the approaches and when that is likely to be useful have not been thoroughly studied. A challenge in combining the approaches is that the response-based approach produces a large set of sparse features (e.g., word n-gram indicators), while the reference-based approach produces a small set of continuous features (e.g., similarity scores between the response and exemplars for different score levels). A simple combination method is to train a model on the union of the feature se"
N15-1111,S13-2045,0,0.194809,"ods for integrating scoring guidelines and labeled responses, and we find that stacked generalization (Wolpert, 1992) improves performance, especially for small training sets. 1 Introduction Educational applications of NLP have considerable potential for real-world impact, particularly in helping to score responses to assessments, which could allow educators to focus more on instruction. We focus on the task of analyzing short, contentfocused responses from an assessment of reading comprehension, following previous work on short answer scoring (Leacock and Chodorow, 2003; Mohler et al., 2011; Dzikovska et al., 2013). This task is typically defined as a text regression or classification problem: we label student responses that consist of one or more sentences with scores on an ∗ Work done when Keisuke Sakaguchi was an intern at ETS. Michael Heilman is now a data scientist at Civis Analytics. ordinal scale (e.g. correct, partially correct, or incorrect; 1–5 score range, etc.). Importantly, in addition to the student response itself, we may also have available other information such as reference answers or descriptions of key concepts from the scoring guidelines for human scorers. Such information can be ch"
N15-1111,S13-2046,1,0.731146,"cribes the experiences of European immigrants in the late 19th and early 20th centuries. There are 3 questions associated with this passage: two that ask the reader to summarize one section each in the article (“Q2” and “Q4”) and a third that asks to summarize the entire article (“Q3”). These 3 questions ask the reader to restrict his or her responses to 1–2 sentences each. Each question includes the following: 1 Some applications have used stacking but not analyzed its value. For example, many participants used stacking in the ASAP2 competition http://http://www.kaggle. com/c/asap-sas. Also, Heilman and Madnani (2013) used stacking for Task 7 of SemEval 2013. 1050 • scored responses: short responses written by students, scored on a 0 to 4 scale for the first question, and 0 to 3 for the other 3. • exemplars: one or two exemplar responses for each score level, and • key concepts: several (≤ 10) sentences briefly expressing key concepts in a correct answer. The data for each question is split into a training and testing sets. For each question, we have about 2,000 scored student responses. Following previous work on automatic scoring (Shermis and Burstein, 2013), we evaluate performance using the quadratical"
N15-1111,P11-1076,0,0.274716,"Here, we explore methods for integrating scoring guidelines and labeled responses, and we find that stacked generalization (Wolpert, 1992) improves performance, especially for small training sets. 1 Introduction Educational applications of NLP have considerable potential for real-world impact, particularly in helping to score responses to assessments, which could allow educators to focus more on instruction. We focus on the task of analyzing short, contentfocused responses from an assessment of reading comprehension, following previous work on short answer scoring (Leacock and Chodorow, 2003; Mohler et al., 2011; Dzikovska et al., 2013). This task is typically defined as a text regression or classification problem: we label student responses that consist of one or more sentences with scores on an ∗ Work done when Keisuke Sakaguchi was an intern at ETS. Michael Heilman is now a data scientist at Civis Analytics. ordinal scale (e.g. correct, partially correct, or incorrect; 1–5 score range, etc.). Importantly, in addition to the student response itself, we may also have available other information such as reference answers or descriptions of key concepts from the scoring guidelines for human scorers. S"
N15-1111,P02-1040,0,0.094133,"SVR #2 SVR #1 Reference-based Our implementation of the reference-based approach (“ref” in §4) uses SVR to estimate a model to predict human scores from various measures of the similarity between the response and information from the scoring guidelines provided to the human scorers. Specifically, we use the following information from §2: (a) sentences expressing key concepts that should be present in correct responses, and (b) small sets of exemplar responses for each score level. For each type of reference, we use the following similarity metrics: • BLEU: the BLEU machine translation metric (Papineni et al., 2002), with the student response as the translation hypothesis. When using BLEU to compare the student response to the (much shorter) sentences containing key concepts, we ignore the brevity penalty. • word2vec cosine: the cosine similarity between the averages of the word2vec vectors (Mikolov et al., 2013) of content words in the response and reference texts (e.g., exemplar), respectively.5,6 • word2vec alignment: the alignment method below with word2vec word similarities. • WordNet alignment: the alignment method below with the Wu and Palmer (1994) WordNet (Miller, 1995) similarity score. The Wor"
N15-1111,W01-0506,0,0.1032,"Missing"
N15-1111,D08-1017,0,0.0347079,"Missing"
N15-1111,P94-1019,0,\N,Missing
N18-3008,W10-1013,0,0.0349693,"peech processing and NLP techniques have also been used to detect other types of non-scorable responses: language identification technology for non-English detection (Yoon and Higgins, 2011) and speaker recognition technology for automated impostor detection (Qian et al., 2016). “Banging on the keyboard” can be identified by analyzing part-ofspeech sequences and looking for ill-formed sequences (Higgins et al., 2006). have been developed based on question-specific content models, such as a standard vector space model (VSM) built for each question (Bernstein et al., 2000; Higgins et al., 2006; Louis and Higgins, 2010). For speaking tests eliciting highly or moderately restricted speech, filtering models based on features derived from ASR systems such as normalized confidence scores and language model (LM) scores can achieve good performance in identifying topic-related non-scorable responses (van Doremalen et al., 2009; Lo et al., 2010; Cheng and Shen, 2011). However, this approach is not appropriate for a speaking test that elicits unconstrained spontaneous speech. More recently, similar to techniques that have been applied in essay scoring, systems based on document similarity measures and topic detectio"
N18-3008,W11-1420,1,0.808429,"overall architecture of a generic automated scoring pipeline. Above the dotted line are the key stages in automated scoring. Below the dotted line are the possible additions to the pipeline to handle atypical inputs using filtering models (FMs). task on plagiarism detection. Wang et al. (2016) developed a spoken canned response detection system using similar techniques applied in essay plagiarism detection. In addition, various speech processing and NLP techniques have also been used to detect other types of non-scorable responses: language identification technology for non-English detection (Yoon and Higgins, 2011) and speaker recognition technology for automated impostor detection (Qian et al., 2016). “Banging on the keyboard” can be identified by analyzing part-ofspeech sequences and looking for ill-formed sequences (Higgins et al., 2006). have been developed based on question-specific content models, such as a standard vector space model (VSM) built for each question (Bernstein et al., 2000; Higgins et al., 2006; Louis and Higgins, 2010). For speaking tests eliciting highly or moderately restricted speech, filtering models based on features derived from ASR systems such as normalized confidence score"
P11-2089,W10-0731,0,0.0164388,"lice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in"
P11-2089,D09-1030,0,0.0334174,"ferent groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in ne"
P11-2089,W10-0708,0,0.00908944,"e there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of"
P11-2089,C08-1022,0,0.0718184,"Missing"
P11-2089,W10-0713,0,0.00614558,"s the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s A"
P11-2089,I08-1059,0,0.0380613,"of crowdsourcing. 1 Motivation and Contributions to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambigu"
P11-2089,N10-1019,0,0.0860787,"Missing"
P11-2089,W10-0717,0,0.0141234,"ated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) a"
P11-2089,N10-1024,0,0.0278613,"orpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL)."
P11-2089,W10-1004,1,0.378088,"Missing"
P11-2089,D10-1094,1,0.419692,"Missing"
P11-2089,C08-1109,1,0.664593,"ies, both based on a novel use of crowdsourcing. 1 Motivation and Contributions to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009),"
P11-2089,W10-1006,1,0.836109,"allison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s ACL conference, there are four long papers devoted to this topic. Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human rater. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than sim1 ple rule violations such as number agreement. As a There has been a rec"
P11-2089,W10-0725,0,0.0169367,"rent evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 20"
P11-2089,N10-1057,0,0.046017,"Missing"
P11-2089,W10-4236,0,\N,Missing
P13-4025,W12-3134,0,0.0241482,"Missing"
P13-4025,J10-3003,1,0.84075,"ocessing (NLP) tasks, such as information retrieval, question answering, recognizing textual entailment, text simplification etc. For example, a question answering system facing a question “Who invented bifocals and lightning rods?” could retrieve the correct answer from the text “Benjamin Franklin invented strike termination devices and bifocal reading glasses” given the information that “bifocal reading glasses” is a paraphrase of “bifocals” and “strike termination devices” is a paraphrase of “lightning rods”. There are numerous approaches for automatically extracting paraphrases from text (Madnani and Dorr, 2010). We focus on generating paraphrases by pivoting on bilingual parallel corpora as originally suggested by Bannard and CallisonBurch (2005). This technique operates by attempting to infer semantic equivalence between phrases in the same language by using a second language as a bridge. It builds on one of the initial steps used to train a phrase-based statistical machine translation system. Such systems rely on phrase tables – 2 ParaQuery In this section we first briefly describe how to set up ParaQuery (§2.1) and then demonstrate its use in detail for interactively exploring and characterizing"
P13-4025,P07-1059,0,0.0293967,"mply by using the phrases in the other language as pivots, e.g., if both “man” and “person” correspond to “personne” in French, then they can be considered paraphrases. Each paraphrase pair (rule) in a pivoted paraphrase collection is defined by a source phrase e1 , the target phrase e2 that has been inferred as its paraphrase, and a probability score p(e2 |e1 ) obtained from the probability values in the bilingual phrase table.1 Pivoted paraphrase collections have been successfully used in different NLP tasks including automated document summarization (Zhou et al., 2006), question answering (Riezler et al., 2007), and machine translation (Madnani, 2010). Yet, it is still difficult to get an estimate of the intrinsic quality and coverage of the paraphrases contained in these collections. To remedy this, we propose ParaQuery – a tool that can help explore and analyze pivoted paraphrase collections. Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications, it is still difficult to get an intrinsic estimate of the quality and coverage of"
P13-4025,N06-1057,0,0.0264827,"rases in one language may be inferred simply by using the phrases in the other language as pivots, e.g., if both “man” and “person” correspond to “personne” in French, then they can be considered paraphrases. Each paraphrase pair (rule) in a pivoted paraphrase collection is defined by a source phrase e1 , the target phrase e2 that has been inferred as its paraphrase, and a probability score p(e2 |e1 ) obtained from the probability values in the bilingual phrase table.1 Pivoted paraphrase collections have been successfully used in different NLP tasks including automated document summarization (Zhou et al., 2006), question answering (Riezler et al., 2007), and machine translation (Madnani, 2010). Yet, it is still difficult to get an estimate of the intrinsic quality and coverage of the paraphrases contained in these collections. To remedy this, we propose ParaQuery – a tool that can help explore and analyze pivoted paraphrase collections. Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications, it is still difficult to get an intrins"
P13-4025,P05-1074,0,0.0693368,"Missing"
P13-4025,P09-1051,0,\N,Missing
P14-2029,P03-1054,0,0.0255583,"tem for the binary task by binarizing its predictions.12 We compare our work to a modified version of the publicly available13 system from Post (2011), which performed very well on an artificial dataset. To our knowledge, it is the only publicly available system for grammaticality prediction. It is very Table 1: Pearson’s r on the development set, for our full system and variations excluding each feature type. “− X” indicates the full model without the “X” features. 3.2.4 PCFG Parsing Features We find phrase structure trees and basic dependencies with the Stanford Parser’s English PCFG model (Klein and Manning, 2003; de Marneffe et al., 2006).11 We then compute the following: • the parse score as provided by the Stanford PCFG Parser, normalized for sentence length, later referred to as parse prob • a binary feature that captures whether the top node of the tree is sentential or not (i.e. the assumption is that if the top node is nonsentential, then the sentence is a fragment) • features binning the number of dep relations returned by the dependency conversion. These dep relations are underspecified for function and indicate that the parser was unable to find a standard relation such as subj, possibly ind"
P14-2029,N07-2024,0,0.0193024,"eilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our syste"
P14-2029,W11-2111,1,0.923297,"Missing"
P14-2029,copestake-flickinger-2000-open,0,0.0167849,"iss + 1) as features. To identify misspellings, we use a freely available spelling dictionary for U.S. English.5 3.2.3 Precision Grammar Features Following Wagner et al. (2007) and Wagner et al. (2009), we use features extracted from precision grammar parsers. These grammars have been hand-crafted and designed to only provide complete syntactic analyses for grammatically correct sentences. This is in contrast to treebanktrained grammars, which will generally provide some analysis regardless of grammaticality. Here, we use (1) the Link Grammar Parser8 and (2) the HPSG English Resource Grammar (Copestake and Flickinger, 2000) and PET parser.9 We use a binary feature, complete link, from the Link grammar that indicates whether at least one complete linkage can be found for a sentence. We also extract several features from the HPSG analyses.10 They mostly reflect information about unification success or failure and the associated costs. In each instance, we use the logarithm of one plus the frequency. 3.2.2 n-gram Count and Language Model Features Given each sentence, the model obtains the counts of n-grams (n = 1 . . . 3) from English Gigaword and computes the following features:6 • of to X log(count(s) + 1) kSn k"
P14-2029,2003.mtsummit-papers.9,0,0.0574979,"Missing"
P14-2029,P11-2038,0,0.180454,"sa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Post (2011) and"
P14-2029,D11-1010,0,0.0137361,"resent a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammatical2 Dataset Description We created a dataset consisting of 3,129 sentences randomly selected from essays written by nonnative speakers of English as part of a test of English language proficiency. We oversampled lower-scoring essays to increase the chances of finding ungrammatical sentences. Two of the a"
P14-2029,P07-1011,0,0.0284516,"al Scale Michael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we"
P14-2029,de-marneffe-etal-2006-generating,0,0.0433313,"Missing"
P14-2029,1993.eamt-1.1,0,0.520675,"Missing"
P14-2029,C08-1109,1,0.271581,"lop a system for the task of predicting the grammaticality of sentences, and present a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammatical2 Dataset Description We created a dataset consisting of 3,129 sentences randomly selected from essays written by nonnative speakers of English as part of a test of English language proficiency. We oversampled lower-scoring essays"
P14-2029,2005.eamt-1.15,0,0.0187352,"Missing"
P14-2029,D07-1012,0,0.257114,"Missing"
P14-2029,U10-1011,0,0.017105,"l Nitin Madnani Melissa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Pos"
P14-2029,W13-2201,0,\N,Missing
P14-2041,P13-1000,0,0.185265,"Missing"
P14-2041,W04-1013,0,0.00447918,"tance – the content that is unique to the lecture is more important than the content that is shared by the lecture and the reading. The following two models capitalize on evidence of use of information in better and worse essays. For estimating these models, we sample, for each prompt, a development set of 750 essays responding to the prompt (that is, addressing a given pair of lecture and reading stimuli). Out of these, we take, for each prompt, all essays at score points 2 In the future, we intend to explore more complex realization functions, allowing paraphrase, skip n-grams (as in ROUGE (Lin, 2004)), and other approximate matches, such as misspellings and inflectional variants. 3 Prob, Position, and LectVsRead models normalize by nR and nL to enable comparison of essays responding to different lecture + reading stimuli (prompts). 248 4 and 5 (EGood) and all essays at score points 1 and 2 (EBad). These data do not overlap with the experimental data described in section 4. In both definitions below, e is an essay. We therefore evaluate each content importance model for different granularities of the content unit x: n-grams for n = 1, 2, 3, 4. Table 2 shows the correlations with essay scor"
P14-2041,N10-1122,0,0.0324406,"er, such as examples, discourse markers, evaluations, introduction and conclusion, etc. Our approach allows focusing on a particular aspect of content quality, namely, selection of appropriate materials from the source. 5 such as that fish, of fish, farming is, “, fish” 250 the other two being fishing and used. the reading sometimes interpret the same facts in a positive or negative light (for example, the fact that chemicals are used in fish farms is negative if compared to wild fish, but not so if compared to other farm-raised foods like poultry). Relationships between aspect and sentiment (Brody and Elhadad, 2010; Lazaridou et al., 2013) are also relevant, since aspects of the same fact are emphasized with different evaluations (the quantity vs the variety of species that go into fish meal for farmed fish). We hypothesize that units participating in sentiment and aspect contrasts are of higher importance; this is a direction for future work. Our results are related to the findings of Gurevich and Deane (2007) who studied the difference between the reading and the lecture in their impact on essay scores for this test. Using data from a single prompt, they showed that the difference between the essay’s"
P14-2041,N04-1019,0,0.0630845,"to summarize the points made in the lecture, explaining how they cast doubt on points made in the reading. The quality of the information selected from the lecture is emphasized in excerpts from the scoring rubric for this test (below); essays are scored on a 1-5 scale: 2 Design of Experiment In evaluations of summarization algorithms, it is common practice to derive the gold standard content importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). Selection of the appropriate content plays a crucial role in attaining a high score for the essays we consider here, as suggested by the quotes from the scoring rubric in §1, as well as by a corpus study by Plakans and Gebril (2013). We therefore observe that high-scoring essays can be thought Score 5 successfully selects the important information from the lecture and coherently and accurately presents this information in relation to the relevant information presented in the reading. 1 http://www.corestandards.org/ ELA-Literacy/CCRA/W. 247 Proceedings of the 52nd Annual Meeting of the Associ"
P14-2041,W13-1721,0,0.0194633,"ssay scores for this test. Using data from a single prompt, they showed that the difference between the essay’s average cosine similarity to the reading and its average cosine similarity to the lecture is predictive of the score for non-native speakers of English, thus using a model similar to LectVsRead, although they took all lecture, reading, and essay words into account, in contrast to our model that looks only at n-grams that appear in the lecture. Our study shows that the effectiveness of lecture-reading contrast models for essay scoring generalizes to a large set of prompts. Similarly, Evanini et al. (2013) found that overlap with material that is unique to the lecture (not shared with the reading) was predictive of scores in a spoken source-based question answering task. 8 Conclusion In this paper, we addressed the task of automatically assigning importance scores to parts of a lecture that is to be summarized as part of an English language proficiency test. We investigated the optimal units of information to which importance should be assigned, as well as a variety of importance scoring models, drawing on the news summarization and essay scoring literature. We found that bigrams and trigrams w"
P14-2041,D10-1007,0,0.0432653,"Missing"
P14-2041,N07-2013,0,0.0318622,"le, the fact that chemicals are used in fish farms is negative if compared to wild fish, but not so if compared to other farm-raised foods like poultry). Relationships between aspect and sentiment (Brody and Elhadad, 2010; Lazaridou et al., 2013) are also relevant, since aspects of the same fact are emphasized with different evaluations (the quantity vs the variety of species that go into fish meal for farmed fish). We hypothesize that units participating in sentiment and aspect contrasts are of higher importance; this is a direction for future work. Our results are related to the findings of Gurevich and Deane (2007) who studied the difference between the reading and the lecture in their impact on essay scores for this test. Using data from a single prompt, they showed that the difference between the essay’s average cosine similarity to the reading and its average cosine similarity to the lecture is predictive of the score for non-native speakers of English, thus using a model similar to LectVsRead, although they took all lecture, reading, and essay words into account, in contrast to our model that looks only at n-grams that appear in the lecture. Our study shows that the effectiveness of lecture-reading"
P14-2041,E14-1075,0,0.122239,"tion, and LectVsRead models normalize by nR and nL to enable comparison of essays responding to different lecture + reading stimuli (prompts). 248 4 and 5 (EGood) and all essays at score points 1 and 2 (EBad). These data do not overlap with the experimental data described in section 4. In both definitions below, e is an essay. We therefore evaluate each content importance model for different granularities of the content unit x: n-grams for n = 1, 2, 3, 4. Table 2 shows the correlations with essay scores. Good: w(x) = |{e∈EGood|x∈e}| . An x is more im|EGood| portant if more good essays use it. Hong and Nenkova (2014) showed that a variant of this measure used on pairs of articles and their abstracts from the New York Times effectively identified words that typically go into summaries, across topics. In contrast, our measurements are prompt-specific. GoodVsBad: w(x) = |{e∈EBad|x∈e}| . |EBad| |{e∈EGood|x∈e}| |EGood| Content Importance Model Na¨ıve Prob Position LectVsRead Good GoodVsBad − An x is more important if good essays use it more than bad essays. To our knowledge, this measure has not been used in the summarization literature, probably because a large sample of human summaries of varying quality is"
P14-2041,P13-1160,0,0.0172766,"course markers, evaluations, introduction and conclusion, etc. Our approach allows focusing on a particular aspect of content quality, namely, selection of appropriate materials from the source. 5 such as that fish, of fish, farming is, “, fish” 250 the other two being fishing and used. the reading sometimes interpret the same facts in a positive or negative light (for example, the fact that chemicals are used in fish farms is negative if compared to wild fish, but not so if compared to other farm-raised foods like poultry). Relationships between aspect and sentiment (Brody and Elhadad, 2010; Lazaridou et al., 2013) are also relevant, since aspects of the same fact are emphasized with different evaluations (the quantity vs the variety of species that go into fish meal for farmed fish). We hypothesize that units participating in sentiment and aspect contrasts are of higher importance; this is a direction for future work. Our results are related to the findings of Gurevich and Deane (2007) who studied the difference between the reading and the lecture in their impact on essay scores for this test. Using data from a single prompt, they showed that the difference between the essay’s average cosine similarity"
P16-4014,H05-1103,0,0.0540262,"b-based tool that can address these needs.2 Specifically, Language Muse can help subject area teachers support ELLs by automatically generating customizable activities derived from actual texts used in their classrooms. The activities are generated using several existing NLP algorithms and are designed to help ELLs with multiple aspects of language learning needed to support content comprehension: vocabulary, grammatical structures, and discourse & text organization. Although Language Muse is related to existing work in the NLP literature on automatic question generation (Mitkov and Ha, 2003; Brown et al., 2005; Heilman and Smith, 2010), it can generate multiple activities for teachers’ own texts, cover a significantly larger set of language constructs, and offer teachers much more customizability. In subsequent sections, we first provide a description of the Language Muse NLP. Next, we describe how teachers interact with the backend and create activities. Finally, we present the results of a survey conducted with actual ELL teachers, and conclude with future work. Current education standards in the U.S. require school students to read and understand complex texts from different subject areas (e.g.,"
P16-4014,W98-0303,1,0.185667,"Missing"
P16-4014,N10-1086,0,0.0131847,"n address these needs.2 Specifically, Language Muse can help subject area teachers support ELLs by automatically generating customizable activities derived from actual texts used in their classrooms. The activities are generated using several existing NLP algorithms and are designed to help ELLs with multiple aspects of language learning needed to support content comprehension: vocabulary, grammatical structures, and discourse & text organization. Although Language Muse is related to existing work in the NLP literature on automatic question generation (Mitkov and Ha, 2003; Brown et al., 2005; Heilman and Smith, 2010), it can generate multiple activities for teachers’ own texts, cover a significantly larger set of language constructs, and offer teachers much more customizability. In subsequent sections, we first provide a description of the Language Muse NLP. Next, we describe how teachers interact with the backend and create activities. Finally, we present the results of a survey conducted with actual ELL teachers, and conclude with future work. Current education standards in the U.S. require school students to read and understand complex texts from different subject areas (e.g., social studies). However,"
P16-4014,P98-2127,0,0.0323527,"Missing"
P16-4014,W03-0203,0,0.0234022,"e, an open-access, web-based tool that can address these needs.2 Specifically, Language Muse can help subject area teachers support ELLs by automatically generating customizable activities derived from actual texts used in their classrooms. The activities are generated using several existing NLP algorithms and are designed to help ELLs with multiple aspects of language learning needed to support content comprehension: vocabulary, grammatical structures, and discourse & text organization. Although Language Muse is related to existing work in the NLP literature on automatic question generation (Mitkov and Ha, 2003; Brown et al., 2005; Heilman and Smith, 2010), it can generate multiple activities for teachers’ own texts, cover a significantly larger set of language constructs, and offer teachers much more customizability. In subsequent sections, we first provide a description of the Language Muse NLP. Next, we describe how teachers interact with the backend and create activities. Finally, we present the results of a survey conducted with actual ELL teachers, and conclude with future work. Current education standards in the U.S. require school students to read and understand complex texts from different"
P16-4014,P05-1074,0,\N,Missing
P16-4014,C98-2122,0,\N,Missing
P19-3024,W18-0530,1,0.893066,"Missing"
Q13-1009,E06-1027,0,0.0893898,"er of machine learning algorithms: SVM tion 7.2. Linear and RBF, Na¨ıve Bayes, Logistic Regression The task of assigning sentiment profiles to words (using WEKA (Hall et al., 2009)), and c5.0 Decision in a sentiment lexicon has been addressed in the liteTrees (Quinlan, 1993).5 rature. SentiWordNet assigns profiles to all words in WordNet based on a propagation algorithm from a 5.1 Data small seed set manually annotated by a small numWe generated the data for training and testing the ber of judges (Baccianella et al., 2010; Cerini et al., machine learning systems as follows. We used our 2007). Andreevskaia and Bergler (2006) use graph 5 propagation algorithms on WordNet to assign cenavailable from http://rulequest.com/ 800 −1.0 −0.5 0.0 −1.0 0.5 −0.5 1.0 0.0 0.5 1.0 800 102 pool of 100,000 essays to sample a second, nonoverlapping set of 5,000 essays, so that no essay used for lexicon development appears in this set. From these essays, we randomly sampled 550 sentences, and submitted them to sentiment polarity annotation by two experienced research assistants; 50 double-annotated sentenced showed κ=0.8. TEST set contains the 43 agreed double-annotated sentences, and additional 238 sampled from the 500 single-anno"
Q13-1009,baccianella-etal-2010-sentiwordnet,0,0.0987014,"of the observed trends, we experiment with with WordNet-based expansion as descibed in seca number of machine learning algorithms: SVM tion 7.2. Linear and RBF, Na¨ıve Bayes, Logistic Regression The task of assigning sentiment profiles to words (using WEKA (Hall et al., 2009)), and c5.0 Decision in a sentiment lexicon has been addressed in the liteTrees (Quinlan, 1993).5 rature. SentiWordNet assigns profiles to all words in WordNet based on a propagation algorithm from a 5.1 Data small seed set manually annotated by a small numWe generated the data for training and testing the ber of judges (Baccianella et al., 2010; Cerini et al., machine learning systems as follows. We used our 2007). Andreevskaia and Bergler (2006) use graph 5 propagation algorithms on WordNet to assign cenavailable from http://rulequest.com/ 800 −1.0 −0.5 0.0 −1.0 0.5 −0.5 1.0 0.0 0.5 1.0 800 102 pool of 100,000 essays to sample a second, nonoverlapping set of 5,000 essays, so that no essay used for lexicon development appears in this set. From these essays, we randomly sampled 550 sentences, and submitted them to sentiment polarity annotation by two experienced research assistants; 50 double-annotated sentenced showed κ=0.8. TEST se"
Q13-1009,P05-1074,0,0.0214597,"itive and negative sentiment words manually selected from a full list of word types in these data, and (2) words marked in a small-scale annotation of a sample of sentences from these data for all positive and negative words. A more detailed descrip100 tion of the construction of seed lexicon can be found in Beigman Klebanov et al (2012). The seed lexicon contains 749 single words, 406 positive and 343 negative. 2.2 Expanded Lexicon We used a pivot-based lexical and phrasal paraphrase generation system (Madnani and Dorr, 2013). The paraphraser implements the pivot-based method as described by Bannard and Callison-Burch (2005) with several additional filtering mechanisms to increase the precision of the extracted pairs. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: We first identify phrasal correspondences between English and a given foreign language F , then map from English to English by following translation units from English to the other language and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f , then they may be considered to be paraphrases of each other with the following probability: p(e1|e2) ≈ p(e1"
Q13-1009,W11-1716,0,0.0265561,"Missing"
Q13-1009,P12-1105,0,0.211161,"Missing"
Q13-1009,E06-1025,0,0.307381,"Missing"
Q13-1009,N09-1002,0,0.0368913,"Missing"
Q13-1009,P97-1023,0,0.0619707,"n-enrichment procedure for a sentiment clas400 400 400 400Qiu et400 400 2010; Velikovich et al., 2010; al., 2009; Mo- sification400 task. hammad et al., 2009; Esuli and Sebastiani, 2006; profiles 200 and Hovy, 2004; Andreevskaia 200 200 for sentence-level 200 200 Kim and Bergler, 5 Using 200 200 sentiment polarity classification 2006; Hu and Liu, 2004; Kanayama and Nasukawa, 2006; Strapparava and Valitutti, 2004; Kamps et al., 0 0 0 0 To evaluate 0 the usefulness of the lexicons, we 0 use 2004; Takamura et al., 2005;0 Turney and Littman, them to generate features for machine learning sys2003; Hatzivassiloglou and McKeown, 1997). The tems, and compare performance on 3-way sentenceparaphrase-based expansion method is in the dislevel sentiment polarity classification. To ensure rotributional similarity camp; we also experimented bustness of the observed trends, we experiment with with WordNet-based expansion as descibed in seca number of machine learning algorithms: SVM tion 7.2. Linear and RBF, Na¨ıve Bayes, Logistic Regression The task of assigning sentiment profiles to words (using WEKA (Hall et al., 2009)), and c5.0 Decision in a sentiment lexicon has been addressed in the liteTrees (Quinlan, 1993).5 rature. SentiW"
Q13-1009,E09-1046,0,0.021996,"available for research and education only1 and under GNU GPL license that disallows their incorporation into proprietary materials,2 respectively. 1 2 http://www.wjh.harvard.edu/ inquirer/j1 1/manual/ http://www.gnu.org/copyleft/gpl.html Those wishing to integrate sentiment analysis into products, along with those studying subjectivity in languages other than English, or for specific domains such as finance, or for particular genres such as MySpace comments, reported construction of lexicons (Taboada et al., 2011; Loughran and McDonald, 2011; Thelwall et al., 2010; Rao and Ravichandran, 2009; Jijkoun and Hofmann, 2009; Pitel and Grefenstette, 2008; Mihalcea et al., 2007). In this paper, we address the step of expanding a small-scale, manually-built subjectivity lexicon (a seed lexicon, typically for a domain or language in question) into a much larger but noisier lexicon using an automatic procedure. We present a novel expansion method using a state-of-the-art paraphrasing system. The expansion yields a 4-fold increase in lexicon size; yet, the expansion alone is insufficient in order to improve performance on sentence-level sentiment polarity classification. In this paper we test the following hypothesis."
Q13-1009,kamps-etal-2004-using,0,0.236109,"Missing"
Q13-1009,W06-1642,0,0.25578,"Missing"
Q13-1009,C04-1200,0,0.674053,"Missing"
Q13-1009,2005.mtsummit-papers.11,0,0.00782496,"y are all used in computing the probability: p(e1|e2) ≈ Seed abuse accuse anxiety conflict X f0 Expansion exploitation reproach disquiet crisis p(e1|f 0 )p(f 0 |e2) Seed costly dangerous improve invaluable Expansion onerous unsafe reinforce precious Table 1: Examples of paraphraser expansions. Some examples of expansions generated by the paraphraser are shown in Table 1. More details about this kind of approach can be found in Bannard and Callison-Burch (2005). We use the FrenchEnglish parallel corpus (approximately 1.2 million sentences) from the corpus of European parliamentary proceedings (Koehn, 2005) as the data on which pivoting is performed to extract the paraphrases. However, the base paraphrase system is susceptible to large amounts of noise due to the imperfect bilingual word alignments. Therefore, we implement additional heuristics in order to minimize the number of noisy paraphrase pairs (Madnani and Dorr, 2013). For example, one such heuristic filters out pairs where a function word may have been inferred as a paraphrase of a content word. For the lexicon expansion experiment reported here, we use the top 15 single-word paraphrases for every word from the seed lexicon, excluding m"
Q13-1009,P98-2127,0,0.147367,"Missing"
Q13-1009,P07-1123,0,0.0284025,"GPL license that disallows their incorporation into proprietary materials,2 respectively. 1 2 http://www.wjh.harvard.edu/ inquirer/j1 1/manual/ http://www.gnu.org/copyleft/gpl.html Those wishing to integrate sentiment analysis into products, along with those studying subjectivity in languages other than English, or for specific domains such as finance, or for particular genres such as MySpace comments, reported construction of lexicons (Taboada et al., 2011; Loughran and McDonald, 2011; Thelwall et al., 2010; Rao and Ravichandran, 2009; Jijkoun and Hofmann, 2009; Pitel and Grefenstette, 2008; Mihalcea et al., 2007). In this paper, we address the step of expanding a small-scale, manually-built subjectivity lexicon (a seed lexicon, typically for a domain or language in question) into a much larger but noisier lexicon using an automatic procedure. We present a novel expansion method using a state-of-the-art paraphrasing system. The expansion yields a 4-fold increase in lexicon size; yet, the expansion alone is insufficient in order to improve performance on sentence-level sentiment polarity classification. In this paper we test the following hypothesis. We suggest that the effectiveness of the expansion is"
Q13-1009,D09-1063,0,0.38205,"Missing"
Q13-1009,pitel-grefenstette-2008-semi,0,0.0206582,"education only1 and under GNU GPL license that disallows their incorporation into proprietary materials,2 respectively. 1 2 http://www.wjh.harvard.edu/ inquirer/j1 1/manual/ http://www.gnu.org/copyleft/gpl.html Those wishing to integrate sentiment analysis into products, along with those studying subjectivity in languages other than English, or for specific domains such as finance, or for particular genres such as MySpace comments, reported construction of lexicons (Taboada et al., 2011; Loughran and McDonald, 2011; Thelwall et al., 2010; Rao and Ravichandran, 2009; Jijkoun and Hofmann, 2009; Pitel and Grefenstette, 2008; Mihalcea et al., 2007). In this paper, we address the step of expanding a small-scale, manually-built subjectivity lexicon (a seed lexicon, typically for a domain or language in question) into a much larger but noisier lexicon using an automatic procedure. We present a novel expansion method using a state-of-the-art paraphrasing system. The expansion yields a 4-fold increase in lexicon size; yet, the expansion alone is insufficient in order to improve performance on sentence-level sentiment polarity classification. In this paper we test the following hypothesis. We suggest that the effective"
Q13-1009,E09-1077,0,0.0517249,"ebe and Riloff, 2005) – are available for research and education only1 and under GNU GPL license that disallows their incorporation into proprietary materials,2 respectively. 1 2 http://www.wjh.harvard.edu/ inquirer/j1 1/manual/ http://www.gnu.org/copyleft/gpl.html Those wishing to integrate sentiment analysis into products, along with those studying subjectivity in languages other than English, or for specific domains such as finance, or for particular genres such as MySpace comments, reported construction of lexicons (Taboada et al., 2011; Loughran and McDonald, 2011; Thelwall et al., 2010; Rao and Ravichandran, 2009; Jijkoun and Hofmann, 2009; Pitel and Grefenstette, 2008; Mihalcea et al., 2007). In this paper, we address the step of expanding a small-scale, manually-built subjectivity lexicon (a seed lexicon, typically for a domain or language in question) into a much larger but noisier lexicon using an automatic procedure. We present a novel expansion method using a state-of-the-art paraphrasing system. The expansion yields a 4-fold increase in lexicon size; yet, the expansion alone is insufficient in order to improve performance on sentence-level sentiment polarity classification. In this paper we tes"
Q13-1009,strapparava-valitutti-2004-wordnet,0,0.195547,"Missing"
Q13-1009,W08-1207,0,0.0328751,"Missing"
Q13-1009,P05-1017,0,0.0938726,"xicon, showing the effectiveness of this ture thereof (Cruz et al., 2011; Baccianella et al., lexicon-enrichment procedure for a sentiment clas400 400 400 400Qiu et400 400 2010; Velikovich et al., 2010; al., 2009; Mo- sification400 task. hammad et al., 2009; Esuli and Sebastiani, 2006; profiles 200 and Hovy, 2004; Andreevskaia 200 200 for sentence-level 200 200 Kim and Bergler, 5 Using 200 200 sentiment polarity classification 2006; Hu and Liu, 2004; Kanayama and Nasukawa, 2006; Strapparava and Valitutti, 2004; Kamps et al., 0 0 0 0 To evaluate 0 the usefulness of the lexicons, we 0 use 2004; Takamura et al., 2005;0 Turney and Littman, them to generate features for machine learning sys2003; Hatzivassiloglou and McKeown, 1997). The tems, and compare performance on 3-way sentenceparaphrase-based expansion method is in the dislevel sentiment polarity classification. To ensure rotributional similarity camp; we also experimented bustness of the observed trends, we experiment with with WordNet-based expansion as descibed in seca number of machine learning algorithms: SVM tion 7.2. Linear and RBF, Na¨ıve Bayes, Logistic Regression The task of assigning sentiment profiles to words (using WEKA (Hall et al., 200"
Q13-1009,N03-1033,0,0.0138253,"Missing"
Q13-1009,N10-1119,0,0.0992446,"Missing"
Q13-1009,P06-1134,0,0.0744594,"Missing"
Q13-1009,J11-2001,0,\N,Missing
Q13-1009,C98-2122,0,\N,Missing
S12-1076,S12-1051,0,0.419016,"e translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pairs. See the task description (Agirre et al., 2012) for additional details. 3 TER, TERp, and PERP In this section, we briefly describe the TER and TERp machine translation metrics, and how the PERP system extends them in order to better model semantic textual similarity. TER (Snover et al., 2006) uses a greedy search algorithm to find a set of edits to convert one of the paired input sentences into the other. We can view this set of edits as an alignment a between the two input sentences x1 and x2 , and when two words in x1 and x2 , respectively, are part of an edit operation, we say that those words are aligned.1 Unlike tradi1 For machine tra"
S12-1076,P05-1074,0,0.0491564,"An example of a PERP alignment for a sentence pair from the Microsoft Research Paraphrase Corpus. The search algorithm first performs shifts on x1 and then performs other edits on x2 . The zero cost edits that match individual words are not shown. reference. It does so by looking up—in a precomputed phrase table—paraphrases of phrases in the reference and using its associated edit cost as the cost of performing a match against the hypothesis. The paraphrase table used in PERP was identical to the one used by Snover et al. (2009a). It was extracted using the pivot-based method as described by Bannard and Callison-Burch (2005) with several additional filtering mechanisms to increase the precision of the extracted pairs. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: we first identify phrasal correspondences between English and a given foreign language F , then map from English to English by following translation units from English to the other language and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f , then they may be considered to be paraphrases of each other with the following probability: p(e1|e2) ≈ p(e1"
S12-1076,P11-1020,0,0.0476123,"PERP, we discuss how it was applied for the SemEval 2012 Semantic Textual Similarity (STS) task. 2 Problem Definition In this work, our goal is to create a system that can take as input two sentences (or short texts) x1 and x2 and produce as output a prediction yˆ for how similar they are. Here, we use the 0 to 5 ordinal scale from the STS task, where increasing values indicate greater semantic similarity. The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pair"
S12-1076,W02-1001,0,0.182733,"rithm. For example, there is a single feature for lexical substitution, even though it is clear that different types of substitutions have different effects on similarity (e.g., substituting “43.6” with “17” versus substituting “a” for “an”). In addition, the heuristic learning algorithm, which involves perturbing the weight vector by small amounts as in grid search, seems unscalable to larger sets of overlapping features. Therefore, here, we use TERp’s inference algorithms that find low cost edit sequences but use a discriminative learning algorithm based on the Perceptron (Rosenblatt, 1958; Collins, 2002) to estimate edit cost parameters, along with an expanded feature set for broader coverage of the phenomena that are relevant to sentence-to-sentence similarity. We 529 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 529–535, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics refer to this new approach as Paraphrase Edit Rate with the Perceptron (PERP). In addition to describing PERP, we discuss how it was applied for the SemEval 2012 Semantic Textual Similarity (STS) task. 2 Problem Definition In this work, our goal is to create a s"
S12-1076,C04-1051,0,0.0765564,"se Edit Rate with the Perceptron (PERP). In addition to describing PERP, we discuss how it was applied for the SemEval 2012 Semantic Textual Similarity (STS) task. 2 Problem Definition In this work, our goal is to create a system that can take as input two sentences (or short texts) x1 and x2 and produce as output a prediction yˆ for how similar they are. Here, we use the 0 to 5 ordinal scale from the STS task, where increasing values indicate greater semantic similarity. The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all fi"
S12-1076,2005.mtsummit-papers.11,0,0.00481045,"Problem Definition In this work, our goal is to create a system that can take as input two sentences (or short texts) x1 and x2 and produce as output a prediction yˆ for how similar they are. Here, we use the 0 to 5 ordinal scale from the STS task, where increasing values indicate greater semantic similarity. The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pairs. See the task description (Agirre et al., 2012) for additional details. 3 TER, TERp, an"
S12-1076,W08-0902,0,0.325391,"al., 2009a), by adding support for more detailed feature types and by implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline. 1 Introduction Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (Nielsen et al., 2008; Leacock and Chodorow, 2003), question answering (Wang et al., 2007), machine translation evaluation (Przybocki et al., 2009; Snover et al., 2009a), etc. An important aspect of this problem is that similarity is not binary. Sentences can be very semantically similar, such that they might be called paraphrases of each other. They might be completely different. Or, they might be somewhere in between. Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity. Therefore, it is desirable to develop methods that model sentence pair similarit"
S12-1076,N09-4006,0,0.0759726,". The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pairs. See the task description (Agirre et al., 2012) for additional details. 3 TER, TERp, and PERP In this section, we briefly describe the TER and TERp machine translation metrics, and how the PERP system extends them in order to better model semantic textual similarity. TER (Snover et al., 2006) uses a greedy search algorithm to find a set of edits to convert one of the paired input sentences into the other"
S12-1076,2006.amta-papers.25,0,0.0499626,"e two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pairs. See the task description (Agirre et al., 2012) for additional details. 3 TER, TERp, and PERP In this section, we briefly describe the TER and TERp machine translation metrics, and how the PERP system extends them in order to better model semantic textual similarity. TER (Snover et al., 2006) uses a greedy search algorithm to find a set of edits to convert one of the paired input sentences into the other. We can view this set of edits as an alignment a between the two input sentences x1 and x2 , and when two words in x1 and x2 , respectively, are part of an edit operation, we say that those words are aligned.1 Unlike tradi1 For machine translation evaluation with TERp and PERP, x1 is a system’s hypothesis and x2 is a reference translation. For 530 tional edit distance measures, TER allow for shifts— that is, edits that change the positions of words or phrases in the input sentence"
S12-1076,W09-0441,1,0.839612,"applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline. 1 Introduction Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (Nielsen et al., 2008; Leacock and Chodorow, 2003), question answering (Wang et al., 2007), machine translation evaluation (Przybocki et al., 2009; Snover et al., 2009a), etc. An important aspect of this problem is that similarity is not binary. Sentences can be very semantically similar, such that they might be called paraphrases of each other. They might be completely different. Or, they might be somewhere in between. Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity. Therefore, it is desirable to develop methods that model sentence pair similarity on a continuous, or at least ordinal, scale. In this paper, we describe a system for measuring the semantic similarity of pairs of short texts."
S12-1076,D07-1003,0,0.0643477,"implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline. 1 Introduction Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (Nielsen et al., 2008; Leacock and Chodorow, 2003), question answering (Wang et al., 2007), machine translation evaluation (Przybocki et al., 2009; Snover et al., 2009a), etc. An important aspect of this problem is that similarity is not binary. Sentences can be very semantically similar, such that they might be called paraphrases of each other. They might be completely different. Or, they might be somewhere in between. Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity. Therefore, it is desirable to develop methods that model sentence pair similarity on a continuous, or at least ordinal, scale. In this paper, we desc"
S13-1013,S12-1051,0,0.054577,"c (the OnWn and FNWN datasets were smaller than the headlines and SMT datasets). Introduction We aim to develop an automatic measure of the semantic similarity between two short texts (e.g., sentences). Such a measure could be useful for various applications, including automated short answer scoring (Leacock and Chodorow, 2003; Nielsen et al., 2008), question answering (Wang et al., 2007), ∗ System description papers for this task were required to have a team ID and task ID (e.g., “HENRY-CORE”) as a prefix. The suggested training data for the 2013 STS task was the data from the 2012 STS task (Agirre et al., 2012), including both the training and test sets for that year. The 2012 task was similar except that the data were from a different set of subtasks: measuring similarity between sentences from the Microsoft Research Paraphrase corpus (Dolan et al., 2004) (“MSRpar”), between sentences from the Microsoft Research Video Description corpus (Chen and Dolan, 2011) (“MSRvid”), and between human and machine translations of parliamentary 96 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 96–102, Atlanta, Georgi"
S13-1013,S13-1004,0,0.0308605,"new domains. Therefore, for the three variations of our system that we were allowed to submit, we used stacking (Wolpert, 1992) to combine PERP with word and ngram features and applied the domain adaptation approach outlined by Daume III (2007) to facilitate generalization to new domains. Our submissions performed well at most subtasks, particularly at measuring the similarity of news headlines, where one of our submissions ranked 2nd among 89 from 34 teams, but there is still room for improvement. In this paper, we describe our submissions to the 2013 Semantic Textual Similarity (STS) task (Agirre et al., 2013), which evaluated implementations of text-to-text similarity measures. Submissions were evaluated according to Pearson correlations between gold standard similarity values acquired from human raters and machine-produced similarity values. Teams were allowed to submit up to three submissions. For each submission, correlations were calculated separately for four subtasks: measuring similarity between news headlines (“headlines”), between machine translation outputs and human reference translations (“SMT”), between word glosses from OntoNotes (Pradhan and Xue, 2009) and WordNet (Fellbaum, 1998) ("
S13-1013,S12-1059,0,0.123348,"Missing"
S13-1013,P11-1020,0,0.0121907,"estion answering (Wang et al., 2007), ∗ System description papers for this task were required to have a team ID and task ID (e.g., “HENRY-CORE”) as a prefix. The suggested training data for the 2013 STS task was the data from the 2012 STS task (Agirre et al., 2012), including both the training and test sets for that year. The 2012 task was similar except that the data were from a different set of subtasks: measuring similarity between sentences from the Microsoft Research Paraphrase corpus (Dolan et al., 2004) (“MSRpar”), between sentences from the Microsoft Research Video Description corpus (Chen and Dolan, 2011) (“MSRvid”), and between human and machine translations of parliamentary 96 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 96–102, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics proceedings (“SMTeuroparl”). The 2012 task provided training and test sets for those three subtasks and also included two additional tasks with just test sets: a similar OnWN task, and measuring similarity between human and machine translations of news broadcasts (“SMTnews”). Heilman and"
S13-1013,P07-1033,0,0.396859,"Missing"
S13-1013,C04-1051,0,0.032279,"cations, including automated short answer scoring (Leacock and Chodorow, 2003; Nielsen et al., 2008), question answering (Wang et al., 2007), ∗ System description papers for this task were required to have a team ID and task ID (e.g., “HENRY-CORE”) as a prefix. The suggested training data for the 2013 STS task was the data from the 2012 STS task (Agirre et al., 2012), including both the training and test sets for that year. The 2012 task was similar except that the data were from a different set of subtasks: measuring similarity between sentences from the Microsoft Research Paraphrase corpus (Dolan et al., 2004) (“MSRpar”), between sentences from the Microsoft Research Video Description corpus (Chen and Dolan, 2011) (“MSRvid”), and between human and machine translations of parliamentary 96 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 96–102, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics proceedings (“SMTeuroparl”). The 2012 task provided training and test sets for those three subtasks and also included two additional tasks with just test sets: a similar OnWN task, a"
S13-1013,S12-1076,1,0.528062,"olan, 2011) (“MSRvid”), and between human and machine translations of parliamentary 96 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 96–102, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics proceedings (“SMTeuroparl”). The 2012 task provided training and test sets for those three subtasks and also included two additional tasks with just test sets: a similar OnWN task, and measuring similarity between human and machine translations of news broadcasts (“SMTnews”). Heilman and Madnani (2012) described the PERP system and submitted it to the 2012 STS task. PERP measures the similarity of a sentence pair by finding a sequence of edit operations (e.g., insertions, deletions, substitutions, and shifts) that converts one sentence to the other. It then uses various features of the edits, with weights learned from labeled sentence pairs, to assign a similarity score. PERP performed well, ranking 7th out of 88 submissions from 35 teams according to the weighted mean correlation. However, PERP lacked some of the useful word and n-gram overlap features included in some of the other top-per"
S13-1013,W08-0902,0,0.340993,"ses from OntoNotes (Pradhan and Xue, 2009) and WordNet (Fellbaum, 1998) (“OnWN”), and between frame descriptions from FrameNet (Fillmore et al., 2003) and glosses from WordNet (“FNWN”). A weighted mean of the correlations was also computed as an overall evaluation metric (the OnWn and FNWN datasets were smaller than the headlines and SMT datasets). Introduction We aim to develop an automatic measure of the semantic similarity between two short texts (e.g., sentences). Such a measure could be useful for various applications, including automated short answer scoring (Leacock and Chodorow, 2003; Nielsen et al., 2008), question answering (Wang et al., 2007), ∗ System description papers for this task were required to have a team ID and task ID (e.g., “HENRY-CORE”) as a prefix. The suggested training data for the 2013 STS task was the data from the 2012 STS task (Agirre et al., 2012), including both the training and test sets for that year. The 2012 task was similar except that the data were from a different set of subtasks: measuring similarity between sentences from the Microsoft Research Paraphrase corpus (Dolan et al., 2004) (“MSRpar”), between sentences from the Microsoft Research Video Description corp"
S13-1013,N09-4006,0,0.0134994,"c Textual Similarity (STS) task (Agirre et al., 2013), which evaluated implementations of text-to-text similarity measures. Submissions were evaluated according to Pearson correlations between gold standard similarity values acquired from human raters and machine-produced similarity values. Teams were allowed to submit up to three submissions. For each submission, correlations were calculated separately for four subtasks: measuring similarity between news headlines (“headlines”), between machine translation outputs and human reference translations (“SMT”), between word glosses from OntoNotes (Pradhan and Xue, 2009) and WordNet (Fellbaum, 1998) (“OnWN”), and between frame descriptions from FrameNet (Fillmore et al., 2003) and glosses from WordNet (“FNWN”). A weighted mean of the correlations was also computed as an overall evaluation metric (the OnWn and FNWN datasets were smaller than the headlines and SMT datasets). Introduction We aim to develop an automatic measure of the semantic similarity between two short texts (e.g., sentences). Such a measure could be useful for various applications, including automated short answer scoring (Leacock and Chodorow, 2003; Nielsen et al., 2008), question answering"
S13-1013,S12-1060,0,0.0763839,"Missing"
S13-1013,D07-1003,0,0.0284571,"and WordNet (Fellbaum, 1998) (“OnWN”), and between frame descriptions from FrameNet (Fillmore et al., 2003) and glosses from WordNet (“FNWN”). A weighted mean of the correlations was also computed as an overall evaluation metric (the OnWn and FNWN datasets were smaller than the headlines and SMT datasets). Introduction We aim to develop an automatic measure of the semantic similarity between two short texts (e.g., sentences). Such a measure could be useful for various applications, including automated short answer scoring (Leacock and Chodorow, 2003; Nielsen et al., 2008), question answering (Wang et al., 2007), ∗ System description papers for this task were required to have a team ID and task ID (e.g., “HENRY-CORE”) as a prefix. The suggested training data for the 2013 STS task was the data from the 2012 STS task (Agirre et al., 2012), including both the training and test sets for that year. The 2012 task was similar except that the data were from a different set of subtasks: measuring similarity between sentences from the Microsoft Research Paraphrase corpus (Dolan et al., 2004) (“MSRpar”), between sentences from the Microsoft Research Video Description corpus (Chen and Dolan, 2011) (“MSRvid”), an"
S13-2046,P07-1033,0,0.107406,"Missing"
S13-2046,N12-1021,0,0.0355973,"ernational Workshop on Semantic c Evaluation (SemEval 2013), pages 275–279, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics ing different science domains: the Beetle dataset, which pertains to basic electricity and electronics (Dzikovska et al., 2010), and the Science Entailments corpus (SciEntsBank) (Nielsen et al., 2008), which covers a wider range of scientific topics. Responses were organized into five categories: correct, partially correct, contradictory, irrelevant, and non-domain. The SciEntsBank responses were converted to this format as described by Dzikovska et al. (2012). The Beetle training data had about 4,000 student answers to 47 questions. The SciEntsBank training data had about 5,000 prescored student answers to 135 questions from 12 domains (different learning modules). For each item, one or more model responses were provided by the task organizers. There were three different evaluation scenarios: “unseen answers”, for scoring new answers to items represented in the training data; “unseen questions”, for scoring answers to new items from domains represented in the training data; and “unseen domains”, for scoring answers to items from new domains (only"
S13-2046,S13-2045,0,0.483465,"omain, in addition to any machine learning required for the overall system. In this paper, we describe a machine learning approach to short answer scoring that allows us to incorporate both item-specific and general features by using the domain adaptation technique of Daume III (2007). In addition, the approach employs stacking (Wolpert, 1992) to support the integration of components that require tuning or machine learning. 2 Task Overview In this section, we describe the task to which we applied our system: the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013), which was task 7 at SemEval 2013. The aim of the task is to classify student responses to assessment items from two datasets represent275 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 275–279, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics ing different science domains: the Beetle dataset, which pertains to basic electricity and electronics (Dzikovska et al., 2010), and the Science Entailments corpus (SciEntsBank) (Nielsen et al., 2008), which c"
S13-2046,S12-1076,1,0.923125,", human-scored responses) may or may not be available due to the variety of possible questions and topics. As such, it seems desirable to integrate various approaches, making use of model answers from experts (e.g., to give higher scores to responses that are similar), prescored student responses (e.g., to learn direct associations between particular phrases and scores), etc. Here, we describe a system that uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measures (Heilman and Madnani, 2012). We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. 1 Introduction In this paper, we address the problem of automatically scoring short text responses to educational assessment items for measuring content knowledge. Many approaches can be and have been taken to this problem—e.g., Leacock and Chodorow (2003), Nielsen et al. (2008), inter alia. The effectiveness of any particular approach likely depends on the the availability of data (among other factors). For example, if thousands of prescored responses are avail∗ System de"
S13-2046,S13-1013,1,0.896125,"Missing"
S13-2046,W08-0902,0,0.161041,"hat uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measures (Heilman and Madnani, 2012). We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge. 1 Introduction In this paper, we address the problem of automatically scoring short text responses to educational assessment items for measuring content knowledge. Many approaches can be and have been taken to this problem—e.g., Leacock and Chodorow (2003), Nielsen et al. (2008), inter alia. The effectiveness of any particular approach likely depends on the the availability of data (among other factors). For example, if thousands of prescored responses are avail∗ System description papers for SemEval 2013 are required to have a team ID (e.g., “ETS”) as a prefix. able, then a simple classifier using n-gram features may suffice. However, if only model answers (i.e., reference answers) or rubrics are available, more general semantic similarity measures (or even rulebased approaches) would be more effective. It seems likely that, in many cases, there will be model answer"
S13-2046,P02-1040,0,0.0890113,"o 10,000 bins by hashing and using a modulo operation (i.e., the “hashing trick”) (Weinberger et al., 2009). • lowercased character n-grams in the response text for n ∈ {5, 6, 7, 8} 2 At the time of writing, the baseline code could be downloaded at http://www.cs.york.ac.uk/ semeval-2013/task7/. 3.1.4 Text Similarity Features The system includes the following text similarity features that compare the student response either to a) the reference answers for the appropriate item, or b) the student answers in the training set that are labeled “correct”. • the maximum of the smoothed, uncased BLEU (Papineni et al., 2002) scores obtained by comparing the student response to each correct reference answer. We also include the word n-gram precision and recall values for n ∈ {1, 2, 3, 4} for the maximally similar reference answer. • the maximum of the smoothed, uncased BLEU scores obtained by comparing the student response to each correct training set student answer. We also include the word n-gram precision and recall values for n ∈ {1, 2, 3, 4} for the maximally similar student answer. • the maximum PERP (Heilman and Madnani, 2012) score obtained by comparing the student response to the correct reference answers"
W07-0716,P02-1038,0,0.0853391,"In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality. 1 Introduction Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003). Since their introduction in statistical MT by Och and Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems. Typically such a model takes the form X λi φi (f¯, e¯) (1) i where φi are features of the hypothesis e and λi are weights associated with those features. Selecting appropriate weights λi is essential in order to obtain good translation performance. Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear Minimum error rate training—and more generally, optimization of parameters relative to a translation quality measure—relies on data sets in which source language sentences a"
W07-0716,W05-0909,0,0.06195,",nfa,resnik,bonnie}@umiacs.umd.edu Abstract model parameters relative to a measure of translation quality. This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously. Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can b"
W07-0716,P05-1074,0,0.601747,"Missing"
W07-0716,N03-1003,0,0.572191,"Missing"
W07-0716,N06-1003,0,0.538654,"Missing"
W07-0716,P00-1056,0,0.104185,"The underlying strength of a hierarchical phrase is that it allows for effective learning of not only the lexical re-orderings, but 121 phrasal re-orderings, as well. Each φ(¯ e, f¯, X) denotes a feature function defined on the pair of hierarchical phrases.1 Feature functions represent conditional and joint co-occurrence probabilities over the hierarchical paraphrase pair. The Hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data. Briefly, training a Hiero model proceeds as follows: • GIZA++ (Och and Ney, 2000) is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields a many-to-many alignment for each parallel sentence. • Initial phrase pairs are identified following the procedure typically employed in phrase based systems (Koehn et al., 2003; Och and Ney, 2004). • Grammar rules in the form of equation (2) are induced by “subtracting” out hierarchical phrase pairs from these initial phrase pairs. • Fractional counts are assigned to each produced rule: c(X → h¯ e, f¯i) = m X 1 j=1 njr (3) where m is the number of initial phrase pairs that give rise to"
W07-0716,J04-4002,0,0.0597953,"nt co-occurrence probabilities over the hierarchical paraphrase pair. The Hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data. Briefly, training a Hiero model proceeds as follows: • GIZA++ (Och and Ney, 2000) is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields a many-to-many alignment for each parallel sentence. • Initial phrase pairs are identified following the procedure typically employed in phrase based systems (Koehn et al., 2003; Och and Ney, 2004). • Grammar rules in the form of equation (2) are induced by “subtracting” out hierarchical phrase pairs from these initial phrase pairs. • Fractional counts are assigned to each produced rule: c(X → h¯ e, f¯i) = m X 1 j=1 njr (3) where m is the number of initial phrase pairs that give rise to this grammar rule and njr is the number of grammar rules produced by the j th initial phrase pair. • Feature functions φk1 (f¯, e¯, X) are calculated for each rule using the accumulated counts. Once training has taken place, minimum error rate training (Och, 2003) is used to tune the parameters λi . Fina"
W07-0716,P03-1021,0,0.0625549,"on Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003). Since their introduction in statistical MT by Och and Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems. Typically such a model takes the form X λi φi (f¯, e¯) (1) i where φi are features of the hypothesis e and λi are weights associated with those features. Selecting appropriate weights λi is essential in order to obtain good translation performance. Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear Minimum error rate training—and more generally, optimization of parameters relative to a translation quality measure—relies on data sets in which source language sentences are paired with (sets of) reference translations. It is widely agreed that, at least for the widely used BLEU criterion, which is based on n-gram overlap between hypotheses and reference translations, the criterion is most accurate when computed with as many distinct reference translations as possible. Intuitively this makes sense: if there"
W07-0716,N03-1024,0,0.450807,"Missing"
W07-0716,P02-1040,0,0.118446,"ion Nitin Madnani, Necip Fazil Ayan, Philip Resnik & Bonnie J. Dorr Institute for Advanced Computer Studies University of Maryland College Park, MD, 20742 {nmadnani,nfa,resnik,bonnie}@umiacs.umd.edu Abstract model parameters relative to a measure of translation quality. This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously. Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper,"
W07-0716,W04-3219,0,0.482508,"Missing"
W07-0716,J07-2003,0,0.20005,"ique for paraphrasing, designed with the application to parameter tuning in mind. Section 4 presents evaluation results using a state of the art statistical MT system, demonstrating that half the human reference translations in a standard 4-reference tuning set can be replaced with automatically generated paraphrases, with no significant decrease in MT system performance. In Section 5 we discuss related work, and in Section 6 we summarize the results and discuss plans for future research. 2 Translation Framework The work described in this paper makes use of the Hiero statistical MT framework (Chiang, 2007). Hiero is formally based on a weighted synchronous context-free grammar (CFG), containing synchronous rules of the form X → h¯ e, f¯, φk1 (f¯, e¯, X)i (2) where X is a symbol from the nonterminal alphabet, and e¯ and f¯ can contain both words (terminals) and variables (nonterminals) that serve as placeholders for other phrases. In the context of statistical MT, where phrase-based models are frequently used, these synchronous rules can be interpreted as pairs of hierarchical phrases. The underlying strength of a hierarchical phrase is that it allows for effective learning of not only the lexic"
W07-0716,2006.amta-papers.25,1,0.912501,".umd.edu Abstract model parameters relative to a measure of translation quality. This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously. Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically"
W07-0716,W03-1608,0,0.16138,"Missing"
W07-0716,strassel-etal-2006-integrated,0,0.0201083,"count as possible. To do otherwise is to risk the possibility that the criterion might judge good translations to be poor when they fail to match the exact wording within the reference translations that have been provided. This reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. A common source of translated data for MT research is the Linguistic Data Consortium (LDC), where an elaborate process is undertaken that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Some 120 Proceedings of the Second Workshop on Statistical Machine Translation, pages 120–127, c Prague, June 2007. 2007 Association for Computational Linguistics efforts have been made to develop alternative processes for eliciting translations, e.g., from users on the Web (Oard, 2003) or from informants in lowdensity languages (Probst et al., 2002). However, reference translations for parameter tuning and evaluation remain a severe data bottleneck for such approaches. Note, however, one crucial property of reference translations: they are paraphrases, i.e., multiple expressions of the same"
W07-0716,N03-1017,0,0.254743,"nslations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality. 1 Introduction Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003). Since their introduction in statistical MT by Och and Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems. Typically such a model takes the form X λi φi (f¯, e¯) (1) i where φi are features of the hypothesis e and λi are weights associated with those features. Selecting appropriate weights λi is essential in order to obtain good translation performance. Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear Minimum error rate training—and more generally, optimization of parameters relative to a translation qualit"
W07-0716,P06-1091,0,0.0340642,"we have an example input and a single labeled, correct output. However, this output is chosen from a space in which the number of possible outputs is exponential in the input size, and in which there are many good outputs in this space (although they are vastly outnumbered by the bad outputs). Various discriminative learning methods have attempted to deal with the first of these issues, often by restricting the space of examples. For instance, some max-margin methods restrict their computations to a set of examples from a “feasible set,” where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). The present approach deals with the second issue: in a learning problem where the use of a single positive example is likely to be highly biased, how can we produce a set of positive examples that is more representative of the space of correct outcomes? Our method exploits alternative sources of information to produce new positive examples that are, we hope, reasonably likely to represent a consensus of good examples. Quite a bit of work has been done on paraphrase, 6 We anticipate doing significance tests for differences in TER in future work. some clearly related to our technique, although"
W07-0716,W04-3250,0,0.0352334,"d in expanding the reference set via paraphrase: • Expanded (4H + 4P): This is the same as Condition 2, but using all four human references. Note that since we have only four human references per item, this fourth condition does not permit comparison with an upper bound of eight human references. Table 4 shows BLEU and TER scores on the test set for all four conditions.5 If only two human references were available (simulated by using only two of the available four), expanding to four using paraphrases would yield a clear improvement. Using bootstrap resampling to compute confidence intervals (Koehn, 2004), we find that the improvement in BLEU score is statistically significant at p < .01. Equally interesting, expanding the number of reference translations from two to four using paraphrases yields performance that approaches the upper bound obtained by doing MERT using all four human reference translations. The difference in BLEU between conditions 2 and 3 is not significant. Finally, our fourth condition asks whether it is possible to improve MT performance given the typical four human reference translations used for MERT in most statistical MT systems, by adding a paraphrase to each one for a"
W07-0716,2006.iwslt-papers.3,0,0.0336729,"Missing"
W07-0716,N06-1057,0,0.159707,"Missing"
W07-2312,P06-1049,0,0.613813,". O’Learya,b Judith D. Schlesingerd a Department of Computer Science b Institute for Advanced Computer Studies University of Maryland, College Park {nmadnani,nfa,bonnie,oleary}@cs.umd.edu c Center for Computational Learning Systems, Columbia University becky@cs.columbia.edu d IDA/Center for Computing Sciences {conroy,judith}@super.org e College of Information Studies, University of Maryland, College Park jklavans@umd.edu While a good ordering is essential for summary comprehension (Barzilay et al., 2002), and recent The issue of sentence ordering is an important one work on sentence ordering (Bollegala et al., 2006) for natural language tasks such as multi-document does show promise, it is important to note that desummarization, yet there has not been a quantitatermining an optimal sentence ordering for a given tive exploration of the range of acceptable sentence summary may not be feasible. The question for orderings for short texts. We present results of a evaluation of ordering is whether there is a single sentence reordering experiment with three experibest ordering that humans will converge on, or that mental conditions. Our findings indicate a very high would lead to maximum reading comprehension,"
W07-2312,W98-1507,0,0.0368907,"timates of the expected values within each cell. Given a confusion matrix where the cells on the matrix diagonal are denoted as nii , the row marginals as ni+ , the column marginals as n+i and the matrix total as n++ , the formula for κ is: Variability across Experimental Conditions To measure the variability across the experimental conditions, we developed two methods that assign a global score to each set of reorderings by comparing them to a particular reference point. 4.1 Method 1: Confusion Matrices and κ In NLP evaluation, confusion matrices have typically been used in annotation tasks (Bruce and Wiebe (1998), Tomuro (2001)) where the matrix represents the comparison of two judges, and the κ inter-annotator agreement metric value (Cohen, 1960) gives a measure of the amount of agreement between the two judges, after factoring out chance. However, κ has been used to quantify the observed distribution in confusion matrices of other types in a range of other fields and applications (e.g., assessing map accuracy (Hardin, 1999), or optical recognition (Ross et al., 2002)). Here we use it to quantify variability within sets of reorderings for a summary. Given a representation of each summary as a set of"
W07-2312,W05-1621,0,0.241703,"ber is a significant factor in predicting mean τ scores, we can conclude that the 9 summaries differ from each other in terms of the variability among individuals. As in the earlier ANOVA presented in Table 1, we use Tukey’s HSD to determine the magnitude of the difference in means that is necessary for statistical significance, and use this to identify which summaries have significant differences in the amount of similarity among subjects’ reorderings. Applying Tukey’s method to summary number as a factor yields the differences shown in Table 2. 6 Related Work on Evaluating Sentence Ordering Karamanis and Mellish (2005) also measure the amount of variability between human subjects. However, there are several dimensions of contrast between our experiment and theirs: Their experiment operates in a very distinct domain (archaeology) and genre (descriptions of museum artifacts) whereas we use domain-independent multidocument summaries derived from news articles. We use ordinary, English-speaking volunteers as compared to the domain and genre experts that they employ (archaeologists trained in museum labeling). In terms of the experimental design, we use a Latin square design with three experimental condi86 tions"
W07-2312,P03-1069,0,0.678408,"; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that the sum of the distances between adjacent sentences is minimized. The distance (cjk ) between any pair of sentences j and k is computed by first obtaining a similarity score (bjk ) for the pair, and then normalizing this score: bjk , (cjj = 0) cjk = 1 − p"
W07-2312,J06-4002,0,0.267326,"sum to 6, κ ranges from 1 to 0, with 1 indicating that the set of reorderings all reproduce the initial ordering, and 0 indicating that the set of reorderings conforms to chance. 1 2 6 6 2 2 6 4 2 3 1 1 3 3 1 2 3 4 2 2 4 4 2 3 4 5 3 3 5 5 3 4 5 6 4 4 6 6 4 5 6 1 5 5 1 1 5 3 Figure 3: A hypothetical example illustrating Means Vectors compute means vectors for each condition for each summary, giving 27 such vectors. We compare each means vector representing a set of reorderings to each initial ordering O, R and T using three correlation coefficients: Pearson’s r, Spearman’s ρ, and Kendall’s τ (Lapata, 2006). The three correlation coefficients test the closeness of two series of numbers, or two variables x and y, in different ways. Pearson’s r is a parametric test of whether there is a perfect linear relation between the two variables. Spearman’s ρ and Kendall’s τ are non-parametric tests. Spearman’s ρ is computed by replacing the variable values by their rank and computing the correlation. Kendall’s τ is based on counting the number of pairs xi , xi+1 and yi , yi+1 where the deltas of both pairs have the same sign. In sum, the three metrics test whether x and y are in a linear relation, a rank-p"
W07-2312,C04-1108,0,0.578699,"in Section 8. 2 Sentence Ordering Algorithms A number of approaches have been applied to sentence ordering for multi-document summarization (Radev and McKeown, 1999). The first techniques exploited chronological information in the documents (McKeown et al., 1999; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that t"
W07-2312,P04-1051,0,0.118959,"Missing"
W07-2312,J98-3005,0,\N,Missing
W07-2312,I05-1055,0,\N,Missing
W08-0209,W08-0208,0,0.329,"orts high-level dynamic data types such as lists and hashes (termed dictionaries in Python), has very readable syntax and, most importantly, ships with an extensive standard library for almost every conceivable task. Although Python already has most of the functionality needed to perform very simple NLP tasks, its still not powerful enough for most standard ones. This is where the Natural Language Toolkit (NLTK) comes in. NLTK1 , written entirely in Python, is a collection of modules and corpora, released under an open-source license, that allows students to learn and conduct research in NLP (Bird et al., 2008). The most important advantage of using NLTK is that it is entirely self-contained. Not only does it provide convenient functions and wrappers that can be used as building blocks for common NLP tasks, it also provides raw and pre-processed versions of standard corpora used frequently in NLP literature. Together, Python and NLTK constitute one of the most potent tools for instruction of NLP (Madnani, 2007) and allow us to develop hands-on assignments that can appeal to a broad audience including both linguistics and computer science students. 1 http://nltk.org Figure 1: An Excerpt from the outp"
W08-0209,J00-4006,0,0.0068115,"tudents to gain insight into this important, but often omitted, idea from computational linguistics. Given the success that we had in our first attempt to re-engineer the introductory NLP course, we plan to continue: (1) our hands-on approach to programming assignments in the NLTK framework and, (2) our practice of adapting ideas from research publications as the bases for assignment and examination problems. Below we describe two concrete ideas for the next iteration of the course. 1. Hands-on Statistical Language Modeling. For this topic, we have so far restricted ourselves to the textbook (Jurafsky and Martin, 2000); the in-class discussion and programming assignments have been missing a hands-on component. We have written a Python interface to the SRI Language Modeling toolkit (Stolcke, 2002) for use in our research work. This interface uses the Simplified Wrapper & Interface Generator (SWIG) to generate a Python wrapper around our C code that does all the heavy lifting via the SRILM libraries. We are currently working on integrating this module into NLTK which would allow all NLTK users, including our students in the next version of the course, to build and query statistical language models directly in"
W08-0209,W02-0109,0,0.0918414,"ramming Framework. The previous version of our introductory course took a more fragmented approach and used different programming languages and tools for different assignments. For example, we used an in-house HMM library written in C for any HMM-based assignments and Perl for some other assignments. As expected, such an approach requires students to familiarize themselves with a different programming interface for each assignment and discourages students to explore on their own. To address this concern, we chose the Python (Python, 2007) programming language and the Natural Language Toolkit (Loper and Bird, 2002), written entirely in Python, for all our assignments and programming tasks. We discuss our use of NLTK in more detail in the next section. • Real-world Data & Corpora. In our previous course, students did not have access to any of the corpora that are used in actual NLP research. We found this to be a serious shortcoming and wanted to ensure that our new curriculum allowed students to use real corpora for evaluating their programming assignments. • Exposure to Research. While we had certainly made it a point to introduce recent research work in our lectures for all topics in the previous cour"
W08-0209,P93-1024,0,0.0168625,"offer such comparisons in the future. 6 Future Plans the same distribution, i.e., the same set of words or in the same context in a corpus, tend to have similar meanings. This is an extremely popular concept in corpus linguistics and forms the basis of a large body of work. We believe that this is an important topic that should be included in the curriculum. We plan to do so in the context of lexical paraphrase acquisition or synonyms automatically from corpora, a task that relies heavily on this notion of distributional similarity. There has been a lot of work in this area in the past years (Pereira et al., 1993; Gasperin et al., 2001; Glickman and Dagan, 2003; Shimohata and Sumita, 2005), much of which can be easily replicated using the Python-NLTK combination. This would allow for a very hands-on treatment and would allow the students to gain insight into this important, but often omitted, idea from computational linguistics. Given the success that we had in our first attempt to re-engineer the introductory NLP course, we plan to continue: (1) our hands-on approach to programming assignments in the NLTK framework and, (2) our practice of adapting ideas from research publications as the bases for as"
W08-0209,I05-1021,0,0.0127146,"ion, i.e., the same set of words or in the same context in a corpus, tend to have similar meanings. This is an extremely popular concept in corpus linguistics and forms the basis of a large body of work. We believe that this is an important topic that should be included in the curriculum. We plan to do so in the context of lexical paraphrase acquisition or synonyms automatically from corpora, a task that relies heavily on this notion of distributional similarity. There has been a lot of work in this area in the past years (Pereira et al., 1993; Gasperin et al., 2001; Glickman and Dagan, 2003; Shimohata and Sumita, 2005), much of which can be easily replicated using the Python-NLTK combination. This would allow for a very hands-on treatment and would allow the students to gain insight into this important, but often omitted, idea from computational linguistics. Given the success that we had in our first attempt to re-engineer the introductory NLP course, we plan to continue: (1) our hands-on approach to programming assignments in the NLTK framework and, (2) our practice of adapting ideas from research publications as the bases for assignment and examination problems. Below we describe two concrete ideas for th"
W08-0209,P06-4018,0,\N,Missing
W09-0441,W05-0909,0,0.098914,"frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality. HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference. This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005). One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent. In this way, only true errors in the MT output are counted. While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequacy, it is much more time consuming and taxing on human annotators than other types of human"
W09-0441,P05-1074,0,0.0378241,"ons, its average rank would be 1), although it performed well on average. In particular, TERp did significantly better than the TER metric, indicating the benefit of the enhancements made to TER. 4 Paraphrases TERp uses probabilistic phrasal substitutions to align phrases in the hypothesis with phrases in the reference. It does so by looking up—in a precomputed phrase table—paraphrases of phrases in the reference and using its associated edit cost as the cost of performing a match against the hypothesis. The paraphrases used in TERp were extracted using the pivot-based method as described in (Bannard and Callison-Burch, 2005) with several additional filtering mechanisms to increase the precision. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: we first identify English-to-F phrasal correspondences, then map from English to English by following translation units from English to F and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f, then they may be considered to be paraphrases of each other with the following probability: p(e1|e2) ≈ p(e1|f ) ∗ p(f |e2) 3 System description of metrics are also distributed by AMTA"
W09-0441,D08-1021,0,0.0160003,"s set included pairs that only shared partial semantic content. Most paraphrases extracted by the pivot method are expected to be of this nature. These pairs are not directly beneficial to TERp since they cannot be substituted for each other in all contexts. However, the fact that they share at least some semantic content does suggest that they may not be entirely useless either. Examples include: Varying Paraphrase Pivot Corpora To determine the effect that the pivot language might have on the quality and utility of the extracted paraphrases in TERp, we used paraphrase pairsmade available by Callison-Burch (2008). These paraphrase pairs were extracted from Europarl data using each of 10 European languages (German, Italian, French etc.) as a pivot language separately and then combining the extracted paraphrase pairs. Callison-Burch (2008) also extracted and made available syntactically constrained paraphrase pairs from the same data that are more likely to be semantically related. We used both sets of paraphrases in TERp as alternatives to the paraphrase pairs that we extracted from the Arabic newswire bitext. The results are shown in the last four rows of Table 5 and show that using a pivot language o"
W09-0441,N06-1058,0,0.0246276,"culating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER a"
W09-0441,E06-1031,0,0.0692041,"cost of a phrase substitution between the reference phrase, p1 and the hypothesis phrase p2 is: and Tomkins, 1997), TER addresses this by using a greedy search to select the words to be shifted, as well as further constraints on the words to be shifted. These constraints are intended to simulate the way in which a human editor might choose the words to shift. For exact details on these constraints, see Snover et al. (2006). There are other automatic metrics that follow the general formulation as TER but address the complexity of shifting in different ways, such as the CDER evaluation metric (Leusch et al., 2006). When TER is used with multiple references, it does not combine the references. Instead, it scores the hypothesis against each reference individually. The reference against which the hypothesis has the fewest number of edits is deemed the closet reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all the references. 2.2 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem"
W09-0441,W07-0716,1,0.299608,"ot only when they are exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Match"
W09-0441,2008.amta-papers.13,1,0.681799,"exact matches but also when the words share a stem or are synonyms. In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. These phrases are generated by considering possible paraphrases of the reference words. Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation. Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp. While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments. This is because while a set of constant weights might prove adequate for the purpose of measuring translation quality—as evidenced by correlation with human judgments both for TER and HTER—they may not be ideal for maximizing correlation. TERp uses all the edit operations of TER— Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and"
W09-0441,niessen-etal-2000-evaluation,0,0.0236852,"ce. The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. 2.1 TER One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference translation divided by the length of the reference translation. Unlike speech recognition, there are many correct translations for any given foreign sentence. These correct translations differ not only in their word choice but also in the order in which the words occur. WER is generally seen as inadequate for evaluation for machine translation as it fails to combine knowledge from multip"
W09-0441,P02-1040,0,0.102317,"n Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments. 1 schwartz@bbn.com Introduction Since the introduction of the BLEU metric (Papineni et al., 2002), statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics. These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality. Numerous methods of judging MT output by humans Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 259–268, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 259 or a missing determinator). Different types of translation errors va"
W09-0441,P07-1040,1,0.824501,"an alignment between the hypothesis and the reference, enabling it to be useful beyond general translation evaluation. While TER has been shown to correlate well with human judgments of translation quality, it has several flaws, including the use of only a single reference translation and the measuring of similarity only by exact word matches between the hypothesis and the reference. The handicap of using a single reference can be addressed by the construction of a lattice of reference translations. Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007). TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER. A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2. 2.1 TER One of the first automatic metrics used to evaluate automatic machine translation (MT) systems was Word Error Rate (WER) (Niessen et al., 2000), which is the standard evaluation metric for Automatic Speech Recognition. WER is computed as the Levenshtein (Levenshtein, 1966) distance between the words of the system output and the words of the reference trans"
W09-0441,2006.amta-papers.25,1,0.965034,"correct meaning, even if the translation is not fully fluent. Fluency and Adequacy are frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation quality. HTER is a more complex and semi-automatic measure in which humans do not score translations directly, but rather generate a new reference translation that is closer to the MT output but retains the fluency and meaning of the original reference. This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005). One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent. In this way, only true errors in the MT output are counted. While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequa"
W09-0441,W06-1610,0,0.0609664,"align the two phrases according to TERp. In effect, the probability of the paraphrase is used to determine how much to discount the alignment of the two phrases. Specifically, the cost of a phrase substitution between the reference phrase, p1 and the hypothesis phrase p2 is: and Tomkins, 1997), TER addresses this by using a greedy search to select the words to be shifted, as well as further constraints on the words to be shifted. These constraints are intended to simulate the way in which a human editor might choose the words to shift. For exact details on these constraints, see Snover et al. (2006). There are other automatic metrics that follow the general formulation as TER but address the complexity of shifting in different ways, such as the CDER evaluation metric (Leusch et al., 2006). When TER is used with multiple references, it does not combine the references. Instead, it scores the hypothesis against each reference individually. The reference against which the hypothesis has the fewest number of edits is deemed the closet reference, and that number of edits is used as the numerator for calculating the TER score. For the denominator, TER uses the average number of words across all"
W10-0730,N09-1057,1,0.929182,"e) and captures how much “action” takes place in a sentence. Such notions of Transitivity are not apparent from surface features alone; identical syntactic constructions can have vastly different Transitivity. This well-established linguistic theory, however, is not useful for real-world applications without a Transitivity-annotated corpus. Given such a substantive corpus, conventional machine learning techniques could help determine the Transitivity of verbs within sentences. Transitivity has been found to play a role in what is called “syntactic framing,” which expresses implicit sentiment (Greene and Resnik, 2009). 1 We use capital “T” to differentiate from conventional syntactic transitivity throughout the paper. Table 1 shows the subset of the Hopper-Thompson dimensions of Transitivity used in this study. We excluded noun-specific aspects as we felt that these were well covered by existing natural language processing (NLP) approaches (e.g. whether the object / subject is person, abstract entity, or abstract concept is handled well by existing named entity recognition systems) and also excluded aspects which we felt had significant overlap with the dimensions we were investigating (e.g. affirmation an"
W10-0730,P03-1054,0,0.00337267,"and that raters were less confident about their answers, prompting more hedging and a flat distribution. 3.4 Predicting Transitivity We also performed an set of initial experiments to investigate our ability to predict Transitivity values for held out data. We extracted three sets of features from the sentences: lexical features, syntactic features, and features derived from WordNet (Miller, 1990). Lexical Features A feature was created for each word in a sentence after being stemmed using the Porter stemmer (Porter, 1980). Syntactic Features We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. “running”), or whether the sentence is passive. If any of these constructions appear in the sentence, we generate a corresponding feature. These represent features identified by Greene and Resnik (2009). WordNet Features For each word in the sentence, we extracted all the possible senses for each word. If any possible sense was a hyponym (i.e. an instance of) one of: artifact, living thing, abstract entity, location, or food, we added a feature corresponding to that top level s"
W11-0810,E03-1038,0,0.0471609,"k in Computational Linguistics for more than 15 years. The aim is to recognize and classify different types of entities in text. These might be people’s names, or organizations, or locations, as well as dates, times, and currencies. Performance assessment is usually made in the context of Information Extraction, of which NER is generally a component. Competitions have been held from the earliest days of MUC (Message Understanding Conference), to the more recent shared tasks in CoNLL. Recent research has focused on non-English languages such as Spanish, Dutch, and German (Meulder et al., 2002; Carreras et al., 2003; Rossler, 2004), and on improving the performance of unsupervised learning methods (Nadeau et al., 2006; Elsner et al., 2009). There are no well-established standards for evaluation of NER. Since criteria for membership in the classes can change from one competition to another, it is often not possible to compare performance directly. Moreover, since some of the systems in the competition may use proprietary software, the results in a competition might not be replicable by others in the community; however, this applies to the state of the art for most NLP applications rather than just NER. Ou"
W11-0810,W03-0424,0,0.0368501,"nded form and some without. 6. Last names that appear in close proximity to the full name (first and last). This is to check on the impact of discourse and consistency of tagging. 7. Terms that contain a preposition, such as “Massachusetts Institute of Technology”. This is intended to test for correct extent in identifying the entity. 8. Terms that are a part of a location as well as an organization. For example, “Amherst, MA” vs. “Amherst College”. An excerpt from this unit test is shown in Table 7. We provide more information about the full unit test at the end of the paper. 6 et al., 1999; Curran and Clark, 2003). The Stanford and LBJ taggers have features for non-local dependencies for this reason. The observation is similar to a hypothesis proposed by Gale, Church, and Yarowsky with respect to word-sense disambiguation and discourse (Gale et al., 1992). They hypothesized that when an ambiguous word appears in a document, all subsequent instances of that word in the document will have the same sense. This hypothesis is incorrect for word senses that we find in a dictionary (Krovetz, 1998) but is likely to be correct for the subset of the senses that are homonymous (unrelated in meaning). Ambiguity be"
W11-0810,N09-1019,0,0.0134666,". These might be people’s names, or organizations, or locations, as well as dates, times, and currencies. Performance assessment is usually made in the context of Information Extraction, of which NER is generally a component. Competitions have been held from the earliest days of MUC (Message Understanding Conference), to the more recent shared tasks in CoNLL. Recent research has focused on non-English languages such as Spanish, Dutch, and German (Meulder et al., 2002; Carreras et al., 2003; Rossler, 2004), and on improving the performance of unsupervised learning methods (Nadeau et al., 2006; Elsner et al., 2009). There are no well-established standards for evaluation of NER. Since criteria for membership in the classes can change from one competition to another, it is often not possible to compare performance directly. Moreover, since some of the systems in the competition may use proprietary software, the results in a competition might not be replicable by others in the community; however, this applies to the state of the art for most NLP applications rather than just NER. Our work is motivated by a vocabulary assessment project in which we needed to identify multi-word expressions and determine the"
W11-0810,P05-1045,0,0.0347236,"Missing"
W11-0810,H92-1045,0,0.0461934,"f Technology”. This is intended to test for correct extent in identifying the entity. 8. Terms that are a part of a location as well as an organization. For example, “Amherst, MA” vs. “Amherst College”. An excerpt from this unit test is shown in Table 7. We provide more information about the full unit test at the end of the paper. 6 et al., 1999; Curran and Clark, 2003). The Stanford and LBJ taggers have features for non-local dependencies for this reason. The observation is similar to a hypothesis proposed by Gale, Church, and Yarowsky with respect to word-sense disambiguation and discourse (Gale et al., 1992). They hypothesized that when an ambiguous word appears in a document, all subsequent instances of that word in the document will have the same sense. This hypothesis is incorrect for word senses that we find in a dictionary (Krovetz, 1998) but is likely to be correct for the subset of the senses that are homonymous (unrelated in meaning). Ambiguity between named entities is similar to homonymy, and for most entities it is unlikely that they would co-occur in a document.7 However, there are cases that are exceptions. For example, Finkel et al. (2005) note that in the CoNLL dataset, the same te"
W11-0810,E99-1001,0,0.197178,"Missing"
W11-0810,W09-1119,0,0.230917,"on. One of the contributions of this paper is a freely available unit test based on the systematic problems we found with existing taggers. 2 Evaluation Methodology We compared three state-of-the-art NER taggers: one from Stanford University (henceforth, Stanford tagger), one from the University of Illinois (henceforth, the LBJ tagger) and BBN IdentiFinder (henceforth, IdentiFinder). The Stanford Tagger is based on Conditional Random Fields (Finkel et al., 2005). It was trained on 100 million words from the English Gigawords corpus. The LBJ Tagger is based on a regularized average perceptron (Ratinov and Roth, 2009). It was trained on a subset of the Reuters 1996 news corpus, a subset of the North American News Corpus, and a set of 20 web pages. The features for both these taggers are based on local context for a target word, orthographic features, label sequences, and distributional similarity. Both taggers include nonlocal features to ensure consistency in the tagging of identical tokens that are in close proximity. IdentiFinder is a state-of-the-art commercial NER tagger that uses Hidden Markov Models (HMMs) (Bikel et al., 1999). Since we did not have gold standard annotations for any of the real-worl"
W11-0810,W04-1218,0,0.0172353,"uistics for more than 15 years. The aim is to recognize and classify different types of entities in text. These might be people’s names, or organizations, or locations, as well as dates, times, and currencies. Performance assessment is usually made in the context of Information Extraction, of which NER is generally a component. Competitions have been held from the earliest days of MUC (Message Understanding Conference), to the more recent shared tasks in CoNLL. Recent research has focused on non-English languages such as Spanish, Dutch, and German (Meulder et al., 2002; Carreras et al., 2003; Rossler, 2004), and on improving the performance of unsupervised learning methods (Nadeau et al., 2006; Elsner et al., 2009). There are no well-established standards for evaluation of NER. Since criteria for membership in the classes can change from one competition to another, it is often not possible to compare performance directly. Moreover, since some of the systems in the competition may use proprietary software, the results in a competition might not be replicable by others in the community; however, this applies to the state of the art for most NLP applications rather than just NER. Our work is motiva"
W11-2111,P07-1111,0,0.0171842,"tions using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them. Gamon et al. (2005) build a classifier to distinguish machine-generated translations from human 108 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108–115, c"
W11-2111,P07-1038,0,0.0169553,"tions using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them. Gamon et al. (2005) build a classifier to distinguish machine-generated translations from human 108 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108–115, c"
W11-2111,W10-1703,0,0.313827,"ver et al. (2009) were used to produce scores tuned for fluency and adequacy. 3. METEOR (Lavie and Denkowski, 2009): Meteor scores were produced using Meteor-next v1.2. All types of matches were allowed (exact, stem, synonym and paraphrase) and scores tuned specifically to rank, HTER and adequacy were produced using the “-t” flag in the tool. We also implemented features closely related to or inspired by other MT metrics. The set of these auxiliary features is referred to as “Aux”. 1. Character-level statistics: Based on the success of the i-letter-BLEU and i-letter-recall metrics from WMT10 (Callison-Burch et al., 2010), we added the harmonic mean of precision (or recall) for character n-grams (from 1 to 10) as features. 2. Raw n-gram matches: We calculated the precision and precision for word n-grams (up to n=6) and added each as a separate feature (for a total of 12). Although these statistics are also calculated as part of the MT metrics above, breaking them into separate features gives the model more information. 3. Length ratios: The ratio between the lengths of the MT output and the reference translation was calculated on a character level and a word level. These ratios were also calculated between the"
W11-2111,A00-2019,1,0.607784,"ary automated essay scoring system developed by Educational Testing Service (ETS) to assess writing quality.1 The system has been used operationally for over 10 years in highstakes exams such as the GRE and TOEFL given its speed, reliability and high agreement with human raters. E-rater combines 8 main features using linear regression to produce a numerical score for an essay. These features are grammar, usage, mechanics, style, organization, development, lexical complexity and vocabulary usage. The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000). The usage feature detects errors related to articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al., 2008). The mechanics feature checks for spelling, punctuation and capitalization errors. The style feature checks for passive constructions and word repetition, among others. Organization and development tabulate the presence or absence of discourse elements and the length of each element. Finally, the lexical complexity feature details how complex the writer’s words are based on frequency indices and writing scales, and the vocabulary feature"
W11-2111,P01-1020,0,0.105398,"compare translation hypotheses to a set of human-authored reference translations. However, there has also been some work on methods that are not dependent on human-authored translations. One subset of such methods is task-based in that the methods determine the quality of a translation in terms of how well it serves the need of an extrinsic task. These tasks can either be downstream NLP Besides extrinsic evaluation, there is another set of methods that attempt to “learn” what makes a good translation and then predict the quality of new translations without comparing to reference translations. Corston-Oliver et al. (2001) proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidenc"
W11-2111,W08-0331,0,0.0463365,"cs above, breaking them into separate features gives the model more information. 3. Length ratios: The ratio between the lengths of the MT output and the reference translation was calculated on a character level and a word level. These ratios were also calculated between the MT output and the source sentence. 4. OOV heuristic: The percentage of tokens in the MT that match the source sentence. This is a low-precision heuristic for counting out of vocabulary (OOV) words, since it also counts named entities and words that happen to be the same in different languages. 4.3 Ranking Model Following (Duh, 2008), we represent sentence-level MT evaluation as a ranking problem. For a particular source sentence, there are N machine translations and one reference translation. A feature vector is extracted from each {source, reference, MT} tuple. The training data consists of sets of translations that have been annotated with relative ranks. During training, all ranked sets are converted to sets of feature vectors, where the label for each feature vector is the rank. The ranking model is a linear SVM that predicts a relative score for each feature vector, and is implemented by SVM-rank (Joachims, 2006). W"
W11-2111,2005.eamt-1.15,0,0.151528,"r it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them. Gamon et al. (2005) build a classifier to distinguish machine-generated translations from human 108 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108–115, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics ones using fluency-based features and show that by combining the scores of this classifier with LM perplexities, they obtain an MT metric that has good correlation with human judgments but not better than the baseline BLEU metric. The fundamental questions that inspired our proposed metrics are as follows: • Can an operational English-profici"
W11-2111,N07-2020,0,0.0667457,"Missing"
W11-2111,2004.tmi-1.8,0,0.0272496,"of such methods is task-based in that the methods determine the quality of a translation in terms of how well it serves the need of an extrinsic task. These tasks can either be downstream NLP Besides extrinsic evaluation, there is another set of methods that attempt to “learn” what makes a good translation and then predict the quality of new translations without comparing to reference translations. Corston-Oliver et al. (2001) proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study"
W11-2111,P02-1040,0,0.0878901,"se the MTeRater-Plus meta-metric that uses e-rater features plus all of the hybrid features described below. Both metrics were trained on the same data using the same machine learning model, and differ only in their feature sets. 4.1 E-rater Features Each sentence is associated with an e-rater sentencelevel vector and a document-level vector as previously described and each column in these vectors was used a feature. 4.2 Features for Hybrid Models We used existing automatic MT metrics as baselines in our evaluation, and also as features in our hybrid metric. The metrics we used were: 1. BLEU (Papineni et al., 2002): Case-insensitive and case-sensitive BLEU scores were produced using mteval-v13a.pl, which calculates smoothed sentence-level scores. 2. TERp (Snover et al., 2009): Translation Edit Rate plus (TERp) scores were produced using terp v1. The scores were case-insensitive and edit costs from Snover et al. (2009) were used to produce scores tuned for fluency and adequacy. 3. METEOR (Lavie and Denkowski, 2009): Meteor scores were produced using Meteor-next v1.2. All types of matches were allowed (exact, stem, synonym and paraphrase) and scores tuned specifically to rank, HTER and adequacy were produ"
W11-2111,quirk-2004-training,0,0.0527501,"ct the quality of new translations without comparing to reference translations. Corston-Oliver et al. (2001) proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations"
W11-2111,W09-0441,1,0.84083,"machine learning model, and differ only in their feature sets. 4.1 E-rater Features Each sentence is associated with an e-rater sentencelevel vector and a document-level vector as previously described and each column in these vectors was used a feature. 4.2 Features for Hybrid Models We used existing automatic MT metrics as baselines in our evaluation, and also as features in our hybrid metric. The metrics we used were: 1. BLEU (Papineni et al., 2002): Case-insensitive and case-sensitive BLEU scores were produced using mteval-v13a.pl, which calculates smoothed sentence-level scores. 2. TERp (Snover et al., 2009): Translation Edit Rate plus (TERp) scores were produced using terp v1. The scores were case-insensitive and edit costs from Snover et al. (2009) were used to produce scores tuned for fluency and adequacy. 3. METEOR (Lavie and Denkowski, 2009): Meteor scores were produced using Meteor-next v1.2. All types of matches were allowed (exact, stem, synonym and paraphrase) and scores tuned specifically to rank, HTER and adequacy were produced using the “-t” flag in the tool. We also implemented features closely related to or inspired by other MT metrics. The set of these auxiliary features is referre"
W11-2111,C08-1109,1,0.871697,"stem has been used operationally for over 10 years in highstakes exams such as the GRE and TOEFL given its speed, reliability and high agreement with human raters. E-rater combines 8 main features using linear regression to produce a numerical score for an essay. These features are grammar, usage, mechanics, style, organization, development, lexical complexity and vocabulary usage. The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000). The usage feature detects errors related to articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al., 2008). The mechanics feature checks for spelling, punctuation and capitalization errors. The style feature checks for passive constructions and word repetition, among others. Organization and development tabulate the presence or absence of discourse elements and the length of each element. Finally, the lexical complexity feature details how complex the writer’s words are based on frequency indices and writing scales, and the vocabulary feature evaluates how appropriate the words are for the given topic). Since many of the features are essay-specific, there is"
W11-2111,vilar-etal-2006-error,0,0.0565763,"Missing"
W11-2111,P09-1048,1,\N,Missing
W12-2005,N03-1003,0,0.0574673,"; Rosti et al., 2007; Feng et al., 2009). However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages. Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases. However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). 3 Methodology The basic"
W12-2005,P06-1032,0,0.470893,"Missing"
W12-2005,D09-1115,0,0.0341623,"Missing"
W12-2005,W09-2110,0,0.28295,"Missing"
W12-2005,J10-3003,1,0.826535,"ould simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases. However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). 3 Methodology The basic idea underlying our error correction technique is quite simple: if we can automatically generate alternative surface renderings of the meaning expressed in the original sentence and then pick the one that is most fluent, we are likely to have picked a version of the sentence in which the original grammatical errors have been fixed. In this paper, we propose generating such alternative formulations using statistical machine translation. For example, we take the original sentence E and translate it to Chinese using the Google TransOriginal Swedish Italian Russian French"
W12-2005,N03-1024,0,0.0396708,"r detection, our combination approach is directly related to the research on machine translation system combination wherein translation hypotheses produced by different SMT systems are combined to allow the extraction of a better, combined hypothesis (Bangalore et al., 2001; Rosti et al., 2007; Feng et al., 2009). However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages. Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf"
W12-2005,P11-1094,0,0.0311841,"Missing"
W12-2005,N07-1029,0,0.0345598,"f different round-trip translations and explore a whole new set of corrections that go beyond the translations themselves. Finally, we do not restrict our analysis to any single type of 45 error. In fact, our test sentences contain several different types of grammatical errors. Outside of the literature on grammatical error detection, our combination approach is directly related to the research on machine translation system combination wherein translation hypotheses produced by different SMT systems are combined to allow the extraction of a better, combined hypothesis (Bangalore et al., 2001; Rosti et al., 2007; Feng et al., 2009). However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages. Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (20"
W12-2005,W09-0441,1,0.823581,"ree itself. Therefore, the task is more complex than simply selecting the right round-trip translation. We posit that a better approach will be to combine the evidence of correction produced by each independent translation model and increase the likelihood of producing a final whole-sentence correction. Additionally, by engineering such a combination, we increase the likelihood that the final correction will preserve the meaning of the original sentence. In order to combine the round-trip translations, we developed a heuristic alignment algorithm that uses the TERp machine translation metric (Snover et al., 2009). The TERp metric takes a pair of sentences and computes the least number of edit operations that can be employed to turn one sentence into the other.2 As a by-product of computing the edit sequence, TERp produces an alignment between the two sentences where each alignment link is defined by an edit operation. Figure 2 shows an example of the alignment produced by TERp between the original sentence from Figure 1 and its Russian roundtrip translation. Note that TERp also allows shifting words and phrases in the second sentence in order to obtain a smaller edit cost (as indicated by the asterisk"
W12-2005,C10-1149,0,0.0207563,"combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases. However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). 3 Methodology The basic idea underlying our error correction technique is quite simple: if we can automatically generate alternative surface renderings of the meaning expressed in the original sentence and then pick the one that is mo"
W13-1722,N03-1020,0,0.273104,"Papineni et al., 2002) is an automated metric used extensively in automatically scoring the output of machine translation systems. 2. 3. 4. 5. 6. 7. 8. It is a precision-based metric that computes ngram overlap (n=1 . . . 4) between the summary (treated as a single sentence) against the passage (treated as a single sentence). We chose to use BLEU since it measures how many of the words and phrases are borrowed directly from the passage. Note that some amount of borrowing from the passage is essential for writing a good summary. ROUGE: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin and Hovy, 2003) is an automated metric used for scoring summaries produced by automated document summarization systems. It is a recall-based metric that measures the lexical and phrasal overlap between the summary under consideration and a set of “model” (or reference) summaries. We used a single model summary for the two passages by randomly selecting each from the set of student summaries assigned a score of 4 by the human rater. CopiedSumm: Ratio of the sum of lengths of all 3-word (or longer) sequences that are copied from the passage to the length of the summary. CopiedPassage: Same as CopiedSumm but wi"
W13-1722,P11-1076,0,0.101302,"sion models. These rankings show that the features performed consistently for both models. South Pole BLEU (.375) CopiedSumm (.290) ROUGE (.264) Length (.257) CopiedPassage (.246) MaxCopy (.231) FirstSent (.120) Coherence (.103) Perm. Housing BLEU (.450) ROUGE (.400) CopiedSumm (.347) Length (.340) MaxCopy(.253) CopiedPassage (.206) Coherence (.155) FirstSent (.058) Table 3: Classifier features for both passages ranked by average merit values obtained using information-gain. 5 Although work on automatically scoring student essays (Burstein, 2012) and short answers (Leacock and Chodorow, 2003; Mohler et al., 2011) is marginally relevant to the work done here, we believe it is different in significant aspects based on the scoring rubric and on the basis of the underlying RfU framework. We believe that the work most directly related to ours is the Summary Street system (Franzke et al., 2005; Kintsch et al., 2007) which attempts to score summaries written for tasks not based on the RfU framework and uses latent semantic analysis (LSA) rather than a feature-based classification approach. 6 Conclusion & Future Work We briefly introduced the Reading for Understanding cognitive framework and how it motivates"
W13-1722,P02-1040,0,0.095877,"d an automated system for scoring summaries of the type described in §3. To train and test our system, we used summaries written by more than 2600 students from the 6th, 7th and 9th grades about two different passages. Specifically, there were a total of 2695 165 0 0 1 2 3 Score 4 0 1 2 3 Score 4 Figure 2: A histogram illustrating the human score distribution of the summaries written for the two passages. Our approach to automatically scoring these summaries is driven by features based on the rubric. Specifically, we use the following features: 1. BLEU: BLEU (BiLingual Evaluation Understudy) (Papineni et al., 2002) is an automated metric used extensively in automatically scoring the output of machine translation systems. 2. 3. 4. 5. 6. 7. 8. It is a precision-based metric that computes ngram overlap (n=1 . . . 4) between the summary (treated as a single sentence) against the passage (treated as a single sentence). We chose to use BLEU since it measures how many of the words and phrases are borrowed directly from the passage. Note that some amount of borrowing from the passage is essential for writing a good summary. ROUGE: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin and Hovy, 2003) is"
W13-1722,W01-0100,0,\N,Missing
W13-1739,N13-1055,1,0.88711,"Missing"
W13-1739,W11-2838,0,0.0291019,"age learners. Therefore, we place more importance on the precision of the system than recall. We train our model on features that take the context of a pair of words into account, as well as other discriminative features. We present a number of evaluations on both artificially generated errors and naturally occurring learner errors and show that our classifiers achieve high precision and reasonable recall. 2 Related Work The task of detecting missing hyphens is related to previous work on detecting punctuation errors. One of the classes of errors in the Helping Our Own (HOO) 2011 shared task (Dale and Kilgarriff, 2011) was punctuation. Comma errors are the most frequent kind of punctuation error made by learners. Israel et al. (2012) present a model for detecting these kinds of errors in learner texts. They train CRF models on sentences from unedited essays written by high-level college students and show that they performs well on detecting errors in learner text. As 300 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300–305, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics far as we are aware, the HOO 2011 system desc"
W13-1739,N12-1029,1,0.854062,"Missing"
W13-1739,W11-2843,0,0.105521,"ation. Comma errors are the most frequent kind of punctuation error made by learners. Israel et al. (2012) present a model for detecting these kinds of errors in learner texts. They train CRF models on sentences from unedited essays written by high-level college students and show that they performs well on detecting errors in learner text. As 300 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300–305, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics far as we are aware, the HOO 2011 system description of Rozovskaya et al. (2011) is the only work to specifically reference hyphen errors. They use rules derived from frequencies in the training corpus to determine whether a hyphen was required between two words separated by white space. The task of detecting missing hyphens is related to the task of inserting punctuation into the output of unpunctuated text (for example, the output of speech recognition, automatic generation, machine translation, etc.). Systems that are built on the output of speech recognition can obviously take features like prosody into account. In our case, we are dealing only with written text. Grav"
W13-1739,P11-1019,0,0.0559877,"uating on the Brown Corpus with hyphens removed combined news and Wikipedia revision text achieve the highest overall f-score. Figure (1a) shows the Precision Recall curves for the Wikipedia baselines and the three classifiers. The curves mirror the results in the table, showing that the classifier trained on the newswire text, and the classifier trained on the combined data perform best. The Wikipedia counts baseline performs worst. 6 Evaluating on Learner Text We carry out two evaluations of our system on learner text. We first evaluate on the missing hyphen errors contained in the CLC-FCE (Yannakoudakis et al., 2011). This corpus contains 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English. In total, there are 173 instances of missing hyphen errors. The results are given in Table 4, and the precision recall curves are displayed in Figure (1b). The results show that the classifiers consistently achieve high precision on this data set. This is as expected, given the high threshold set. Looking at the curves, it seems that a slightly lower threshold in this case may lead to better results. The curves show that the combined classifier is performing slig"
W14-1810,N04-1043,0,0.00986009,"a revisions. Acknowledgments We would like to thank Beata Beigman Klebanov, Michael Heilman, Jill Burstein, and the anonymous reviewers for their helpful comments about the paper. We also thank Ani Nenkova, Chris Callison-Burch, Lyle Ungar and their students at the University of Pennsylvania for their feedback on this work. • Using other types of expansions. In this paper, we used a very simple method of generating query expansions – a distributional thesaurus. However, in the future, it may be worth exploring other distributional similarity methods such as Brown clusters (Brown et al., 1992; Miller et al., 2004; Liang, 2005) or word2vec (Mikolov et al., 2013). 8 References Yigal Attali. 2004. Exploring the Feedback and Revision Features of Criterion. Paper presented at the National Council on Measurement in Education (NCME), Educational Testing Service, Princeton, NJ. Conclusions In this paper, we presented our work on building a proof-of-concept tool that can provide automated explicit feedback for preposition errors. We used an existing, error-annotated preposition corpus produced by mining Wikipedia revisions Douglas Biber, Tatiana Nekrasova, and Brad Horn. 2011. The Effectiveness of Feedback for"
W14-1810,J92-4003,0,0.105481,"Missing"
W14-1810,W10-0405,0,0.0265754,"ol. §4 outlines the core system 79 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 79–88, c Baltimore, Maryland USA, June 26, 2014. 2014 Association for Computational Linguistics tions from a very large corpus of annotated errors, rather than performing a web search on all possible alternatives in the context. The advantage of using an error-annotated corpus is that it contains implicit information about frequent confusion pairs (e.g. “at” instead of “in”) that are independent of the frequency of the preposition and the current context. Milton and Cheng (2010) describe a toolkit for helping Chinese learners of English become more independent writers. The toolkit gives the learners access to online resources including web searches, online concordance tools, and dictionaries. Users are provided with snapshots of the word or structure in context. In Milton (2006), 500 revisions to 323 journal entries were made using an earlier version of this tool. Around 70 of these revisions had misinterpreted the evidence presented or were careless mistakes; the remaining revisions resulted in more natural sounding sentences. for generating feedback and §5 presents"
W14-1810,N13-1055,1,0.848494,"le they were composing email messages. They reported that users were able to make effective use of the explicit feedback for that task. The tool had been offered as a web service but has since been discontinued. Our tool is similar to ESL Assistant in that both produce a list of possible corrections. The main difference between the tools is that ours automatically derives the ranked list of correction sugges3 Wikipedia Revisions Our goal is to build a tool that can provide explicit feedback about errors to writers. We take advantage of the recently released Wikipedia preposition error corpus (Cahill et al., 2013) and design our tool based on this large corpus containing sentences annotated for preposition errors and their corrections. The corpus was produced automatically by mining a total of 288 million revisions for 8.8 million articles present in a Wikipedia XML snapshot from 2011. The Wikipedia error corpus, as we refer to in the rest of the paper, contains 2 million sentences annotated with preposition errors and their respective corrections. There are two possible approaches to building an explicit feedback tool for preposition errors based on this corpus: 1. Classifier-based. We could train a c"
W14-1810,N10-1018,0,0.0709325,"Missing"
W14-1810,P11-1092,0,0.0400203,"Missing"
W14-1810,P12-2064,0,0.0264706,"Missing"
W14-1810,C08-1109,0,0.0855632,"Missing"
W14-1810,P10-2065,0,0.0504334,"Missing"
W14-1810,P98-2127,0,0.0102681,"he lower MRR values. In the field of information retrieval, a common practice is to expand the query with words similar to words in the query in order to increase the likelihood of finding documents relevant to the query (Sp¨arck-Jones and Tait, 1984). In this section, we examine whether we can use a similar technique to improve the coverage of the tool. We employ a simple query expansion technique for the cases where no results would otherwise be returned by the tool. For these cases, we first obtain a list of K words similar to the two words around the error from a distributional thesaurus (Lin, 1998), ranked by similarity. We then generate a list of additional queries by combining these two ranked lists of similar words. We then run each query in the list against the Wikipedia index until one of them yields results. Note that since we are using a word-based thesaurus, this expansion technique can only increase coverage when applied to the words1 condition, i.e., single word contexts. We investigate K = 1, 2, 5, or 10 expansions for each of the context words. Table 2 shows the a detailed breakdown of the distribution of the three classes and the MRR values with query expansion integrated i"
W14-1810,N03-1033,0,0.014126,"he erroneous preposition was changed to the correction in the Wikipedia revision index. In this example, the preposition of with the left context of <DT, NNS> and the right context of <DT, NN> was changed to the preposition in 242 times in the Wikipedia revisions. When the user clicks on a bar, the box on the top shows the sentence with the change and the gray box on the right shows 5 (randomly chosen) actual sentences from Wikipedia where the change represented by the bar was made. If parts-of-speech are chosen as context, the tool uses WebSockets to send the sentence to the Stanford Tagger (Toutanova et al., 2003) in the background and compute its part-of-speech tags before searching the index. viding evidence for each correction to the user. 2. Corpus-based. We could use the Wikipedia error corpus directly for feedback. Although this means that suggestions can only be generated for contexts occurring in the Wikipedia data, it also means that all suggestion would be grounded in actual revisions made by other humans on Wikipedia. We believe that anchoring suggestions to human-authored corrections affords greater utility to a language learner, in line with the current practice in lexicography that emphas"
W14-1810,P11-1019,0,0.0499453,"ion. • The words, bigrams, and trigrams before (and after) the preposition error (indexed separately). • The part-of-speech tags, tag bigrams, and tag trigrams before (and after) the error (indexed separately). 5 • The title and URL of the Wikipedia article in which the sentence occurred. Evaluation In order to determine how well the tool performs at suggesting corrections, we used sentences containing preposition errors from the CLC FCE dataset. The CLC FCE Dataset is a collection of 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English (Yannakoudakis et al., 2011). Our evaluation set consists of 3,134 sentences, each containing a single preposition error. We evaluate the tool on two criteria: • The ID of the article revision containing the preposition error. • The ID of the article revision in which the correction was made. Once the index is constructed, eliciting explicit feedback is straightforward. The input to the system is a tokenized sentence with a marked up preposition error (e.g. from an automated preposition error detection system). For each input sentence, the Wikipedia index is then searched with the identified preposition error and the wor"
W14-1810,C98-2122,0,\N,Missing
W15-0610,P11-1076,0,0.193856,"nce. We believe this will help future researchers and practitioners working on short answer scoring to answer practically important questions such as, “How much training data do I need?” 1 Introduction Automated short answer scoring is a challenging educational application of natural language processing that has received considerable attention in recent years, including a SemEval shared task (Dzikovska et al., 2013), a public competition on the Kaggle data science website (https://www.kaggle.com/ c/asap-sas), and various other research papers (Leacock and Chodorow, 2003; Nielsen et al., 2008; Mohler et al., 2011). The goal of short answer scoring is to create a predictive model that can take as input a text response to a given prompt (e.g., a question about a reading passage) and produce a score representing the accuracy ∗ Michael Heilman is now a data scientist at Civis Analytics. or correctness of that response. One well-known approach is to learn a prompt-specific model using detailed linguistic features such as word n-grams from a large training set of responses that have been previously scored by humans.1 This approach works very well when large sets of training data are available, such as in the"
W15-0610,W08-0902,0,0.0264315,"ate to system performance. We believe this will help future researchers and practitioners working on short answer scoring to answer practically important questions such as, “How much training data do I need?” 1 Introduction Automated short answer scoring is a challenging educational application of natural language processing that has received considerable attention in recent years, including a SemEval shared task (Dzikovska et al., 2013), a public competition on the Kaggle data science website (https://www.kaggle.com/ c/asap-sas), and various other research papers (Leacock and Chodorow, 2003; Nielsen et al., 2008; Mohler et al., 2011). The goal of short answer scoring is to create a predictive model that can take as input a text response to a given prompt (e.g., a question about a reading passage) and produce a score representing the accuracy ∗ Michael Heilman is now a data scientist at Civis Analytics. or correctness of that response. One well-known approach is to learn a prompt-specific model using detailed linguistic features such as word n-grams from a large training set of responses that have been previously scored by humans.1 This approach works very well when large sets of training data are ava"
W15-0610,N15-1111,1,0.750101,"ing sample size and a few other factors, in order to help answer extremely practical questions like, “How much data should I gather and label before deploying automated scoring for a new prompt?” Specifically, we explore the following research questions: • How strong is the association between training sample size and automated scoring performance? 1 Information from the scoring guidelines, such as exemplars for different score levels, can also be used in the scoring model, though in practice we have observed that this does not add much predictive power to a model that uses student responses (Sakaguchi et al., 2015). 2 Syntactic parsing performance varies considerably depending on the domain, but most applications use parsing models that depend almost exclusively on the Penn Treebank. 81 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 81–85, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics • If the training set size is doubled, how much improvement in performance should we expect? • Are there other factors such as the number of score levels that are strongly associated with performance? • Can we create a model t"
W15-0610,S13-2045,0,\N,Missing
W15-0619,N13-1055,1,0.848424,"on P1 may be incorrect. 3. Detailed Feedback 1. The incorrect preposition is highlighted and the feedback message is of the form: The highlighted preposition P1 may be incorrect; the preposition P2 may be more appropriate, where P2 is a human expert’s suggested correction for the error. 4. Detailed Feedback 2. The incorrect preposition is highlighted and the feedback message is of the form: The highlighted preposition P1 may be incorrect; the preposition P2 may be more appropriate, where P2 is the correction assigned the highest probability by an automated preposition error correction system (Cahill et al., 2013). 5. Detailed Feedback 3. The incorrect preposition is highlighted and the feedback message is of the form: The highlighted preposition P1 may be incorrect; the following is a list of prepositions that may be more appropriate, where the list contains the top 5 suggested corrections from the automated error correction system. For all three detailed feedback types, Turkers were told that the corrections were generated by an automated system. Table 1 shows the design of our experimental study wherein all recruited Turkers were divided into five mutually exclusive groups, each corresponding to one"
W15-0619,C10-2103,0,0.580405,"n the quite restricted question of the extent to which error correction influences writing accuracy for L2-English students. This study concluded that overt error correction actually has a small negative influence on learners’ abilities to write accurately. However, the meta-analysis was based on only six research studies, making it somewhat difficult to be confident about the generalizability of the findings.” Biber et al. (2011) also mention that “In actual practice, direct feedback is rarely used as a treatment in empirical research.” The work most directly relevant to our study is that of Nagata and Nakatani (2010), who attempt to measure actual impact of feedback on learning outcomes for English language learners whose native language is Japanese. At the beginning of the study, students wrote English essays on 10 different topics. Errors involving articles and noun number were then flagged either by a human or by two different automatic error detection systems: one with high precision and another with high recall. A control group received no error feedback. Learning was measured in terms of reduction of error rate for the noun phrases in the students’ essays. Results showed that learning was quite simi"
W15-0619,P11-1019,0,0.0336793,"completed Session 1, we were able to use our own qualifications and a range of qualification scores to assign Turkers to groups and control the order in which they completed the sessions. Although the Turkers were assigned randomly to groups, we manually ensured that the distributions of Session 1 scores were similar across groups. 165 amount increased by 50 cents for each new session, adding up to a total of $10 per Turker if they completed all five sessions. Table 2 shows the number of Turkers assigned to each group who participated in each of the five sessions. We used the CLC-FCE corpus (Yannakoudakis et al., 2011), which has been manually annotated for preposition errors by professional English language instructors. We randomly selected 90 sentences with preposition errors and 45 sentences without errors and manually reviewed them to ensure their suitability. Unsuitable sentences were replaced from the pool of automatically extracted sentences until we had a total of 135 suitable sentences. We annotated each sentence containing an error with a correct preposition. The 135 sentences were then randomly divided into 5 HITs (Human Intelligence Tasks, the basic unit of work on MTurk), one for each of the fi"
W16-0501,W13-1703,0,0.0958381,"analysis also depends on the quality of the NUCLE annotations. When correcting ungrammatical text, annotators are faced with the decisions of whether a text needs to be corrected and, if so, how to edit it. Previous work has found low interannotator agreement for the basic task of judging whether a sentence is grammatical (0.16 ≤ κ ≤ 0.40) (Rozovskaya and Roth, 2010). The NUCLE corpus is no different, with the three NUCLE annotators having moderate agreement on how to correct a span of text (κ = 0.48) and only fair agreement for identifying what span of text needs to be corrected (κ = 0.39) (Dahlmeier et al., 2013). Low inter-annotator agreement is not necessarily an indication of the quality of the annotations, since it could 4 Using the EnglishGrammaticalStructure class with the flags -nonCollapsed -keepPunct. 3 also be attributed to the diversity of appropriate corrections that have been made. We assume that the annotations are correct and complete, meaning that the spans and labels of annotations are correct and that all of the grammatical errors are annotated. We further assume that the annotations only fix grammatical errors, instead of providing a stylistic alternatives to grammatical text. 3 Met"
W16-0501,de-marneffe-etal-2014-universal,0,0.0410261,"Missing"
W16-0501,P08-2056,0,0.0742464,"Missing"
W16-0501,W11-2925,0,0.0638227,"Missing"
W16-0501,foster-2004-parsing,0,0.0548065,"ases likely reflect a structural change in the dependency graph of the ungrammatical sentence, which affects more relations than those containing the ungrammatical tokens. 6 Related work There is a modest body of work focused on improving parser performance of ungrammatical sentences. 7 Unlike our experiments, most previous work has used small (around 1,000 sentences) or artificially generated corpora of ungrammatical/grammatical sentence pairs. The most closely related works compared the structure of constituent parses of ungrammatical to corrected sentences: with naturally occurring errors, Foster (2004) and Kaljahi et al. (2015) and evaluate parses of ungrammatical text based on the constituent parse and Geertzen et al. (2013) evaluate performance over dependencies. Cahill (2015) examines the parser performance using artificially generated errors, and Foster (2007) analyzes the parses of both natural and artificial errors. In Wagner and Foster (2009), the authors compared the parse probabilities of naturally occurring and artificially generated ungrammatical sentences to the probabilities of the corrected sentences. They found that the natural ungrammatical sentences had a lower reduction in"
W16-0501,N10-1060,0,0.0256916,"l ungrammatical sentences had a lower reduction in parse probability than artificial sentences, suggesting that artificial errors are not interchangeable with spontaneous errors. This analysis suggests the importance of using naturally occurring errors, which is why we chose to generate sentences from the spontaneous NUCLE errors. Several studies have attempted to improve the accuracy of parsing ungrammatical text. Some approaches include self-training (Foster et al., 2011; Cahill et al., 2014), retraining (Foster et al., 2008), and transforming the input and training text to be more similar (Foster, 2010). Other work with ungrammatical learner text includes Caines and Buttery (2014), which identifies the need to improve parsing of spoken learner English, and Tetreault et al. (2010), which analyzes the accuracy of prepositional phrase attachment in the presence of preposition errors. 7 Conclusion and future work The performance of NLP tools over ungrammatical text is little understood. Given the expense of annotating a grammatical-error corpus, previous studies have used either small annotated corpora or generated artificial grammatical errors in clean text. This study represents the first larg"
W16-0501,D15-1157,0,0.0314948,"Missing"
W16-0501,P08-1021,0,0.0254338,"Missing"
W16-0501,W04-2407,0,0.0381016,"Missing"
W16-0501,W07-2460,0,0.0701427,"Missing"
W16-0501,roark-etal-2006-sparseval,0,0.0431963,"s the accuracy of the dependency triples from the candidate dependency graph with respect to those of the gold standard, where each triple represents one relation, consisting of the head, dependent, and type of relation. The LAS assumes that the surface forms of the sentences are identical but only the relations have changed. In this work, we require a method that accommodates unaligned tokens, which occur when an error involves deleting or inserting tokens and unequal surface forms (replacement errors). There are some metrics that compare the parses of unequal sentences, including SParseval (Roark et al., 2006) and TEDeval (Tsarfaty et al., 2011), however neither of these metrics operate over dependencies. We chose to evaluate dependencies because dependency-based evaluation has been shown to be more closely related to the linguistic intuition of good parses compared to two other tree-based evaluations (Rehbein and van Genabith, 2007). Since we cannot calculate the LAS over sentences of unequal lengths, we instead measure the F1 -score of the dependency relations. So that substitutions (such as morphological changes) are not severely penalized, we represent tokens with their index instead of the sur"
W16-0501,W10-1004,0,0.123171,"he Stanford Dependency Parser4 and the universal dependencies representation (De Marneffe et al., 2014). We make the over-confident assumption that the automatic analyses in our pipeline (tokenization, parsing, and error-type labeling) are all correct. Our analysis also depends on the quality of the NUCLE annotations. When correcting ungrammatical text, annotators are faced with the decisions of whether a text needs to be corrected and, if so, how to edit it. Previous work has found low interannotator agreement for the basic task of judging whether a sentence is grammatical (0.16 ≤ κ ≤ 0.40) (Rozovskaya and Roth, 2010). The NUCLE corpus is no different, with the three NUCLE annotators having moderate agreement on how to correct a span of text (κ = 0.48) and only fair agreement for identifying what span of text needs to be corrected (κ = 0.39) (Dahlmeier et al., 2013). Low inter-annotator agreement is not necessarily an indication of the quality of the annotations, since it could 4 Using the EnglishGrammaticalStructure class with the flags -nonCollapsed -keepPunct. 3 also be attributed to the diversity of appropriate corrections that have been made. We assume that the annotations are correct and complete, me"
W16-0501,P10-2065,0,0.0339927,"errors. This analysis suggests the importance of using naturally occurring errors, which is why we chose to generate sentences from the spontaneous NUCLE errors. Several studies have attempted to improve the accuracy of parsing ungrammatical text. Some approaches include self-training (Foster et al., 2011; Cahill et al., 2014), retraining (Foster et al., 2008), and transforming the input and training text to be more similar (Foster, 2010). Other work with ungrammatical learner text includes Caines and Buttery (2014), which identifies the need to improve parsing of spoken learner English, and Tetreault et al. (2010), which analyzes the accuracy of prepositional phrase attachment in the presence of preposition errors. 7 Conclusion and future work The performance of NLP tools over ungrammatical text is little understood. Given the expense of annotating a grammatical-error corpus, previous studies have used either small annotated corpora or generated artificial grammatical errors in clean text. This study represents the first large-scale analysis of the effect of grammatical errors on a NLP task. We have used a large, annotated corpus of grammatical errors to generate more than 44,000 sentences with up to f"
W16-0501,D11-1036,0,0.0309765,"triples from the candidate dependency graph with respect to those of the gold standard, where each triple represents one relation, consisting of the head, dependent, and type of relation. The LAS assumes that the surface forms of the sentences are identical but only the relations have changed. In this work, we require a method that accommodates unaligned tokens, which occur when an error involves deleting or inserting tokens and unequal surface forms (replacement errors). There are some metrics that compare the parses of unequal sentences, including SParseval (Roark et al., 2006) and TEDeval (Tsarfaty et al., 2011), however neither of these metrics operate over dependencies. We chose to evaluate dependencies because dependency-based evaluation has been shown to be more closely related to the linguistic intuition of good parses compared to two other tree-based evaluations (Rehbein and van Genabith, 2007). Since we cannot calculate the LAS over sentences of unequal lengths, we instead measure the F1 -score of the dependency relations. So that substitutions (such as morphological changes) are not severely penalized, we represent tokens with their index instead of the surface form. First, we align the token"
W16-0501,W09-3827,0,0.0140517,"ll (around 1,000 sentences) or artificially generated corpora of ungrammatical/grammatical sentence pairs. The most closely related works compared the structure of constituent parses of ungrammatical to corrected sentences: with naturally occurring errors, Foster (2004) and Kaljahi et al. (2015) and evaluate parses of ungrammatical text based on the constituent parse and Geertzen et al. (2013) evaluate performance over dependencies. Cahill (2015) examines the parser performance using artificially generated errors, and Foster (2007) analyzes the parses of both natural and artificial errors. In Wagner and Foster (2009), the authors compared the parse probabilities of naturally occurring and artificially generated ungrammatical sentences to the probabilities of the corrected sentences. They found that the natural ungrammatical sentences had a lower reduction in parse probability than artificial sentences, suggesting that artificial errors are not interchangeable with spontaneous errors. This analysis suggests the importance of using naturally occurring errors, which is why we chose to generate sentences from the spontaneous NUCLE errors. Several studies have attempted to improve the accuracy of parsing ungra"
W16-0501,P11-1019,0,0.0492129,"is is relies on the idiosyncrasies of this particular corpus, such as the typical sentence length and complexity. The essays were written by students at the National University of Singapore, who do not have a wide variety of native languages. The types and frequency of errors differ depending on the native language of the student (Rozovskaya and Roth, 2010), which may bias the analysis herein. The available corpora that contain a broader representation of native languages are much smaller than the NUCLE corpus: the Cambridge Learner Corpus–First Certificate in English has 420 thousand tokens (Yannakoudakis et al., 2011), and the corpus annotated by (Rozovskaya and Roth, 2010) contains only 63 thousand words. One limitation to our method for generating ungrammatical sentences is that relatively few sentences are the source of ungrammatical sentences with four errors. Even though we drew sentences from a large corpus, only 570 sentences had at least four errors (of the types we were considering), compared to 14,500 sentences with at least one error. Future work examining the effect of multiple errors 8 would need to consider a more diverse set of sentences with more instances of at least four errors, since the"
W16-0501,J11-1005,0,0.0284901,"tly n errors. with n = 1 to 4 errors, when there were at least n corrections to the original sentence. For example, a NUCLE sentence with 6 annotated corrections would yield the following number of ungrammatical  sentences: 6 sentences with one error, 62 = 15  sentences with two errors, 63 = 20 sentences with three errors, and so on. The number of original NUCLE sentences and generated sentences with each number of errors is shown in Table 1. We also generated a grammatical sentence with all of the corrections applied for comparison. We parsed each sentence with the ZPar constituent parser (Zhang and Clark, 2011) and generated dependency parses from the ZPar output using the Stanford Dependency Parser4 and the universal dependencies representation (De Marneffe et al., 2014). We make the over-confident assumption that the automatic analyses in our pipeline (tokenization, parsing, and error-type labeling) are all correct. Our analysis also depends on the quality of the NUCLE annotations. When correcting ungrammatical text, annotators are faced with the decisions of whether a text needs to be corrected and, if so, how to edit it. Previous work has found low interannotator agreement for the basic task of"
W16-0501,W14-6107,0,\N,Missing
W16-0501,W14-6106,1,\N,Missing
W16-0501,E14-3013,0,\N,Missing
W16-0501,W15-1616,1,\N,Missing
W16-0515,N13-1055,1,0.856085,"lts with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a data scientist at Civis Analytics. Much of"
W16-0515,C12-1038,0,0.0157331,"e, we compute precision, recall, and F1 score for each dataset. Precision is the percentage of system corrections that are correct according to the gold standard, and recall is the percentage of the gold standard corrections that were correctly marked by the system. Our evaluation metric can be viewed as similar to a micro-averaged F1 score for a multi-class document classification task where documents are the original prepositions, classes are the possible corrections, and only documents for ungrammatical prepositions have class labels. Our F1 score is similar to the WAS evaluation scheme of Chodorow et al. (2012), except that we treat cases where the original preposition, system prediction, and gold standard all differ as false negatives. Chodorow et al. (2012) instead treat such cases as both false positives and false negatives, and as a result, the sum of true positives, false positives, true negatives, and false negatives does not equal the number of examples. 3 Methods This section describes our implementations of the classifier, language modeling, and system combina137 3.1 3.2 Classifier Language Model Our second system uses a language modeling approach. We use KenLM (Heafield, 2011) to estimate"
W16-0515,P11-1092,0,0.0184016,"result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a dat"
W16-0515,W11-2838,0,0.0292933,"Missing"
W16-0515,W12-2006,0,0.030744,"Missing"
W16-0515,1993.eamt-1.1,0,0.511941,"best F1 score and that it 139 Dataset FCE HOO System Classifier LM Heuristic Interpolation Ensemble Classifier LM Heuristic Interpolation Ensemble P 65.63 28.42 30.91 50.67 51.72 59.26 12.90 21.24 34.29 32.93 R 17.77 30.39 37.26 36.30 38.35 19.75 14.81 29.63 29.63 33.33 F1 27.97 29.37 33.79 42.30 44.04 29.63 13.79 24.74 31.79 33.13 Sig. ∗ ∗ ∗ ∗ ∗ ∗ Table 1: P, R and F1 scores for the FCE and HOO test sets. Bold indicates the best result for each metric. “∗” indicates that the F1 score for a system was significantly different (p < .05) from that of the ensemble system as per the BCa Bootstrap (Efron and Tibshirani, 1993) test with 10,000 replications. performs significantly better than all other approaches, including both the classifier and the language model. 3. For the HOO test set — which is quite different from the FCE data in both its genre (ACL papers) and the distribution of grammatical errors — the ensemble approach still attains the best performance and is significantly better than the language model and the heuristic system combination approach. Finally, we also compared the ensemble approach to a current state-of-the-art preposition error correction system. To do this, we evaluated on the CoNLL 201"
W16-0515,N10-1019,0,0.154274,"In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a data scientist at Civis Analytics. Much of the previous work has used well-formed text when training contextual classifiers due to the lack of large error-annotated corpora. Han et al. (2010) conducted experiments with a relatively small error-annotated corpus and showed that it outperfor"
W16-0515,W11-2123,0,0.0204373,"me of Chodorow et al. (2012), except that we treat cases where the original preposition, system prediction, and gold standard all differ as false negatives. Chodorow et al. (2012) instead treat such cases as both false positives and false negatives, and as a result, the sum of true positives, false positives, true negatives, and false negatives does not equal the number of examples. 3 Methods This section describes our implementations of the classifier, language modeling, and system combina137 3.1 3.2 Classifier Language Model Our second system uses a language modeling approach. We use KenLM (Heafield, 2011) to estimate an unpruned model for n = 6 with modified KneserNey smoothing (Chen and Goodman, 1998) on the text of all articles contained in a snapshot of English Wikipedia from June 2012 (68,356,743 sentences). We use this n-gram language model to obtain scores (f (w,s,i)) gLM (w, s, i) = log10 pLM , where w is the |s|+1 preposition to be scored, s is the writer’s original sentence, i is the position of the original preposition in s, f is a function that returns a variant of s with the preposition at i replaced with w, and pLM returns the probability for a sentence. We divide the language mod"
W16-0515,N10-1018,0,0.0325042,", we report a more nuanced result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Mich"
W16-0515,P11-1093,0,0.318922,"as studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a data scientist at Civis Analytics. Much of the previous work has used well-formed text when training contextual classifiers due to the lack of large error-annotated corpora. Han et al. (2010) conducted experiments with a relatively small error-annotated corpus and showed that it outperformed a contextual classifier trained on well-edited text. More recently, Cahill et al. (2013) mined Wikipedia revisions to produce a large, publicly availabl"
W16-0515,P12-2064,0,0.0150656,"roach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a data scientist at Civ"
W16-0515,D14-1102,0,0.0183523,"th respect to the second question, Gamon (2010) previously reported that a combination of a contextual classifier trained on well-edited text and a language modeling approach outperformed each individual method. However, given that the performance of his classifier was lower than what has been reported on other datasets (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2011), we believe it is worth reinvestigating the merits of system combination but with publicly available data sets and with a classifier trained on error-annotated data instead of on well-edited text. This work differs from Susanto et al. (2014) in that we are interested in combining statistical models in order to more accurately correct individual preposition errors, while their work 136 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 136–141, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics combined — at the sentence level — the outputs of multiple systems designed to correct different types of grammatical errors. tion approaches to preposition error correction. 2 Our first system is a classifier trained on errorannotated data, following Ca"
W16-0515,C08-1109,0,0.253018,"h (2011) reported that classifiers outperformed a language modeling approach. Here, we report a more nuanced result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeli"
W16-0515,P10-2065,0,0.0288913,"modeling approach. Here, we report a more nuanced result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear whi"
W16-0515,P11-1019,0,0.0850561,"Missing"
W16-0524,S13-2045,0,0.124319,"Missing"
W16-0524,P11-1076,0,0.152534,"17 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 217–222, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics Figure 1: A sample question from the music teaching proficiency test. Note that only a part of the entire music sample included with the question is shown here. ered similar in spirit to some of the previous work on short-answer scoring, where the focus is on scoring content-driven responses to math, biology, or computer science questions (Sukkarieh and Stoyanchev, 2009; Sukkarieh et al., 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 However, we claim that there is very little in that body of previous work that focuses on responses exhibiting all of the following characteristics: • The responses we examine are written by a population of adults that are generally proficient English writers. This is different from most previous work where the population is generally composed of middle- and high-school students with varying levels of English proficiency. • The average length of these responses is approximately 220 words which is m"
W16-0524,W15-0612,0,0.0952505,"tive Use of NLP for Building Educational Applications, pages 217–222, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics Figure 1: A sample question from the music teaching proficiency test. Note that only a part of the entire music sample included with the question is shown here. ered similar in spirit to some of the previous work on short-answer scoring, where the focus is on scoring content-driven responses to math, biology, or computer science questions (Sukkarieh and Stoyanchev, 2009; Sukkarieh et al., 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 However, we claim that there is very little in that body of previous work that focuses on responses exhibiting all of the following characteristics: • The responses we examine are written by a population of adults that are generally proficient English writers. This is different from most previous work where the population is generally composed of middle- and high-school students with varying levels of English proficiency. • The average length of these responses is approximately 220 words which is much longer than the responses considered in much of"
W16-0524,N15-1111,1,0.813781,"g Educational Applications, pages 217–222, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics Figure 1: A sample question from the music teaching proficiency test. Note that only a part of the entire music sample included with the question is shown here. ered similar in spirit to some of the previous work on short-answer scoring, where the focus is on scoring content-driven responses to math, biology, or computer science questions (Sukkarieh and Stoyanchev, 2009; Sukkarieh et al., 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 However, we claim that there is very little in that body of previous work that focuses on responses exhibiting all of the following characteristics: • The responses we examine are written by a population of adults that are generally proficient English writers. This is different from most previous work where the population is generally composed of middle- and high-school students with varying levels of English proficiency. • The average length of these responses is approximately 220 words which is much longer than the responses considered in much of the previous work (10-1"
W16-0524,W09-2509,0,0.0333287,"experts are chosen randomly from a pool of 9 experts. 217 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 217–222, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics Figure 1: A sample question from the music teaching proficiency test. Note that only a part of the entire music sample included with the question is shown here. ered similar in spirit to some of the previous work on short-answer scoring, where the focus is on scoring content-driven responses to math, biology, or computer science questions (Sukkarieh and Stoyanchev, 2009; Sukkarieh et al., 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 However, we claim that there is very little in that body of previous work that focuses on responses exhibiting all of the following characteristics: • The responses we examine are written by a population of adults that are generally proficient English writers. This is different from most previous work where the population is generally composed of middle- and high-school students with varying levels of English proficiency. • The average length of these re"
W16-0524,J11-1005,0,\N,Missing
W17-1605,W99-0411,1,0.465897,"the fairness of test scores as measured by the impact of construct-irrelevant factors. As Xi (2010) discusses in detail, unfair decisions based on scores assigned to test-takers from oft-disadvantaged 42 their own biases (Feldman et al., 2015) either due to an existing bias in the training data or due to a minority group being inadequately represented in the training data. Automated scoring is certainly not immune to such biases and, in fact, several studies have documented differing performance of automated scoring models for test-takers with different native languages or with disabilities (Burstein and Chodorow, 1999; Bridgeman et al., 2012; Wang and von Davier, 2014; Wang et al., 2016; An et al., 2016; Loukina and Buzick, In print). groups are likely to have profound consequences: they may be denied career opportunities and access to resources that they deserve. Therefore, it is important to ensure — among other things — that construct-irrelevant factors do not introduce systematic biases in test scores, irrespective of whether they are produced by human raters or by an automated scoring system. Over the last few years, there has been a significant amount of work done on ensuring fairness, accountability"
W17-1605,P98-1032,1,0.678341,"Missing"
W17-1605,W15-0610,1,0.829664,"en linear models with a small number of interpretable features because such models lend themselves more easily to a detailed fairness review and allow decisionmakers to understand how, and to what extent, different parts of the test-takers’ skill set are being covered by the features in the model (Loukina et al., 2015). For such linear models, RSMTool displays a detailed model description including the model fit (R2 ) computed on the training set as well as the contribution of each feature to the final score (via raw, standardized, and relative coefficients). At the same time, recent studies (Heilman and Madnani, 2015; Madnani et al., 2016) on scoring actual content rather than just language proficiency suggest that it is possible to achieve higher performance, as measured by agreement with human raters, by employing many low-level features and more sophisticated machine learning algorithms such as support vector machines or random forests. Generally, these models are built using sparse feature types such as word n-grams, often resulting in hundreds of thousands of predominantly binary features. Using models with such a large feature space means that it is no longer clear how to map the individual features"
W17-1605,D13-1180,0,0.0217575,"automated scoring model and a set of psychometric and statistical analyses aimed at detecting possible bias in 43 cus solely on the fairness-driven evaluation capabilities of the tool that are directly relevant to the issues we have discussed so far. Readers interested in other parts of the RSMToolare referred to the comprehensive documentation available at http://rsmtool.readthedocs.org. Before we describe the fairness analyses implemented in the tool, we want to acknowledge that there are many different ways in which researchers might approach building as well as evaluating scoring models (Chen and He, 2013; Shermis, 2014a). The list of learners and fairness analyses the tool provides is not, and cannot be, exhaustive. In fact, later in the paper, we discuss some analyses that could be implemented in future versions of the tool since one of the core characteristics of the tool is its flexible architecture. See §4.4 for more details. In the next section, we present in detail the analyses incorporated into RSMTool aimed at detecting the various sources of biases we introduced earlier. As it is easier to show the analyses in the context of an actual example, we use data from the Hewlett Foundation"
W17-1605,W15-0602,1,0.886546,"ws users to integrate various components of RSMTool into their own applications. 4.5 The automated scoring models used in systems such as e-rater for assessing writing proficiency in English (Attali and Burstein, 2006) or SpeechRater for spoken proficiency (Zechner et al., 2009) have traditionally been linear models with a small number of interpretable features because such models lend themselves more easily to a detailed fairness review and allow decisionmakers to understand how, and to what extent, different parts of the test-takers’ skill set are being covered by the features in the model (Loukina et al., 2015). For such linear models, RSMTool displays a detailed model description including the model fit (R2 ) computed on the training set as well as the contribution of each feature to the final score (via raw, standardized, and relative coefficients). At the same time, recent studies (Heilman and Madnani, 2015; Madnani et al., 2016) on scoring actual content rather than just language proficiency suggest that it is possible to achieve higher performance, as measured by agreement with human raters, by employing many low-level features and more sophisticated machine learning algorithms such as support"
W17-1605,C14-1090,1,0.844936,"view the features and ensure that their description and method of computation are in line with the definition of the specific set of skills that the given test purports to measure (Deane, 2013). However, features incorporated into a modern automated scoring system often rely on multiple underlying NLP components such as part-of-speech taggers and syntactic parsers as well as complex computational algorithms and, therefore, a qualitative review may not be sufficient. Furthermore, some aspects of spoken or written text can only be measured indirectly given the current state of NLP technologies (Somasundaran et al., 2014). RSMTool allows the user to explore the quantitative effect of two types of construct-irrelevant factors that may affect feature performance: categorical and continuous. 4.1.1 4.1.2 Continuous Factors This type of construct-irrelevant factors includes continuous covariates which despite being correlated with human scores are either not directly relevant to the construct measured by the test or, even if they are, should not be the primary contributor to the model’s predictions. Response length, as previously discussed, is an example of such covariates. Even though it provides an important indi"
W17-1605,W16-0524,1,0.808858,"ll number of interpretable features because such models lend themselves more easily to a detailed fairness review and allow decisionmakers to understand how, and to what extent, different parts of the test-takers’ skill set are being covered by the features in the model (Loukina et al., 2015). For such linear models, RSMTool displays a detailed model description including the model fit (R2 ) computed on the training set as well as the contribution of each feature to the final score (via raw, standardized, and relative coefficients). At the same time, recent studies (Heilman and Madnani, 2015; Madnani et al., 2016) on scoring actual content rather than just language proficiency suggest that it is possible to achieve higher performance, as measured by agreement with human raters, by employing many low-level features and more sophisticated machine learning algorithms such as support vector machines or random forests. Generally, these models are built using sparse feature types such as word n-grams, often resulting in hundreds of thousands of predominantly binary features. Using models with such a large feature space means that it is no longer clear how to map the individual features and their weights to v"
W17-1605,W15-0625,0,0.053079,"easurement guidelines currently implemented in RSMTool follow the psychometric framework suggested by Williamson et al. (2012). It was developed for the evaluation of e-rater, an automated system designed to score English writing proficiency (Attali and Burstein, 2006), but is generalizable to other applications of automated scoring. This framework was chosen because it offers a comprehensive set of criteria for both the accuracy as well as the fairness of the predicted scores. Note that not all of these recommendations are universally accepted by the automated scoring community. For example, Yannakoudakis and Cummins (2015) recently proposed a different set of metrics for evaluating the accuracy of automated scoring models. Furthermore, the machine learning community has recently developed various analyses aimed at detecting bias in algorithm performance that could be applied in the context of automated scoring. For example, in addition to reviewing individual features, one could also attempt to predict the subgroup membership from the features used to score the responses (Feldman et al., 2015). If this Figure 2: The performance of our scoring model (R2 ) for different subgroups of test-takers as defined by thei"
W17-1605,C98-1032,1,\N,Missing
W17-4609,N09-1050,0,0.435319,"ters, length bin 7 (blog2 151c) would be the binary feature that gets a value of 1 with the other two bins getting the value of 0. The second set of features (“quality”) captures how much the pronunciation of individual segments deviates from the pronunciation that would be expected from a proficient speaker. This includes the average confidence scores and acoustic model scores computed by the ASR system for the words in the 1-best ASR hypothesis. Since the ASR is trained on a wide range of proficiency levels, we also include features computed using the two-pass approach (Herron et al., 1999; Chen et al., 2009). In this approach, the acoustic model scores for words in the ASR hypothesis are recomputed using acoustic models trained on native We refer to these features as “text-driven” features in subsequent sections. 2 Speech-driven features See Table 3 in Burrows et al. (2015) for a detailed list. 70 speakers of English. The third set of features captures pausing patterns in the response such as mean duration of pauses, mean number of words between two pauses, and the ratio of pauses to speech. For all features in this group the pauses were determined based on silences in the ASR output. Only silenc"
W17-4609,N15-1111,1,0.937468,"ave some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of pauses Patterns of durations of individual segments Time intervals between stressed syllables Nf eat 3 6 9 r .42 .41"
W17-4609,S13-2045,0,0.0218939,"nguage proficiency and the features in this group have some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of pauses Patterns of durations of individual segments Time intervals"
W17-4609,W15-0605,0,0.0163845,"ent) & speech-driven features (for proficiency) will perform better than the individual text-only and speech-only models. Although it may seem obvious that, given the nature of the task, a model using both speechbased and content-based features should outperform models using only one of them, it may not turn out that way. Multiple studies that have developed new features measuring vocabulary, grammar or content for spoken responses have reported only limited improvements when these features were combined with features based on fluency and pronunciation (Bhat and Yoon, 2015; Yoon et al., 2012; Somasundaran et al., 2015). Crossley and McNamara (2013) used a large set of text-based measures including Coh-Metrix (Graesser et al., 2004) to obtain fairly accurate predictions of proficiency scores for spoken responses to general questions similar to the ones used in this study based on transcription only, without using any information based on acoustic analysis of speech. It is not possible to establish from published results how their system would compare to the one that also evaluates pronunciation and fluency. They did not compute any such features and their results based on text are not directly comparable to"
W17-4609,W09-2509,0,0.0139613,"has been consistently identified as one of the major covariates of language proficiency and the features in this group have some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of paus"
W17-4609,W16-0514,1,0.524425,"xt-driven features was significantly better than both the text-only model as well as the speech-only model. The effect size of the improvement over the text-only model was small with the average R2 increasing only slightly from .335 to .352 for source-based questions and from .431 to .442 for general questions (p = 0.002). Model text + speech text-only speech-only Xie et al. Loukina & Cahill general source-based .60 .67 .59 .66 .58 .63 .40 .59 .64 (overall) Table 4: Average Pearson’s r achieved by the three of the models in this study and the best performing models reported in the literature; Loukina and Cahill (2016) combine language proficiency features from speech and text and do not report performance by question type; Xie et al. (2012) use content features based on cosine similarity but no other language proficiency features. If a paper reports results based on both ASR hypothesis and human transcription, we only use the results based on ASR hypothesis. 2. The performance of the text-only model was significantly better than the performance of each of the 5 models trained using only one group of speech-driven features (p &lt; 0.0001). 3. There was no significant difference between the performance of the t"
W17-4609,P11-1076,0,0.0132585,"ajor covariates of language proficiency and the features in this group have some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of pauses Patterns of durations of individual"
W17-4609,N12-1011,0,0.0358578,"provement over the text-only model was small with the average R2 increasing only slightly from .335 to .352 for source-based questions and from .431 to .442 for general questions (p = 0.002). Model text + speech text-only speech-only Xie et al. Loukina & Cahill general source-based .60 .67 .59 .66 .58 .63 .40 .59 .64 (overall) Table 4: Average Pearson’s r achieved by the three of the models in this study and the best performing models reported in the literature; Loukina and Cahill (2016) combine language proficiency features from speech and text and do not report performance by question type; Xie et al. (2012) use content features based on cosine similarity but no other language proficiency features. If a paper reports results based on both ASR hypothesis and human transcription, we only use the results based on ASR hypothesis. 2. The performance of the text-only model was significantly better than the performance of each of the 5 models trained using only one group of speech-driven features (p &lt; 0.0001). 3. There was no significant difference between the performance of the text-only model and the 5 models combining the text-driven features with each of the individual speechdriven feature sets. 4.2"
W17-4609,W12-2021,0,0.219222,"features (for content) & speech-driven features (for proficiency) will perform better than the individual text-only and speech-only models. Although it may seem obvious that, given the nature of the task, a model using both speechbased and content-based features should outperform models using only one of them, it may not turn out that way. Multiple studies that have developed new features measuring vocabulary, grammar or content for spoken responses have reported only limited improvements when these features were combined with features based on fluency and pronunciation (Bhat and Yoon, 2015; Yoon et al., 2012; Somasundaran et al., 2015). Crossley and McNamara (2013) used a large set of text-based measures including Coh-Metrix (Graesser et al., 2004) to obtain fairly accurate predictions of proficiency scores for spoken responses to general questions similar to the ones used in this study based on transcription only, without using any information based on acoustic analysis of speech. It is not possible to establish from published results how their system would compare to the one that also evaluates pronunciation and fluency. They did not compute any such features and their results based on text are"
W17-4609,W15-0612,0,0.0223131,"he features in this group have some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of pauses Patterns of durations of individual segments Time intervals between stressed syllables"
W17-4609,J11-1005,0,0.0609992,"Missing"
W17-5052,Q13-1032,0,0.0143644,"and did not investigate any questions about specific modeling strategies such as the choice of learning algorithm or the impact of hyper-parameter tuning. be useful in guiding further development of supervised content scoring models. 2 Related Work Content scoring is sometimes also referred to in the literature as “short-answer” scoring. Although it is true that many content-based questions tend to be very specific and elicit responses that are relatively short, this is not always the case. Previously published studies have considered responses that span a range of lengths — from a few words (Basu et al., 2013) to a few dozen words (Madnani et al., 2013; Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Su"
W17-5052,D13-1180,0,0.0163748,"on457 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 457–467 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Our study is different from the work we have discussed so far in that its goal is not simply to obtain the best performance for a given question or a set of questions. Instead, we focus on metaanalyses of scoring performance as a function of modeling strategies and data set characteristics. Some previous studies have considered the choice of learner in automated scoring for writing quality. Chen and He (2013) compared support vector classification, regression, and ranking for automatically scoring writing quality using a single dataset. Chen et al. (2016) reported that using support vector regression with a radial kernel produced better performance than a simple linear model. In addition, several studies (Feng et al., 2003; Haberman and Sinharay, 2010; Santos et al., 2012) have consistently reported that use of probabilistic classifiers such as cumulative logistic regression might be more appropriate for the task of automated scoring than linear regression since such models incorporate the assumpt"
W17-5052,S13-2045,0,0.0134709,"he authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-based approaches. Since"
W17-5052,W15-1905,0,0.0142042,"Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-ba"
W17-5052,W15-0610,1,0.919218,"ber of continuous-valued features. More generally in the machine learning literature, papers have analyzed and compared the performance of different learning algorithms on standard machine learning datasets from the UCI repository and/or synthetic datasets (Caruana and Niculescu-Mizil, 2006; Matykiewicz and Pestian, 2012; Doan and Kalita, 2015). These studies reported substantial variability in learner performance across problems which suggests that the learners that performed best for other applications may not necessarily do so for our task. The work that might be closest to ours is that of Heilman and Madnani (2015) who explored the impact of the amount of training data available on content scoring performance across a range of questions. However, they used a much smaller set of 44 questions and did not investigate any questions about specific modeling strategies such as the choice of learning algorithm or the impact of hyper-parameter tuning. be useful in guiding further development of supervised content scoring models. 2 Related Work Content scoring is sometimes also referred to in the literature as “short-answer” scoring. Although it is true that many content-based questions tend to be very specific a"
W17-5052,S13-1041,0,0.0176789,"trategies such as the choice of learning algorithm or the impact of hyper-parameter tuning. be useful in guiding further development of supervised content scoring models. 2 Related Work Content scoring is sometimes also referred to in the literature as “short-answer” scoring. Although it is true that many content-based questions tend to be very specific and elicit responses that are relatively short, this is not always the case. Previously published studies have considered responses that span a range of lengths — from a few words (Basu et al., 2013) to a few dozen words (Madnani et al., 2013; Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015)"
W17-5052,W15-0612,0,0.275744,"on or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-based approaches. Since several of the questions in"
W17-5052,W13-1722,1,0.927832,"Missing"
W17-5052,N15-1111,1,0.947411,"tailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-based approaches. Since several of the questions in our dataset are relatively open-ended and we have sufficient scored data available for all of them, we focus on the response-based approach in this paper. 2.1 Research Questions We aim to answer the following specific questions about supervised learning specifically in the context of automated scoring of content: 1. What type of learner has the best performance for respo"
W17-5052,W16-0524,1,0.927767,"ithm or the impact of hyper-parameter tuning. be useful in guiding further development of supervised content scoring models. 2 Related Work Content scoring is sometimes also referred to in the literature as “short-answer” scoring. Although it is true that many content-based questions tend to be very specific and elicit responses that are relatively short, this is not always the case. Previously published studies have considered responses that span a range of lengths — from a few words (Basu et al., 2013) to a few dozen words (Madnani et al., 2013; Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number o"
W17-5052,W12-2424,0,0.0289494,"e consistently reported that use of probabilistic classifiers such as cumulative logistic regression might be more appropriate for the task of automated scoring than linear regression since such models incorporate the assumption that the score is categorical in nature. All of these studies used a small number of continuous-valued features. More generally in the machine learning literature, papers have analyzed and compared the performance of different learning algorithms on standard machine learning datasets from the UCI repository and/or synthetic datasets (Caruana and Niculescu-Mizil, 2006; Matykiewicz and Pestian, 2012; Doan and Kalita, 2015). These studies reported substantial variability in learner performance across problems which suggests that the learners that performed best for other applications may not necessarily do so for our task. The work that might be closest to ours is that of Heilman and Madnani (2015) who explored the impact of the amount of training data available on content scoring performance across a range of questions. However, they used a much smaller set of 44 questions and did not investigate any questions about specific modeling strategies such as the choice of learning algorithm or"
W17-5052,W11-2401,0,0.0204622,"ords (Basu et al., 2013) to a few dozen words (Madnani et al., 2013; Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a se"
W17-5052,W15-0625,0,0.122365,"eal values on a continuous scale. One advantage of the real-valued scores is that they allow for more fine-grained distinction than a small set of integers. In this paper, we predict real-valued scores on a continuous scale and evaluate the accuracy of the predicted scores by using mean squared error (MSE) as our default metric. Although some previous studies have used quadratically-weighted kappa (QWK) as another possible metric for evaluating content-scoring models, more recent work has shown that QWK may possess properties that render it less than suitable for automated scoring evaluation (Yannakoudakis and Cummins, 2015). 3.1 been shown to perform well with feature sets comparable to ours in previously published work — Mohler et al. (2011), Sakaguchi et al. (2015), and Zesch et al. (2015) all used support vector machines; Ramachandran et al. (2015) use a random forest regressor — or (b) are generally known to perform well with a large number of sparse features (Hastie et al., 2001; Fan et al., 2008; Chang and Lin, 2011). We use the scikit-learn (Pedregosa et al., 2011) implementations for all learners. All the implementations incorporate some means of reducing learner variance either by design — random forest"
W17-5052,P11-1076,0,0.350434,"answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-"
W17-5052,W15-0615,1,0.936647,"g high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-based approaches. Since several of the questions in our dataset are rel"
W17-5052,J11-1005,0,0.0219845,"in responses to questions on a given subject. Madnani et al., 2016). We extract the following features for all of the responses in our corpus: scoring? Do non-linear learners offer a substantial advantage over linear models? Do margin-based methods such as support vector machines perform better than bagging ensembles like random forests? (a) character n-grams including whitespace and punctuation (n=2–5) 2. How do probabilistic classifiers perform compared to regressors when predicting realvalued scores? (b) word n-grams (n=1,2) (c) triples extracted over dependency parses obtained from ZPar (Zhang and Clark, 2011), and 3. Do hyper-parameters matter? Is it worth spending extra time and effort to tune the hyper-parameters of any given learner over simply using the default values provided by the implementation being used? 3 (d) length bins (specifically, whether the log of 1 plus the number of characters in the response, rounded down to the nearest integer, equals x, for all possible x from the training set). For example, consider a question for which the responses in the training data are between 50 and 200 characters long. For this question, we will have 3 length bins numbered from 5 (blog2 51c) to 7 (b"
W18-0506,P15-1166,0,0.0884005,"Missing"
W18-0506,W18-0544,0,0.0214525,"6 .432 .175 .389 ↑ Team 1 2 2 4 5 6 7 8 8 10 10 10 13 14 15 SanaLabs ♢♣ singsound ♢ NYU ♣‡ CECL ‡ TMU ♢‡ Cambridge ♢ UCSD ♣ LambdaLab ♣ Grotoco nihalnayak jilljenn ymatusevych simplelinear renhk SLAM_baseline AUC F1 .857 .854 .854 .843 .839 .835 .823 .815 .813 .811 .809 .808 .807 .796 .771 .573 .569 .493 .487 .502 .508 .442 .415 .502 .431 .406 .441 .394 .481 .281 Table 2: Final results. Ranks (↑) are determined by statistical ties (see text). Markers indicate which systems include recurrent neural architectures (♢), decision tree ensembles (♣), or a multitask model across all tracks (‡). TMU (Kaneko et al., 2018) used a combination of two bidirectional RNNs—the ﬁrst to predict potential user errors at a given token, and a second to track the history of previous answers by each user. These networks were jointly trained through a uniﬁed objective function. The authors did not engineer any additional features, but did train a single model for all three tracks (using a track ID feature to distinguish among them). CECL (Bestgen, 2018) used a logistic regression approach. The base feature set was expanded to include many feature conjunctions, including word n-grams crossed with the token, user, format, and"
W18-0506,W18-0523,0,0.0290911,"mbdaLab (Chen et al., 2018) used GBDT models independently for each track, deriving their features from conﬁrmatory analysis of psychologically-motivated hypotheses on the TRAIN set. These include proxies for student engagement, spacing effect, response time, etc. nihalnayak (Nayak and Rao, 2018) used a logistic regression model similar to the baseline, but added features inspired by research in codemixed language-learning where context plays an important role. In particular, they included word, part of speech, and metaphone features for previous:current and current:next token pairs. Grotoco (Klerke et al., 2018) also used logistic regression, including word lemmas, frequency, cognates, and user-speciﬁc features such as word error rate. Interestingly, the authors found that ignoring each user’s ﬁrst day of exercise data improved their predictions, suggesting that learners ﬁrst needed to familiarize themselves with app before their data were reliable for modeling. jilljenn (Vie, 2018) used a deep factorization machine (DeepFM), a neural architecture developed for click-through rate prediction in recommender systems. This model allows learning from both lower-order and higher-order induced features and"
W18-0506,P16-1231,0,0.0322794,"Missing"
W18-0506,W18-0542,0,0.0212714,"ies (see text). Markers indicate which systems include recurrent neural architectures (♢), decision tree ensembles (♣), or a multitask model across all tracks (‡). TMU (Kaneko et al., 2018) used a combination of two bidirectional RNNs—the ﬁrst to predict potential user errors at a given token, and a second to track the history of previous answers by each user. These networks were jointly trained through a uniﬁed objective function. The authors did not engineer any additional features, but did train a single model for all three tracks (using a track ID feature to distinguish among them). CECL (Bestgen, 2018) used a logistic regression approach. The base feature set was expanded to include many feature conjunctions, including word n-grams crossed with the token, user, format, and session features provided with the data set. Cambridge (Yuan, 2018) trained two RNNs— a sequence labeler, and a sequence-to-sequence model taking into account previous answers—and found that averaging their predictions yielded the best results. They focused on the English track, experimenting with additional features derived from other English learner corpora. Hyper-parameters were tuned for English and used as-is for oth"
W18-0506,W17-5007,0,0.0456064,"1st team 2nd team 3rd team Average (all) 4th team Ensemble Analysis English Spanish French .995 .867 .867 .861 .861 .859 .857 .848 .996 .844 .843 .838 .835 .835 .832 .824 .993 .863 .863 .857 .854 .854 .852 .843 Table 5: AUC results for the ensemble analysis. Another interesting research question is: what is the upper-bound for this task? This can be estimated by treating each team’s best submission as an independent system, and combining the results using ensemble methods in a variety of ways. Such analyses have been previously applied to other shared task challenges and meta-analyses (e.g., Malmasi et al., 2017). The oracle system is meant to be an upperbound: for each token in the TEST set, the oracle outputs the team prediction with the lowest error for that particular token. We also experiment with stacking (Wolpert, 1992) by training a logistic regression classiﬁer using each team’s prediction as an input feature6 . Finally, we also pool system predictions together by taking their average (mean). Table 5 reports AUC for various ensemble methods as well as some of the top performing team systems for all three tracks. Interestingly, the oracle is exceptionally accurate (&gt;.993 AUC and &gt;.884 F1, not"
W18-0506,W18-0524,0,0.0220019,"t classiﬁer with a set of engineered features motivated by previous research in memory and linguistic effects in SLA, including “word neighborhoods,” corpus frequency, cognates, and repetition/experience with a given word. The system also included features speciﬁc to each user, such as mean and variance of error rates. LambdaLab (Chen et al., 2018) used GBDT models independently for each track, deriving their features from conﬁrmatory analysis of psychologically-motivated hypotheses on the TRAIN set. These include proxies for student engagement, spacing effect, response time, etc. nihalnayak (Nayak and Rao, 2018) used a logistic regression model similar to the baseline, but added features inspired by research in codemixed language-learning where context plays an important role. In particular, they included word, part of speech, and metaphone features for previous:current and current:next token pairs. Grotoco (Klerke et al., 2018) also used logistic regression, including word lemmas, frequency, cognates, and user-speciﬁc features such as word error rate. Interestingly, the authors found that ignoring each user’s ﬁrst day of exercise data improved their predictions, suggesting that learners ﬁrst needed"
W18-0506,W18-0543,0,0.0224296,"y focused on the English track, experimenting with additional features derived from other English learner corpora. Hyper-parameters were tuned for English and used as-is for other tracks, with comparable results. UCSD (Tomoschuk and Lovelett, 2018) used a random forest classiﬁer with a set of engineered features motivated by previous research in memory and linguistic effects in SLA, including “word neighborhoods,” corpus frequency, cognates, and repetition/experience with a given word. The system also included features speciﬁc to each user, such as mean and variance of error rates. LambdaLab (Chen et al., 2018) used GBDT models independently for each track, deriving their features from conﬁrmatory analysis of psychologically-motivated hypotheses on the TRAIN set. These include proxies for student engagement, spacing effect, response time, etc. nihalnayak (Nayak and Rao, 2018) used a logistic regression model similar to the baseline, but added features inspired by research in codemixed language-learning where context plays an important role. In particular, they included word, part of speech, and metaphone features for previous:current and current:next token pairs. Grotoco (Klerke et al., 2018) also u"
W18-0506,W13-3601,0,0.256116,"sks, and can be interpreted as the probability that the system will rank a randomly-chosen error above a randomlychosen non-error. We argue that this notion of ranking quality is particularly useful for evaluating systems that might be used for personalized learning, e.g., if we wish to prioritize words or exercises for an individual learner’s review based on how likely they are to have forgotten or make errors at a given point in time. We also report F1 score—the harmonic mean of precision and recall—as a secondary metric, since it is more common in similar skewed-class labeling tasks (e.g., Ng et al., 2013). Note, however, that F1 can be signiﬁcantly improved simply by tuning the classiﬁcation threshold (ﬁxed at 0.5 for our evaluations) without affecting AUC. 3 Results A total of 15 teams participated in the task, of which 13 responded to a brief survey about their approach, and 11 submitted system description papers. All but two of these teams submitted predictions for all three language tracks. Ofﬁcial shared task results are reported in Table 2. System ranks are determined by sorting teams according to AUC, and using DeLong’s test (DeLong et al., 1988) to identify statistical ties. For the re"
W18-0506,W18-0526,0,0.0213776,"everal token context features, and learner/token history features such as number of times seen, time since last practice, etc. singsound (Xu et al., 2018) used an RNN architecture using four types of encoders, representing different types of features: token context, linguistic information, user data, and exercise format. The RNN decoder integrated information from all four encoders. Ablation experiments revealed the context encoder (representing the token) contributed the most to model performance, while the linguistic encoder (representing grammatical information) contributed the least. NYU (Rich et al., 2018) used an ensemble of GBDTs with features engineered based on psychological theories of cognition. Predictions for each track were averaged between a track-speciﬁc model and a uniﬁed model (trained on data from all three tracks). In addition to the word, user, and exercise features provided, the authors included word lemmas, corpus frequency, L1-L2 cognates, and features indicating user motivation and diligence (derived from usage patterns), and others. Ablation studies indicated that most of the performance was due to the user and token features. We use area under the ROC curve (AUC) as the pr"
W18-0506,P16-1174,1,0.620758,"dditive factor IRT models. For decades, tutoring systems have also employed sequence models like HMMs to perform knowledge tracing (Corbett and Anderson, 1995), a way of estimating a learner’s mastery of knowledge over time. RNN-based approaches that encode user performance over time (i.e., that span across exercises) are therefore variants of deep knowledge tracing (Piech et al., 2015). Relatedly, the spacing effect (Dempster, 1989) is the observation that people will not only learn but also forget over time, and they remember more effectively through scheduled practices that are spaced out. Settles and Meeder (2016) and Ridgeway et al. (2017) recently proposed non-linear regressions that explicitly encode the rate of forgetting as part of a decision surface, however none of the current teams chose to do this. Instead, forgetting was either modeled through engineered features (e.g., user/token histories), or opaquely handled by sequential RNN architectures. SLA modeling also bears some similarity to research in grammatical error detection (Leacock et al., 2010) and correction (Ng et al., 2013). For these tasks, a model is given a (possibly ill-formed) sequence of words produced by a learner, and 5 Meta-An"
W18-0506,W18-0527,0,0.0309412,"base feature set was expanded to include many feature conjunctions, including word n-grams crossed with the token, user, format, and session features provided with the data set. Cambridge (Yuan, 2018) trained two RNNs— a sequence labeler, and a sequence-to-sequence model taking into account previous answers—and found that averaging their predictions yielded the best results. They focused on the English track, experimenting with additional features derived from other English learner corpora. Hyper-parameters were tuned for English and used as-is for other tracks, with comparable results. UCSD (Tomoschuk and Lovelett, 2018) used a random forest classiﬁer with a set of engineered features motivated by previous research in memory and linguistic effects in SLA, including “word neighborhoods,” corpus frequency, cognates, and repetition/experience with a given word. The system also included features speciﬁc to each user, such as mean and variance of error rates. LambdaLab (Chen et al., 2018) used GBDT models independently for each track, deriving their features from conﬁrmatory analysis of psychologically-motivated hypotheses on the TRAIN set. These include proxies for student engagement, spacing effect, response tim"
W18-0506,W18-0545,0,0.023023,"esearch in codemixed language-learning where context plays an important role. In particular, they included word, part of speech, and metaphone features for previous:current and current:next token pairs. Grotoco (Klerke et al., 2018) also used logistic regression, including word lemmas, frequency, cognates, and user-speciﬁc features such as word error rate. Interestingly, the authors found that ignoring each user’s ﬁrst day of exercise data improved their predictions, suggesting that learners ﬁrst needed to familiarize themselves with app before their data were reliable for modeling. jilljenn (Vie, 2018) used a deep factorization machine (DeepFM), a neural architecture developed for click-through rate prediction in recommender systems. This model allows learning from both lower-order and higher-order induced features and their interactions. The DeepFM outperformed a simple logistic regression baseline without much additional feature engineering. Other teams did not submit system description papers. However, according to a task organizer survey ymatusevych used a linear model with multilingual word embeddings, corpus frequency, and several L1-L2 features such as cognates. Additionally, simplel"
W18-0506,W18-0546,0,0.0216621,"deling choice indicators (♢, ♣, ‡), which we discuss further in §5. SanaLabs (Nilsson et al., 2018) used a combination of recurrent neural network (RNN) predictions with those of a Gradient Boosted Decision Tree (GBDT) ensemble, trained independently for each track. This was motivated by the observation that RNNs work well for sequence data, while GBDTs are often the best-performing non-neural model for shared tasks using tabular data. They also engineered several token context features, and learner/token history features such as number of times seen, time since last practice, etc. singsound (Xu et al., 2018) used an RNN architecture using four types of encoders, representing different types of features: token context, linguistic information, user data, and exercise format. The RNN decoder integrated information from all four encoders. Ablation experiments revealed the context encoder (representing the token) contributed the most to model performance, while the linguistic encoder (representing grammatical information) contributed the least. NYU (Rich et al., 2018) used an ensemble of GBDTs with features engineered based on psychological theories of cognition. Predictions for each track were averag"
W18-0506,W18-0547,0,0.0228201,"o predict potential user errors at a given token, and a second to track the history of previous answers by each user. These networks were jointly trained through a uniﬁed objective function. The authors did not engineer any additional features, but did train a single model for all three tracks (using a track ID feature to distinguish among them). CECL (Bestgen, 2018) used a logistic regression approach. The base feature set was expanded to include many feature conjunctions, including word n-grams crossed with the token, user, format, and session features provided with the data set. Cambridge (Yuan, 2018) trained two RNNs— a sequence labeler, and a sequence-to-sequence model taking into account previous answers—and found that averaging their predictions yielded the best results. They focused on the English track, experimenting with additional features derived from other English learner corpora. Hyper-parameters were tuned for English and used as-is for other tracks, with comparable results. UCSD (Tomoschuk and Lovelett, 2018) used a random forest classiﬁer with a set of engineered features motivated by previous research in memory and linguistic effects in SLA, including “word neighborhoods,” c"
W18-2504,W12-3202,0,0.726029,"handled through volunteer efforts coordinated by the Anthology editor. Running a key service for the computational linguistics community that needs to be continuously available and updated frequently is one of the main issues in administering the Anthology. We discuss this issue along with the challenges of running a large scale project on a volunteer basis and its resulting technical debt. As we look towards the future, previous research has shown that it can also be used as a data source to characterize the work and workings of the ACL community (Bird et al., 2008; Vogel and Jurafsky, 2012; Anderson et al., 2012). Extensions to the Anthology that build on this information could make the Anthology an even more valuable resource for the community. We will discuss two possibl eextensions – anonymous pre-prints and support for finding relevant submission reviewers by linking au2 https://creativecommons.org/licenses/ by-nc-sa/3.0/ 3 https://creativecommons.org/licenses/ by/4.0/ thors in the Anthology with their research interests and community connections. Beyond being useful in itself, work on such challenges has the potential to motivate the ACL community to further support the Anthology. 2 Current State"
W18-2504,bird-etal-2008-acl,1,0.930936,"e maintenance of the code and the website is handled through volunteer efforts coordinated by the Anthology editor. Running a key service for the computational linguistics community that needs to be continuously available and updated frequently is one of the main issues in administering the Anthology. We discuss this issue along with the challenges of running a large scale project on a volunteer basis and its resulting technical debt. As we look towards the future, previous research has shown that it can also be used as a data source to characterize the work and workings of the ACL community (Bird et al., 2008; Vogel and Jurafsky, 2012; Anderson et al., 2012). Extensions to the Anthology that build on this information could make the Anthology an even more valuable resource for the community. We will discuss two possibl eextensions – anonymous pre-prints and support for finding relevant submission reviewers by linking au2 https://creativecommons.org/licenses/ by-nc-sa/3.0/ 3 https://creativecommons.org/licenses/ by/4.0/ thors in the Anthology with their research interests and community connections. Beyond being useful in itself, work on such challenges has the potential to motivate the ACL community"
W18-2504,buitelaar-etal-2014-hot,0,0.0232717,"institution, contributors that work on Anthology-related system administration and development tasks have been recruited in response to calls for volunteers at the main ACL conferences. In contrast, new features have been developed by researchers using the ACL Anthology as a resource in their own work, unconnected with the daily operation of the Anthology. Such research deliverables include, for example, the creation of a corpus of research papers (Bird et al., 2008), an author citation network (Radev et al., 2013) or a 4 https://www.softconf.com/ faceted search engine (Sch¨afer et al., 2012; Buitelaar et al., 2014). These factors, in combination with the multiple, changing responsibilities and shifting research interests of community members, mean that new volunteers join and leave the Anthology team in unpredictable and sporadic patterns. Preserving knowledge about the Anthology’s operational workflow is thus one of the most important challenges for the Anthology. The Anthology editor has played a key role ensuring the continuity of the entire project. This position has so far always been filled for multiple years, longer than the normal time frame for an ACL officer. The role has been critical in ensu"
W18-2504,W12-3209,1,0.776602,"lenge of reviewer matching we encourage the community to rally towards. 1 Introduction The ACL Anthology1 is a service offered by the Association for Computational Linguistics (ACL) allowing open access to the proceedings of all ACL sponsored conferences and journal articles. As a community goodwill gesture, it also hosts third-party computational linguistics literature from sister organizations and their national venues. It offers both text and faceted search of the indexed papers, author-specific pages, and can incorporate third-party metadata and services that can be embedded within pages (Bysani and Kan, 2012). As of this paper, it hosts over 1 https://aclanthology.info/ Min-Yen Kan School of Computing National University of Singapore kanmy@comp.nus.edu.sg 43,000 computational linguistics and natural language processing papers, along with their metadata. Over 4,500 daily requests are served by the Anthology. The code for the Anthology is available at https://github.com/acl-org/ acl-anthology under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License2 . Slightly different from the Anthology source code, ACL also licenses its papers with a more liberal license, supporti"
W18-2504,P11-4002,0,0.0607328,"Missing"
W18-2504,W12-3210,0,0.0498541,"Missing"
W18-2504,W12-3204,0,0.606146,"e code and the website is handled through volunteer efforts coordinated by the Anthology editor. Running a key service for the computational linguistics community that needs to be continuously available and updated frequently is one of the main issues in administering the Anthology. We discuss this issue along with the challenges of running a large scale project on a volunteer basis and its resulting technical debt. As we look towards the future, previous research has shown that it can also be used as a data source to characterize the work and workings of the ACL community (Bird et al., 2008; Vogel and Jurafsky, 2012; Anderson et al., 2012). Extensions to the Anthology that build on this information could make the Anthology an even more valuable resource for the community. We will discuss two possibl eextensions – anonymous pre-prints and support for finding relevant submission reviewers by linking au2 https://creativecommons.org/licenses/ by-nc-sa/3.0/ 3 https://creativecommons.org/licenses/ by/4.0/ thors in the Anthology with their research interests and community connections. Beyond being useful in itself, work on such challenges has the potential to motivate the ACL community to further support the An"
W19-4401,P16-2096,0,0.0571079,"ot immediately related to the construct (i.e., “constructirrelevant”) may indicate that the test is unfair (Xi, 2010; Zieky, 2016). Notably such “constructThe issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups (Kamiran and Calders, 2009; Kamishima et al., 2012; Luong et al., 2011; Zemel et al., 2013; Feldman et al., 2015; Friedler et al., 2016). Like any algorithm, NLP systems are not immune to such bias (Hovy and Spruit, 2016; Caliskan et al., 2017). These days it is hardly necessary to justify the importance of ensuring algorithmic fairness, especially in applications that can have a substantial impact on users’ lives. 1 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–10 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics known outcome; (4) Conditional use accuracy equality: the prediction is equally accurate for all groups when conditioned on a predicted outcome; (5) Treatment equality: the ratio of false negatives and f"
W19-4401,W17-1605,1,0.652202,"Missing"
W19-4401,W17-5007,0,0.0409918,"during a large-scale assessment of English language speaking proficiency. For this assessment all test-takers answered 6 questions that elicited spontaneous speech. Depending on the question, the speakers were given 45 seconds or 1 minute to record their responses. We will focus on whether automated scoring might disadvantage test-takers depending on their native language (L1), a common concern in automated scoring contexts. Learners with different L1 might have different linguistic profile and it has been shown that it is possible to identify learner L1 from their written or spoken response (Malmasi et al., 2017). Therefore there is a danger that the scoring engine might inadvertently assign different scores to speakers of different L1 even when there is no difference in English proficiency, the actual construct measured by the test. In other words the system would introduce group-related construct-irrelevant differences. In an actual operational scenario there are many additional factors that can introduce bias to the performance of an automated scoring system: some L1s might be over- or under-represented in the data used for model training and evaluation; sometimes different versions of the test are"
W19-4432,W16-0514,0,0.0215391,"generator. Errors at the ASR stage may negatively affect the content features such that they are noisy and distorted to some extent. Secondly, and more importantly, spontaneous speech, unlike read speech, is highly variable, and particular aspects of content can be expressed in many different ways by different speakers. Consequently, relatively few studies have explored content of spontaneous spoken responses. Xie et al. (2012) and Cheng et al. (2014) assessed content using similarity scores between test responses and highly proficient sample responses, based on content vector analysis (CVA). Loukina and Cahill (2016) used a content-scoring engine based on many sparse features, such as unigrams and bigrams, trained on a large corpus of existing responses. These studies were based on traditional character or word ngrams. Recently, significant improvement in ASR systems, semantic modeling technology based on 3 Overview of the approach In order to address this gap, we developed an automated algorithm which provides feedback about content completeness for non-native speakers’ spontaneous speech. Distinct from previous content scoring approaches that look at correctness of overall content by calculating similar"
W19-4432,W14-1802,0,0.0368273,"Missing"
W19-4432,W16-0533,0,0.0295958,"ovative Use of NLP for Building Educational Applications, pages 306–315 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics more advanced deep-neural networks (DNN), and larger training data sets encouraged researchers in the automated scoring field to explore contentmodeling for spoken responses. For instance, Chen et al. (2018) and Qian et al. (2018) developed automated oral proficiency scoring models using diverse neural models and achieved comparable or superior performance to sophisticated linguistic feature-based systems. In addition, Yoon et al. (2018) and Rei and Cummins (2016) used similarity scores between the prompt texts and test responses based on word embeddings. Compared to the traditional word-matching based method, they have the advantage of capturing topical relevance that is not based on specific, identical words. However, these studies have focused only on scoring, and based on our knowledge, no study has explored content feedback for spontaneous speech. this study, we trained automated models to detect the absence of key points that are the core content expected in correct answers. Next, we discussed possible ways to generate content feedback based on t"
W19-4432,S13-2046,1,0.92334,"r algorithm first determines absence of individual key points. The absence of a key point signals an issue in the content completeness of a spoken response. Next, we provide a list of missing key points with feedback about how to improve content completeness to the speakers. Our approach is able to provide much more fine-grained and targeted feedback about the content of a response, as compared to a traditional holistic approach. In order to determine the absence of the key points, we calculated similarity scores between a spoken response and a key point using a short response scoring engine (Heilman and Madnani, 2013) and new word-embedding based features. The short response scoring engine generally requires a sizable amount of response data for each question to achieve a reliable performance. Collecting question-specific data is a difficult task. Thus, the word-embedding features, that do not require any sample responses for each question for the feature training, have a strong advantage for practical systems. We evaluated the algorithm in two different conditions (questions in the training data vs. questions not in the training data) and ex307 plored the impact of a question-specific training dataset. 4"
W19-4432,N12-1011,1,0.643843,"task for a variety of reasons. First, an automated speech recognition (ASR) system is used to generate an automated transcription of a spoken response as an input of the content feature generator. Errors at the ASR stage may negatively affect the content features such that they are noisy and distorted to some extent. Secondly, and more importantly, spontaneous speech, unlike read speech, is highly variable, and particular aspects of content can be expressed in many different ways by different speakers. Consequently, relatively few studies have explored content of spontaneous spoken responses. Xie et al. (2012) and Cheng et al. (2014) assessed content using similarity scores between test responses and highly proficient sample responses, based on content vector analysis (CVA). Loukina and Cahill (2016) used a content-scoring engine based on many sparse features, such as unigrams and bigrams, trained on a large corpus of existing responses. These studies were based on traditional character or word ngrams. Recently, significant improvement in ASR systems, semantic modeling technology based on 3 Overview of the approach In order to address this gap, we developed an automated algorithm which provides fee"
W19-4432,W18-4002,1,0.833025,"rteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 306–315 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics more advanced deep-neural networks (DNN), and larger training data sets encouraged researchers in the automated scoring field to explore contentmodeling for spoken responses. For instance, Chen et al. (2018) and Qian et al. (2018) developed automated oral proficiency scoring models using diverse neural models and achieved comparable or superior performance to sophisticated linguistic feature-based systems. In addition, Yoon et al. (2018) and Rei and Cummins (2016) used similarity scores between the prompt texts and test responses based on word embeddings. Compared to the traditional word-matching based method, they have the advantage of capturing topical relevance that is not based on specific, identical words. However, these studies have focused only on scoring, and based on our knowledge, no study has explored content feedback for spontaneous speech. this study, we trained automated models to detect the absence of key points that are the core content expected in correct answers. Next, we discussed possible ways to generate"
