2020.parlaclarin-1.6,The si{P}arl corpus of {S}lovene parliamentary proceedings,2020,-1,-1,2,0,15752,andrej pancur,Proceedings of the Second ParlaCLARIN Workshop,0,"The paper describes the process of acquisition, up-translation, encoding, annotation, and distribution of siParl, a collection of the parliamentary debates from the Assembly of the Republic of Slovenia from 1990{--}2018, covering the period from just before Slovenia became an independent country in 1991, and almost up to the present. The entire corpus, comprising over 8 thousand sessions, 1 million speeches and 200 million words was uniformly encoded in accordance with the TEI-based Parla-CLARIN schema for encoding corpora of parliamentary debates, and contains extensive meta-data about the speakers, a typology of sessions etc. and structural and editorial annotations. The corpus was also part-of-speech tagged and lemmatised using state-of-the-art tools. The corpus is maintained on GitHub with its major versions archived in the CLARIN.SI repository and is available for linguistic analysis in the scope of the on-line CLARIN.SI concordancers, thus offering an invaluable resource for scholars studying Slovenian political history."
2020.lrec-1.409,Gigafida 2.0: The Reference Corpus of Written Standard {S}lovene,2020,-1,-1,3,0,17455,simon krek,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We describe a new version of the Gigafida reference corpus of Slovene. In addition to updating the corpus with new material and annotating it with better tools, the focus of the upgrade was also on its transformation from a general reference corpus, which contains all language variants including non-standard language, to the corpus of standard (written) Slovene. This decision could be implemented as new corpora dedicated specifically to non-standard language emerged recently. In the new version, the whole Gigafida corpus was deduplicated for the first time, which facilitates automatic extraction of data for the purposes of compilation of new lexicographic resources such as the collocations dictionary and the thesaurus of Slovene."
W19-8004,Improving {UD} processing via satellite resources for morphology,2019,0,0,2,0.740741,17528,kaja dobrovoljc,"Proceedings of the Third Workshop on Universal Dependencies (UDW, SyntaxFest 2019)",0,None
W18-5116,Datasets of {S}lovene and {C}roatian Moderated News Comments,2018,-1,-1,2,0.225882,264,nikola ljubevsic,Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2),0,"This paper presents two large newly constructed datasets of moderated news comments from two highly popular online news portals in the respective countries: the Slovene RTV MCC and the Croatian 24sata. The datasets are analyzed by performing manual annotation of the types of the content which have been deleted by moderators and by investigating deletion trends among users and threads. Next, initial experiments on automatically detecting the deleted content in the datasets are presented. Both datasets are published in encrypted form, to enable others to perform experiments on detecting content to be deleted without revealing potentially inappropriate content. Finally, the baseline classification models trained on the non-encrypted datasets are disseminated as well to enable real-world use."
L18-1210,{CLARIN}{'}s Key Resource Families,2018,0,0,3,0.575187,443,darja fivser,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-3007,"Legal Framework, Dataset and Annotation Schema for Socially Unacceptable Online Discourse Practices in {S}lovene",2017,4,12,2,0.677679,443,darja fivser,Proceedings of the First Workshop on Abusive Language Online,0,"In this paper we present the legal framework, dataset and annotation schema of socially unacceptable discourse practices on social networking platforms in Slovenia. On this basis we aim to train an automatic identification and classification system with which we wish contribute towards an improved methodology, understanding and treatment of such practices in the contemporary, increasingly multicultural information society."
W17-2901,Language-independent Gender Prediction on {T}witter,2017,6,2,3,0.258054,264,nikola ljubevsic,Proceedings of the Second Workshop on {NLP} and Computational Social Science,0,"In this paper we present a set of experiments and analyses on predicting the gender of Twitter users based on language-independent features extracted either from the text or the metadata of users{'} tweets. We perform our experiments on the TwiSty dataset containing manual gender annotations for users speaking six different languages. Our classification results show that, while the prediction model based on language-independent features performs worse than the bag-of-words model when training and testing on the same language, it regularly outperforms the bag-of-words model when applied to different languages, showing very stable results across various languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances."
W17-1406,The {U}niversal {D}ependencies Treebank for {S}lovenian,2017,4,1,2,0.740741,17528,kaja dobrovoljc,Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"This paper introduces the Universal Dependencies Treebank for Slovenian. We overview the existing dependency treebanks for Slovenian and then detail the conversion of the ssj200k treebank to the framework of Universal Dependencies version 2. We explain the mapping of part-of-speech categories, morphosyntactic features, and the dependency relations, focusing on the more problematic language-specific issues. We conclude with a quantitative overview of the treebank and directions for further work."
W17-1410,Adapting a State-of-the-Art Tagger for {S}outh {S}lavic Languages to Non-Standard Text,2017,9,3,2,0.258054,264,nikola ljubevsic,Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"In this paper we present the adaptations of a state-of-the-art tagger for South Slavic languages to non-standard texts on the example of the Slovene language. We investigate the impact of introducing in-domain training data as well as additional supervision through external resources or tools like word clusters and word normalization. We remove more than half of the error of the standard tagger when applied to non-standard texts by training it on a combination of standard and non-standard training data, while enriching the data representation with external resources removes additional 11 percent of the error. The final configuration achieves tagging accuracy of 87.41{\%} on the full morphosyntactic description, which is, nevertheless, still quite far from the accuracy of 94.27{\%} achieved on standard text."
L16-1242,Corpus vs. Lexicon Supervision in Morphosyntactic Tagging: the Case of {S}lovene,2016,10,6,2,0.270378,264,nikola ljubevsic,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we present a tagger developed for inflectionally rich languages for which both a training corpus and a lexicon are available. We do not constrain the tagger by the lexicon entries, allowing both for lexicon incompleteness and noisiness. By using the lexicon indirectly through features we allow for known and unknown words to be tagged in the same manner. We test our tagger on Slovene data, obtaining a 25{\%} error reduction of the best previous results both on known and unknown words. Given that Slovene is, in comparison to some other Slavic languages, a well-resourced language, we perform experiments on the impact of token (corpus) vs. type (lexicon) supervision, obtaining useful insights in how to balance the effort of extending resources to yield better tagging results."
L16-1573,Corpus-Based Diacritic Restoration for {S}outh {S}lavic Languages,2016,6,5,2,0.270378,264,nikola ljubevsic,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In computer-mediated communication, Latin-based scripts users often omit diacritics when writing. Such text is typically easily understandable to humans but very difficult for computational processing because many words become ambiguous or unknown. Letter-level approaches to diacritic restoration generalise better and do not require a lot of training data but word-level approaches tend to yield better results. However, they typically rely on a lexicon which is an expensive resource, not covering non-standard forms, and often not available for less-resourced languages. In this paper we present diacritic restoration models that are trained on easy-to-acquire corpora. We test three different types of corpora (Wikipedia, general web, Twitter) for three South Slavic languages (Croatian, Serbian and Slovene) and evaluate them on two types of text: standard (Wikipedia) and non-standard (Twitter). The proposed approach considerably outperforms charlifter, so far the only open source tool available for this task. We make the best performing systems freely available."
R15-1049,Predicting the Level of Text Standardness in User-generated Content,2015,11,6,3,0.351572,264,nikola ljubevsic,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Non-standard language as it appears in user-generated content has recently attracted much attention. This paper proposes that non-standardness comes in two basic varieties, technical and linguistic, and develops a machine-learning method to discriminate between standard and nonstandard texts in these two dimensions. We describe the manual annotation of a dataset of Slovene user-generated content and the features used to build our regression models. We evaluate and discuss the results, where the mean absolute error of the best performing method on a three-point scale is 0.38 for technical and 0.42 for linguistic standardness prediction. Even when using no language-dependent information sources, our predictor still outperforms an OOVratio baseline by a wide margin. In addition, we show that very little manually annotated training data is required to perform good prediction. Predicting standardness can help decide when to attempt to normalise the data to achieve better annotation results with standard tools, and provide linguists who are interested in nonstandard language with a simple way of selecting only such texts for their research."
fiser-etal-2014-slowcrowd,slo{WC}rowd: A crowdsourcing tool for lexicographic tasks,2014,3,1,3,0.885607,443,darja fivser,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The paper presents sloWCrowd, a simple tool developed to facilitate crowdsourcing lexicographic tasks, such as error correction in automatically generated wordnets and semantic annotation of corpora. The tool is open-source, language-independent and can be adapted to a broad range of crowdsourcing tasks. Since volunteers who participate in our crowdsourcing tasks are not trained lexicographers, the tool has been designed to obtain multiple answers to the same question and compute the majority vote, making sure individual unreliable answers are discarded. We also make sure unreliable volunteers, who systematically provide unreliable answers, are not taken into account. This is achieved by measuring their accuracy against a gold standard, the questions from which are posed to the annotators on a regular basis in between the real question. We tested the tool in an extensive crowdsourcing task, i.e. error correction of the Slovene wordnet, the results of which are encouraging, motivating us to use the tool in other annotation tasks in the future as well."
ljubesic-etal-2014-tweetcat,{T}weet{C}a{T}: a tool for building {T}witter corpora of smaller languages,2014,8,12,3,0.351572,264,nikola ljubevsic,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents TweetCaT, an open-source Python tool for building Twitter corpora that was designed for smaller languages. Using the Twitter search API and a set of seed terms, the tool identifies users tweeting in the language of interest together with their friends and followers. By running the tool for 235 days we tested it on the task of collecting two monitor corpora, one for Croatian and Serbian and the other for Slovene, thus also creating new and valuable resources for these languages. A post-processing step on the collected corpus is also described, which filters out users that tweet predominantly in a foreign language thus further cleans the collected corpora. Finally, an experiment on discriminating between Croatian and Serbian Twitter users is reported."
W13-2409,Modernizing historical {S}lovene words with character-based {SMT},2013,17,17,2,0,263,yves scherrer,Proceedings of the 4th Biennial International Workshop on {B}alto-{S}lavic Natural Language Processing,0,"We propose a language-independent word normalization method exemplified on modernizing historical Slovene words. Our method relies on character-based statistical machine translation and uses only shallow knowledge. We present the relevant lexicons and two experiments. In one, we use a lexicon of historical wordxe2x80x90 contemporary word pairs and a list of contemporary words; in the other, we only use a list of historical words and one of contemporary ones. We show that both methods produce significantly better results than the baseline."
W12-1001,Lexicon Construction and Corpus Annotation of Historical Language with the {C}o{B}a{LT} Editor,2012,6,6,2,0,23107,tom kenter,"Proceedings of the 6th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"This paper describes a Web-based editor called CoBaLT (Corpus-Based Lexicon Tool), developed to construct corpus-based computational lexica and to correct word-level annotations and transcription errors in corpora. The paper describes the tool as well as our experience in using it to annotate a reference corpus and compile a large lexicon of historical Slovene. The annotations used in our project are modern-day word form equivalent, lemma, part-of-speech tag and optional gloss. The CoBaLT interface is word form oriented and compact. It enables wildcard word searching and sorting according to several criteria, which makes the editing process flexible and efficient. The tool accepts preannotated corpora in TEI P5 format and is able to export the corpus and lexicon in TEI P5 as well. The tool is implemented using the LAMP architecture and is freely available for research purposes."
erjavec-2012-goo300k,The goo300k corpus of historical {S}lovene,2012,11,8,1,1,15753,tomavz erjavec,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The paper presents a gold-standard reference corpus of historical Slovene containing 1,000 sampled pages from over 80 texts, which were, for the most part, written between 1750-1900. Each page of the transcription has an associated facsimile and the words in the texts have been manually annotated with their modern-day equivalent, lemma and part-of-speech. The paper presents the structure of the text collection, the sampling procedure, annotation process and encoding of the corpus. The corpus is meant to facilitate HLT research and enable corpus based diachronic studies for historical Slovene. The corpus is encoded according to the Text Encoding Initiative Guidelines (TEI P5), is available via a concordancer and for download from http://nl.ijs.si/imp/ under the Creative Commons Attribution licence."
W11-1505,Automatic linguistic annotation of historical language: {T}o{T}r{T}a{L}e and {XIX} century {S}lovene,2011,11,10,1,1,15753,tomavz erjavec,"Proceedings of the 5th {ACL}-{HLT} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"The paper describes a tool developed to process historical (Slovene) text, which annotates words in a TEI encoded corpus with their modern-day equivalents, morphosyntactic tags and lemmas. Such a tool is useful for developing historical corpora of highly-inflecting languages, enabling full text search in digital libraries of historical texts, for modernising such texts for today's readers and making it simpler to correct OCR transcriptions."
W11-0402,{OWL}/{DL} formalization of the {MULTEXT}-East morphosyntactic specifications,2011,27,14,2,0,2108,christian chiarcos,Proceedings of the 5th Linguistic Annotation Workshop,0,"This paper describes the modeling of the morphosyntactic annotations of the MULTEXT-East corpora and lexicons as an OWL/DL ontology. Formalizing annotation schemes in OWL/DL has the advantages of enabling formally specifying interrelationships between the various features and making logical inferences based on the relationships between them. We show that this approach provides us with a top-down perspective on a large set of morphosyntactic specifications for multiple languages, and that this perspective helps to identify and to resolve conceptual problems in the original specifications. Furthermore, the ontological modeling allows us to link the MULTEXT-East specifications with repositories of annotation terminology such as the General Ontology of Linguistics Descriptions or the ISO TC37/SC4 Data Category Registry."
erjavec-2010-multext,"{MULTEXT}-East Version 4: Multilingual Morphosyntactic Specifications, Lexicons and Corpora",2010,10,55,1,1,15753,tomavz erjavec,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The paper presents the fourth, ``Mondilex'' edition of the MULTEXT-East language resources, a multilingual dataset for language engineering research and development, focused on the morphosyntactic level of linguistic description. This standardised and linked set of resources covers a large number of mainly Central and Eastern European languages and includes the EAGLES-based morphosyntactic specifications; morphosyntactic lexica; and annotated parallel, comparable, and speech corpora. The fourth release of these resources introduces XML-encoded morphosyntactic specifications and adds six new languages, bringing the total to 16: to Bulgarian, Croatian, Czech, Estonian, English, Hungarian, Romanian, Serbian, Slovene, and the Resian dialect of Slovene it adds Macedonian, Persian, Polish, Russian, Slovak, and Ukrainian. This dataset, unique in terms of languages covered and the wealth of encoding, is extensively documented, and freely available for research purposes at http://nl.ijs.si/ME/V4/."
erjavec-etal-2010-jos,The {JOS} Linguistically Tagged Corpus of {S}lovene,2010,6,34,1,1,15753,tomavz erjavec,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The JOS language resources are meant to facilitate developments of HLT and corpus linguistics for the Slovene language and consist of the morphosyntactic specifications, defining the Slovene morphosyntactic features and tagset; two annotated corpora (jos100k and jos1M); and two web services (a concordancer and text annotation tool). The paper introduces these components, and concentrates on jos100k, a 100,000 word sampled balanced monolingual Slovene corpus, manually annotated for three levels of linguistic description. On the morphosyntactic level, each word is annotated with its morphosyntactic description and lemma; on the syntactic level the sentences are annotated with dependency links; on the semantic level, all the occurrences of 100 top nouns in the corpus are annotated with their wordnet synset from the Slovene semantic lexicon sloWNet. The JOS corpora and specifications have a standardised encoding (Text Encoding Initiative Guidelines TEI P5) and are available for research from http://nl.ijs.si/jos/ under the Creative Commons licence."
javorsek-erjavec-2010-experimental,Experimental Deployment of a Grid Virtual Organization for Human Language Technologies,2010,8,0,2,0,46351,jan javorvsek,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We propose to create a grid virtual organization for human language technologies, at first chiefly with the task of enabling linguistic researches to use existing distributed computing facilities of the European grid infrastructure for more efficient processing of large data sets. After a brief overview of modern grid computing, a number of common use-cases of natural language processing tasks running on the grid are presented, notably corpus annotation with morpho-syntactic tagging (600+ million-word corpus annotated in less than a day), {\$}n{\$}-gram statistics processing of a corpus and creation of grid-backed web-accessible services with annotation and term-extraction as examples. Implementation considerations and common problems of using grid for this type of tasks are laid out. We conclude with an outline of a simple action plan for evolving the infrastructure created for these experiments into a fully functional Human Language Technology grid Virtual Organization with the goal of making the power of European grid infrastructure available to the linguistic community."
erjavec-krek-2008-jos,The {JOS} Morphosyntactically Tagged Corpus of {S}lovene,2008,9,10,1,1,15753,tomavz erjavec,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The JOSmorphosyntactic resources for Slovene consist of the specifications, lexicon, and two corpora: jos100k, a 100,000 word balanced monolingual sampled corpus annotated with hand validated morphosyntactic descriptions (MSDs) and lemmas, and jos1M, the 1 million-word partially hand validated corpus. The two corpora have been sampled from the 600M-word Slovene reference corpus FidaPLUS. The JOS resources have a standardised encoding, with the MULTEXT-East-type morphosyntactic specifications and the corpora encoded according to the Text Encoding Initiative Guidelines P5. JOS resources are available as a dataset for research under the Creative Commons licence and are meant to facilitate developments of HLT for Slovene."
sharoff-etal-2008-designing,Designing and Evaluating a {R}ussian Tagset,2008,15,39,3,0,519,serge sharoff,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper reports the principles behind designing a tagset to cover Russian morphosyntactic phenomena, modifications of the core tagset, and its evaluation. The tagset is based on the MULTEXT-East framework, while the decisions in designing it were aimed at achieving a balance between parameters important for linguists and the possibility to detect and disambiguate them automatically. The final tagset contains about 500 tags and achieves about 95{\%} accuracy on the disambiguated portion of the Russian National Corpus. We have also produced a test set that can be shared with other researchers."
dzeroski-etal-2006-towards,Towards a {S}lovene Dependency Treebank,2006,8,93,2,1,50165,savso dvzeroski,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The paper presents the initial release of the Slovene Dependency Treebank, currently containing 2000 sentences or 30.000 words. Ourapproach to annotation is based on the Prague Dependency Treebank, which serves as an excellent model due to the similarity of the languages, the existence of a detailed annotation guide and an annotation editor. The initial treebank contains a portion of theMULTEXT-East parallel word-level annotated corpus, namely the firstpart of the Slovene translation of Orwell's Â1984Â. This corpus was first parsed automatically, to arrive at the initial analytic level dependency trees. These were then hand corrected using the tree editorTrEd; simultaneously, the Czech annotation manual was modified forSlovene. The current version is available in XML/TEI, as well asderived formats, and has been used in a comparative evaluation using the MALT parser, and as one of the languages present in the CoNLL-Xshared task on dependency parsing. The paper also discusses further work, in the first instance the composition of the corpus to be annotated next."
erjavec-2006-english,The {E}nglish-{S}lovene {ACQUIS} corpus,2006,7,11,1,1,15753,tomavz erjavec,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The paper presents the SVEZ-IJS corpus, a large parallel annotated English-Slovene corpus containing translated legal texts of the European Union, the ACQUIS Communautaire. The corpus contains approx. 2 x 5 million words and was compiled from the translation memory obtained from the Translation Unit of the Slovene Government Office for European Affairs. The corpus is encoded in XML, accordingto the Text Encoding Initiative Guidelines TEI P4, where each translation memory unit contains useful metadata and the two aligned segments (sentences). Both the Slovene and English text islinguistically annotated at the word-level, by context disambiguatedlemmas and morphosyntactic descriptions, which follow the MULTEXTguidelines. The complete corpus is freely available for research, either via an on-line concordancer, or for downloading from the corpushome page at http://nl.ijs.si/svez/."
erjavec-fiser-2006-building,Building {S}lovene {W}ord{N}et,2006,19,18,1,1,15753,tomavz erjavec,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"A WordNet is a lexical database in which nouns, verbs, adjectives and adverbs are organized in a conceptual hierarchy, linking semantically and lexically related concepts. Such semantic lexicons have become oneof the most valuable resources for a wide range of NLP research and applications, such as semantic tagging, automatic word-sense disambiguation, information retrieval and document summarisation. Following the WordNet design for the English languagedeveloped at Princeton, WordNets for a number of other languages havebeen developed in the past decade, taking the idea into the domain ofmultilingual processing. This paper reports on the prototype SloveneWordNet which currently contains about 5,000 top-level concepts. Theresource has been automatically translated from the Serbian WordNet, with the help of a bilingual dictionary, synset literals ranked according to the frequency of corpus occurrence, and results manually corrected. The paper presents the results obtained, discusses some problems encountered along the way and points out some possibilitiesof automated acquisition and refinement of synsets in the future."
steinberger-etal-2006-jrc,The {JRC}-{A}cquis: A Multilingual Aligned Parallel Corpus with 20+ Languages,2006,12,341,5,0,24362,ralf steinberger,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We present a new, unique and freely available parallel corpus containing European Union (EU) documents of mostly legal nature. It is available in all 20 official EU languages, with additional documents being available in the languages of the EU candidate countries. The corpus consists of almost 8,000 documents per language, with an average size of nearly 9 million words per language. Pair-wise paragraph alignment information produced by two different aligners (Vanilla and HunAlign) is available for all 190+ language pair combinations. Most texts have been manually classified according to the EUROVOC subject domains so that the collection can also be used to train and test multi-label classification algorithms and keyword-assignment software. The corpus is encoded in XML, according to the Text Encoding Initiative Guidelines. Due to the large number of parallel texts in many languages, the JRC-Acquis is particularly suitable to carry out all types of cross-language research, as well as to test and benchmark text analysis software across different languages (for instance for alignment, sentence splitting and term extraction)."
erjavec-etal-2004-making,Making an {XML}-based {J}apanese-{S}lovene Learners{'} Dictionary,2004,5,1,1,1,15753,tomavz erjavec,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we present a hypertext dictionary of Japanese lexical units for Slovene students of Japanese at the Faculty of Arts of Ljubljana University. The dictionary is planned as a long-term project in which a simple dictionary is to be gradually enlarged and enhanced, taking into account the needs of the students. Initially, the dictionary was encoded in a tabular format, in a mixture of encodings, and subsequently rendered in HTML. The paper first discusses the conversion of the dictionary into XML, into an encoding that complies with the Text Encoding Initiative (TEI) Guidelines. The conversion into such an encoding validates, enriches, explicates and standardises the structure of the dictionary, thus making it more usable for further development and linguistically oriented research. We also present the current Web implementation of the dictionary, which offers full text search and a tool for practising inflected parts of speech. The paper gives an overview of related research, i.e. other XML oriented Web dictionaries of Slovene and East Asian languages and presents planned developments, i.e. the inclusion of the dictionary into the Reading Tutor program."
erjavec-2004-multext,"{MULTEXT}-East Version 3: Multilingual Morphosyntactic Specifications, Lexicons and Corpora",2004,9,121,1,1,15753,tomavz erjavec,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The paper presents the third edition of the MULTEXT-East language resources, a multilingual dataset for language engineering research and development. This standardised and linked set of resources covers a large number of mainly Central and Eastern European languages and includes the EAGLES-based morphosyntactic specifications, defining the features that describe word-level syntactic annotations; medium scale morphosyntactic lexica; and annotated parallel, comparable, and speech corpora. The most important component is the linguistically annotated corpus consisting of Orwellxe2x80x99s novel xe2x80x9c1984xe2x80x9d in the English original and translations. The resources are the results of several EU projects: MULTEXT-East (produced linked resources for Romanian, Slovene, Czech, Bulgarian, Estonian, Hungarian and English), TELRI (added resources for Lithuanian, Croatian, Serbian, and Russian; first release), and CONCEDE (validation, re-encoding; partial re-release). This paper presents the third release of the resources, which brings together the first two, makes them available in TEI P4 XML, and introduces further extensions, e.g., the specification for Resian, a dialect of Slovene. This dataset, unique in terms of languages and the wealth of encoding, is extensively documented, and freely available for research purposes. The paper presents the component resources, reviews some research undertaken on the basis of the first two editions, and discusses future plans."
bauman-etal-2004-migrating,Migrating Language Resources from {SGML} to {XML}: The Text Encoding Initiative Recommendations,2004,3,3,4,0,52183,syd bauman,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The Text Encoding Initiative (TEI), established in 1987, has been the largest effort in the area of standardisation of computer encoding of language resources. TEI chose SGML (Standard Generalized Markup Language) as its underlying standard, and in the years before the inception of XML, a number of projects encoded their data according to some SGML DTD, TEI compliant, or otherwise. These projects could now benefit from migrating their data to XML. Apart from validation, the most compelling reason for migration is the scarcity of SGML-aware software and the abundance of XML-based tools and related recommendations. However, despite the fact that XML is a subset of SGML, migration is not a trivial process, especially in the case of large holdings of legacy language resources. This is why in 2002 the TEI Consortium established a Task Force on SGML to XML migration. The TF has now produced a number of reports that simplify and make explicit the conversion of SGML TEI (version P3) to XML TEI (version P4) documents. The reports are also relevant for a general audience of SGML users that are considering migrating their language resources to XML. This paper presents the recommendations made by the TF, concentrating on strategic considerations, the practical guide, and one case study, the conversion of the British National Corpus."
lee-etal-2004-towards,Towards an International Standard on Feature Structure Representation,2004,22,18,9,0,18931,kiyong lee,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,This paper describes the preliminary results of a joint initiative of the TEI (Text Encoding Initiative) Consortium and the ISO Committee TC 37SC 4 (language Resource management) to provide a standard for the representation and interchange of feature structures.
W03-2904,The {MULTEXT}-East Morphosyntactic Specification for {S}lavic Languages,2003,9,23,1,1,15753,tomavz erjavec,Proceedings of the 2003 {EACL} Workshop on Morphological Processing of {S}lavic Languages,0,"Word-level morphosyntactic descriptions, such as Ncmsn designating a common masculine singular noun in the nominative, have been developed for all Slavic languages, yet there have been few attempts to arrive at a proposal that would be harmonised across the languages. Standardisation adds to the interchange potential of the resources, making it easier to develop multilingual applications or to evaluate language technology tools across several languages. The process of the harmonisation of morphosyntactic categories, esp. for morphologically rich Slavic languages is also interesting from a language-typological perspective. The EU Multext-East project developed corpora, lexica and tools for seven languages, with the focus being on morphosyntactic data, including formal, EAGLES-based specifications for lexical morphosyntactic descriptions. The specifications were later extended, so that they currently cover nine languages, five from the Slavic family: Bulgarian, Croatian, Czech, Serbian and Slovene. The paper presents these morphosyntactic specifications, giving their background and structure, including the encoding of the tables as TEI feature structures. The five Slavic language specifications are discussed in more depth."
W01-1503,The {TELRI} tool catalogue: structure and prospects,2001,9,1,1,1,15753,tomavz erjavec,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"In the scope of the TELRI concerted action a working group is investigating the formation of a tool catalogue and repository. The idea is similar to that of the ACL Natural Language Software Registry, but the contents should be mostly limited to corpus processing tools available free of cost for research use. The catalogue should also offer a help-line for installing and using the software. The paper reports on the setup of this catalogue, and concentrates on the technical issues involved in its creation, storage and display. This involves the form interface on the Web, the XML DocBook encoding, and the XSL stylesheets used to present the catalogue either on the Web or in print. The paper lists the current entries in the catalogue and discusses plans for their expansion and maintenance."
dzeroski-etal-2000-morphosyntactic,Morphosyntactic Tagging of {S}lovene: Evaluating Taggers and Tagsets,2000,14,20,2,0,50165,savso dvzeroski,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The paper evaluates tagging techniques on a corpus of Slovene, where we are faced with a large number of possible word-class tags and only a small (hand-tagged) dataset. We report on training and testing of four different taggers on the Slovene MULTEXT-East corpus containing about 100.000 words and 1000 different morphosyntactic tags. Results show, first of all, that training times of the Maximum Entropy Tagger and the Rule Based Tagger are unacceptably long, while they are negligible for the Memory Based Taggers and the TnT tri-gram tagger. Results on a random split show that tagging accuracy varies between 86% and 89% overall, between 92% and 95% on known words and between 54% and 55% on unknown words. Best results are obtained by TnT. The paper also investigates performance in relation to our EAGLES-based morphosyntactic tagset. Here we compare the per-feature accuracy on the full tagset, and accuracies on these features when training on a reduced tagset. Results show that PoS accuracy is quite high, while accuracy on Case is lowest. Tagset reduction helps improve accuracy, but less than might be expected."
gros-etal-2000-corpora,Corpora of {S}lovene Spoken Language for Multi-lingual Applications,2000,0,1,4,0,49450,jerneja gros,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,None
erjavec-etal-2000-concede,The Concede Model for Lexical Databases,2000,6,18,1,1,15753,tomavz erjavec,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The value of language resources is greatly enhanced if they share a common markup with an explicit minimal semantics. Achieving this goal for lexical databases is difficult, as large-scale resources can realistically only be obtained by up-translation from pre-existing dictionaries, each with its own proprietary structure. This paper describes the approach we have taken in the Concede project, which aims to develop compatible lexical databases for six Central and Eastern European languages. Starting with sample entries from original presentation-oriented electronic representations of dictionaries, we transformed the data into an intermediate TEI-compatible representation to provide a common baseline for evaluating and comparing the dictionaries. We then developed a more restrictive encoding, formalised as an XML DTD with a clearly-defined semantic interpretation. We present this DTD and discuss a sample conversion from TEI, together with an application which hyperlinks a HTML representation of the dictionary to on-line concordancing over a corpus."
2000.eamt-1.7,{S}lovene{--}{E}nglish Datasets for {MT},2000,-1,-1,1,1,15753,tomavz erjavec,5th EAMT Workshop: Harvesting Existing Resources,0,None
