1993.iwpt-1.3,H92-1030,1,0.888584,"Missing"
1993.iwpt-1.3,H91-1060,0,0.0362138,"Missing"
1993.iwpt-1.3,H90-1021,0,0.0202786,"Missing"
1993.iwpt-1.3,P91-1030,0,0.0591744,"Missing"
1993.iwpt-1.3,H92-1022,1,0.888454,"Missing"
1993.iwpt-1.3,H91-1065,0,0.0462785,"Missing"
1993.iwpt-1.3,J93-2004,0,0.0369948,"Missing"
1993.iwpt-1.3,P92-1017,0,0.0435978,"Missing"
1993.iwpt-1.3,C86-1033,0,0.0743321,"Missing"
1993.iwpt-1.3,H90-1054,0,0.0907792,"Missing"
1993.iwpt-1.3,E93-1040,0,0.0645439,"Missing"
2000.iwpt-1.2,1993.iwpt-1.3,1,0.659039,"he outputs of these parsers. Similar results have been obtained for English. 3 Transformation-Based Parsing There have been great advances recently in the accuracy of parsers that are automatically trained from parsed corpora. One disadvantage of these grammar induction methods is that the derived linguistic knowledge is captured in opaque parameter files, typically many megabytes large. This makes it a challenge to capitalize on human intuitions to improve the machine-derived grammars. An alternative to these statistical induction methods is a method called Transformation-Based Parsing (TBP) [2, 11, 12]. In TBP, a grammar consists of an ordered sequence of tree transform rules. To learn the rules, we begin with some initial annotation of the training corpus (for instance, every sentence parsed as a flat structure under an S-node), and then we iteratively search for the transform rule whose application will bring the training set closest to the set of true parses, we append that rule to our rule list and apply it to the training corpus. Transform rules can add, delete or rearrange structure. An example of a learned rule is: If a sequence begins with a Determiner and ends with a Noun and has a"
2000.iwpt-1.2,P99-1009,1,0.87851,"Missing"
2000.iwpt-1.2,P98-1029,1,0.812383,"by other techniques. Finally, we review a recent study exploring how people and machines compare at the task of creating a program to automatically annotate noun phrases. 1 Introduction This paper briefly surveys three research directions we hope will positively impact the field of parsing: parser output combination, automatic rule sequence learning, and manual rule sequence creation. 2 2.1 Parser Combination Combining Off-the-Shelf Parsers There has been a great deal of recent research in the machine learning community on combining the outputs of multiple classifiers to improve accuracy. In [4], we demonstrated that taking three off-the-shelf part of speech taggers and �ombining their outputs results in a significant performance improvement over any single tagger. More recently [8], we have demonstrated that the same is true 1 Best Original Parser Constituent Voting Parser Switching Precision 89.6 92.4 90.8 Recall 89.7 90.1 90.7 I (P+R)/2 I F-Measure 89.7 91.3 90.7 89.7 91.3 90.7 Table 1: Comparison of Single Parser with Combination Techniques for parsing. We took three statistical parsers that had been trained on a portion of the Penn Treebank [5, 6, 10]. We explored two methods of"
2000.iwpt-1.2,P97-1003,0,0.0109668,"lassifiers to improve accuracy. In [4], we demonstrated that taking three off-the-shelf part of speech taggers and �ombining their outputs results in a significant performance improvement over any single tagger. More recently [8], we have demonstrated that the same is true 1 Best Original Parser Constituent Voting Parser Switching Precision 89.6 92.4 90.8 Recall 89.7 90.1 90.7 I (P+R)/2 I F-Measure 89.7 91.3 90.7 89.7 91.3 90.7 Table 1: Comparison of Single Parser with Combination Techniques for parsing. We took three statistical parsers that had been trained on a portion of the Penn Treebank [5, 6, 10]. We explored two methods of combination: constituent voting and parser switching. In constituent voting, each parser posits a set of constituents with weights (votes). We take the union of all posited constituents for a sentence, accumulating their weights, and then discard all constituents with a weight below a threshold. In the simplest version of constituent voting, each parser gives a weight of 1 to any constituent it posits, and then we retain all constuents posited by more than half of the parsers. We can also weight the vote according to the accuracy of the individual parser, or accord"
2000.iwpt-1.2,W99-0623,1,0.807585,"paper briefly surveys three research directions we hope will positively impact the field of parsing: parser output combination, automatic rule sequence learning, and manual rule sequence creation. 2 2.1 Parser Combination Combining Off-the-Shelf Parsers There has been a great deal of recent research in the machine learning community on combining the outputs of multiple classifiers to improve accuracy. In [4], we demonstrated that taking three off-the-shelf part of speech taggers and �ombining their outputs results in a significant performance improvement over any single tagger. More recently [8], we have demonstrated that the same is true 1 Best Original Parser Constituent Voting Parser Switching Precision 89.6 92.4 90.8 Recall 89.7 90.1 90.7 I (P+R)/2 I F-Measure 89.7 91.3 90.7 89.7 91.3 90.7 Table 1: Comparison of Single Parser with Combination Techniques for parsing. We took three statistical parsers that had been trained on a portion of the Penn Treebank [5, 6, 10]. We explored two methods of combination: constituent voting and parser switching. In constituent voting, each parser posits a set of constituents with weights (votes). We take the union of all posited constituents for"
2000.iwpt-1.2,W97-0301,0,0.0133004,"lassifiers to improve accuracy. In [4], we demonstrated that taking three off-the-shelf part of speech taggers and �ombining their outputs results in a significant performance improvement over any single tagger. More recently [8], we have demonstrated that the same is true 1 Best Original Parser Constituent Voting Parser Switching Precision 89.6 92.4 90.8 Recall 89.7 90.1 90.7 I (P+R)/2 I F-Measure 89.7 91.3 90.7 89.7 91.3 90.7 Table 1: Comparison of Single Parser with Combination Techniques for parsing. We took three statistical parsers that had been trained on a portion of the Penn Treebank [5, 6, 10]. We explored two methods of combination: constituent voting and parser switching. In constituent voting, each parser posits a set of constituents with weights (votes). We take the union of all posited constituents for a sentence, accumulating their weights, and then discard all constituents with a weight below a threshold. In the simplest version of constituent voting, each parser gives a weight of 1 to any constituent it posits, and then we retain all constuents posited by more than half of the parsers. We can also weight the vote according to the accuracy of the individual parser, or accord"
2000.iwpt-1.2,P96-1034,1,0.873605,"Missing"
A00-2005,W99-0606,0,0.275031,"5 Set Training Instance Original Parser Initial Test BestF(15) Final(15) Original Parser Initial TrainBestF(15) TestBestF(13) Final(15) P 96.25 93.61 96.16 96.16 88.73 88.43 89.54 89.55 89.54 R 96.31 93.63 95.86 95.86 88.54 88.34 88.80 88.84 88.80 F 96.28 93.62 96.01 96.01 88.63 88.38 89.17 89.19 89.17 Gain NA 0.00 2.39 2.39 NA 0.00 0.79 0.81 0.79 Exact 64.7 55.5 62.1 62.1 34.9 33.3 34.6 34.7 34.6 Gain NA 0.0 6.6 6.6 NA 0.0 1.3 1.4 1.3 Table 1: Bagging the Treebank mechanisms of a parser for Japanese. The creators of AdaBoost used it to perform text classification (Schapire and Singer, 2000). Abney et al. (1999) performed part-of-speech tagging and prepositional phrase attachment using AdaBoost as a core component. They found they could achieve accuracies on both tasks t h a t were competitive with the state of the art. As a side effect, they found that inspecting the samples that were consistently given the most weight during boosting revealed some faulty annotations in the corpus. In all of these systems, AdaBoost has been used as a traditional classification system. 3.2 Boosting for Parsing Our goal is to recast boosting for parsing while considering a parsing system as the embedded learner. The f"
A00-2005,P97-1003,0,0.0443073,"samples, where each sample in a bag is drawn randomly with replacement from the bag corresponding to C. 2. Create parser f~ = g(Ci) for each i. 3. Given a novel sentence 8test E C t e s t , combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999). 2.3 E x p e r i m e n t The training set for these experiments was sections 01-21 of the Penn Treebank (Marcus et al., 1993). The test set was section 23. The parser induction algorithm used in all of the experiments in this paper was a distribution of Collins&apos;s model 2 parser (Collins, 1997). All comparisons made below refer to results we obtained using Collins&apos;s parser. The results for bagging are shown in Figure 2 and Table 1. The row of figures are (from left-to-right) training set F-measure ~, test set F-measure, percent perfectly parsed sentences in training set, and percent perfectly parsed sentences in test set. An ensemble of bags was produced one bag at a time. In the table, the I n i t i a l row shows the performance achieved when the ensemble contained only one bag, F i n a l ( X ) shows the performance when the ensemble contained X bags, BestF gives the performance of"
A00-2005,1993.eamt-1.1,0,0.0167917,"for each x~ observed in the test set, yi = m o d e ( ¢ j ( x i ) . . . Cj(x~)). y~ is the value predicted by the most predictors, the majority vote. 2.2 Bagging for Parsing An algorithm that applies the technique of bagging to parsing is given in Algorithm 2. Previous work on combining independent parsers is leveraged to produce the combined parser. The rest of the algorithm is a straightforward transformation of bagging for classifiers. Exploratory work in this vein was described by HajiC et al. (1999). Algorithm: Bagging A Parser 2 B a g g i n g and P a r s i n g 2.1 Background The work of Efron and Tibshirani (1993) enabled Breiman&apos;s refinement and application of their techniques for machine learning (Breiman, 1996). His technique is called bagging, short for &quot;bootstrap aggregating&quot;. In brief, bootstrap techniques and bag34 (2) Given: A corpus (again as a f u n c t i o n ) C : S × T ~ N , S is the set of possible sentences, and T is the set of trees, with size m = [C] = ~ s , t C(s, t) and parser induction algorithm g. 1. Draw k bootstrap replicates C1 ... Ck of C each containing m samples of (s,t) pairs randomly picked from the domain of C according to the distribution D ( s , t ) = C(s,t)/]C]. Each boo"
A00-2005,P98-1083,0,0.0332904,"ent. 1 D, + , ( i) = -~- Dt ( i ) exp(-c~tYiCt( xi ) ) 5. Increment t. Quit if t &gt; T. 6. Repeat from step 1. 7. The final hypothesis is ~)boost(:g) ~- sign Z ~t¢,(x) t The value of at should generally be chosen to minimize Z Dt (i) e x p ( - a ~ Yi Ct (x,)) i in order to minimize the expected per-sample training error of the ensemble, which Schapire and Singer show can be concisely expressed by I-] Zt. They also give several examples for how to pick an appropriate a, and selection generally depends on the possible outputs of the underlying learner. Boosting has been used in a few NLP systems. Haruno et al. (1998) used boosting to produce more accurate classifiers which were embedded as control 35 Set Training Instance Original Parser Initial Test BestF(15) Final(15) Original Parser Initial TrainBestF(15) TestBestF(13) Final(15) P 96.25 93.61 96.16 96.16 88.73 88.43 89.54 89.55 89.54 R 96.31 93.63 95.86 95.86 88.54 88.34 88.80 88.84 88.80 F 96.28 93.62 96.01 96.01 88.63 88.38 89.17 89.19 89.17 Gain NA 0.00 2.39 2.39 NA 0.00 0.79 0.81 0.79 Exact 64.7 55.5 62.1 62.1 34.9 33.3 34.6 34.7 34.6 Gain NA 0.0 6.6 6.6 NA 0.0 1.3 1.4 1.3 Table 1: Bagging the Treebank mechanisms of a parser for Japanese. The creat"
A00-2005,W99-0623,1,0.874708,"is the set of trees, with size m = [C] = ~ s , t C(s, t) and parser induction algorithm g. 1. Draw k bootstrap replicates C1 ... Ck of C each containing m samples of (s,t) pairs randomly picked from the domain of C according to the distribution D ( s , t ) = C(s,t)/]C]. Each bootstrap replicate is a bag of samples, where each sample in a bag is drawn randomly with replacement from the bag corresponding to C. 2. Create parser f~ = g(Ci) for each i. 3. Given a novel sentence 8test E C t e s t , combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999). 2.3 E x p e r i m e n t The training set for these experiments was sections 01-21 of the Penn Treebank (Marcus et al., 1993). The test set was section 23. The parser induction algorithm used in all of the experiments in this paper was a distribution of Collins&apos;s model 2 parser (Collins, 1997). All comparisons made below refer to results we obtained using Collins&apos;s parser. The results for bagging are shown in Figure 2 and Table 1. The row of figures are (from left-to-right) training set F-measure ~, test set F-measure, percent perfectly parsed sentences in training set, and percent perfectly"
A00-2005,J93-2004,0,0.0227639,"of C each containing m samples of (s,t) pairs randomly picked from the domain of C according to the distribution D ( s , t ) = C(s,t)/]C]. Each bootstrap replicate is a bag of samples, where each sample in a bag is drawn randomly with replacement from the bag corresponding to C. 2. Create parser f~ = g(Ci) for each i. 3. Given a novel sentence 8test E C t e s t , combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999). 2.3 E x p e r i m e n t The training set for these experiments was sections 01-21 of the Penn Treebank (Marcus et al., 1993). The test set was section 23. The parser induction algorithm used in all of the experiments in this paper was a distribution of Collins&apos;s model 2 parser (Collins, 1997). All comparisons made below refer to results we obtained using Collins&apos;s parser. The results for bagging are shown in Figure 2 and Table 1. The row of figures are (from left-to-right) training set F-measure ~, test set F-measure, percent perfectly parsed sentences in training set, and percent perfectly parsed sentences in test set. An ensemble of bags was produced one bag at a time. In the table, the I n i t i a l row shows th"
A00-2005,C98-1080,0,\N,Missing
A92-1021,A88-1019,0,0.014774,"Missing"
A92-1021,A92-1018,0,0.0922575,"Missing"
A92-1021,J88-1003,0,0.0192455,"Missing"
A92-1021,P89-1015,0,0.031187,"Missing"
A92-1021,H89-2014,0,0.0193965,"Missing"
A92-1021,H91-1065,0,0.0240502,"Missing"
C94-2195,P91-1030,0,0.088907,"Missing"
C94-2195,H93-1054,1,0.532712,"Missing"
C94-2195,H91-1037,0,0.0584758,"Missing"
C94-2195,P90-1004,0,0.0524791,"Missing"
C94-2195,W94-0111,0,\N,Missing
C94-2195,J93-2004,0,\N,Missing
C94-2195,J93-1005,0,\N,Missing
C94-2195,H94-1048,0,\N,Missing
C94-2195,P93-1035,1,\N,Missing
C94-2195,H93-1047,1,\N,Missing
C94-2195,J92-4003,0,\N,Missing
C98-1029,H92-1023,0,0.0580591,"Missing"
C98-1029,W96-0102,0,0.0791663,"Missing"
C98-1029,J93-2004,0,0.0519895,"er, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees, connectionist machines, transformations, nearest-neighbor algorithms, and maximum entropy (Weischedel(1993), Black(1992), Schmid(1994), Brill(1995),Daelemans(1995),Ratnaparkhi(1996 )). All of these methods seem to achieve roughly comparable accuracy. The fact that most machine-learningbased taggers achieve comparable results could be attributed to a number of causes. It is possible that the 80/20 rule of engineering is applying: a certain n"
C98-1029,C94-1027,0,0.0173147,"Missing"
C98-1029,J93-2006,0,0.138613,"Missing"
C98-1029,W96-0213,0,\N,Missing
C98-1029,J95-4004,1,\N,Missing
H01-1052,W95-0104,0,0.153059,"the field to concentrate considerably more effort into enlarging our training corpora and addressing scalability issues, rather than continuing to explore different learning methods applied to the relatively small extant training corpora. 2. PREVIOUS WORK 2.1 Confusion Set Disambiguation Several methods have been presented for confusion set disambiguation. The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose. Most learners that have been applied to this problem use as features the words and part of speech tags appearing within a fixed window, as well as collocations surrounding the ambiguity site; these are essentially the same features as those used for the other disambiguation-in-stringcontext"
H01-1052,P96-1010,0,0.0772844,"the field to concentrate considerably more effort into enlarging our training corpora and addressing scalability issues, rather than continuing to explore different learning methods applied to the relatively small extant training corpora. 2. PREVIOUS WORK 2.1 Confusion Set Disambiguation Several methods have been presented for confusion set disambiguation. The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose. Most learners that have been applied to this problem use as features the words and part of speech tags appearing within a fixed window, as well as collocations surrounding the ambiguity site; these are essentially the same features as those used for the other disambiguation-in-stringcontext"
H01-1052,A97-1025,0,0.191507,"per we explore what happens when significantly larger training corpora are used. Our results suggest that it may make sense for the field to concentrate considerably more effort into enlarging our training corpora and addressing scalability issues, rather than continuing to explore different learning methods applied to the relatively small extant training corpora. 2. PREVIOUS WORK 2.1 Confusion Set Disambiguation Several methods have been presented for confusion set disambiguation. The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose. Most learners that have been applied to this problem use as features the words and part of speech tags appearing within a fixed window, as well as collocations"
H01-1052,W97-1011,0,0.060794,"corpora are used. Our results suggest that it may make sense for the field to concentrate considerably more effort into enlarging our training corpora and addressing scalability issues, rather than continuing to explore different learning methods applied to the relatively small extant training corpora. 2. PREVIOUS WORK 2.1 Confusion Set Disambiguation Several methods have been presented for confusion set disambiguation. The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose. Most learners that have been applied to this problem use as features the words and part of speech tags appearing within a fixed window, as well as collocations surrounding the ambiguity site; these are essentially the same"
H01-1052,P94-1013,0,0.026933,"r results suggest that it may make sense for the field to concentrate considerably more effort into enlarging our training corpora and addressing scalability issues, rather than continuing to explore different learning methods applied to the relatively small extant training corpora. 2. PREVIOUS WORK 2.1 Confusion Set Disambiguation Several methods have been presented for confusion set disambiguation. The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5]. In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker. Then everywhere the system sees this marker, it must decide which member of the confusion set to choose. Most learners that have been applied to this problem use as features the words and part of speech tags appearing within a fixed window, as well as collocations surrounding the ambiguity site; these are essentially the same features as those use"
H90-1055,A88-1019,0,\N,Missing
H90-1055,P89-1010,0,\N,Missing
H92-1022,A88-1019,0,0.0193152,"models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact t h a t very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust. One area in which the statistical approach has done particularly well is automatic part of speech tagging, assigning each word in an input sentence its proper part of speech [1, 2, 3, 4, 6, 9, 11, 12]. Stochastic taggers have *A version of this paper appears in Proceedings of the Third Conference on Applied Computational Linguistics (ACL), Trento, Italy, 1992. Used by permission of the Association for Computational Linguistics; copies of the publication from which this material is derived can can be obtained from Dr. Donald E. Walker (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910, Morristown, NJ 07960-1910, USA. The author would like to thank Mitch Marcus and Rich Pito for valuable input. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO g"
H92-1022,A92-1018,0,0.302158,"models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact t h a t very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust. One area in which the statistical approach has done particularly well is automatic part of speech tagging, assigning each word in an input sentence its proper part of speech [1, 2, 3, 4, 6, 9, 11, 12]. Stochastic taggers have *A version of this paper appears in Proceedings of the Third Conference on Applied Computational Linguistics (ACL), Trento, Italy, 1992. Used by permission of the Association for Computational Linguistics; copies of the publication from which this material is derived can can be obtained from Dr. Donald E. Walker (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910, Morristown, NJ 07960-1910, USA. The author would like to thank Mitch Marcus and Rich Pito for valuable input. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO g"
H92-1022,J88-1003,0,0.0301434,"models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact t h a t very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust. One area in which the statistical approach has done particularly well is automatic part of speech tagging, assigning each word in an input sentence its proper part of speech [1, 2, 3, 4, 6, 9, 11, 12]. Stochastic taggers have *A version of this paper appears in Proceedings of the Third Conference on Applied Computational Linguistics (ACL), Trento, Italy, 1992. Used by permission of the Association for Computational Linguistics; copies of the publication from which this material is derived can can be obtained from Dr. Donald E. Walker (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910, Morristown, NJ 07960-1910, USA. The author would like to thank Mitch Marcus and Rich Pito for valuable input. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO g"
H92-1022,P89-1015,0,0.0322225,"n the input. These stochastic p a r t of speech taggers make use of a Markov model which captures lexical and contextual information. The parameters of the model can be estimated from tagged [1, 3, 4, 6, 12] or untagged [2, 9, 11] text. Once the parameters of the model are estimated, a sentence can then be automatically tagged by assigning it the tag sequence which is assigned the highest probability by the model. Performance is often enhanced with the aid of various higher level pre- and postprocessing procedures or by manually tuning the model. A number of rule-based taggers have been built [10, 7, 8]. [10] and [7] b o t h have error rates substantially higher than state of the art stochastic taggers. [8] disambiguates words within a deterministic parser. We wanted to determine whether a simple rule-based tagger without any knowledge of syntax can perform as well as a stochastic tagger, or if part of speech tagging really is a domain to which stochastic techniques are better suited. In this paper we describe a rule-based tagger which performs as well as taggers based upon probabilistic models. T h e rule-based tagger overcomes the limitations common in rule-based approaches to language pro"
H92-1022,H89-2014,0,0.00878957,"models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact t h a t very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust. One area in which the statistical approach has done particularly well is automatic part of speech tagging, assigning each word in an input sentence its proper part of speech [1, 2, 3, 4, 6, 9, 11, 12]. Stochastic taggers have *A version of this paper appears in Proceedings of the Third Conference on Applied Computational Linguistics (ACL), Trento, Italy, 1992. Used by permission of the Association for Computational Linguistics; copies of the publication from which this material is derived can can be obtained from Dr. Donald E. Walker (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910, Morristown, NJ 07960-1910, USA. The author would like to thank Mitch Marcus and Rich Pito for valuable input. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO g"
H92-1022,H91-1065,0,0.0263201,"models to natural language processing over the last few years. The appeal of stochastic techniques over traditional rule-based techniques comes from the ease with which the necessary statistics can be automatically acquired and the fact t h a t very little handcrafted knowledge need be built into the system. In contrast, the rules in rule-based systems are usually difficult to construct and are typically not very robust. One area in which the statistical approach has done particularly well is automatic part of speech tagging, assigning each word in an input sentence its proper part of speech [1, 2, 3, 4, 6, 9, 11, 12]. Stochastic taggers have *A version of this paper appears in Proceedings of the Third Conference on Applied Computational Linguistics (ACL), Trento, Italy, 1992. Used by permission of the Association for Computational Linguistics; copies of the publication from which this material is derived can can be obtained from Dr. Donald E. Walker (ACL), Bellcore, MRE 2A379, 445 South Street, Box 1910, Morristown, NJ 07960-1910, USA. The author would like to thank Mitch Marcus and Rich Pito for valuable input. This work was supported by DARPA and AFOSR jointly under grant No. AFOSR-90-0066, and by ARO g"
H92-1030,H90-1055,1,0.927487,"e tagged Brown Corpus [4] as input. The system acquires a context-free g r a m m a r where each rule is assigned a score. Once the g r a m m a r is learned, it can be used to find and score phrase structure analyses of a string of part of speech tags. The nonterminal nodes of the resulting phrase structure tree are not labelled. The system is able to assign a phrase structure analysis consistent with the string of part of speech tags with high accuracy. There have been several other recent proposals for automatic phrase structure acquisition based on statistics gathered over large corpora. In [1, 9], a statistic based on mutual information is used to find phrase boundaries. [11] defines a function to score the quality of parse trees, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence. A number of papers describe results obtained using the Inside-Outside algorithm to train a probabilistic context-free g r a m m a r [10, 6, 8]. Below we describe an alternate method of phrase structure acquisition. 2. H O W IT W O R K S The system automatically acquires a grammar of scored context-free rules, where each rule is binary branchin"
H92-1030,P91-1047,1,0.909305,"cess to informants, who are in effect infinite corpora. If one hears the boy finished the homework, the informant can be queried whether the girl finished the homework is also permissible. The procedures Harris outlines for the linguist to use to discover linguistic structure could be used to automatically acquire grammatical information if it were possible to do away with the need for a h u m a n informant. It is possible that a variation of these procedures could extract information by observing distributional similarities in a sufficiently large corpus of unparsed text. In an earlier paper [2], we demonstrated t h a t simple distributional analysis over a corpus can lead to the discovery of word classes. In this paper, we describe work in which we apply distributional analysis in an a t t e m p t to automatically acquire the phrase structure of a language. We describe a system which automatically acquires English phrase structure, given only the tagged Brown Corpus [4] as input. The system acquires a context-free g r a m m a r where each rule is assigned a score. Once the g r a m m a r is learned, it can be used to find and score phrase structure analyses of a string of part of spe"
H92-1030,P92-1017,0,0.042563,"stent with the string of part of speech tags with high accuracy. There have been several other recent proposals for automatic phrase structure acquisition based on statistics gathered over large corpora. In [1, 9], a statistic based on mutual information is used to find phrase boundaries. [11] defines a function to score the quality of parse trees, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence. A number of papers describe results obtained using the Inside-Outside algorithm to train a probabilistic context-free g r a m m a r [10, 6, 8]. Below we describe an alternate method of phrase structure acquisition. 2. H O W IT W O R K S The system automatically acquires a grammar of scored context-free rules, where each rule is binary branching. Two sources of distributional information are used to acquire and score the rules. The score for the rule tag~ tagy tagz is a function of: 1. The distributional similarity of the part of speech tag tagx and the pair of tags tagy tagz. 2. A comparison of the entropy of the environment tagy _ and tagy tagz --. The entropy of environment tag~ _ is a measure of the randomness of the distribution"
H92-1030,C86-1033,0,0.290287,"r where each rule is assigned a score. Once the g r a m m a r is learned, it can be used to find and score phrase structure analyses of a string of part of speech tags. The nonterminal nodes of the resulting phrase structure tree are not labelled. The system is able to assign a phrase structure analysis consistent with the string of part of speech tags with high accuracy. There have been several other recent proposals for automatic phrase structure acquisition based on statistics gathered over large corpora. In [1, 9], a statistic based on mutual information is used to find phrase boundaries. [11] defines a function to score the quality of parse trees, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence. A number of papers describe results obtained using the Inside-Outside algorithm to train a probabilistic context-free g r a m m a r [10, 6, 8]. Below we describe an alternate method of phrase structure acquisition. 2. H O W IT W O R K S The system automatically acquires a grammar of scored context-free rules, where each rule is binary branching. Two sources of distributional information are used to acquire and score the ru"
H92-1030,H92-1024,0,\N,Missing
H92-1030,J92-4003,0,\N,Missing
H93-1047,H92-1022,1,0.466236,"output which better resembles the phrase structure found in the training corpus. Once a set of transformations has been learned, the system is capable of taking sentences tagged with parts of speech and returning a binary-branching structure with nonterminals unlabelled 3. 2.1. T h e Initial State O f T h e Parser Initially, the parser operates by assigning a right-linear structure to all sentences. The only exception is that final punctuation is attached high. So, the sentence &quot;The dog and old cat ate .&quot; would be incorrectly bracketed as: ( ( The ( dog ( and ( old ( cat.ate ) ) ) ) ). ) see [5, 4]. 3This is the same output given:bysystemsdescribed in [10, 3, 12, 14] 237 The parser in its initial state will obviously not bracket sentences with great accuracy. In some experiments below, we begin with an even more naive initial state of knowledge: sentences are parsed by assigning them a random binarybranching structure with final punctuation always attached high. carrying out these operations will transform this subtree intoS: Z X 2.2. Structural Transformations The next stage involves learning a set of transformations that can be applied to the output of the naive parser to make these s"
H93-1047,H90-1021,0,0.0410088,"Missing"
H93-1047,J93-2004,0,0.0636671,"training corpUs is necessary. In addition, since some tokens in a sentence are not even considered in parsing, the method could prove to be considerably more resistant to noise than a CFG-based approach. After describing the algorithm, we present results and compare these results to other recent results in automatic phrase structure induction. 2. T H E A L G O R I T H M The learning algorithm is trained on a small corpus of partially bracketed text which is also annotated with part of speech information. All of the experiments presented below were done using the Penn Treebank annotated corpus[11]. The learner begins in a naive initial state, knowing very little about the phrase structure of the target corpus. In particular, all that is initially known is that English tends to be right branching and that final punctuation is final punctuation. Transformations are then learned automatically which transform the output of the naive parser into output which better resembles the phrase structure found in the training corpus. Once a set of transformations has been learned, the system is capable of taking sentences tagged with parts of speech and returning a binary-branching structure with no"
H93-1047,P92-1017,0,0.30409,"or a given sentence. In [3], distributional analysis techniques are applied to a large corpus to learn a context-free grammar. The most promising results to date have been based on the inside-outside algorithm (i-o algorithm), which can be used to train stochastic context-free grammars. The i-o algorithm is an extension of the finite-state based Hidden Markov Model (by [1]), which has been applied successfully in many areas, including speech recognition and part of speech tagging. A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar [9, 15, 12, 6, 7, 14]. Below, we describe a new technique for grammar induction. 2 *The author would like to thank Mark Liberman, Meiting Lu, David Magerman, Mitch Marcus, Rich Pito, GiorgioSatta, Yves Schabes and Tom Veatch. This work was supportedby DARPAand AFOSRjointly undergrant No. AFOSR-90-0066,and by A&apos;ROgrant No. DAAL03-89-C0031PRI. INot in the traditional sense of the term. 2A similar methodhas been applied effectivelyin part of speech tagging; The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of parsing in the current state to"
H93-1047,C86-1033,0,0.0206673,"utomatic grammar induction. 1. I N T R O D U C T I O N There has been a great deal of interest of late in the automatic induction of natural language grammar. Given the difficulty inherent in manually building a robust parser, along with the availability of large amounts of training material, automatic grammar induction seems like a path worth pursuing. A number of systems have been built which can be trained automatically to bracket text into syntactic constituents. In [ 10] mutual information statistics are extracted from a corpus of text and this information is then used to parse new text. [13] defines a function to score the quality of parse trees, and then uses simulated annealing to heuristically explore the entire space of possible parses for a given sentence. In [3], distributional analysis techniques are applied to a large corpus to learn a context-free grammar. The most promising results to date have been based on the inside-outside algorithm (i-o algorithm), which can be used to train stochastic context-free grammars. The i-o algorithm is an extension of the finite-state based Hidden Markov Model (by [1]), which has been applied successfully in many areas, including speech r"
H93-1047,E93-1040,0,0.0839158,"or a given sentence. In [3], distributional analysis techniques are applied to a large corpus to learn a context-free grammar. The most promising results to date have been based on the inside-outside algorithm (i-o algorithm), which can be used to train stochastic context-free grammars. The i-o algorithm is an extension of the finite-state based Hidden Markov Model (by [1]), which has been applied successfully in many areas, including speech recognition and part of speech tagging. A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar [9, 15, 12, 6, 7, 14]. Below, we describe a new technique for grammar induction. 2 *The author would like to thank Mark Liberman, Meiting Lu, David Magerman, Mitch Marcus, Rich Pito, GiorgioSatta, Yves Schabes and Tom Veatch. This work was supportedby DARPAand AFOSRjointly undergrant No. AFOSR-90-0066,and by A&apos;ROgrant No. DAAL03-89-C0031PRI. INot in the traditional sense of the term. 2A similar methodhas been applied effectivelyin part of speech tagging; The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of parsing in the current state to"
H93-1047,H90-1054,0,0.0244555,"or a given sentence. In [3], distributional analysis techniques are applied to a large corpus to learn a context-free grammar. The most promising results to date have been based on the inside-outside algorithm (i-o algorithm), which can be used to train stochastic context-free grammars. The i-o algorithm is an extension of the finite-state based Hidden Markov Model (by [1]), which has been applied successfully in many areas, including speech recognition and part of speech tagging. A number of recent papers have explored the potential of using the i-o algorithm to automatically learn a grammar [9, 15, 12, 6, 7, 14]. Below, we describe a new technique for grammar induction. 2 *The author would like to thank Mark Liberman, Meiting Lu, David Magerman, Mitch Marcus, Rich Pito, GiorgioSatta, Yves Schabes and Tom Veatch. This work was supportedby DARPAand AFOSRjointly undergrant No. AFOSR-90-0066,and by A&apos;ROgrant No. DAAL03-89-C0031PRI. INot in the traditional sense of the term. 2A similar methodhas been applied effectivelyin part of speech tagging; The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of parsing in the current state to"
H93-1047,H92-1030,1,\N,Missing
H94-1037,H90-1020,0,0.079663,"Missing"
H94-1037,H93-1003,0,0.0165873,"ormation, such as flight schedules from one city to another, obtained from a small relational database excised from the Official Airline Guide. By requiring that all system developers use the same database, it has been possible to compare the performance of various spoken language systems based on their ability to extract the correct information from the database, using a set of prescribed training and test data, and a set of interpretation guidelines. Indeed, periodic common evaluations have occurred at regular intervals, and steady performance improvements have been observed for all systems [2, 3, 4]. While the ATIS task has been instrumental in the development of technologies that can understand spontaneously generated verbal queries in a limited domain, it 1This research was supported by ARPA under Contract N0001489-J-1332, monitored through the Office of Naval Research. 2The authors are listed in reversed alphabetical order. does have some shortcomings. First, the current common evaluation focuses on the correctness of the information extracted from the database without any regard to the system&apos;s side of the interchange (e.g., clarification queries and helpful suggestions). Thus it has"
H94-1037,H89-1027,1,0.856684,"Missing"
H94-1037,J92-1004,1,0.853058,"Missing"
H94-1037,H93-1009,0,0.0130462,"entations. Our view on the appropriate structural units of a semantic frame has evolved over time. Our present view is that all major constituents in a sentence can be classified as one of only three distinct categories, which we label as [clause], [topic], and [ p r e d i c a t e ] . Thus, verbs, adjectives, prepositions and modifier nouns are all considered to be predicates. Furthermore, grammatical constituents such as ""subject"" and ""direct object"" are not explicitly marked in the semantic frame. We have applied this new formalism successfully across several languages in our VOYAGER domain [10], and we are also using it in P E G A S U S . An example semantic frame for the sentence, ""Is there a United flight connecting in Denver,"" is shown in Figure 2. 202 System Manager During the design phase of our project, we made a commitment not to alter the interface and protocols of EAASY SABRE. W e see no benefit, nor do we feel competent, in making changes to a proven system used by many users. In fact, PEGASUS&apos;s interface to EAASY SABRE is identical to that of a user on a PC or a travel agent EAASY SABRE cannot distinguish between a user speaking a natural utterance (such as ""I want to go"
H94-1037,H91-1070,1,0.90177,"Like a travel agent, PEGASUS needs to know the source, destination, and date before it can provide the flight information. The user utilized additional constraints to narrow down the choices before settling on a particular flight. It took two exchanges to arrive at the appropriate fare, and three more to book the return flight. The entire booking took nine exchanges, and lasted approximately 5 minutes. Note that a large fraction of the time is spent waiting for EAASY SABRE to respond. The dialogue component of PEGASUS is significantly more complicated than t h a t of the original ATIS system [11]. This is in large part due to the fact that it must monitor not only the user&apos;s dialogue state and the degree of completion of the booking, but also the state of the EAASY SABRE system. For instance, it must preprocess fare restrictions, warning the user of limits imposed on return dates for restricted fares, screening selected fares for possible constraint failure, and confirming availability on selected flights before attempting to issue bookings. Otherwise, the EAASY SABRE system would invoke a complex subdialogue which we wish to avoid. The system keeps a record of the most recently displ"
H94-1037,H92-1016,1,\N,Missing
H94-1037,H94-1011,0,\N,Missing
H94-1037,H91-1014,1,\N,Missing
H94-1037,H92-1004,0,\N,Missing
H94-1049,H92-1022,1,0.913454,"Missing"
H94-1049,P93-1035,1,0.87683,"Missing"
H94-1049,A88-1019,0,0.135507,"Missing"
H94-1049,A92-1018,0,0.0667344,"Missing"
H94-1049,J88-1003,0,0.16304,"Missing"
H94-1049,P90-1031,0,0.0294261,"Missing"
H94-1049,J93-2004,0,0.0331238,"Missing"
H94-1049,J93-2006,0,\N,Missing
H94-1049,H93-1047,1,\N,Missing
H94-1049,C92-2067,0,\N,Missing
J95-4004,P93-1005,0,0.0477528,"tion bottleneck inhibiting the creation of robust and accurate natural language processing systems. A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993). Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Jelinek, and Mercer 1990; Black et al. 1993) and by computing statistical measures of lexical association (Hindle and Rooth 1993). Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with high accuracy when all information is derived automatically from corpora (Brown, Lai, and Mercer 1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994). An effort has recently been undertaken to create automated machine translation systems in which the linguistic information needed for translation is extracte"
J95-4004,H92-1023,0,0.0918625,"s (1994). 6 All of the p r o g r a m s described herein are freely available with no restrictions on u s e or redistribution. For information on obtaining the tagger, contact the author. 551 Computational Linguistics Volume 21, Number 4 learner, for several reasons. There are a number of large tagged corpora available, allowing for a variety of experiments to be run. Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years (e.g., Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo 1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994). Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography. Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation and prepositional phrase attachment disambigua"
J95-4004,H92-1022,1,0.245817,"ect fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging. 1. Introduction It has recently become clear that automatically extracting linguistic information from a sample text corpus can be an extremely powerful method of overcoming the linguistic knowledge acquisition bottleneck inhibiting the creation of robust and accurate natural language processing systems. A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993). Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Jelinek, and Mercer 1990; Black et al. 1993) and by computing statistical measures of lexical association (Hindle and Rooth 1993). Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in"
J95-4004,P93-1035,1,0.0931461,"Missing"
J95-4004,W16-4913,0,0.0949018,"Missing"
J95-4004,1993.iwpt-1.3,1,0.139321,"Missing"
J95-4004,C94-2195,1,0.189576,"ger, the information about words that follow modals would be hidden deeply in the thousands or tens of thousands of contextual probabilities (P(Tagi I Zagi-lZagi-2) ) and the result of multiplying different combinations of these probabilities together. Below, we describe a new approach to corpus-based natural language processing, called transformation-based error-driven learning. This algorithm has been applied to a number of natural language problems, including part-of-speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing (Brill 1992; Brill 1993a; Brill 1993b; Brill and Resnik 1994; Brill 1994). We have also recently begun exploring the use of this technique for letter-to-sound generation and for building pronunciation networks for speech recognition. In this approach, the learned linguistic information is represented in a concise and easily understood form. This property should make transformation-based learning a useful tool for further exploring linguistic modeling and attempting to discover ways of more tightly coupling the underlying linguistic systems and our approximating models. 544 Brill Transformation-Based Error-Driven Learning UNANNOTATED TEXT STATE /~kI~INO"
J95-4004,J90-2002,0,0.236546,"of lexical association (Hindle and Rooth 1993). Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with high accuracy when all information is derived automatically from corpora (Brown, Lai, and Mercer 1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994). An effort has recently been undertaken to create automated machine translation systems in which the linguistic information needed for translation is extracted automatically from aligned corpora (Brown et al. 1990). These are just a few of the many recent applications of corpus-based techniques in natural language processing. • Department of Computer Science,Baltimore, MD 21218-2694.E-mail: brill@cs.jhu.edu. © 1995Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 Along with great research advances, the infrastructure is in place for this line of research to grow even stronger, with on-line corpora, the grist of the corpus-based natural language processing grindstone, getting bigger and better and becoming more readily available. There are a number of efforts worldwi"
J95-4004,P91-1034,0,0.203712,"Missing"
J95-4004,A88-1019,0,0.0903203,"ormation in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging. 1. Introduction It has recently become clear that automatically extracting linguistic information from a sample text corpus can be an extremely powerful method of overcoming the linguistic knowledge acquisition bottleneck inhibiting the creation of robust and accurate natural language processing systems. A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993). Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Jelinek, and Mercer 1990; Black et al. 1993) and by computing statistical measures of lexical association (Hindle and Rooth 1993). Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic"
J95-4004,A92-1018,0,0.165568,"clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging. 1. Introduction It has recently become clear that automatically extracting linguistic information from a sample text corpus can be an extremely powerful method of overcoming the linguistic knowledge acquisition bottleneck inhibiting the creation of robust and accurate natural language processing systems. A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993). Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Jelinek, and Mercer 1990; Black et al. 1993) and by computing statistical measures of lexical association (Hindle and Rooth 1993). Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge,"
J95-4004,P90-1031,0,0.130413,"gorithms, see R a m s h a w a n d M a r c u s (1994). 6 All of the p r o g r a m s described herein are freely available with no restrictions on u s e or redistribution. For information on obtaining the tagger, contact the author. 551 Computational Linguistics Volume 21, Number 4 learner, for several reasons. There are a number of large tagged corpora available, allowing for a variety of experiments to be run. Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years (e.g., Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo 1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994). Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography. Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation"
J95-4004,J88-1003,0,0.0611179,"ding these two learning algorithms, see R a m s h a w a n d M a r c u s (1994). 6 All of the p r o g r a m s described herein are freely available with no restrictions on u s e or redistribution. For information on obtaining the tagger, contact the author. 551 Computational Linguistics Volume 21, Number 4 learner, for several reasons. There are a number of large tagged corpora available, allowing for a variety of experiments to be run. Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years (e.g., Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo 1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994). Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography. Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such"
J95-4004,W89-0209,0,0.0679707,"ful method of overcoming the linguistic knowledge acquisition bottleneck inhibiting the creation of robust and accurate natural language processing systems. A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993). Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Jelinek, and Mercer 1990; Black et al. 1993) and by computing statistical measures of lexical association (Hindle and Rooth 1993). Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with high accuracy when all information is derived automatically from corpora (Brown, Lai, and Mercer 1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994). An effort has recently been undertaken to create automated machine translation systems in which the li"
J95-4004,P91-1023,0,0.109351,"s-based natural language processing grindstone, getting bigger and better and becoming more readily available. There are a number of efforts worldwide to manually annotate large corpora with linguistic information, including parts of speech, phrase structure and predicate-argument structure (e.g., the Penn Treebank and the British National Corpus (Marcus, Santorini, and Marcinkiewicz 1993; Leech, Garside, and Bryant 1994)). A vast amount of on-line text is now available, and much more will become available in the future. Useful tools, such as large aligned corpora (e.g., the aligned Hansards (Gale and Church 1991)) and semantic word hierarchies (e.g., Wordnet (Miller 1990)), have also recently become available. Corpus-based methods are often able to succeed while ignoring the true complexities of language, banking on the fact that complex linguistic phenomena can often be indirectly observed through simple epiphenomena. For example, one could accurately assign a part-of-speech tag to the word race in (1-3) without any reference to phrase structure or constituent movement: One would only have to realize that, usually, a word one or two words to the right of a modal is a verb and not a noun. An exception"
J95-4004,C94-1103,0,0.07033,"Missing"
J95-4004,P89-1015,0,0.0563665,"o learning algorithms, see R a m s h a w a n d M a r c u s (1994). 6 All of the p r o g r a m s described herein are freely available with no restrictions on u s e or redistribution. For information on obtaining the tagger, contact the author. 551 Computational Linguistics Volume 21, Number 4 learner, for several reasons. There are a number of large tagged corpora available, allowing for a variety of experiments to be run. Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years (e.g., Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo 1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994). Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography. Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense"
J95-4004,J93-1005,0,0.0840831,"rocessing systems. A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993). Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Jelinek, and Mercer 1990; Black et al. 1993) and by computing statistical measures of lexical association (Hindle and Rooth 1993). Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with high accuracy when all information is derived automatically from corpora (Brown, Lai, and Mercer 1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994). An effort has recently been undertaken to create automated machine translation systems in which the linguistic information needed for translation is extracted automatically from aligned corpora (Brown et al. 1990). These are just a few of the"
J95-4004,C94-1024,0,0.0732806,"xical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation and prepositional phrase attachment disambiguation. 7 Also, it is possible to cast a number of other useful problems as part-of-speech tagging problems, such as letter-to-sound translation (Huang, Son-Bell, and Baggett 1994) and building pronunciation networks for speech recognition. Recently, a method has been proposed for using part-of-speech tagging techniques as a method for parsing with lexicalized grammars (Joshi and Srinivas 1994). When automated part-of-speech tagging was initially explored (Klein and Simmons 1963; Harris 1962), people manually engineered rules for tagging, sometimes with the aid of a corpus. As large corpora became available, it became clear that simple Markov-model based stochastic taggers that were automatically trained could achieve high rates of tagging accuracy (Jelinek 1985). Markov-model based taggers assign to a sentence the tag sequence that maximizes Prob(word I tag),Prob(tag I previous n tags). These probabilities can be estimated directly from a manually tagged corpus, s These stochastic"
J95-4004,J93-2004,0,0.0748437,"Missing"
J95-4004,J94-2001,0,0.475954,"a m s h a w a n d M a r c u s (1994). 6 All of the p r o g r a m s described herein are freely available with no restrictions on u s e or redistribution. For information on obtaining the tagger, contact the author. 551 Computational Linguistics Volume 21, Number 4 learner, for several reasons. There are a number of large tagged corpora available, allowing for a variety of experiments to be run. Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years (e.g., Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo 1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994). Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography. Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation and preposition"
J95-4004,W94-0111,0,0.106568,"hen making the transformation given the part-of-speech tag of the previous word (lines 8 and 9). If X is the current tag and Y is the correct tag, then the transformation will result in one less error, so we increment the number of improvements caused when making the transformation given the part-of-speech tag of the previous word (lines 6 and 7). In certain cases, a significant increase in speed for training the transformationbased tagger can be obtained by indexing in the corpus where different transformations can and do apply. For a description of a fast index-based training algorithm, see Ramshaw and Marcus (1994). In figure 4, we list the first twenty transformations learned from training on the Penn Treebank Wall Street Journal Corpus (Marcus, Santorini, and Marcinkiewicz 1993). 12 The first transformation states that a noun should be changed to a verb if 12 Version 0.5 of the P e n n Treebank w a s u s e d in all e x p e r i m e n t s reported in this paper. 554 Brill Transformation-Based Error-Driven Learning # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Change Tag From To NN VB VBP VB NN VB VB NN VBD VBN VBN VBD VBN VBD VBD VBN VBP VB POS VBZ VB VBP VBD VBN IN WDT VBD VBN VB VBP IN WDT IN D"
J95-4004,J95-2004,0,0.123898,"Missing"
J95-4004,P94-1025,0,0.12678,"on u s e or redistribution. For information on obtaining the tagger, contact the author. 551 Computational Linguistics Volume 21, Number 4 learner, for several reasons. There are a number of large tagged corpora available, allowing for a variety of experiments to be run. Part-of-speech tagging is an active area of research; a great deal of work has been done in this area over the past few years (e.g., Jelinek 1985; Church 1988; Derose 1988; Hindle 1989; DeMarcken 1990; Merialdo 1994; Brill 1992; Black et al. 1992; Cutting et al. 1992; Kupiec 1992; Charniak et al. 1993; Weischedel et al. 1993; Schutze and Singer 1994). Part-of-speech tagging is also a very practical application, with uses in many areas, including speech recognition and generation, machine translation, parsing, information retrieval and lexicography. Insofar as tagging can be seen as a prototypical problem in lexical ambiguity, advances in part-of-speech tagging could readily translate to progress in other areas of lexical, and perhaps structural, ambiguity, such as wordsense disambiguation and prepositional phrase attachment disambiguation. 7 Also, it is possible to cast a number of other useful problems as part-of-speech tagging problems,"
J95-4004,H90-1054,0,0.0877924,"Missing"
J95-4004,J93-2006,0,0.125647,"without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging. 1. Introduction It has recently become clear that automatically extracting linguistic information from a sample text corpus can be an extremely powerful method of overcoming the linguistic knowledge acquisition bottleneck inhibiting the creation of robust and accurate natural language processing systems. A number of part-of-speech taggers are readily available and widely used, all trained and retrainable on text corpora (Church 1988; Cutting et al. 1992; Brill 1992; Weischedel et al. 1993). Endemic structural ambiguity, which can lead to such difficulties as trying to cope with the many thousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Jelinek, and Mercer 1990; Black et al. 1993) and by computing statistical measures of lexical association (Hindle and Rooth 1993). Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with h"
J95-4004,C92-2070,0,0.150669,"ousands of possible parses that a grammar can assign to a sentence, can be greatly reduced by adding empirically derived probabilities to grammar rules (Fujisaki et al. 1989; Sharman, Jelinek, and Mercer 1990; Black et al. 1993) and by computing statistical measures of lexical association (Hindle and Rooth 1993). Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with high accuracy when all information is derived automatically from corpora (Brown, Lai, and Mercer 1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994). An effort has recently been undertaken to create automated machine translation systems in which the linguistic information needed for translation is extracted automatically from aligned corpora (Brown et al. 1990). These are just a few of the many recent applications of corpus-based techniques in natural language processing. • Department of Computer Science,Baltimore, MD 21218-2694.E-mail: brill@cs.jhu.edu. © 1995Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 Along with great research advances, t"
J95-4004,J93-1004,0,\N,Missing
J95-4004,H93-1047,1,\N,Missing
J95-4004,P94-1020,0,\N,Missing
N04-1008,J93-2003,0,\N,Missing
N04-1008,P02-1040,0,\N,Missing
N04-1008,P03-1003,0,\N,Missing
N04-1008,W03-1210,0,\N,Missing
N04-1008,N03-1020,0,\N,Missing
N04-1008,P03-1021,0,\N,Missing
P00-1037,J00-4006,0,\N,Missing
P00-1037,A94-1037,0,\N,Missing
P01-1005,P98-1029,1,0.108162,"and then combining their outputs in classification, it is often possible to achieve a target accuracy with less labeled training data than would be needed if only one classifier was being used. Voting can be effective in reducing both the bias of a particular training corpus and the bias of a specific learner. When a training corpus is very small, there is much more room for these biases to surface and therefore for voting to be effective. But does voting still offer performance gains when classifiers are trained on much larger corpora? The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy. As training size increases significantly, we would expect complementarity between classifiers to decrease. This is due in part to the fact that a larger training corpus will reduce the data set variance and any bias arising from this. Also, some of As a result of comparing a sample of two learners as a function of increasingly large training sets, we see in Table 1 that complementarity does indeed decrease as training size increa"
P01-1005,W95-0104,0,0.0604482,"rious machine learning algorithms : winnow 1 , perceptron, naïve Bayes, and a very simple memory-based learner. For the first three learners, we used the standard collection of features employed for this problem: the set of words within a window of the target word, and collocations containing words and/or parts of 1 Thanks to Dan Roth for making both Winnow and Perceptron available. speech. The memory-based learner used only the word before and word after as features. 1.00 0.95 0.90 Test Accuracy 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose. Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in which the ambiguity site appears. Other such problems include word sense disambiguation, pa"
P01-1005,P96-1010,0,0.00390144,"earning algorithms : winnow 1 , perceptron, naïve Bayes, and a very simple memory-based learner. For the first three learners, we used the standard collection of features employed for this problem: the set of words within a window of the target word, and collocations containing words and/or parts of 1 Thanks to Dan Roth for making both Winnow and Perceptron available. speech. The memory-based learner used only the word before and word after as features. 1.00 0.95 0.90 Test Accuracy 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996). In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g. {to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose. Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in which the ambiguity site appears. Other such problems include word sense disambiguation, part of speech tagging and som"
P01-1005,W99-0623,1,0.125767,"out an effort made to compress information. In such cases, one could look at numerous methods for compressing data (e.g. Dagan and Engleson, 1995, Weng, et al, 1998). 4 the differences between classifiers might be due to how they handle a sparse training set. 1000000 100000 10000 1000 100 10 Winnow Memory-Based 1 1 10 100 1000 Millions of Words Figure 2. Representation Size vs. Training Corpus Size The Efficacy of Voting Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al, 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000). By training a set of classifiers on a single training corpus and then combining their outputs in classification, it is often possible to achieve a target accuracy with less labeled training data than would be needed if only one classifier was being used. Voting can be effective in reducing both the bias of a particular training corpus and the bias of a specific learner. When a training corpus is very small, there is much more room for these biases to surface and therefore for voting to be effective. But does voting still offer performance gains"
P01-1005,A97-1025,0,0.00766892,"able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost. 2 Confusion Set Disambiguation Confusion set disambiguation is the problem of choosing the correct use of a word, given a set of words with which it is commonly confused. Example confusion sets include: {principle , principal}, {then, than}, {to,two,too}, and {weather,whether}. Numerous methods have been presented for confusable disambiguation. The more recent set of techniques includes mult iplicative weightupdate algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation-based learning (Mangu and Brill, 1997), differential grammars (Powers, 3 Learning Curve Expe riments This work was partially motivated by the desire to develop an improved grammar checker. Given a fixed amount of time, we considered what would be the most effective way to focus our efforts in order to attain the greatest performance improvement. Some possibilities included modifying standard learning algorithms, exploring new learning techniques, and using more sophisticated features. Before exploring these somewhat expensive paths, we decided to first see what happened if we"
P01-1005,A00-2000,0,0.0516797,"Missing"
P01-1005,W97-1011,0,0.0345035,"Missing"
P01-1005,P98-1081,0,0.0220574,"Missing"
P01-1005,J94-2001,0,0.0389843,"Missing"
P01-1005,A00-2009,0,0.00800697,"Missing"
P01-1005,P95-1026,0,0.382863,"Missing"
P01-1005,H01-1052,1,\N,Missing
P01-1005,C98-1078,0,\N,Missing
P01-1005,P94-1013,0,\N,Missing
P01-1005,C98-1029,1,\N,Missing
P04-1078,N03-1020,0,0.714545,"duction of the BLEU metric for machine translation evaluation (Papineni et al, 2002), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors in human assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance (Och 2003). Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (Lin and Hovy, 2003), with an automatic evaluation metric called ROUGE, inspired by BLEU but twisted towards the specifics of the summarization task. An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations. Human evaluations, however, are subject to specific guidelines given to the human assessors when performing the evaluation task; the variation in human judgment is therefore highly influenced by these guidelines. It follows that, in order for an automatic evaluation to agree with a humanperformed evaluation, the evaluation metric used by th"
P04-1078,P02-1040,0,0.0927742,"ce statistics. The automatic evaluation metrics proposed to date for Machine Translation and Automatic Summarization are particular instances from the family of metrics we propose. We show that different members of the same family of metrics explain best the variations obtained with human evaluations, according to the application being evaluated (Machine Translation, Automatic Summarization, and Automatic Question Answering) and the evaluation guidelines used by humans for evaluating such applications. 1 Introduction With the introduction of the BLEU metric for machine translation evaluation (Papineni et al, 2002), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors in human assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance (Och 2003). Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (Lin and Hovy, 2003), with an automatic evaluation metric called ROUGE, inspired by B"
P04-1078,P03-1021,0,0.0123013,"swering) and the evaluation guidelines used by humans for evaluating such applications. 1 Introduction With the introduction of the BLEU metric for machine translation evaluation (Papineni et al, 2002), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors in human assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance (Och 2003). Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (Lin and Hovy, 2003), with an automatic evaluation metric called ROUGE, inspired by BLEU but twisted towards the specifics of the summarization task. An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations. Human evaluations, however, are subject to specific guidelines given to the human assessors when performing the evaluation task; the variation in human judgment is therefore highly influenced by these guidelines. It"
P04-1078,N04-1008,1,0.718984,"to be handled to so-called factoid questions. Automatic evaluation of factoid QA is often straightforward, as the number of correct answers is most of the time limited, and exhaustive lists of correct answers are available. When removing the factoid constraint, however, the set of possible answer to a (complex, beyondfactoid) question becomes unfeasibly large, and consequently automatic evaluation becomes a challenge. In this section, we focus on an evaluation carried out in order to assess the performance of a QA system for answering questions from the Frequently-Asked-Question (FAQ) domain (Soricut and Brill, 2004). These are generally questions requiring a more elaborated answer than a simple factoid (e.g., questions such as: “How does a film qualify for an Academy Award?”). In order to evaluate such a system a humanperformed evaluation was performed, in which 11 versions of the QA system (various modules were implemented using various algorithms) were separately evaluated. Each version was evaluated by a human evaluator, with no reference answer available. For this evaluation 115 test questions were used, and the human evaluator was asked to assess whether the proposed answer was correct, somehow rela"
P93-1035,H92-1030,1,0.907887,"Missing"
P93-1035,H92-1022,1,0.572035,"Missing"
P93-1035,H91-1060,0,0.015767,"Missing"
P93-1035,H90-1021,0,0.0424961,"Missing"
P93-1035,J93-2004,0,0.0605921,"Missing"
P93-1035,P92-1017,0,0.0368246,"Missing"
P93-1035,C86-1033,0,0.0894649,"Missing"
P93-1035,H90-1054,0,0.0640749,"Missing"
P93-1035,E93-1040,0,0.0352496,"Missing"
P96-1034,P93-1035,1,0.949729,"esky, who attempted to define a set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see (Chomsky, 1965)). Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with (Chomsky and Halle, 1968). More recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, prepositional phrase attachment disambiguation, and parsing, under the paradigm of transformation-based error-driven learning (see (Brill, 1993; Brill, 1995) and (Brill and Resnik, 1994)). In this paradigm, rules can be learned automatically from a training corpus, instead of being written by hand. Transformation-based systems are typically deterministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed. Since for each rule the application algorithm must check for a matching at all possible sites to see whether the rule can apply, these systems run in O(rrpn) time, where 7r is the number of rules, p is the"
P96-1034,J95-4004,1,0.388756,"empted to define a set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see (Chomsky, 1965)). Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with (Chomsky and Halle, 1968). More recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, prepositional phrase attachment disambiguation, and parsing, under the paradigm of transformation-based error-driven learning (see (Brill, 1993; Brill, 1995) and (Brill and Resnik, 1994)). In this paradigm, rules can be learned automatically from a training corpus, instead of being written by hand. Transformation-based systems are typically deterministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed. Since for each rule the application algorithm must check for a matching at all possible sites to see whether the rule can apply, these systems run in O(rrpn) time, where 7r is the number of rules, p is the cost of a sing"
P96-1034,C94-2195,1,0.921742,"set of transformations that would apply to a word sequence to map it from deep structure to surface structure (see (Chomsky, 1965)). Transformations have also been used in much of generative phonology to capture contextual variants in pronunciation, starting with (Chomsky and Halle, 1968). More recently, transformations have been applied to a diverse set of problems, including part of speech tagging, pronunciation network creation, prepositional phrase attachment disambiguation, and parsing, under the paradigm of transformation-based error-driven learning (see (Brill, 1993; Brill, 1995) and (Brill and Resnik, 1994)). In this paradigm, rules can be learned automatically from a training corpus, instead of being written by hand. Transformation-based systems are typically deterministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed. Since for each rule the application algorithm must check for a matching at all possible sites to see whether the rule can apply, these systems run in O(rrpn) time, where 7r is the number of rules, p is the cost of a single rule matching, and n is th"
P96-1034,J94-3001,0,0.0687146,"Missing"
P96-1034,J95-2004,0,0.0885849,"ormation-based systems are typically deterministic. Each rule in an ordered list of rules is applied once wherever it can apply, then is discarded, and the next rule is processed until the last rule in the list has been processed. Since for each rule the application algorithm must check for a matching at all possible sites to see whether the rule can apply, these systems run in O(rrpn) time, where 7r is the number of rules, p is the cost of a single rule matching, and n is the size of the input structure. While this results in fast processing, it is possible to create much faster systems. In (Roche and Schabes, 1995), 255 a method is described for converting a list of transformations that operates on strings into a deterministic finite state transducer, resulting in an optimal tagger in the sense that tagging requires only one state transition per word, giving a linear time tagger whose run-time is independent of the number and size of rules. In this paper we consider transformation-based parsing, introduced in (Brill, 1993), and we improve upon the O(Trpn) time upper bound.. In transformation-based parsing, an ordered sequence of tree-rewriting rules (tree transformations) are applied to an initial parse"
P96-1034,H93-1047,1,\N,Missing
P98-1029,H92-1023,0,0.0607683,"Missing"
P98-1029,W96-0102,0,0.0743667,"Missing"
P98-1029,J93-2004,0,0.0552868,"er, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary. Next, we show how this complementary behavior can be used to our advantage. By using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers. Introduction Part of speech tagging has been a central problem in natural language processing for many years. Since the advent of manually tagged corpora such as the Brown Corpus and the Penn Treebank (Francis(1982), Marcus(1993)), the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques, including: Markov models, decision trees, connectionist machines, transformations, nearest-neighbor algorithms, and maximum entropy (Weischedel(1993), Black(1992), Schmid(1994), Brill(1995),Daelemans(1995),Ratnaparkhi(1996 )). All of these methods seem to achieve roughly comparable accuracy. The fact that most machine-learningbased taggers achieve comparable results could be attributed to a number of causes. It is possible that the 80/20 rule of engineering is applying: a certain n"
P98-1029,C94-1027,0,0.0171006,"Missing"
P98-1029,J93-2006,0,0.102481,"Missing"
P98-1029,W96-0213,0,\N,Missing
P98-1029,J95-4004,1,\N,Missing
P99-1009,P98-1010,0,0.0361009,"nnotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&M) as the machine learning system with which to compare the hu*and Woman. 65 of 500 rules. man learners. It is difficult to compare different machine learning approaches to base NP annotation, since different definiti"
P99-1009,C92-3150,0,0.075161,"from a small number of examples, it is possible that a person could derive effective linguistic knowledge from a much smaller training corpus than that needed by a machine. A person could also potentially learn more powerful representations than a machine, thereby achieving higher accuracy. In this paper we describe experiments we performed to ascertain how well humans, given an annotated training set, can generate rules for base noun phrase chunking. Much previous work has been done on this problem and many different methods have been used: Church's PARTS (1988) program uses a Markov model; Bourigault (1992) uses heuristics along with a grammar; Voutilainen's NPTool (1993) uses a lexicon combined with a constraint grammar; Juteson and Katz (1995) use repeated phrases; Veenstra (1998), Argamon, Dagan & Krymolowski(1998) and Daelemaus, van den Bosch & Zavrel (1999) use memory-based systems; Ramshaw & Marcus (In Press) and Cardie & Pierce (1998) use rule-based systems. A great deal of work has been done demonstrating the ability of machine learning algorithms to automatically extract linguistic knowledge from annotated corpora. Very little work has gone into quantifying the difference in ability at"
P99-1009,C94-2195,1,0.822308,"rated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&M) as the machine learning system with which to compare the hu*and Woman. 65 of 500 rul"
P99-1009,J95-4004,1,0.728777,"nto quantifying the difference in ability at this task between a person and a machine. This paper is a first step in that direction. 1 Introduction Machine learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based system"
P99-1009,P98-1034,0,0.70149,"ng linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&M) as the machine learning system with which to compare the hu*and Woman. 65 of 500 rules. man learners. It is difficult to compare different machine learning approaches to base"
P99-1009,A88-1019,0,0.135652,"Missing"
P99-1009,A97-1051,0,0.0246087,"learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo2 Learning Base Noun Phrases by Machine We used the"
P99-1009,H92-1045,0,0.0103223,"p in that direction. 1 Introduction Machine learning has been very successful at solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo2 Learning"
P99-1009,P98-2188,0,0.0156947,"solving many problems in the field of natural language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Ma"
P99-1009,J93-2004,0,0.0251771,"imple rules, it raises the question of whether peo2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&M) as the machine learning system with which to compare the hu*and Woman. 65 of 500 rules. man learners. It is difficult to compare different machine learning approaches to base NP annotation, since different definitions of base NP are used in many of the papers, but the R&M system is the best of those that have been tested on the Penn Treebank. 1 To train their system, R&M used a 200k-word chunk of the Penn Treebank Parsed Wall Street Journal (Marcus et al., 1993) tagged using a transformation-based tagger (Brill, 1995) and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics (like treating the possessive marker as the first word of a new base noun phrase) to flatten the recursive structure of the parse. They cast the problem as a transformation-based tagging problem, where each word is to be labelled with a chunk structure tag from the set {I, O, B}, where words marked 'T' are inside some base NP chunk, those marked &quot;O&quot; are not part of any base"
P99-1009,W98-1224,0,0.0593939,"Missing"
P99-1009,C96-1047,0,0.0296504,"ro or more adjectives (JJ, JJR, JJS), and one or more nouns (NN, NNP, NNS, NNPS), if they are followed by a verb (VB, VBD, VBG, VBN, VBP, VBZ). In our language, the rule is written thus: 3 A (* .) ({i} t=DT) (* t=JJ[RS]?) (+ t=NNP?S?) ({i} t=VB [DGNPZ] ?) The first line denotes the action, in this case, A d d a bracketing. The second line defines the context preceding the sequence we want to have bracketed - - in this case, we do not care what this sequence is. The third line defines the sequence which we want bracketed, and the last 2The rule types we have chosen are similar to those used by Vilain and Day (1996) in transformation-based parsing, b u t are more powerful. SA full description of the rule language can be found 1We would like to thank Lance Ramshaw for providing us with the base-NP-annotated training and test corpora that were used in the R&M system, as well as the rules learned by this system. at http://nlp, cs. jhu.edu/,~baseNP/manual. 6B The actual system is located at h t t p : / / n l p , cs. jhu. edu/~basenp/chunking. A screenshot of this system is shown in figure 4. The correct base NPs are enclosed in parentheses and those annotated by the human's rules in brackets. line defines th"
P99-1009,W93-0306,0,0.0748852,"Missing"
P99-1009,P94-1013,0,0.0110599,"ral language processing. It has been amply demonstrated that a wide assortment of machine learning algorithms are quite effective at extracting linguistic information from manually annotated corpora. Among the machine learning algorithms studied, rule based systems have proven effective on many natural language processing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Press; Cardie and Pierce, 1998; Veenstra, 1998; Argamon et al., 1998). Many of these rule based systems learn a short list of simple rules (typically on the order of 50-300) which are easily understood by humans. Since these rule-based systems achieve good performance while learning a small list of simple rules, it raises the question of whether peo2 Learning Base Noun Phrases by Machine We used the base noun phrase system of Ramshaw and Marcus (R&M) as the machine learning sy"
P99-1009,W94-0111,0,\N,Missing
P99-1009,C98-1034,0,\N,Missing
P99-1009,C98-2183,0,\N,Missing
W00-1301,J93-2004,0,0.0259844,"Missing"
W00-1301,W97-0309,0,0.0164793,"collocational information as well as the presence of a word within a fixed window of an ambiguity site. Indeed, one of the great insights in both speech recognition and natural language processing is the realization that fixed local cues provide a great deal of useful information. While the n-gram reins supreme in language modeling, there has been some interesting work done building language models based on linguistically richer features. Bahl, Brown et al. (1989) describe a language model that builds a decision tree that is allowed to ask questions about the history up to twenty words back. Saul and Pereira (1997) describe a language model that can in essence skip over uninformative words in the history. Della Pietra et al. (1994) discuss an approach to language modeling based on link grammars, where the model can look beyond the two previous words to condition on linguistically relevant words in the history. The language model described by Chelba and Jelinek (1998) similarly conditions on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords. Samuellson, Tapanainen et al. (1996) describe a method for learning a particular 'C Introduction Many natu"
W00-1301,J95-4004,1,0.368432,"Missing"
W00-1301,P98-1035,0,0.012408,"nteresting work done building language models based on linguistically richer features. Bahl, Brown et al. (1989) describe a language model that builds a decision tree that is allowed to ask questions about the history up to twenty words back. Saul and Pereira (1997) describe a language model that can in essence skip over uninformative words in the history. Della Pietra et al. (1994) discuss an approach to language modeling based on link grammars, where the model can look beyond the two previous words to condition on linguistically relevant words in the history. The language model described by Chelba and Jelinek (1998) similarly conditions on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords. Samuellson, Tapanainen et al. (1996) describe a method for learning a particular 'C Introduction Many natural language tasks are essentially nway classification problems, where classification decisions are made from a small set of choices, based upon the linguistic context in which the ambiguity site occurs. Examples of such tasks include: confusable word set disambiguation; word sense disambiguation; determining such lexical features as pronoun case and determ"
W00-1301,A00-2017,0,0.0126355,"of the target, and collocations based on the words and part of speech tags of up to two words to the left and two words to the fight of the target. Below we present a machine learning algorithm that learns from a much richer feature set than that typically used for classification in 1 useful type of pattern, which they call a barrier. Given two symbols X and Y, and a set of symbols S, they learn conditions of the form: take an action if there is an X preceded by a Y, with no intervening symbols from S. In their paper they demonstrate how such patterns can be useful for part of speech tagging. Even-Zohar and Roth (2000) show that by including linguistic features based on relations such as subject and object, they can better disambiguate between verb pairs. 2 (2) . is an RRE denoting the set E (3) .+ is an RRE denoting the positive closure of the set E (4) .* is an RRE denoting the Kleene closure of the set (5) if r and s are RREs denoting the languages R and S, respectively, then rs is an RRE denoting the set RS. Some examples of strings that are regular expressions but not reduced regular expressions include: (ab)*, a(b]c)d, (a (bc)+)* Next, we need some definitions to allow us to make reference to particul"
W00-1301,C98-1035,0,\N,Missing
W02-1033,A00-1041,0,0.00727631,"Missing"
W04-3238,P00-1037,1,0.741619,"Missing"
W04-3238,W95-0104,0,0.151052,"Missing"
W04-3238,C90-2036,0,0.900773,"Missing"
W04-3238,P02-1019,0,0.490969,"Missing"
W95-0101,H92-1022,1,0.904051,"Missing"
W95-0101,P93-1035,1,0.898193,"Missing"
W95-0101,1993.iwpt-1.3,1,0.848277,"Missing"
W95-0101,A88-1019,0,0.408347,"Missing"
W95-0101,A92-1018,0,0.400756,"Missing"
W95-0101,P90-1031,0,0.0437113,"Missing"
W95-0101,J88-1003,0,0.280307,"Missing"
W95-0101,A94-1009,0,0.0987745,"Missing"
W95-0101,P89-1015,0,0.144051,"Missing"
W95-0101,W94-0111,0,0.0753578,"Missing"
W95-0101,J95-2004,0,0.0227157,"Missing"
W95-0101,P94-1025,0,0.0246986,"Missing"
W95-0101,J93-2006,0,0.0611644,"Missing"
W99-0623,W98-1118,0,0.0113247,"d: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers. The theory has also been validated empirically. Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998). The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997). These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). We used these three parsers to explore parser combination techniques. 2 Techniques for Combining Parsers 2.1 Parse Hybridization We are interested in combining the substructures of the input parses to produce a better parse. We call this approach parse hybridization. The sub"
W99-0623,P98-1029,1,0.667685,"position of having many available approaches to solving some of its most fundamental problems. The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992; Heath et al., 1996). Their theoreticalIfinding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers. The theory has also been validated empirically. Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998). The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997). These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (M"
W99-0623,P97-1003,0,0.00928142,"dated empirically. Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998). The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997). These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). We used these three parsers to explore parser combination techniques. 2 Techniques for Combining Parsers 2.1 Parse Hybridization We are interested in combining the substructures of the input parses to produce a better parse. We call this approach parse hybridization. The substructures that are unanimously hypothesized by the parsers should be preserved after combination, and the combination technique should not foolish"
W99-0623,A94-1016,0,0.0320081,"ltiple classifiers (Wolpert, 1992; Heath et al., 1996). Their theoreticalIfinding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers. The theory has also been validated empirically. Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998). The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997). These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). We used these three parsers to explore parser combination techniques. 2 Techniques for Combining Parsers 2.1 Parse Hybridization We are interested in combining the substructures of the i"
W99-0623,J93-2004,0,0.0348776,"). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998). The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997). These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). We used these three parsers to explore parser combination techniques. 2 Techniques for Combining Parsers 2.1 Parse Hybridization We are interested in combining the substructures of the input parses to produce a better parse. We call this approach parse hybridization. The substructures that are unanimously hypothesized by the parsers should be preserved after combination, and the combination technique should not foolishly create substructures for which there is no supporting evidence. These two principles guide experimentation in this framework, and together with the evaluation measures help"
W99-0623,W97-0301,0,0.00890138,"tion techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998). In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998). The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997). These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). We used these three parsers to explore parser combination techniques. 2 Techniques for Combining Parsers 2.1 Parse Hybridization We are interested in combining the substructures of the input parses to produce a better parse. We call this approach parse hybridization. The substructures that are unanimously hypothesized by the parsers should be preserved after combination, and the combination technique should not foolishly create substructures for which there"
