2021.deelio-1.3,D13-1160,0,0.0649286,"iever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work done at Google Research 25 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 25–30 Online, June 10, 2021. ©2021 Association for Computational"
2021.deelio-1.3,D14-1067,0,0.0270901,"nto text-based QA systems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work done at Google Research 25 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration f"
2021.deelio-1.3,P17-1171,0,0.0187296,"aseline setting. Variants of PPR do not improve over the text only baseline. BM25 PPR(Q) QI PPR(Q) WS PPR(Q) Shortest Path Fact R Ans R 19.1 33.0 31.2 51.0 29.8 28.8 25.2 40.0 DecAtt + DocReader BERTjoint BERTjoint * Traditional PPR QI PPR WS PPR Table 2: Answer Recall and Shortest Path Fact Recall metrics for the different Retrieval Methods. Traditional and QI PPR methods have very low recall and WS PPR method improves the recall significantly. Short F1 54.8 64.7 68.1 66.7 65.8 67.5 31.4 52.7 54.0 54.3 53.6 54.4 Table 3: Results on Full NQ. Baselines: DecAtt (Parikh et al., 2016), DocReader (Chen et al., 2017), and BertJoint (Alberti et al., 2019). *- our reimplementation. WS PPR improves over previous baseline on Short F1 and has comparable performance to BertJoint on Long F1. QA model in place of the KB retrieved facts. As the oracle setting uses gold KB links, this setting is tested on the FQ subset where such links exist and is called the Clean Oracle. To establish a harder upper bound setting, random facts about the question are added in addition to the oracle shortest path facts using PPR, forming a Noisy Oracle setting. 3.2 Long F1 contain the answer, we simply set the answer span to be the"
2021.deelio-1.3,N19-1423,0,0.0609056,"Missing"
2021.deelio-1.3,D19-1242,1,0.891701,"time. As a consequence injecting retrieved KB paths in a realistic QA setting like NQ yields only small, inconsistent improvements. To summarize our contributions, we (1) identify a new experimental subset of NQ that supports (2) the study of effectiveness of KB path-retrieval approaches. We also (3) describe a simple, modelagnostic method to using oracle KB paths that can significantly improve QA performance and evaluate PPR based path-retrieval methods. To our Existing work shows the benefits of integrating KBs with textual evidence for QA only on questions that are answerable by KBs alone (Sun et al., 2019). In contrast, real world QA systems often have to deal with questions that might not be directly answerable by KBs. Here, we investigate the effect of integrating background knowledge from KBs for the Natural Questions (NQ) task. We create a subset of the NQ data, Factual Questions (FQ), where the questions have evidence in the KB in the form of paths that link question entities to answer entities but still must be answered using text, to facilitate further research into KB integration methods. We propose and analyze a simple, model-agnostic approach for incorporating KB paths into text-based"
2021.deelio-1.3,D18-1455,1,0.870826,"rther research into KB integration methods. We propose and analyze a simple, model-agnostic approach for incorporating KB paths into text-based QA systems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work don"
2021.deelio-1.3,Q19-1026,1,0.838535,"ntities (nodes) w.r.t seed entities. Sun et al. (2018) present an improved PPR version, Question Informed (QI) PPR, to weigh relations which are semantically closer to the question higher. Specifically, they average the GLOVE (Pennington et al., 2014) embeddings to compute a relation vector v(R) from the relation surface form, and a question vector v(Q) from the question text, and use cosine similarity between them as edgeweights for PPR. For every node, the γ probability is multiplied by the edge-score to weigh entities along relevant paths higher. Dataset The Natural Questions (NQ) dataset (Kwiatkowski et al., 2019) is a large scale QA dataset containing 307,373 training, 7,830 dev, and 7,842 test examples. Each example is a user query paired with Wikipedia documents annotated with a passage (long answer) answering the question and one or more short spans (short answer) containing the answer. The questions in NQ are not artificially constructed, making the NQ task more difficult (Lee et al., 2019). We use Sling (Ringgaard et al., 2017) (which uses an NP chunker and phrase table for linking entities to Wikidata) to entity link the questions and documents. To focus on knowledge-driven factoid question answ"
2021.deelio-1.3,P19-1219,0,0.0169232,"o KB integration methods. We propose and analyze a simple, model-agnostic approach for incorporating KB paths into text-based QA systems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work done at Google Research 2"
2021.deelio-1.3,2020.deelio-1.5,0,0.0402008,"Missing"
2021.deelio-1.3,P19-1612,0,0.0165772,"milarity between them as edgeweights for PPR. For every node, the γ probability is multiplied by the edge-score to weigh entities along relevant paths higher. Dataset The Natural Questions (NQ) dataset (Kwiatkowski et al., 2019) is a large scale QA dataset containing 307,373 training, 7,830 dev, and 7,842 test examples. Each example is a user query paired with Wikipedia documents annotated with a passage (long answer) answering the question and one or more short spans (short answer) containing the answer. The questions in NQ are not artificially constructed, making the NQ task more difficult (Lee et al., 2019). We use Sling (Ringgaard et al., 2017) (which uses an NP chunker and phrase table for linking entities to Wikidata) to entity link the questions and documents. To focus on knowledge-driven factoid question answering, we create a subset of NQ having relevant knowledge in the KB. Shortest paths between entities in KB is very often used as a proxy for gold knowledge linking questions to answer (Sun et al., 2019) and we use the same proxy in our setting. Specifically, we select questions whose short answers are entities in the KB and have a short path (up to 3 steps) from a question entity to an"
2021.deelio-1.3,D18-1242,0,0.0197972,"tems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets1 . 1 Introduction Prior work has shown the benefit of retrieving paths of related entities (Sun et al., 2018; Wang and Jiang, 2019; Sun et al., 2019) and learning relevant knowledge graph embeddings (Sawant et al., 2018; Bordes et al., 2014; Luo et al., 2018) for answering questions on KBQA datasets such as WebQuestions (Berant et al., 2013) and MetaQA (Zhang et al., 2018). But such datasets are often curated to questions with KB paths that contain the right path to the answer and hence are directly answerable via KB. An open question remains whether such approaches are useful for questions not specifically 1 Data and Code available at: https://github.com/ vidhishanair/fact_augmented_text ∗ Work done at Google Research 25 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Ar"
2021.deelio-1.3,D16-1244,0,0.103473,"Missing"
2021.deelio-1.3,D14-1162,0,0.0842017,"Missing"
2021.emnlp-main.600,2020.emnlp-main.19,0,0.501346,". We improve the state-of-the-art by 19 points. All the code is available as open source.3 2 Related Work Transformers for tabular data Traditionally, tasks involving tables were tackled by searching for logical forms in a semantic parsing setting. More recently Transformers (Vaswani et al., 2017) have been used to train end-to-end models on tabular data as well (Chen et al., 2020a). For example, TAPAS (Herzig et al., 2020) relies on Transformerbased masked language model pre-training and special row and column embeddings to encode the table structure. Chen et al. (2021) use a variant of ETC (Ainslie et al., 2020) on an open-domain version of H YBRID QA to read and choose an answer span from multiple candidate passages and cells, but the proposed model does not jointly process the 3 github.com/google-research/tapas full table with passages. In order to overcome the limitations on sequence length Eisenschlos et al. (2020) propose heuristic column selection techniques, and they also propose pre-training on synthetic data. Krichene et al. (2021) propose a model based cell selection technique that is differentiable and trained end-to-end together with the main task model. Our approach is orthogonal to thes"
2021.emnlp-main.600,D19-1609,0,0.0524526,"Missing"
2021.emnlp-main.600,D13-1160,0,0.0601556,"]) S(c) = avgt∈c S(t) exp(S(c)) P (c) = P 0 c0 ∈e exp(S(c )) We use cross entropy loss for training the model to select expanded cells that contain the answer span. Even though the correct span may appear in multiple cells or passages, in practice many of these do so only by chance and do not correspond to a reasoning path consistent with the question asked. In Figure 3 for instance, there could be other British divers but we are only interested in selecting the cell marked with a star symbol (?). In order to handle these cases we rely on Maximum Marginal Likelihood (MML) (Liang et al., 2013; Berant et al., 2013). As shown by Guu et al. (2017) MML can be interpreted as using the online model predictions (without gradients) to compute a soft label distribution over candidates. For an input query x, and a set C of candidate cells, the loss is: X L(Θ, x, C) = −q(z) log pΘ (z|x) z∈C with q(z) = pΘ (z|x, z ∈ C) the probability distribution given by the model restricted to candidate cells containing the answer span, taken here as a constant with zero gradient. and score it using a multi-layer perceptron: hstart = BERTr (q, c)[START(s)] hend = BERTr (q, c)[END(s)] Sread (q, c) = MLP([hstart , hend ]) A softm"
2021.emnlp-main.600,P17-1171,0,0.0240525,"on model that reads the answer from a single expanded stage, equivalent to our final stage. The first three cell selected by the P OINT R Cell Selector. In order stages are equivalent to our single cell selection to construct the expanded cell for each example, we stage; hence, we use their reported error rates to concatenate the cell content with all the sentences estimate the retrieval rate. The simpler approach of the linked entities and keep the first 512 tokens. enabled by M ATE avoids error propagation and Following various recent neural machine read- yields improved results. ing works (Chen et al., 2017; Lee et al., 2019; We also consider two recent efficient transformer Herzig et al., 2021), we fine-tune a pre-trained architectures as alternatives for the P OINT R Cell B ERT-uncased-large model (Devlin et al., 2019) Selector, one based on L INFORMER (Wang et al., that attempts to predict a text span from the text 2020) and one based on ETC (Ainslie et al., 2020). in a given table cell c (and its linked paragraphs) In both cases we preserve the row, column and rank and the input query q. We compute a span rep- embeddings introduced by Herzig et al. (2020). resentation as the concatenation of"
2021.emnlp-main.600,P19-1285,0,0.0684359,"Missing"
2021.emnlp-main.600,2020.emnlp-main.692,0,0.0289594,"Missing"
2021.emnlp-main.600,N19-1423,0,0.0311155,"construct the expanded cell for each example, we stage; hence, we use their reported error rates to concatenate the cell content with all the sentences estimate the retrieval rate. The simpler approach of the linked entities and keep the first 512 tokens. enabled by M ATE avoids error propagation and Following various recent neural machine read- yields improved results. ing works (Chen et al., 2017; Lee et al., 2019; We also consider two recent efficient transformer Herzig et al., 2021), we fine-tune a pre-trained architectures as alternatives for the P OINT R Cell B ERT-uncased-large model (Devlin et al., 2019) Selector, one based on L INFORMER (Wang et al., that attempts to predict a text span from the text 2020) and one based on ETC (Ainslie et al., 2020). in a given table cell c (and its linked paragraphs) In both cases we preserve the row, column and rank and the input query q. We compute a span rep- embeddings introduced by Herzig et al. (2020). resentation as the concatenation of the contextual L INFORMER learns a projection matrix that reembeddings of the first and last token in a span s duces the sequence length dimension of the keys 7611 and values tensor to a fixed length of 256 (which per"
2021.emnlp-main.600,2020.findings-emnlp.27,1,0.93132,"used to train end-to-end models on tabular data as well (Chen et al., 2020a). For example, TAPAS (Herzig et al., 2020) relies on Transformerbased masked language model pre-training and special row and column embeddings to encode the table structure. Chen et al. (2021) use a variant of ETC (Ainslie et al., 2020) on an open-domain version of H YBRID QA to read and choose an answer span from multiple candidate passages and cells, but the proposed model does not jointly process the 3 github.com/google-research/tapas full table with passages. In order to overcome the limitations on sequence length Eisenschlos et al. (2020) propose heuristic column selection techniques, and they also propose pre-training on synthetic data. Krichene et al. (2021) propose a model based cell selection technique that is differentiable and trained end-to-end together with the main task model. Our approach is orthogonal to these methods, and can be usefully combined with them, as shown in Table 4. Recently, Zhang et al. (2020) proposed SAT, which uses an attention mask to restrict attention to tokens in the same row and same column. SAT also computes an additional histogram row appended at the bottom of the table and encodes the table"
2021.emnlp-main.600,P17-1097,0,0.0121846,"(c) = P 0 c0 ∈e exp(S(c )) We use cross entropy loss for training the model to select expanded cells that contain the answer span. Even though the correct span may appear in multiple cells or passages, in practice many of these do so only by chance and do not correspond to a reasoning path consistent with the question asked. In Figure 3 for instance, there could be other British divers but we are only interested in selecting the cell marked with a star symbol (?). In order to handle these cases we rely on Maximum Marginal Likelihood (MML) (Liang et al., 2013; Berant et al., 2013). As shown by Guu et al. (2017) MML can be interpreted as using the online model predictions (without gradients) to compute a soft label distribution over candidates. For an input query x, and a set C of candidate cells, the loss is: X L(Θ, x, C) = −q(z) log pΘ (z|x) z∈C with q(z) = pΘ (z|x, z ∈ C) the probability distribution given by the model restricted to candidate cells containing the answer span, taken here as a constant with zero gradient. and score it using a multi-layer perceptron: hstart = BERTr (q, c)[START(s)] hend = BERTr (q, c)[END(s)] Sread (q, c) = MLP([hstart , hend ]) A softmax is computed over valid spans"
2021.emnlp-main.600,2021.naacl-main.43,1,0.795362,"age. The first three cell selected by the P OINT R Cell Selector. In order stages are equivalent to our single cell selection to construct the expanded cell for each example, we stage; hence, we use their reported error rates to concatenate the cell content with all the sentences estimate the retrieval rate. The simpler approach of the linked entities and keep the first 512 tokens. enabled by M ATE avoids error propagation and Following various recent neural machine read- yields improved results. ing works (Chen et al., 2017; Lee et al., 2019; We also consider two recent efficient transformer Herzig et al., 2021), we fine-tune a pre-trained architectures as alternatives for the P OINT R Cell B ERT-uncased-large model (Devlin et al., 2019) Selector, one based on L INFORMER (Wang et al., that attempts to predict a text span from the text 2020) and one based on ETC (Ainslie et al., 2020). in a given table cell c (and its linked paragraphs) In both cases we preserve the row, column and rank and the input query q. We compute a span rep- embeddings introduced by Herzig et al. (2020). resentation as the concatenation of the contextual L INFORMER learns a projection matrix that reembeddings of the first and l"
2021.emnlp-main.600,2020.acl-main.398,1,0.835879,"We propose P OINT R (Section 4), a novel two-phase framework that exploits M ATE to tackle large-scale QA tasks, like H YBRID QA, that require multi-hop reasoning over tabular and textual data. We improve the state-of-the-art by 19 points. All the code is available as open source.3 2 Related Work Transformers for tabular data Traditionally, tasks involving tables were tackled by searching for logical forms in a semantic parsing setting. More recently Transformers (Vaswani et al., 2017) have been used to train end-to-end models on tabular data as well (Chen et al., 2020a). For example, TAPAS (Herzig et al., 2020) relies on Transformerbased masked language model pre-training and special row and column embeddings to encode the table structure. Chen et al. (2021) use a variant of ETC (Ainslie et al., 2020) on an open-domain version of H YBRID QA to read and choose an answer span from multiple candidate passages and cells, but the proposed model does not jointly process the 3 github.com/google-research/tapas full table with passages. In order to overcome the limitations on sequence length Eisenschlos et al. (2020) propose heuristic column selection techniques, and they also propose pre-training on synthet"
2021.emnlp-main.600,P17-1167,0,0.062089,"Missing"
2021.emnlp-main.600,N19-1357,0,0.0237727,"9 86.6 Table 6: Retrieval results over H YBRID QA (dev set) for models used in P OINT R Cell Selection stage. Efficient transformer models are grouped together. HYBRIDER results are obtained from Chen et al. (2020b) by composing the errors for the first components. Observed Attention Sparsity Since we are interested to motivate our choices on how to sparsify the attention matrix, we can inspect the magnitude of attention connections in a trained dense TAPAS model for table question answering. It is important to note that in this context we are not measuring attention as an explanation method (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Instead we are treating the attention matrix in the fashion of magnitude based pruning techniques (Han et al., 2015; Ablation Study In Table 6 we compare architec- See et al., 2016), and simply consider between tures for cell selection on H YBRID QA. Hits@k which pairs of tokens the scores are concentrated. corresponds to whether a cell containing an answer Given a token in the input we can aggregate the span was among the top-k retrieved candidates. As attention weights flowing from it depending on an ablation, we remove the sparse pre-training and the position"
2021.emnlp-main.600,2021.findings-acl.289,1,0.77507,"n Transformerbased masked language model pre-training and special row and column embeddings to encode the table structure. Chen et al. (2021) use a variant of ETC (Ainslie et al., 2020) on an open-domain version of H YBRID QA to read and choose an answer span from multiple candidate passages and cells, but the proposed model does not jointly process the 3 github.com/google-research/tapas full table with passages. In order to overcome the limitations on sequence length Eisenschlos et al. (2020) propose heuristic column selection techniques, and they also propose pre-training on synthetic data. Krichene et al. (2021) propose a model based cell selection technique that is differentiable and trained end-to-end together with the main task model. Our approach is orthogonal to these methods, and can be usefully combined with them, as shown in Table 4. Recently, Zhang et al. (2020) proposed SAT, which uses an attention mask to restrict attention to tokens in the same row and same column. SAT also computes an additional histogram row appended at the bottom of the table and encodes the table content as text only (unlike TAPAS). The proposed method is not head dependent as ours is, which prevents it from being imp"
2021.emnlp-main.600,Q19-1026,0,0.0151573,"at every layer in such a way that similar contextual embeddings have a higher chance of being clustered together. We instead rely on the input data structure to define ways to cluster the tokens. Although the limitation can be circumvented by adapting the proposed architecture, R EFORMER was originally defined for auto-regressive training. Ainslie et al. (2020) introduce ETC, a framework for global memory and local sparse attention, and use the mechanism of relative positional attention (Dai et al., 2019) to encode hierarchy. ETC was applied to large document tasks such as Natural Questions (Kwiatkowski et al., 2019). The method does not allow different dynamic or static data re-ordering. In practice, we have observed that the use of relative positional attention introduces a large overhead during training. B IG B IRD (Zaheer et al., 2020) presents a similar approach with the addition of attention to random tokens. 3 The M ATE model table, we propose having some attention heads limited to attending between tokens in the same row (plus the non-table tokens), and likewise for columns. We call these row heads and column heads respectively. In both cases, we allow attention to and from all the non-table token"
2021.emnlp-main.600,P19-1612,0,0.0127467,"the answer from a single expanded stage, equivalent to our final stage. The first three cell selected by the P OINT R Cell Selector. In order stages are equivalent to our single cell selection to construct the expanded cell for each example, we stage; hence, we use their reported error rates to concatenate the cell content with all the sentences estimate the retrieval rate. The simpler approach of the linked entities and keep the first 512 tokens. enabled by M ATE avoids error propagation and Following various recent neural machine read- yields improved results. ing works (Chen et al., 2017; Lee et al., 2019; We also consider two recent efficient transformer Herzig et al., 2021), we fine-tune a pre-trained architectures as alternatives for the P OINT R Cell B ERT-uncased-large model (Devlin et al., 2019) Selector, one based on L INFORMER (Wang et al., that attempts to predict a text span from the text 2020) and one based on ETC (Ainslie et al., 2020). in a given table cell c (and its linked paragraphs) In both cases we preserve the row, column and rank and the input query q. We compute a span rep- embeddings introduced by Herzig et al. (2020). resentation as the concatenation of the contextual L"
2021.emnlp-main.600,J13-2005,0,0.0140491,") = MLP(MATE(q, e)[t]) S(c) = avgt∈c S(t) exp(S(c)) P (c) = P 0 c0 ∈e exp(S(c )) We use cross entropy loss for training the model to select expanded cells that contain the answer span. Even though the correct span may appear in multiple cells or passages, in practice many of these do so only by chance and do not correspond to a reasoning path consistent with the question asked. In Figure 3 for instance, there could be other British divers but we are only interested in selecting the cell marked with a star symbol (?). In order to handle these cases we rely on Maximum Marginal Likelihood (MML) (Liang et al., 2013; Berant et al., 2013). As shown by Guu et al. (2017) MML can be interpreted as using the online model predictions (without gradients) to compute a soft label distribution over candidates. For an input query x, and a set C of candidate cells, the loss is: X L(Θ, x, C) = −q(z) log pΘ (z|x) z∈C with q(z) = pΘ (z|x, z ∈ C) the probability distribution given by the model restricted to candidate cells containing the answer span, taken here as a constant with zero gradient. and score it using a multi-layer perceptron: hstart = BERTr (q, c)[START(s)] hend = BERTr (q, c)[END(s)] Sread (q, c) = MLP([hs"
2021.emnlp-main.600,P15-1142,0,0.0284325,". We will refer to this model as TABLE E TC. Finally we consider two non-efficient models: A simple TAPAS model without any sparse mask, and an SAT (Zhang et al., 2020) model pretrained on the same MLM task as TAPAS for a fair comparison. For the cell selection task TAPAS obtains similar results to M ATE, but both TAPAS and SAT lack the efficiency improvements of M ATE. 5.2 Other datasets We also apply M ATE to three other datasets involving tables to demonstrate that the sparse attention bias yields stronger table reasoning models. SQA (Iyyer et al., 2017) is a sequential QA task, W IK I TQ (Pasupat and Liang, 2015) is a QA task that sometimes also requires aggregation of table cells, and TAB FACT (Chen et al., 2020a) is a binary entailment task. See Table 3 for dataset statistics. We evaluate with and without using the intermediate pre-training tasks (CS) (Eisenschlos et al., 2020). 6 Results Figure 4: Comparison of inference speed on a cloud VM with 64GB. At a sequence length of 2048, M ATE is nearly twice as fast as TAPAS. Model SQA ALL SQA SEQ W IKI TQ TAB FACT TAPAS M ATE 67.2 ±0.5 71.6 ±0.1 40.4 ±0.9 46.4 ±0.3 42.6 ±0.8 42.8 ±0.8 76.3 ±0.2 77.0 ±0.3 TAPAS + CS M ATE + CS 71.0 ±0.4 71.7 ±0.4 44.8 ±0"
2021.emnlp-main.600,K16-1029,0,0.0657382,"Missing"
2021.emnlp-main.600,D19-1002,0,0.0216983,"l results over H YBRID QA (dev set) for models used in P OINT R Cell Selection stage. Efficient transformer models are grouped together. HYBRIDER results are obtained from Chen et al. (2020b) by composing the errors for the first components. Observed Attention Sparsity Since we are interested to motivate our choices on how to sparsify the attention matrix, we can inspect the magnitude of attention connections in a trained dense TAPAS model for table question answering. It is important to note that in this context we are not measuring attention as an explanation method (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Instead we are treating the attention matrix in the fashion of magnitude based pruning techniques (Han et al., 2015; Ablation Study In Table 6 we compare architec- See et al., 2016), and simply consider between tures for cell selection on H YBRID QA. Hits@k which pairs of tokens the scores are concentrated. corresponds to whether a cell containing an answer Given a token in the input we can aggregate the span was among the top-k retrieved candidates. As attention weights flowing from it depending on an ablation, we remove the sparse pre-training and the position of the target token in the in"
2021.emnlp-main.600,2020.acl-main.745,0,0.0189472,"ated with half the interquartile range over 5 runs. 2.5 points, for both exact-match accuracy and F1. We also applied M ATE to three tasks involving table reasoning over shorter sequences. In Table 4 we see that M ATE provides improvements in accuracy, which we attribute to a better inductive bias for tabular data. When combining M ATE with Counterfactual + Synthetic intermediate pretraining (CS) (Eisenschlos et al., 2020) we often get even better results. For TAB FACT and SQA we improve over the previous state-of-the-art. For W IKI TQ we close the gap with the best published system TA B ERT (Yin et al., 2020) (51.8 mean test accuracy), which relies on traditional semantic parsing, instead of an end-to-end approach. Dev results show a similar trend and can be found in Appendix B. No special tuning was done on these models—we used the same hyper-parameters as the open source release of TAPAS. In Figure 4 we compare inference speed of different models as we increase the sequence length. Similar results showing number of FLOPS and memory usage are in Appendix A. The linear scaling of L INFORMER and the linear-time version M ATE can be seen clearly. Although L INFORMER has a slightly smaller linear con"
2021.emnlp-main.600,2020.emnlp-main.126,0,0.0736888,"Missing"
2021.naacl-main.288,N19-1423,0,0.576799,"re each element is derived from a triple of vectors, representing a KB entity or relation. Since these triples are defined compositionally from (representations of) entities and relations, they have an interpretable symbolic meaning: e.g., if emtv is the vector representation of KB entity “Mountain View, CA” and egoogle and rhq similarly correspond to “Google Inc” and the relation “headquartered in”, these vectors can be used to construct a memory element f (egoogle , rhq , emtv ) for the KB assertion “Google, Inc is headquartered in Mountain Neural language models (LMs) (Peters et al., 2018; Devlin et al., 2019; Raffel et al., 2019) that have been pre-trained by self-supervision on large corpora contain rich knowledge about the syntax and semantics of natural language (Tenney et al., 2019), and are the basis of much recent work in NLP. Pretrained LMs also contain large amounts of factual knowledge about the world (Petroni et al., 2019; Roberts et al., 2020; Brown et al., 2020). However, while large LMs can be coerced to answer factual queries, they still lack many of the properties that knowledge bases (KBs) typically have. In particular, it is difficult to distinguish answers produced by memorizing"
2021.naacl-main.288,2020.emnlp-main.400,1,0.819266,"ines on several benchmark open-domain QA datasets, and dramatically if test-train overlap in the datasets are removed. 3. We show FILM can easily adapt to newly injected and modified facts without retraining. 2 Fact Injected Language Model Model The Fact Injected Language Model (FILM) model (see Figure 1) extends the Transformer (Vaswani et al., 2017) architecture of BERT (Devlin et al., 2019) with additional entity and facts memories. These memories store semantic information which can later be retrieved and incorporated into the representations of the transformer. Similar to the approach in Févry et al. (2020), entity embeddings will (ideally) store information about the textual contexts in which that entity appears, and by inference, the entity’s semantic properties. The fact memory encodes triples from a symbolic KB, constructed compositionally from the learned embeddings of the entities that comprise it and implemented as a key-value memory which is used to 1. We propose a neural LM for knowledge- retrieve entities given their KB properties. This intensive question-answering tasks that incor- combination results in a neural LM which learns to 3679 access information from a symbolic KB. 2.1 Defin"
2021.naacl-main.288,N19-1028,0,0.174814,"dataset containing 4737 natural language questions linked to corresponding Freebase entities and relations (Yih et al., 2015) derived from WebQuestions(Berant et al., 2013). LAMA TREx is a set of fact-related cloze questions. Since we are interested in entity prediction models, we restrict our LAMA investigations to TREx, which has answers linked to Wikidata. TriviaQA (open) contains questions scraped from quiz-league websites (Joshi et al., 2017). We use the open splits following Lee et al. (2019). FreebaseQA is an Open-domain QA dataset derived from TriviaQA and other trivia resources (See Jiang et al. (2019) for full details). Every answer can be resolved to at least one Freebase entity and each question contains at least one entity. 3.2 Baselines T5 (Raffel et al., 2019) and BART (Lewis et al., 2019) are large text-to-text transformers. Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) is a two stage retrieve and read model. Retrieval Augmented Generation (RAG) (Lewis et al., 2020a) and Fusion in Decoder (FID) (Izacard and Grave, 2020) use DPR retrieval, followed 7 All data is English. See A.1 for additional details. by generative decoders based on BART and T5 respectively. FID is the curre"
2021.naacl-main.288,P17-1147,0,0.0297788,"3.4) and examples filtered for train/test overlap (§3.5). 3.1 Data We evaluate on four knowledge intensive tasks7 . WebQuestionsSP is an Open-domain Question Answering dataset containing 4737 natural language questions linked to corresponding Freebase entities and relations (Yih et al., 2015) derived from WebQuestions(Berant et al., 2013). LAMA TREx is a set of fact-related cloze questions. Since we are interested in entity prediction models, we restrict our LAMA investigations to TREx, which has answers linked to Wikidata. TriviaQA (open) contains questions scraped from quiz-league websites (Joshi et al., 2017). We use the open splits following Lee et al. (2019). FreebaseQA is an Open-domain QA dataset derived from TriviaQA and other trivia resources (See Jiang et al. (2019) for full details). Every answer can be resolved to at least one Freebase entity and each question contains at least one entity. 3.2 Baselines T5 (Raffel et al., 2019) and BART (Lewis et al., 2019) are large text-to-text transformers. Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) is a two stage retrieve and read model. Retrieval Augmented Generation (RAG) (Lewis et al., 2020a) and Fusion in Decoder (FID) (Izacard and Gra"
2021.naacl-main.288,2020.emnlp-main.550,0,0.0251032,"Missing"
2021.naacl-main.288,2020.findings-emnlp.307,0,0.0285669,"Missing"
2021.naacl-main.288,P19-1612,0,0.0199487,"5). 3.1 Data We evaluate on four knowledge intensive tasks7 . WebQuestionsSP is an Open-domain Question Answering dataset containing 4737 natural language questions linked to corresponding Freebase entities and relations (Yih et al., 2015) derived from WebQuestions(Berant et al., 2013). LAMA TREx is a set of fact-related cloze questions. Since we are interested in entity prediction models, we restrict our LAMA investigations to TREx, which has answers linked to Wikidata. TriviaQA (open) contains questions scraped from quiz-league websites (Joshi et al., 2017). We use the open splits following Lee et al. (2019). FreebaseQA is an Open-domain QA dataset derived from TriviaQA and other trivia resources (See Jiang et al. (2019) for full details). Every answer can be resolved to at least one Freebase entity and each question contains at least one entity. 3.2 Baselines T5 (Raffel et al., 2019) and BART (Lewis et al., 2019) are large text-to-text transformers. Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) is a two stage retrieve and read model. Retrieval Augmented Generation (RAG) (Lewis et al., 2020a) and Fusion in Decoder (FID) (Izacard and Grave, 2020) use DPR retrieval, followed 7 All data is"
2021.naacl-main.288,2020.acl-main.703,0,0.0420911,"Missing"
2021.naacl-main.288,P19-1598,0,0.0584413,"iques to improve the performance of large LMs in answering factual probes, by adding additional supervision in pre-training (Xiong et al., 2019; Wang et al., 2020b) or by adding entity embeddings into an extended LM (Peters et al., 2019; Zhang et al., 2019; Févry et al., 2020). Our entity memory extends the Entities-asExperts (EaE) model (Févry et al., 2020). It is both the current state-of-the-art for a number of tasks and simpler to use than most prior models because it does not require external components for entity linking or entity encoding (like (Peters et al., 2019; Zhang et al., 2019; Logan et al., 2019)) and is not restricted to lexical KBs like WordNet and ConceptNet (like (Weissenborn et al., 2017; Chen et al., 2018; Mihaylov and Frank, 2018)). with millions of entities, whereas prior systems that make use of KB triples have been with only a few hundreds of triples in the model at any point, necessitating a separate heuristic process to retrieve candidate KB triples (Ahn et al., 2016; Henaff et al., 2016; Weissenborn et al., 2017; Chen et al., 2018; Mihaylov and Frank, 2018; Logan et al., 2019). There have been a few exploratory experiments on modifying the predictions of retrieval augment"
2021.naacl-main.288,P18-1076,0,0.0195205,"2019; Wang et al., 2020b) or by adding entity embeddings into an extended LM (Peters et al., 2019; Zhang et al., 2019; Févry et al., 2020). Our entity memory extends the Entities-asExperts (EaE) model (Févry et al., 2020). It is both the current state-of-the-art for a number of tasks and simpler to use than most prior models because it does not require external components for entity linking or entity encoding (like (Peters et al., 2019; Zhang et al., 2019; Logan et al., 2019)) and is not restricted to lexical KBs like WordNet and ConceptNet (like (Weissenborn et al., 2017; Chen et al., 2018; Mihaylov and Frank, 2018)). with millions of entities, whereas prior systems that make use of KB triples have been with only a few hundreds of triples in the model at any point, necessitating a separate heuristic process to retrieve candidate KB triples (Ahn et al., 2016; Henaff et al., 2016; Weissenborn et al., 2017; Chen et al., 2018; Mihaylov and Frank, 2018; Logan et al., 2019). There have been a few exploratory experiments on modifying the predictions of retrieval augmented language models by changing the underlying text corpus (Guu et al., 2020; Lewis et al., 2020a). However, text passages are not easily interpr"
2021.naacl-main.288,D16-1147,0,0.0251039,"et al., 2020; Cohen et al., 2020). Our fact memory builds on this prior work, and is most closely related to the memory used in EmQL (Sun et al., 2020), one KB embedding model that supports compositional query language. EmQL implements “projection” using neural retrieval over vectorized KB triples. Unlike this work, however, EmQL did not embed its fact memory into a LM, which could be finetuned for many NLP tasks: instead requiring the implementation of a “neural module” into some task-specific architecture. At a more abstract level, the fact memory is a key-value memory (Weston et al., 2014; Miller et al., 2016), a construct used in many neural models in the past. It has been shown that sufficiently large LMs trained through self supervision (Peters et al., 2018; Devlin et al., 2019; Raffel et al., 2019; Brown et al., 2020) also encode factual information, motivating work on the extent to which a LM can serve as a KB (Roberts et al., 2020; Petroni et al., 2019; Poerner et al., 2019). Other work has explored techniques to improve the performance of large LMs in answering factual probes, by adding additional supervision in pre-training (Xiong et al., 2019; Wang et al., 2020b) or by adding entity embedd"
2021.naacl-main.288,N18-1202,0,0.0561805,"des a fact memory where each element is derived from a triple of vectors, representing a KB entity or relation. Since these triples are defined compositionally from (representations of) entities and relations, they have an interpretable symbolic meaning: e.g., if emtv is the vector representation of KB entity “Mountain View, CA” and egoogle and rhq similarly correspond to “Google Inc” and the relation “headquartered in”, these vectors can be used to construct a memory element f (egoogle , rhq , emtv ) for the KB assertion “Google, Inc is headquartered in Mountain Neural language models (LMs) (Peters et al., 2018; Devlin et al., 2019; Raffel et al., 2019) that have been pre-trained by self-supervision on large corpora contain rich knowledge about the syntax and semantics of natural language (Tenney et al., 2019), and are the basis of much recent work in NLP. Pretrained LMs also contain large amounts of factual knowledge about the world (Petroni et al., 2019; Roberts et al., 2020; Brown et al., 2020). However, while large LMs can be coerced to answer factual queries, they still lack many of the properties that knowledge bases (KBs) typically have. In particular, it is difficult to distinguish answers p"
2021.naacl-main.288,D19-1005,0,0.0956119,"Missing"
2021.naacl-main.288,D19-1250,0,0.101231,"similarly correspond to “Google Inc” and the relation “headquartered in”, these vectors can be used to construct a memory element f (egoogle , rhq , emtv ) for the KB assertion “Google, Inc is headquartered in Mountain Neural language models (LMs) (Peters et al., 2018; Devlin et al., 2019; Raffel et al., 2019) that have been pre-trained by self-supervision on large corpora contain rich knowledge about the syntax and semantics of natural language (Tenney et al., 2019), and are the basis of much recent work in NLP. Pretrained LMs also contain large amounts of factual knowledge about the world (Petroni et al., 2019; Roberts et al., 2020; Brown et al., 2020). However, while large LMs can be coerced to answer factual queries, they still lack many of the properties that knowledge bases (KBs) typically have. In particular, it is difficult to distinguish answers produced by memorizing factual statements in the pre-training corpus from lower-precision answers produced by linguistic generalization (Poerner et al., 2019). It 1 Models large enough to achieve good factual coverage is also difficult to add or remove factual informa- require extreme amounts of compute, and the largest neural tion without retraining"
2021.naacl-main.288,N13-1008,0,0.156452,"for 10,000 steps. We then 2007; Google, 2012; Dong, 2017; Vrandeˇci´c and modify the memory of this model without applying Krötzsch, 2014). In machine learning, a well studany additional training on the new memory. In ied problem is learning KB embeddings (Bordes addition to adding new memories which correspond et al., 2013; Lin et al., 2015; Trouillon et al., 2017; 3685 Dettmers et al., 2018) which enable generalization from known KB triples to novel triples that are plausibly true. KB embeddings can often be improved by incorporating raw text and symbolic KGs into a shared embedding space (Riedel et al., 2013; Verga et al., 2016, 2017), to be jointly reasoned over (Sun et al., 2018, 2019). Many prior neural-symbolic methods have attempted to unify symbolic KBs and neural methods (Pinkas, 1991; de Penning et al., 2011; Laird et al., 2017; Besold et al., 2017). Recently, researchers have explored query languages for embedded KBs that are similar to symbolic KB query languages (Cohen et al., 2017; Hamilton et al., 2018; Ren et al., 2020; Cohen et al., 2020). Our fact memory builds on this prior work, and is most closely related to the memory used in EmQL (Sun et al., 2020), one KB embedding model tha"
2021.naacl-main.288,2020.emnlp-main.437,0,0.0323233,"Missing"
2021.naacl-main.288,D19-1242,1,0.89544,"Missing"
2021.naacl-main.288,D18-1455,1,0.837489,"dify the memory of this model without applying Krötzsch, 2014). In machine learning, a well studany additional training on the new memory. In ied problem is learning KB embeddings (Bordes addition to adding new memories which correspond et al., 2013; Lin et al., 2015; Trouillon et al., 2017; 3685 Dettmers et al., 2018) which enable generalization from known KB triples to novel triples that are plausibly true. KB embeddings can often be improved by incorporating raw text and symbolic KGs into a shared embedding space (Riedel et al., 2013; Verga et al., 2016, 2017), to be jointly reasoned over (Sun et al., 2018, 2019). Many prior neural-symbolic methods have attempted to unify symbolic KBs and neural methods (Pinkas, 1991; de Penning et al., 2011; Laird et al., 2017; Besold et al., 2017). Recently, researchers have explored query languages for embedded KBs that are similar to symbolic KB query languages (Cohen et al., 2017; Hamilton et al., 2018; Ren et al., 2020; Cohen et al., 2020). Our fact memory builds on this prior work, and is most closely related to the memory used in EmQL (Sun et al., 2020), one KB embedding model that supports compositional query language. EmQL implements “projection” usin"
2021.naacl-main.288,P19-1452,0,0.0119897,"ations, they have an interpretable symbolic meaning: e.g., if emtv is the vector representation of KB entity “Mountain View, CA” and egoogle and rhq similarly correspond to “Google Inc” and the relation “headquartered in”, these vectors can be used to construct a memory element f (egoogle , rhq , emtv ) for the KB assertion “Google, Inc is headquartered in Mountain Neural language models (LMs) (Peters et al., 2018; Devlin et al., 2019; Raffel et al., 2019) that have been pre-trained by self-supervision on large corpora contain rich knowledge about the syntax and semantics of natural language (Tenney et al., 2019), and are the basis of much recent work in NLP. Pretrained LMs also contain large amounts of factual knowledge about the world (Petroni et al., 2019; Roberts et al., 2020; Brown et al., 2020). However, while large LMs can be coerced to answer factual queries, they still lack many of the properties that knowledge bases (KBs) typically have. In particular, it is difficult to distinguish answers produced by memorizing factual statements in the pre-training corpus from lower-precision answers produced by linguistic generalization (Poerner et al., 2019). It 1 Models large enough to achieve good fac"
2021.naacl-main.288,N16-1103,0,0.0269472,"then 2007; Google, 2012; Dong, 2017; Vrandeˇci´c and modify the memory of this model without applying Krötzsch, 2014). In machine learning, a well studany additional training on the new memory. In ied problem is learning KB embeddings (Bordes addition to adding new memories which correspond et al., 2013; Lin et al., 2015; Trouillon et al., 2017; 3685 Dettmers et al., 2018) which enable generalization from known KB triples to novel triples that are plausibly true. KB embeddings can often be improved by incorporating raw text and symbolic KGs into a shared embedding space (Riedel et al., 2013; Verga et al., 2016, 2017), to be jointly reasoned over (Sun et al., 2018, 2019). Many prior neural-symbolic methods have attempted to unify symbolic KBs and neural methods (Pinkas, 1991; de Penning et al., 2011; Laird et al., 2017; Besold et al., 2017). Recently, researchers have explored query languages for embedded KBs that are similar to symbolic KB query languages (Cohen et al., 2017; Hamilton et al., 2018; Ren et al., 2020; Cohen et al., 2020). Our fact memory builds on this prior work, and is most closely related to the memory used in EmQL (Sun et al., 2020), one KB embedding model that supports compositi"
2021.naacl-main.288,E17-1058,0,0.0527202,"Missing"
2021.naacl-main.288,2021.findings-acl.121,0,0.0687334,"Missing"
2021.naacl-main.288,P15-1128,0,0.0150796,"w facts without retraining (§4.1) and updating stale facts (§4.2). However, we first validate the efficacy of our model on standard splits of widely used knowledge-intensive benchmarks against many state-of-the-art systems (§3.3), as well as two subsets of these benchmarks restricted to examples answerable with wikidata (§3.4) and examples filtered for train/test overlap (§3.5). 3.1 Data We evaluate on four knowledge intensive tasks7 . WebQuestionsSP is an Open-domain Question Answering dataset containing 4737 natural language questions linked to corresponding Freebase entities and relations (Yih et al., 2015) derived from WebQuestions(Berant et al., 2013). LAMA TREx is a set of fact-related cloze questions. Since we are interested in entity prediction models, we restrict our LAMA investigations to TREx, which has answers linked to Wikidata. TriviaQA (open) contains questions scraped from quiz-league websites (Joshi et al., 2017). We use the open splits following Lee et al. (2019). FreebaseQA is an Open-domain QA dataset derived from TriviaQA and other trivia resources (See Jiang et al. (2019) for full details). Every answer can be resolved to at least one Freebase entity and each question contains"
2021.naacl-main.288,P19-1139,0,0.0187484,"past. It has been shown that sufficiently large LMs trained through self supervision (Peters et al., 2018; Devlin et al., 2019; Raffel et al., 2019; Brown et al., 2020) also encode factual information, motivating work on the extent to which a LM can serve as a KB (Roberts et al., 2020; Petroni et al., 2019; Poerner et al., 2019). Other work has explored techniques to improve the performance of large LMs in answering factual probes, by adding additional supervision in pre-training (Xiong et al., 2019; Wang et al., 2020b) or by adding entity embeddings into an extended LM (Peters et al., 2019; Zhang et al., 2019; Févry et al., 2020). Our entity memory extends the Entities-asExperts (EaE) model (Févry et al., 2020). It is both the current state-of-the-art for a number of tasks and simpler to use than most prior models because it does not require external components for entity linking or entity encoding (like (Peters et al., 2019; Zhang et al., 2019; Logan et al., 2019)) and is not restricted to lexical KBs like WordNet and ConceptNet (like (Weissenborn et al., 2017; Chen et al., 2018; Mihaylov and Frank, 2018)). with millions of entities, whereas prior systems that make use of KB triples have been wit"
2021.naacl-main.288,N19-1064,0,0.0575436,"Missing"
2021.naacl-main.366,P17-1171,0,0.0300472,"se knowledge graphs (CSKGs) such as ConceptNet (Speer et al., 2017). However, the binary relations in CSKGs greatly limit the types of the knowledge that can be encoded. Here, instead of a KB, we use a corpus of generic sentences about commonsense facts, in particular GenericsKB (Bhakthavatsalam et al., 2020). The advantage of this approach is that text can represent more complex commonsense knowledge, including facts that relate three or more concepts. Formalized in this way, OpenCSR is a question answering task requiring (possibly) iterative retrieval, similar to other open-domain QA tasks (Chen et al., 2017) such as HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). As noted above, however, the surface of commonsense questions in OpenCSR have fewer hints about kinds of multi-hop reasoning required to answer them than the factoid questions in open-domain QA, resulting in a particularly challenging reasoning problem (see Sec. 3). begins with an entity-linked corpus, and computes both sparse and dense indices of entity mentions (i.e., linked named-entity spans). DrKIT’s fundamental reasoning operation is to “hop” from one weighted set of X entities to another, by 1) findi"
2021.naacl-main.366,P19-1222,0,0.0503849,"Missing"
2021.naacl-main.366,2020.emnlp-main.99,1,0.864124,"Missing"
2021.naacl-main.366,2020.emnlp-main.550,0,0.0527905,"a concept-to-fact sparse matrix E and a fact-to-fact sparse matrix S. The dense fact index D is pre-computed with a pre-trained bi-encoder. A weighed set of facts is represented as a sparse vector F . The workflow (left) of D R FACT starts mapping a question to a set of initial facts that have common concepts with it. Then, it recursively performs Fact-Follow operations (right) for computing Ft and At . Finally, it uses learnable hop-weights αt to aggregate the answers. 2019), which learns to maximize the score of facts that contain correct answers to a given question, following the steps of Karpukhin et al. (2020) (i.e., dense passage retrieval), so that we can use MIPS to do dense retrieval over the facts. After pre-training, we embed each fact in F with a dense vector (using the [CLS] token representation). Hence D is a |F |× d dense matrix. Sparse Fact-to-Fact Index S. We pre-compute the sparse links between facts by a set of connection rules, such as fi → fj when fi and fj have at least one common concept and fj introduces at least two more new concepts that are not in fi (see Appendix B (2) for more). Hence S is a binary sparse tensor with the dense shape |F |× |F|. Sparse Index of Concept-to-Fact"
2021.naacl-main.366,2020.findings-emnlp.171,0,0.046857,"dule, the gap between D R FACT and others is even larger. (Sec. 5) 2 Related Work Commonsense Reasoning. Many recent commonsense-reasoning (CSR) methods focus on multiple-choice QA. For example, KagNet (Lin et al., 2019) and MHGRN (Feng et al., 2020) use an external commonsense knowledge graph as structural priors to individually score each choice. These methods, though powerful in determining the best choice for a multi-choice question, are less realistic for practical applications where answer candidates are typically not available. To address this question, we extend work by Seo UnifiedQA (Khashabi et al., 2020) and other et al. (2019) and Dhingra et al. (2020), and proclosed-book QA models (Roberts et al., 2020) generate answers to questions by fine-tuning a pose an efficient, differentiable multi-hop reasoning method for OpenCSR, named D R FACT (for text-to-text transformer such as BART (Lewis Differentiable Reasoning over Facts). Specifically, et al., 2020a) or T5 (Raffel et al., 2020), but a 4612 disadvantage of closed-book QA models is that they do not provide intermediate explanations for their answers, i.e., the supporting facts, which makes them less trustworthy in downstream applications. Al"
2021.naacl-main.366,Q19-1026,0,0.0192674,"he binary relations in CSKGs greatly limit the types of the knowledge that can be encoded. Here, instead of a KB, we use a corpus of generic sentences about commonsense facts, in particular GenericsKB (Bhakthavatsalam et al., 2020). The advantage of this approach is that text can represent more complex commonsense knowledge, including facts that relate three or more concepts. Formalized in this way, OpenCSR is a question answering task requiring (possibly) iterative retrieval, similar to other open-domain QA tasks (Chen et al., 2017) such as HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). As noted above, however, the surface of commonsense questions in OpenCSR have fewer hints about kinds of multi-hop reasoning required to answer them than the factoid questions in open-domain QA, resulting in a particularly challenging reasoning problem (see Sec. 3). begins with an entity-linked corpus, and computes both sparse and dense indices of entity mentions (i.e., linked named-entity spans). DrKIT’s fundamental reasoning operation is to “hop” from one weighted set of X entities to another, by 1) finding mentions of new entities x0 that are related to some entity in X, guided by the ind"
2021.naacl-main.366,P19-1612,0,0.0444597,"Missing"
2021.naacl-main.366,2020.acl-main.703,0,0.0495943,"s et al., 2020) generate answers to questions by fine-tuning a pose an efficient, differentiable multi-hop reasoning method for OpenCSR, named D R FACT (for text-to-text transformer such as BART (Lewis Differentiable Reasoning over Facts). Specifically, et al., 2020a) or T5 (Raffel et al., 2020), but a 4612 disadvantage of closed-book QA models is that they do not provide intermediate explanations for their answers, i.e., the supporting facts, which makes them less trustworthy in downstream applications. Although closed-book models exist that are augmented with an additional retrieval module (Lewis et al., 2020b), these models mainly work for single-hop reasoning. QA over KGs or Text. A conventional source of commonsense knowledge is triple-based symbolic commonsense knowledge graphs (CSKGs) such as ConceptNet (Speer et al., 2017). However, the binary relations in CSKGs greatly limit the types of the knowledge that can be encoded. Here, instead of a KB, we use a corpus of generic sentences about commonsense facts, in particular GenericsKB (Bhakthavatsalam et al., 2020). The advantage of this approach is that text can represent more complex commonsense knowledge, including facts that relate three or"
2021.naacl-main.366,D19-1282,1,0.862088,"ning towards practical applications, we prochoice question answering (QA) — i.e., given pose to study open-ended commonsense reasona question and a small set of pre-defined aning (OpenCSR), where answers are generated efswer choices, models are required to determine ficiently, rather than selected from a small list which of the candidate choices best answers the of candidates (see Figure 1). As a step toquestion. Existing commonsense reasoning modward this, here we explore a setting where the els usually work by scoring a question-candidate model produces a ranked list of answers from a pair (Lin et al., 2019; Lv et al., 2020; Feng et al., large question-independent set of candidate con2020). Hence, even an accurate multiple-choice cepts that are extracted offline from a corpus of ∗ common-sense facts written in natural language. The work was mainly done during Bill Yuchen Lin’s internship at Google Research. The OpenCSR task is inherently challenging. 1 Our code and data are available at the project website — One problem is that for many questions, findhttps://open-csr.github.io/. The human annoing an answer requires reasoning over two or tations were collected by the USC-INK group. 4611 Proceedi"
2021.naacl-main.366,D18-1260,0,0.109204,"Missing"
2021.naacl-main.366,D19-1261,0,0.036518,"Missing"
2021.naacl-main.366,D18-1455,1,0.896027,"Missing"
2021.naacl-main.366,D18-1259,1,0.821544,"mputational Linguistics: Human Language Technologies, pages 4611–4625 June 6–11, 2021. ©2021 Association for Computational Linguistics more natural-language facts from a corpus. In the multiple-choice QA setting, as the set of candidates is small, we can pair a question with an answer, and use the combination to retrieve relevant facts and then reason with them. In the open-ended setting, this is impractical: instead one needs to retrieve facts from the corpus using the question alone. In this respect, OpenCSR is similar to multi-hop factoid QA about named entities, e.g. as done for HotpotQA (Yang et al., 2018). However, the underlying reasoning chains of most multi-hop factoid QA datasets are relatively clear and context-independent, and are thus easier to infer. Commonsense questions, in contrast, exhibit more variable types of reasoning, and the relationship between a question and the reasoning to answer the question is often unclear. (For example, a factoid question like “who starred in a movie directed by Bradley Cooper?” clearly suggests following a directed-by relationship and then a starred-in relationship, while the underlying reasoning chains of a question like “what can help alleviate glo"
2021.naacl-main.366,2020.emnlp-main.437,0,0.0229099,"Missing"
2021.naacl-main.366,P19-1436,0,0.034591,"Missing"
2021.naacl-main.366,D19-1242,1,0.848928,"information from a fact for multi-hop reasoning. 3 Open-Ended Commonsense Reasoning Task Formulation. We denote a corpus of knowledge facts as F, and use V to denote a vocabulary of concepts; both are sets consisting of Multi-Hop Reasoning. Many recent models unique elements. A fact fi ∈ F is a sentence for open-domain QA tackle multi-hop reasoning that describes generic commonsense knowledge, through iterative retrieval, e.g., GRAFT-Net (Sun such as “trees remove carbon dioxide from the et al., 2018), MUPPET (Feldman and El-Yaniv, atmosphere through photosynthesis.” A concept 2019), PullNet (Sun et al., 2019), and GoldEn (Qi cj ∈ V is a noun or base noun phrase mentioned et al., 2019). These models, however, are not endfrequently in these facts (e.g., ‘tree’ and ‘carbon to-end differentiable and thus tend to have slower dioxide’). Concepts are considered identical if inference speed, which is a limitation shared by their surface forms are the same (after lemmamany other works using reading comprehension tization). Given only a question q (e.g., “what for multi-step QA (Das et al., 2019; Lee et al., can help alleviate global warming?”), an open2019). As another approach, Neural Query Lanended commo"
D08-1095,H05-1091,0,0.123342,"Missing"
D08-1095,J05-1003,0,0.117714,"to the final node score. 4.2 Node Reranking Learning We consider a supervised setting, where we are given a dataset of example queries and labels over the graph nodes, indicating which nodes are relevant to which query. For completeness, we describe here two methods previously described by Minkov and 2 In this paper, we consider either uniform edge weights; or, learn the set of weights Θ from examples. 3 We tune K empirically and set γ = 0.5, as in (Minkov and Cohen, 2007). 909 Reranking of the top candidates in a ranked list has been successfully applied to multiple NLP tasks (Collins, 2002; Collins and Koo, 2005). In essence, discriminative reranking allows the re-ordering of results obtained by methods that perform some form of local search, using features that encode higher level information. 4 For every example query, a handful of the retrieved nodes are considered, including both relevant and irrelevant nodes. A number of features describing the set of paths from Vq can be conveniently computed in the process of executing the graph walk, and it has been shown that reranking using these features can improve results significantly. It has also been shown that reranking is complementary to weight tuni"
D08-1095,de-marneffe-etal-2006-generating,0,0.0339003,"Missing"
D08-1095,C92-2082,0,0.0155662,"e the text representation schema and the proposed set of graph-based similarity measures on the task of coordinate term extraction. In particular, 7 We refer the reader to the TextGraph workshop proceedings, http://textgraphs.org. 912 we evaluate the extraction of named entities, including city names and person names from newswire data, using word similarity measures. Coordinate terms reflect a particular type of word similarity (relatedness), and are therefore an appropriate test case for our framework. While coordinate term extraction is often addressed by a rule-based (templates) approach (Hearst, 1992), this approach was designed for very large corpora such as the Web, where the availability of many redundant documents allows use of high-precision and low-recall rules. In this paper we focus on relatively small corpora. Small limited text collections may correspond to documents residing on a personal desktop, email collections, discussion groups and other specialized sets of documents. The task defined in the experiments is to retrieve a ranked list of city or person names given a small set of seeds. This task is implemented in the graph as a query, where we let the query distribution Vq be"
D08-1095,D07-1061,0,0.305367,"f language processing applications. In this paper we assume directed graphs, where typed nodes denote entities and labelled directed and weighted edges denote the relations between them. In this framework, graph walks can be applied to draw a measure of similarity between the graph nodes. Previous works have applied graph walks to draw a notion of semantic similarity over such graphs that were carefully designed and manually tuned, based on WordNet reations (Toutanova ∗ Current address: Nokia Research Center Cambridge, Cambridge, MA 02142, USA. et al., 2004; Collins-Thompson and Callan, 2005; Hughes and Ramage, 2007). While these and other researchers have used WordNet to evaluate similarity between words, there has been much interest in extracting such a measure from text corpora (e.g., (Snow et al., 2005; Pad´o and Lapata, 2007)). In this paper, we suggest processing dependency parse trees within the general framework of directed labelled graphs. We construct a graph that directly represents a corpus of structured (parsed) text. In the suggested graph scheme, nodes denote words and weighted edges represent the dependency relations between them. We apply graph walks to derive an inter-word similarity mea"
D08-1095,P98-2127,0,0.522783,"al distribution R. In this multi-step walk, nodes that are reached from the query nodes by many shorter paths will be assigned a higher score than nodes connected over fewer longer paths. 4 Cohen (Minkov and Cohen, 2007): a hill-climbing method that tunes the graph weights; and a reranking method. We also specify the feature set to be used by the reranking method in the domain of parsed text. 4.1 Weight Tuning There are several motivations for learning the graph weights Θ in this domain. First, some dependency relations – foremost, subject and object – are in general more salient than others (Lin, 1998; Pad´o and Lapata, 2007). In addition, dependency relations may have varying importance per different notions of word similarity (e.g., noun vs. verb similarity (Resnik and Diab, 2000)). Weight tuning allows the adaption of edge weights to each task (i.e., distribution of queries). The weight tuning method implemented in this work is based on an error backpropagation hill climbing algorithm (Diligenti et al., 2005). The algorithm minimizes the following cost function: E= 1 X 1 1 X 2 (pz − pOpt ez = z ) N z∈N N z∈N 2 where ez is the error for a target node z defined as the squared difference b"
D08-1095,J07-2002,0,0.11127,"Missing"
D08-1095,N03-1032,0,0.0517904,"preferable for the city name extraction task, and the syntactic dependency vectors model gives substantially better performance for person name extraction. We conjecture that city name mentions are less structured in the underlying text. In addition, the syntactic weighting scheme of the DV model is probably not optimal for the case of city names. For example, a conjunction relation was 914 found highly indicative for city names (see below). However, this relation is not emphasized by the DV weighting schema. As expected, the performance of the vector-based models improves for larger corpora (Terra and Clarke, 2003). These models demonstrate good performance for the larger MUC+AP corpus, but only mediocre performance for the smaller MUC corpus. Contrasting the graph-based methods with the vector-based models, the difference in performance in favor of reranking and PCW, especially for the smaller corpus, can be attributed to two factors. The first factor is learning, which optimizes performance for the underlying data. A second factor is the incorporation of non-local information, encoding properties of the traversed paths. Models. Following is a short description of the models learned by the different me"
D08-1095,C98-2122,0,\N,Missing
D08-1095,P02-1062,0,\N,Missing
D08-1099,N07-1066,1,0.809616,"he Ephyra pattern matching approach learns textual patterns that relate question key terms to possible answers and applies these patterns to candidate sentences to extract factoid answers. The semantic approach generates a semantic representation of the question that is based on predicate-argument structures and extracts answer candidates from similar structures in the corpus. The source code of the answer extractors is included in OpenEphyra, an open source release of the system.2 The answer candidates from these extractors are combined and ranked by a statistical answer selection framework (Ko et al., 2007), which estimates the probability of an answer based on a number of answer validation and similarity features. Validation features use resources such as gazetteers and Wikipedia to verify an answer, whereas similarity features measure the syntactic and semantic similarity to other candidates, e.g. using string distance measures and WordNet relations. 2.2 Set Expander for Any Language (SEAL) Set expansion (SE) refers to expanding a given partial set of objects into a more complete set. SEAL3 (Wang and Cohen, 2007) is a SE system which accepts input elements (seeds) of some target set St and aut"
D09-1156,W09-2201,0,0.0162097,"cuses on finding small closed sets 1 http://rcwang.com/seal of items (e.g., Disney movies) rather than large and more open sets (e.g., scientists). In this paper, we explore the impact on performance of one of the innovations in SEAL, specifically, the use of character-level techniques to detect candidate regular structures, or wrappers, in web pages. Although some early systems for web-page analysis induce rules at character-level (e.g., such as WIEN (Kushmerick et al., 1997) and DIPRE (Brin, 1998)), most recent approaches for set expansion have used either tokenized and/or parsed free-text (Carlson et al., 2009; Talukdar et al., 2006; Snow et al., 2006; Pantel and Pennacchiotti, 2006), or have incorporated heuristics for exploiting HTML structures that are likely to encode lists and tables (Nadeau et al., 2006; Etzioni et al., 2005). In this paper, we experimentally evaluate SEAL’s performance under two settings: 1) using the character-level page analysis techniques of the original SEAL, and 2) using page analysis techniques constrained to identify only HTMLrelated wrappers. Our conjecture is that the less constrained character-level methods will produce more candidate wrappers than HTML-based techn"
D09-1156,P06-1015,0,0.0862629,"tems (e.g., Disney movies) rather than large and more open sets (e.g., scientists). In this paper, we explore the impact on performance of one of the innovations in SEAL, specifically, the use of character-level techniques to detect candidate regular structures, or wrappers, in web pages. Although some early systems for web-page analysis induce rules at character-level (e.g., such as WIEN (Kushmerick et al., 1997) and DIPRE (Brin, 1998)), most recent approaches for set expansion have used either tokenized and/or parsed free-text (Carlson et al., 2009; Talukdar et al., 2006; Snow et al., 2006; Pantel and Pennacchiotti, 2006), or have incorporated heuristics for exploiting HTML structures that are likely to encode lists and tables (Nadeau et al., 2006; Etzioni et al., 2005). In this paper, we experimentally evaluate SEAL’s performance under two settings: 1) using the character-level page analysis techniques of the original SEAL, and 2) using page analysis techniques constrained to identify only HTMLrelated wrappers. Our conjecture is that the less constrained character-level methods will produce more candidate wrappers than HTML-based techniques. We also conjecture that a larger number of candidate wrappers will l"
D09-1156,P06-1101,0,0.0942557,"cwang.com/seal of items (e.g., Disney movies) rather than large and more open sets (e.g., scientists). In this paper, we explore the impact on performance of one of the innovations in SEAL, specifically, the use of character-level techniques to detect candidate regular structures, or wrappers, in web pages. Although some early systems for web-page analysis induce rules at character-level (e.g., such as WIEN (Kushmerick et al., 1997) and DIPRE (Brin, 1998)), most recent approaches for set expansion have used either tokenized and/or parsed free-text (Carlson et al., 2009; Talukdar et al., 2006; Snow et al., 2006; Pantel and Pennacchiotti, 2006), or have incorporated heuristics for exploiting HTML structures that are likely to encode lists and tables (Nadeau et al., 2006; Etzioni et al., 2005). In this paper, we experimentally evaluate SEAL’s performance under two settings: 1) using the character-level page analysis techniques of the original SEAL, and 2) using page analysis techniques constrained to identify only HTMLrelated wrappers. Our conjecture is that the less constrained character-level methods will produce more candidate wrappers than HTML-based techniques. We also conjecture that a larger nu"
D09-1156,W06-2919,0,0.0707564,"closed sets 1 http://rcwang.com/seal of items (e.g., Disney movies) rather than large and more open sets (e.g., scientists). In this paper, we explore the impact on performance of one of the innovations in SEAL, specifically, the use of character-level techniques to detect candidate regular structures, or wrappers, in web pages. Although some early systems for web-page analysis induce rules at character-level (e.g., such as WIEN (Kushmerick et al., 1997) and DIPRE (Brin, 1998)), most recent approaches for set expansion have used either tokenized and/or parsed free-text (Carlson et al., 2009; Talukdar et al., 2006; Snow et al., 2006; Pantel and Pennacchiotti, 2006), or have incorporated heuristics for exploiting HTML structures that are likely to encode lists and tables (Nadeau et al., 2006; Etzioni et al., 2005). In this paper, we experimentally evaluate SEAL’s performance under two settings: 1) using the character-level page analysis techniques of the original SEAL, and 2) using page analysis techniques constrained to identify only HTMLrelated wrappers. Our conjecture is that the less constrained character-level methods will produce more candidate wrappers than HTML-based techniques. We also conjectu"
D11-1049,C10-1057,0,0.00921353,"is extremely efficient at link prediction or retrieval tasks, in which we are interested in identifying top links from a large number of candidates, instead of focusing on a particular node pair or joint inferences. 1.4 Related Work The TextRunner system (Cafarella et al., 2006) answers list queries on a large knowledge base produced by open domain information extraction. Spreading activation is used to measure the closeness of any node to the query term nodes. This approach is similar to the random walk with restart approach which is used as a baseline in our experiment. The FactRank system (Jain and Pantel, 2010) compares different ways of constructing random walks, and combining them with extraction scores. However, the shortcoming of both approaches is that they ignore edge type 532 information, which is important for achieving high accuracy predictions. The HOLMES system (Schoenmackers et al., 2008) derives new assertions using a few manually written inference rules. A Markov network corresponding to the grounding of these rules to the knowledge base is constructed for each query, and then belief propagation is used for inference. In comparison, our proposed approach discovers inference rules autom"
D11-1049,D08-1095,1,0.514992,"eraged over 96 tasks. `=3 `=4 all paths up to length L 15, 376 1, 906, 624 +query support≥ α = 0.01 522 5016 +ever reach a target entity 136 792 +L1 regularization 63 271 enumeration; however, for domains with a large number of edge types (e.g., a knowledge base), it is impractical to enumerate all possible relation paths even for small `. For instance, if the number of edge types related to each node type is 100, even the number of length three paths types easily reaches millions. For other domains like parsed natural language sentences, useful relation paths can be as long as ten relations (Minkov and Cohen, 2008). In this case, even with smaller number of possible edge types, the total number of relation paths is still too large for systematic enumeration. In order to apply PRA to these domains, we modify the path generation procedure in PRA to produce only relation paths which are potentially useful for the task. Define a query s to be supporting a path P if hs,P (e) 6= 0 for any entity e. We require that any path node created during path finding needs to be supported by at least a fraction α of the training queries si , as well as being of length no more than ` (In the experiments, we set α = 0.01)"
D11-1049,P06-1015,0,0.0330095,"Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical 529 inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect knowledge. The KB we consider is a"
D11-1049,D08-1009,0,0.0392401,"Missing"
D11-1049,P06-1101,0,0.00851504,"Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL’s earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. 1 Introduction Although there is a great deal of recent research on extracting knowledge from text (Agichtein and Gravano, 2000; Etzioni et al., 2005; Snow et al., 2006; Pantel and Pennacchiotti, 2006; Banko et al., 2007; Yates et al., 2007), much less progress has been made on the problem of drawing reliable inferences from this imperfectly extracted knowledge. In particular, traditional logical 529 inference methods are too brittle to be used to make complex inferences from automatically-extracted knowledge, and probabilistic inference methods (Richardson and Domingos, 2006) suffer from scalability problems. This paper considers the problem of constructing inference methods that can scale to large knowledge bases (KB’s), and that are robust to imperfect kn"
D11-1049,N07-4013,0,\N,Missing
D12-1093,N06-1038,0,0.0074744,"y define features to be shortest paths on dependency trees which connect pairs of NP candidates. 1019 This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. Culotta et al. (2006)’s work is similar to our approach in the sense of relation extraction by discovering relational patterns. However while they focus on identifying relation mentions in text (microreading),this work attempts to infer new tuples by gathering path evidence over the whole corpus (macroreading). In addition, their work involves a few thousand examples, while we aim for Web-scale extraction. Do and Roth (2010) use a KB (YAGO) to aid the generation of features from free text. However their method is designed specifically for extracting hierarchical taxonomic structures, while our algorithm can be use"
D12-1093,D10-1107,0,0.0120882,"heir path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. Culotta et al. (2006)’s work is similar to our approach in the sense of relation extraction by discovering relational patterns. However while they focus on identifying relation mentions in text (microreading),this work attempts to infer new tuples by gathering path evidence over the whole corpus (macroreading). In addition, their work involves a few thousand examples, while we aim for Web-scale extraction. Do and Roth (2010) use a KB (YAGO) to aid the generation of features from free text. However their method is designed specifically for extracting hierarchical taxonomic structures, while our algorithm can be used to discover relations for general general graph-based KBs. In this paper we extend the PRA algorithm along two dimensions: combining syntactic and semantic cues in text with existing knowledge in the KB; and a distributed implementation of the learning and inference algorithms that works at Web scale. 2 Path Ranking Algorithm We briefly review the Path Ranking algorithm (PRA), described in more detail"
D12-1093,D09-1120,0,0.0146574,"also collect a large Web corpus and identify 60 million pages that mention concepts relevant to this study. The free text on those pages are POS-tagged and dependency parsed with an accuracy comparable to that of the current Stanford dependency parser (Klein and Manning, 2003). The parser produces a dependency tree for each sentence with each edge labeled with a standard dependency tag (see Figure 1). In each of the parsed documents, we use POS tags and dependency edges to identify potential referring noun phrases (NPs). We then use a within-document coreference resolver comparable to that of Haghighi and Klein (2009) to group referring NPs into co-referring clusters. For each cluster that contains a proper-name mention, we find the Freebase concept or concepts, if any, with a name or alias that matches 3 www.wikipedia.org, www.allmusic.com, www. imdb.com. 1022 provided that there exists at least one c ∈ C(m) and m ∈ M (u) such that p(c|m) &gt; 0. Note that M (c) only contains the proper-name mentions in cluster c. 5 Results We use three relations profession, nationality and parents for our experiments. For each relation, we select its current set of triples in Freebase, and apply the stratified sampling (Sec"
D12-1093,C92-2082,0,0.285256,"rning (SRL) seeks to combine statistical and relational learning methods to address such tasks. However, most SRL approaches (Friedman et al., 1999; Richardson and Domingos, 2006) suffer the complexity of inference and learning when applied to large scale problems. Recently, Lao and Cohen (2010) introduced Path Ranking algorithm, which is applicable to larger scale problems such as literature recommendation (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be sho"
D12-1093,P03-1054,0,0.00390782,"ase as our knowledge base. Freebase data is harvested from many sources, including Wikipedia, AMG, and IMDB.3 As of this writing, it contains more than 21 million concepts and 70 million labeled edges. For a large majority of concepts that appear both in Freebase and Wikipedia, Freebase maintains a link to the Wikipedia page of that concept. We also collect a large Web corpus and identify 60 million pages that mention concepts relevant to this study. The free text on those pages are POS-tagged and dependency parsed with an accuracy comparable to that of the current Stanford dependency parser (Klein and Manning, 2003). The parser produces a dependency tree for each sentence with each edge labeled with a standard dependency tag (see Figure 1). In each of the parsed documents, we use POS tags and dependency edges to identify potential referring noun phrases (NPs). We then use a within-document coreference resolver comparable to that of Haghighi and Klein (2009) to group referring NPs into co-referring clusters. For each cluster that contains a proper-name mention, we find the Freebase concept or concepts, if any, with a name or alias that matches 3 www.wikipedia.org, www.allmusic.com, www. imdb.com. 1022 pro"
D12-1093,D11-1049,1,0.487434,"William W. Cohen1 1 Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA 2 Google Research, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA nlao@cs.cmu.edu, {asubram, pereira}@google.com, wcohen@cs.cmu.edu Abstract then the person is a national of the country.” Of course, rules like this may be defeasible, in this case for example because of naturalization or political changes. Nevertheless, many such imperfect rules can be learned and combined to yield useful KB completions, as demonstrated in particular with the Path-Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011), which learns such rules on heterogenous graphs for link prediction tasks. We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the pa"
D12-1093,P09-1113,0,0.820977,"to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. 1019 This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. Culotta et al. (2006)’s work is similar to our approach in the sense of relation extraction by discovering relational patterns. However while they focus on identifying"
D12-1093,P02-1006,0,0.0266667,"tion (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. 1019 This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-"
D12-1119,P08-1029,1,0.814281,"ly distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1 . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other fa"
D12-1119,W06-1615,0,0.15196,"ecific distribution Ddi , and yi is the label (e.g. yi ∈ {−1, +1} for binary labels). Standard learning ignores di , but MDL uses these to improve learning accuracy. Why should we care about the domain label? Domain differences can introduce errors in a number of ways (Ben-David et al., 2007; Ben-David et al., 2009). First, the domain-specific distributions Ddi can differ such that they favor different features, i.e. p(x) changes between domains. As a result, some features may only appear in one domain. This aspect of domain difference is typically the focus of unsupervised domain adaptation (Blitzer et al., 2006; Blitzer et al., 2007). Second, the features may behave differently with respect to the label in each domain, i.e. p(y|x) changes between domains. As a result, a learning algorithm cannot generalize the behavior of features from one domain to another. The key idea behind many MDL algorithms is to target one or both of these properties of domain difference 1303 to improve performance. Prior approaches to MDL can be broadly categorized into two classes. The first set of approaches (Daum´e III, 2007; Dredze et al., 2008) introduce parameters to capture domain-specific behaviors while preserving"
D12-1119,P07-1056,1,0.862631,"has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art. 1 Introduction Research efforts in recent years have demonstrated the importance of domains in statistical natural language processing. A mismatch between training and test domains can negatively impact system accuracy as it violates a core assumption in many machine learning algorithms: that data points are independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product r"
D12-1119,W04-3237,0,0.00951458,"ssues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art. 1 Introduction Research efforts in recent years have demonstrated the importance of domains in statistical natural language processing. A mismatch between training and test domains can negatively impact system accuracy as it violates a core assumption in many machine learning algorithms: that data points are independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One su"
D12-1119,W10-2608,0,0.0251531,"Missing"
D12-1119,P07-1033,0,0.72823,"Missing"
D12-1119,D08-1072,1,0.960015,"independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1 . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would"
D12-1119,N09-1068,0,0.0948268,".). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1 . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other factors contributing to lear"
D12-1119,D07-1111,0,0.0119993,"ety of tasks. The key idea behind ensemble learning, that of combining a diverse array of models, has been applied to settings in which data preprocessing is used to create many different classifiers. Examples include instance bagging and feature bagging (Dietterich, 2000). The core idea of using diverse inputs in making classification decisions is common in the MDL literature. In fact, the top performing and only successful entry to the 2007 CoNLL shared task on domain adaptation for dependency parsing was a straightforward implementation of ensemble learning by creating variants of parsers (Sagae and Tsujii, 2007). Many MDL algorithms, among them Dredze and Crammer (2008), Daum´e III (2009), Zhang and Yeung (2010) and Saha et al. (2011), all include some notion of learning domain-specific classifiers on the training data, and combining them in the best way possible. To be clear, we do not claim that these approaches can be reduced to an existing ensemble learning algorithm. There are crucial elements in each of these algorithms that separate them from existing ensemble learning algorithms. One example of such a distinction is the learning of domain relationships by both Zhang and Yeung (2010) and Saha"
D12-1119,W06-1639,0,0.0122179,", electronics and kitchen appliances. The original dataset contained 2,000 reviews for each of the four domains, with 1,000 positive and 1,000 negative reviews per domain. Feature extraction follows Blitzer et al. (2007): we use case insensitive unigrams and bigrams, although we remove rare features (those that appear less than five times in the training set). The reduced feature set was selected given the sensitivity to feature size of some of the MDL methods. ConVote (C ONVOTE) Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al. (2006). The binary classification task on this dataset is that of predicting whether a given speech segment supports or opposes a bill under discussion in the floor debate. We select this dataset because, unlike the A MAZON data, C ONVOTE can be divided into domains in several ways based on different metadata attributes available with the dataset. We consider two types of domain divisions: the bill identifier and the political party of the speaker. Division based on the bill creates domain differences in that each bill has its own topic. Division based on political party implies preference for diffe"
D14-1122,W09-2307,0,0.0610002,"Missing"
D14-1122,C10-3004,0,0.0239477,"Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have al"
D14-1122,C02-1126,0,0.0732128,"Missing"
D14-1122,D07-1098,0,0.0226316,"first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chine"
D14-1122,I11-1136,0,0.0214741,"endency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the"
D14-1122,D14-1108,1,0.831036,"(Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, besides the Chinese posts, it also includes a copy of the English translations. This allows us to observe some interesting phenomena that mark the differences of the two languages. For example: • Function words are more frequently used in English than in Chinese. When examinFigure 1: An example of pro-drop phenomenon from the Weibo d"
D14-1122,P03-1056,0,0.306413,"d L is the length; (2) a database D of token relations from the corpus; (3) first-order logic inference rule set R. 4 4.1 A Programmable Parser with Personalized PageRank Inference A key problem in multilingual dependency parsing is that generic feature templates may not work well for every language. For example, Martins (2012) shows that for Chinese dependency parsing, when adding the generic grandparents and siblings features, the performance was worse than using the standard bilexical, unilexical, and part-of-speech features. Unfortunately, for many parsers such as Stanford Chinese Parser (Levy and Manning, 2003) and MaltParser (Nivre et al., 2007), it is very difficult for programmers to specify the feature templates and inference rules for dependency arc prediction. In this work, we present a Chinese dependency parsing method for Weibo, based on efficient probabilistic first-order logic programming (Wang et al., 2013). The advantage of probabilistic programming for parsing is that, software engineers can simply conduct theory engineering, and optimize the performance of the parser for a specific genre of the target language. Recently, probabilistic programming approaches (Goodman et al., 2012; Wang"
D14-1122,D12-1132,0,0.0227334,"hared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Li"
D14-1122,D11-1109,0,0.0154232,"rized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 201"
D14-1122,P13-1018,0,0.0336836,"and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, besides the Chinese posts, it also includes a copy of the English translations. This allows us to observe some interesting phenomena that mark the differences of the two languages. For example: • Function words are more frequently used in English than in Chinese. When examinFigure 1: An example of pro-drop phenomenon from the Weibo data. ing this English version of the Weibo corpus for the total counts of the word “the”, there are 2,084 occurrences in 2,003 sentences. Whereas in Chinese, there are only 52 occurren"
D14-1122,C12-1106,0,0.0370394,"Missing"
D14-1122,J93-2004,0,0.046559,"Internet users (Yang et al., 2012), making it one of the most popular social media services in the world. While Weibo posts are abundantly available, NLP techniques for analyzing Weibo posts have not been well-studied in the past. Syntactic analysis of Weibo is made difficult for three reasons: first, in the last few decades, Computational Linguistics researchers have primarily focused on building resources and tools using standard English newswire corpora2 , and thus, 1 http://en.wikipedia.org/wiki/Sina Weibo For example, Wall Street Journal articles are used for building the Penn Treebank (Marcus et al., 1993). 2 • We show that the proposed approach outperforms an off-the-shelf dependency parser, as well as a strong baseline trained on the same in-domain data. In the next section, we describe existing work on dependency parsing for Chinese. In Section 3, we present the new Chinese Weibo Treebank to the research community. In Section 4, we introduce the proposed efficient probabilistic programming approach for parsing Weibo. We show the experimental results in Section 5, and conclude in Section 6. 3 http://www.cs.cmu.edu/˜yww/data/WeiboTreebank.zip 1152 Proceedings of the 2014 Conference on Empirica"
D14-1122,P05-1012,0,0.0784823,", William W. Cohen Language Technologies Institute & Machine Learning Department Carnegie Mellon University Pittsburgh, PA 15213, USA. {yww,lingpenk,krivard,wcohen}@cs.cmu.edu Abstract there are fewer resources in other languages in general. Second, microblog posts are typically short, noisy (Gimpel et al., 2011), and can be considered as a “dialect”, which is very different from news data. Due to the differences in genre, part-of-speech taggers and parsers trained on newswire corpora typically fail on social media texts. Third, most existing parsers use languageindependent standard features (McDonald et al., 2005), and these features may not be optimal for Chinese (Martins, 2012). To most of the application developers, the parser is more like a blackbox, which is not directly programmable. Therefore, it is non-trivial to adapt these generic parsers to language-specific social media text. In this paper, we present a new probabilistic dependency parsing approach for Weibo, with the following contributions: Dependency parsing is a core task in NLP, and it is widely used by many applications such as information extraction, question answering, and machine translation. In the era of social media, a big chall"
D14-1122,P14-5021,0,0.0195765,"ly prominent in the short text, which clearly creates a problem for parsing. For example, in the Chinese Weibo data, we have observed the sentence in Figure 1. To facilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and produce a draft version of the treebank; in the second stage, the annotators actively discuss the difficult cases to reach agreements, manually correct the mis-segmented word tokens, and revise the annotations of the tricky cases. The final inter-annotator agreement rate on a randomly-selected subset of"
D14-1122,I05-3027,0,0.0331806,"head of the tree occurs more frequent on the left-to-middle of the sentence, while the distribution of the head is more complicated in Chinese. This is also verified from the parallel Weibo data. • Another well-known issue in Chinese is that Chinese is a pro-drop topical language. This is extremely prominent in the short text, which clearly creates a problem for parsing. For example, in the Chinese Weibo data, we have observed the sentence in Figure 1. To facilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and pr"
D14-1122,D07-1096,0,0.138857,"Missing"
D14-1122,C08-1132,0,0.015777,"that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese"
D14-1122,D07-1111,0,0.0513418,"fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and r"
D14-1122,W13-2307,0,0.0473676,"Missing"
D14-1122,D08-1059,0,0.015826,"g has attracted many interests in the last fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc pre"
D14-1122,P14-1125,0,0.0166563,"red tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, be"
D14-1122,W00-1308,0,0.0168013,"dle of the sentence, while the distribution of the head is more complicated in Chinese. This is also verified from the parallel Weibo data. • Another well-known issue in Chinese is that Chinese is a pro-drop topical language. This is extremely prominent in the short text, which clearly creates a problem for parsing. For example, in the Chinese Weibo data, we have observed the sentence in Figure 1. To facilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and produce a draft version of the treebank; in the second stage, the a"
D14-1122,W00-1201,0,\N,Missing
D14-1122,W06-2920,0,\N,Missing
D14-1122,P11-2008,0,\N,Missing
D14-1122,D07-1101,0,\N,Missing
D15-1060,P11-1055,0,0.172048,"Missing"
D15-1060,D14-1203,0,0.0265206,"Missing"
D15-1060,W12-2402,1,0.552018,"Missing"
D15-1060,D07-1111,0,0.0206166,"Missing"
D15-1060,D12-1042,0,0.0384392,"Missing"
D15-1060,P10-1013,0,0.0891479,"Missing"
D15-1192,D11-1072,0,\N,Missing
D15-1192,de-marneffe-etal-2006-generating,0,\N,Missing
D15-1192,kilgarriff-rosenzweig-2000-english,0,\N,Missing
D15-1192,Q15-1005,0,\N,Missing
D15-1192,W06-2501,0,\N,Missing
D15-1192,H05-1052,0,\N,Missing
D15-1192,S07-1071,0,\N,Missing
D15-1192,J07-2002,0,\N,Missing
D15-1192,S07-1016,0,\N,Missing
D15-1192,H93-1052,0,\N,Missing
D15-1192,E09-1005,0,\N,Missing
D15-1192,H05-1053,0,\N,Missing
D15-1192,P06-2104,0,\N,Missing
D15-1192,S07-1086,0,\N,Missing
D15-1192,D07-1061,0,\N,Missing
D15-1192,P10-1154,0,\N,Missing
D15-1192,N13-1133,0,\N,Missing
D18-1259,P17-1171,0,0.123334,"Missing"
D18-1259,D16-1264,0,0.483551,"is direction. However, existing datasets have limitations that hinder further advancements of machine reasoning over natural language, especially in testing QA systems’ ability to perform multi-hop reasoning, where the system has to reason with information taken from more than one document to arrive at the answer. ∗ These authors contributed equally. The order of authorship is decided through dice rolling. † Work done when WWC was at CMU. First, some datasets mainly focus on testing the ability of reasoning within a single paragraph or document, or single-hop reasoning. For example, in SQuAD (Rajpurkar et al., 2016) questions are designed to be answered given a single paragraph as the context, and most of the questions can in fact be answered by matching the question with a single sentence in that paragraph. As a result, it has fallen short at testing systems’ ability to reason over a larger context. TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) create a more challenging setting by using information retrieval to collect multiple documents to form the context given existing question-answer pairs. Nevertheless, most of the questions can be answered by matching the question with a few nearb"
D18-1259,N18-2088,0,0.0132483,"17) as our baseline model. We note that our implementation without weight averaging achieves performance very close to what the authors reported on SQuAD (about 1 point worse in F1 ). Our implemented model subsumes the latest technical advances on question answering, including character-level models, self-attention (Wang et al., 2017), and bi-attention (Seo et al., 2017). Combining these three key components is becoming standard practice, and various state-of-the-art or competitive architectures (Liu et al., 2018; Clark and Gardner, 2017; Wang et al., 2017; Seo et al., 2017; Pan et al., 2017; Salant and Berant, 2018; Xiong et al., 2018) on SQuAD can be viewed as similar to our implemented model. To accommodate yes/no questions, we also add a 3-way classifier after the last recurrent layer to produce the probabilities of “yes”, “no”, and span-based answers. During decoding, we first use the 3-way output to determine whether the answer is “yes”, “no”, or a text span. If it is a text span, we further search for the most probable span. Supporting Facts as Strong Supervision. To evaluate the baseline model’s performance in predicting explainable supporting facts, as well as how much they improve QA performanc"
D18-1259,P17-1147,0,0.124395,"authors contributed equally. The order of authorship is decided through dice rolling. † Work done when WWC was at CMU. First, some datasets mainly focus on testing the ability of reasoning within a single paragraph or document, or single-hop reasoning. For example, in SQuAD (Rajpurkar et al., 2016) questions are designed to be answered given a single paragraph as the context, and most of the questions can in fact be answered by matching the question with a single sentence in that paragraph. As a result, it has fallen short at testing systems’ ability to reason over a larger context. TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) create a more challenging setting by using information retrieval to collect multiple documents to form the context given existing question-answer pairs. Nevertheless, most of the questions can be answered by matching the question with a few nearby sentences in one single paragraph, which is limited as it does not require more complex reasoning (e.g., 2369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics over"
D18-1259,N18-1059,0,0.281768,"given existing question-answer pairs. Nevertheless, most of the questions can be answered by matching the question with a few nearby sentences in one single paragraph, which is limited as it does not require more complex reasoning (e.g., 2369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics over multiple paragraphs). Second, existing datasets that target multi-hop reasoning, such as QAngaroo (Welbl et al., 2018) and C OMPLEX W EB Q UESTIONS (Talmor and Berant, 2018), are constructed using existing knowledge bases (KBs). As a result, these datasets are constrained by the schema of the KBs they use, and therefore the diversity of questions and answers is inherently limited. Third, all of the above datasets only provide distant supervision; i.e., the systems only know what the answer is, but do not know what supporting facts lead to it. This makes it difficult for models to learn about the underlying reasoning process, as well as to make explainable predictions. To address the above challenges, we aim at creating a QA dataset that requires reasoning over mu"
D18-1259,P18-1157,0,0.0183116,"ading QA systems on our data, we reimplemented the architecture described in Clark and Gardner (2017) as our baseline model. We note that our implementation without weight averaging achieves performance very close to what the authors reported on SQuAD (about 1 point worse in F1 ). Our implemented model subsumes the latest technical advances on question answering, including character-level models, self-attention (Wang et al., 2017), and bi-attention (Seo et al., 2017). Combining these three key components is becoming standard practice, and various state-of-the-art or competitive architectures (Liu et al., 2018; Clark and Gardner, 2017; Wang et al., 2017; Seo et al., 2017; Pan et al., 2017; Salant and Berant, 2018; Xiong et al., 2018) on SQuAD can be viewed as similar to our implemented model. To accommodate yes/no questions, we also add a 3-way classifier after the last recurrent layer to produce the probabilities of “yes”, “no”, and span-based answers. During decoding, we first use the 3-way output to determine whether the answer is “yes”, “no”, or a text span. If it is a text span, we further search for the most probable span. Supporting Facts as Strong Supervision. To evaluate the baseline model"
D18-1259,P17-1018,0,0.194684,"Missing"
D18-1259,P14-5010,1,0.0146877,"Missing"
D18-1259,Q18-1021,0,0.111105,"to collect multiple documents to form the context given existing question-answer pairs. Nevertheless, most of the questions can be answered by matching the question with a few nearby sentences in one single paragraph, which is limited as it does not require more complex reasoning (e.g., 2369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics over multiple paragraphs). Second, existing datasets that target multi-hop reasoning, such as QAngaroo (Welbl et al., 2018) and C OMPLEX W EB Q UESTIONS (Talmor and Berant, 2018), are constructed using existing knowledge bases (KBs). As a result, these datasets are constrained by the schema of the KBs they use, and therefore the diversity of questions and answers is inherently limited. Third, all of the above datasets only provide distant supervision; i.e., the systems only know what the answer is, but do not know what supporting facts lead to it. This makes it difficult for models to learn about the underlying reasoning process, as well as to make explainable predictions. To address the above challenges, we aim a"
D18-1259,D17-2014,0,0.0126166,"upporting facts necessary to arrive at the answer, when the answer is generated. To this end, we also collect the sentences that determine the answers from crowd workers. These supporting facts can serve as strong supervision for what sentences to pay attention to. Moreover, we can now test the explainability of a model by comparing the predicted supporting facts to the ground truth ones. The overall procedure of data collection is illustrated in Algorithm 1. 3 Processing and Benchmark Settings We collected 112,779 valid examples in total on Amazon Mechanical Turk4 using the ParlAI interface (Miller et al., 2017) (see Appendix A).To isolate potential single-hop questions from the desired multi-hop ones, we first split out a subset of data called train-easy. Specifically, we randomly sampled questions (∼3–10 per Turker) from top-contributing turkers, and categorized all 2371 4 https://www.mturk.com/ Name Desc. Usage train-easy train-medium train-hard dev test-distractor test-fullwiki Total single-hop multi-hop hard multi-hop hard multi-hop hard multi-hop hard multi-hop training training training dev test test # Examples 18,089 56,814 15,661 7,405 7,405 7,405 112,779 Table 1: Data split. The splits trai"
D18-1259,D17-1238,0,0.0421206,"Missing"
D18-1455,P17-1171,0,0.451663,"ity Mention Figure 1: To answer a question posed in natural language, GRAFT-Net considers a heterogeneous graph constructed from text and KB facts, and thus can leverage the rich relational structure between the two information sources. Introduction Open domain Question Answering (QA) is the task of finding answers to questions posed in natural language. Historically, this required a specialized pipeline consisting of multiple machinelearned and hand-crafted modules (Ferrucci et al., 2010). Recently, the paradigm has shifted towards training end-to-end deep neural network models for the task (Chen et al., 2017; Liang et al., 2017; Raison et al., 2018; Talmor and Berant, 2018; Iyyer et al., 2017). Most existing models, however, answer questions using a single information source, usually either text from an encyclopedia, or a single knowledge base (KB). Intuitively, the suitability of an information source for QA depends on both its coverage and ∗ Haitian Sun and Bhuwan Dhingra contributed equally to this work. the difficulty of extracting answers from it. A large text corpus has high coverage, but the information is expressed using many different text patterns. As a result, models which operate on t"
D18-1455,P17-1167,0,0.0519226,"Missing"
D18-1455,N16-2016,0,0.0275458,"017). GRAFT-Nets are also inductive representation learners like GraphSAGE (Hamilton et al., 2017), but operate on a heterogeneous mixture of nodes and use retrieval for getting a subgraph instead of random sampling. The recently proposed Walk-Steered Convolution model uses random walks for learning graph representations (Jiang et al., 2018). Our personalization technique also borrows from such random walk literature, but uses it to localize propagation of embeddings. Tremendous progress on QA over KB has been made with deep learning based approaches like memory networks (Bordes et al., 2015; Jain, 2016) and reinforcement learning (Liang et al., 2017; Das et al., 2017a). But extending them with text, which is our main focus, is non-trivial. In another direction, there is also work on producing parsimonious graphical representations of textual data (Krause et al., 2016; Lu et al., 2017); however in this paper we use a simple sequential representation augmented with entity links to the KB which works well. For QA over text only, a major focus has been on the task of reading comprehension (Seo et al., 2017; Gong and Bowman, 2017; Hu et al., 2017; Shen et al., 2017; Yu et al., 2018) since the int"
D18-1455,E17-1013,0,0.353747,"n the graph to follow paths starting from seed nodes linked to the question (§ 3.3). Empirically, we show that both these extensions are crucial for the task of QA. We evaluate these methods on a new suite of benchmark tasks for testing QA models when both KB and text are present. Using WikiMovies (Miller et al., 2016) and WebQuestionsSP (Yih et al., 2016), we construct datasets with a varying amount of training supervision and KB completeness, and with a varying degree of question complexity. We report baselines for future comparison, including Key Value Memory Networks (Miller et al., 2016; Das et al., 2017c), and show that our proposed GRAFT-Nets have superior performance across a wide range of conditions (§ 5). We also show that GRAFT-Nets are competitive with the state-of-the-art methods developed specifically for text-only QA, and state-of-the art methods developed for KB-only QA (§ 5.4)1 . 1 Source code and data are available at https:// github.com/OceanskySun/GraftNet 2 2.1 Task Setup Description A knowledge base is denoted as K = (V, E, R), where V is the set of entities in the KB, and the edges E are triplets (s, r, o) which denote that relation r ∈ R holds between the subject s ∈ V and"
D18-1455,P17-2057,1,0.854188,"n the graph to follow paths starting from seed nodes linked to the question (§ 3.3). Empirically, we show that both these extensions are crucial for the task of QA. We evaluate these methods on a new suite of benchmark tasks for testing QA models when both KB and text are present. Using WikiMovies (Miller et al., 2016) and WebQuestionsSP (Yih et al., 2016), we construct datasets with a varying amount of training supervision and KB completeness, and with a varying degree of question complexity. We report baselines for future comparison, including Key Value Memory Networks (Miller et al., 2016; Das et al., 2017c), and show that our proposed GRAFT-Nets have superior performance across a wide range of conditions (§ 5). We also show that GRAFT-Nets are competitive with the state-of-the-art methods developed specifically for text-only QA, and state-of-the art methods developed for KB-only QA (§ 5.4)1 . 1 Source code and data are available at https:// github.com/OceanskySun/GraftNet 2 2.1 Task Setup Description A knowledge base is denoted as K = (V, E, R), where V is the set of entities in the KB, and the edges E are triplets (s, r, o) which denote that relation r ∈ R holds between the subject s ∈ V and"
D18-1455,N18-2092,1,0.807039,"ver, answer questions using a single information source, usually either text from an encyclopedia, or a single knowledge base (KB). Intuitively, the suitability of an information source for QA depends on both its coverage and ∗ Haitian Sun and Bhuwan Dhingra contributed equally to this work. the difficulty of extracting answers from it. A large text corpus has high coverage, but the information is expressed using many different text patterns. As a result, models which operate on these patterns (e.g. BiDAF (Seo et al., 2017)) do not generalize beyond their training domains (Wiese et al., 2017; Dhingra et al., 2018) or to novel types of reasoning (Welbl et al., 2018; Talmor and Berant, 2018). KBs, on the other hand, suffer from low coverage due to their inevitable incompleteness and restricted schema (Min et al., 2013), but are easier to extract answers from, since they are constructed precisely for the purpose of being queried. In practice, some questions are best answered using text, while others are best answered using KBs. A natural question, then, is how to effectively combine both types of information. Surprisingly little prior work has looked at this problem. In this paper we focus on a scenario i"
D18-1455,D12-1093,1,0.792218,"proaches in our experiments (§5), and show that GRAFT-Nets outperform KV-MemNNs over all tasks. Non-deep learning approaches have been also attempted for QA over both text assertions and KB facts. Gardner and Krishnamurthy (2017) use traditional feature extraction methods of openvocabulary semantic parsing for the task. Ryu et al. (2014) use a pipelined system aggregating evidence from both unstructured and semistructured sources for open-domain QA. Another line of work has looked at learning combined representations of KBs and text for relation extraction and Knowledge Base Completion (KBC) (Lao et al., 2012; Riedel et al., 2013; Toutanova et al., 2015; Verga et al., 2016; Das et al., 2017b; Han et al., 2016). The key difference in QA compared to KBC is that in QA the inference process on the knowledge source has to be conditioned on the question, so different questions induce different representations of the KB and warrant a different inference process. Furthermore, KBC operates under the fixed schema defined by the KB before-hand, whereas natural language questions might not adhere to this schema. The GRAFT-Net model itself is motivated from the large body of work on graph representation learni"
D18-1455,P17-1003,0,0.277423,"1: To answer a question posed in natural language, GRAFT-Net considers a heterogeneous graph constructed from text and KB facts, and thus can leverage the rich relational structure between the two information sources. Introduction Open domain Question Answering (QA) is the task of finding answers to questions posed in natural language. Historically, this required a specialized pipeline consisting of multiple machinelearned and hand-crafted modules (Ferrucci et al., 2010). Recently, the paradigm has shifted towards training end-to-end deep neural network models for the task (Chen et al., 2017; Liang et al., 2017; Raison et al., 2018; Talmor and Berant, 2018; Iyyer et al., 2017). Most existing models, however, answer questions using a single information source, usually either text from an encyclopedia, or a single knowledge base (KB). Intuitively, the suitability of an information source for QA depends on both its coverage and ∗ Haitian Sun and Bhuwan Dhingra contributed equally to this work. the difficulty of extracting answers from it. A large text corpus has high coverage, but the information is expressed using many different text patterns. As a result, models which operate on these patterns (e.g."
D18-1455,N18-1059,0,0.0544206,"language, GRAFT-Net considers a heterogeneous graph constructed from text and KB facts, and thus can leverage the rich relational structure between the two information sources. Introduction Open domain Question Answering (QA) is the task of finding answers to questions posed in natural language. Historically, this required a specialized pipeline consisting of multiple machinelearned and hand-crafted modules (Ferrucci et al., 2010). Recently, the paradigm has shifted towards training end-to-end deep neural network models for the task (Chen et al., 2017; Liang et al., 2017; Raison et al., 2018; Talmor and Berant, 2018; Iyyer et al., 2017). Most existing models, however, answer questions using a single information source, usually either text from an encyclopedia, or a single knowledge base (KB). Intuitively, the suitability of an information source for QA depends on both its coverage and ∗ Haitian Sun and Bhuwan Dhingra contributed equally to this work. the difficulty of extracting answers from it. A large text corpus has high coverage, but the information is expressed using many different text patterns. As a result, models which operate on these patterns (e.g. BiDAF (Seo et al., 2017)) do not generalize be"
D18-1455,D15-1174,0,0.0557165,"how that GRAFT-Nets outperform KV-MemNNs over all tasks. Non-deep learning approaches have been also attempted for QA over both text assertions and KB facts. Gardner and Krishnamurthy (2017) use traditional feature extraction methods of openvocabulary semantic parsing for the task. Ryu et al. (2014) use a pipelined system aggregating evidence from both unstructured and semistructured sources for open-domain QA. Another line of work has looked at learning combined representations of KBs and text for relation extraction and Knowledge Base Completion (KBC) (Lao et al., 2012; Riedel et al., 2013; Toutanova et al., 2015; Verga et al., 2016; Das et al., 2017b; Han et al., 2016). The key difference in QA compared to KBC is that in QA the inference process on the knowledge source has to be conditioned on the question, so different questions induce different representations of the KB and warrant a different inference process. Furthermore, KBC operates under the fixed schema defined by the KB before-hand, whereas natural language questions might not adhere to this schema. The GRAFT-Net model itself is motivated from the large body of work on graph representation learning (Scarselli et al., 2009; Li et al., 2016;"
D18-1455,D16-1147,0,0.583569,"des differently from the text nodes: for instance, LSTM-based updates are used to propagate information into and out of text nodes (§ 3.2). Second, we introduce a directed propagation method, inspired by personalized Pagerank in IR (Haveliwala, 2002), which constrains the propagation of embeddings in the graph to follow paths starting from seed nodes linked to the question (§ 3.3). Empirically, we show that both these extensions are crucial for the task of QA. We evaluate these methods on a new suite of benchmark tasks for testing QA models when both KB and text are present. Using WikiMovies (Miller et al., 2016) and WebQuestionsSP (Yih et al., 2016), we construct datasets with a varying amount of training supervision and KB completeness, and with a varying degree of question complexity. We report baselines for future comparison, including Key Value Memory Networks (Miller et al., 2016; Das et al., 2017c), and show that our proposed GRAFT-Nets have superior performance across a wide range of conditions (§ 5). We also show that GRAFT-Nets are competitive with the state-of-the-art methods developed specifically for text-only QA, and state-of-the art methods developed for KB-only QA (§ 5.4)1 . 1 Source c"
D18-1455,N13-1095,0,0.0287449,"s coverage and ∗ Haitian Sun and Bhuwan Dhingra contributed equally to this work. the difficulty of extracting answers from it. A large text corpus has high coverage, but the information is expressed using many different text patterns. As a result, models which operate on these patterns (e.g. BiDAF (Seo et al., 2017)) do not generalize beyond their training domains (Wiese et al., 2017; Dhingra et al., 2018) or to novel types of reasoning (Welbl et al., 2018; Talmor and Berant, 2018). KBs, on the other hand, suffer from low coverage due to their inevitable incompleteness and restricted schema (Min et al., 2013), but are easier to extract answers from, since they are constructed precisely for the purpose of being queried. In practice, some questions are best answered using text, while others are best answered using KBs. A natural question, then, is how to effectively combine both types of information. Surprisingly little prior work has looked at this problem. In this paper we focus on a scenario in which a large-scale KB (Bollacker et al., 2008; Auer et al., 2007) and a text corpus are available, but neither is sufficient alone for answering all questions. 4231 Proceedings of the 2018 Conference on E"
D18-1455,D16-1264,0,0.0657423,"earning (Liang et al., 2017; Das et al., 2017a). But extending them with text, which is our main focus, is non-trivial. In another direction, there is also work on producing parsimonious graphical representations of textual data (Krause et al., 2016; Lu et al., 2017); however in this paper we use a simple sequential representation augmented with entity links to the KB which works well. For QA over text only, a major focus has been on the task of reading comprehension (Seo et al., 2017; Gong and Bowman, 2017; Hu et al., 2017; Shen et al., 2017; Yu et al., 2018) since the introduction of SQuAD (Rajpurkar et al., 2016). These systems assume that the answer-containing passage is known apriori, but there has been progress when this assumption is relaxed (Chen et al., 2017; Raison et al., 2018; Dhingra et al., 2017; Wang et al., 2018, 2017; Watanabe et al., 2017). We work in the latter setting, where relevant information must be retrieved from large information sources, but we also incorporate KBs into this process. 5 Experiments & Results 5.1 Datasets WikiMovies-10K consists of 10K randomly sampled training questions from the WikiMovies dataset (Miller et al., 2016), along with the original test and validatio"
D18-1455,N13-1008,0,0.0582893,"which exploits all available sources of information, we randomly drop edges from the graph during training with probability p0 . We call this fact-dropout. It is usually easier to extract answers from the KB than from the documents, so the model tends to rely on the former, especially when the KB is complete. This method is similar to DropConnect (Wan et al., 2013). 4 Related Work The work of Das et al. (2017c) attempts an early fusion strategy for QA over KB facts and text. Their approach is based on Key-Value Memory Networks (KV-MemNNs) (Miller et al., 2016) coupled with a universal schema (Riedel et al., 2013) to populate a memory module with representations of KB triples and text snippets independently. The key limitation for this model is that it ignores the rich relational structure between the 4235 facts and text snippets. Our graph-based method, on the other hand, explicitly uses this structure for the propagation of embeddings. We compare the two approaches in our experiments (§5), and show that GRAFT-Nets outperform KV-MemNNs over all tasks. Non-deep learning approaches have been also attempted for QA over both text assertions and KB facts. Gardner and Krishnamurthy (2017) use traditional fe"
D18-1455,N16-1103,0,0.0216061,"erform KV-MemNNs over all tasks. Non-deep learning approaches have been also attempted for QA over both text assertions and KB facts. Gardner and Krishnamurthy (2017) use traditional feature extraction methods of openvocabulary semantic parsing for the task. Ryu et al. (2014) use a pipelined system aggregating evidence from both unstructured and semistructured sources for open-domain QA. Another line of work has looked at learning combined representations of KBs and text for relation extraction and Knowledge Base Completion (KBC) (Lao et al., 2012; Riedel et al., 2013; Toutanova et al., 2015; Verga et al., 2016; Das et al., 2017b; Han et al., 2016). The key difference in QA compared to KBC is that in QA the inference process on the knowledge source has to be conditioned on the question, so different questions induce different representations of the KB and warrant a different inference process. Furthermore, KBC operates under the fixed schema defined by the KB before-hand, whereas natural language questions might not adhere to this schema. The GRAFT-Net model itself is motivated from the large body of work on graph representation learning (Scarselli et al., 2009; Li et al., 2016; Kipf and Welling, 20"
D18-1455,Q18-1021,0,0.0264734,"e, usually either text from an encyclopedia, or a single knowledge base (KB). Intuitively, the suitability of an information source for QA depends on both its coverage and ∗ Haitian Sun and Bhuwan Dhingra contributed equally to this work. the difficulty of extracting answers from it. A large text corpus has high coverage, but the information is expressed using many different text patterns. As a result, models which operate on these patterns (e.g. BiDAF (Seo et al., 2017)) do not generalize beyond their training domains (Wiese et al., 2017; Dhingra et al., 2018) or to novel types of reasoning (Welbl et al., 2018; Talmor and Berant, 2018). KBs, on the other hand, suffer from low coverage due to their inevitable incompleteness and restricted schema (Min et al., 2013), but are easier to extract answers from, since they are constructed precisely for the purpose of being queried. In practice, some questions are best answered using text, while others are best answered using KBs. A natural question, then, is how to effectively combine both types of information. Surprisingly little prior work has looked at this problem. In this paper we focus on a scenario in which a large-scale KB (Bollacker et al., 2008; A"
D18-1455,K17-1029,0,0.0237302,"xisting models, however, answer questions using a single information source, usually either text from an encyclopedia, or a single knowledge base (KB). Intuitively, the suitability of an information source for QA depends on both its coverage and ∗ Haitian Sun and Bhuwan Dhingra contributed equally to this work. the difficulty of extracting answers from it. A large text corpus has high coverage, but the information is expressed using many different text patterns. As a result, models which operate on these patterns (e.g. BiDAF (Seo et al., 2017)) do not generalize beyond their training domains (Wiese et al., 2017; Dhingra et al., 2018) or to novel types of reasoning (Welbl et al., 2018; Talmor and Berant, 2018). KBs, on the other hand, suffer from low coverage due to their inevitable incompleteness and restricted schema (Min et al., 2013), but are easier to extract answers from, since they are constructed precisely for the purpose of being queried. In practice, some questions are best answered using text, while others are best answered using KBs. A natural question, then, is how to effectively combine both types of information. Surprisingly little prior work has looked at this problem. In this paper w"
D18-1455,P16-2033,0,0.136921,"instance, LSTM-based updates are used to propagate information into and out of text nodes (§ 3.2). Second, we introduce a directed propagation method, inspired by personalized Pagerank in IR (Haveliwala, 2002), which constrains the propagation of embeddings in the graph to follow paths starting from seed nodes linked to the question (§ 3.3). Empirically, we show that both these extensions are crucial for the task of QA. We evaluate these methods on a new suite of benchmark tasks for testing QA models when both KB and text are present. Using WikiMovies (Miller et al., 2016) and WebQuestionsSP (Yih et al., 2016), we construct datasets with a varying amount of training supervision and KB completeness, and with a varying degree of question complexity. We report baselines for future comparison, including Key Value Memory Networks (Miller et al., 2016; Das et al., 2017c), and show that our proposed GRAFT-Nets have superior performance across a wide range of conditions (§ 5). We also show that GRAFT-Nets are competitive with the state-of-the-art methods developed specifically for text-only QA, and state-of-the art methods developed for KB-only QA (§ 5.4)1 . 1 Source code and data are available at https://"
D19-1242,P17-1171,0,0.275881,"n domain Question Answering (QA) is the task of finding answers to questions posed in natural language, usually using text from a corpus (Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017), or triples from a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Yih et al., 2015). Both of these approaches have limitations. Even the largest KBs are incomplete (Min et al., 2013), which limits recall of a KB-based QA system. On the other hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult (Chen et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Yang et al., 2018). In this paper we follow previous research (Sawant et al., 2019; Sun et al., 2018) in deriving answers using both a corpus and a KB. We focus on tasks in which questions require compositional (sometimes called “multi-hop”) reasoning, and a setting in which the KB is incomplete, and hence must be supplemented with information extracted from text. We also restrict ourselves in this paper to answers which correspond to KB entities. For this setting, we propose an integrated framework for (1) learning what to retrieve, from either"
D19-1242,P17-1147,0,0.257765,"ess allows us to answer multi-hop questions using large KBs and corpora. PullNet is weakly supervised, requiring question-answer pairs but not gold inference paths. Experimentally PullNet improves over the prior state-ofthe art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting. 1 Introduction Open domain Question Answering (QA) is the task of finding answers to questions posed in natural language, usually using text from a corpus (Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017), or triples from a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Yih et al., 2015). Both of these approaches have limitations. Even the largest KBs are incomplete (Min et al., 2013), which limits recall of a KB-based QA system. On the other hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult (Chen et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Yang et al., 2018). In this paper we follow previous research (Sawant et al., 2019; Sun et al., 2018) in deriving"
D19-1242,Q19-1026,0,0.0436202,"task of finding answers to questions posed in natural language, usually using text from a corpus (Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017), or triples from a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Yih et al., 2015). Both of these approaches have limitations. Even the largest KBs are incomplete (Min et al., 2013), which limits recall of a KB-based QA system. On the other hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult (Chen et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Yang et al., 2018). In this paper we follow previous research (Sawant et al., 2019; Sun et al., 2018) in deriving answers using both a corpus and a KB. We focus on tasks in which questions require compositional (sometimes called “multi-hop”) reasoning, and a setting in which the KB is incomplete, and hence must be supplemented with information extracted from text. We also restrict ourselves in this paper to answers which correspond to KB entities. For this setting, we propose an integrated framework for (1) learning what to retrieve, from either a corpus, a KB, or a combination, and (2) comb"
D19-1242,D12-1093,1,0.801798,"ntroducing a question subgraph built with facts and text, and uses a learned graph representation (Kipf and Welling, 2016; Li et al., 2016; Schlichtkrull et al., 2017; Scarselli et al., 2009) to perform the reasoning required to select the answer. We use the same representation and reasoning scheme as GRAFTNet, but do not require that the entire graph be retrieved in a single step. In our experimental comparisons, this gives significant performance gains for multi-hop reasoning tasks. Combinations of KBs and text have also been used for relation extraction and Knowledge Base Completion (KBC) (Lao et al., 2012; Toutanova et al., 2015; Das et al., 2017). The QA task differs from KBC in that in QA, the inference process must be conditioned on a natural-language question, which leads to different constraints on which methods can be used. 3 The PullNet Model PullNet retrieves from two “knowledge sources”, a text corpus and a KB. Given a question, PullNet will use these to construct a question subgraph that can be used to answer the question. The question subgraph is constructed iteratively. Initially the subgraph depends only on the question. PullNet then iteratively expands the subgraph by choosing no"
D19-1242,P17-1003,0,0.146436,"Missing"
D19-1242,D16-1147,0,0.568015,"ncomplete KB only, and incomplete KB paired with the corpus. In the complete KB only setting, the answer always exists in knowledge base: for all of these datasets, this is true because the questions were crowd-sourced to enforce this conditions. This is the easiest setting for QA, but arguably unrealistic, since with a more natural distribution of ques4.1 Experiments and Results Datasets MetaQA (Zhang et al., 2018) contains more than 400k single and multi-hop (up to 3-hop) questions in the domain of movies. The questions were constructed using the knowledge base provided with the WikiMovies (Miller et al., 2016) dataset. We use the “vanilla” version of the queries2 We use the KB and text corpus supplied with the WikiMovies dataset, and use exact match on surface forms to perform entity linking. The KB used here is relatively small, with about 43k entities and 135k 2 For this version, the 1-hop questions in MetaQA are exactly the same as WikiMovies. MetaQA 1-hop MetaQA 2-hop MetaQA 3-hop WebQuestionsSP Complex WebQ Train 96,106 118,980 114,196 2,848 27,623 Dev 9,992 14,872 14,274 250 3,518 Test 9,947 14,872 14,274 1,639 3,531 Table 1: Statistics of all datasets. 4.2 Tasks 3 We use the same train/dev/t"
D19-1242,N13-1095,0,0.0221441,"in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting. 1 Introduction Open domain Question Answering (QA) is the task of finding answers to questions posed in natural language, usually using text from a corpus (Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017), or triples from a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Yih et al., 2015). Both of these approaches have limitations. Even the largest KBs are incomplete (Min et al., 2013), which limits recall of a KB-based QA system. On the other hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult (Chen et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Yang et al., 2018). In this paper we follow previous research (Sawant et al., 2019; Sun et al., 2018) in deriving answers using both a corpus and a KB. We focus on tasks in which questions require compositional (sometimes called “multi-hop”) reasoning, and a setting in which the KB is incomplete, and hence must be supplemented with information"
D19-1242,D18-1052,0,0.0246343,"8). Generally, these “reading comprehension” systems are operated by encoding the passage and question into an embedding space, and due to memory limitations cannot be applied to a large corpus instead of a short passage. To address this limitation a number of systems have been designed which use a “retrieve and read” pipeline (Chen et al., 2017; Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Wang et al., 2018, 2017), in which a retrieval system with high recall is piped into a reading comprehension system that can find the answer. An alternative approach is “phrase-indexed” QA (Seo et al., 2018), where embedded phrases in a document are indexed and searched over. Existing systems are also not able to use both KB and text for QA. They also differ from PullNet in using only a single round of retrieval; however, for questions that require multi-hop reasoning, it is difficult for a single retrieval step to find the relevant information. SplitQA (Talmor and Berant, 2018) is a textbased QA system that decomposes complex questions (e.g., with conjunction or composition) into simple subquestions, and performs retrieval sequentially for the subquestions. Although it uses iterative retrieval f"
D19-1242,D18-1455,1,0.859949,"al., 2017; Joshi et al., 2017; Dunn et al., 2017), or triples from a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Yih et al., 2015). Both of these approaches have limitations. Even the largest KBs are incomplete (Min et al., 2013), which limits recall of a KB-based QA system. On the other hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult (Chen et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Yang et al., 2018). In this paper we follow previous research (Sawant et al., 2019; Sun et al., 2018) in deriving answers using both a corpus and a KB. We focus on tasks in which questions require compositional (sometimes called “multi-hop”) reasoning, and a setting in which the KB is incomplete, and hence must be supplemented with information extracted from text. We also restrict ourselves in this paper to answers which correspond to KB entities. For this setting, we propose an integrated framework for (1) learning what to retrieve, from either a corpus, a KB, or a combination, and (2) combining this heterogeneous information into a single data structure that allows the system to reason and"
D19-1242,N18-1059,0,0.0788292,"Joshi et al., 2017; Dunn et al., 2017; Wang et al., 2018, 2017), in which a retrieval system with high recall is piped into a reading comprehension system that can find the answer. An alternative approach is “phrase-indexed” QA (Seo et al., 2018), where embedded phrases in a document are indexed and searched over. Existing systems are also not able to use both KB and text for QA. They also differ from PullNet in using only a single round of retrieval; however, for questions that require multi-hop reasoning, it is difficult for a single retrieval step to find the relevant information. SplitQA (Talmor and Berant, 2018) is a textbased QA system that decomposes complex questions (e.g., with conjunction or composition) into simple subquestions, and performs retrieval sequentially for the subquestions. Although it uses iterative retrieval for multi-hop questions, unlike PullNet, SplitQA does not also use a KB as an information source. Also, SplitQA has been applied only to the Complex WebQuestions dataset, so it is unclear how general this approach is. There has also been much work on QA from KBs alone, often using methods based on memory networks (Sukhbaatar et al., 2015), semantic parsing (Zelle and Mooney, 1"
D19-1242,D15-1174,0,0.0176041,"ion subgraph built with facts and text, and uses a learned graph representation (Kipf and Welling, 2016; Li et al., 2016; Schlichtkrull et al., 2017; Scarselli et al., 2009) to perform the reasoning required to select the answer. We use the same representation and reasoning scheme as GRAFTNet, but do not require that the entire graph be retrieved in a single step. In our experimental comparisons, this gives significant performance gains for multi-hop reasoning tasks. Combinations of KBs and text have also been used for relation extraction and Knowledge Base Completion (KBC) (Lao et al., 2012; Toutanova et al., 2015; Das et al., 2017). The QA task differs from KBC in that in QA, the inference process must be conditioned on a natural-language question, which leads to different constraints on which methods can be used. 3 The PullNet Model PullNet retrieves from two “knowledge sources”, a text corpus and a KB. Given a question, PullNet will use these to construct a question subgraph that can be used to answer the question. The question subgraph is constructed iteratively. Initially the subgraph depends only on the question. PullNet then iteratively expands the subgraph by choosing nodes from which to “pull”"
D19-1242,Q18-1021,0,0.0291652,"nswering (QA) is the task of finding answers to questions posed in natural language, usually using text from a corpus (Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017), or triples from a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Yih et al., 2015). Both of these approaches have limitations. Even the largest KBs are incomplete (Min et al., 2013), which limits recall of a KB-based QA system. On the other hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult (Chen et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Yang et al., 2018). In this paper we follow previous research (Sawant et al., 2019; Sun et al., 2018) in deriving answers using both a corpus and a KB. We focus on tasks in which questions require compositional (sometimes called “multi-hop”) reasoning, and a setting in which the KB is incomplete, and hence must be supplemented with information extracted from text. We also restrict ourselves in this paper to answers which correspond to KB entities. For this setting, we propose an integrated framework for (1) learning what to retrieve, from either a corpus, a KB, or a"
D19-1242,D18-1259,1,0.877077,"o questions posed in natural language, usually using text from a corpus (Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017), or triples from a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Yih et al., 2015). Both of these approaches have limitations. Even the largest KBs are incomplete (Min et al., 2013), which limits recall of a KB-based QA system. On the other hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult (Chen et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Yang et al., 2018). In this paper we follow previous research (Sawant et al., 2019; Sun et al., 2018) in deriving answers using both a corpus and a KB. We focus on tasks in which questions require compositional (sometimes called “multi-hop”) reasoning, and a setting in which the KB is incomplete, and hence must be supplemented with information extracted from text. We also restrict ourselves in this paper to answers which correspond to KB entities. For this setting, we propose an integrated framework for (1) learning what to retrieve, from either a corpus, a KB, or a combination, and (2) combining this heterogen"
D19-1242,P15-1128,0,0.601425,"but not gold inference paths. Experimentally PullNet improves over the prior state-ofthe art, and in the setting where a corpus is used with incomplete KB these improvements are often dramatic. PullNet is also often superior to prior systems in a KB-only setting or a text-only setting. 1 Introduction Open domain Question Answering (QA) is the task of finding answers to questions posed in natural language, usually using text from a corpus (Dhingra et al., 2017; Joshi et al., 2017; Dunn et al., 2017), or triples from a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Yih et al., 2015). Both of these approaches have limitations. Even the largest KBs are incomplete (Min et al., 2013), which limits recall of a KB-based QA system. On the other hand, while a large corpus may contain more answers than a KB, the diversity of natural language makes corpus-based QA difficult (Chen et al., 2017; Welbl et al., 2018; Kwiatkowski et al., 2019; Yang et al., 2018). In this paper we follow previous research (Sawant et al., 2019; Sun et al., 2018) in deriving answers using both a corpus and a KB. We focus on tasks in which questions require compositional (sometimes called “multi-hop”) reas"
D19-1242,N13-1008,0,\N,Missing
D19-1242,P17-2057,0,\N,Missing
D19-1259,W19-2011,1,0.810125,"eural network on the shallow features instead of using a logistic classifier. BiLSTM: We simply concatenate the question and context/long answer with learnable segment embeddings appended to the biomedical word2vec embeddings (Pyysalo et al., 2013) of each token. The concatenated sentence is then fed to a biLSTM, and the final hidden states of the forward and backward network are used for classifying the yes/no/maybe label. 2572 ESIM with BioELMo: Following the state-ofthe-art recurrent architecture of NLI (Peters et al., 2018), we use pre-trained biomedical contextualized embeddings BioELMo (Jin et al., 2019) for word representations. Then we apply the ESIM model (Chen et al., 2016), where a biLSTM is used to encode the question and context/long answer, followed by an attentional local inference layer and a biLSTM inference composition layer. After pooling, a softmax output unit is applied for predicting the yes/no/maybe label. 4.5 Compared Training Schedules Final Phase Only: Under this setting, we train models only on PQA-L. It’s an extremely low resources setting where there are only 450 training instances in each fold of cross-validation. Phase I + Final Phase: Under this setting, we skip the"
D19-1259,Q18-1023,0,0.136527,"Missing"
D19-1259,Q19-1026,0,0.180618,"experts annotated the Answer yes. Supporting fact for the answer is highlighted. Introduction A long-term goal of natural language understanding is to build intelligent systems that can reason and infer over natural language. The question answering (QA) task, in which models learn how to answer questions, is often used as a benchmark for quantitatively measuring the reasoning and inferring abilities of such intelligent systems. While many large-scale annotated general domain QA datasets have been introduced (Rajpurkar et al., 2016; Lai et al., 2017; Koˇcisk`y et al., 2018; Yang et al., 2018; Kwiatkowski et al., 2019), the largest annotated biomedical QA dataset, BioASQ (Tsatsaronis et al., 2015) has less than 3k training instances, most of which are simple factual questions. Some works proposed automatically constructed biomedical QA datasets (Pampari et al., 2018; Pappas et al., 2018; Kim et al., 2018), which have much larger sizes. However, questions of these datasets are mostly factoid, whose answers can be extracted in the contexts without much reasoning. In this paper, we aim at building a biomedical QA dataset which (1) has substantial instances with some expert annotations and (2) requires reasonin"
D19-1259,N19-1300,0,0.0117199,"i et al., 2018) is an extractive QA dataset for electronic medical records (EHR) built by re-purposing existing annotations on EHR corpora. BioRead (Pappas et al., 2018) and BMKC (Kim et al., 2018) both collect cloze-style QA instances by masking biomedical named entities in sentences of research articles and using other parts of the same article as context. Yes/No QA: Datasets such as HotpotQA (Yang et al., 2018), Natural Questions (Kwiatkowski et al., 2019), ShARC (Saeidi et al., 2018) and BioASQ (Tsatsaronis et al., 2015) contain yes/no questions as well as other types of questions. BoolQ (Clark et al., 2019) specifically focuses on naturally occurring yes/no questions, and those questions are shown to be surprisingly difficult to answer. We add a “maybe” choice in PubMedQA to cover uncertain instances. Typical neural approaches to answering yes/no questions involve encoding both the question and context, and decoding the encoding to a class output, which is similar to the well-studied natural language inference (NLI) task. Recent breakthroughs of pre-trained language models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) show significant performance im2568 2 https://trec.nist.gov/"
D19-1259,P18-2053,0,0.0269679,"arious biomedical NLP tasks. We denote the original transformer weights of BioBERT as θ0 . While fine-tuning, we feed PubMedQA questions and contexts (or long answers), separated 7 by the special [SEP] token, to BioBERT. The yes/no/maybe labels are predicted using the special [CLS] embedding using a softmax function. Cross-entropy loss of predicted and true label distribution is denoted as LQA . 4.2 Long Answer as Additional Supervision Under reasoning-required setting, long answers are available in training but not inference phase. We use them as an additional signal for training: similar to Ma et al. (2018) regularizing neural machine translation models with binary bag-of-word (BoW) statistics, we fine-tune BioBERT with an auxiliary task of predicting the binary BoW statistics of the long answers, also using the special [CLS] embedding. We minimize binary crossentropy loss of this auxiliary task: 1 X LBoW = − bi logbˆi + (1 − bi )log(1 − bˆi ) N i where bi and bˆi are ground-truth and predicted probability of whether token i is in the long answers (i.e.: bi ∈ {0, 1} and bˆi ∈ [0, 1]), and N is the BoW vocabulary size. The total loss is: L = LQA + βLBoW https://www.ncbi.nlm.nih.gov/pmc/ 2571 Trai"
D19-1259,P14-5010,0,0.00427134,"Missing"
D19-1259,D18-1258,0,0.265676,"sk, in which models learn how to answer questions, is often used as a benchmark for quantitatively measuring the reasoning and inferring abilities of such intelligent systems. While many large-scale annotated general domain QA datasets have been introduced (Rajpurkar et al., 2016; Lai et al., 2017; Koˇcisk`y et al., 2018; Yang et al., 2018; Kwiatkowski et al., 2019), the largest annotated biomedical QA dataset, BioASQ (Tsatsaronis et al., 2015) has less than 3k training instances, most of which are simple factual questions. Some works proposed automatically constructed biomedical QA datasets (Pampari et al., 2018; Pappas et al., 2018; Kim et al., 2018), which have much larger sizes. However, questions of these datasets are mostly factoid, whose answers can be extracted in the contexts without much reasoning. In this paper, we aim at building a biomedical QA dataset which (1) has substantial instances with some expert annotations and (2) requires reasoning over the contexts to answer the questions. For this, we turn to the PubMed1 , a search engine providing access to over 25 million references of 1 https://www.ncbi.nlm.nih.gov/pubmed/ 2567 Proceedings of the 2019 Conference on Empirical Methods in Nat"
D19-1259,L18-1439,0,0.157108,"arn how to answer questions, is often used as a benchmark for quantitatively measuring the reasoning and inferring abilities of such intelligent systems. While many large-scale annotated general domain QA datasets have been introduced (Rajpurkar et al., 2016; Lai et al., 2017; Koˇcisk`y et al., 2018; Yang et al., 2018; Kwiatkowski et al., 2019), the largest annotated biomedical QA dataset, BioASQ (Tsatsaronis et al., 2015) has less than 3k training instances, most of which are simple factual questions. Some works proposed automatically constructed biomedical QA datasets (Pampari et al., 2018; Pappas et al., 2018; Kim et al., 2018), which have much larger sizes. However, questions of these datasets are mostly factoid, whose answers can be extracted in the contexts without much reasoning. In this paper, we aim at building a biomedical QA dataset which (1) has substantial instances with some expert annotations and (2) requires reasoning over the contexts to answer the questions. For this, we turn to the PubMed1 , a search engine providing access to over 25 million references of 1 https://www.ncbi.nlm.nih.gov/pubmed/ 2567 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process"
D19-1259,N18-1202,0,0.0484365,"satsaronis et al., 2015) contain yes/no questions as well as other types of questions. BoolQ (Clark et al., 2019) specifically focuses on naturally occurring yes/no questions, and those questions are shown to be surprisingly difficult to answer. We add a “maybe” choice in PubMedQA to cover uncertain instances. Typical neural approaches to answering yes/no questions involve encoding both the question and context, and decoding the encoding to a class output, which is similar to the well-studied natural language inference (NLI) task. Recent breakthroughs of pre-trained language models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) show significant performance im2568 2 https://trec.nist.gov/ PQA-Artificial (211.3k) PQA-Unlabeled (61.2k) PQA-Labeled (1k) Ori. Title -> Question Ori. Question Title Ori. Question Title Structured Context (Ori. Abstract w/o conclusion) Structured Context (Ori. Abstract w/o conclusion) Structured Context (Ori. Abstract w/o conclusion) Long Answer (Conclusion) Long Answer (Conclusion) Long Answer (Conclusion) Generated yes/no Unlabeled Yes/no/maybe Algorithm 1 PQA-L data collection procedure Input: pre-PQA-U ReasoningFreeAnnotation ← {} ReasoningRequiredAnnotatio"
D19-1259,D16-1264,0,0.185118,"tructured abstract except its conclusive part, which serves as the Long Answer; Human experts annotated the Answer yes. Supporting fact for the answer is highlighted. Introduction A long-term goal of natural language understanding is to build intelligent systems that can reason and infer over natural language. The question answering (QA) task, in which models learn how to answer questions, is often used as a benchmark for quantitatively measuring the reasoning and inferring abilities of such intelligent systems. While many large-scale annotated general domain QA datasets have been introduced (Rajpurkar et al., 2016; Lai et al., 2017; Koˇcisk`y et al., 2018; Yang et al., 2018; Kwiatkowski et al., 2019), the largest annotated biomedical QA dataset, BioASQ (Tsatsaronis et al., 2015) has less than 3k training instances, most of which are simple factual questions. Some works proposed automatically constructed biomedical QA datasets (Pampari et al., 2018; Pappas et al., 2018; Kim et al., 2018), which have much larger sizes. However, questions of these datasets are mostly factoid, whose answers can be extracted in the contexts without much reasoning. In this paper, we aim at building a biomedical QA dataset wh"
D19-1259,D18-1233,0,0.0928487,"Missing"
D19-1259,D18-1259,1,0.935587,"Long Answer; Human experts annotated the Answer yes. Supporting fact for the answer is highlighted. Introduction A long-term goal of natural language understanding is to build intelligent systems that can reason and infer over natural language. The question answering (QA) task, in which models learn how to answer questions, is often used as a benchmark for quantitatively measuring the reasoning and inferring abilities of such intelligent systems. While many large-scale annotated general domain QA datasets have been introduced (Rajpurkar et al., 2016; Lai et al., 2017; Koˇcisk`y et al., 2018; Yang et al., 2018; Kwiatkowski et al., 2019), the largest annotated biomedical QA dataset, BioASQ (Tsatsaronis et al., 2015) has less than 3k training instances, most of which are simple factual questions. Some works proposed automatically constructed biomedical QA datasets (Pampari et al., 2018; Pappas et al., 2018; Kim et al., 2018), which have much larger sizes. However, questions of these datasets are mostly factoid, whose answers can be extracted in the contexts without much reasoning. In this paper, we aim at building a biomedical QA dataset which (1) has substantial instances with some expert annotation"
H05-1056,P04-1056,0,0.173355,"ct There has been little prior work on Named Entity Recognition for ”informal” documents like email. We present two methods for improving performance of person name recognizers for email: emailspecific structural features and a recallenhancing method which exploits name repetition across multiple documents. 1 Introduction Named entity recognition (NER), the identification of entity names in free text, is a well-studied problem. In most previous work, NER has been applied to news articles (e.g., (Bikel et al., 1999; McCallum and Li, 2003)), scientific articles (e.g., (Craven and Kumlien, 1999; Bunescu and Mooney, 2004)), or web pages (e.g., (Freitag, 1998)). These genres of text share two important properties: documents are written for a fairly broad audience, and writers take care in preparing documents. Important genres that do not share these properties include instant messaging logs, newsgroup postings and email messages. We refer to these genres as “informal” text. Informal text is harder to process automatically. Informal documents do not obey strict grammatical conventions. They contain grammatical and spelling errors. Further, since the audience is more restricted, informal documents often use group"
H05-1056,M98-1007,0,0.0756083,"are shown to significantly improve performance on our corpora. We also present and evaluate a novel method for exploiting repetition of names in a test corpus. Techniques for exploiting name repetition within documents have been recently applied to newswire text 1 Two of these are publicly available. The others can not be distributed due to privacy considerations. 443 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 443–450, Vancouver, October 2005. 2005 Association for Computational Linguistics (e.g., (Humphreys et al., 1998)), scientific abstracts (e.g., (Bunescu and Mooney, 2004)) and seminar announcements (Sutton and Mccallum, 2004); however, these techniques rely on either NP analysis or capitalization information to pre-identify candidate coreferent name mentions, features which are not reliable in email. Furthermore, we argue that name repetition in email should be inferred by examining multiple documents in a corpus, which is not common practice. We therefore present an alternative efficient scheme for increasing recall in email, using the whole corpus. This technique is shown to always improve recall subst"
H05-1056,W03-0430,0,0.0615856,"arnegie Mellon University Pittsburgh, PA 15217 {einat,rcwang}@cs.cmu.edu Abstract There has been little prior work on Named Entity Recognition for ”informal” documents like email. We present two methods for improving performance of person name recognizers for email: emailspecific structural features and a recallenhancing method which exploits name repetition across multiple documents. 1 Introduction Named entity recognition (NER), the identification of entity names in free text, is a well-studied problem. In most previous work, NER has been applied to news articles (e.g., (Bikel et al., 1999; McCallum and Li, 2003)), scientific articles (e.g., (Craven and Kumlien, 1999; Bunescu and Mooney, 2004)), or web pages (e.g., (Freitag, 1998)). These genres of text share two important properties: documents are written for a fairly broad audience, and writers take care in preparing documents. Important genres that do not share these properties include instant messaging logs, newsgroup postings and email messages. We refer to these genres as “informal” text. Informal text is harder to process automatically. Informal documents do not obey strict grammatical conventions. They contain grammatical and spelling errors."
H05-1056,W98-1118,0,\N,Missing
H05-1056,W02-1001,0,\N,Missing
H05-1056,W02-2019,0,\N,Missing
H05-1056,W03-0435,0,\N,Missing
H05-1056,P95-1026,0,\N,Missing
H05-1056,P01-1039,0,\N,Missing
H05-1056,J95-4004,0,\N,Missing
H05-1056,N03-1028,0,\N,Missing
H05-1056,N04-4028,0,\N,Missing
H05-1056,W02-1041,0,\N,Missing
H05-1056,A00-1040,0,\N,Missing
H05-1056,P02-1062,0,\N,Missing
H05-1056,W04-3240,1,\N,Missing
N06-2024,W02-1001,0,0.034944,"split such that its test set contains a different mix of entity names comparing to training exmaples. Further details about these datasets are available elsewhere (Minkov et al., 2005). 70 0.2 0.5 1.0 2.0 5.0 65 60 55 50 0.2 0.5 1.0 2.0 5.0 Beta # tokens 204,071 204,423 104,662 # names per doc. 6.8 3.0 3.7 90 0.2 0.5 1.0 2.0 5.0 85 80 75 Table 1: Summary of the corpora used in the experiments F(Beta) MUC-6 Enron Mgmt-Groups # documents Train Test 347 30 833 143 631 128 70 65 60 We used an implementation of Collins’ votedpercepton method for discriminatively training HMMs (henceforth, VP-HMM) (Collins, 2002) as well as CRF (Lafferty et al., 2001) to learn a NER. Both VP-HMM and CRF were trained for 20 epochs on every dataset, using a simple set of features such as word identity and capitalization patterns for a window of three words around each word being classified. Each word is classified as either inside or outside a person name.4 55 50 0.2 0.5 4 This problem encoding is basic. However, in the context of this paper we focus on precision-recall trade-off in the general case, avoiding settings’ optimization. 5 E.g, the token-level F2 curve peaks at β = 5. 95 2.0 5.0 Beta Figure 1: Results of tok"
N06-2024,N04-4028,0,0.126998,"d to optimize other measures (e.g., loglikelihood of the training data for CRFs). Obviously, different applications of NER have different requirements for precision and recall. A One way to manipulate an extractor’s precisionrecall tradeoff is to assign a confidence score to each extracted entity and then apply a global threshold to confidence level. However, confidence thresholding of this sort cannot increase recall. Also, while confidence scores are straightforward to compute in many classification settings, there is no inherent mechanism for computing confidence of a sequential extractor. Culotta and McCallum (2004) suggest several methods for doing this with CRFs. In this paper, we suggest an alternative simple method for exploring and optimizing the relationship between precision and recall for NER systems. In particular, we describe and evaluate a technique called “extractor tweaking” that optimizes a learned extractor with respect to a specific evaluation metric. In a nutshell, we directly tweak the threashold term that is part of any linear classifier, including sequential extractors. Though simple, this approach has not been empirically evaluated before, to our knowledge. Further, although sequenti"
N06-2024,W03-0430,0,0.0271552,"is evaluated on the task of recognizing personal names in email and newswire text, and proves to be both simple and effective. 1 William W. Cohen Machine Learning Dept. Carnegie Mellon University Introduction Named entity recognition (NER) is the task of identifying named entities in free text—typically personal names, organizations, gene-protein entities, and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used successfully for a number of applications, including NER (Sha and Pereira, 2003; Pinto et al., 2003; Mccallum and Lee, 2003). In practice, these methods provide imperfect performance: precision and recall, even for well-studied problems on clean wellwritten text, reach at most the mid-90’s. While performance of NER systems is often evaluated in terms of F 1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall. Furthermore, learned NER models may be sub-optimal also in terms of F1, as they are trained to optimize other measures (e.g., loglikelihood of the training data for CRFs). Obviously, different applications of NER have different requireme"
N06-2024,H05-1056,1,0.773602,"gmt-Groups dataset is a second email 2 from http://billharlan.com/pub/code/inv. In the experiments, this is usually within around 10 iterations. Each iteration requires evaluating a “tweaked” extractor on a training set. 3 90 85 80 75 F(Beta) collection, extracted from the CSpace email corpus, which contains email messages sent by MBA students taking a management course conducted at Carnegie Mellon University in 1997. This data was split such that its test set contains a different mix of entity names comparing to training exmaples. Further details about these datasets are available elsewhere (Minkov et al., 2005). 70 0.2 0.5 1.0 2.0 5.0 65 60 55 50 0.2 0.5 1.0 2.0 5.0 Beta # tokens 204,071 204,423 104,662 # names per doc. 6.8 3.0 3.7 90 0.2 0.5 1.0 2.0 5.0 85 80 75 Table 1: Summary of the corpora used in the experiments F(Beta) MUC-6 Enron Mgmt-Groups # documents Train Test 347 30 833 143 631 128 70 65 60 We used an implementation of Collins’ votedpercepton method for discriminatively training HMMs (henceforth, VP-HMM) (Collins, 2002) as well as CRF (Lafferty et al., 2001) to learn a NER. Both VP-HMM and CRF were trained for 20 epochs on every dataset, using a simple set of features such as word ident"
N06-2024,N03-1028,0,0.0179906,"provided performance criterion. This method is evaluated on the task of recognizing personal names in email and newswire text, and proves to be both simple and effective. 1 William W. Cohen Machine Learning Dept. Carnegie Mellon University Introduction Named entity recognition (NER) is the task of identifying named entities in free text—typically personal names, organizations, gene-protein entities, and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used successfully for a number of applications, including NER (Sha and Pereira, 2003; Pinto et al., 2003; Mccallum and Lee, 2003). In practice, these methods provide imperfect performance: precision and recall, even for well-studied problems on clean wellwritten text, reach at most the mid-90’s. While performance of NER systems is often evaluated in terms of F 1 measure (a harmonic mean of precision and recall), this measure may not match user preferences regarding precision and recall. Furthermore, learned NER models may be sub-optimal also in terms of F1, as they are trained to optimize other measures (e.g., loglikelihood of the training data for CRFs). Obviously, different"
N09-1054,P08-1031,0,0.0234631,"patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more gen"
N09-1054,P08-1036,0,0.0273657,"ed text analysis tool. The basic aim of those models is to discover recurring patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of t"
N09-1054,P08-1000,0,\N,Missing
N13-1080,P07-1056,1,0.766444,"t the domains induced by that metadata attribute. Each instance xi is drawn from a distribution xi ∼ Da specific to a set of attribute values Ai associated with each instance. Additionally, each unique set of attributes indexes a function fA .1 Ai could contain a value for each attribute, or no values for any attribute (which would index a domain-agnostic “background” distribution and labeling function). Just as a domain can change a feature’s probability and behavior, so can each metadata attribute. Examples of data for MAMD learning abound. The commonly used Amazon product reviews data set (Blitzer et al., 2007) only includes product types, but the original reviews can be attributed with author, product price, brand, and so on. Additional examples include congressional floor debate records (e.g. political party, speaker, bill) (Joshi et al., 2012). In this paper, we use restaurant reviews (Chahuneau et al., 2012), which have upto 20 metadata attributes that define domains, and congressional floor debates, with two attributes that define domains. It is difficult to apply multi-domain learning algorithms when it is unclear which metadata attribute to choose for defining the “domains”. It is possible th"
N13-1080,D12-1124,0,0.333936,"metadata attributes). We introduce the multi-attribute multi-domain (MAMD) learning problem, in which each learning instance is associated with multiple metadata attributes, each of which may impact feature behavior. We present extensions to two popular multi-domain learning algorithms, FEDA (Daum´e III, 2007) and MDR (Dredze et al., 2009). Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes. We evaluate our algorithms using two different data sets – a data set of restaurant reviews (Chahuneau et al., 2012), and a dataset of transcribed speech segments from floor debates in the United States Congress (Thomas et al., 2006). We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute. 2 MAMD Learning In multi-domain learning, each instance x is drawn from a domain d with distribution x ∼ Dd over a vectors space RD and labeled with a domain specific function fd with label y ∈ {−1, +1} (for binary classification). In multi-attribute multi-domain 685 Proceedings of NAACL-HLT 2013, pages 685–690, c Atlanta, Geor"
N13-1080,P07-1033,0,0.690089,"Missing"
N13-1080,D08-1072,1,0.890708,"-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain le"
N13-1080,N09-1068,0,0.194056,"multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain learning, we should use the"
N13-1080,D12-1119,1,0.876767,"Missing"
N13-1080,W06-1639,0,0.150433,"ance is associated with multiple metadata attributes, each of which may impact feature behavior. We present extensions to two popular multi-domain learning algorithms, FEDA (Daum´e III, 2007) and MDR (Dredze et al., 2009). Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes. We evaluate our algorithms using two different data sets – a data set of restaurant reviews (Chahuneau et al., 2012), and a dataset of transcribed speech segments from floor debates in the United States Congress (Thomas et al., 2006). We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute. 2 MAMD Learning In multi-domain learning, each instance x is drawn from a domain d with distribution x ∼ Dd over a vectors space RD and labeled with a domain specific function fd with label y ∈ {−1, +1} (for binary classification). In multi-attribute multi-domain 685 Proceedings of NAACL-HLT 2013, pages 685–690, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics (MAMD) learning, we have M metadata attributes in"
N13-1080,P12-1078,0,0.0137892,"is possible that there is a single “best” attribute to use for defining domains, one that when used in multi-domain learning will yield the best classifier. To find this attribute, one must rely on one’s intuition about the problem,2 or perform an exhaustive empirical search over all attributes using some validation set. Both these strategies can be brittle, because as the nature of data changes over time so may the “best” domain distinction. Additionally, multi-domain learning was not designed to benefit from multiple helpful attributes. We note here that Eisenstein et al. (2011), as well as Wang et al. (2012), worked with a “multifaceted topic model” using the framework of sparse additive generative models (SAGE). Both those models capture interactions between topics and multiple as1 Distributions and functions that share attributes could share parameters. 2 Intuition is often critical for learning and in some cases can help, such as in the Amazon product reviews data set, where product type clearly corresponds to domain. However, for other data sets the choice may be less clear. 686 pects, and can be adapted to the case of MAMD. While our problem formulation has significant conceptual overlap wit"
N16-4005,P15-1035,1,0.882666,"Missing"
N16-4005,D14-1122,1,0.870886,"Missing"
N18-2007,D14-1162,0,0.07996,"Missing"
N18-2007,P17-1168,1,0.943308,"coreference annotations, we see small but significant improvements over a state-of-the-art baseline. As we reduce the training data, the gains become larger. Lastly, we apply the same model to a broad-context language modeling task (Paperno et al., 2016), where coreference resolution is an important factor, and show improved performance over state-of-the-art. 2 hft hfyt … f ht 1 xt hbt0 hbyt0 b xt0 ht0 +1 … … Mary went … … she Mary went … she Figure 2: Forward (left) and Backward (right) CorefGRU layers. Mary and she are coreferent. Part of this work was described in an unpublished preprint (Dhingra et al., 2017b). The current paper extends that version and focuses exclusively on coreference relations. We also report results on the WikiHop dataset, including the performance of the model in the low-data regime. Related Work Entity-based models. Ji et al. (2017) presented a generative model for jointly predicting the next word in the text and its gold-standard coreference annotation. The difference in our work is that we look at the task of reading comprehension, and also work in the more practical setting of system extracted coreferences. EntNets (Henaff et al., 2016) also maintain dynamic memory slot"
N18-2007,P15-1150,0,0.0162433,"leads to poor performance in the low-data regime (c.f. Table 1). Yang et al. (2017) model references in text as explicit latent variables, but limit their work to text generation. Kobayashi et al. (2016) used a pooling operation to aggregate entity information across multiple mentions. Wang et al. (2017) also noted the importance of reference resolution for reading comprehension, and we compare our model to their one-hot pointer reader. Syntactic-recency. Recent work has used syntax, in the form of dependency trees, to replace the sequential recency bias in RNNs with a syntactic recency bias (Tai et al., 2015; Swayamdipta, 2017; Qian et al., 2017; Chen et al., 2017). However, syntax only looks at dependencies within sentence boundaries, whereas our focus here is on longer ranges. Our resulting layer is structurally similar to GraphLSTMs (Peng et al., 2017), with an additional attention mechanism over the graph edges. However, while Peng et al. (2017) found that using coreference did not lead to any gains for the task of relation extraction, here we show that it has a positive impact on the reading comprehension task. Self-Attention (Vaswani et al., 2017) models are becoming popular for modeling lo"
N18-2007,D17-1195,0,0.0300667,"where coreference resolution is an important factor, and show improved performance over state-of-the-art. 2 hft hfyt … f ht 1 xt hbt0 hbyt0 b xt0 ht0 +1 … … Mary went … … she Mary went … she Figure 2: Forward (left) and Backward (right) CorefGRU layers. Mary and she are coreferent. Part of this work was described in an unpublished preprint (Dhingra et al., 2017b). The current paper extends that version and focuses exclusively on coreference relations. We also report results on the WikiHop dataset, including the performance of the model in the low-data regime. Related Work Entity-based models. Ji et al. (2017) presented a generative model for jointly predicting the next word in the text and its gold-standard coreference annotation. The difference in our work is that we look at the task of reading comprehension, and also work in the more practical setting of system extracted coreferences. EntNets (Henaff et al., 2016) also maintain dynamic memory slots for entities, but do not use coreference signals and instead update all memories after reading each sentence, which leads to poor performance in the low-data regime (c.f. Table 1). Yang et al. (2017) model references in text as explicit latent variabl"
N18-2007,N16-1099,0,0.0313563,"dicting the next word in the text and its gold-standard coreference annotation. The difference in our work is that we look at the task of reading comprehension, and also work in the more practical setting of system extracted coreferences. EntNets (Henaff et al., 2016) also maintain dynamic memory slots for entities, but do not use coreference signals and instead update all memories after reading each sentence, which leads to poor performance in the low-data regime (c.f. Table 1). Yang et al. (2017) model references in text as explicit latent variables, but limit their work to text generation. Kobayashi et al. (2016) used a pooling operation to aggregate entity information across multiple mentions. Wang et al. (2017) also noted the importance of reference resolution for reading comprehension, and we compare our model to their one-hot pointer reader. Syntactic-recency. Recent work has used syntax, in the form of dependency trees, to replace the sequential recency bias in RNNs with a syntactic recency bias (Tai et al., 2015; Swayamdipta, 2017; Qian et al., 2017; Chen et al., 2017). However, syntax only looks at dependencies within sentence boundaries, whereas our focus here is on longer ranges. Our resultin"
N18-2007,D17-1018,0,0.0280598,"he football before the hallway ? Context: Louis-Philippe Fiset [...] was a local physician and politician in the Mauricie area [...] is located in the Mauricie region of Quebec, Canada [...] Question: country of citizenship – louis-philippe fiset ? Figure 1: Example questions which require coreference-based reasoning from the bAbi dataset (top) and Wikihop dataset (bottom). Coreferences are in bold, and the correct answers are underlined. At the same time, systems for coreference resolution have seen a gradual increase in accuracy over the years (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017). Hence, in this work we use the annotations produced by such systems to adapt a standard RNN layer by introducing a bias towards coreferent recency. Specifically, given an input sequence and coreference clusters extracted from an external system, we introduce a term in the update equations for Gated Recurrent Units (GRU) (Cho et al., 2014) which depends on the hidden state of the coreferent antecedent of the current token (if it exists). This way hidden states are propagated along coreference chains and the original sequence in parallel. We compare our Coref-GRU layer with the regular GRU lay"
N18-2007,D16-1147,0,0.0310226,"rds coreferent recency instead: f (W xt + αt U φs (ht−1 ) + (1 − αt )U 0 φc (hyt ) + b), where hyt is the hidden state of the coreferent antecedent of wt (with h0 = 0), φs and φc are nonlinear functions applied to the hidden states coming from the sequential antecedent and the coreferent antecedent, respectively, and αt is a scalar weight which decides the relative importance of the two terms based on the current input (so that, for example, pronouns may assign a higher weight for the coreference state). When yt = 0, αt is set to 1, otherwise it is computed using a keybased addressing scheme (Miller et al., 2016), as αt = softmax(xTt k), where k is a trainable key 43 correct answer from C, and all models are trained using cross-entropy loss. When comparing two models we ensure the number of parameters are the same in each. Other implementation details are listed in Appendix B. vector. In this work we use simple slicing functions φs (x) = x[1 : d/2], and φc (x) = x[d/2 : d] which decompose the hidden states into a sequential and a coreferent component, respectively. Figure 2 (left) shows an illustration of the layer, and the full update equations are given in Appendix A. 4 Connection to Memory Networks"
N18-2007,P16-1144,0,0.0412594,"Missing"
N18-2007,N16-1114,0,0.028587,"Question: where was the football before the hallway ? Context: Louis-Philippe Fiset [...] was a local physician and politician in the Mauricie area [...] is located in the Mauricie region of Quebec, Canada [...] Question: country of citizenship – louis-philippe fiset ? Figure 1: Example questions which require coreference-based reasoning from the bAbi dataset (top) and Wikihop dataset (bottom). Coreferences are in bold, and the correct answers are underlined. At the same time, systems for coreference resolution have seen a gradual increase in accuracy over the years (Durrett and Klein, 2013; Wiseman et al., 2016; Lee et al., 2017). Hence, in this work we use the annotations produced by such systems to adapt a standard RNN layer by introducing a bias towards coreferent recency. Specifically, given an input sequence and coreference clusters extracted from an external system, we introduce a term in the update equations for Gated Recurrent Units (GRU) (Cho et al., 2014) which depends on the hidden state of the coreferent antecedent of the current token (if it exists). This way hidden states are propagated along coreference chains and the original sequence in parallel. We compare our Coref-GRU layer with"
N18-2007,Q17-1008,0,0.0374319,"nformation across multiple mentions. Wang et al. (2017) also noted the importance of reference resolution for reading comprehension, and we compare our model to their one-hot pointer reader. Syntactic-recency. Recent work has used syntax, in the form of dependency trees, to replace the sequential recency bias in RNNs with a syntactic recency bias (Tai et al., 2015; Swayamdipta, 2017; Qian et al., 2017; Chen et al., 2017). However, syntax only looks at dependencies within sentence boundaries, whereas our focus here is on longer ranges. Our resulting layer is structurally similar to GraphLSTMs (Peng et al., 2017), with an additional attention mechanism over the graph edges. However, while Peng et al. (2017) found that using coreference did not lead to any gains for the task of relation extraction, here we show that it has a positive impact on the reading comprehension task. Self-Attention (Vaswani et al., 2017) models are becoming popular for modeling long-term dependencies, and may also benefit from coreference information to bias the learning of those dependencies. Here we focus on recurrent layers and leave such an analysis to future work. 3 Model Coref-GRU (C-GRU) Layer. Suppose we are given an in"
N18-2007,D17-1197,0,0.0290232,"he low-data regime. Related Work Entity-based models. Ji et al. (2017) presented a generative model for jointly predicting the next word in the text and its gold-standard coreference annotation. The difference in our work is that we look at the task of reading comprehension, and also work in the more practical setting of system extracted coreferences. EntNets (Henaff et al., 2016) also maintain dynamic memory slots for entities, but do not use coreference signals and instead update all memories after reading each sentence, which leads to poor performance in the low-data regime (c.f. Table 1). Yang et al. (2017) model references in text as explicit latent variables, but limit their work to text generation. Kobayashi et al. (2016) used a pooling operation to aggregate entity information across multiple mentions. Wang et al. (2017) also noted the importance of reference resolution for reading comprehension, and we compare our model to their one-hot pointer reader. Syntactic-recency. Recent work has used syntax, in the form of dependency trees, to replace the sequential recency bias in RNNs with a syntactic recency bias (Tai et al., 2015; Swayamdipta, 2017; Qian et al., 2017; Chen et al., 2017). However"
N18-2007,D13-1203,0,\N,Missing
N18-2007,E17-2009,0,\N,Missing
P08-1029,W06-1615,0,0.0707766,"first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold 252 et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of these meta-level features often found in NLP tasks. Daum´e allows an extra degree of freedom among the features of his domains, implicitly creating a two-level feature hierarchy with one branch for general features, and another for domain specific ones, but does not extend his hierarchy further (Daum´e III, 2007)). Similarly, work on hierarchical penalization ("
P08-1029,M98-1018,0,0.0216594,"C6 (b) CAT: tuned on MUC6+7 (c) HIER: MUC6+7 prior, tuned on MUC6 (d) CHELBA: MUC6+7 prior, tuned on MUC6 0.3 Investigation 0.2 3.1 Data, domains and tasks 0.1 For our experiments, we have chosen five different corpora (summarized in Table 3). Although each corpus can be considered its own domain (due to variations in annotation standards, specific task, date of collection, etc), they can also be roughly grouped into three different genres. These are: abstracts from biological journals [UT (Bunescu et al., 2004), Yapex (Franz´en et al., 2002)]; news articles [MUC6 (Fisher et al., 1995), MUC7 (Borthwick et al., 1998)]; and personal e-mails [CSPACE (Kraut et al., 2004)]. Each corpus, depending on its genre, is labeled with one of two name-finding tasks: • protein names in biological abstracts • person names in news articles and e-mails We chose this array of corpora so that we could evaluate our hierarchical prior’s ability to generalize across and incorporate information from a variety of domains, genres and tasks. In each case, each item (abstract, article or e-mail) was tokenized and each token was hand-labeled as either being part of a name (protein or person) or not, respectively. We used a standard n"
P08-1029,W04-3237,0,0.03714,"ver the features so as to maximize the conditional likelihood of the training data, p(Ytrain |Xtrain ), given the model pΛ . 2.2 CRF with Gaussian priors To avoid overfitting the training data, these λ’s are often further constrained by the use of a Gaussian prior (Chen and Rosenfeld, 1999) with diagonal covariance, N (µ, σ 2 ), which tries to maximize: argmax Λ N  X k=1  F X (λj − µj )2 log pΛ (yk |xk ) − β 2σj2 j where β > 0 is a parameter controlling the amount of regularization, and N is the number of sentences in the training set. 2.3 Source trained priors One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models 1 involves modifying the µ’s of this Gaussian prior. First a model of the source domain, Λsource , source , Y source }. Then a is learned by training on {Xtrain train model of the target domain is trained over oa limited n target target set of labeled target data Xtrain , Ytrain , but instead of regularizing this Λtarget to be near zero (i.e. setting µ = 0), Λtarget is instead regularized towards the previously learned source values Λsource (by setting µ = Λsource , while σ 2 remains 1) and thus minimizing (Λtarget − Λsource )2 . 1 Maximum Entro"
P08-1029,P07-1033,0,0.31751,"Missing"
P08-1029,M95-1011,0,0.00842588,"E 0.4 (a) GAUSS: tuned on MUC6 (b) CAT: tuned on MUC6+7 (c) HIER: MUC6+7 prior, tuned on MUC6 (d) CHELBA: MUC6+7 prior, tuned on MUC6 0.3 Investigation 0.2 3.1 Data, domains and tasks 0.1 For our experiments, we have chosen five different corpora (summarized in Table 3). Although each corpus can be considered its own domain (due to variations in annotation standards, specific task, date of collection, etc), they can also be roughly grouped into three different genres. These are: abstracts from biological journals [UT (Bunescu et al., 2004), Yapex (Franz´en et al., 2002)]; news articles [MUC6 (Fisher et al., 1995), MUC7 (Borthwick et al., 1998)]; and personal e-mails [CSPACE (Kraut et al., 2004)]. Each corpus, depending on its genre, is labeled with one of two name-finding tasks: • protein names in biological abstracts • person names in news articles and e-mails We chose this array of corpora so that we could evaluate our hierarchical prior’s ability to generalize across and incorporate information from a variety of domains, genres and tasks. In each case, each item (abstract, article or e-mail) was tokenized and each token was hand-labeled as either being part of a name (protein or person) or not, res"
P08-1029,N06-1010,0,0.0677658,"we have shown that hierarchical priors allow the user enough flexibility to customize their semantics to a specific problem, while providing enough structure to resist unintended negative effects when used inappropriately. Thus hierarchical priors seem a natural, effective and robust choice for transferring learning across NER datasets and tasks. Some of the first formulations of the transfer learning problem were presented over 10 years ago (Thrun, 1996; Baxter, 1997). Other techniques have tried to quantify the generalizability of certain features across domains (Daum´e III and Marcu, 2006; Jiang and Zhai, 2006), or tried to exploit the common structure of related problems (Ben-David et al., 2007; Sch¨olkopf et al., 2005). Most of this prior work deals with supervised transfer learning, and thus requires labeled source domain data, though there are examples of unsupervised (Arnold 252 et al., 2007), semi-supervised (Grandvalet and Bengio, 2005; Blitzer et al., 2006), and transductive approaches (Taskar et al., 2003). Recent work using so-called meta-level priors to transfer information across tasks (Lee et al., 2007), while related, does not take into explicit account the hierarchical structure of th"
P08-1029,H05-1056,1,0.563702,"or book true false 2 C ... LeftToken.* LeftToken.IsWord.* LeftToken.IsWord.IsTitle.* LeftToken.IsWord.IsTitle.equals.* LeftToken.IsWord.IsTitle.equals.mr Table 1: A few examples of the feature hierarchy to the named entity status of the current word. This is easily accomplished by backing up one level from a leaf in the tree structure to its parent, to represent a class of features such as L.1.*. It has been shown empirically that, while the significance of particular features might vary between domains and tasks, certain generalized classes of features retain their importance across domains (Minkov et al., 2005). By backing-off in this way, we can use the feature hierarchy as a prior for transferring beliefs about the significance of entire classes of features across domains and tasks. Some examples illustrating this idea are shown in table 1. R 1.3 Transfer learning ... ... ... ... ... Figure 1: Graphical representation of a hierarchical feature tree for token Caldwell in example Sentence 1. Representing feature spaces with this kind of tree, besides often coinciding with the explicit language used by common natural language toolkits (Cohen, 2004), has the added benefit of allowing a model to easily"
P08-1029,H05-1094,0,0.0153076,"urce and Dtarget , from which data may be drawn. Given this notation we can then precisely state the transfer learning target problem as trying to assign labels Ytest to test target target data Xtest drawn from D , given training source , Y source ) drawn from D source . data (Xtrain train In this paper we focus on two subproblems of transfer learning: • domain adaptation, where we assume Y (the set of possible labels) is the same for both Dsource and Dtarget , while Dsource and Dtarget themselves are allowed to vary between domains. • multi-task learning (Ando and Zhang, 2005; Caruana, 1997; Sutton and McCallum, 2005; Zhang et al., 2005) in which the task (and label set) is allowed to vary from source to target. Domain adaptation can be further distinguished by the degree of relatedness between the source and target domains. For example, in this work we group data collected in the same medium (e.g., all annotated e-mails or all annotated news articles) as belonging to the same genre. Although the specific boundary between domain and genre for a particular set of data is often subjective, it is nevertheless a useful distinction to draw. One common way of addressing the transfer learning problem is to use a"
P08-1029,M95-1006,0,\N,Missing
P09-1050,C92-2082,0,0.732199,"efficiently find and process many semi-structured web documents containing instances of the set being expanded. Figure 1 shows some examples of SEAL’s input and output. SEAL has been recently extended to be robust to errors in its initial set of seeds (Wang et al., Introduction An important and well-studied problem is the production of semantic lexicons for classes of interest; that is, the generation of all instances of a set (e.g., “apple”, “orange”, “banana”) given a name of that set (e.g., “fruits”). This task is often addressed by linguistically analyzing very large collections of text (Hearst, 1992; Kozareva et al., 2008; Etzioni et al., 2005; Pantel and Ravichandran, 2004; Pasca, 2004), often using hand-constructed or machine-learned shallow linguistic patterns to detect hyponym instances. A hyponym is a word or phrase whose semantic range 1 http://rcwang.com/asia 441 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 441–449, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 2008), and to use bootstrapping to iteratively improve its performance (Wang and Cohen, 2008). These extensions allow ASIA to extract instances of sets from the Web,"
P09-1050,P08-1119,0,0.565126,"ind and process many semi-structured web documents containing instances of the set being expanded. Figure 1 shows some examples of SEAL’s input and output. SEAL has been recently extended to be robust to errors in its initial set of seeds (Wang et al., Introduction An important and well-studied problem is the production of semantic lexicons for classes of interest; that is, the generation of all instances of a set (e.g., “apple”, “orange”, “banana”) given a name of that set (e.g., “fruits”). This task is often addressed by linguistically analyzing very large collections of text (Hearst, 1992; Kozareva et al., 2008; Etzioni et al., 2005; Pantel and Ravichandran, 2004; Pasca, 2004), often using hand-constructed or machine-learned shallow linguistic patterns to detect hyponym instances. A hyponym is a word or phrase whose semantic range 1 http://rcwang.com/asia 441 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 441–449, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 2008), and to use bootstrapping to iteratively improve its performance (Wang and Cohen, 2008). These extensions allow ASIA to extract instances of sets from the Web, as follows. First, give"
P09-1050,D08-1099,1,0.37796,"he class name in the excerpts. 3.2 Set Expander - SEAL In this paper, we rely on a set expansion system named SEAL (Wang and Cohen, 2007), which stands for Set Expander for Any Language. The system accepts as input a few seeds of some target set S (e.g., “fruits”) and automatically finds other probable instances (e.g., “apple”, “banana”) of S in web documents. As its name implies, SEAL is independent of document languages: both the written (e.g., English) and the markup language (e.g., HTML). SEAL is a research system that has shown good performance in published results (Wang and Cohen, 2007; Wang et al., 2008; Wang and Cohen, 2008). Figure 1 shows some examples of SEAL’s input and output. In more detail, SEAL contains three major components: the Fetcher, Extractor, and Ranker. The Fetcher is responsible for fetching web documents, and the URLs of the documents come from top results retrieved from the search engine using the concatenation of all seeds as the query. This ensures that every fetched web page contains all seeds. The Extractor automatically constructs “wrappers” (i.e. page-specific extraction rules) for each page that contains the seeds. Every wrapper comprises two character strings tha"
P09-1050,E06-3004,0,0.00568987,"using the evaluation set presented in (Wang and Cohen, 2007), which contains 36 manually constructed lists across three different languages: English, Chinese, and Japanese (12 lists per language). Each list contains all instances of a particular semantic class in a certain language, and each instance contains a set of synonyms (e.g., USA, America). There are a total of 2515 instances, with an average of 70 instances per semantic class. Figure 4 shows the datasets and their corresponding semantic class names that we use in our experiments. 4.2 Bootstrapper Bootstrapping (Etzioni et al., 2005; Kozareva, 2006; Nadeau et al., 2006) is an unsupervised iterative process in which a system continuously consumes its own outputs to improve its own performance. Wang (Wang and Cohen, 2008) showed that it is feasible to bootstrap the results of set expansion to improve the quality of a list. The paper introduces an iterative version of SEAL called iSEAL, which expands a list in multiple iterations. In each iteration, iSEAL expands a few candidates extracted in previous iterations and aggregates statistics. The Bootstrapper utilizes iSEAL to further improve the quality of the list returned by the Expander. I"
P09-1050,N04-1041,0,0.779927,"ments containing instances of the set being expanded. Figure 1 shows some examples of SEAL’s input and output. SEAL has been recently extended to be robust to errors in its initial set of seeds (Wang et al., Introduction An important and well-studied problem is the production of semantic lexicons for classes of interest; that is, the generation of all instances of a set (e.g., “apple”, “orange”, “banana”) given a name of that set (e.g., “fruits”). This task is often addressed by linguistically analyzing very large collections of text (Hearst, 1992; Kozareva et al., 2008; Etzioni et al., 2005; Pantel and Ravichandran, 2004; Pasca, 2004), often using hand-constructed or machine-learned shallow linguistic patterns to detect hyponym instances. A hyponym is a word or phrase whose semantic range 1 http://rcwang.com/asia 441 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 441–449, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 2008), and to use bootstrapping to iteratively improve its performance (Wang and Cohen, 2008). These extensions allow ASIA to extract instances of sets from the Web, as follows. First, given a semantic class name (e.g., “fruits”), ASIA uses a"
P09-1050,P06-1101,0,0.284687,"Missing"
P13-2007,P11-1027,0,0.0127822,"ss comment wc , are all considered as text tokens. The rest of the tokens in the document wr , are considered to be the class definition, and they may contain both code and text tokens (from string literals and other comments in the source file). We then compute the posterior probability of document topics by solving the following inference problem conditioned on the wr tokens Models We train n-gram models (n = 1, 2, 3) over source code documents containing sequences of combined code and text tokens from multiple training datasets (described below). We use the Berkeley Language Model package (Pauls and Klein, 2011) with absolute discounting (Kneser-Ney smoothing; (1995)) which includes a backoff strategy to lower-order n-grams. Next, we use LDA topic models (Blei et al., 2003) trained on the same data, with 1, 5, 10 and 20 topics. The joint distribution of a topic mixture θ, and a set of N topics z, for a single source code document with N observed word tokens, d = {wi }N i=1 , given the Dirichlet parameters α and β, is therefore p(θ, z, w|α, β) = Y p(z|θ)p(w|z, β) p(θ|α) (1) w p(θ, z r |wr , α, β) = Under the models described so far, there is no distinction between text and code tokens. Finally, we con"
P13-2007,O03-1008,0,0.0885271,"Missing"
P13-2007,P10-1126,0,\N,Missing
P15-1035,D14-1164,0,0.0144003,"tems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7. 1 For example, KBP slot filling is known for its complex pipeline, and the best overall F1 scores (Wiegand and Klakow, 2013; Angeli et al., 2014) for recent competitions are within the range of 30-40. 355 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 355–364, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work about(X,Z) :- handLabeled(X,Z) about(X,Z) :- sim(X,Y),about(Y,Z) sim(X,Y) :- links(X,Y) sim(X,Y) :hasWord(X,W),hasWord(Y,W), linkedBy(X,Y,W) linkedBy(X,Y,W) :- true In NLP, our work clearly aligns with recent work on joint models of individual text processing task"
P15-1035,W03-2201,0,0.0187715,"inference for IE and SL significantly improve both tasks; that latent context invention further improves the results. 1 • We present a joint model for IE and relational learning in a statistical relational learning setting which outperforms universal schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, an"
P15-1035,D11-1049,1,0.872368,"schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7. 1 For example, KBP slot filling is known for its complex pipeline, and the best overall F1 scores (Wiegand and Klakow, 2013; Angeli et al., 2014) for recent competitions are within the range of 30-40. 355 Proceedings"
P15-1035,D12-1093,1,0.852018,"rom text, without large-scale annotations. In extracting Infobox information from Wikipedia text, Wu and Weld (2007; 2010) also use a similar idea. In an open IE project, Banko et al. (2007) use a seed KB, and utilize weak supervision techniques to extend it. Note that weakly supervised extraction approaches can be noisy, as a pair of entities in context may be associated with one, none, or several of the possible relation labels, a property which complicates the application of distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). Lao et al. (2012) learned syntactic rules for finding relations defined by “lexico-semantic” paths spanning KB relations and text data. Wang et al. (2015) extends the methods used by Lao et al. to learn mutually recursive relations. Recently, Riedel et al. (2013) propose a matrix factorization technique for relation embedding, but their method requires a large amount of negative and unlabeled examples. Weston et al. (2013) connect text with KB embedding by adding a scoring term, though no shared parameters/embeddings are used. All these prior works make use of text and KBs. Unlike these prior works, our method"
P15-1035,N07-4013,0,0.0768777,"Missing"
P15-1035,P14-1016,0,0.0116867,"context invention further improves the results. 1 • We present a joint model for IE and relational learning in a statistical relational learning setting which outperforms universal schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7. 1 For example, KBP slot filling is k"
P15-1035,P09-1113,0,0.0263458,"hasWord(X,W),hasWord(Y,W), linkedBy(X,Y,W) linkedBy(X,Y,W) :- true In NLP, our work clearly aligns with recent work on joint models of individual text processing tasks. For example, Finkel and Manning (2009) work on the problem of joint IE and parsing, where they use tree representations to combine named entities and syntactic chunks. Recently, Devlin et al. (Devlin et al., 2014) use a joint neural network model for machine translation, and obtain an impressive 6.3 BLEU point improvement over a hierarchical phrase-based system. In information extraction, weak supervision (Craven et al., 1999; Mintz et al., 2009) is a common technique for extracting knowledge from text, without large-scale annotations. In extracting Infobox information from Wikipedia text, Wu and Weld (2007; 2010) also use a similar idea. In an open IE project, Banko et al. (2007) use a seed KB, and utilize weak supervision techniques to extend it. Note that weakly supervised extraction approaches can be noisy, as a pair of entities in context may be associated with one, none, or several of the possible relation labels, a property which complicates the application of distant supervision methods (Mintz et al., 2009; Riedel et al., 2010"
P15-1035,U06-1009,0,0.0346256,"Missing"
P15-1035,D13-1136,0,0.0207702,"he possible relation labels, a property which complicates the application of distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). Lao et al. (2012) learned syntactic rules for finding relations defined by “lexico-semantic” paths spanning KB relations and text data. Wang et al. (2015) extends the methods used by Lao et al. to learn mutually recursive relations. Recently, Riedel et al. (2013) propose a matrix factorization technique for relation embedding, but their method requires a large amount of negative and unlabeled examples. Weston et al. (2013) connect text with KB embedding by adding a scoring term, though no shared parameters/embeddings are used. All these prior works make use of text and KBs. Unlike these prior works, our method is posed in an SRL setting, using a scalable probabilistic first-order logic, and allows learning of relational rules that are mutually recursive, thus allowing learning of multi-step inferences. Unlike some prior methods, our method also does not require negative examples, or large numbers of unlabeled examples. 3 # base. # prop. # sim,link. # sim,word. # by(W). Table 1: A simple program in ProPPR. See t"
P15-1035,N13-1008,0,0.334078,"extraction with structure learning (SL) in a scalable probabilistic logic framework. We then propose a latent context invention (LCI) approach to improve the performance. In experiments, we show that our approach outperforms state-of-the-art baselines over three real-world Wikipedia datasets from multiple domains; that joint learning and inference for IE and SL significantly improve both tasks; that latent context invention further improves the results. 1 • We present a joint model for IE and relational learning in a statistical relational learning setting which outperforms universal schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task some"
P15-1035,D12-1042,0,0.0307787,"r extracting knowledge from text, without large-scale annotations. In extracting Infobox information from Wikipedia text, Wu and Weld (2007; 2010) also use a similar idea. In an open IE project, Banko et al. (2007) use a seed KB, and utilize weak supervision techniques to extend it. Note that weakly supervised extraction approaches can be noisy, as a pair of entities in context may be associated with one, none, or several of the possible relation labels, a property which complicates the application of distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). Lao et al. (2012) learned syntactic rules for finding relations defined by “lexico-semantic” paths spanning KB relations and text data. Wang et al. (2015) extends the methods used by Lao et al. to learn mutually recursive relations. Recently, Riedel et al. (2013) propose a matrix factorization technique for relation embedding, but their method requires a large amount of negative and unlabeled examples. Weston et al. (2013) connect text with KB embedding by adding a scoring term, though no shared parameters/embeddings are used. All these prior works make use of text and KBs. Unlike these prio"
P15-1035,P10-1013,0,0.0463274,"Missing"
P15-1035,P14-1109,1,0.825976,"tasks; that latent context invention further improves the results. 1 • We present a joint model for IE and relational learning in a statistical relational learning setting which outperforms universal schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7. 1 For example, KBP"
P15-1035,N09-1037,0,\N,Missing
P15-1035,P11-1055,0,\N,Missing
P15-1035,P14-1129,0,\N,Missing
P15-1065,D13-1080,0,0.0337426,"Missing"
P15-1065,D14-1044,0,0.026518,"Missing"
P15-1065,E09-1005,0,0.0221974,"targets t is unknown, but rather has to be revealed. 5 Table 1: MAP and training time [sec] on KB inference and NE extraction tasks. consti denotes constant paths up to length i. RWR FOIL PRA CoR-PRA-no-const CoR-PRA-const2 CoR-PRA-const3 KB inference Time MAP 25.6 0.429 18918.1 0.358 10.2 0.477 16.7 0.479 23.3 0.524 27.1 0.530 NE extraction Time MAP 7,375 0.017 366,558 0.167 277 0.107 449 0.167 556 0.186 643 0.316 known as personalized PageRank (Haveliwala, 2002), a popular random walk based graph similarity measure, that has been shown to be fairly successful for many types of tasks (e.g., (Agirre and Soroa, 2009; Moro et al., 2014)). Finally, we compare against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P (s → t; π). All experiments were run on a machine with a 16 core Intel Xeon 2.33GHz CPU and 24Gb of memory. All methods are trained and tested with the same data splits. We report the total training time of each method, measuring the efficiency of inference and induction as a whole. Experiments In this section, we report the results of applying Cor-PRA to the tasks of knowledge base inference and person named entity"
P15-1065,D11-1049,1,0.89516,"s defined to be 1 if s = z, and 0 otherwise. The probability P (z → t; r) is defined as 1/|r(z) |if r(z, t), and 0 otherwise, where r(z) is the set of nodes linked to node z over edge type r. It has been shown that P (s → t; π) can be effectively estimated using random walk sampling techniques, with bounded complexity and bounded error, for all graph nodes that can be reached from s over path type π (Lao and Cohen, 2010a). Due to the exponentially large feature space in relational domains, candidate path features are first generated using a dedicated particle filtering path-finding procedure (Lao et al., 2011), which is informed by training signals. Meaningful features are then selected using the following goodness measures, considering path precision and coverage: precision(π) = 1X P (si → Gi ; π), n P (t ← s; π) = X P (t ← z; π 0−1 )P (z ← s; r−1 ) (5) where is the path that results from removing the last edge type r in π 0 . Here, in the terminal condition that π 0−1 = φ, P (t ← z; π 0−1 ) is defined to be 1 for z = t, and 0 otherwise. In what follows, the starting point of the random walk calculation is indicated at the left side of the arrow symbol; i.e., P (s → t; π) denotes the probability o"
P15-1065,D12-1093,1,0.426911,"Missing"
P15-1065,D08-1095,1,0.881077,"ntly, the search and computation of the extended set of features is performed efficiently, maintaining high scalability of the framework. Concretely, using backward walks, one can compute random walk probabilities in a bi-directional fashion; this means that for paths of length 2M , the time complexity of path finding is reduced from O(|V |2M ) to O(|V |M ), where |V |is the number of edge types in graph. Finally, we report experimental results for relational inference tasks in two different domains, including knowledge base link prediction and person named entity extraction from parsed text (Minkov and Cohen, 2008). It is shown that the proposed extensions allow one to effectively explore a larger feature space, significantly improving model quality over previously published results in both domains. In particular, incorporating paths with constants significantly improves model quality on both tasks. Bi-directional walk probability computation also enables the learning of longer predicate chains, and the modeling of long paths is shown to substantially improve performance on the person name extraction task. Importantly, learning and inference remain highly efficient in both these settings. NFL MLB Figure"
P15-1065,Q14-1019,0,0.0168144,"t rather has to be revealed. 5 Table 1: MAP and training time [sec] on KB inference and NE extraction tasks. consti denotes constant paths up to length i. RWR FOIL PRA CoR-PRA-no-const CoR-PRA-const2 CoR-PRA-const3 KB inference Time MAP 25.6 0.429 18918.1 0.358 10.2 0.477 16.7 0.479 23.3 0.524 27.1 0.530 NE extraction Time MAP 7,375 0.017 366,558 0.167 277 0.107 449 0.167 556 0.186 643 0.316 known as personalized PageRank (Haveliwala, 2002), a popular random walk based graph similarity measure, that has been shown to be fairly successful for many types of tasks (e.g., (Agirre and Soroa, 2009; Moro et al., 2014)). Finally, we compare against PRA, which models relational paths in the form of edge-sequences (no constants), using only uni-directional path probabilities, P (s → t; π). All experiments were run on a machine with a 16 core Intel Xeon 2.33GHz CPU and 24Gb of memory. All methods are trained and tested with the same data splits. We report the total training time of each method, measuring the efficiency of inference and induction as a whole. Experiments In this section, we report the results of applying Cor-PRA to the tasks of knowledge base inference and person named entity extraction from par"
P15-1065,de-marneffe-etal-2006-generating,0,\N,Missing
P15-1140,D11-1142,0,0.615623,"ruction, where the list of categories and relations that define the schema of the KB are explicit, versus open IE methods, where they are not. Another dimension is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new domain. Ontology design involves assembling a set of categories, organized"
P15-1140,C92-2082,0,0.395472,"d along several dimensions. One dimension is ontology-guided construction, where the list of categories and relations that define the schema of the KB are explicit, versus open IE methods, where they are not. Another dimension is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new do"
P15-1140,P13-2007,1,0.831342,"with relational data in the form of tensor structures. Nickel et al. (2012) factorized the ontology of Yago 2 for relational learning. A related approach was using Neural Tensor Networks to extract new facts from an existing KB (Chen et al., 2013; Socher et al., 2013). In contrast, in KB-LDA, relational data is learned jointly with the model through the Relations component. Statistical language models have recently been adapted for modeling software code and text documents. Most tasks focused on enhancing the software development workflow with code and comment completion (Hindle et al., 2012; Movshovitz-Attias and Cohen, 2013), learning coding conventions (Allamanis et al., 2014), and extracting actionable tasks from software documentation (Treude et al., 2014). In related work, specific semantic relations, coordinate relations, have been extracted for a restricted class of software entities, ones that refer to Java classes (Movshovitz-Attias and Cohen, 2015). KB-LDA extends previous work by reasoning over a large variety of semantic relations among general software entities, as found in a document corpus. 5 Conclusions We presented a model that jointly learns a latent ontological structure of a corpus augmented by"
P15-1140,P93-1024,0,0.562249,"(KB) construction methods can be broadly categorized along several dimensions. One dimension is ontology-guided construction, where the list of categories and relations that define the schema of the KB are explicit, versus open IE methods, where they are not. Another dimension is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete,"
P15-1140,P06-1101,0,0.0622245,"ion is ontology-guided construction, where the list of categories and relations that define the schema of the KB are explicit, versus open IE methods, where they are not. Another dimension is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new domain. Ontology design involves assembling a"
P15-1140,N07-4013,0,0.0134776,"s and relations that define the schema of the KB are explicit, versus open IE methods, where they are not. Another dimension is the type of relations and types included in the KB: some KBs, like WordNet, are hierarchical, in that they contain mainly concept types, supertypes and instances, while other KBs contain many types of relationships between concepts. Hierarchical knowledge can be learned by methods including distributional clustering (Pereira et al., 1993), as well as Hearst patterns (Hearst, 1992) and similar techniques (Snow et al., 2006). Reverb (Fader et al., 2011) and TextRunner (Yates et al., 2007) are open methods for learning multirelation KBs. Finally, NELL (Carlson et al., 2010; Mitchell et al., 2015), FreeBase (Google, 2011) and Yago (Suchanek et al., 2007; Hoffart et al., 2013) are ontology-guided methods for extracting KBs containing both hierarchies and relations. One advantage of ontology-guided methods is that the extracted knowledge is easier to reason with. An advantage of open IE methods is that ontologies may be incomplete, and are expensive to construct for a new domain. Ontology design involves assembling a set of categories, organized in a meaningful hierarchical struct"
P16-2044,D14-1194,0,0.0414675,"modeling (Karpathy et al., 2015; Kim et al., 2015). Previously, (Luong et al., 2013) dealt with the problem of estimating rare word representations by building them from their constituent morphemes. While they show improved performance over word-based models, their approach requires a morpheme parser for preprocessing which may not perform well on noisy text like Twitter. Also the space of all morphemes, though smaller than the space of all words, is still large enough that modelling all morphemes is impractical. Hashtag prediction for social media has been addressed earlier, for example in (Weston et al., 2014; Godin et al., 2013). (Weston et al., 2014) also use a neural architecture, but compose text embeddings from a lookup table of words. They also show that the learned embeddings can generalize to an unrelated task of document recommendation, justifying the use of hashtags as supervision for learning text representations. include language independence of the methods, and no requirement of NLP preprocessing such as word-segmentation. A crucial step in learning good text representations is to choose an appropriate objective function to optimize. Unsupervised approaches attempt to reconstruct the"
P16-2044,W13-3512,0,0.0501103,"del scales to large data sets better than other stateof-the-art approaches. While (Ling et al., 2015) generate word embeddings from character representations, we propose to generate vector representations of entire tweets from characters in our tweet2vec model. Our work adds to the growing body of work showing the applicability of character models for a variety of NLP tasks such as Named Entity Recognition (Santos and Guimarães, 2015), POS tagging (Santos and Zadrozny, 2014), text classification (Zhang et al., 2015) and language modeling (Karpathy et al., 2015; Kim et al., 2015). Previously, (Luong et al., 2013) dealt with the problem of estimating rare word representations by building them from their constituent morphemes. While they show improved performance over word-based models, their approach requires a morpheme parser for preprocessing which may not perform well on noisy text like Twitter. Also the space of all morphemes, though smaller than the space of all words, is still large enough that modelling all morphemes is impractical. Hashtag prediction for social media has been addressed earlier, for example in (Weston et al., 2014; Godin et al., 2013). (Weston et al., 2014) also use a neural arc"
P16-2044,W15-3904,0,0.0121291,"nal LSTMs as a potential solution to these problems. A major benefit of this approach is that large word lookup tables can be compacted into character lookup tables and the compositional model scales to large data sets better than other stateof-the-art approaches. While (Ling et al., 2015) generate word embeddings from character representations, we propose to generate vector representations of entire tweets from characters in our tweet2vec model. Our work adds to the growing body of work showing the applicability of character models for a variety of NLP tasks such as Named Entity Recognition (Santos and Guimarães, 2015), POS tagging (Santos and Zadrozny, 2014), text classification (Zhang et al., 2015) and language modeling (Karpathy et al., 2015; Kim et al., 2015). Previously, (Luong et al., 2013) dealt with the problem of estimating rare word representations by building them from their constituent morphemes. While they show improved performance over word-based models, their approach requires a morpheme parser for preprocessing which may not perform well on noisy text like Twitter. Also the space of all morphemes, though smaller than the space of all words, is still large enough that modelling all morphemes"
P16-2044,D15-1176,0,\N,Missing
P17-1096,D13-1160,0,0.0344397,"framework obtains substantial improvement from unlabeled text. 1 Introduction Recently, various neural network models were proposed and successfully applied to the tasks of questions answering (QA) and/or reading comprehension (Xiong et al., 2016; Dhingra et al., 2016; Yang et al., 2017). While achieving stateof-the-art performance, these models rely on a large amount of labeled data. However, it is extremely difficult to collect large-scale question answering datasets. Historically, many of the question answering datasets have only thousands of question answering pairs, such as WebQuestions (Berant et al., 2013), MCTest (Richardson et al., 2013), WikiQA (Yang et al., 2015), and TREC-QA (Voorhees and Tice, 2000). Although larger question answering datasets with hundreds of thousands of question-answer pairs have been collected, including SQuAD (Rajpurkar et al., 2016), MSMARCO (Nguyen et al., 2016), and NewsQA (Trischler et al., 2016a), the data collection process is expensive and time-consuming in practice. This hinders real-world applications for domain-specific question answering. Compared to obtaining labeled question answer pairs, it is trivial to obtain unlabeled text data. In this work, we stud"
P17-1096,P16-1223,0,0.00921101,"Missing"
P17-1096,P17-2061,0,0.0545767,"Missing"
P17-1096,P05-1045,0,0.0138191,"SQuAD dataset can be categorized into ten types, i.e., “Date”, “Other Numeric”, “Person”, “Location”, “Other Entity”, “Common Noun Phrase”, “Adjective Phrase”, “Verb Phrase”, “Clause” and “Other” (Rajpurkar et al., 2016). For each paragraph from the unlabeled articles, we utilize Stanford Part-Of-Speech (POS) tagger (Toutanova et al., 2003) to label each word with the corresponding POS tag, and implement a simple constituency parser to extract the noun phrase, verb phrase, adjective and clause based on a small set of constituency grammars. Next, we use Stanford Named Entity Recognizer (NER) (Finkel et al., 2005) to assign each word with one of the seven labels, i.e., “Date”, “Money”, “Percent”, “location”, “Organization” and “Time”. We then categorize a span of consecutive words with the same NER tags of either “Money” or “Percent” as the answer of the type “Other Numeric”. Similarly, we categorize a span of consecutive words with the same NER tags of “Organization” as the answer of the type “Other Entity”. Finally, we subsample five answers from all the extracted answers for each paragraph according to the percentage of answer types in the SQuAD dataset. We obtain 4,753,060 answers in total, which i"
P17-1096,P16-1154,0,0.0394967,"a practice in domain adaptation (Johnson et al., 2016; Chu et al., 2017), we append the domain tag to the end of both the questions and the paragraphs. By introducing the domain tags, we expect the discriminative model to factor out domain-specific and domain-invariant representations. At test time, the tag “d true” is appended. 3.2 Generative Model The generative model learns the conditional probability of generating a question given the paragraph and the answer, i.e., P(q|p, a). We implement the generative model as a sequence-tosequence model (Sutskever et al., 2014) with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016). The generative model consists of an encoder and a decoder. An encoder is a GRU that encodes the input paragraph into a sequence of hidden states H. We inject the answer information by appending an additional zero/one feature to the word embeddings of the paragraph tokens; i.e., if a word token appears in the answer, the feature is set at one, otherwise zero. The decoder is another GRU with an attention mechanism over the encoder hidden states H. At each time step, the generation probabilities over all 1042 Algorithm 1 Training Generative DomainAdaptive Nets Input: lab"
P17-1096,P16-1014,0,0.00799331,"main adaptation (Johnson et al., 2016; Chu et al., 2017), we append the domain tag to the end of both the questions and the paragraphs. By introducing the domain tags, we expect the discriminative model to factor out domain-specific and domain-invariant representations. At test time, the tag “d true” is appended. 3.2 Generative Model The generative model learns the conditional probability of generating a question given the paragraph and the answer, i.e., P(q|p, a). We implement the generative model as a sequence-tosequence model (Sutskever et al., 2014) with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016). The generative model consists of an encoder and a decoder. An encoder is a GRU that encodes the input paragraph into a sequence of hidden states H. We inject the answer information by appending an additional zero/one feature to the word embeddings of the paragraph tokens; i.e., if a word token appears in the answer, the feature is set at one, otherwise zero. The decoder is another GRU with an attention mechanism over the encoder hidden states H. At each time step, the generation probabilities over all 1042 Algorithm 1 Training Generative DomainAdaptive Nets Input: labeled data L, unlabeled d"
P17-1096,P16-1086,0,0.00655938,"eking a domain transition from the source domain to the target domain (Gong et al., 2012; Gopalan et al., 2011; Pan et al., 2011). Our work gets inspiration from a practice in Johnson et al. (2016) and Chu et al. (2017) based on appending domain tags. However, our method is different from the above methods in that we apply domain adaptation techniques to the outputs of a generative model rather than a natural data domain. Question Answering. Various neural models based on attention mechanisms (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Wang et al., 2016; Dhingra et al., 2016; Kadlec et al., 2016; Trischler et al., 2016b; Sordoni et al., 2016; Cui et al., 2016; Chen et al., 2016) have been proposed to tackle the tasks of question answering and reading comprehension. However, the performance of these neural models largely relies on a large amount of labeled data available for training. Learning with Multiple Models. GANs (Goodfellow et al., 2014) formulated a adversarial game between a discriminative model and a generative model for generating realistic images. Ganin and Lempitsky (Ganin and Lempitsky, 2014) employed a similar idea to use two models for domain adaptation. Review networ"
P17-1096,D16-1264,0,0.804964,"t al., 2016; Yang et al., 2017). While achieving stateof-the-art performance, these models rely on a large amount of labeled data. However, it is extremely difficult to collect large-scale question answering datasets. Historically, many of the question answering datasets have only thousands of question answering pairs, such as WebQuestions (Berant et al., 2013), MCTest (Richardson et al., 2013), WikiQA (Yang et al., 2015), and TREC-QA (Voorhees and Tice, 2000). Although larger question answering datasets with hundreds of thousands of question-answer pairs have been collected, including SQuAD (Rajpurkar et al., 2016), MSMARCO (Nguyen et al., 2016), and NewsQA (Trischler et al., 2016a), the data collection process is expensive and time-consuming in practice. This hinders real-world applications for domain-specific question answering. Compared to obtaining labeled question answer pairs, it is trivial to obtain unlabeled text data. In this work, we study the following problem of semi-supervised question answering: is it possible to leverage unlabeled text to boost the performance of question answering models, especially when only a small amount of labeled data is available? The problem is challenging because"
P17-1096,D13-1020,0,0.0148876,"improvement from unlabeled text. 1 Introduction Recently, various neural network models were proposed and successfully applied to the tasks of questions answering (QA) and/or reading comprehension (Xiong et al., 2016; Dhingra et al., 2016; Yang et al., 2017). While achieving stateof-the-art performance, these models rely on a large amount of labeled data. However, it is extremely difficult to collect large-scale question answering datasets. Historically, many of the question answering datasets have only thousands of question answering pairs, such as WebQuestions (Berant et al., 2013), MCTest (Richardson et al., 2013), WikiQA (Yang et al., 2015), and TREC-QA (Voorhees and Tice, 2000). Although larger question answering datasets with hundreds of thousands of question-answer pairs have been collected, including SQuAD (Rajpurkar et al., 2016), MSMARCO (Nguyen et al., 2016), and NewsQA (Trischler et al., 2016a), the data collection process is expensive and time-consuming in practice. This hinders real-world applications for domain-specific question answering. Compared to obtaining labeled question answer pairs, it is trivial to obtain unlabeled text data. In this work, we study the following problem of semi-su"
P17-1096,N03-1033,0,0.0147108,"is the term of a single all all all all encompassing definition of a single all Q (RL): what writes ” the formulation of a single all - encompassing definition of the term all encompassing encompassing encompassing encompassing paragraphs from unlabelled articles. Answers in the SQuAD dataset can be categorized into ten types, i.e., “Date”, “Other Numeric”, “Person”, “Location”, “Other Entity”, “Common Noun Phrase”, “Adjective Phrase”, “Verb Phrase”, “Clause” and “Other” (Rajpurkar et al., 2016). For each paragraph from the unlabeled articles, we utilize Stanford Part-Of-Speech (POS) tagger (Toutanova et al., 2003) to label each word with the corresponding POS tag, and implement a simple constituency parser to extract the noun phrase, verb phrase, adjective and clause based on a small set of constituency grammars. Next, we use Stanford Named Entity Recognizer (NER) (Finkel et al., 2005) to assign each word with one of the seven labels, i.e., “Date”, “Money”, “Percent”, “location”, “Organization” and “Time”. We then categorize a span of consecutive words with the same NER tags of either “Money” or “Percent” as the answer of the type “Other Numeric”. Similarly, we categorize a span of consecutive words wi"
P17-1096,D16-1013,0,0.0263231,"formance, these models rely on a large amount of labeled data. However, it is extremely difficult to collect large-scale question answering datasets. Historically, many of the question answering datasets have only thousands of question answering pairs, such as WebQuestions (Berant et al., 2013), MCTest (Richardson et al., 2013), WikiQA (Yang et al., 2015), and TREC-QA (Voorhees and Tice, 2000). Although larger question answering datasets with hundreds of thousands of question-answer pairs have been collected, including SQuAD (Rajpurkar et al., 2016), MSMARCO (Nguyen et al., 2016), and NewsQA (Trischler et al., 2016a), the data collection process is expensive and time-consuming in practice. This hinders real-world applications for domain-specific question answering. Compared to obtaining labeled question answer pairs, it is trivial to obtain unlabeled text data. In this work, we study the following problem of semi-supervised question answering: is it possible to leverage unlabeled text to boost the performance of question answering models, especially when only a small amount of labeled data is available? The problem is challenging because conventional manifold-based semi-supervised learning algorithms (Z"
P17-1096,P16-1008,0,0.0253344,"beled data available for training. Learning with Multiple Models. GANs (Goodfellow et al., 2014) formulated a adversarial game between a discriminative model and a generative model for generating realistic images. Ganin and Lempitsky (Ganin and Lempitsky, 2014) employed a similar idea to use two models for domain adaptation. Review networks (Yang et al., 2016b) employ a discriminative model as a regularizer for training a generative model. In the context of machine translation, given a language pair, various recent work studied jointly training models to learn the mappings in both directions (Tu et al., 2016; Xia et al., 2016). 6 Conclusions We study a critical and challenging problem, semi-supervised question answering. We propose a novel neural framework called Generative Domain-Adaptive Nets, which incorporate domain adaptation techniques in combination with generative models for semi-supervised learning. Empirically, we show that our approach leads to substantial improvements over supervised learning models and outperforms several strong baselines including GANs and dual learning. In the future, we plan to apply our approach to more question answering datasets in different domains. It will al"
P17-1096,P17-1055,0,\N,Missing
P17-1168,P16-1223,0,0.712188,"ark to measure a system’s performance at text comprehension. ∗ BD and HL contributed equally to this work. Source code is available on github: https:// github.com/bdhingra/ga-reader 1 Deep learning models have been shown to outperform traditional shallow approaches on text comprehension tasks (Hermann et al., 2015). The success of many recent models can be attributed primarily to two factors: (1) Multi-hop architectures (Weston et al., 2015; Sordoni et al., 2016; Shen et al., 2016), allow a model to scan the document and the question iteratively for multiple passes. (2) Attention mechanisms, (Chen et al., 2016; Hermann et al., 2015) borrowed from the machine translation literature (Bahdanau et al., 2014), allow the model to focus on appropriate subparts of the context document. Intuitively, the multi-hop architecture allows the reader to incrementally refine token representations, and the attention mechanism re-weights different parts in the document according to their relevance to the query. The effectiveness of multi-hop reasoning and attentions have been explored orthogonally so far in the literature. In this paper, we focus on combining both in a complementary manner, by designing a novel atten"
P17-1168,P17-1055,0,0.452843,"Missing"
P17-1168,P16-2044,1,0.0295143,"bilities and the true answers. 3.1.4 Further Enhancements Character-level Embeddings: Given a token w from the document or query, its vector space representation is computed as x = L(w)||C(w). L(w) retrieves the word-embedding for w from a lookup table L ∈ R|V |×nl , whose rows hold a vector for each unique token in the vocabulary. We also utilize a character composition model C(w) which generates an orthographic embedding of the token. Such embeddings have been previously shown to be helpful for tasks like Named Entity Recognition (Yang et al., 2016) and dealing with OOV tokens at test time (Dhingra et al., 2016). The embedding C(w) is generated by taking the final outputs znf c and znb c of a Bi-GRU applied to embeddings from 1835 a lookup table of characters in the token, and applying a linear transformation: z = znf c ||znb c C(w) = W z + b Question Evidence Common Word Feature (qecomm): Li et al. (2016) recently proposed a simple token level indicator feature which significantly boosts reading comprehension performance in some cases. For each token in the document we construct a one-hot vector fi ∈ {0, 1}2 indicating its presence in the query. It can be incorporated into the GA reader by assigning"
P17-1168,P16-1086,0,0.317709,"hop architecture allows the reader to incrementally refine token representations, and the attention mechanism re-weights different parts in the document according to their relevance to the query. The effectiveness of multi-hop reasoning and attentions have been explored orthogonally so far in the literature. In this paper, we focus on combining both in a complementary manner, by designing a novel attention mechanism which gates the evolving token representations across hops. More specifically, unlike existing models where the query attention is applied either token-wise (Hermann et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Hill et al., 2016) or sentence-wise (Weston et al., 2015; Sukhbaatar et al., 2015) to allow weighted aggregation, the Gated-Attention (GA) module proposed in this work allows the query to directly interact with each dimension of the token embeddings at the semantic-level, and is applied layer-wise as information filters during the multi-hop representation learning process. Such a fine-grained attention enables our model to learn conditional token representations w.r.t. the given question, leading to accurate answer selections. We show in our experiments that the proposed G"
P17-1168,N16-1099,0,0.013885,"Missing"
P17-1168,P08-1028,0,0.00828169,"nd is applied per hop to act as fine-grained information filters during the multi-step reasoning. The filters weigh individual components of the vector representation of each token in the document separately. The design of gated-attention layers is motivated by the effectiveness of multiplicative interaction among vector-space representations, e.g., in various types of recurrent units (Hochreiter & Schmidhuber, 1997; Wu et al., 2016) and in relational learning (Yang et al., 2014; Kiros et al., 2014). While other types of compositional operators are possible, such as concatenation or addition (Mitchell & Lapata, 2008), we find that multiplication has strong empirical performance (section 4.3), where query representations naturally serve as information filters across hops. 3.1 ouput sequence H = [h1 , h2 , . . . , hT ] as follows: rt = σ(Wr xt + Ur ht−1 + br ), zt = σ(Wz xt + Uz ht−1 + bz ), ˜ ht = tanh(Wh xt + Uh (rt ht−1 ) + bh ), ˜ t. ht = (1 − zt ) ht−1 + zt h where denotes the Hadamard product or the element-wise multiplication. rt and zt are called ˜t the reset and update gates respectively, and h the candidate output. A Bi-directional GRU (BiGRU) processes the sequence in both forward and backward di"
P17-1168,E17-1038,0,0.0255617,"Missing"
P17-1168,D16-1241,0,0.13279,"e Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. 1 Introduction A recent trend to measure progress towards machine reading is to test a system’s ability to answer questions about a document it has to comprehend. Towards this end, several large-scale datasets of cloze-style questions over a context document have been introduced recently, which allow the training of supervised machine learning systems (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016). Such datasets can be easily constructed automatically and the unambiguous nature of their queries provides an objective benchmark to measure a system’s performance at text comprehension. ∗ BD and HL contributed equally to this work. Source code is available on github: https:// github.com/bdhingra/ga-reader 1 Deep learning models have been shown to outperform traditional shallow approaches on text comprehension tasks (Hermann et al., 2015). The success of many recent models can be attributed primarily to two factors: (1) Multi-hop architectures (Weston et al., 2015; Sordoni et al., 2016; Shen"
P17-1168,D14-1162,0,0.119158,"Missing"
P17-1168,D16-1013,0,0.0808515,"Missing"
P19-1483,W00-1401,0,0.21157,"e an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated tab"
P19-1483,H05-1042,0,0.0465407,"Missing"
P19-1483,E06-1040,0,0.0647752,"ema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours i"
P19-1483,W07-0718,0,0.171507,"Missing"
P19-1483,E06-1032,0,0.111485,"al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show an"
P19-1483,W17-3209,0,0.0568619,"Missing"
P19-1483,D16-1128,0,0.611103,"the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has lar"
P19-1483,P18-1060,0,0.0664973,"Missing"
P19-1483,W14-3346,0,0.0160021,"f the longest common subsequence between x and y. The LCS function, borrowed from ROUGE, ensures that entity names in r¯k appear in the same order in the text as the table. Higher values of Er (T i ) denote that more records are likely to be mentioned in Gi . The entailed precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a Smoothing & Multiple References. The danger with geometric averages is that if any of the components being averaged become 0, the average will also be 0. Hence, we adopt a smoothing technique from Chen and Cherry (2014) that assigns a small positive value  to any of Epn , Ern (Ri ) and Er (T i ) which are 0. When multiple references are available for a table, we compute PARENT against each reference and take the maximum as its overall score, similar to METEOR (Denkowski and Lavie, 2014). Choosing λ and . To set the value of λ we can tune it to maximize the correlation of the metric with human judgments, when such data is available. When such data is not available, we can use the recall of the reference against the table, using Eq. 6, as the value of 1 − λ. The intuition here is that if the recall of the re"
P19-1483,P09-1011,0,0.345045,"Missing"
P19-1483,W04-1013,0,0.218157,"et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated table. We call such reference texts divergent from the table. We show that existing automatic metrics, including BLEU, correlate"
P19-1483,W14-3348,0,0.355805,"led precision and recall are combined into an F-score to give the PARENT metric for one instance. The system-level PARENT score for a Smoothing & Multiple References. The danger with geometric averages is that if any of the components being averaged become 0, the average will also be 0. Hence, we adopt a smoothing technique from Chen and Cherry (2014) that assigns a small positive value  to any of Epn , Ern (Ri ) and Er (T i ) which are 0. When multiple references are available for a table, we compute PARENT against each reference and take the maximum as its overall score, similar to METEOR (Denkowski and Lavie, 2014). Choosing λ and . To set the value of λ we can tune it to maximize the correlation of the metric with human judgments, when such data is available. When such data is not available, we can use the recall of the reference against the table, using Eq. 6, as the value of 1 − λ. The intuition here is that if the recall of the reference against the table is high, it already covers most of the information, and we can assign it a high weight in Eq. 4. This leads to a separate value of λ automatically set for each instance.7  is set to 10−5 for all experiments. 4 Evaluation via Information Extractio"
P19-1483,P17-1017,0,0.371266,"al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Th"
P19-1483,W05-1208,0,0.0215623,"recision Epn for n-grams of order n is given by: Epn = P g∈Gin P   / Rni )w(g) #Gin (g) P r(g ∈ Rni ) + P r(g ∈ P , g∈Gin #Gin (g) g∈Gin = #Gin (g)w(g) + #Gin ,Rni (g)[1 − w(g)] P . g∈Gin #Gin (g) (2) In words, an n-gram receives a reward of 1 if it appears in the reference, with probability P r(g ∈ Rni ), and otherwise it receives a reward of w(g). Both numerator and denominator are weighted by the count of the n-gram in Gin . P r(g ∈ Rni ) rewards an n-gram for appearing as many times as it appears in the reference, not more. We combine precisions for n-gram orders 1-4 using a geometric 5 Glickman and Dagan (2005) used a product instead of geometric mean. Here we use a geometric mean to ensure that n-grams of different lengths have comparable probabilities of being entailed. 6 It is unlikely that an automated system produces the same extra n-gram as present in the reference, thus a match with the reference n-gram is considered positive. For example, in Figure 1, it is highly unlikely that a system would produce “Silkworm” when it is not present in the table. 4886 model M is the average of instance level PARENT scores across the evaluation set: average, similar to BLEU: Ep = exp 4 X 1 n=1 4 ! log Epn N"
P19-1483,D14-1020,0,0.0226453,"an idea of how the correlation of each metric varies with the underlying data. In each iteration, we sample with replacement, tables along with their references and all the generated texts for that table. Then we compute aggregated human evaluation and metric scores for each of the models and compute the correlation between the two. We report the average correlation across all bootstrap samples for each metric in Table 2. The distribution of correlations for the best performing metrics are shown in Figure 3. Table 2 also indicates whether PARENT is significantly better than a baseline metric. Graham and Baldwin (2014) suggest using the William’s test for this purpose, but since we are computing correlations between only 4/13 systems at a time, this test has very weak power in our case. Hence, we use the bootstrap samples to obtain a 1 − α confidence interval of the difference in correlation 4889 1.0 WikiBio-Systems 0.8 1.0 WikiBio-Hyperparams 0.8 0.8 0.6 0.6 0.4 0.6 0.2 0.0 U -T -F C BLE BLEU RG PRT- PRT-WPRT*-W 0.0 U -T -F C BLE BLEU RG PRT- PRT-WPRT*-W Figure 3: Distribution of metric correlations across 500 bootstrap samples. PRT = PARENT. between PARENT and any other metric and check whether this is ab"
P19-1483,D17-1274,0,0.041252,"Missing"
P19-1483,P83-1022,0,0.360705,"gh a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system."
P19-1483,D10-1090,0,0.0335412,"Bio, the recall of the references against the table is generally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic met"
P19-1483,D16-1230,0,0.0372707,"nder the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first study which checks t"
P19-1483,D18-1429,0,0.0249933,"relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006"
P19-1483,D17-1238,0,0.267569,"proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) a"
P19-1483,W17-5525,0,0.311156,"proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) a"
P19-1483,P02-1040,0,0.106059,"7; Novikova et al., 2017b; Gardent et al., 2017). For text generation tasks, the only gold standard metric is to show the output to humans for judging its quality, but this is too expensive to apply ∗ Work done during an internship at Google. Code and Data: http://www.cs.cmu.edu/ ~bdhingra/pages/parent.html repeatedly anytime small modifications are made to a system. Hence, automatic metrics that compare the generated text to one or more reference texts are routinely used to compare models (Bangalore et al., 2000). For table-to-text generation, automatic evaluation has largely relied on BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). The underlying assumption behind these metrics is that the reference text is gold-standard, i.e., it is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an example from the WikiBio dataset (Lebret et al., 2016). Here the reference contains extra information which no system can be expected to produce given only the associated table. We call such reference texts divergent from the table. We show that existing automatic metrics, inc"
P19-1483,D14-1162,0,0.0825959,"Missing"
P19-1483,J18-3002,0,0.0906513,"PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Burch et al., 2006; Stent et al., 2005; Belz and Reiter, 2006; Reiter, 2018; Liu et al., 2016; Kilickaya et al., 2017; Gatt and Krahmer, 2018). The only consensus from these studies seems to be that no single metric is suitable across all tasks. A recurring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first st"
P19-1483,D18-1437,0,0.0608232,"curring theme is that metrics like BLEU and NIST (Doddington, 2002) are not suitable for judging content quality in NLG. Recently, Novikova et al. (2017a) did a comprehensive study of several metrics on the outputs of state-of-the-art NLG systems, and found that while they showed acceptable correlation with human judgments at the system level, they failed to show any correlation at the sentence level. Ours is the first study which checks the quality of metrics when tableto-text references are divergent. We show that in this case even system level correlations can be unreliable. Hallucination (Rohrbach et al., 2018; Lee et al., 2018) refers to when an NLG system generates text which mentions extra information than what is present in the source from which it is generated. Divergence can be viewed as hallucination in the reference text itself. PARENT deals with hallucination by discounting n-grams which do not overlap with either the reference or the table. PARENT draws inspiration from iBLEU (Sun and Zhou, 2012), a metric for evaluating paraphrase generation, which compares the generated text to both the source text and the reference. Conclusions We study the automatic evaluation of table-to-text systems"
P19-1483,P12-2008,0,0.369925,"erences were also included in the evaluation, and they received a lower score than PG-Net, highlighting the divergence in WikiBio. 5.3 Compared Metrics Text only: We compare BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr and CIDErD (Vedantam et al., 2015) using their publicly available implementations. Information Extraction based: We compare the CS, RG and RG-F metrics discussed in §4. Text & Table: We compare a variant of BLEU, denoted as BLEU-T, where the values from the table are used as additional references. BLEUT draws inspiration from iBLEU (Sun and Zhou, 2012) but instead rewards n-grams which match the table rather than penalizing them. For PARENT, we compare both the word-overlap model (PARENT-W) and the co-occurrence model (PARENT-C) for determining entailment. We also compare versions where a single λ is tuned on the entire dataset to maximize correlation with human judgments, denoted as PARENT*-W/C. WikiBio Systems WikiBio Hyperparams Avg ROUGE CIDEr CIDEr-D METEOR BLEU 0.518±0.07C,W 0.674±0.06C,W 0.646±0.06C,W 0.697±0.06C,W 0.548±0.07C,W -0.585±0.15C,W -0.516±0.15C,W -0.372±0.16C,W -0.079±0.24C,W 0.407±0.15C,W -0.034 0.079 0.137 0.309 0.478 C"
P19-1483,N18-1136,0,0.0306942,"Missing"
P19-1483,D17-1239,0,0.530722,"red data. We show that metrics which rely solely on the reference texts, such as BLEU and ROUGE, show poor correlation with human judgments when those references diverge. We propose a new metric, PARENT, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall. Through a large scale human evaluation study of table-to-text models for WikiBio, we show that PARENT correlates with human judgments better than existing text generation metrics. We also adapt and evaluate the information extraction based evaluation proposed in Wiseman et al. (2017), and show that PARENT has comparable correlation to it, while being easier to use. We show that PARENT is also applicable when the reference texts are elicited from humans using the data from the WebNLG challenge.1 1 Introduction The task of generating natural language descriptions of structured data (such as tables) (Kukich, 1983; McKeown, 1985; Reiter and Dale, 1997) has seen a growth in interest with the rise of sequence to sequence models that provide an easy way of encoding tables and generating text from them (Lebret et al., 2016; Wiseman et al., 2017; Novikova et al., 2017b; Gardent et"
P19-1483,Q16-1029,0,0.0311333,"ally low, and hence the recall of the generated text relies more on the table. For WebNLG, where the references are elicited from humans, this recall is much higher (often 1.0), and hence 12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work. 4891 While iBLEU penalizes texts which match the source, here we reward such texts since our task values accuracy of generated text more than the need for paraphrasing the tabular content (Liu et al., 2010). Similar to SARI for text simplification (Xu et al., 2016) and Q-BLEU for question generation (Nema and Khapra, 2018), PARENT falls under the category of task-specific metrics. 400 WikiBio WebNLG 350 Frequency 300 250 200 150 100 50 0 0.0 0.2 0.4 0.6 0.8 1.0 1−λ 7 Figure 5: Histogram of the recall of the references against the table (Eq. 6), which is used to set 1 − λ. Lower values indicate that the metric relies more on the table and less on the reference. the recall of the generated text relies more on the reference. 6 Related Work Over the years several studies have evaluated automatic metrics for measuring text generation performance (Callison-Bu"
P19-1483,E17-1019,0,\N,Missing
P19-1483,P17-1099,0,\N,Missing
W04-3240,J96-2004,0,0.0371147,"rocessed by removing quoted material, attachments, and non-subject header information. This preprocessing was performed manually, but was limited to operations which can be reliably automated. The most difficult step is removal of quoted material, which we address elsewhere (Carvalho & Cohen, 2004). 4.2 Inter-Annotator Agreement Each message may be annotated with several labels, as it may contain several speech acts. To evaluate inter-annotator agreement, we doublelabeled N03F2 for the verbs Deliver, Commit, Request, Amend, and Propose, and the noun, Meeting, and computed the kappa statistic (Carletta, 1996) for each of these, defined as κ= A −R 1− R where A is the empirical probability of agreement on a category, and R is the probability of agreement for two annotators that label documents at random (with the empirically observed frequency of each label). Hence kappa ranges from -1 to +1. The results in Table 1 show that agreement is good, but not perfect. Email Act Kappa Meeting 0.82 Deliver 0.75 Commit 0.72 Request 0.81 Amend 0.83 Propose 0.72 Table 1 - Inter-Annotator Agreement on N03F2. We also took doubly-annotated messages which had only a single verb label and constructed the 5-class conf"
W04-3240,W02-1011,0,0.0295964,"Missing"
W04-3240,C96-2125,0,0.0226534,"Missing"
W04-3240,W01-1626,0,0.00873616,"Missing"
W06-3315,P02-1034,0,0.05476,"Missing"
W06-3315,W06-3801,1,0.755518,"neId nodes and synonym string nodes created from the dictionary, and for each historical-data abstract, we include links to its associated geneId nodes. Given this graph, gene identifiers for an abstract are generated by traversing the graph away from the abstract node, and looking for geneId nodes that are “close” to the abstract according to a certain proxim3 In fact, all edges have inverses in the graph. 94 ity measure for nodes. Similarity between two nodes is defined by a lazy walk process, similar to PageRank with decay. The details of this are described in the full paper and elsewhere (Minkov et al., 2006). Intuitively, however, this measures the similarity of two nodes by the weighted sum of all paths that connect the nodes, where shorter paths will be weighted exponentially higher than longer paths. One consequence of this measure is that information associated with paths like the one on the left-hand side of the graph—which represents a soft-match between a likely-protein and a synonym—can be reinforced by other types of paths, like the one on the right-hand side of the figure. As shown in Table 2, the graph-based approach has performance intermediate between the two baseline systems. Howeve"
W06-3406,J96-2004,0,0.13209,"Missing"
W06-3406,W04-3240,1,0.83209,"Missing"
W06-3801,J05-1003,0,0.084058,"work could be used for many types of tasks, and it is unlikely that a single set of parameter values will be best for all tasks. It is thus important to consider the problem of learning how to better rank graph nodes. Previous researchers have described schemes for adjusting the parameters θ using gradient descentlike methods (Diligenti et al., 2005; Nie et al., 2005). In this paper, we suggest an alternative approach of learning to re-order an initial ranking. This reranking approach has been used in the past for metasearch (Cohen et al., 1999) and also several naturallanguage related tasks (Collins and Koo, 2005). The advantage of reranking over parameter tuning is that the learned classifier can take advantage of “global” features that are not easily used in walk. Note that node reranking, while can be used as an alternative to weight manipulation, it is better viewed as a complementary approach, as the techniques can be naturally combined by first tuning the parameters θ, and then reranking the result using a classifier which exploits non-local features. This hybrid approach has been used successfully in the past on tasks like parsing (Collins and Koo, 2005). We here give a short overview of the rer"
W06-3801,H05-1056,1,0.925903,"(xi,j ,α)) ¯ e−(F (xi,1 ,α)−F i j=2 where xi,1 is, without loss of generality, a correct target node. The weights for the function are learned with a boosting-like method, where in each iteration the feature fk that has the most impact on the loss function is chosen, and αk is modified. Closed form formulas exist for calculating the optimal additive updates and the impact per feature (Schapire and Singer, 1999). 4 5 Evaluation We experiment with three separate corpora. The Cspace corpus contains email messages collected from a management course conducted at Carnegie Mellon University in 1997 (Minkov et al., 2005). In this course, MBA students, organized in teams of four to six members, ran simulated companies in different market scenarios. The corpus we used here includes the emails of all teams over a period of four days. The Enron corpus is a collection of mail from the Enron corpus that has been made available for the research community (Klimt and Yang, 2004). Here, we used the saved email of two different users.2 To eliminate spam and news postings we removed email files sent from email addresses with suffix “.com” that are not Enron’s; widely distributed email files (sent from “enron.announcement"
W06-3801,J97-1003,0,\N,Missing
W06-3801,P02-1062,0,\N,Missing
W11-0703,baccianella-etal-2010-sentiwordnet,0,0.309526,"t, in this work, we focus on the task of predicting the sentiment that a block of text will evoke in readers, expressed in the comment section, as a response to the blog post. This task is related to, but distinct from, several other studies that have been made using comments and discussions in political communities, or analysis of sentiment in comments - (Yano et al., 2009), (O’Connor et al., 2010), (Tumasjan et al., 2010). Below we discuss the methods used to address the various parts of this task. First, we evaluate two methods to automatically determine the comment polarity: SentiWordNet (Baccianella and Sebastiani, 2010) a general purpose resource that assigns sentiment scores to entries in WordNet, and an auto12 Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 12–19, c Portland, Oregon, 23 June 2011. 2011 Association for Computational Linguistics mated corpus-specific technique based on pointwise mutual information. The quality of the polarity assessments by these techniques are made by comparing them to hand annotated assessments on a small number of blog posts. Second, we consider two methods for predicting comment polarity from post content: support vector machine classification,"
W11-0703,P02-1053,0,0.00347423,"d because we don’t perform word sense disambiguation, the SentiWordNet polarity of the most dominant word sense is used for words in the comment section. The sentiment in the comment section is deemed to be positive if the net positive score exceeds the negative score and negative otherwise. Therefore, each blog post is now associated with a binary response variable indicating the polarity of the sentiment expressed in the comments. 3.2 Using pointwise mutual information A second technique to determine the sentiment polarity of comments uses the principle of pointwise mutual information (PMI)(Turney, 2002). We first construct a seed list of positive and negative words by choosing the 100 topmost positive and negative words from SentiWordNet and manually eliminating words from this list that don’t pertain to sentiment in our context. (Appendix A has the list of seed words used.) This seed list is used to construct a larger set of positive and negative words by computing the PMI of the words in the seed lists with every other word in the vocabulary. It’s important to note that this list is constructed for the specific corpus that we work with. Because every blog is processed separately, we constr"
W11-0703,N09-1054,1,0.830442,"Missing"
W11-2202,S07-1012,0,0.0908542,"Missing"
W11-2202,D08-1031,0,0.0174584,"rence resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future work might consider how to exploit such features for the more holistic information extraction setting. 10 9 Conclusion This paper presents a Bayesian nonparametric approach to recover structured records from text. Using only a small set of prototype records, we are able to recover an accurate table that jointly identifies entities and internal name structure. In our view, the main advantage of a Bayesian approach compared to more heuristic alternatives is that it facilitates incorporation of additional information sources when availab"
W11-2202,W98-1118,0,0.0439867,"rk. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches be"
W11-2202,P11-1098,0,0.0389599,"text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not"
W11-2202,N01-1007,0,0.441624,"m of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem tha"
W11-2202,doddington-etal-2004-automatic,0,0.0381729,"ave many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a system’s response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). Systems The initial seed set for our system consists of a partial annotation of five entities (Table 1) — larger seed sets did not improve performance. We run the inference procedure described in the previous section for 20,000 iterations, and then obtain a final database by taking the intersection of the in¯ obtained at every 100 iterations, startferred tables x ing with iteration 15,000. To account for variance across Markov chains, we perform three different runs. We evaluate a non-temporal version of our model (as described in Sections 3 and 4), and a temporal version with 5 epochs. For"
W11-2202,S07-1058,0,0.0623979,"that appear in names (such as titles and first names). We are aware of no existing system that performs all three of these tasks jointly. We evaluate on a dataset of political blogs, measuring our system’s ability to discover a set of reference entities (recall) while maintaining a compact number of rows and columns (precision). With as few as five partially-complete prototype examples, our approach gives accurate tables that match well against a manually-annotated reference list. Our method outperforms a baseline singlelink clustering approach inspired by one of the most successful entries (Elmacioglu et al., 2007) in the SEMEVAL “Web People Search” shared task (Artiles et al., 2007). 2 Task Definition In this work, we assume that a bag of M mentions in text have been identified. The mth mention wm is a sequence of contiguous word tokens (its length is denoted Nm ) understood to refer to a real-world entity. The entities (and the mapping of mentions to entities) are not known in advance. While our focus in this paper is names of people, the task is defined in a more generic way. Formally, the task is to construct a table x where rows correspond to entities and columns to functional fields. The number of"
W11-2202,N09-1019,0,0.0814631,"d entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem that we consider here, none provides a holistic treatment of name disambiguation and structure. Resolving Mentions to Entities The problem of resolving mentions to entities has been approach from a var"
W11-2202,P05-1045,0,0.0716754,"the entities which are mentioned in raw text. We annotate a new dataset of blog text for this purpose, and design precision and recall metrics to reward systems that recover as much of the reference set as possible, while avoiding spurious entities and fields. We also perform a qualitative analysis, noting the areas where our method outperforms string matching approaches, and where there is need for further improvement. Data Evaluation was performed on a corpus of blogs describing United States politics in 2008 (Eisenstein and Xing, 2010). We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people. We then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens (these were usually errors). The resulting dataset has 19,247 mentions comprising 45,466 word tokens, and 813 unique mention strings. Gold standard We develop a reference set of 100 entities for evaluation. This set was created by sorting the unique name strings in the training set by frequency, and manually merging strings that reference the same entity. We also manually discarded strings from"
W11-2202,P07-1107,0,0.0705131,"ns and abbreviations lead to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi a"
W11-2202,P10-2054,0,0.24949,"in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache"
W11-2202,N10-1061,0,0.177867,"in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache"
W11-2202,D08-1068,0,0.0585186,"to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future"
W11-2202,P11-1080,0,0.346535,"for the 100 entities. Most entities only include first and last names, though the most frequent entities have many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a system’s response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). Systems The initial seed set for our system consists of a partial annotation of five entities (Table 1) — larger seed sets did not improve performance. We run the inference procedure described in the previous section for 20,000 iterations, and then obtain a final database by taking the intersection of the in¯ obtained at every 100 iterations, startferred tables x ing with iteration 15,000. To account for variance across Markov chains, we perform three different runs. We evaluate a"
W11-2202,W02-2024,0,0.109779,"ng over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametri"
W11-2202,N10-1082,0,\N,Missing
W12-2037,N07-4013,0,0.0113971,"lize rigorously, it seems intuitively plausible. For instance, user-generated content from Twitter and Facebook is mainly comprised of short, shallow snippets of information. Most current research in AI (and more broadly in computer science) does not seem likely to reverse this trend: e.g., work in crowdsourcing has concentrated on tasks that can be easily decomposed into small pieces, and much current NLP research aims at facilitating short-term “shallow” goals, such as answering well-formulated questions (e.g., (Kwok et al., 2001)) and extracting concrete facts (e.g., (Etzioni et al., 2006; Yates et al., 2007; Carlson et al., 2010)). This raises the question: what can AI do to facilitate deep, contemplative study? In this paper we address one aspect of this larger goal. Specifically, we consider automation of a novel task—using AI methods to facilitate the “deep comprehension” of complex technical material. We conjecture that the primary reason that technical documents are difficult to understand is lack of modularity: unlike a self-contained document written for a general reader, technical documents require certain background knowledge to comprehend—while that background knowledge may also be ava"
W12-2037,W09-3206,0,0.0170773,"ing for domain-independent prerequisite-structure prediction, as this suggests that for the prerequisite classification task, close to optimal (i.e., in-domain performance) is possible when the classifiers are trained in an out-of-domain setting. 3 Related Work We believe the task of prerequisite structure prediction to be novel; however, it is clearly related to a number of other well-studied research problems. In light of our emphasis on Wikipedia, a connection can be drawn between identifying prerequisites and measuring the semantic relatedness of concepts using Wikipedia’s link structure (Yeh et al., 2009). We consider here a related but narrower question, namely whether an inter-page link will improve comprehension for a specific reader. In the area of intelligent tutoring and educational data mining, recent research has looked at enriching textbooks with authoritative web content (Agrawal et al., 2010). Also, the problem of detecting prerequisite structure from differential student performance on tests has been considered (e.g., (Pavlik et al., 2008; Vuong et al., 2011)). Our proposal considers discovering prerequisite structure from text, rather than from exercises, and relies on different s"
W12-2402,W09-2201,0,0.194635,"ies with seed examples. However, as we will show, using NELL’s bootstrapping algorithm to extract facts from a biomedical corpus is susceptible to noisy and ambiguous terms. Such ambiguities are common in biomedical terminology (see examples in Table 1), and some ambiguous terms are heavily used in the literature. For example, in the sentence “We have cloned an induced white mutation and characterized the insertion sequence responsible for the mutant phenotype”, white is an ambiguous term referring to the name of a gene. In NELL, ambiguity is limited us12 ing coupled semi-supervised learning (Carlson et al., 2009): if two categories in the ontology are declared mutually exclusive, instances of one category are used as negative examples for the other, and the two categories cannot share any instances. To resolve the ambiguity of white with mutual exclusion, we would have to include a Color category in the ontology, and declare it mutually exclusive with the Gene category. Then, instances of Color will not be able to refer to genes in the KB. It is hard to estimate what additional categories should be added, and building a “complete” ontology tree is practically infeasible. NELL also includes a polysemy"
W12-2402,J90-1003,0,0.17651,"a seed quality metric based on a large corpus of Web data. Let s and c be a seed and a target category, respectively. For example, we can take 14 s = “white”, the name of a gene of the fruit-fly, and c = “fly gene”. Now, let D be a document corpus (Section 3.2 describes the Web corpus used for ranking), and let Dc be a subset of the documents containing a mention of the category name. We measure the collocation of the seed and the category by the number of times s appears in Dc , |Occur(s, Dc )|. The overall occurrence of s in the corpus is given by |Occur(s, D)|. Following the formulation of Church and Hanks (1990), we compute the PMIrank of s and c as PMI(s, c) = |Occur(s, Dc )| |Occur(s, D)| (1) Since this measure is used to compare seeds of the same category, we omit the log from the original formulation. In our example, as white is a highly ambiguous gene name, we find that it appears in many documents that do not discuss the fruit fly, resulting in a PMI rank close to 0. The proposed ranking is sensitive to the descriptive name given to categories. For a more robust ranking, we use a combination of rankings of the seed with several of its ancestors in the ontology hierarchy. In (Movshovitz-Attias a"
W12-2402,N10-1087,0,0.191671,"Missing"
W12-2402,P11-1058,0,0.0841033,"e ambiguity of white with mutual exclusion, we would have to include a Color category in the ontology, and declare it mutually exclusive with the Gene category. Then, instances of Color will not be able to refer to genes in the KB. It is hard to estimate what additional categories should be added, and building a “complete” ontology tree is practically infeasible. NELL also includes a polysemy resolution component that acknowledges that one term, for example white, may refer to two distinct concepts, say a color and a gene, that map to different ontology categories, such as Color and Fly Gene (Krishnamurthy and Mitchell, 2011). By including a Color category, this component can identify that white is both a color and a gene. The polysemy resolver performs word sense induction and synonym resolution based on relations defined between categories in the ontology, and labeled synonym examples. However, at present, BioNELL’s ontology does not contain relation definitions (it is based only on categories), so we cannot include this component in our experiments. Additionally, it is unclear how to avoid the use of polysemous terms as category seeds, and no method has been suggested for selecting seeds that are representative"
W12-2402,P09-1045,0,0.0735181,"in NELL to learn a large set of category classifiers with high precision. One drawback of using iterative bootstrapping is the sensitivity of this method to the set of initial seeds (Pantel et al., 2009). An ambiguous set of seeds can lead to semantic drift, i.e., accumulation of erroneous terms and contexts when learning a semantic class. Strict bootstrapping environments reduce this problem by adding boundaries or limiting the learning process, including learning mutual terms and contexts (Riloff and Jones, 1999) and using mutual exclusion and negative class examples (Curran et al., 2007). McIntosh and Curran (2009) propose a metric for measuring the semantic drift introduced by a learned term, favoring terms different than the recent m learned terms and similar to the first n, (shown 13 for n=20 and n=100), following the assumption that semantic drift develops in late bootstrapping iterations. As we will show, for biomedical categories, semantic drift in NELL occurs within a handful of iterations (&lt; 5), however according to the authors, using low values for n produces inadequate results. In fact, selecting effective n and m parameters may not only be a function of the data being used, but also of the sp"
W12-2402,W12-2402,1,0.0528483,"ch and Hanks (1990), we compute the PMIrank of s and c as PMI(s, c) = |Occur(s, Dc )| |Occur(s, D)| (1) Since this measure is used to compare seeds of the same category, we omit the log from the original formulation. In our example, as white is a highly ambiguous gene name, we find that it appears in many documents that do not discuss the fruit fly, resulting in a PMI rank close to 0. The proposed ranking is sensitive to the descriptive name given to categories. For a more robust ranking, we use a combination of rankings of the seed with several of its ancestors in the ontology hierarchy. In (Movshovitz-Attias and Cohen, 2012) we describe this hierarchical ranking in more detail and additionally explore the use of the binomial loglikelihood ratio test (BLRT) as an alternative collocation measure for ranking. We further note that some specialized biomedical terms follow strict nomenclature rules making them easily identifiable as category specific. These terms may not be frequent in general Web context, leading to a low PMI rank under the proposed method. Given such a set of high confidence seeds from a reliable source, one can enforce their inclusion in the learning process, and specialized seeds can additionally b"
W12-2402,D09-1098,0,0.0609703,"ach, using bootstrapping to extend existing biomedical ontologies, including a wide range of sub-domains and many categories. The current implementation of BioNELL includes an ontology with over 100 categories. To the best of our knowledge, such large-scale biomedical bootstrapping has not been done before. Bootstrap Learning and Semantic Drift. Carlson et al. (2010) use coupled semi-supervised bootstrap learning in NELL to learn a large set of category classifiers with high precision. One drawback of using iterative bootstrapping is the sensitivity of this method to the set of initial seeds (Pantel et al., 2009). An ambiguous set of seeds can lead to semantic drift, i.e., accumulation of erroneous terms and contexts when learning a semantic class. Strict bootstrapping environments reduce this problem by adding boundaries or limiting the learning process, including learning mutual terms and contexts (Riloff and Jones, 1999) and using mutual exclusion and negative class examples (Curran et al., 2007). McIntosh and Curran (2009) propose a metric for measuring the semantic drift introduced by a learned term, favoring terms different than the recent m learned terms and similar to the first n, (shown 13 fo"
W12-2402,D09-1156,1,0.871198,"3.1 NELL’s Bootstrapping System We have implemented BioNELL based on the system design of NELL. NELL’s bootstrapping algorithm is initiated with an input ontology structure of categories and seeds. Three sub-components operate to introduce new facts based on the semantic and morphological attributes of known facts. At every iteration, each component proposes candidate facts, specifying the supporting evidence for each candidate, and the candidates with the most strongly supported evidence are added to the KB. The process and sub-components are described in detail by Carlson et al. (2010) and Wang and Cohen (2009). 3.2 Text Corpora PubMed Corpus: We used a corpus of 200K fulltext biomedical articles taken from the PubMed Central Open Access Subset (extracted in October 2010)1 , which were processed using the OpenNLP package2 . This is the main BioNELL corpus and it 1 2 http://www.ncbi.nlm.nih.gov/pmc/ http://opennlp.sourceforge.net is used to extract category instances in all the experiments presented in this paper. Web Corpus: BioNELL’s seed-quality collocation measure (Section 3.4) is based on a domainindependent Web corpus, the English portion of the ClueWeb09 data set (Callan and Hoy, 2009), which"
W12-2406,W10-1911,0,0.060718,"Missing"
W12-2406,W01-0516,0,0.554394,"Missing"
W12-2406,W02-0312,0,0.0891248,"Missing"
W12-3002,D08-1061,0,0.0843659,"Missing"
W12-3002,P09-1050,1,0.850621,"his paper, we propose a single lowdimensional representation of a large collection of table and hyponym data, and show that with a small number of primitive operations, this representation can be used effectively for many purposes. Specifically we consider queries like set expansion, class prediction etc. We evaluate our methods on publicly available semi-structured datasets from the Web. 1 Introduction Semi-structured data extracted from the web (in some cases extended with hyponym data derived from Hearst patterns like “X such as Y”) have been used in several tasks, including set expansion (Wang and Cohen, 2009b; Dalvi et al., 2010) automatic setinstance acquisition (Wang and Cohen, 2009a), fact extraction (Dalvi et al., 2012; Talukdar et al., 2008)), and semi-supervised learning of concepts (Carlson et al., 2010). In past work, these tasks have been addressed using different methods and data structures. In this paper, we propose a single low-dimensional representation of a large collection of table and hyponym data, and show that with a small number of primitive operations, this representation can be used effectively for many purposes. In particular, we propose a low-dimensional representation for"
W12-3002,D09-1156,1,0.932037,"his paper, we propose a single lowdimensional representation of a large collection of table and hyponym data, and show that with a small number of primitive operations, this representation can be used effectively for many purposes. Specifically we consider queries like set expansion, class prediction etc. We evaluate our methods on publicly available semi-structured datasets from the Web. 1 Introduction Semi-structured data extracted from the web (in some cases extended with hyponym data derived from Hearst patterns like “X such as Y”) have been used in several tasks, including set expansion (Wang and Cohen, 2009b; Dalvi et al., 2010) automatic setinstance acquisition (Wang and Cohen, 2009a), fact extraction (Dalvi et al., 2012; Talukdar et al., 2008)), and semi-supervised learning of concepts (Carlson et al., 2010). In past work, these tasks have been addressed using different methods and data structures. In this paper, we propose a single low-dimensional representation of a large collection of table and hyponym data, and show that with a small number of primitive operations, this representation can be used effectively for many purposes. In particular, we propose a low-dimensional representation for"
W12-4104,de-marneffe-etal-2006-generating,0,0.0466623,"Missing"
W12-4104,D07-1061,0,0.167638,"show that learning specialized similarity measures for different word types is advantageous. Figure 1: A joint graph of dependency structures 1 Introduction Many applications of natural language processing require measures of lexico-semantic similarity. Examples include summarization (Barzilay and Elhadad, 1999), question answering (Lin and Pantel, 2001), and textual entailment (Mirkin et al., 2006). Graph-based methods have been successfully applied to evaluate word similarity using available ontologies, where the underlying graph included word senses and semantic relationships between them (Hughes and Ramage, 2007). Another line of research aims at eliciting semantic similarity measures directly from freely available corpora, based on the distributional similarity assumption (Harria, 1968). In this domain, vector-space methods give state-ofthe-art performance (Pad´o and Lapata, 2007). Previously, a graph based framework has been proposed that models word semantic similarity from parsed text (Minkov and Cohen, 2008). The underlying graph in this case describes a text corpus as connected dependency structures, according to the schema shown in Figure 1. The toy graph shown includes the dependency analysis"
W12-4104,P98-2127,0,0.179952,"op 300 words retrieved by PCW as candidate terms for DV. We evaluate the following variants of DV: hav3 http://nlp.stanford.edu/software/lex-parser.shtml 22 movie : film murderer : assassin answered : replied enquire : investigate contemporary : modern infrequent : rare http://www.coli.uni-saarland.de/˜ pado/dv.html CO-Lin DV-Cos DV-Lin PCW PCW-P PCW-P-U Nouns 0.34 0.24 0.45 0.47 0.53 0.49 Verbs 0.37 0.36 0.49 0.55 0.68 0.65 Adjs 0.37 0.26 0.54 0.47 0.55 0.50 All 0.37 0.29 0.50 0.49 0.59 0.54 Table 2: 5-fold cross validation results: MAP ing inter-word similarity computed using Lin’s measure (Lin, 1998) (DV-Lin), or using cosine similarity (DV-Cos). In addition, we consider a non-syntactic variant, where a word’s vector consists of its cooccurrence counts with other terms (using a window of two words); that is, ignoring the dependency structure (CO-Lin). Finally, in addition to the PCW model described above (PCW), we evaluate the PCW approach in settings where random, noisy, edges have been eliminated from the underlying graph. Specifically, dependency links in the graph may be associated with pointwise mutual information (PMI) scores of the linked word mention pairs (Manning and Sch¨utze, 1"
W12-4104,D08-1095,1,0.8878,"-based methods have been successfully applied to evaluate word similarity using available ontologies, where the underlying graph included word senses and semantic relationships between them (Hughes and Ramage, 2007). Another line of research aims at eliciting semantic similarity measures directly from freely available corpora, based on the distributional similarity assumption (Harria, 1968). In this domain, vector-space methods give state-ofthe-art performance (Pad´o and Lapata, 2007). Previously, a graph based framework has been proposed that models word semantic similarity from parsed text (Minkov and Cohen, 2008). The underlying graph in this case describes a text corpus as connected dependency structures, according to the schema shown in Figure 1. The toy graph shown includes the dependency analysis of two sentences: “a major environmental disaster is under way“, and “combat the environmental catastrophe”. In the graph, word mentions (in circles) and word types (in squares) are both represented as nodes. Each word mention is linked to its corresponding word type; for example, the nodes “environmental3 ” and “environmental204 ” represent distinct word mentions and both nodes are linked to the word typ"
W12-4104,P06-2075,0,0.0179012,"us of parsed text. A constrained graph walk variant that has been successfully applied in the past in similar settings is shown to outperform a state-of-the-art syntactic vectorbased approach on this task. Further, we show that learning specialized similarity measures for different word types is advantageous. Figure 1: A joint graph of dependency structures 1 Introduction Many applications of natural language processing require measures of lexico-semantic similarity. Examples include summarization (Barzilay and Elhadad, 1999), question answering (Lin and Pantel, 2001), and textual entailment (Mirkin et al., 2006). Graph-based methods have been successfully applied to evaluate word similarity using available ontologies, where the underlying graph included word senses and semantic relationships between them (Hughes and Ramage, 2007). Another line of research aims at eliciting semantic similarity measures directly from freely available corpora, based on the distributional similarity assumption (Harria, 1968). In this domain, vector-space methods give state-ofthe-art performance (Pad´o and Lapata, 2007). Previously, a graph based framework has been proposed that models word semantic similarity from parsed"
W12-4104,J07-2002,0,0.0400321,"Missing"
W12-4104,P10-1097,0,0.0251197,"Missing"
W12-4104,C98-2122,0,\N,Missing
W16-1301,D15-1060,1,0.812529,"g examples. If the subject of a triple matches with a drug or disease title entity in a corpus and its object value also appears in that document, it is extracted. In total, we get 2022, 2453, 905, 753, and 164 triples for 5 disease relations respectively, and 3112, 315, and 265 triples for 3 drug relations, respectively. Each triple is used to label the document whose subject entity is the same as the triple subject. For instance, triple sideEffects(Aspirin,heartburn) will label a mention “heartburn” from the Aspirin article as an example of sideEffects relation. This raw data is very noisy (Bing et al., 2015; Bing et al., 2016), so we add a distillation step. We first distantly label these relations in two small structured corpora, namely, WebMD for drug and MayoClinic for disease.2 They have well-defined section information, which can be matched with target relations. We only label usedToTreat and conditionsThisMayPrevent from the “Uses” section, and label sideEffects from “Side Effects” section of WebMD. Similarly, the disease relations are labeled from “Treatments and drugs”,“Symptoms”, “Risk factors”, “Causes”, and “Prevention” sections of MayoClinic. After that we build a graph containing ex"
W16-1301,P11-1055,0,0.249315,"nt predictions at unlabeled points, and many graph-based SSL approaches require that the instances associated with the endpoints of an edge have similar labels (Zhu et al., 2003; Talukdar and Crammer, 2009). Other weakly-supervised methods also can be viewed as imposing constraints predictions made by a classifier: for instance, in distantlysupervised information extraction, constraints sometimes are imposed which requires that the classifier, when applied to the set S of mentions of an entity pair that is a member of relation r, classify at least one mention in S as a positive instance of r (Hoffmann et al., 2011). Different constraints (and different assumptions about the loss function for the learner) lead to different SSL algorithms. In this paper, we propose a general approach to modeling such constraints. In particular, we show that many types of constraints can be modeled by specifying the desired behavior of random walks through a graph of classifiers. In the graph, nodes correspond to relational conditions on small subsets of the data, and edges are annotated by feature vectors. Feature weights, combined with the feature vector at each edge and a non-linear postprocessing step, define a weighti"
W16-1301,P09-1113,0,0.0405789,"consider some combinations of these: SSL sd; SSL st; SSL dt; and SSL sdt. All the SSL pipelines employ the evaluation pages as unlabeled data for those constraints (i.e., they are used transductively). As one baseline, we compare to a standard supervised learning pipeline, SL, which learns a classifier with no constraints using ProPPR. We also compare against three existing methods: MultiR, (Hoffmann et al., 2011) which models each relation mention separately and aggregates their labels using a deterministic OR; Mintz++ from (Surdeanu et al., 2012), which improves on the original model from (Mintz et al., 2009) by training multiple classifiers, and allowing multiple labels per entity pair; and MIML-RE (Surdeanu et al., 2012) which has a similar structure to MultiR, but uses a classifier to aggregate the mention level predictions into an entity pair prediction. We used the publicly available code from the authors 3 for the experiments. Since these methods do not distinguish between structured and unstructured corpora, we used the union of these corpora in our experiments. We found that the performance of these methods varies significantly with the number of negative examples used during training, and"
W16-1301,D12-1042,0,0.0221559,"ument constraint; SSL t, only the section title constraint. We also consider some combinations of these: SSL sd; SSL st; SSL dt; and SSL sdt. All the SSL pipelines employ the evaluation pages as unlabeled data for those constraints (i.e., they are used transductively). As one baseline, we compare to a standard supervised learning pipeline, SL, which learns a classifier with no constraints using ProPPR. We also compare against three existing methods: MultiR, (Hoffmann et al., 2011) which models each relation mention separately and aggregates their labels using a deterministic OR; Mintz++ from (Surdeanu et al., 2012), which improves on the original model from (Mintz et al., 2009) by training multiple classifiers, and allowing multiple labels per entity pair; and MIML-RE (Surdeanu et al., 2012) which has a similar structure to MultiR, but uses a classifier to aggregate the mention level predictions into an entity pair prediction. We used the publicly available code from the authors 3 for the experiments. Since these methods do not distinguish between structured and unstructured corpora, we used the union of these corpora in our experiments. We found that the performance of these methods varies significantl"
W18-5306,E17-1104,0,0.0139805,"l which part of the input supports the indexed MeSH term. This would allow human indexers to be more effective at annotating the article. 2.2 Methods Deep Learning for Text Classification Automatic indexing of MeSH terms to PubMed articles is a multi-label text classification problem. FastText (Joulin et al., 2016) is a simple and effective method for classifying texts based on n-gram embeddings. (Kim, 2014) used Convolutional Neural Networks (CNNs) for sentencelevel classification tasks with state-of-the-art performance on 4 out of 7 tasks they tried. Very deep architectures such as that of (Conneau et al., 2017) have also been proposed for text classification. Motivated by these works we use an RNN-based model for classifying each MeSH term as being a positive label for a given article. We further use attention mechanism to boost performance and provide word-level interpretability. 3.1 Document Representation For each article to be indexed, we first tokenize the journal name, title and abstract to words. In order to use the pre-trained word embeddings6 provided by BioASQ organizer, we use the same tokenizer as they did. The pre-trained word embeddings are denoted as E ∈ R|V|×de1 , where |V |is the vo"
W18-5306,D14-1181,0,0.0110121,"Missing"
W18-5306,N18-1100,0,0.0204881,"s they did. The pre-trained word embeddings are denoted as E ∈ R|V|×de1 , where |V |is the vocabulary size and de1 is the embedding size. We can represent each article by a sequence of word embeddings corresponding to the tokenized text. The word embeddings are initialized by the BioASQ pre-trained word embeddings.  T D = w1 ... wL ∈ RL×de1 , Recently, there has been work on automatic annotation of International Classification of Diseases codes from clinical texts. (Shi et al., 2017) used character-level and word-level Long Short-Term Memory netowrks to get the document representations and (Mullenbach et al., 2018) used wordlevel 1-D CNN to get the document representations. Both these works utilized a soft attention strategy where each class gets a specific document represetation by weighted sum of the attention over words or phrases. Mullenbach et al. (2018) also highlighted the need for interpretability when annotating medical texts – in this work we apply similar ideas to the domain of MeSH indexing. where L is the number of words in the journal name, title and abstract, and wi is a vector for word at position i. For each document representation D, we feed this sequence of word vectors to an BiGRU to"
W18-6122,E17-1060,0,0.0134608,"ctors as shown in (Mikolov et al., 2013b). losscosine (w∗ , e∗wr ) = 1 − cos(E(w∗ ), e∗wr ) where, w∗ is the input term, and e∗wr is the reconstructed term vector. The resulting network can be trained end-toend to minimize the cross entropy loss between the output and target sequence L(y, y 0 ) in addition to the reconstruction loss between the input and reconstructed input vector L(w∗ , e∗wr ). Since the decode step is a greedy decode step, gradients cannot propagate through it. To solve this, we share the parameters between the two LSTM networks and forward and reconstruction linear layers (Chisholm et al., 2017). To generate definitions at test time, the backward network does not need to be evaluated. x01 = E[w∗ ] + E 0 [c(w∗ )] 6 To train the model, we use the 25k definitions dataset built as described in Section 4. We split the data randomly into 90:5:5 train, test and validation sets as shown in Table 2. The words being defined are mutually exclusive across the three sets, and thus our experiments evaluate how well the models generalize to new words. All of the models utilize the same set of fixed word embeddings from two different corpora. The first set of vectors are trained entirely on the Stac"
W18-6122,P85-1037,0,0.616058,"ion generation process. Our model also uses an additional loss function to reconstruct the entity word representation from the generated sequence. Our best model can generate definitions in software domain with a BLEU score of 10.91, improving upon the baseline by 2 points. In summary, our contributions are as follows, 1. We propose a new dataset of entities in the Introduction Dictionary definitions have been previously used in various Natural Language Processing (NLP) pipelines like knowledge base population (Dolan et al., 1993), relationship extraction, and extracting semantic information (Chodorow et al., 1985). Creating dictionaries in a new domain is time consuming, often requiring hand curation by domain experts with significant expertise. Developing systems to automatically learn and generate definitions of words can lead to greater time-efficiency (Muresan and Klavans, 2002). Additionally, it helps accelerate resource-building efforts for any new domain. In this paper, we study the task of generating definitions of domain-specific entities. In particular, our goal is to generate definitions for technical terms with the freely available Stack Overflow1 (SO) as our primary corpus. Stack Overflow"
W18-6122,muresan-klavans-2002-method,0,0.0702394,"nts. In summary, our contributions are as follows, 1. We propose a new dataset of entities in the Introduction Dictionary definitions have been previously used in various Natural Language Processing (NLP) pipelines like knowledge base population (Dolan et al., 1993), relationship extraction, and extracting semantic information (Chodorow et al., 1985). Creating dictionaries in a new domain is time consuming, often requiring hand curation by domain experts with significant expertise. Developing systems to automatically learn and generate definitions of words can lead to greater time-efficiency (Muresan and Klavans, 2002). Additionally, it helps accelerate resource-building efforts for any new domain. In this paper, we study the task of generating definitions of domain-specific entities. In particular, our goal is to generate definitions for technical terms with the freely available Stack Overflow1 (SO) as our primary corpus. Stack Overflow is a technical question-and-answer forum aimed 1 https://stackoverflow.com 164 Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 164–172 c Brussels, Belgium, Nov 1, 2018. 2018 Association for Computational Linguistics softwar"
W18-6122,P02-1040,0,0.102305,"flow is a technical question-and-answer forum aimed 1 https://stackoverflow.com 164 Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 164–172 c Brussels, Belgium, Nov 1, 2018. 2018 Association for Computational Linguistics software domain and their corresponding definitions, for the definition generation task. 2. We provide ways to incorporate domainspecific knowledge such as co-occurring entities and ontology information into a language model trained for the definition generation task. 3. We study the effectiveness of the model using the BLEU (Papineni et al., 2002) metric and present the results and discussion about our results. Section 4 of this paper presents the dataset. Section 5 discusses the model in detail. In Section 6 and 7 we present the experimental details and results of our experiments. Section 8 provides an analysis and discussion of the results and the generated definitions. chine translation (Luong et al., 2014; Bahdanau et al., 2014) and conversations and dialog systems (Shang et al., 2015; Wen et al., 2015). Reconstruction Loss Framework : We also build an explicit loss framework to reconstruct the term by reducing the cosine distance"
W18-6122,P15-1152,0,0.0345031,"Missing"
W18-6122,D17-1024,0,0.0153294,"fic differentia and genus concepts to the template structure. Definition modeling: The closest work related to ours is Noraset et al. (2017) who learn to generate definitions for general English words using a RNN language model initialized with pre-trained word embeddings. We adapt the method proposed by them and use it in a domain-specific construct. We aim to learn definitions of entities in the software domain. Hill et al. (2015) learn to produce distributed embeddings for words using their dictionary definitions as a means to bridge the gap between lexical and phrase semantics. Similarly, Tissier et al. (2017) use lexical definitions to augment the Word2Vec algorithm by adding an objective of reconstructing the words in the definition. In contrast, we focus solely on generating the definitions of entities. We add an objective of reconstructing the embedding of the word from the generated sequence. Also, all the above work focus on lexical definitions of general English words, while we focus on closed domain software terms. Dhingra et al. (2017) present a dataset of cloze-style queries constructed from definitions of software entities on Stack Overflow. In contrast to their work, we focus on generat"
W18-6122,D15-1199,0,0.0194158,"into a language model trained for the definition generation task. 3. We study the effectiveness of the model using the BLEU (Papineni et al., 2002) metric and present the results and discussion about our results. Section 4 of this paper presents the dataset. Section 5 discusses the model in detail. In Section 6 and 7 we present the experimental details and results of our experiments. Section 8 provides an analysis and discussion of the results and the generated definitions. chine translation (Luong et al., 2014; Bahdanau et al., 2014) and conversations and dialog systems (Shang et al., 2015; Wen et al., 2015). Reconstruction Loss Framework : We also build an explicit loss framework to reconstruct the term by reducing the cosine distance between the embedding of the term and the embedding of the reconstructed term. We adapt this approach from Hill et al. (2015) who apply it to learn word representations using dictionaries. Inan et al. (2016) propose a loss framework for language modeling to minimize the distribution distance between the prediction distribution and the true data distribution. Though we use a different loss framework, we use a similar type of parameter tying in our implementation. 2"
W19-2011,P18-2003,0,0.0249963,"ng layer and 12 transformer layers) for BioBERT. As fixed feature extractors, BioELMo and BioBERT are not finetuned by downstream tasks. Probing Tasks: Designing tasks to probe sentence or token representations for linguistic properties has been a widespread practice in NLP. InferSent (Conneau et al., 2017) uses transfer tasks to probe for sentence embeddings pre-trained on supervised data. Many studies (Dasgupta et al., 2018; Poliak et al., 2018) design new test sets to probe for specific linguistic signals in sentence rerpesentations. Tasks to probe for tokenlevel properties are explored by Blevins et al. (2018); Peters et al. (2018b), where they test whether token embeddings from different pretraining schemes encode part-of-speech and constituent structure. Tenney et al. (2018) extend token-level probing to span-level probing and consider a broader range of tasks. Our work is different from them in the following ways – (1) We probe for biomedical domain-specific contextualized embeddings and 3.2 Downstream Tasks We first use BioELMo with state-of-the-art models and fine-tune BioBERT on the downstream tasks, to test their full capacity. In §3.3 we introduce our probing setup which tests BioBERT and 3"
W19-2011,D15-1075,0,0.325765,"r, with contextual word embeddings, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), setting state-of-the-art performance on many tasks. These empirical successes suggest that unsupervised pre-training from large corpora could be a vital part of NLP models. In specific domains like biomedicine, NLP datasets are much smaller than their general-domain counterparts1 , which leads to a lot of ad-hoc models: some infer through knowledge bases (Chandu 1 For example, MedNLI (Romanov and Shivade, 2018) only has about 11k training instances while the general domain NLI dataset SNLI (Bowman et al., 2015) has 550k. 2 Available at https://github.com/Andy-jqa/ bioelmo. 82 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 82–89 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics 2 Related Work compare them to the general-domain embeddings; (2) For NER, instead of classifying the tag for a given span, we adopt an end-to-end setting where the spans must also be identified. This allows us to compare the probing results to state-of-the-art numbers; (3) We also probe for relational information using the NLI task in an end-to-end styl"
W19-2011,W17-2307,0,0.0665582,"Missing"
W19-2011,N18-1202,0,0.718272,"ontextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks. Pre-training on domain-specific corpora, such as biomedical articles, further improves their performance. In this paper, we conduct probing experiments to determine what additional information is carried intrinsically by the in-domain trained contextualized embeddings. For this we use the pre-trained LMs as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers. We compare BERT (Devlin et al., 2018), ELMo (Peters et al., 2018a), BioBERT (Lee et al., 2019) and BioELMo, a biomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We use visualization and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority. 1 Introduction NLP has seen an upheaval in the last year, with contextual word embeddings, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), setting stat"
W19-2011,D18-1179,0,0.307518,"ontextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks. Pre-training on domain-specific corpora, such as biomedical articles, further improves their performance. In this paper, we conduct probing experiments to determine what additional information is carried intrinsically by the in-domain trained contextualized embeddings. For this we use the pre-trained LMs as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers. We compare BERT (Devlin et al., 2018), ELMo (Peters et al., 2018a), BioBERT (Lee et al., 2019) and BioELMo, a biomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We use visualization and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority. 1 Introduction NLP has seen an upheaval in the last year, with contextual word embeddings, such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018), setting stat"
W19-2011,W18-5441,0,0.0267606,"Missing"
W19-2011,D17-1070,0,0.0403983,"okens in total). BioBERT was initialized with BERT and further trained on PubMed for 200K steps.5 To get fixed features of tokens, we use the learnt downstream task-specific layer weights to calculate the average of 3 layers (1 token embedding layer and 2 biLSTM layers) for BioELMo and 13 layers (1 token embedding layer and 12 transformer layers) for BioBERT. As fixed feature extractors, BioELMo and BioBERT are not finetuned by downstream tasks. Probing Tasks: Designing tasks to probe sentence or token representations for linguistic properties has been a widespread practice in NLP. InferSent (Conneau et al., 2017) uses transfer tasks to probe for sentence embeddings pre-trained on supervised data. Many studies (Dasgupta et al., 2018; Poliak et al., 2018) design new test sets to probe for specific linguistic signals in sentence rerpesentations. Tasks to probe for tokenlevel properties are explored by Blevins et al. (2018); Peters et al. (2018b), where they test whether token embeddings from different pretraining schemes encode part-of-speech and constituent structure. Tenney et al. (2018) extend token-level probing to span-level probing and consider a broader range of tasks. Our work is different from t"
W19-2011,C16-1030,0,0.0464031,"Missing"
W19-2011,D18-1187,0,0.222514,"Missing"
W19-2011,W18-5618,0,0.0227512,"ranks 824 in the 1B Word Benchmark dataset (Chelba et al., 2013). We use the Tensorflow implementation4 of ELMo to train BioELMo. We keep the default hyperparameters and it takes about 1.7K GPU hours to train 8 epochs. BioELMo achieves an averaged forward and backward perplexity of 31.37 on test set. Biomedical Word Embeddings: Contextindependent word embeddings, such as word2vec (w2v) (Mikolov et al., 2013) trained on biomedical corpora, are widely used in biomedical NLP models. Some recent works reported better NER performance with in-domain trained ELMo than general ELMo (Zhu et al., 2018; Sheikhshab et al., 2018). Lee et al. (2019) introduce BioBERT, which is BERT pre-trained on biomedical texts and set new state-of-the-art performance on several biomedical NLP tasks. We reaffirm these results on biomedical NER and NLI datasets with in-domain trained contextualized embeddings, and further explore why they are superior. BioBERT: In parallel to our work, Lee et al. (2019) developed BioBERT, which is pre-trained on English Wikipedia, BooksCorpus and finetuned on PubMed (7.8B tokens in total). BioBERT was initialized with BERT and further trained on PubMed for 200K steps.5 To get fixed features of tokens,"
W19-2011,W03-0419,0,0.352727,"Missing"
W19-2011,K17-1029,0,0.0656922,"Missing"
W19-2011,N16-1030,0,\N,Missing
W19-2011,P17-1152,0,\N,Missing
