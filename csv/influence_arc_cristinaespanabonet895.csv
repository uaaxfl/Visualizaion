2010.eamt-1.15,2007.mtsummit-papers.3,0,0.217612,"s there exist domain adaptation methods to improve results when these data sets are available, we devote most of the work to the first case, but we also check that the method does not hurt the performance in the second case. For this purpose, we complement the standard minimisation methods with an averaged perceptron-based re-estimation of parameters. Perceptrons have been used before with the aim of adding a large amount of new features to statistical systems avoiding the problem of the numerical minimisation of such a large vector of parameters (Liang et al., 2006; Tillmann and Zhang, 2006; Arun and Koehn, 2007). Other algorithms such as MIRA have been used for the same purpose (Arun and Koehn, 2007; Chiang et al., 2008). Here, the philosophy is different. We do not intend to include new information, but to profit better the available data as we will argue in the following. Even using the same data sets, the combination of MERT and the perceptron training can improve more than 2 points of BLEU the result of MERT alone. When including specialised data for development, the difference between MERT and the combined training is not so spectacular, but, still, the perceptron stage attains the leading resul"
2010.eamt-1.15,W05-0909,0,0.0382136,"e original test sets (Koehn, 2004). All the enhancements with respect to the MERT baseline result to be significant and are written in boldface in Table 2. Since the perceptron is maximising the BLEU score, it is on this metric that we mostly analyse the results, but the quality of the translation cannot be only judged in terms of BLEU. We thus investigate if the positive effects are also captured by other metrics. Table 3 summarises the results for a set of lexical metrics: WER (Nießen et al., 2000), BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The last metric, ULC (Gim´enez and Amig´o, 2006), performs a linear combination of a set of 33 lexical metrics, most of them variants of the ones appearing in the table (see Gim´enez (2007) for details). As a general trend, the same conclusions seen with BLEU can be extracted here. For the outof-domain test sets, the addition of the perceptron stage improves for all the metrics but WER the results with respect to MERT alone. For indomain test sets, the answer is not unique and the behaviour depends on the metric, so, there is no a clear effect of the second stage as the similarity of the ULC"
2010.eamt-1.15,W08-0304,0,0.0134784,"optimising the translation performance on a development set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this line, since we are not interested in finding the optimal parameters on development but on test. In this study, we see how parameters estimated with MERT can generalise quite bad on test sets that depart substantially from the training and development sets. Our goal is to find a more robust − → vector of weights λ that, even without being optimal on development or test when the domain is akin, better generalise on the other cases. We show empirically that this can be don"
2010.eamt-1.15,D08-1024,0,0.0663832,"the work to the first case, but we also check that the method does not hurt the performance in the second case. For this purpose, we complement the standard minimisation methods with an averaged perceptron-based re-estimation of parameters. Perceptrons have been used before with the aim of adding a large amount of new features to statistical systems avoiding the problem of the numerical minimisation of such a large vector of parameters (Liang et al., 2006; Tillmann and Zhang, 2006; Arun and Koehn, 2007). Other algorithms such as MIRA have been used for the same purpose (Arun and Koehn, 2007; Chiang et al., 2008). Here, the philosophy is different. We do not intend to include new information, but to profit better the available data as we will argue in the following. Even using the same data sets, the combination of MERT and the perceptron training can improve more than 2 points of BLEU the result of MERT alone. When including specialised data for development, the difference between MERT and the combined training is not so spectacular, but, still, the perceptron stage attains the leading results. The outline of the paper is as follows. Section 2 introduces the perceptron training, details the algorithm"
2010.eamt-1.15,W02-1001,0,0.0226998,"the choice of a gold standard, two key aspects of using this algorithm for machine translation. Section 3 describes and classifies the data used in the analysis. Afterwards, in Section 4, we detail our experiments. The first one, Crossdomain testing, is devoted to demonstrate how one can enhance his system for an out-of-domain test set by appending a perceptron training. The second experiment focuses on using an out-of-domain development set for tuning the system into the new domain. Finally, we draw our conclusions in Section 5. 2 Perceptron-based training The averaged structured perceptron (Collins, 2002) is an online mistake driven algorithm that determines the weights of a linear feature function by correcting their values according to the distance to the true solution. The score function that quantifies the quality of a translation in SMT, Eq. 1, is a linear function of the hm components. Therefore, the corresponding weights can be learned with the perceptron. Figure 1 details the perceptron algorithm. Given the training data set {f i ,ei }, an initial value for − → the weights λ 0 , the learning rate , and the number of epochs N , the perceptron translates (decodes) every sentence in the"
2010.eamt-1.15,W09-0439,0,0.0161484,"evelopment set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this line, since we are not interested in finding the optimal parameters on development but on test. In this study, we see how parameters estimated with MERT can generalise quite bad on test sets that depart substantially from the training and development sets. Our goal is to find a more robust − → vector of weights λ that, even without being optimal on development or test when the domain is akin, better generalise on the other cases. We show empirically that this can be done by including ma− → chine learning techiques to es"
2010.eamt-1.15,gimenez-amigo-2006-iqmt,0,0.0283452,"Missing"
2010.eamt-1.15,P07-2045,0,0.010094,"Missing"
2010.eamt-1.15,W04-3250,0,0.051125,"stopping of MERT does not have the same effect. We checked that the almost monotonous increment of BLEU throught MERT iterations on development sometimes translates into erratic BLEU results on test, especially when the domain of the data sets differs. The variance of BLEU scores on test can be large. There are values quite better than the last one but also quite worse, and there is no way to know when to obtain the best one from the training. In order to find out whether the results are statistically significant, we generated 1,000 sets by pair bootstrap resampling of the original test sets (Koehn, 2004). All the enhancements with respect to the MERT baseline result to be significant and are written in boldface in Table 2. Since the perceptron is maximising the BLEU score, it is on this metric that we mostly analyse the results, but the quality of the translation cannot be only judged in terms of BLEU. We thus investigate if the positive effects are also captured by other metrics. Table 3 summarises the results for a set of lexical metrics: WER (Nießen et al., 2000), BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The las"
2010.eamt-1.15,P06-1096,0,0.0583171,"Missing"
2010.eamt-1.15,P04-1077,0,0.0118277,"pair bootstrap resampling of the original test sets (Koehn, 2004). All the enhancements with respect to the MERT baseline result to be significant and are written in boldface in Table 2. Since the perceptron is maximising the BLEU score, it is on this metric that we mostly analyse the results, but the quality of the translation cannot be only judged in terms of BLEU. We thus investigate if the positive effects are also captured by other metrics. Table 3 summarises the results for a set of lexical metrics: WER (Nießen et al., 2000), BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The last metric, ULC (Gim´enez and Amig´o, 2006), performs a linear combination of a set of 33 lexical metrics, most of them variants of the ones appearing in the table (see Gim´enez (2007) for details). As a general trend, the same conclusions seen with BLEU can be extracted here. For the outof-domain test sets, the addition of the perceptron stage improves for all the metrics but WER the results with respect to MERT alone. For indomain test sets, the answer is not unique and the behaviour depends on the metric, so, there is no a clear effect of the sec"
2010.eamt-1.15,C08-1074,0,0.0162189,"nslation performance on a development set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this line, since we are not interested in finding the optimal parameters on development but on test. In this study, we see how parameters estimated with MERT can generalise quite bad on test sets that depart substantially from the training and development sets. Our goal is to find a more robust − → vector of weights λ that, even without being optimal on development or test when the domain is akin, better generalise on the other cases. We show empirically that this can be done by including ma− → chi"
2010.eamt-1.15,niessen-etal-2000-evaluation,0,0.0340119,"d out whether the results are statistically significant, we generated 1,000 sets by pair bootstrap resampling of the original test sets (Koehn, 2004). All the enhancements with respect to the MERT baseline result to be significant and are written in boldface in Table 2. Since the perceptron is maximising the BLEU score, it is on this metric that we mostly analyse the results, but the quality of the translation cannot be only judged in terms of BLEU. We thus investigate if the positive effects are also captured by other metrics. Table 3 summarises the results for a set of lexical metrics: WER (Nießen et al., 2000), BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The last metric, ULC (Gim´enez and Amig´o, 2006), performs a linear combination of a set of 33 lexical metrics, most of them variants of the ones appearing in the table (see Gim´enez (2007) for details). As a general trend, the same conclusions seen with BLEU can be extracted here. For the outof-domain test sets, the addition of the perceptron stage improves for all the metrics but WER the results with respect to MERT alone. For indomain test sets, the answer is not unique"
2010.eamt-1.15,P02-1038,0,0.0604686,"ptimum value on a development set with the expectation that these optimal weights generalise well to other test sets. However, this is not always the case when domains differ. This work uses a perceptron algorithm to learn more robust weights to be used on out-of-domain corpora without the need for specialised data. For an Arabic-to-English translation system, the generalisation of weights represents an improvement of more than 2 points of BLEU with respect to the MERT baseline using the same information. 1 Introduction In Statistical Machine Translation (SMT) and within the log-linear model (Och and Ney, 2002), the best translation eˆ for a given source sentence f is the most probable one, and the probability is expressed as a weighted sum of different elements: T (f ) = eˆ = argmaxe X λm hm (f, e) . (1) m In the standard most simple form, one considers 8 components being hm (f, e) log-probabilities of: the language model P (e), the generative and discriminative lexical translation probabilities lex(f |e) and lex(e|f ) respectively, the generative and discriminative translation models P (f |e) and P (e|f ), the distortion model Pd (e, f ), and the phrase and word penalties, ph(e) and w(e). c 2010 E"
2010.eamt-1.15,J03-1002,0,0.00375623,"Missing"
2010.eamt-1.15,P03-1021,0,0.0116114,"el P (e), the generative and discriminative lexical translation probabilities lex(f |e) and lex(e|f ) respectively, the generative and discriminative translation models P (f |e) and P (e|f ), the distortion model Pd (e, f ), and the phrase and word penalties, ph(e) and w(e). c 2010 European Association for Machine Translation. ° The λ weights, which account for the relative importance of each feature in the log-linear probabilistic model, are commonly estimated by optimising the translation performance on a development set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this"
2010.eamt-1.15,P02-1040,0,0.0871189,"e and discriminative lexical translation probabilities lex(f |e) and lex(e|f ) respectively, the generative and discriminative translation models P (f |e) and P (e|f ), the distortion model Pd (e, f ), and the phrase and word penalties, ph(e) and w(e). c 2010 European Association for Machine Translation. ° The λ weights, which account for the relative importance of each feature in the log-linear probabilistic model, are commonly estimated by optimising the translation performance on a development set. For this optimisation one can use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference score. MERT estimates the 8D best fit by searching the minimum in each dimension of the parameter space. The line search used in Och (2003) is demonstrated to find the absolute minimum in that direction, still, this does not guarantee that the best parameters obtained are the optimum ones. In fact, the larger the number of features, the less reliable the global minimisation will be. Some works such as Cer et al. (2008), Moore and Quirk (2008), or Foster and Kuhn (2009) try to improve the standard MERT minimisation. Here we do not follow this line, since we are not interested"
2010.eamt-1.15,P06-1091,0,0.209153,"s can be easier adapted. As there exist domain adaptation methods to improve results when these data sets are available, we devote most of the work to the first case, but we also check that the method does not hurt the performance in the second case. For this purpose, we complement the standard minimisation methods with an averaged perceptron-based re-estimation of parameters. Perceptrons have been used before with the aim of adding a large amount of new features to statistical systems avoiding the problem of the numerical minimisation of such a large vector of parameters (Liang et al., 2006; Tillmann and Zhang, 2006; Arun and Koehn, 2007). Other algorithms such as MIRA have been used for the same purpose (Arun and Koehn, 2007; Chiang et al., 2008). Here, the philosophy is different. We do not intend to include new information, but to profit better the available data as we will argue in the following. Even using the same data sets, the combination of MERT and the perceptron training can improve more than 2 points of BLEU the result of MERT alone. When including specialised data for development, the difference between MERT and the combined training is not so spectacular, but, still, the perceptron stage at"
2011.mtsummit-papers.63,2010.eamt-1.33,0,0.129673,"p towards hybridization. Although it has been shown to help in improving translation quality, the combination does not represent a real hybridization since systems do not interact among them (see Thurmair 555 (2009) for a classiﬁcation of HMT architectures). In the case of actual interdependences, one of the systems in action leads the translation process and the other ones strengthen it. Much work has been done in building systems where the statistical component is in charge of the translation and the companion system provides complementary information. For instance, Eisele et al. (2008) and Chen and Eisele (2010) introduce lexical information coming from a rule-based translator into an SMT system, in the form of new phrase pairs for the translation table. In both cases results are positive on out-of-domain tests. The opposite direction, that is, where the RBMT system leads the translation and the SMT system provides complementary information, has been less explored. Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally,"
2011.mtsummit-papers.63,W08-0328,0,0.0945801,"s’ outputs, is a ﬁrst step towards hybridization. Although it has been shown to help in improving translation quality, the combination does not represent a real hybridization since systems do not interact among them (see Thurmair 555 (2009) for a classiﬁcation of HMT architectures). In the case of actual interdependences, one of the systems in action leads the translation process and the other ones strengthen it. Much work has been done in building systems where the statistical component is in charge of the translation and the companion system provides complementary information. For instance, Eisele et al. (2008) and Chen and Eisele (2010) introduce lexical information coming from a rule-based translator into an SMT system, in the form of new phrase pairs for the translation table. In both cases results are positive on out-of-domain tests. The opposite direction, that is, where the RBMT system leads the translation and the SMT system provides complementary information, has been less explored. Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their"
2011.mtsummit-papers.63,W10-1708,0,0.424766,"arge of the translation and the companion system provides complementary information. For instance, Eisele et al. (2008) and Chen and Eisele (2010) introduce lexical information coming from a rule-based translator into an SMT system, in the form of new phrase pairs for the translation table. In both cases results are positive on out-of-domain tests. The opposite direction, that is, where the RBMT system leads the translation and the SMT system provides complementary information, has been less explored. Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Similar in spirit to Federmann et al. (2010), translations given by SMatxinT are controlled by the RBMT system in a way that will be clariﬁed in the following sections, but SMatxinT is enriched with a wider variety of SMT translation options. 3 3.1 A Hybrid MT Model Guided by RBMT Individual MT Systems Our hybrid model builds"
2011.mtsummit-papers.63,P07-2045,0,0.00881348,"way that will be clariﬁed in the following sections, but SMatxinT is enriched with a wider variety of SMT translation options. 3 3.1 A Hybrid MT Model Guided by RBMT Individual MT Systems Our hybrid model builds on three individual machine translation systems, a rule-based SpanishBasque system and two variants of regular phrase based statistical MT systems. These three subsystems are described below. SMT basic system (SMTb) The development of the baseline system was carried out using available state-of-the-art tools: GIZA++ toolkit (Och, 2003), SRILM toolkit (Stolcke, 2002) and Moses Decoder (Koehn et al., 2007). More particularly, we used a log-linear combination of several common feature functions: phrase translation probabilities (in both directions), word-based translation probabilities (lexicon model, in both directions), a phrase length penalty and the target language model. The language model is a simple 3-gram language model Figure 1: General architecture of SMatxinT. The RBMT modules which guide the MT process are the grey boxes with modiﬁed Kneser-Ney smoothing. We also used a lexical reordering model (‘msd-bidirectional-fe’ training option). Parameter optimization was done following the us"
2011.mtsummit-papers.63,W04-3250,0,0.076376,"Missing"
2011.mtsummit-papers.63,P03-1021,0,0.0159102,"ons given by SMatxinT are controlled by the RBMT system in a way that will be clariﬁed in the following sections, but SMatxinT is enriched with a wider variety of SMT translation options. 3 3.1 A Hybrid MT Model Guided by RBMT Individual MT Systems Our hybrid model builds on three individual machine translation systems, a rule-based SpanishBasque system and two variants of regular phrase based statistical MT systems. These three subsystems are described below. SMT basic system (SMTb) The development of the baseline system was carried out using available state-of-the-art tools: GIZA++ toolkit (Och, 2003), SRILM toolkit (Stolcke, 2002) and Moses Decoder (Koehn et al., 2007). More particularly, we used a log-linear combination of several common feature functions: phrase translation probabilities (in both directions), word-based translation probabilities (lexicon model, in both directions), a phrase length penalty and the target language model. The language model is a simple 3-gram language model Figure 1: General architecture of SMatxinT. The RBMT modules which guide the MT process are the grey boxes with modiﬁed Kneser-Ney smoothing. We also used a lexical reordering model (‘msd-bidirectional-"
2011.mtsummit-papers.63,P02-1040,0,0.0860558,"ieces of the source language corresponding to the tree constituents. The ﬁnal decoding accounts also for ﬂuency by using language models, and can be monotonic (and so, fast) because the structure has been already decided by the RBMT component. As a proof of concept we have instantiated and applied the SMatxinT architecture to a pair of structurally and morphologically distant languages, Spanish and Basque. The results obtained on several benchmark corpora show that the hybrid approach is able to signiﬁcantly improve the out-of-domain results of the best individual SMT system in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. A manual evaluation has been performed on a set of 100 samples from the test set verifying the signiﬁcant advantage of the SMatxinT hybrid system. More detailed analyses reveal that all the components of the hybrid system play an important role in the system (i.e., RBMT structural translation, SMT translation candidates and RBMT original translation). We think that the improvement obtained is remarkable given the simple statistical decoding process implemented so far. Indeed, the upper bound performance for the hybrid method calculated with the current se"
2011.mtsummit-papers.63,2006.amta-papers.25,0,0.0157519,"responding to the tree constituents. The ﬁnal decoding accounts also for ﬂuency by using language models, and can be monotonic (and so, fast) because the structure has been already decided by the RBMT component. As a proof of concept we have instantiated and applied the SMatxinT architecture to a pair of structurally and morphologically distant languages, Spanish and Basque. The results obtained on several benchmark corpora show that the hybrid approach is able to signiﬁcantly improve the out-of-domain results of the best individual SMT system in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. A manual evaluation has been performed on a set of 100 samples from the test set verifying the signiﬁcant advantage of the SMatxinT hybrid system. More detailed analyses reveal that all the components of the hybrid system play an important role in the system (i.e., RBMT structural translation, SMT translation candidates and RBMT original translation). We think that the improvement obtained is remarkable given the simple statistical decoding process implemented so far. Indeed, the upper bound performance for the hybrid method calculated with the current setting reveals that there is st"
2011.mtsummit-papers.63,2009.mtsummit-posters.21,0,0.336101,"Missing"
2011.mtsummit-wpt.7,P07-2045,0,0.0412064,"tistical translations the parts not covered by GF. However, the grammar must be expanded so that the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST ROUGE-W GTM-2 METEOR-pa ULC FR2DE Bing Google Domain Bing Google Domain 0.42 0.47"
2011.mtsummit-wpt.7,P03-1021,0,0.0589389,"st be expanded so that the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST ROUGE-W GTM-2 METEOR-pa ULC FR2DE Bing Google Domain Bing Google Domain 0.42 0.47 0.29 6.72 0.31 0.24 0.45 0.03 0.52 0.56 0.43 8.21 0.38 0.30 0.56 0.22"
2011.mtsummit-wpt.7,J03-1002,0,0.00441486,"h ambiguities, i.e., multiple translation options, and can complete with statistical translations the parts not covered by GF. However, the grammar must be expanded so that the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST RO"
2011.mtsummit-wpt.7,P02-1040,0,0.0882648,"t the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST ROUGE-W GTM-2 METEOR-pa ULC FR2DE Bing Google Domain Bing Google Domain 0.42 0.47 0.29 6.72 0.31 0.24 0.45 0.03 0.52 0.56 0.43 8.21 0.38 0.30 0.56 0.22 0.76 0.68 0.56 9.10 0"
2012.eamt-1.15,2011.eamt-1.9,0,0.0850637,"Missing"
2012.eamt-1.15,D07-1091,0,0.142983,"Missing"
2012.eamt-1.15,N03-1017,0,0.0074291,"Missing"
2012.eamt-1.15,2010.amta-commercial.12,0,0.0636708,"Missing"
2012.eamt-1.15,P05-1033,0,\N,Missing
2012.eamt-1.61,2010.eamt-1.33,0,0.0341778,"at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this context but is being applied in different domains and language pairs. Besides system combination strategies, hybrid models are designed so that there is one leading translation system assisted or complemented by other kinds of engines. This way the final translator benefits from the features of all the approaches. A family of models are based on SMT systems enriched with lexical information from RBMT (Eisele et al., 2008; Chen and Eisele, 2010). On the other side there are the models that start from the RBMT analysis and use SMT to complement it (Habash et al., 2009; Federmann et al., 2010; Espa˜na-Bonet et al., 2011b). Our work can be classified in the two families. On the one hand, SMT helps on the construction of the RBMT translator but, on the other hand, there is the final decoding step to integrate translations and complete those phrases untranslated by RBMT. We use GF as rule-based system. GF is a type-theoretical grammar formalism, 1 2 http://www.pluto-patenttranslation.eu/ http://www.molto-project.eu/ mainly used for multil"
2012.eamt-1.61,2007.mtsummit-wpt.4,0,0.085674,"tion 5 summarises the work and outlines possible lines to follow. 2 Related work This work tackles two topics which are lately attracting the attention of researchers, patent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a"
2012.eamt-1.61,W08-0328,0,0.0588025,"OLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this context but is being applied in different domains and language pairs. Besides system combination strategies, hybrid models are designed so that there is one leading translation system assisted or complemented by other kinds of engines. This way the final translator benefits from the features of all the approaches. A family of models are based on SMT systems enriched with lexical information from RBMT (Eisele et al., 2008; Chen and Eisele, 2010). On the other side there are the models that start from the RBMT analysis and use SMT to complement it (Habash et al., 2009; Federmann et al., 2010; Espa˜na-Bonet et al., 2011b). Our work can be classified in the two families. On the one hand, SMT helps on the construction of the RBMT translator but, on the other hand, there is the final decoding step to integrate translations and complete those phrases untranslated by RBMT. We use GF as rule-based system. GF is a type-theoretical grammar formalism, 1 2 http://www.pluto-patenttranslation.eu/ http://www.molto-project.eu"
2012.eamt-1.61,2011.mtsummit-wpt.7,1,0.890558,"Missing"
2012.eamt-1.61,P02-1040,0,0.0863732,"e using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2006; Koehn et al., 2007). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. A wider explanation of this system, the preprocess applied to the corpus before training the system and a deep evaluation of the translations can be found in Espa˜na-Bonet et al. (2011a). 3.3 GF system As explained in Section 2, the extension of GF to a new domain implies the construction of a specialised grammar that expands the general resource grammar. Since in our case of applica4 Figure 1: Architecture of the GF translation system. http://www.epo.org/ 271 tion we are far from a close and limited domain, some probabilistic components are also necessary. The general arch"
2012.eamt-1.61,2011.mtsummit-papers.63,1,0.780941,"Missing"
2012.eamt-1.61,W10-1708,0,0.0126031,"s not only useful in this context but is being applied in different domains and language pairs. Besides system combination strategies, hybrid models are designed so that there is one leading translation system assisted or complemented by other kinds of engines. This way the final translator benefits from the features of all the approaches. A family of models are based on SMT systems enriched with lexical information from RBMT (Eisele et al., 2008; Chen and Eisele, 2010). On the other side there are the models that start from the RBMT analysis and use SMT to complement it (Habash et al., 2009; Federmann et al., 2010; Espa˜na-Bonet et al., 2011b). Our work can be classified in the two families. On the one hand, SMT helps on the construction of the RBMT translator but, on the other hand, there is the final decoding step to integrate translations and complete those phrases untranslated by RBMT. We use GF as rule-based system. GF is a type-theoretical grammar formalism, 1 2 http://www.pluto-patenttranslation.eu/ http://www.molto-project.eu/ mainly used for multilingual natural language applications. Grammars in GF are represented as a pair of an abstract syntax –an interlingua that captures the semantics of"
2012.eamt-1.61,P07-2045,0,0.0204122,"patents. The language of patents follows a formal style adequate to be analysed with a grammar, but at the same time uses a rich and particular vocabulary adequate to be gathered statistically. We focus on the English-French language pair so that the effects of translating into a morphologically rich language can be studied. With respect to the engine, a grammar-based translator is developed to assure grammatically correct translations. We extend GF (Grammatical Framework, Ranta (2011)) and write a new grammar for patent translation. The SMT system that complements the RBMT is based on Moses (Koehn et al., 2007). This system works on two different levels. First, it is used to build the parallel lexicon of the GF translator on the fly. Second, it is the top level decoder that takes the final decision about which phrases should be used. In the following Section 2 describes recent work both in patent translation and hybrid systems. Section 3 explains our hybrid system and Section 4 evaluates its performance. Finally, Section 5 summarises the work and outlines possible lines to follow. 2 Related work This work tackles two topics which are lately attracting the attention of researchers, patent translation"
2012.eamt-1.61,J03-1002,0,0.00340697,"rk-up within the patent such as paragraph tags for example. Two small sets for development and test purposes have also been selected with the same restrictions: 993 fragments for development and 1008 for test. 3.2 In-domain SMT system The first component is a standard state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 3.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2006; Koehn et al., 2007). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. A wider explanation of this system, the preprocess applied to the corpus before training the system and a deep evaluation of the translations can be found in Espa˜"
2012.eamt-1.61,P03-1021,0,0.0115529,"Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2006; Koehn et al., 2007). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. A wider explanation of this system, the preprocess applied to the corpus before training the system and a deep evaluation of the translations can be found in Espa˜na-Bonet et al. (2011a). 3.3 GF system As explained in Section 2, the extension of GF to a new domain implies the construction of a specialised grammar that expands the general resource grammar. Since in our case of applica4 Figure 1: Architecture of the GF translation system. http://www.epo.org/ 271 tion we are far from a close and limited domain, some probabilistic compon"
2012.eamt-1.61,W04-2104,0,0.0234676,"Missing"
2012.eamt-1.61,W03-2008,0,0.0281322,"acting the attention of researchers, patent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not"
2012.eamt-1.61,2005.mtsummit-wpt.6,0,0.0495801,"of researchers, patent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this"
2012.eamt-1.61,2009.eamt-1.28,0,0.0457617,"ent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this context but is being"
2012.eamt-1.61,2009.mtsummit-posters.21,0,0.0427205,"sted, the technology evolved towards rule-based systems (RBMT). Later in the 90s the everyday more powerful computers allowed to develop empirical translation systems. Recently a type of empirical system, the statistical one (SMT), has become a widely used standard for translation. At this point the two main paradigms, RBMT and SMT, coexist with their strengths and weaknesses. Luckily these strengths and weaknesses are complementary and current efforts are being made to hybridise both of them and develop new technologies. A classification and description of hybrid translation can be found in (Thurmair, 2009). In general RBMT provides high precision, due to an analysis of the text, but has limited coverage c 2012 European Association for Machine Translation. 269 and a considerable amount of effort and linguistic knowledge is required in order to build such a system. On the other hand, SMT can achieve a huge coverage and is good at lexical selection and fluency but has problems in building structurally and grammatically correct translations. Hybrid MT (HMT) is an emerging and challenging area of machine translation, which aims at combining the known techniques into systems that retain the best feat"
2012.eamt-1.61,2011.eamt-1.5,0,\N,Missing
2012.eamt-1.61,N09-2055,0,\N,Missing
2012.freeopmt-1.7,E06-1032,0,0.0360443,"h et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of metric combination have been"
2012.freeopmt-1.7,P08-1007,0,0.0201128,"have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of metric combination have been tested. Due to its simplicity, we decided to use the idea presented by Gim´enez and M`arquez (2008), where a set of simple metrics are combined by means of the arithmetic mean. 66 This work presents a deep evaluation experiment of a hybrid architecture that tries to get the best of both worlds, rule-based and statistical. The results obtained corroborated the known doubts about BLEU. And suggests that the further development of the hybrid system should be guided by a linguistically more informed metric that should be able to capture the s"
2012.freeopmt-1.7,2011.mtsummit-papers.63,1,0.814633,"Missing"
2012.freeopmt-1.7,W10-1708,0,0.0154751,"n applied to corpora different from those used for training (out-of-domain evaluation). Because of these complementary virtues and drawbacks several works are being devoted to build hybrid systems with components of both approaches. A classification and a summary of hybrid architectures can be seen in Thurmair (2009). The case we present here is within the philosophy of those systems where the RBMT system leads the translation and the SMT system provides complementary information. Following this line, Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to i"
2012.freeopmt-1.7,W08-0332,1,0.87217,"Missing"
2012.freeopmt-1.7,P07-2045,0,0.00584996,"cing the sparseness produced by the agglutinative nature of Basque and the small amount of parallel corpora. Adapting the baseline system to work at the morpheme level mainly consists of training the decoder on the segmented text. The SMT system trained on segmented words generates a sequence of morphemes. So, in order to obtain the final Basque text from the segmented output, a word-generation post-process is applied. State-of-the-art tools are used in this case. GIZA++ toolkit (Och, 2003) is used for the alignments, SRILM toolkit (Stolcke, 2002) for the language model and the Moses Decoder (Koehn et al., 2007). We used a log-linear functions: phrase translation probabilities (in both directions), word-based translation probabilities (lexicon model, in both directions), a phrase length penalty and the target language model. The language model is a simple 3gram language model with modified Kneser-Ney smoothing. We also used a lexical reordering 1 http://www.opentrad.com 67 model (‘msd-bidirectional-fe’ training option). Parameter optimization was done following the usual practice, i.e., Minimum-Error-Rate Training (Och, 2003), however, the metric used for the optimization is not only BLEU, but it dep"
2012.freeopmt-1.7,W06-3114,0,0.0279294,"ctionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of metric combination have been tested. Due to its sim"
2012.freeopmt-1.7,W05-0904,0,0.0204949,"the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of metric combination have been tested. Due to its simplicity, we decided to use the idea presented by Gim´enez and M`arquez (2008), where a set of simple metrics are combined by means of the arithmetic mean. 66 This work presents a deep evaluation experiment of a hybrid architecture that tries to get the best of both worlds, rule-based and statistical. The results obtained corroborated the known doubts about BLEU. And suggests that the further development of the hybrid system should be guided by a linguistically more informed"
2012.freeopmt-1.7,N03-2021,0,0.0335585,"owing this line, Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic information (Liu and Gildea, 2005, Popovi´c and Ney, 2007, Chan and Ng, 2008), and different methods of"
2012.freeopmt-1.7,P03-1021,0,0.00972803,"system, words are split into several morphemes by using a Basque morphological analyzer/lemmatizer, aiming at reducing the sparseness produced by the agglutinative nature of Basque and the small amount of parallel corpora. Adapting the baseline system to work at the morpheme level mainly consists of training the decoder on the segmented text. The SMT system trained on segmented words generates a sequence of morphemes. So, in order to obtain the final Basque text from the segmented output, a word-generation post-process is applied. State-of-the-art tools are used in this case. GIZA++ toolkit (Och, 2003) is used for the alignments, SRILM toolkit (Stolcke, 2002) for the language model and the Moses Decoder (Koehn et al., 2007). We used a log-linear functions: phrase translation probabilities (in both directions), word-based translation probabilities (lexicon model, in both directions), a phrase length penalty and the target language model. The language model is a simple 3gram language model with modified Kneser-Ney smoothing. We also used a lexical reordering 1 http://www.opentrad.com 67 model (‘msd-bidirectional-fe’ training option). Parameter optimization was done following the usual practic"
2012.freeopmt-1.7,P02-1040,0,0.0992093,"he RBMT system leads the translation and the SMT system provides complementary information. Following this line, Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluation of the final system and its components, still nowadays, the BLEU metric (Papineni et al., 2002) is the most used metric in MT, but several doubts have arisen around it (Melamed et al., 2003, Callison-Burch et al., 2006, Koehn and Monz, 2006). In addition to the fact that it is extremely difficult to interpret what is being expressed in BLEU (Melamed et al., 2003), improving its value neither guarantees an improvement in the translation quality (Callison-Burch et al., 2006) nor offers such high correlation with human judgment as was believed (Koehn and Monz, 2006). In the last few years, several new evaluation metrics have been suggested to consider a higher level of linguistic informati"
2012.freeopmt-1.7,W07-0707,0,0.0606548,"Missing"
2012.freeopmt-1.7,2009.mtsummit-posters.21,0,0.0266657,"nslation more locally and have problems with long distance reordering. They also tend to produce very obvious errors, which are annoying for regular users, e.g., lack of gender and number agreement, bad punctuation, etc. Moreover, SMT systems can experience a severe degradation of performance when applied to corpora different from those used for training (out-of-domain evaluation). Because of these complementary virtues and drawbacks several works are being devoted to build hybrid systems with components of both approaches. A classification and a summary of hybrid architectures can be seen in Thurmair (2009). The case we present here is within the philosophy of those systems where the RBMT system leads the translation and the SMT system provides complementary information. Following this line, Habash et al. (2009) enrich the dictionary of a RBMT system with phrases from an SMT system. Federmann et al. (2010) use the translations obtained with a RBMT system and substitute selected noun phrases by their SMT counterparts. Globally, their results improve the individual systems when the hybrid system is applied to translate into languages with a richer morphology than the source. Regarding the evaluati"
2013.mtsummit-posters.10,W13-2715,1,0.877354,"Missing"
2013.mtsummit-posters.10,2011.mtsummit-wpt.7,1,0.89823,"Missing"
2013.mtsummit-posters.10,2010.amta-commercial.14,0,0.114865,"Missing"
2015.eamt-1.9,D13-1176,0,\N,Missing
2015.eamt-1.9,W14-4015,1,\N,Missing
2015.eamt-1.9,P02-1040,0,\N,Missing
2015.eamt-1.9,P12-3024,1,\N,Missing
2015.eamt-1.9,P07-2045,0,\N,Missing
2015.eamt-1.9,D14-1003,0,\N,Missing
2015.eamt-1.9,D11-1084,0,\N,Missing
2015.eamt-1.9,N13-1090,0,\N,Missing
2015.eamt-1.9,P13-4033,0,\N,Missing
2015.eamt-1.9,J03-1002,0,\N,Missing
2015.eamt-1.9,2010.iwslt-papers.10,0,\N,Missing
2015.eamt-1.9,P13-4014,0,\N,Missing
2015.eamt-1.9,W04-3250,0,\N,Missing
2015.eamt-1.9,Y14-1004,0,\N,Missing
2015.eamt-1.9,D12-1108,0,\N,Missing
2015.eamt-1.9,tiedemann-2012-parallel,0,\N,Missing
2015.eamt-1.9,C14-1017,0,\N,Missing
2015.eamt-1.9,P03-1021,0,\N,Missing
2015.eamt-1.9,P14-1129,0,\N,Missing
2020.cl-2.1,2020.cl-2.3,0,0.0613442,"Missing"
2020.cl-2.1,2020.acl-main.447,0,0.0363097,"he 2010s; red line). The fraction of papers mentioning two or more languages (yellow line) and the average per year (green line) showed increases in the 1990s and 2000s, though these appear to have slowed recently.3 The other trend is a matter of increasing supply: The diversity of computational tools now available—from conceptual definitions of language meaning to operationalizations in downloadable models—has exploded in the past decade. The term “semantic representation” was, not long ago, one that referred to a range of linguistic abstractions. 1 We explored ACL Anthology papers in S2ORC (Lo et al. 2020) with publication years 1980–2019, a total of 40,402 papers. 2 The list is Ethnologue’s list of the 20 most spoken languages in 2019, with Mandarin and Wu Chinese mapped to the string chinese. See https://www.ethnologue.com/guides/ethnologue200. Less dominant languages are, of course, also interesting, but also more sparse in the data. 3 The leveling off of these last two trends is, we speculate, due to the emergence of new representation learning methods that work best with very large data sets. We expect increasing multilinguality of the largest data sets and pretrained representations will"
2020.cl-2.1,2020.cl-2.2,0,0.426794,"cept may help to obtain more robust embeddings at sense level as shown by one of the works presented here. The contributions to this special issue are summarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granularity Word Paper (Mohiuddin and Joty 2020) Technique Application Languages Unsupervised Adv"
2020.cl-2.1,2020.cl-2.6,0,0.07929,"entary might also be true, and realizations in different languages of the same concept may help to obtain more robust embeddings at sense level as shown by one of the works presented here. The contributions to this special issue are summarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granulari"
2020.cl-2.1,J82-2005,0,0.392652,"us lines of work that illustrate a range of creative advances exploring natural language meaning, specifically with a multilingual focus. In inviting submissions, we encouraged a broad reading of the term “representations,” in granularity (words, sentences, paragraphs, etc.) and in theoretical assumptions (symbolic, neural, hybrid, etc.). We anticipated breadth as well in the set of motivating applications and evaluation methods. Our deliberate reference to interlingual—not only multilingual—representations evokes recent re-imaginings of interlingual machine translation, a classical approach (Richens 1958). We explicitly encouraged submissions that consider less-commonly studied languages and that go beyond mere projection of representations from text in one language to another. Of particular interest to our editorial team is the potential for multilingual representations (of any kind) to help overcome challenges of polysemy in individual languages. It has been shown that translations into other languages can help at distinguishing senses monolingually (Resnik and Yarowsky 1999). But the complementary might also be true, and realizations in different languages of the same concept may help to ob"
2020.cl-2.1,2020.cl-2.4,0,0.0730249,"ummarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granularity Word Paper (Mohiuddin and Joty 2020) Technique Application Languages Unsupervised Adversarial Translation en, es, de, it, fi, ar, ms, he Linked Data Intrinsic/extrinsic evaluation Word Similarity POS, dependencies, SRL, NER,"
2020.cl-2.1,2020.cl-2.5,0,0.0495885,"Missing"
2020.cl-2.1,D18-1268,0,0.0204012,"ion. The challenges addressed include learning unsupervised representations, introducing priors and linguistic knowledge to compute the representations, and evaluating the quality of these representations, taking into account linguistic features. Unsupervised Word Translation with Adversarial Encoder (Mohiuddin and Joty 2020). Crosslingual word embeddings are becoming crucial in multilingual natural language processing tasks and, recently, several authors claim that unsupervised methods even outperform the supervised ones (see for instance Lample et al. 2018, Artetxe, Labaka, and Agirre 2018, Xu et al. 2018), making them appealing also in the low-resource setting. This is not true in all cases, and specifically, adversarial techniques for dictionary induction show stability and convergence issues for some language pairs (Zhang et al. 2017; Lample et al. 2018). In general, unsupervised adversarial bilingual embeddings are learned in two phases: (i) induction of an initial seed dictionary using an adversarial network and (ii) refinement of the initial mapping, and therefore, dictionary, until convergence. This paper tries to address those limitations by extending adversarial autoencoders. One of th"
2020.cl-2.1,D17-1207,0,0.0236204,"tic features. Unsupervised Word Translation with Adversarial Encoder (Mohiuddin and Joty 2020). Crosslingual word embeddings are becoming crucial in multilingual natural language processing tasks and, recently, several authors claim that unsupervised methods even outperform the supervised ones (see for instance Lample et al. 2018, Artetxe, Labaka, and Agirre 2018, Xu et al. 2018), making them appealing also in the low-resource setting. This is not true in all cases, and specifically, adversarial techniques for dictionary induction show stability and convergence issues for some language pairs (Zhang et al. 2017; Lample et al. 2018). In general, unsupervised adversarial bilingual embeddings are learned in two phases: (i) induction of an initial seed dictionary using an adversarial network and (ii) refinement of the initial mapping, and therefore, dictionary, until convergence. This paper tries to address those limitations by extending adversarial autoencoders. One of the main contributions is training the adversarial mapping in a latent space, with the hope that this will minimize the effect of a lack of isomorphism between the two original embedding spaces. In addition, the authors combine several l"
2020.cl-2.1,Q17-1010,0,\N,Missing
2020.cl-2.1,Q19-1038,0,\N,Missing
2020.coling-main.532,E17-2039,0,0.0558051,"Missing"
2020.coling-main.532,C16-1333,0,0.0130792,"16 languages covering three language families: Romance (French, Italian, Spanish, Romanian, Portuguese), Germanic (Dutch, German, Swedish, Danish) and Balto-Slavic (Latvian, Lithuanian, Czech, Slovak, Slovenian, Polish and Bulgarian) into English and English original text. Abstractions. In addition to using raw word tokens, we create multiple views of the data at the morphological (PoS), lexical semantic (ST) and conceptual-semantic (SS) levels. We use the spaCy tagger (Honnibal and Johnson, 2015) with the OntoNotes 5 version (Weischedel et al., 2013) of the Penn Treebank PoS tag set. For ST (Bjerva et al., 2016; Abzianidze et al., 2017), we use the best model of Brants 1 Since these views represent diversified and complementary information of the same data, we refer to them as multi-view representations. 6057 Feature Annotated Output Vocabulary PoS DT NN VBD DT NN IN NN 37 ST DEF CON EPS DIS CON REL CON 57 Synset ministry.n.04 send.v.02 answer.n.04 inquiry.n.0.1 1667 Raw the ministry sent an answer to inquiry 6739 Table 1: Examples of the level of abstraction with the vocabulary size. (2000) which works directly on the words as input, and determines formal lexical semantics. Their implementation ach"
2020.coling-main.532,J19-2006,0,0.219248,"ion 3 introduces our experimental setup. The distance measure is described in Section 4. We present our results and analysis in Section 5, followed by conclusions in Section 6. 2 Phylogenetics and Shining-through Historical comparative linguistics determines genetic relationships between languages using concept lists of words that share a common origin, similar meaning and pronunciation across multiple languages (Swadesh, 1952; Dyen et al., 1992). By contrast, computational analysis methods aim to reconstruct language phylogeny based on measurable linguistic patterns (Rabinovich et al., 2017; Bjerva et al., 2019). Rabinovich et al. (2017) showed that source language interference is visible in translation. Specifically, they leverage interference (PoS trigrams and function words) and translation universal features (cohesive markers) to construct phylogenetic trees. Agglomerative clustering with variance minimisation (Ward Jr, 1963) is used as linkage procedure to cluster the data. The result is compared to the pruned gold tree of Serva and Petroni (2008) (henceforth referred to as SP08) used as the linguistic phylogenetic gold standard tree. Their comparison metric, which is based on the L2 norm, is ba"
2020.coling-main.532,Q17-1010,0,0.0766292,"Missing"
2020.coling-main.532,A00-1031,0,0.299709,"Missing"
2020.coling-main.532,D15-1162,0,0.0119336,"s Lj ’s, where j = 1, 2, ..., n; and to originally written text in English as Le . We select the subset of translations from 16 languages covering three language families: Romance (French, Italian, Spanish, Romanian, Portuguese), Germanic (Dutch, German, Swedish, Danish) and Balto-Slavic (Latvian, Lithuanian, Czech, Slovak, Slovenian, Polish and Bulgarian) into English and English original text. Abstractions. In addition to using raw word tokens, we create multiple views of the data at the morphological (PoS), lexical semantic (ST) and conceptual-semantic (SS) levels. We use the spaCy tagger (Honnibal and Johnson, 2015) with the OntoNotes 5 version (Weischedel et al., 2013) of the Penn Treebank PoS tag set. For ST (Bjerva et al., 2016; Abzianidze et al., 2017), we use the best model of Brants 1 Since these views represent diversified and complementary information of the same data, we refer to them as multi-view representations. 6057 Feature Annotated Output Vocabulary PoS DT NN VBD DT NN IN NN 37 ST DEF CON EPS DIS CON REL CON 57 Synset ministry.n.04 send.v.02 answer.n.04 inquiry.n.0.1 1667 Raw the ministry sent an answer to inquiry 6739 Table 1: Examples of the level of abstraction with the vocabulary size."
2020.coling-main.532,2005.mtsummit-papers.11,0,0.030569,"ently, Bjerva et al. (2019) built on this work and compared different languages based on distance metrics computed on phrase structure trees and dependency relations. They claimed that such language representations correlate better with structural family distances between languages than genetic similarities. These examples show that phylogenetic reconstruction approaches and in particular, the evaluation of generated trees remains a highly debated topic in the history of linguistics and is beyond the scope of this study. 3 Experimental Settings Data. We use the comparable portion of Europarl (Koehn, 2005) with translations from 21 European Union languages into English to minimise the impact of domain difference. The amount of tokens per language varies, ranging from 67 k tokens for Maltese to 7.2 M for German. We refer to the multiple translations into English as Lj ’s, where j = 1, 2, ..., n; and to originally written text in English as Le . We select the subset of translations from 16 languages covering three language families: Romance (French, Italian, Spanish, Romanian, Portuguese), Germanic (Dutch, German, Swedish, Danish) and Balto-Slavic (Latvian, Lithuanian, Czech, Slovak, Slovenian, P"
2020.coling-main.532,P11-1132,0,0.0360795,"target text” (Toury, 2012). Prominent evidence for shining-through as a translationese effect is found in the work of Rabinovich et al. (2017), who show that footprints of the source language remain visible in translations, to the extent that it is possible to predict the original source language from the translation. In the similar vein, a significant amount of work has gone into training classifiers to distinguish between translations and originally authored text and then investigating the contributions of individual features to the result of the classification (Baroni and Bernardini, 2005; Koppel and Ordan, 2011; Volansky et al., 2015; Avner et al., 2016). Features that contribute strongly to classification are interpreted as indicating important dimensions of translationese. In contrast, in this work, we leverage departures from isomorphism between embedding-based semantic spaces to detect translationese. We construct embedding spaces from original English (O) data and translations into English (T ) from comparable data in a number of languages. We hypothesize that the closer the source language is to English, the more isomorphic the embedding spaces are. In other words, departure from isomorphism i"
2020.coling-main.532,P18-2036,0,0.0239579,"(Bojanowski et al., 2017). Embeddings have 300 dimensions and only words with more than 5 occurrences are retained for training. We use skip-gram with negative sampling (Mikolov et al., 2013) and standard hyper-parameters. 4 Measuring Isomorphism An empirical measure of semantic proximity between two languages is often computed using the degree of isomorphism, that is, how similar the structures of two languages are in topological space (Søgaard et al., 2018). Research in cross-lingual transfer tasks shows that linguistic differences across languages often make spaces depart from isomorphism (Nakashole and Flauger, 2018; Søgaard et al., 2018; Patra et al., 2019; Vuli´c et al., 2020). While this degrades the quality of bilingual embeddings, it is a desired characteristic in our case: since our task involves processing of (multi-view) representations of monolingual text, departures from isomorphism indicate diversity in the source that generates them. To quantify isomorphism, we compute embeddings on a corpus in language L. Embeddings reflect distributional properties in the data: words in similar contexts have similar meanings (Harris, 1954) and should be close in embedding space. We then view the points repr"
2020.coling-main.532,2020.emnlp-main.187,0,0.0650884,"o the pruned gold tree of Serva and Petroni (2008) (henceforth referred to as SP08) used as the linguistic phylogenetic gold standard tree. Their comparison metric, which is based on the L2 norm, is basically the sum of squared deviations between each pair’s gold-tree distance g and computed distance P : Dist(P, g) = X (DP (li , lj ) − Dg (li , lj ))2 (1) i,j SP08 was constructed by computing the Levenshtein (edit) distance between words from an open cross-lingual list (Dyen et al., 1992) to compare linguistic divergence through time and thus partially encodes lexical similarity of languages (Oncevay et al., 2020). Rabinovich et al. (2017) also acknowledges that SP08 has been disputed and researchers have not yet agreed on a commonly accepted tree of the Indo-European languages (Ringe et al., 2002). More recently, Bjerva et al. (2019) built on this work and compared different languages based on distance metrics computed on phrase structure trees and dependency relations. They claimed that such language representations correlate better with structural family distances between languages than genetic similarities. These examples show that phylogenetic reconstruction approaches and in particular, the evalu"
2020.coling-main.532,P19-1018,0,0.0173548,"nsions and only words with more than 5 occurrences are retained for training. We use skip-gram with negative sampling (Mikolov et al., 2013) and standard hyper-parameters. 4 Measuring Isomorphism An empirical measure of semantic proximity between two languages is often computed using the degree of isomorphism, that is, how similar the structures of two languages are in topological space (Søgaard et al., 2018). Research in cross-lingual transfer tasks shows that linguistic differences across languages often make spaces depart from isomorphism (Nakashole and Flauger, 2018; Søgaard et al., 2018; Patra et al., 2019; Vuli´c et al., 2020). While this degrades the quality of bilingual embeddings, it is a desired characteristic in our case: since our task involves processing of (multi-view) representations of monolingual text, departures from isomorphism indicate diversity in the source that generates them. To quantify isomorphism, we compute embeddings on a corpus in language L. Embeddings reflect distributional properties in the data: words in similar contexts have similar meanings (Harris, 1954) and should be close in embedding space. We then view the points representing words or tags in the resulting hy"
2020.coling-main.532,P17-1049,0,0.454526,"nguage of the translation, in the same genre and style (Gellerstam, 1986; Baker and others, 1993; Baroni and Bernardini, 2005; Volansky et al., 2015). Characteristics such as simplification, over-adherence to conventions of the target language, and explicitation can occur as a communicative process itself. This is contrasted with “interference” or “shining-through” (Teich, 2003), described as “phenomena pertaining to the make-up of the source text tend to be transferred to the target text” (Toury, 2012). Prominent evidence for shining-through as a translationese effect is found in the work of Rabinovich et al. (2017), who show that footprints of the source language remain visible in translations, to the extent that it is possible to predict the original source language from the translation. In the similar vein, a significant amount of work has gone into training classifiers to distinguish between translations and originally authored text and then investigating the contributions of individual features to the result of the classification (Baroni and Bernardini, 2005; Koppel and Ordan, 2011; Volansky et al., 2015; Avner et al., 2016). Features that contribute strongly to classification are interpreted as ind"
2020.coling-main.532,P18-1072,0,0.0449613,"Missing"
2020.coling-main.532,2020.emnlp-main.257,0,0.0217513,"Missing"
2020.emnlp-main.202,P17-1042,0,0.0282177,"The number of sentences, tokens and average article length is reported in Table 1. For validation we use newstest2012 (NT12) and for testing newstest2013 (NT13) for en–es and newstest2014 (NT14) or newstest2016 (NT16) for en–{f r, de}. The SSNMT implementation4 builds on the transformer base (Vaswani et al., 2017) in OpenNMT (Klein et al., 2017). All systems are trained using a batch size of 50 sentences with maximum length of 50 tokens. Monolingual embeddings trained using word2vec (Mikolov et al., 2013)5 on the complete WP editions are projected into a common multilingual space via vecmap6 (Artetxe et al., 2017) to attain bilingual embeddings between en–{f r,de,es}. These initialise the NMT word embeddings (Cw ). 2 Dumps were downloaded on January 2019 from dumps. wikimedia.org/ 3 github.com/cristinae/WikiTailor 4 github.com/ruitedk6/comparableNMT 5 github.com/tmikolov/word2vec 6 github.com/artetxem/vecmap 2562 WP, L1 L1–L2 en–f r en–de en–es # Sent. WP, L2 # Tokens Sent./Article 117 / 42 2693/1205 117 / 37 2693/987 117 / 35 2693/937 28 29 32 EP, L1 # Sent. # Tokens Sent./Article 38/25 644/710 51/30 1081/742 27/20 691/572 EP, L2 # Sent. # Tokens # Sent. # Tokens 1+6 25+80 1+9 25+180 1+7 24+84 1+3 27+"
2020.emnlp-main.202,D18-1399,0,0.0205255,"further work in Section 5. 2 Related Work Machine translation has experienced major improvements in translation quality due to the introduction of neural architectures (Cho et al., 2014; 2560 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2560–2571, c November 16–20, 2020. 2020 Association for Computational Linguistics Bahdanau et al., 2015; Vaswani et al., 2017). However, these rely on the availability of large amounts of parallel data. To overcome the need for labelled data, unsupervised neural machine translation (USNMT) (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018) focuses on the exploitation of very large amounts of monolingual sentences by combining denoising autoencoders with back-translation and multilingual encoders. Further combining these with phrase tables from statistical machine translation leads to impressive results (Lample et al., 2018b; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019). USNMT can be combined with pre-trained language models (LMs) (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). Brown et al. (2020) train a very large LM on billions of monolingual sentences which allows them"
2020.emnlp-main.202,P19-1019,0,0.0305005,"au et al., 2015; Vaswani et al., 2017). However, these rely on the availability of large amounts of parallel data. To overcome the need for labelled data, unsupervised neural machine translation (USNMT) (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018) focuses on the exploitation of very large amounts of monolingual sentences by combining denoising autoencoders with back-translation and multilingual encoders. Further combining these with phrase tables from statistical machine translation leads to impressive results (Lample et al., 2018b; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019). USNMT can be combined with pre-trained language models (LMs) (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). Brown et al. (2020) train a very large LM on billions of monolingual sentences which allows them to perform NMT in a few-shot setting. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) is an alternative approach focusing on comparable, rather than parallel data. The internal representations of an emergent NMT system are used to identify useful sentence pairs in comparable documents. Selection depends on the current state of the model, resembling a type of self-paced l"
2020.emnlp-main.202,J82-2005,0,0.540679,"Missing"
2020.emnlp-main.202,P19-1309,0,0.170458,"hang et al. (2018), showing that they, too, can speed-up training without a loss in translation performance. SSNMT jointly learns to find and extract similar sentence pairs from comparable data and to translate. The extractions can be compared to those obtained by parallel data mining systems where strictly parallel sentences are expected. Beating early feature-based approaches, sentence representations obtained from NMT systems or tailored architectures are achieving a new state-of-the-art in parallel sentence extraction and filtering (Espa˜naBonet et al., 2017; Gr´egoire and Langlais, 2018; Artetxe and Schwenk, 2019; Hangya and Fraser, 2019; Chaudhary et al., 2019). Using a highly multilingual sentence encoder, Schwenk et al. (2019) scored Wikipedia sentence pairs across various language combinations (WikiMatrix). Due to its multi2561 lingual aspect and the close similarity with the raw Wikipedia data we use, we also use scored WikiMatrix data for one of the comparisons (Section 3.2). 3 Self-Supervised Neural Machine Translation (SSNMT) SSNMT is a joint data selection and training framework for machine translation, introduced in Ruiter et al. (2019). SSNMT enables learning NMT from comparable rather than"
2020.emnlp-main.202,W15-3402,1,0.896662,"Missing"
2020.emnlp-main.202,W19-5435,0,0.14859,"Missing"
2020.emnlp-main.202,D14-1179,0,0.0209941,"Missing"
2020.emnlp-main.202,D18-1045,0,0.0273502,"report the sizes for both the monolingual/comparable editions; for Europarl (EP), true+false splits (see Section 3.2). SSNMT SotA L1-to-L2 L2-to-L1 L1-to-L2 L2-to-L1 L1–L2 BLEU TER METEOR BLEU TER METEOR BLEU BLEU en–f r en–de en–es 29.5±.6 15.2±.5 28.6±.7 51.9±.6 68.5±.7 52.6±.7 46.4±.6 30.3±.5 47.8±.7 27.7±.6 21.2±.6 28.4±.7 53.4±.7 62.8±.9 54.1±.7 30.3±.4 25.4±.4 30.5±.4 45.6/25.1/37.5 37.9/17.2/28.3 –/–/– –/24.2/34.9 –/21.0/35.2 –/–/– Table 2: Automatic evaluation of SSNMT on NT14 (f r) NT16 (de) NT13 (es). Most right columns show the comparison with three SotA systems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translatio"
2020.emnlp-main.202,C18-1122,0,0.0268214,"Missing"
2020.emnlp-main.202,P19-1118,0,0.0186012,"that they, too, can speed-up training without a loss in translation performance. SSNMT jointly learns to find and extract similar sentence pairs from comparable data and to translate. The extractions can be compared to those obtained by parallel data mining systems where strictly parallel sentences are expected. Beating early feature-based approaches, sentence representations obtained from NMT systems or tailored architectures are achieving a new state-of-the-art in parallel sentence extraction and filtering (Espa˜naBonet et al., 2017; Gr´egoire and Langlais, 2018; Artetxe and Schwenk, 2019; Hangya and Fraser, 2019; Chaudhary et al., 2019). Using a highly multilingual sentence encoder, Schwenk et al. (2019) scored Wikipedia sentence pairs across various language combinations (WikiMatrix). Due to its multi2561 lingual aspect and the close similarity with the raw Wikipedia data we use, we also use scored WikiMatrix data for one of the comparisons (Section 3.2). 3 Self-Supervised Neural Machine Translation (SSNMT) SSNMT is a joint data selection and training framework for machine translation, introduced in Ruiter et al. (2019). SSNMT enables learning NMT from comparable rather than parallel data, where com"
2020.emnlp-main.202,W11-2123,0,0.01649,"ser in the cross-lingual space, and the system is able to exploit this by extracting increasingly similar and accurate pairs. 4.2 Order & Complexity Establishing the complexity of a sentence is a complex task by itself. Complexity can be estimated by the loss of an instance with respect to the gold or target. In our self-supervised approach, there is no target for the sentence extraction task, so we try to infer complexity by other means. First, we study the behaviour of the average perplexity throughout training. Perplexities of the extracted data are estimated using a LM trained with KenLM (Heafield, 2011) on the monolingual WPs for the four languages in our study. We observe the same behaviour in the four cases illustrated by the English curves plotted in Figure 2 (top). Perplexity drops heavily within the first 10 k steps for all languages and models. This indicates that the data extracted in the first epoch includes more outliers, and the distribution of extracted sentences moves closer to the average observed in the monolingual WPs as training advances. The larger number of outliers at the beginning of training can be attributed to the larger number of homographs (bottom Figure 3) and short"
2020.emnlp-main.202,P17-4012,0,0.0320479,"ruecased using standard Moses (Koehn et al., 2007) scripts. For each language pair, a shared byte-pair encoding (BPE) (Sennrich et al., 2016) of 100 k merge operations is applied. Following Johnson et al. (2017), a language tag is added to the beginning of each sequence. The number of sentences, tokens and average article length is reported in Table 1. For validation we use newstest2012 (NT12) and for testing newstest2013 (NT13) for en–es and newstest2014 (NT14) or newstest2016 (NT16) for en–{f r, de}. The SSNMT implementation4 builds on the transformer base (Vaswani et al., 2017) in OpenNMT (Klein et al., 2017). All systems are trained using a batch size of 50 sentences with maximum length of 50 tokens. Monolingual embeddings trained using word2vec (Mikolov et al., 2013)5 on the complete WP editions are projected into a common multilingual space via vecmap6 (Artetxe et al., 2017) to attain bilingual embeddings between en–{f r,de,es}. These initialise the NMT word embeddings (Cw ). 2 Dumps were downloaded on January 2019 from dumps. wikimedia.org/ 3 github.com/cristinae/WikiTailor 4 github.com/ruitedk6/comparableNMT 5 github.com/tmikolov/word2vec 6 github.com/artetxem/vecmap 2562 WP, L1 L1–L2 en–f r"
2020.emnlp-main.202,kocmi-bojar-2017-curriculum,0,0.0658892,"n data, which comes with a high computational cost. To alleviate this cost, Kumar et al. (2019) use reinforcement learning on the pre-scored noisy corpus to jointly learn the denoising curriculum with NMT. In Section 3.2 we show that our model exploits its self-supervised nature to perform denoising by selecting parallel pairs with increasing accuracy, without the need of additional noise metrics. Difficulty-based curricula for NMT that take into account sentence length and vocabulary frequency have been shown to improve translation quality when samples are presented in increasing complexity (Kocmi and Bojar, 2017). Platanios et al. (2019) link the introduction of difficult samples with the NMT models’ competence. Other difficulty-orderings have been explored extensively in Zhang et al. (2018), showing that they, too, can speed-up training without a loss in translation performance. SSNMT jointly learns to find and extract similar sentence pairs from comparable data and to translate. The extractions can be compared to those obtained by parallel data mining systems where strictly parallel sentences are expected. Beating early feature-based approaches, sentence representations obtained from NMT systems or"
2020.emnlp-main.202,W04-3250,0,0.504822,"Missing"
2020.emnlp-main.202,2005.mtsummit-papers.11,0,0.0596494,"15.2±.5 28.6±.7 51.9±.6 68.5±.7 52.6±.7 46.4±.6 30.3±.5 47.8±.7 27.7±.6 21.2±.6 28.4±.7 53.4±.7 62.8±.9 54.1±.7 30.3±.4 25.4±.4 30.5±.4 45.6/25.1/37.5 37.9/17.2/28.3 –/–/– –/24.2/34.9 –/21.0/35.2 –/–/– Table 2: Automatic evaluation of SSNMT on NT14 (f r) NT16 (de) NT13 (es). Most right columns show the comparison with three SotA systems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translation quality. For calculating BLEU, we use multi-bleu.perl, while TER and METEOR are calculated using the scoring package7 which also provides confidence scores. SSNMT translation performance training on the en–{f r, de, es} comparable"
2020.emnlp-main.202,P07-2045,0,0.00683322,"ium recall approach in Ruiter et al. (2019). Whenever enough pairs have been collected to create a batch, the system trains on it, updating its weights, improving both its translation and extraction ability to fill the next batch. 3.1 Translation Quality Experimental Setup We use Wikipedia (WP) as a comparable corpus and download the English, French, German and Spanish dumps,2 pre-process them and extract comparable articles per language pair using WikiTailor3 (Barr´on-Cede˜no et al., 2015; Espa˜na-Bonet et al., 2020). All articles are normalized, tokenized and truecased using standard Moses (Koehn et al., 2007) scripts. For each language pair, a shared byte-pair encoding (BPE) (Sennrich et al., 2016) of 100 k merge operations is applied. Following Johnson et al. (2017), a language tag is added to the beginning of each sequence. The number of sentences, tokens and average article length is reported in Table 1. For validation we use newstest2012 (NT12) and for testing newstest2013 (NT13) for en–es and newstest2014 (NT14) or newstest2016 (NT16) for en–{f r, de}. The SSNMT implementation4 builds on the transformer base (Vaswani et al., 2017) in OpenNMT (Klein et al., 2017). All systems are trained using"
2020.emnlp-main.202,N19-1208,0,0.115059,"Missing"
2020.emnlp-main.202,W07-0734,0,0.00925881,"ystems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translation quality. For calculating BLEU, we use multi-bleu.perl, while TER and METEOR are calculated using the scoring package7 which also provides confidence scores. SSNMT translation performance training on the en–{f r, de, es} comparable Wikipedia data is reported in Table 2 together with a comparison to the current stateof-the-art (SotA) in supervised and (pre-trained) USNMT. SSNMT is on par with the current SotA in USNMT, outperforming it by 3–4 BLEU points in en–f r with lower performance on en–de (∼3 BLEU). Note that unsupervised systems such as Lample et al. (2018b) use"
2020.emnlp-main.202,2020.tacl-1.47,0,0.0248273,"need for labelled data, unsupervised neural machine translation (USNMT) (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018) focuses on the exploitation of very large amounts of monolingual sentences by combining denoising autoencoders with back-translation and multilingual encoders. Further combining these with phrase tables from statistical machine translation leads to impressive results (Lample et al., 2018b; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019). USNMT can be combined with pre-trained language models (LMs) (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). Brown et al. (2020) train a very large LM on billions of monolingual sentences which allows them to perform NMT in a few-shot setting. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) is an alternative approach focusing on comparable, rather than parallel data. The internal representations of an emergent NMT system are used to identify useful sentence pairs in comparable documents. Selection depends on the current state of the model, resembling a type of self-paced learning (Kumar et al., 2010). Data selection in SSNMT is directly related to curriculum learning, the idea of presenting train"
2020.emnlp-main.202,P02-1040,0,0.108728,"(es). Most right columns show the comparison with three SotA systems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translation quality. For calculating BLEU, we use multi-bleu.perl, while TER and METEOR are calculated using the scoring package7 which also provides confidence scores. SSNMT translation performance training on the en–{f r, de, es} comparable Wikipedia data is reported in Table 2 together with a comparison to the current stateof-the-art (SotA) in supervised and (pre-trained) USNMT. SSNMT is on par with the current SotA in USNMT, outperforming it by 3–4 BLEU points in en–f r with lower performance on en–de (∼3 BLEU). N"
2020.emnlp-main.202,N19-1119,0,0.0929573,"Missing"
2020.emnlp-main.202,P19-1178,1,0.890518,"Missing"
2020.emnlp-main.202,P16-1162,0,0.0614651,"o create a batch, the system trains on it, updating its weights, improving both its translation and extraction ability to fill the next batch. 3.1 Translation Quality Experimental Setup We use Wikipedia (WP) as a comparable corpus and download the English, French, German and Spanish dumps,2 pre-process them and extract comparable articles per language pair using WikiTailor3 (Barr´on-Cede˜no et al., 2015; Espa˜na-Bonet et al., 2020). All articles are normalized, tokenized and truecased using standard Moses (Koehn et al., 2007) scripts. For each language pair, a shared byte-pair encoding (BPE) (Sennrich et al., 2016) of 100 k merge operations is applied. Following Johnson et al. (2017), a language tag is added to the beginning of each sequence. The number of sentences, tokens and average article length is reported in Table 1. For validation we use newstest2012 (NT12) and for testing newstest2013 (NT13) for en–es and newstest2014 (NT14) or newstest2016 (NT16) for en–{f r, de}. The SSNMT implementation4 builds on the transformer base (Vaswani et al., 2017) in OpenNMT (Klein et al., 2017). All systems are trained using a batch size of 50 sentences with maximum length of 50 tokens. Monolingual embeddings trai"
2020.emnlp-main.202,2006.amta-papers.25,0,0.0510763,"the comparison with three SotA systems for supervised NMT (Edunov et al., 2018) / USNMT (Lample et al., 2018b) / pre-trained+LM USNMT (Song et al., 2019). As a control experiment and purely in order to analyse the quality of the SSNMT data selection auxiliary task, we use the Europarl (EP) corpus (Koehn, 2005). The corpus is pre-processed in the same way as WP, and we create a synthetic comparable corpus from it as explained in Section 3.2. For these experiments, we use the same data for validation and testing as mentioned above. Automatic Evaluation We use BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Lavie and Agarwal, 2007) to evaluate translation quality. For calculating BLEU, we use multi-bleu.perl, while TER and METEOR are calculated using the scoring package7 which also provides confidence scores. SSNMT translation performance training on the en–{f r, de, es} comparable Wikipedia data is reported in Table 2 together with a comparison to the current stateof-the-art (SotA) in supervised and (pre-trained) USNMT. SSNMT is on par with the current SotA in USNMT, outperforming it by 3–4 BLEU points in en–f r with lower performance on en–de (∼3 BLEU). Note that unsupervised syste"
2020.emnlp-main.202,P18-1005,0,0.0384186,"5. 2 Related Work Machine translation has experienced major improvements in translation quality due to the introduction of neural architectures (Cho et al., 2014; 2560 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2560–2571, c November 16–20, 2020. 2020 Association for Computational Linguistics Bahdanau et al., 2015; Vaswani et al., 2017). However, these rely on the availability of large amounts of parallel data. To overcome the need for labelled data, unsupervised neural machine translation (USNMT) (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018) focuses on the exploitation of very large amounts of monolingual sentences by combining denoising autoencoders with back-translation and multilingual encoders. Further combining these with phrase tables from statistical machine translation leads to impressive results (Lample et al., 2018b; Artetxe et al., 2018a; Ren et al., 2019; Artetxe et al., 2019). USNMT can be combined with pre-trained language models (LMs) (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020). Brown et al. (2020) train a very large LM on billions of monolingual sentences which allows them to perform NMT in a f"
2020.emnlp-main.202,N19-1189,0,0.199873,"Missing"
2020.emnlp-main.202,P19-1123,0,0.0327964,"Missing"
2020.emnlp-main.202,W18-6314,0,0.159093,"ory behavior of benefiting from both increasingly difficult (domain-distant) and easy (domain-relevant) samples has been analyzed by Weinshall et al. (2018), showing that the initial phases of training benefit from easy samples with respect to a hypothetical competent model (target hypothesis), while also being boosted (Freund and Schapire, 1996) by samples that are difficult with respect to the current state of the model (Hacohen and Weinshall, 2019). In Wang et al. (2019), both domain-relevance and denoising are combined into a single curriculum. The denoising curriculum for NMT proposed by Wang et al. (2018) is related to our approach in that they also use online data selection to build the curriculum based on the current state of the model. However, the noise scores for the dataset at each training step depend on fine-tuning the model on a small selection of clean data, which comes with a high computational cost. To alleviate this cost, Kumar et al. (2019) use reinforcement learning on the pre-scored noisy corpus to jointly learn the denoising curriculum with NMT. In Section 3.2 we show that our model exploits its self-supervised nature to perform denoising by selecting parallel pairs with incre"
2020.emnlp-main.202,D17-1147,0,0.0475825,"Missing"
2020.iwslt-1.34,P04-3031,0,0.178791,"om Open Subtitles to 10 million. With this, both Ct and Cs contain around 200 million tokens per language. Finally, a byte-pair-encoding (BPE) (Sennrich et al., 2016) with 32 k merge operations trained jointly on en– de data is applied before training neural systems. After shuffling, 1,000 sentences are set aside for tuning/validation. 4.2 Machine Translation Engines We train three different architectures, one statistical and two neural, on the corpora above. For each corpus, we train the PoS models on 3000 random sentences and evaluate on the remaining data. We tokenized our data using NLTK (Bird and Loper, 2004) and performed universal PoS tagging via spaCy. We train our language models using a one-layer LSTM with 50 units (Chollet et al., 2015). Due to the small dimensions of the Phrase-Based Statistical Machine Translation (SMT). SMT systems are trained using standard freely available software. We estimate a 5-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) 283 3 https://pypi.org/project/langdetect/ Europarl-UdS German Translation Written German Written English EPIC-UdS lines tokens 137,813 427,779 372,"
2020.iwslt-1.34,W19-5204,0,0.0164475,"al-to-translationese when one wants to translate originals (Lembersky et al., 2013). In this case, more human translationese features are expected, as systems tend to learn to reproduce human features. The effects of translationese in machine translation test sets is also studied in Zhang and Toral (2019a). In fact, texts displaying translationese features seems to be much easier to translate than originals, and recent studies advise to use only translations from original texts in order to (automatically) evaluate translation quality (Graham et al., 2019; Zhang and Toral, 2019b). By contrast, Freitag et al. (2019) show a slight preference of human evaluators to outputs closer to originals; in this case the translation is done from translationese input, because, as noted by Riley et al. (2019), the best of the two worlds is not possible: one cannot create original-to-original corpora to train bias-free systems. Aranberri (2020) analyzed translationese characteristics on translations obtained by five state-of-the-art Spanish-to-Basque translation systems, neural and rule-based. The author quantifies translationese by measuring lexical variety, lexical density, length ratio and perplexity with part of spe"
2020.iwslt-1.34,N16-1111,0,0.0608831,"Missing"
2020.iwslt-1.34,P07-2045,0,0.0221128,"ora from OPUS (Tiedemann, 2012), one of them text-oriented (Ct ) and the other speech-oriented (Cs ). The distribution of their sub-corpora is shown in Table 2. Note that we do not include Europarl data here so that there is no overlap between MT training and our analysis data. Note also that our speech data (TED talks and subtitles) is still made up from translations and not simultaneous interpreting. This is important since it prevents MT systems from simply mimicking interpreting’s pronounced translationese. All datasets are normalised, tokenized and truecased using standard Moses scripts (Koehn et al., 2007) and cleaned for low quality pairs. Duplicates are removed and sentences shorter than 4 tokens or with a length ratio greater than 9 are discarded. We also eliminate sentence pairs which are not identified as English/German by langdetect3 and apply basic cleaning procedures. With this, we reduce the corpus size by more than half of the sentences. In order to build balanced corpora we limit the number of sentences we used from ParaCrawl to 5 million and from Open Subtitles to 10 million. With this, both Ct and Cs contain around 200 million tokens per language. Finally, a byte-pair-encoding (BPE"
2020.iwslt-1.34,P11-1132,0,0.279913,"in two main categories: (i) source’s interference, or shining-though as put forward by Teich (2003). For example, a translation replicating a syntactic pattern which is typical of the source language, and rare in the target language, displays a typical form of shiningthrough; (ii) aherence or over-adherence to the target language’s standards, that is normalisation. For example, translating a sentence displaying marked order in the source with a sentence displaying standard order in the target it a typical example of overnormalization. Nonetheless, translationese’s main causes remain unclear (Koppel and Ordan, 2011; Schmied and Sch¨affler, 1996). Translationese displays different patterns depending on the translation’s mode and register: a typical example is simultaneous interpreting, which shows translationese patterns distinct from those observed in written translations (Bernardini et al., 2016). We can interpret differences as either (i) an effect of the limitations of the human language apparatus that constrain translations, (ii) an inevitable effect of the structural and semantic differences between languages; or (iii) a combination of the two. To test these hypotheses, it is common to compare huma"
2020.iwslt-1.34,W19-8706,0,0.483084,"Missing"
2020.iwslt-1.34,J13-4007,0,0.0133526,"). Due to the particularly harsh constraints imposed on human interpreters, such particularities can be useful to better understand the nature and causes of translationese in general (Shlesinger and Ordan, 2012). The features of machine translated text depend on the nature of both training and test data; and possibly also on the approach to machine translation, i.e. statistical, neural or rule-based. The best translation quality is achieved when the training parallel corpus is in the same direction as the test translation, i.e. original-to-translationese when one wants to translate originals (Lembersky et al., 2013). In this case, more human translationese features are expected, as systems tend to learn to reproduce human features. The effects of translationese in machine translation test sets is also studied in Zhang and Toral (2019a). In fact, texts displaying translationese features seems to be much easier to translate than originals, and recent studies advise to use only translations from original texts in order to (automatically) evaluate translation quality (Graham et al., 2019; Zhang and Toral, 2019b). By contrast, Freitag et al. (2019) show a slight preference of human evaluators to outputs close"
2020.iwslt-1.34,J03-1002,0,0.0114141,"ra above. For each corpus, we train the PoS models on 3000 random sentences and evaluate on the remaining data. We tokenized our data using NLTK (Bird and Loper, 2004) and performed universal PoS tagging via spaCy. We train our language models using a one-layer LSTM with 50 units (Chollet et al., 2015). Due to the small dimensions of the Phrase-Based Statistical Machine Translation (SMT). SMT systems are trained using standard freely available software. We estimate a 5-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) 283 3 https://pypi.org/project/langdetect/ Europarl-UdS German Translation Written German Written English EPIC-UdS lines tokens 137,813 427,779 372,547 3,100,647 7,869,289 8,693,135 German Interpreting Spoken German Spoken English lines tokens 4,080 3,408 3,623 58,371 57,227 68,712 Table 1: Corpus collections used to train our language models: Europarl-UdS (written) and EPIC-UdS (spoken). German translation and interpreting are both from English. lines de tokens en tokens Ct Cs CommonCrawl MultiUN NewsCommentary academia career limiting moves Rapid ParaCrawl-5M TED OpenSubtitles-10M 2,212,292"
2020.iwslt-1.34,P02-1040,0,0.1096,"4,608 8,316,081 24,563,476 96,262,081 3,833,653 85,773,795 54,140,396 4,924,596 46,222,416 148,360,866 103,287,049 20,141,669 93,287,837 3 3 3 3 3 7 7 3 3 3 3 7 3 3 Total clean Speech Total clean Text 13,379,441 9,121,710 187,551,444 198,340,602 197,175,542 207,434,038 7 3 3 7 Table 2: Text-oriented (Ct ) and speech-oriented (Cs ) corpora used for training the MT systems. and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimization of the feature weights of the model is done with Minimum Error Rate Training (MERT) (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. As features, we use the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and lexicalized reordering. The neural systems are trained using the Marian toolkit (Junczys-Dowmunt et al., 2018) in a bidirectional setting {en,de}↔{de,en}: layer encoder–decoder with 8-head self-attention, a 2048-dim hidden feed-forward, and 512-dim word vectors. Optimization algorithm, dropout, smoothings and learning rate (with warmup till update 16,000) are the same as for RNN. RNN-Based Neural Machine Translation (RNN)."
2020.iwslt-1.34,tiedemann-2012-parallel,0,0.0210451,"n Original spoken English (transcript) Original spoken German (transcript) English to German translations English to German interpreting (transcript) vocabulary (17 PoS), 5 iterations over 3000 sentences suffice to converge. In our experiments, we measure the average perplexity of each model on unseen human data from of each category, and on the translations produced by three MT architectures in two different settings (see Section 4.2). MT Training Data. In order to adapt our machine translation engines to the previous modalities as much as possible, we gather two different corpora from OPUS (Tiedemann, 2012), one of them text-oriented (Ct ) and the other speech-oriented (Cs ). The distribution of their sub-corpora is shown in Table 2. Note that we do not include Europarl data here so that there is no overlap between MT training and our analysis data. Note also that our speech data (TED talks and subtitles) is still made up from translations and not simultaneous interpreting. This is important since it prevents MT systems from simply mimicking interpreting’s pronounced translationese. All datasets are normalised, tokenized and truecased using standard Moses scripts (Koehn et al., 2007) and cleaned"
2020.iwslt-1.34,R11-1091,0,0.222431,"k Translationese seems to affect the semantic as well as the structural level of text, but much of its effects can be seen in syntax and grammar (Santos, 1995; Puurtinen, 2003). An interesting aspect of translationese is that, while it is somewhat difficult to detect for the human eye (Tirkkonen-Condit, 2002), it can be machine learned with high accuracy (Baroni and Bernardini, 2006; Rubino et al., 2016). Many ways to automatically detect translationese have been devised, both with respect to textual translations and simultaneous interpreting (Baroni and Bernardini, 2006; Ilisei et al., 2010; Popescu, 2011). Simultaneous interpreting has shown specific forms of translationese distinct from those of textual translation (He et al., 2016; Shlesinger, 1995), with a tendency of going in the direction of simplification and explicitation (Gumul, 2006). Due to the particularly harsh constraints imposed on human interpreters, such particularities can be useful to better understand the nature and causes of translationese in general (Shlesinger and Ordan, 2012). The features of machine translated text depend on the nature of both training and test data; and possibly also on the approach to machine translat"
2020.iwslt-1.34,W19-6627,0,0.218571,"ne translated text depend on the nature of both training and test data; and possibly also on the approach to machine translation, i.e. statistical, neural or rule-based. The best translation quality is achieved when the training parallel corpus is in the same direction as the test translation, i.e. original-to-translationese when one wants to translate originals (Lembersky et al., 2013). In this case, more human translationese features are expected, as systems tend to learn to reproduce human features. The effects of translationese in machine translation test sets is also studied in Zhang and Toral (2019a). In fact, texts displaying translationese features seems to be much easier to translate than originals, and recent studies advise to use only translations from original texts in order to (automatically) evaluate translation quality (Graham et al., 2019; Zhang and Toral, 2019b). By contrast, Freitag et al. (2019) show a slight preference of human evaluators to outputs closer to originals; in this case the translation is done from translationese input, because, as noted by Riley et al. (2019), the best of the two worlds is not possible: one cannot create original-to-original corpora to train"
2020.iwslt-1.34,N16-1110,1,0.685734,"Missing"
2020.iwslt-1.34,W19-6622,0,0.143417,"ined by five state-of-the-art Spanish-to-Basque translation systems, neural and rule-based. The author quantifies translationese by measuring lexical variety, lexical density, length ratio and perplexity with part of speech (PoS) language models and finds no clear correlation with automatic translation quality across different test sets. The results are not conclusive but translation quality seems not to correlate with translationese. Similar results are obtained by Kunilovskaya and Lapshinova-Koltunski (2019) when using 45 morpho-syntactic features to analyze English-to-Russian translations. Vanmassenhove et al. (2019) noted that statistical systems reproduce human lexical diversity better than 281 neural systems, for English–Spanish and English– French even if transformer models, i.e. neural, are those with the highest BLEU score. However, their transformer model did not use subword segmentation putting a limit on the plausible lexical diversity it can achieve. In our study, we focus on English (en) and German (de) texts and compare the presence of translationese in human translations and interpreting, and in machine translations obtained by in-house MT engines. Differently to Aranberri (2020), we develop"
2020.iwslt-1.34,W19-5208,0,0.119171,"s of machine translated text depend on the nature of both training and test data; and possibly also on the approach to machine translation, i.e. statistical, neural or rule-based. The best translation quality is achieved when the training parallel corpus is in the same direction as the test translation, i.e. original-to-translationese when one wants to translate originals (Lembersky et al., 2013). In this case, more human translationese features are expected, as systems tend to learn to reproduce human features. The effects of translationese in machine translation test sets is also studied in Zhang and Toral (2019a). In fact, texts displaying translationese features seems to be much easier to translate than originals, and recent studies advise to use only translations from original texts in order to (automatically) evaluate translation quality (Graham et al., 2019; Zhang and Toral, 2019b). By contrast, Freitag et al. (2019) show a slight preference of human evaluators to outputs closer to originals; in this case the translation is done from translationese input, because, as noted by Riley et al. (2019), the best of the two worlds is not possible: one cannot create original-to-original corpora to train"
2020.iwslt-1.34,P16-1162,0,0.0199201,"d cleaned for low quality pairs. Duplicates are removed and sentences shorter than 4 tokens or with a length ratio greater than 9 are discarded. We also eliminate sentence pairs which are not identified as English/German by langdetect3 and apply basic cleaning procedures. With this, we reduce the corpus size by more than half of the sentences. In order to build balanced corpora we limit the number of sentences we used from ParaCrawl to 5 million and from Open Subtitles to 10 million. With this, both Ct and Cs contain around 200 million tokens per language. Finally, a byte-pair-encoding (BPE) (Sennrich et al., 2016) with 32 k merge operations trained jointly on en– de data is applied before training neural systems. After shuffling, 1,000 sentences are set aside for tuning/validation. 4.2 Machine Translation Engines We train three different architectures, one statistical and two neural, on the corpora above. For each corpus, we train the PoS models on 3000 random sentences and evaluate on the remaining data. We tokenized our data using NLTK (Bird and Loper, 2004) and performed universal PoS tagging via spaCy. We train our language models using a one-layer LSTM with 50 units (Chollet et al., 2015). Due to"
2020.lrec-1.335,P19-1310,0,0.166637,"Missing"
2020.lrec-1.335,Q19-1038,0,0.103659,"ennington et al., 2014; Bojanowski et al., 2017) have been proven to be very useful for training downstream natural language processing (NLP) tasks. Moreover, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to further improve the performance of NLP tasks such as named entity recognition, question answering, and text classification when used as word features because they are able to resolve ambiguities of word representations when they appear in different contexts. Different deep learning architectures such as multilingual BERT (Devlin et al., 2019), LASER (Artetxe and Schwenk, 2019) and XLM (Lample and Conneau, 2019) have proved successful in the multilingual setting. All these architectures learn the semantic representations from unannotated text, making them cheap given the availability of texts in online multilingual resources such as Wikipedia. However, the evaluation of such resources is usually done for the highresourced languages, where one has a smorgasbord of tasks and test sets to evaluate on. This is the best-case scenario, i.e. languages with tonnes of data for training that generate high-quality models. For low-resourced languages, the evaluation is more dif"
2020.lrec-1.335,Q17-1010,0,0.762213,"or these languages. For the evaluation, we manually translate the wordsim-353 word pairs dataset from English into Yor`ub´a and Twi. We extend the analysis to contextual word embeddings and evaluate multilingual BERT on a named entity recognition task. For this, we annotate with named entities the Global Voices corpus for Yor`ub´a. As output of the work, we provide corpora, embeddings and the test suits for both languages. Keywords: Multilingual embeddings, Low-resource language, Yor`ub´a, and Twi 1. Introduction In recent years, word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) have been proven to be very useful for training downstream natural language processing (NLP) tasks. Moreover, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to further improve the performance of NLP tasks such as named entity recognition, question answering, and text classification when used as word features because they are able to resolve ambiguities of word representations when they appear in different contexts. Different deep learning architectures such as multilingual BERT (Devlin et al., 2019), LASER (Artetxe and Schwenk, 2019) and XLM (Lample and C"
2020.lrec-1.335,S17-2002,0,0.0228162,"l tags to transfer the knowledge from related highresource language into the low-resource one. Jiang et al. (2018) apply Positive-Unlabeled Learning for word embedding calculations, assuming that unobserved pairs of words in a corpus also convey information, and this is specially important for small corpora. In order to assess the quality of word embeddings, word similarity and relatedness tasks are usually used. wordsim353 (Finkelstein et al., 2001) is a collection of 353 pairs annotated with semantic similarity scores in a scale from 0 to 10. Even with the problems detected in this dataset (Camacho-Collados et al., 2017), it is widely used by the community. The test set was originally created for English, but the need for comparison with other languages has motivated several translations/adaptations. In Hassan and Mihalcea (2009) the test was translated manually into Spanish, Romanian and Arabic and the scores were adapted to reflect similarities in the new language. The reported correlation between the English scores and the Spanish ones is 0.86. Later, Joubarne and Inkpen (2011) show indications that the measures of similarity highly correlate across languages. Leviant and Reichart (2015) translated also wo"
2020.lrec-1.335,D18-1366,0,0.0176283,"ta, so that the resources are nowadays at hand for more than 100 languages. Some examples include fastText word embeddings (Bojanowski et al., 2017; Grave et al., 2018), MUSE embeddings (Lample et al., 2018), BERT multilingual embeddings (Devlin et al., 2019) and LASER sentence embeddings (Artetxe and Schwenk, 2019). In all cases, embeddings are trained either simultaneously for multiple languages, joining highand low-resource data, or following the same methodology. On the other hand, different approaches try to specifically design architectures to learn embeddings in a lowresourced setting. Chaudhary et al. (2018) follow a transfer learning approach that uses phonemes, lemmas and morphological tags to transfer the knowledge from related highresource language into the low-resource one. Jiang et al. (2018) apply Positive-Unlabeled Learning for word embedding calculations, assuming that unobserved pairs of words in a corpus also convey information, and this is specially important for small corpora. In order to assess the quality of word embeddings, word similarity and relatedness tasks are usually used. wordsim353 (Finkelstein et al., 2001) is a collection of 353 pairs annotated with semantic similarity s"
2020.lrec-1.335,N19-1423,0,0.636431,"gs and evaluate multilingual BERT on a named entity recognition task. For this, we annotate with named entities the Global Voices corpus for Yor`ub´a. As output of the work, we provide corpora, embeddings and the test suits for both languages. Keywords: Multilingual embeddings, Low-resource language, Yor`ub´a, and Twi 1. Introduction In recent years, word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) have been proven to be very useful for training downstream natural language processing (NLP) tasks. Moreover, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to further improve the performance of NLP tasks such as named entity recognition, question answering, and text classification when used as word features because they are able to resolve ambiguities of word representations when they appear in different contexts. Different deep learning architectures such as multilingual BERT (Devlin et al., 2019), LASER (Artetxe and Schwenk, 2019) and XLM (Lample and Conneau, 2019) have proved successful in the multilingual setting. All these architectures learn the semantic representations from unannotated text, making them cheap given the ava"
2020.lrec-1.335,L18-1550,0,0.0905337,"in the internet for multiple languages is facilitating the massive and automatic creation of multilingual resources. The resource par excellence is Wikipedia2 , an online encyclopedia currently available in 307 languages3 . Other initiatives such as Common Crawl4 or the Jehovahs Witnesses site5 are also repositories for multilingual data, usually assumed to be noisier than Wikipedia. Word and contextual embeddings have been pre-trained on these data, so that the resources are nowadays at hand for more than 100 languages. Some examples include fastText word embeddings (Bojanowski et al., 2017; Grave et al., 2018), MUSE embeddings (Lample et al., 2018), BERT multilingual embeddings (Devlin et al., 2019) and LASER sentence embeddings (Artetxe and Schwenk, 2019). In all cases, embeddings are trained either simultaneously for multiple languages, joining highand low-resource data, or following the same methodology. On the other hand, different approaches try to specifically design architectures to learn embeddings in a lowresourced setting. Chaudhary et al. (2018) follow a transfer learning approach that uses phonemes, lemmas and morphological tags to transfer the knowledge from related highresource langua"
2020.lrec-1.335,D09-1124,0,0.0510623,"ords in a corpus also convey information, and this is specially important for small corpora. In order to assess the quality of word embeddings, word similarity and relatedness tasks are usually used. wordsim353 (Finkelstein et al., 2001) is a collection of 353 pairs annotated with semantic similarity scores in a scale from 0 to 10. Even with the problems detected in this dataset (Camacho-Collados et al., 2017), it is widely used by the community. The test set was originally created for English, but the need for comparison with other languages has motivated several translations/adaptations. In Hassan and Mihalcea (2009) the test was translated manually into Spanish, Romanian and Arabic and the scores were adapted to reflect similarities in the new language. The reported correlation between the English scores and the Spanish ones is 0.86. Later, Joubarne and Inkpen (2011) show indications that the measures of similarity highly correlate across languages. Leviant and Reichart (2015) translated also wordsim-353 into German, Italian and Russian and used crowdsourcing to score the pairs. Finally, Jiang et al. (2018) translated with Google Cloud the test set from English into Czech, Danish and Dutch. In our work,"
2020.lrec-1.335,N18-1093,0,0.0386551,"Missing"
2020.lrec-1.335,D14-1162,0,0.0821074,"Missing"
2020.lrec-1.335,N18-1202,0,0.0179644,"textual word embeddings and evaluate multilingual BERT on a named entity recognition task. For this, we annotate with named entities the Global Voices corpus for Yor`ub´a. As output of the work, we provide corpora, embeddings and the test suits for both languages. Keywords: Multilingual embeddings, Low-resource language, Yor`ub´a, and Twi 1. Introduction In recent years, word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) have been proven to be very useful for training downstream natural language processing (NLP) tasks. Moreover, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019) have been shown to further improve the performance of NLP tasks such as named entity recognition, question answering, and text classification when used as word features because they are able to resolve ambiguities of word representations when they appear in different contexts. Different deep learning architectures such as multilingual BERT (Devlin et al., 2019), LASER (Artetxe and Schwenk, 2019) and XLM (Lample and Conneau, 2019) have proved successful in the multilingual setting. All these architectures learn the semantic representations from unannotated text, making th"
2020.lrec-1.335,P16-1162,0,0.00811698,"star–movies are not related in the Twi language and the score has been modified accordingly. 5. Semantic Representations In this section, we describe the architectures used for learning word embeddings for the Twi and Yor`ub´a languages. Also, we discuss the quality of the embeddings as measured by the correlation with human judgements on the translated wordSim-353 test sets and by the F1 score in a NER task. 5.1. Word Embeddings Architectures Modeling sub-word units has recently become a popular way to address out-of-vocabulary word problem in NLP especially in word representation learning (Sennrich et al., 2016; Bojanowski et al., 2017; Devlin et al., 2019). A sub-word unit can be a character, character n-grams, or heuristically learned Byte Pair Encodings (BPE) which work very well in practice especially for morphologically rich languages. Here, we consider two word embedding models that make use of character-level information together with word information: Character Word Embedding (CWE) (Chen et al., 2015) and fastText (Bojanowski et al., 2017). Both of them are extensions of the Word2Vec architectures (Mikolov et al., 2013) that model sub-word units, character embeddings in the case of CWE and c"
2020.lrec-1.502,P19-1309,0,0.0815915,"ences based on the available metadata in Wikipedia texts. Both Yasuda and Sumita (2008) and Plamada and Volk (2012) extracted parallel sentences by translating the articles into a common language and consider those sentences with a high translation quality to be parallel. The ACCURAT project (S¸tef˘anescu et al., 2012; Skadin¸a et al., 2012) also devoted efforts in parallel sentence mining in Wikipedia. Later, Barr´on-Cede˜no et al. (2015) used the combination of cross-lingual similarity measures to extract domain specific parallel sentences. The most recent initiative is the so-called LASER (Artetxe and Schwenk, 2019b), which relies on vector representations of sentences to extract similar pairs. This toolkit has been used to extract the WikiMatrix corpus (Schwenk et al., 2019) which contains 135 million parallel sentences for 1,620 different language pairs in 85 different languages. As far as we are concerned, there is no gender-balanced dataset for machine translation, except for the artificial gold standard created for English–Spanish (Font and Costajuss`a, 2019). However, there are a notable number of works towards doing research in the area: from balancing data sets in monolingual tasks (Webster et a"
2020.lrec-1.502,Q19-1038,0,0.124746,"ences based on the available metadata in Wikipedia texts. Both Yasuda and Sumita (2008) and Plamada and Volk (2012) extracted parallel sentences by translating the articles into a common language and consider those sentences with a high translation quality to be parallel. The ACCURAT project (S¸tef˘anescu et al., 2012; Skadin¸a et al., 2012) also devoted efforts in parallel sentence mining in Wikipedia. Later, Barr´on-Cede˜no et al. (2015) used the combination of cross-lingual similarity measures to extract domain specific parallel sentences. The most recent initiative is the so-called LASER (Artetxe and Schwenk, 2019b), which relies on vector representations of sentences to extract similar pairs. This toolkit has been used to extract the WikiMatrix corpus (Schwenk et al., 2019) which contains 135 million parallel sentences for 1,620 different language pairs in 85 different languages. As far as we are concerned, there is no gender-balanced dataset for machine translation, except for the artificial gold standard created for English–Spanish (Font and Costajuss`a, 2019). However, there are a notable number of works towards doing research in the area: from balancing data sets in monolingual tasks (Webster et a"
2020.lrec-1.502,W15-3402,1,0.87962,"Missing"
2020.lrec-1.502,W19-3805,1,0.731759,"Missing"
2020.lrec-1.502,2012.eamt-1.60,0,0.0168473,"al dataset for 22 European languages plus Arabic with more than 12,000 sentences coming from JRCAcquis, that is, a collection of legislative texts of the European Union. In order to make the test set equal in all the languages, only sentences that are parallel simultaneously in the 22 languages were extracted (Koehn et al., 2009) and, therefore, the document structure of the data is lost. The Web Inventory of Transcribed and Translated Talks, WIT3(2) , includes English subtitles from TED talks and their translations currently in 109 languages. Parallel corpora are extracted for several pairs (Cettolo et al., 2012) and test sets are annually prepared for the International Workshop on Spoken Language Translation (IWSLT) evaluation campaigns. Test sets exist for all the pairs among German, English, Italian, Dutch and Romanian; and from English to Arabic, Basque, Chinese, Czech, Dutch, Farsi, French, German, Hebrew, Italian, Japanese, Korean, Polish, Portuguese-Brazil, Romanian, Russian, Slovak, Slovenian, Spanish, Thai, Turkish and Vietnamese. In this case the whole talks are aligned at sentence level, so, the document structure is kept but the set is not equivalent in all the languages. Similarly, the ne"
2020.lrec-1.502,2012.eamt-1.37,0,0.0225878,"Missing"
2020.lrec-1.502,W19-3821,1,0.718121,"Missing"
2020.lrec-1.502,E17-2038,0,0.0193044,"lel sen1 4081 http://www.statmt.org/wmt19/similar.html tence extraction from Wikipedia and a brief mention to general research lines on gender bias in NLP. Section 3. describes the general architecture and Section 4. the methodology, evaluation and characteristics of the extracted corpora. Finally, Section 5. summarizes the work and points at several future extensions of GeBioToolkit. 2. Related Work There are several multilingual parallel datasets available to evaluate MT outputs. The corpora covering more languages are JRC-Acquis (Acquis Communautaire) and the TED talks corpus. Arab-Acquis (Habash et al., 2017) is a multilingual dataset for 22 European languages plus Arabic with more than 12,000 sentences coming from JRCAcquis, that is, a collection of legislative texts of the European Union. In order to make the test set equal in all the languages, only sentences that are parallel simultaneously in the 22 languages were extracted (Koehn et al., 2009) and, therefore, the document structure of the data is lost. The Web Inventory of Transcribed and Translated Talks, WIT3(2) , includes English subtitles from TED talks and their translations currently in 109 languages. Parallel corpora are extracted for"
2020.lrec-1.502,2009.mtsummit-papers.7,0,0.0397743,"eral future extensions of GeBioToolkit. 2. Related Work There are several multilingual parallel datasets available to evaluate MT outputs. The corpora covering more languages are JRC-Acquis (Acquis Communautaire) and the TED talks corpus. Arab-Acquis (Habash et al., 2017) is a multilingual dataset for 22 European languages plus Arabic with more than 12,000 sentences coming from JRCAcquis, that is, a collection of legislative texts of the European Union. In order to make the test set equal in all the languages, only sentences that are parallel simultaneously in the 22 languages were extracted (Koehn et al., 2009) and, therefore, the document structure of the data is lost. The Web Inventory of Transcribed and Translated Talks, WIT3(2) , includes English subtitles from TED talks and their translations currently in 109 languages. Parallel corpora are extracted for several pairs (Cettolo et al., 2012) and test sets are annually prepared for the International Workshop on Spoken Language Translation (IWSLT) evaluation campaigns. Test sets exist for all the pairs among German, English, Italian, Dutch and Romanian; and from English to Arabic, Basque, Chinese, Czech, Dutch, Farsi, French, German, Hebrew, Itali"
2020.lrec-1.502,W19-5404,0,0.0178211,"able per language. Figure 2 shows the amount of these articles for the 20 languages with the largest number of biographies. The edition with the most available entries is the English one with 922,120 entries. The Spanish Wikipedia contains 148,445 entries (6th largest one) and the Catalan edition 40,983 (20th largest one). All three are highlighted in Figure 2. Even if the Catalan Wikipedia is not as big as the English and Spanish ones, there is a noticeable amount of comparable articles between Spanish and Catalan which translates into significant number of parallel sentences —Schwenk et al. (2019) extracted in WikiMatrix 3,377 k sentences for en–es, 1,580 k sentences for ca–es and 210 k sentences for en–ca from the full editions. GeBioToolkit extracts multilingually aligned parallel sentences, so it is interesting to study also the union of languages. The more languages involved, the lesser number or comparable articles one will obtain. Figure 3 shows the number of articles per sets of languages and broken-down by gender. The total number of documents decays when starting with English and Arabic (set with only 2 languages) and one incrementally adds French, German, Italian, Spanish, Po"
2020.lrec-1.502,D18-1512,0,0.0368879,"Missing"
2020.lrec-1.502,D18-1302,0,0.0436951,"Missing"
2020.lrec-1.502,N18-2002,0,0.114616,"Missing"
2020.lrec-1.502,skadina-etal-2012-collecting,0,0.0297572,"ations of PoS, lemmas, syntactic dependencies, anaphora, discourse connectives, classified named entities and temporal expressions. The authors in Bamman and Smith (2014) also extract 927,403 biographies in this case from the English Wikipedia. The set is pre-processed in order to learn event classes in biographies. Regarding the automatic extraction of parallel corpora, Wikipedia has been traditionally used as a resource. In Adafre and de Rijke (2006), the authors extract parallel sentences based on the available metadata in Wikipedia texts. Both Yasuda and Sumita (2008) and Plamada and Volk (2012) extracted parallel sentences by translating the articles into a common language and consider those sentences with a high translation quality to be parallel. The ACCURAT project (S¸tef˘anescu et al., 2012; Skadin¸a et al., 2012) also devoted efforts in parallel sentence mining in Wikipedia. Later, Barr´on-Cede˜no et al. (2015) used the combination of cross-lingual similarity measures to extract domain specific parallel sentences. The most recent initiative is the so-called LASER (Artetxe and Schwenk, 2019b), which relies on vector representations of sentences to extract similar pairs. This too"
2020.lrec-1.502,P19-1164,0,0.0646622,"Missing"
2020.lrec-1.502,Q18-1042,0,0.09189,": corpora, gender bias, Wikipedia, machine translation 1. Introduction Gender biases are present in many natural language processing applications (Costa-juss`a, 2019). This comes as an undesired characteristic of deep learning architectures where their outputs seem to reflect demographic asymmetries (Prates et al., 2019). This is of course not due to the architecture itself but to the data used to train a system. Recent research is being devoted to correct the asymmetries mainly by data augmentation techniques in fields such as coreference resolution (Rudinger et al., 2018; Zhao et al., 2018; Webster et al., 2018) or abusive language detection (Park et al., 2018). Test sets have been created in those cases, but we are not aware of any test set available for machine translation (MT). From another side, machine translation either neural, statistical or rule-based, usually operates in a sentence-bysentence basis. However, when translating consistently a document, surrounding sentences may have valuable information. The translation of pronouns, verb tenses and even content words might depend on other fragments within a document. This affects also the translation of the gender markers, specially when transl"
2020.lrec-1.502,N18-2003,0,0.122658,"Missing"
2021.emnlp-main.676,2021.motra-1.1,1,0.79007,"Missing"
2021.emnlp-main.676,2020.iwslt-1.34,1,0.834671,"Missing"
2021.emnlp-main.676,J19-2006,0,0.0464818,"Missing"
2021.emnlp-main.676,P11-1132,0,0.0887813,"Missing"
2021.emnlp-main.676,2009.mtsummit-papers.9,0,0.219741,"Missing"
2021.emnlp-main.676,L16-1664,0,0.0285148,"Missing"
2021.emnlp-main.676,R11-1091,0,0.0837069,"Missing"
2021.emnlp-main.676,P17-1049,0,0.0443256,"Missing"
2021.emnlp-main.676,Q15-1030,0,0.0191601,"–ALL 92.4±0.2 82.6±1.1 87.3±0.6 - 76.6±0.7 94.4±0.1 85.3±0.4 - 90.7±0.1 72.9±0.9 81.7±0.5 - 64.7±1.4 91.9±0.4 78.3±0.7 - 92.3±0.2 78.8±1.6 85.9±0.9 78.8±0.9 91.4±0.3 85.0±0.6 90.5±0.3 91.8±0.4 90.9±0.3 - 87.3±0.4 88.6±0.4 87.9±0.4 - 90.6±0.1 89.0±0.2 89.9±0.1 Table 3: BERT translationese classification accuracy of all TRG–SRC and TRG–ALL models on TRG–SRC and TRG–ALL test sets (average and standard deviation over 5 runs). Columns: training set; rows: test set. e.g., trail BERT by ∼20 accuracy points. To make sure our hand-crafted-feature-based SVM results are competitive, we compare them with Rabinovich and Wintner (2015) on our data. Rabinovich and Wintner (2015) show that training a SVM classifier on the top 1000 most frequent POS- or character-trigrams yields SOTA translationese classification results on Europarl data. On our data, POS-trigrams yield around 5 points increase in accuracy for most of the datasets and character-trigrams tend to lower the accuracy by around 4 points (Appendix A.3). For the remainder of the paper we continue to work with our handcrafted features, designed to capture various linguistic aspects of translationese. 5 Multilinguality and Cross-Language Performance Since neural archit"
2021.emnlp-main.676,2020.acl-main.691,0,0.0734577,"Missing"
2021.emnlp-main.676,N16-1110,1,0.899056,"Missing"
2021.emnlp-main.676,R19-1130,0,0.0272166,"Missing"
2021.emnlp-main.676,W17-0230,0,0.0459553,"Missing"
2021.motra-1.1,cartoni-meyer-2012-extracting,0,0.144395,"gned English–French and English–German book corpora, collected from public domain books, and an English–German corpus of political news and commentary collected from the Project Syndicate2 and Diplomatisches Magazin3 . However their parallel corpora do not contain any meta-information, and the monolingual corpora have information that is not always consistent and also scarce, according to Karakanta et al. (2018). Gra¨en et al. (2014) attempted to clean and correct some errors in the Europarl corpus of Koehn (2005). Islam and Mehler (2012); Lembersky et al. (2011); Rabinovich et al. (2015) and Cartoni and Meyer (2012) employed the Europarl corpus of Koehn (2005) for translation studies, relying on its metadata (”language tags”). Ustaszewski (2019) created the EuroparlExtract toolkit that allows extraction of bilingual parallel corpora and monolingual comparable corpora from the Europarl corpus of Koehn (2005) with explicit annotation of translation direction and source language. They also rely on the metadata present in the Europarl corpus of Koehn (2005). Nisioi et al. (2016) additionally crawl the information about the Members of the European Parliament (MEPs) from the European Parliament’s website in or"
2021.motra-1.1,islam-mehler-2012-customization,0,0.0167292,"rpus from TED talks, annotated for translation direction. They also provide aligned English–French and English–German book corpora, collected from public domain books, and an English–German corpus of political news and commentary collected from the Project Syndicate2 and Diplomatisches Magazin3 . However their parallel corpora do not contain any meta-information, and the monolingual corpora have information that is not always consistent and also scarce, according to Karakanta et al. (2018). Gra¨en et al. (2014) attempted to clean and correct some errors in the Europarl corpus of Koehn (2005). Islam and Mehler (2012); Lembersky et al. (2011); Rabinovich et al. (2015) and Cartoni and Meyer (2012) employed the Europarl corpus of Koehn (2005) for translation studies, relying on its metadata (”language tags”). Ustaszewski (2019) created the EuroparlExtract toolkit that allows extraction of bilingual parallel corpora and monolingual comparable corpora from the Europarl corpus of Koehn (2005) with explicit annotation of translation direction and source language. They also rely on the metadata present in the Europarl corpus of Koehn (2005). Nisioi et al. (2016) additionally crawl the information about the Member"
2021.motra-1.1,2005.mtsummit-papers.11,0,0.603632,"lish–French corpus from TED talks, annotated for translation direction. They also provide aligned English–French and English–German book corpora, collected from public domain books, and an English–German corpus of political news and commentary collected from the Project Syndicate2 and Diplomatisches Magazin3 . However their parallel corpora do not contain any meta-information, and the monolingual corpora have information that is not always consistent and also scarce, according to Karakanta et al. (2018). Gra¨en et al. (2014) attempted to clean and correct some errors in the Europarl corpus of Koehn (2005). Islam and Mehler (2012); Lembersky et al. (2011); Rabinovich et al. (2015) and Cartoni and Meyer (2012) employed the Europarl corpus of Koehn (2005) for translation studies, relying on its metadata (”language tags”). Ustaszewski (2019) created the EuroparlExtract toolkit that allows extraction of bilingual parallel corpora and monolingual comparable corpora from the Europarl corpus of Koehn (2005) with explicit annotation of translation direction and source language. They also rely on the metadata present in the Europarl corpus of Koehn (2005). Nisioi et al. (2016) additionally crawl the inf"
2021.motra-1.1,P11-1132,0,0.266496,"some examples. It is also useful to know whether the original text has been produced by a native speaker, as it has been shown that texts produced by non-native speakers can be quite easily separated from the texts produced by native speakers and translated texts (Nisioi et al., 2016). Information about native language and qualifications of the translator is also relevant. For this reason, collecting multilingual (samedomain) data suitable for studying translationese is a challenging task. The proceedings of the European Parliament (Europarl) have often been used previously for this purpose (Koppel and Ordan, 2011; Rabinovich and Wintner, 2015; Lembersky et al., 2011), as they cover a lot of languages and provide relevant metadata. However, one problem with this data is that translation in the European Parliament sometimes happens indirectly, through pivot (also called ”bridge” or ”relay”) languages. With 24 official languages, there are 552 possible direct translation combinations, therefore translations are often made first into one of the most frequently used languages: English, French or German, and then into other languages (Parliament, c; Katsarova, 2011). This can be problematic for studies that"
2021.motra-1.1,D11-1034,0,0.181291,"original text has been produced by a native speaker, as it has been shown that texts produced by non-native speakers can be quite easily separated from the texts produced by native speakers and translated texts (Nisioi et al., 2016). Information about native language and qualifications of the translator is also relevant. For this reason, collecting multilingual (samedomain) data suitable for studying translationese is a challenging task. The proceedings of the European Parliament (Europarl) have often been used previously for this purpose (Koppel and Ordan, 2011; Rabinovich and Wintner, 2015; Lembersky et al., 2011), as they cover a lot of languages and provide relevant metadata. However, one problem with this data is that translation in the European Parliament sometimes happens indirectly, through pivot (also called ”bridge” or ”relay”) languages. With 24 official languages, there are 552 possible direct translation combinations, therefore translations are often made first into one of the most frequently used languages: English, French or German, and then into other languages (Parliament, c; Katsarova, 2011). This can be problematic for studies that compare translations coming from different source lang"
2021.motra-1.1,L16-1664,0,0.671741,"ource language patterns in target text (Toury, 1979; Teich, 2003). In order to be successfully used for studying translationese phenomena, corpora need to be equipped with additional meta-information: whether the text is original or translated, the direction of translation, production mode of the source text (spoken/written) to give some examples. It is also useful to know whether the original text has been produced by a native speaker, as it has been shown that texts produced by non-native speakers can be quite easily separated from the texts produced by native speakers and translated texts (Nisioi et al., 2016). Information about native language and qualifications of the translator is also relevant. For this reason, collecting multilingual (samedomain) data suitable for studying translationese is a challenging task. The proceedings of the European Parliament (Europarl) have often been used previously for this purpose (Koppel and Ordan, 2011; Rabinovich and Wintner, 2015; Lembersky et al., 2011), as they cover a lot of languages and provide relevant metadata. However, one problem with this data is that translation in the European Parliament sometimes happens indirectly, through pivot (also called ”br"
2021.motra-1.1,Q15-1030,0,0.54467,"so useful to know whether the original text has been produced by a native speaker, as it has been shown that texts produced by non-native speakers can be quite easily separated from the texts produced by native speakers and translated texts (Nisioi et al., 2016). Information about native language and qualifications of the translator is also relevant. For this reason, collecting multilingual (samedomain) data suitable for studying translationese is a challenging task. The proceedings of the European Parliament (Europarl) have often been used previously for this purpose (Koppel and Ordan, 2011; Rabinovich and Wintner, 2015; Lembersky et al., 2011), as they cover a lot of languages and provide relevant metadata. However, one problem with this data is that translation in the European Parliament sometimes happens indirectly, through pivot (also called ”bridge” or ”relay”) languages. With 24 official languages, there are 552 possible direct translation combinations, therefore translations are often made first into one of the most frequently used languages: English, French or German, and then into other languages (Parliament, c; Katsarova, 2011). This can be problematic for studies that compare translations coming f"
2021.motra-1.1,N16-1110,1,0.77393,"Missing"
2021.motra-1.1,L16-1561,0,0.0274017,"nised as follows. Section 2 presents previous work done on building corpora for translationese research, and, in particular, corpora based on the proceedings of the European Parliament. Section 3 describes the procedure of creating the corpora. In Section 4, we compare the ”reliable” and ”unreliable” parts of the corpus on the task of translationese classification. Lastly, in Section 5 we present our conclusions and ideas for future work. 2 Related Work 2.1 Data available for translationese research There are only a few multilingual corpora for translationese research. The UN parallel corpus (Ziemski et al., 2016) consists of multilingual parliamentary documents of the United Nations in 6 languages, organized into bilingual parallel corpora. From this corpus Tolochinsky et al. (2018) derived 5 parallel corpora from English into other languages and annotated them for translation direction. The Canadian Hansard corpus1 consists of transcriptions of the Canadian parliament in English and French and their translations, and has metadata indicating the original language. Rabinovich et al. (2015) compile a parallel English–French corpus from TED talks, annotated for translation direction. They also provide al"
2021.mtsummit-at4ssl.5,N19-1423,0,0.00643251,"n between written text and the MMS abstraction. For the task, we will train a neural network that takes sequences of words as input and outputs the most probable class for each element in the vocabulary. Inflection parameters will be predicted as continuous real numbers. Given that machine learning heavily depends on the amount of data used for training, and the corpus might not achieve consistent sizes in the short term, we will adopt both transfer learning and data augmentation techniques. In the first case, we will use pre-trained language models that will be fine-tuned to perform our task [3]. In the second case, we will generate synthetic data using the relations in WordNet, word classes, and our vocabulary joined with unsupervised methods when possible [19, 4]. Avatar creation For the character creation, a state-of-the-art 3D computer graphic program (e.g., Autodesk 3ds Max) will be used. For the development of the photorealistic avatar a 3D photo-scan system for generating high level realistic face textures will be build up. To avoid errors potentially introduced while retargeting between the MoCap data and the avatar’s skeleton, the avatar is tuned according to body measuremen"
2021.mtsummit-at4ssl.5,2020.emnlp-main.488,0,0.0126152,"or each element in the vocabulary. Inflection parameters will be predicted as continuous real numbers. Given that machine learning heavily depends on the amount of data used for training, and the corpus might not achieve consistent sizes in the short term, we will adopt both transfer learning and data augmentation techniques. In the first case, we will use pre-trained language models that will be fine-tuned to perform our task [3]. In the second case, we will generate synthetic data using the relations in WordNet, word classes, and our vocabulary joined with unsupervised methods when possible [19, 4]. Avatar creation For the character creation, a state-of-the-art 3D computer graphic program (e.g., Autodesk 3ds Max) will be used. For the development of the photorealistic avatar a 3D photo-scan system for generating high level realistic face textures will be build up. To avoid errors potentially introduced while retargeting between the MoCap data and the avatar’s skeleton, the avatar is tuned according to body measurements on the actor. As suggested in previous research [8], we will apply high contrast between skin, clothes and background color, and will provide careful lighting with shadow"
2021.mtsummit-at4ssl.5,D19-1670,0,0.0203979,"or each element in the vocabulary. Inflection parameters will be predicted as continuous real numbers. Given that machine learning heavily depends on the amount of data used for training, and the corpus might not achieve consistent sizes in the short term, we will adopt both transfer learning and data augmentation techniques. In the first case, we will use pre-trained language models that will be fine-tuned to perform our task [3]. In the second case, we will generate synthetic data using the relations in WordNet, word classes, and our vocabulary joined with unsupervised methods when possible [19, 4]. Avatar creation For the character creation, a state-of-the-art 3D computer graphic program (e.g., Autodesk 3ds Max) will be used. For the development of the photorealistic avatar a 3D photo-scan system for generating high level realistic face textures will be build up. To avoid errors potentially introduced while retargeting between the MoCap data and the avatar’s skeleton, the avatar is tuned according to body measurements on the actor. As suggested in previous research [8], we will apply high contrast between skin, clothes and background color, and will provide careful lighting with shadow"
2021.mtsummit-research.6,P19-1310,0,0.0641255,"Missing"
2021.mtsummit-research.6,2020.lrec-1.335,1,0.856378,"Missing"
2021.mtsummit-research.6,P19-1309,0,0.016623,"communities covered by MT, this does not scale to the sheer amount of possible language combinations. Another research line focuses on low-resource Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 70 MT from the modeling side, developing methods which allow a MT system to learn the translation task with smaller amounts of supervisory signals. This is done by exploiting the weaker supervisory signals in larger amounts of available monolingual data, e.g. by identifying additional parallel data in monolingual corpora (Artetxe and Schwenk, 2019; Schwenk et al., 2021, 2020), comparable corpora (Ruiter et al., 2019, 2021), or by including auto-encoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017) during training. Low-resource language pairs can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Lample and Conneau, 2019), or finding an optimal path of pivoting through related languages (Leng et al., 2019). By adapting the model hyperparameters to the low-resour"
2021.mtsummit-research.6,2020.lrec-1.296,0,0.0684272,"Missing"
2021.mtsummit-research.6,W17-4715,0,0.0186929,"low-resource Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 70 MT from the modeling side, developing methods which allow a MT system to learn the translation task with smaller amounts of supervisory signals. This is done by exploiting the weaker supervisory signals in larger amounts of available monolingual data, e.g. by identifying additional parallel data in monolingual corpora (Artetxe and Schwenk, 2019; Schwenk et al., 2021, 2020), comparable corpora (Ruiter et al., 2019, 2021), or by including auto-encoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017) during training. Low-resource language pairs can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Lample and Conneau, 2019), or finding an optimal path of pivoting through related languages (Leng et al., 2019). By adapting the model hyperparameters to the low-resource scenario, Sennrich and Zhang (2019) were able to achieve impressive improvements over a standard NMT system. 6 Conclusion We pr"
2021.mtsummit-research.6,N19-1423,0,0.0173885,"2019), low-resourced language pairs benefit from multilinguality when translated into English, but improvements are minor when translating into the non-English language. For the other translation direction, en–yo, we notice that lots of diacritics are lost in Google translations, damaging the BLEU scores. Whether this drop in BLEU scores really affects understanding or not is analyzed via a human evaluation (Section 4.4). Diacritization Diacritics are important for Yor`ub´a embeddings (Alabi et al., 2020). However, they are often ignored in popular multilingual models (e.g. multilingual BERT (Devlin et al., 2019)), and not consistently available in training corpora and even test sets. In order to investigate whether the diacritics in Yor`ub´a MT can help to disambiguate translation choices, we additionally train yo–enu equivalent models on undiacritized JW300, JW300+Bible and JW300+Bible+MENYO-20k (Table 2, indicated as yo–enu in comparison to the ones with diacritics yo–en). Since one cannot generate correct Yor`ub´a text when training without diacritics, en–you systems are not trained. Alternatively, we restore diacritics using our in-house diacritizer in the output of open source models that produc"
2021.mtsummit-research.6,2020.findings-emnlp.195,0,0.0464574,"Missing"
2021.mtsummit-research.6,2009.eamt-1.26,0,0.029557,"rk In order to make MT available for a broader range of linguistic communities, recent years have seen an effort in creating new parallel corpora for low-resource language pairs. Recently, Guzm´an et al. (2019) provided novel supervised, semi-supervised and unsupervised benchmarks for Indo-Aryan languages {Sinhala,Nepali}–English on an evaluation set of professionally translated sentences sourced from the Sinhala, Nepali and English Wikipedias. Novel parallel corpora focusing on African languages cover South African languages ({Afrikaans, isiZulu, Northern Sotho, Setswana, Xitsonga}–English) (Groenewald and Fourie, 2009) with MT benchmarks evaluated in Martinus and Abbott (2019), as well as multidomain (News, Wikipedia, Twitter, Conversational) Amharic–English (Hadgu et al., 2020) and multidomain (Government, Wikipedia, News etc.) Igbo–English (Ezeani et al., 2020). Further, the LORELEI project (Strassel and Tracey, 2016) has created parallel corpora for a variety of lowresource language pairs, including a number of Niger-Congo languages such as {isiZulu, Twi, Wolof, Yor`ub´a }–English. However, these are not open-access. On the contrary, Masakhane (∀ et al., 2020) is an ongoing participatory project focusing"
2021.mtsummit-research.6,D19-1632,0,0.037199,"Missing"
2021.mtsummit-research.6,W11-2123,0,0.187455,"Missing"
2021.mtsummit-research.6,Q17-1024,0,0.059651,"Missing"
2021.mtsummit-research.6,P07-2045,0,0.0240237,"Missing"
2021.mtsummit-research.6,N19-4009,0,0.0563143,"Missing"
2021.mtsummit-research.6,P02-1040,0,0.109228,"Missing"
2021.mtsummit-research.6,W15-3049,0,0.0457539,"Missing"
2021.mtsummit-research.6,D17-1039,0,0.0232049,"Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 70 MT from the modeling side, developing methods which allow a MT system to learn the translation task with smaller amounts of supervisory signals. This is done by exploiting the weaker supervisory signals in larger amounts of available monolingual data, e.g. by identifying additional parallel data in monolingual corpora (Artetxe and Schwenk, 2019; Schwenk et al., 2021, 2020), comparable corpora (Ruiter et al., 2019, 2021), or by including auto-encoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017) during training. Low-resource language pairs can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Lample and Conneau, 2019), or finding an optimal path of pivoting through related languages (Leng et al., 2019). By adapting the model hyperparameters to the low-resource scenario, Sennrich and Zhang (2019) were able to achieve impressive improvements over a standard NMT system. 6 Conclusion We present MENYO-20k, a novel en–yo multi-domain parallel corpus for machine transl"
2021.mtsummit-research.6,P19-1178,1,0.896345,"Missing"
2021.mtsummit-research.6,2021.mtsummit-research.7,1,0.816425,"Missing"
2021.mtsummit-research.6,2021.eacl-main.115,0,0.0855549,"Missing"
2021.mtsummit-research.6,P16-1162,0,0.162025,"Missing"
2021.mtsummit-research.6,L16-1521,0,0.014966,"inhala,Nepali}–English on an evaluation set of professionally translated sentences sourced from the Sinhala, Nepali and English Wikipedias. Novel parallel corpora focusing on African languages cover South African languages ({Afrikaans, isiZulu, Northern Sotho, Setswana, Xitsonga}–English) (Groenewald and Fourie, 2009) with MT benchmarks evaluated in Martinus and Abbott (2019), as well as multidomain (News, Wikipedia, Twitter, Conversational) Amharic–English (Hadgu et al., 2020) and multidomain (Government, Wikipedia, News etc.) Igbo–English (Ezeani et al., 2020). Further, the LORELEI project (Strassel and Tracey, 2016) has created parallel corpora for a variety of lowresource language pairs, including a number of Niger-Congo languages such as {isiZulu, Twi, Wolof, Yor`ub´a }–English. However, these are not open-access. On the contrary, Masakhane (∀ et al., 2020) is an ongoing participatory project focusing on creating new freely-available parallel corpora and MT benchmark models for a large variety of African languages. While creating parallel resources for low-resource language pairs is one approach to increase the number of linguistic communities covered by MT, this does not scale to the sheer amount of p"
2021.mtsummit-research.6,2020.eamt-1.61,0,0.0828348,"Missing"
2021.mtsummit-research.6,2021.naacl-main.41,0,0.0870514,"Missing"
2021.mtsummit-research.6,D16-1163,0,0.0137513,"m to learn the translation task with smaller amounts of supervisory signals. This is done by exploiting the weaker supervisory signals in larger amounts of available monolingual data, e.g. by identifying additional parallel data in monolingual corpora (Artetxe and Schwenk, 2019; Schwenk et al., 2021, 2020), comparable corpora (Ruiter et al., 2019, 2021), or by including auto-encoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017) during training. Low-resource language pairs can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Lample and Conneau, 2019), or finding an optimal path of pivoting through related languages (Leng et al., 2019). By adapting the model hyperparameters to the low-resource scenario, Sennrich and Zhang (2019) were able to achieve impressive improvements over a standard NMT system. 6 Conclusion We present MENYO-20k, a novel en–yo multi-domain parallel corpus for machine translation and domain adaptation. By defining a standardized train-development-test split of this corpus, we provide several NMT benchmar"
2021.mtsummit-research.7,P19-1310,0,0.168016,"Missing"
2021.mtsummit-research.7,P17-1042,0,0.017738,"e monolingual Wikipedias to initialize SSNMT. As the monolingual Wikipedia for Yor`ub´a is especially small (65 k sentences), we use the Yor`ub´a side of JW300 (Agi´c and Vuli´c, 2019) as additional monolingual initialization data. For each monolingual data pair en–{af ,...,yo}, the large English monolingual corpus is downsampled to its low(er)-resource counterpart before using the data (Monolingual in Table 1). For the word-embedding-based initialization, we learn CBOW word embeddings using word2vec (Mikolov et al., 2013), which are then projected into a common multilingual space via vecmap (Artetxe et al., 2017) to attain bilingual embeddings between en–{af ,...,yo}. For the weak-supervision of the bilingual mapping process, we use a list of numbers (en–f r only) which is augmented with 200 Swadesh list4 entries for the low-resource experiments. For DAE initialization, we do not use external, highly-multilingual pre-trained language models, since in practical terms these may not cover the language combination of interest5 . We therefore use the monolingual data to train a bilingual (en+{af ,...yo}) DAE using BART-style 1 Dumps were downloaded on February 2021 from dumps.wikimedia.org/ 2 github.com/at"
2021.mtsummit-research.7,P19-1019,0,0.214691,"g autoencoding, SSNMT with backtranslation and bilingual finetuning enables us to learn machine translation even for distant language pairs for which only small amounts of monolingual data are available, e.g. yielding BLEU scores of 11.6 (English to Swahili). 1 Introduction Neural machine translation (NMT) achieves high quality translations when large amounts of parallel data are available (Barrault et al., 2020). Unfortunately, for most language combinations, parallel data is non-existent, scarce or low-quality. To overcome this, unsupervised MT (UMT) (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019) focuses on exploiting large amounts of monolingual data, which are used to generate synthetic bitext training data via various techniques such as back-translation or denoising. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) learns from smaller amounts of comparable data –i.e. topic-aligned data such as Wikipedia articles– by learning to discover and exploit similar sentence pairs. However, both UMT and SSNMT approaches often do not scale to low-resource languages, for which neither monolingual nor comparable data are available in sufficient quantity (Guzm´an et al., 2019; Espa˜na-Bonet et"
2021.mtsummit-research.7,D18-1549,0,0.230264,"al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on smal"
2021.mtsummit-research.7,P19-1309,0,0.372739,"low-resource similar and distant language pairs, i.e. English (en)–{Afrikaans (af ), Kannada (kn), Burmese (my), Nepali (ne), Swahili (sw), Yor`ub´a (yo)}, chosen based on their differences in typology (analytic, fusional, agglutinative), word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation ofte"
2021.mtsummit-research.7,Q19-1038,0,0.168425,"low-resource similar and distant language pairs, i.e. English (en)–{Afrikaans (af ), Kannada (kn), Burmese (my), Nepali (ne), Swahili (sw), Yor`ub´a (yo)}, chosen based on their differences in typology (analytic, fusional, agglutinative), word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation ofte"
2021.mtsummit-research.7,P06-4018,0,0.0228013,"5) for word sequence masking. We add one random mask insertion per sequence and perform a sequence permutation. For the multilingual DAE (MDAE) setting, we train a single denoising autoencoder on the monolingual data of all languages, where en is downsampled to match the largest non-English monolingual dataset (kn). In all cases SSNMT training is bidirectional between two languages en–{af ,...,yo}, except for MDAE, where SSNMT is trained multilingually between all language combinations in {af ,en,...,yo}. 4.2 Preprocessing On the Wikipedia corpora, we perform sentence tokenization using NLTK (Bird, 2006). For languages using Latin scripts (af ,en,sw,yo) we perform punctuation normalization and truecasing using standard Moses (Koehn et al., 2007) scripts on all datasets. For Yor`ub´a only, we follow Adelani et al. (2021b) and perform automatic diacritic restoration. Lastly, we perform language identification on all Wikipedia corpora using polyglot.6 After exploring different byte-pair encoding (BPE) (Sennrich et al., 2016b) vocabulary sizes of 2 k, 4 k, 8 k, 16 k and 32 k, we choose 2 k (en–yo), 4 k (en–{kn,my,ne,sw}) and 16 k (en–af ) merge operations using sentence-piece7 (Kudo and Richardso"
2021.mtsummit-research.7,W11-2138,0,0.0261784,"., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional"
2021.mtsummit-research.7,W19-5435,0,0.0270045,"Missing"
2021.mtsummit-research.7,2020.acl-main.747,0,0.0744637,"Missing"
2021.mtsummit-research.7,W17-4715,0,0.0387347,"ed on their differences in typology (analytic, fusional, agglutinative), word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been us"
2021.mtsummit-research.7,2020.eamt-1.10,0,0.073161,"Missing"
2021.mtsummit-research.7,D18-1045,0,0.0130344,"d translation. Given a rejected sentence sL1 with tokens wL1 ∈ L1, we replace each token with its nearest neighbor wL2 ∈ L2 in the bilingual word embedding layer of the T WT model to obtain sW L2 . We then train on the synthetic pair in the opposite direction sL2 → sL1 . As with BT, this is applied to both language directions. To ensure sufficient volume of synthetic data (Figure 2, right), WT data is trained on without filtering. Noise (N): To increase robustness and variance in the training data, we add noise, i.e. token deletion, substitution and permutation, to copies of source sentences (Edunov et al., 2018) in parallel pairs identified via SPE, back-translations and word-translated sentences and, as with WT, we use these without additional filtering. Initialization: When languages are related and large amounts of training data is available, the initialization of SSNMT is not important. However, similarly to UMT, initialization becomes crucial in the low-resource setting (Edman et al., 2020). We explore four different initialization techniques: i) no initialization (none), i.e. random initialization for all model parameters, ii) initialization of tied source and target side word embedding layers"
2021.mtsummit-research.7,2020.emnlp-main.480,0,0.0278832,"Missing"
2021.mtsummit-research.7,W19-5315,1,0.881943,"Missing"
2021.mtsummit-research.7,D19-1632,0,0.147595,"Missing"
2021.mtsummit-research.7,W18-2703,0,0.0179433,"amachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results ("
2021.mtsummit-research.7,Q17-1024,0,0.229862,"Missing"
2021.mtsummit-research.7,2020.eamt-1.5,0,0.0358621,"Missing"
2021.mtsummit-research.7,W04-3250,0,0.0618963,"Missing"
2021.mtsummit-research.7,P07-2045,0,0.010509,"(MDAE) setting, we train a single denoising autoencoder on the monolingual data of all languages, where en is downsampled to match the largest non-English monolingual dataset (kn). In all cases SSNMT training is bidirectional between two languages en–{af ,...,yo}, except for MDAE, where SSNMT is trained multilingually between all language combinations in {af ,en,...,yo}. 4.2 Preprocessing On the Wikipedia corpora, we perform sentence tokenization using NLTK (Bird, 2006). For languages using Latin scripts (af ,en,sw,yo) we perform punctuation normalization and truecasing using standard Moses (Koehn et al., 2007) scripts on all datasets. For Yor`ub´a only, we follow Adelani et al. (2021b) and perform automatic diacritic restoration. Lastly, we perform language identification on all Wikipedia corpora using polyglot.6 After exploring different byte-pair encoding (BPE) (Sennrich et al., 2016b) vocabulary sizes of 2 k, 4 k, 8 k, 16 k and 32 k, we choose 2 k (en–yo), 4 k (en–{kn,my,ne,sw}) and 16 k (en–af ) merge operations using sentence-piece7 (Kudo and Richardson, 2018). We prepend a source and a target language token to each sentence. For the en–f r experiments only, we use the data processing by Ruite"
2021.mtsummit-research.7,2021.dravidianlangtech-1.7,0,0.0417202,"n be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on small amounts of monolingual data (Guzm´an et al., 2019), when there is a domain mismatch between the two datasets (Kim et al., 2020) or when the languages in a pair are distant (Koneru et al., 2021). Unfortunately, all of this is the case for most truly low-resource language pairs. Self-supervised NMT (Ruiter et al., 2019) jointly learns to extract data and translate from comparable data and works best on 100s of thousands of documents per language, well beyond what is available in true low-resource settings. 3 UMT-Enhanced SSNMT SSNMT jointly learns MT and extracting similar sentences for training from comparable corpora in a loop on-line. Sentence pairs from documents in languages L1 and L2 are fed as input to a bidirectional NMT system {L1, L2} → {L1, L2}, which filters out non-simila"
2021.mtsummit-research.7,D18-2012,0,0.0218435,"NLTK (Bird, 2006). For languages using Latin scripts (af ,en,sw,yo) we perform punctuation normalization and truecasing using standard Moses (Koehn et al., 2007) scripts on all datasets. For Yor`ub´a only, we follow Adelani et al. (2021b) and perform automatic diacritic restoration. Lastly, we perform language identification on all Wikipedia corpora using polyglot.6 After exploring different byte-pair encoding (BPE) (Sennrich et al., 2016b) vocabulary sizes of 2 k, 4 k, 8 k, 16 k and 32 k, we choose 2 k (en–yo), 4 k (en–{kn,my,ne,sw}) and 16 k (en–af ) merge operations using sentence-piece7 (Kudo and Richardson, 2018). We prepend a source and a target language token to each sentence. For the en–f r experiments only, we use the data processing by Ruiter et al. (2020) in order to minimize experimental differences for later comparison. 4.3 Model Specifications and Evaluation Systems are either not initialized, initialized via bilingual word embeddings, or via pre-training using (M)DAE. Our implementation of SSNMT is a transformer base with default parameters. We use a batch size of 50 sentences and a maximum sequence length of 100 tokens. For evaluation, we use BLEU (Papineni et al., 2002) calculated using Sa"
2021.mtsummit-research.7,P19-1017,0,0.0159691,"rocessing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several la"
2021.mtsummit-research.7,2020.findings-emnlp.371,0,0.0206148,". NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on small amounts of monolingual data (Guzm´an et al., 2019), when there is a domain mismatch between the two datasets (Kim et al., 2020) or when the languages in a pair are distant (Koneru et al., 2021). Unfortunately, all of this is the case for most truly low-resource language pairs. Self-supervised NMT (Ruiter et al., 2019) jointly learns to extract data and translate fr"
2021.mtsummit-research.7,E17-2002,0,0.0448202,"Missing"
2021.mtsummit-research.7,2020.wmt-1.68,0,0.0126971,"exploiting large amounts of monolingual data, which are used to generate synthetic bitext training data via various techniques such as back-translation or denoising. Self-supervised NMT (SSNMT) (Ruiter et al., 2019) learns from smaller amounts of comparable data –i.e. topic-aligned data such as Wikipedia articles– by learning to discover and exploit similar sentence pairs. However, both UMT and SSNMT approaches often do not scale to low-resource languages, for which neither monolingual nor comparable data are available in sufficient quantity (Guzm´an et al., 2019; Espa˜na-Bonet et al., 2019; Marchisio et al., 2020). To date, UMT data augmentation techniques have not been explored in SSNMT. However, both approaches can benefit from each other, as i) SSNMT has strong internal quality checks on the data it admits for training, which can be Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 76 of use to filter low-quality synthetic data, and ii) UMT data augmentation makes monolingual data available for SSNMT. In this paper we explore and test the effect of combining UMT data augmentation with SSNMT on different data sizes, ranging"
2021.mtsummit-research.7,W18-2710,0,0.018043,"high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). Ho"
2021.mtsummit-research.7,P02-1040,0,0.109861,"Missing"
2021.mtsummit-research.7,W18-6319,0,0.022108,"Missing"
2021.mtsummit-research.7,D17-1039,0,0.161776,"word order (SVO, SOV) and writing system (Latin, Brahmic). We also explore the effect of different initialization techniques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and b"
2021.mtsummit-research.7,P19-1178,1,0.848055,"Missing"
2021.mtsummit-research.7,2020.emnlp-main.202,1,0.852975,"Missing"
2021.mtsummit-research.7,2021.eacl-main.115,0,0.354207,"Missing"
2021.mtsummit-research.7,P16-1009,0,0.0936491,"aging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a rel"
2021.mtsummit-research.7,P16-1162,0,0.546366,"aging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a rel"
2021.mtsummit-research.7,P18-1005,0,0.0178722,"zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et al., 2018; Yang et al., 2018) applies bi-directional back-translation in combination with denoising and multilingual shared encoders to learn MT on very large monolingual data. This can be done multilingually across several languages by using language-specific decoders (Sen et al., 2019), or by using additional parallel data for a related pivot language pair (Li et al., 2020). Further combining unsupervised neural MT with phrase tables from statistical MT leads to top results (Lample et al., 2018b; Ren et al., 2019; Artetxe et al., 2019). However, unsupervised systems fail to learn when trained on small amounts of monolin"
2021.mtsummit-research.7,D16-1163,0,0.0451399,"iques for SSNMT in combination with finetuning. 2 Related Work Substantial effort has been devoted to muster training data for low-resource NMT, e.g. by identifying parallel sentences in monolingual or noisy corpora in a pre-processing step (Artetxe and Schwenk, 2019a; Chaudhary et al., 2019; Schwenk et al., 2021) and also by leveraging monolingual data into supervised NMT e.g. by including autoencoding (Currey et al., 2017) or language modeling tasks (Gulcehre et al., 2015; Ramachandran et al., 2017). Low-resource NMT models can benefit from high-resource languages through transfer learning (Zoph et al., 2016), e.g. in a zero-shot setting (Johnson et al., 2017), by using pre-trained language models (Conneau and Lample, 2019; Kuwanto et al., 2021), or finding an optimal path for pivoting through related languages (Leng et al., 2019). Back-translation often works well in high-resource settings (Bojar and Tamchyna, 2011; Sennrich et al., 2016a; Karakanta et al., 2018). NMT training and back-translation have been used in an incremental fashion in both unidirectional (Hoang et al., 2018) and bidirectional systems (Zhang et al., 2018; Niu et al., 2018). Unsupervised NMT (Lample et al., 2018a; Artetxe et"
D19-6501,W05-0909,0,0.376585,"Missing"
D19-6501,N18-1118,0,0.0401054,"so related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embeddings to enforce consistency within documents. Before these works, several methods to post-process the translations and even including a second decoding pass were used (Carpuat, 2009; Xiao et al., 2011; Ture et al., 201"
D19-6501,W09-2404,0,0.0383165,", 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embeddings to enforce consistency within documents. Before these works, several methods to post-process the translations and even including a second decoding pass were used (Carpuat, 2009; Xiao et al., 2011; Ture et al., 2012; Mart´ınez et al., 2014). Recent NMT systems that include context deal with both phenomena, coreference and coherence, but usually context is limited to the previous senSystems, Methods and Resources 3.1 State-of-the-art NMT Our NMT systems are based on a transformer architecture (Vaswani et al., 2017) as implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018) using the transformer big configuration. We train three systems (S1, S2 and S3) with the corpora summarised in Table 1.1 The first two systems are transformer models trained on different am"
D19-6501,D16-1245,0,0.0724909,"Missing"
D19-6501,W19-5321,0,0.0470238,"Missing"
D19-6501,W19-5315,1,0.857744,"Missing"
D19-6501,P18-4020,0,0.0460975,"Missing"
D19-6501,W17-4809,0,0.0532025,"Missing"
D19-6501,W15-3403,0,0.157926,"only on a lexical form, but also on other linguistic means, e.g. articles or modifying pronouns (Kibrik, 2011). The use of these is influenced by various factors which can be language-dependent (range of linguistic means available in grammar) and also contextindependent (pragmatic situation, genre). Thus, the means of expressing reference differ across languages and genres. This has been shown by some studies in the area of contrastive linguistics (Kunz et al., 2017; Kunz and LapshinovaKoltunski, 2015; Kunz and Steiner, 2012). Analyses in cross-lingual coreference resolution (Grishina, 2017; Grishina and Stede, 2015; Nov´ak and ˇ Zabokrtsk´ y, 2014; Green et al., 2011) show that there are still unsolved problems that should be addressed. them in the three translation variants. We also evaluate them from the point of view of coreference chain translation. The goal of this paper is two-fold. On the one hand, we are interested in various properties of coreference chains in these translations. They include total number of chains, average chain length, the size of the longest chain and the total number of annotated mentions. These features are compared to those of the underlying source texts and also the corr"
D19-6501,W18-6435,1,0.923417,"cted to individual phenomena within coreference. For instance, Zinsmeister et al. (2012) analyse abstract anaphors in English-German translations. To our knowledge, they do not consider chains. Lapshinova-Koltunski and Hardmeier (2017b) in their contrastive analysis of potential coreference chain members in English-German translations, describe transformation patterns that contain different types of referring expressions. However, the authors rely on automatic tagging and parsing procedures and do not include chains into their analysis. The data used by Nov´ak and Nedoluzhko (2015) and Nov´ak (2018) contain manual chain annotations. The authors focus on different categories of anaphoric pronouns in English-Czech translations, though not paying attention to chain features (e.g. their number or size). Chain features are considered in a contrastive analysis by Kunz et al. (2017). Their study concerns different phenomena in a variety of genres in English and German comparable texts. Using contrastive interpretations, they suggest preferred translation strategies from English into German, i.e. translators should use demonstrative proBackground and Related Work Coreference Coreference is relat"
D19-6501,W17-4810,1,0.848958,"slation studies Differences between languages and genres in the linguistic means expressing reference are important for translation, as the choice of an appropriate referring expression in the target language poses challenges for both human and machine translation. In translation studies, there is a number of corpus-based works analysing these differences in translation. However, most of them are restricted to individual phenomena within coreference. For instance, Zinsmeister et al. (2012) analyse abstract anaphors in English-German translations. To our knowledge, they do not consider chains. Lapshinova-Koltunski and Hardmeier (2017b) in their contrastive analysis of potential coreference chain members in English-German translations, describe transformation patterns that contain different types of referring expressions. However, the authors rely on automatic tagging and parsing procedures and do not include chains into their analysis. The data used by Nov´ak and Nedoluzhko (2015) and Nov´ak (2018) contain manual chain annotations. The authors focus on different categories of anaphoric pronouns in English-Czech translations, though not paying attention to chain features (e.g. their number or size). Chain features are cons"
D19-6501,W17-1505,0,0.0189275,"ence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embedd"
D19-6501,L18-1065,1,0.922243,"ons in a chain. For every mention, CoreNLP extracts its gender (male, female, neutral, unknown), number (singular, plural, unknown), and animacy (animate, inanimate, unknown). This information is not added directly but used to enrich the single sentence-based MT training data by applying a set of heuristics implemented in DocTrans4 : 3.2 Test data under analysis As one of our aims is to compare coreference chain properties in automatic translation with those of the source texts and human reference, we derive data from ParCorFull, an EnglishGerman corpus annotated with full coreference chains (Lapshinova-Koltunski et al., 2018).7 The corpus contains ca. 160.7 thousand tokens manually annotated with about 14.9 thousand mentions and 4.7 thousand coreference chains. For our analysis, we select a portion of English news texts and TED talks from ParCorFull and translate them with the three NMT systems described in 3.1 above. As texts considerably differ in their length, we select 17 news texts (494 sentences) and four TED talks (518 sentences). The size (in tokens) of the total data set under analysis – source (src) and human translations (ref) from ParCorFull and the automatic translations produced within this study (S1"
D19-6501,W19-2805,1,0.863219,"Missing"
D19-6501,C14-1003,0,0.0917501,"Missing"
D19-6501,P02-1040,0,0.10349,"Missing"
D19-6501,P14-5010,0,0.00262815,"described in the following section are preprocessed in the same way. S2 uses the same data as S1 with the addition of a filtered portion of Paracrawl. This corpus is known to be noisy, so we use it to create a larger training corpus but it is diluted by a factor 4 to give more importance to high quality translations. S3 S3 uses the same data as S1, but this time enriched with the cross- and intra-sentential coreference chain markup as described below.2 The information is included as follows. Source documents are annotated with coreference chains using the neural annotator of Stanford CoreNLP (Manning et al., 2014)3 . The tool detects pronouns, nominal phrases and proper names as mentions in a chain. For every mention, CoreNLP extracts its gender (male, female, neutral, unknown), number (singular, plural, unknown), and animacy (animate, inanimate, unknown). This information is not added directly but used to enrich the single sentence-based MT training data by applying a set of heuristics implemented in DocTrans4 : 3.2 Test data under analysis As one of our aims is to compare coreference chain properties in automatic translation with those of the source texts and human reference, we derive data from ParC"
D19-6501,W18-6306,0,0.0218498,"ystems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a"
D19-6501,W19-6614,0,0.0116979,"d columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more"
D19-6501,P18-1118,0,0.0199575,"apid 1,105,651 ParaCrawl Filtered 12,424,790 S1, S3 S2 x1 x4 x1 x4 x4 x16 x1 x4 x0 x1 Table 1: Number of lines of the corpora used for training the NMT systems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test set"
D19-6501,W17-4811,0,0.0182399,"to find shining through and explicitation effects in automatic translations. 2.3 # lines Common Crawl 2,394,878 Europarl 1,775,445 News Commentary 328,059 Rapid 1,105,651 ParaCrawl Filtered 12,424,790 S1, S3 S2 x1 x4 x1 x4 x4 x16 x1 x4 x0 x1 Table 1: Number of lines of the corpora used for training the NMT systems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance,"
D19-6501,Q18-1029,0,0.0192923,"1,775,445 News Commentary 328,059 Rapid 1,105,651 ParaCrawl Filtered 12,424,790 S1, S3 S2 x1 x4 x1 x4 x4 x16 x1 x4 x0 x1 Table 1: Number of lines of the corpora used for training the NMT systems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the cr"
D19-6501,D18-1325,0,0.0153457,"x1 x4 x4 x16 x1 x4 x0 x1 Table 1: Number of lines of the corpora used for training the NMT systems under study. The 2nd and 3rd columns show the amount of oversampling used. tence, so chains as a whole are never considered. Voita et al. (2018) encode both a source and a context sentence and then combine them to obtain a context-aware input. The same idea was implemented before by Tiedemann and Scherrer (2017) where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et"
D19-6501,N12-1046,0,0.0163913,"en et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embeddings to enforce consistency within documents. Before these works, several methods to post-process the translations and even including a second decoding pass were used (Carpuat, 2009; Xiao et al., 2011; Ture et al., 2012; Mart´ınez et al., 2014). Recent NMT systems that include context deal with both phenomena, coreference and coherence, but usually context is limited to the previous senSystems, Methods and Resources 3.1 State-of-the-art NMT Our NMT systems are based on a transformer architecture (Vaswani et al., 2017) as implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018) using the transformer big configuration. We train three systems (S1, S2 and S3) with the corpora summarised in Table 1.1 The first two systems are transformer models trained on different amounts of data (6M vs. 18M parallel sen"
D19-6501,W12-2503,0,0.0332319,"where they concatenate a source sentence with the previous one to include context. Caches (Tu et al., 2018), memory networks (Maruf and Haffari, 2018) and hierarchical attention methods (Miculicich et al., 2018) allow to use a wider context. Finally, our work is also related to Stojanovski and Fraser (2018) and Stojanovski and Fraser (2019) where their oracle translations are similar to the data-based approach we introduce in Section 3.1. Coreference in MT 3 As explained in the introduction, several recent works tackle the automatic translation of pronouns and also coreference (for instance, Voigt and Jurafsky, 2012; Miculicich Werlen and Popescu-Belis, 2017) and this has, in part, motivated the creation of devoted shared tasks and test sets to evaluate the quality of pronoun translation (Guillou et al., 2016; Webber et al., 2017; Guillou et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translati"
D19-6501,P18-1117,0,0.0821721,"ic coreference translation have shown that dedicated systems can lead to improvements in pronoun translation (Guillou et al., 2016; Lo´aiciga et al., 2017). However, standard NMT systems work at sentence level, so improvements in NMT translate into improvements on pronouns with intra-sentential antecedents, but the phenomenon of coreference is not limited to anaphoric pronouns, and even less to a subset of them. Document-level machine translation (MT) systems are needed to deal with coreference as a whole. Although some attempts to include extrasentential information exist (Wang et al., 2017; Voita et al., 2018; Jean and Cho, 2019; JunczysDowmunt, 2019), the problem is far from being solved. Besides that, some further problems of NMT that do not seem to be related to coreference at first glance (such as translation of unknown words and proper names or the hallucination of additional words) cause coreference-related errors. In our work, we focus on the analysis of complete coreference chains, manually annotating We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sentential anaphoric info"
D19-6501,D17-1301,0,0.0231774,"studies in automatic coreference translation have shown that dedicated systems can lead to improvements in pronoun translation (Guillou et al., 2016; Lo´aiciga et al., 2017). However, standard NMT systems work at sentence level, so improvements in NMT translate into improvements on pronouns with intra-sentential antecedents, but the phenomenon of coreference is not limited to anaphoric pronouns, and even less to a subset of them. Document-level machine translation (MT) systems are needed to deal with coreference as a whole. Although some attempts to include extrasentential information exist (Wang et al., 2017; Voita et al., 2018; Jean and Cho, 2019; JunczysDowmunt, 2019), the problem is far from being solved. Besides that, some further problems of NMT that do not seem to be related to coreference at first glance (such as translation of unknown words and proper names or the hallucination of additional words) cause coreference-related errors. In our work, we focus on the analysis of complete coreference chains, manually annotating We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sente"
D19-6501,2011.mtsummit-papers.13,0,0.0448914,"et al., 2018; Bawden et al., 2018). But coreference is a wider phenomenon that affects more linguistic elements. Noun phrases also appear in coreference chains but they are usually studied under coherence and consistency in MT. Xiong et al. (2015) use topic modelling to extract coherence chains in the source, predict them in the target and then promote them as translations. Mart´ınez et al. (2017) use word embeddings to enforce consistency within documents. Before these works, several methods to post-process the translations and even including a second decoding pass were used (Carpuat, 2009; Xiao et al., 2011; Ture et al., 2012; Mart´ınez et al., 2014). Recent NMT systems that include context deal with both phenomena, coreference and coherence, but usually context is limited to the previous senSystems, Methods and Resources 3.1 State-of-the-art NMT Our NMT systems are based on a transformer architecture (Vaswani et al., 2017) as implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018) using the transformer big configuration. We train three systems (S1, S2 and S3) with the corpora summarised in Table 1.1 The first two systems are transformer models trained on different amounts of data (6M v"
D19-6502,N18-1118,0,0.0399279,"l information within and beyond the sentence level. The interest in making NMT systems able to include wider context information in the translation process has increased in recent years (Jean et al., 2017; Popescu-Belis, 2019), and even in some cases the necessity for exploring new approaches of document-level machine translation has been argued (L¨aubli et al., 2018). On the one hand, several approaches tried to extend the context beyond the sentence information by modifying the system’s input. Tiedemann and Scherrer (2017) concatenate the previous source sentence to the current one, whereas Bawden et al. (2018) also concatenate the previous predicted target sentence. On the other hand, more sophisticated contextaware approaches propose to modify the NMT architecture. Jean et al. (2017) propose a variation of an attentional recurrent NMT system (Bahdanau et al., 2015) by including an additional encoder and attentional model to encode as context sentence the previous source sentence, showing how NMT systems can also benefit from larger contexts. Wang et al. (2017a) propose a crosssentence context-aware approach that integrates the historical contextual information within the NMT system. However, these"
D19-6502,W19-5315,1,0.882614,"Missing"
D19-6502,W15-3014,0,0.0224891,"O PUS2 (Tiedemann, 2012, 2009). We select the E UROPARL-v7, U NITED NATIONS, M ULTILIN GUAL U NITED NATIONS , and S UBTITLES -2012 corpora, which total 759 million words for Spanish. We use NEWS C OMMENTARY 2011 as test set. We take advantage of the document annotations from the NEWS C OMMENTARY corpus to translate the test set document by document to avoid addition of random noise. We evaluate the quality of the outputs with two automatic metrics, BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). have a high computational cost. Following the ranking/filtering approaches of Jean et al. (2015) and Wang et al. (2017b), we speed up this computation by filtering the words to score by the SSLM. In particular, pSSLM is only computed on the N target words with the highest probabilities from the NMT model, that is, only the N best candidates from the NMT model are considered by the SSLM. Figure 1 depicts how the filtering process works in combination with the shallow fusion of the NMT and the SSLM models during the beam search. Recall that although our system does not need any document-level annotation, it will understand any set of sentences in its input as a document, and thus we transl"
D19-6502,P02-1040,0,0.103924,"corpora. Furthermore, this technique can be easily applied to any NMT model, either RNNbased or purely attention-based neural models. In 16 O PUS2 (Tiedemann, 2012, 2009). We select the E UROPARL-v7, U NITED NATIONS, M ULTILIN GUAL U NITED NATIONS , and S UBTITLES -2012 corpora, which total 759 million words for Spanish. We use NEWS C OMMENTARY 2011 as test set. We take advantage of the document annotations from the NEWS C OMMENTARY corpus to translate the test set document by document to avoid addition of random noise. We evaluate the quality of the outputs with two automatic metrics, BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). have a high computational cost. Following the ranking/filtering approaches of Jean et al. (2015) and Wang et al. (2017b), we speed up this computation by filtering the words to score by the SSLM. In particular, pSSLM is only computed on the N target words with the highest probabilities from the NMT model, that is, only the N best candidates from the NMT model are considered by the SSLM. Figure 1 depicts how the filtering process works in combination with the shallow fusion of the NMT and the SSLM models during the beam search. Recall that although our sy"
D19-6502,W19-5321,0,0.123298,"es Creus Eva Mart´ınez Garcia Vicomtech TALP Research Center, Donostia Universitat Polit`ecnica de Catalunya ccreus@vicomtech.org Barcelona emartinez@cs.upc.edu Abstract document-level information, these systems usually propose modifications to the neural architecture (Wang et al., 2017a; Jean et al., 2017; Voita et al., 2018; Tu et al., 2018; Maruf and Haffari, 2018; Miculicich Werlen et al., 2018; Jean and Cho, 2019) making the training process slower, or require the training data to be annotated with document-level information, such as the document boundaries (Tiedemann and Scherrer, 2017; Junczys-Dowmunt, 2019; Talman et al., 2019; Espa˜na-Bonet et al., 2019). The main benefit of these approaches is that the neural translation models they obtain are better tuned and able to handle document-level information. However, the training data with the document-level annotations that they require is still scarce, and also, their design increments the training times since the complexity of their neural architectures increase the model parameters to learn. We propose an alternative to introducing intersentence information in an NMT system that follows the encoder–decoder architecture with attention of Bahdana"
D19-6502,W19-5337,0,0.0218559,"ecoder architecture. The importance of document-level translation is also seen in the recent WMT20191 news translation shared task, where for the first time a specific track for document-level MT was included. The systems presented at the shared task follow the previously explained strategies: introducing the inter-sentence context information into the NMT system by augmenting the training data including document-level information, i.e., including coreference information (Espa˜na-Bonet et al., 2019), or just by increasing the training-sequence length in order to capture a larger data context (Popel et al., 2019; Talman et al., 2019; Junczys-Dowmunt, 2019), or introducing variations in the NMT architecture to take into account document-level information (Stahlberg et al., 2019; Talman et al., 2019). Also related to our work, but far from machine translation, is the work by Wang and Cho (2016). They present an approach to include document-level context into language modeling by implementing fusion approaches that help the LSTM maintain separated the inter- and the intrasentence context dependencies. Their conclusions show how using a wider context helps neural language models. We borrow the idea of (s"
D19-6502,P17-4012,0,0.0380695,"help in producing better translations. O RACLE 3 mimics our fused decoding approach and its goal is to evaluate the potential gain of using an SSLM in combination with an NMT. In other words, with O RACLE 3 we check how much the SSLM can help the NMT disambiguate between its best translation candidates, thus obtaining an upper bound for the improvements that can be achieved by shallow fusing an SSLM and an NMT system. Experiments Settings Our baseline NMT model follows the encoder– decoder architecture with attention of Bahdanau et al. (2015) and it is built using the O PEN NMT- LUA toolkit (Klein et al., 2017). We use a 4-layered bidirectional RNN encoder and a 4-layered RNN-based decoder with 800dimensional hidden layers. Word embeddings are set to 500 dimensions for both source and target vocabularies. Stochastic gradient descent is used as optimizer algorithm for training, setting an initial learning rate of 1 and a learning decay of 0.7 after epoch 10 or if there is no loss improvement over the validation set. Training data is distributed on batches of 64 sentences and we use a 0.3 dropout probability between recurrent layers. Finally, a maximum sentence length of 50 tokens is used for both sou"
D19-6502,W04-3250,0,0.0401485,"ng the target gold standard in the input. 18 31 N 2 3 4 5 7 10 BLEU 30.8 30.6 N=2 N=3 N=4 N=5 N=7 baseline 30.4 30.2 0.05 0.1 0.15 λ 0.2 BLEU↑ 30.77 30.88 † 30.98 † 31.00 † 31.00 † 31.00 † 31.00 METEOR↑ 49.86 50.17 50.14 50.15 50.14 50.14 50.14 #unknown 5901 4632 4501 4475 4459 4463 4463 Table 2: BLEU and METEOR scores obtained with the fused systems with λ = 0.15, together with the amount of unknown words in their output, where the first row corresponds to the baseline. † marks systems that are significantly different to the baseline with a p-value of 0.05, according to bootstrap resampling (Koehn, 2004). 0.25 Figure 3: BLEU score of the fused system as a function of the weight λ, for several values of the parameter N . semantically-close words (e.g., by synonyms), and thus, each of the substitutions preserves the meaning of the replaced word even if in some occasions the computed alignment is not adequate. Conversely, by increasing M the oracle handles lists of candidates that are more semantically distant, and thus, in combination with the uncertainty of the alignments, the system introduces more errors. candidates close together, O RACLE 2 that incomplete attention information does not hin"
D19-6502,P16-1162,0,0.162761,"Missing"
D19-6502,D18-2012,0,0.0520951,"Missing"
D19-6502,W18-6321,0,0.0181308,"how Statistical Machine Translation (SMT) systems integrate the information from different feature functions that represent different probabilistic models. There are four main fusion techniques: deep, shallow, cold, and simple fusion. All of them extend the conditional probability learned by one model introducing the information from a second one, where the specific method that is used to combine both models is the main differentiator between the approaches. Deep, cold, and simple fusion are techniques that need to train the resulting fused network. Deep fusion (G¨ulc¸ehre et al., 2015, 2017; Stahlberg et al., 2018; Sriram et al., 2018) proposes a method to merge a translation model and a language model by introducing a gating mechanism that learns to balance the weight of the additional language model. Cold fusion (Sriram et al., 2018) goes a step beyond and proposes to implement a deep fusion where the NMT model is trained from scratch including the LM as a fixed part of the network. This allows the NMT to better model the conditioning on the source sequence while the target language modeling is covered by the LM. Simple fusion (Stahlberg et al., 2018) is the latest approach. It arises as an alternati"
D19-6502,D18-1512,0,0.10143,"Missing"
D19-6502,W19-5347,0,0.0278207,"Missing"
D19-6502,P18-1118,0,0.0551381,"ntence, showing how NMT systems can also benefit from larger contexts. Wang et al. (2017a) propose a crosssentence context-aware approach that integrates the historical contextual information within the NMT system. However, these approaches only extend the source context but ignore the target side context. In contrast, Tu et al. (2018) take into account the target side context by using a lightweight cache-like memory network which stores bilingual hidden representations as translation history. More recent approaches implement system extensions that handle both source and target side contexts. Maruf and Haffari (2018) use memory networks to capture global source and target document context. Also, Maruf et al. (2019) present an approach to selectively focus on relevant sentences in the document context and not only consider a few previous sentences as context. There are other approaches that study the effect of introducing context information within Transformer-based translation systems. Voita et al. (2018) present a variation of the Transformer (Vaswani et al., 2017) that extends the handled context by taking in the input both the current and previous sentences. Miculicich Werlen et al. (2018) extend it by"
D19-6502,N19-1313,0,0.0203217,"ssentence context-aware approach that integrates the historical contextual information within the NMT system. However, these approaches only extend the source context but ignore the target side context. In contrast, Tu et al. (2018) take into account the target side context by using a lightweight cache-like memory network which stores bilingual hidden representations as translation history. More recent approaches implement system extensions that handle both source and target side contexts. Maruf and Haffari (2018) use memory networks to capture global source and target document context. Also, Maruf et al. (2019) present an approach to selectively focus on relevant sentences in the document context and not only consider a few previous sentences as context. There are other approaches that study the effect of introducing context information within Transformer-based translation systems. Voita et al. (2018) present a variation of the Transformer (Vaswani et al., 2017) that extends the handled context by taking in the input both the current and previous sentences. Miculicich Werlen et al. (2018) extend it by integrating a hierarchical attention model to capture inter-sentence connections, Jean and Cho (201"
D19-6502,tiedemann-2012-parallel,0,0.0121083,"s its weight. The LM used by G¨ulc¸ehre et al. (2017) is an LSTM-based RNN language model, but could be any model that generates as output a probability distribution on the discrete space of the target vocabulary shared with the translation model. An advantage of shallow fusion over the other fusion techniques is that it only needs to adjust the weight λ for the language model by a grid-search on development data, avoiding a long training on large corpora. Furthermore, this technique can be easily applied to any NMT model, either RNNbased or purely attention-based neural models. In 16 O PUS2 (Tiedemann, 2012, 2009). We select the E UROPARL-v7, U NITED NATIONS, M ULTILIN GUAL U NITED NATIONS , and S UBTITLES -2012 corpora, which total 759 million words for Spanish. We use NEWS C OMMENTARY 2011 as test set. We take advantage of the document annotations from the NEWS C OMMENTARY corpus to translate the test set document by document to avoid addition of random noise. We evaluate the quality of the outputs with two automatic metrics, BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005). have a high computational cost. Following the ranking/filtering approaches of Jean et al. (2015) and W"
D19-6502,W17-4811,0,0.146306,"hine Translation Decoding Carles Creus Eva Mart´ınez Garcia Vicomtech TALP Research Center, Donostia Universitat Polit`ecnica de Catalunya ccreus@vicomtech.org Barcelona emartinez@cs.upc.edu Abstract document-level information, these systems usually propose modifications to the neural architecture (Wang et al., 2017a; Jean et al., 2017; Voita et al., 2018; Tu et al., 2018; Maruf and Haffari, 2018; Miculicich Werlen et al., 2018; Jean and Cho, 2019) making the training process slower, or require the training data to be annotated with document-level information, such as the document boundaries (Tiedemann and Scherrer, 2017; Junczys-Dowmunt, 2019; Talman et al., 2019; Espa˜na-Bonet et al., 2019). The main benefit of these approaches is that the neural translation models they obtain are better tuned and able to handle document-level information. However, the training data with the document-level annotations that they require is still scarce, and also, their design increments the training times since the complexity of their neural architectures increase the model parameters to learn. We propose an alternative to introducing intersentence information in an NMT system that follows the encoder–decoder architecture wi"
D19-6502,Q18-1029,0,0.0419708,"ted contextaware approaches propose to modify the NMT architecture. Jean et al. (2017) propose a variation of an attentional recurrent NMT system (Bahdanau et al., 2015) by including an additional encoder and attentional model to encode as context sentence the previous source sentence, showing how NMT systems can also benefit from larger contexts. Wang et al. (2017a) propose a crosssentence context-aware approach that integrates the historical contextual information within the NMT system. However, these approaches only extend the source context but ignore the target side context. In contrast, Tu et al. (2018) take into account the target side context by using a lightweight cache-like memory network which stores bilingual hidden representations as translation history. More recent approaches implement system extensions that handle both source and target side contexts. Maruf and Haffari (2018) use memory networks to capture global source and target document context. Also, Maruf et al. (2019) present an approach to selectively focus on relevant sentences in the document context and not only consider a few previous sentences as context. There are other approaches that study the effect of introducing co"
D19-6502,P18-1117,0,0.0299786,"t cache-like memory network which stores bilingual hidden representations as translation history. More recent approaches implement system extensions that handle both source and target side contexts. Maruf and Haffari (2018) use memory networks to capture global source and target document context. Also, Maruf et al. (2019) present an approach to selectively focus on relevant sentences in the document context and not only consider a few previous sentences as context. There are other approaches that study the effect of introducing context information within Transformer-based translation systems. Voita et al. (2018) present a variation of the Transformer (Vaswani et al., 2017) that extends the handled context by taking in the input both the current and previous sentences. Miculicich Werlen et al. (2018) extend it by integrating a hierarchical attention model to capture inter-sentence connections, Jean and Cho (2019) by including a context-aware regularization, and Zhang et al. 3 Context-Aware Decoding Our document-level extension of the NMT decoding process benefits from the shallow fusion technique. In particular, it exploits the flexibility of being able to combine a general NMT model with a more domai"
D19-6502,D17-1301,0,0.101442,"ation by modifying the system’s input. Tiedemann and Scherrer (2017) concatenate the previous source sentence to the current one, whereas Bawden et al. (2018) also concatenate the previous predicted target sentence. On the other hand, more sophisticated contextaware approaches propose to modify the NMT architecture. Jean et al. (2017) propose a variation of an attentional recurrent NMT system (Bahdanau et al., 2015) by including an additional encoder and attentional model to encode as context sentence the previous source sentence, showing how NMT systems can also benefit from larger contexts. Wang et al. (2017a) propose a crosssentence context-aware approach that integrates the historical contextual information within the NMT system. However, these approaches only extend the source context but ignore the target side context. In contrast, Tu et al. (2018) take into account the target side context by using a lightweight cache-like memory network which stores bilingual hidden representations as translation history. More recent approaches implement system extensions that handle both source and target side contexts. Maruf and Haffari (2018) use memory networks to capture global source and target documen"
D19-6502,P16-1125,0,0.0252244,"rategies: introducing the inter-sentence context information into the NMT system by augmenting the training data including document-level information, i.e., including coreference information (Espa˜na-Bonet et al., 2019), or just by increasing the training-sequence length in order to capture a larger data context (Popel et al., 2019; Talman et al., 2019; Junczys-Dowmunt, 2019), or introducing variations in the NMT architecture to take into account document-level information (Stahlberg et al., 2019; Talman et al., 2019). Also related to our work, but far from machine translation, is the work by Wang and Cho (2016). They present an approach to include document-level context into language modeling by implementing fusion approaches that help the LSTM maintain separated the inter- and the intrasentence context dependencies. Their conclusions show how using a wider context helps neural language models. We borrow the idea of (shallow) fusion and apply it to neural machine translation. In this line, Ji et al. (2015) presented new language models able to capture contextual information within and beyond the sentence level. The interest in making NMT systems able to include wider context information in the trans"
D19-6502,W17-4742,0,0.0267376,"Missing"
D19-6502,D18-1049,0,0.160536,"Missing"
L16-1469,W13-1109,0,0.615616,"Missing"
L16-1469,P13-1018,0,0.0652074,"ies bringing to light the potential and shortcomings of today’s MT techniques applied to tweets, a corpus was compiled in the framework of TweetMT, a workshop and shared task1 on MT applied to tweets. Our parallel corpus includes tweets for the following language pairs: Catalan–Spanish (ca-es), Basque–Spanish (eu-es), Galician–Spanish (gl-es), and Portuguese–Spanish (pt-es). Those are the most common pairings between official languages in the Iberian Peninsula. 2. Collecting parallel tweets To the best of our knowledge, there is no parallel tweet dataset available apart from that produced by (Ling et al., 2013), which differs from our purposes in that they worked on tweets that mix two languages, i.e., providing the translated text within the same tweet. They further improve the quality of the parallel segments by means of crowdsourced annotations (Ling et al., 2014). Since we wanted to work on the translation of entire tweets into new tweets, we generated a corpus for the specific purposes of the TweetMT Workshop. For corpus generation, we developed a semi-automatic method to retrieve and align parallel tweets. The first step of this method consists in identifying multiple Twitter authors that conc"
L16-1469,W14-3356,0,0.045776,"ge pairs: Catalan–Spanish (ca-es), Basque–Spanish (eu-es), Galician–Spanish (gl-es), and Portuguese–Spanish (pt-es). Those are the most common pairings between official languages in the Iberian Peninsula. 2. Collecting parallel tweets To the best of our knowledge, there is no parallel tweet dataset available apart from that produced by (Ling et al., 2013), which differs from our purposes in that they worked on tweets that mix two languages, i.e., providing the translated text within the same tweet. They further improve the quality of the parallel segments by means of crowdsourced annotations (Ling et al., 2014). Since we wanted to work on the translation of entire tweets into new tweets, we generated a corpus for the specific purposes of the TweetMT Workshop. For corpus generation, we developed a semi-automatic method to retrieve and align parallel tweets. The first step of this method consists in identifying multiple Twitter authors that concurrently tweet in multiple languages and crawl those accounts. The second step involves aligning the collected messages. The idea behind this methodology is that the languages involved are official in the Iberian Peninsula, being Catalan, Basque and Galician co"
L16-1469,2010.amta-workshop.1,0,0.0358573,"ing features which are exclusive to the platform, such as hashtags, user mentions, and retweets. These characteristics make the application of MT to tweets a new challenge that requires specific processing techniques to perform effectively. Despite the paucity of research in the specific task of translating tweets, an increasing interest can be observed in the scientific community (Gotti et al., 2013; Peisenieks and Skadin¸sˇ, 2014). Similarly, a related and highly relevant direction of research is the work on MT of SMS texts, such as Munro’s study in the context of the 2010 Haiti earthquake (Munro, 2010). Provided the dearth of benchmark resources and comparison studies bringing to light the potential and shortcomings of today’s MT techniques applied to tweets, a corpus was compiled in the framework of TweetMT, a workshop and shared task1 on MT applied to tweets. Our parallel corpus includes tweets for the following language pairs: Catalan–Spanish (ca-es), Basque–Spanish (eu-es), Galician–Spanish (gl-es), and Portuguese–Spanish (pt-es). Those are the most common pairings between official languages in the Iberian Peninsula. 2. Collecting parallel tweets To the best of our knowledge, there is n"
L16-1469,P02-1040,0,0.103466,"Missing"
P19-1178,P17-1042,0,0.0543905,"der– decoder with 8-head self-attention, 512-dim word embeddings and a 2048-dim hidden feed-forward. Adam optimisation with λ=2 and beta2=0.998; noam λ decay with 8000 warm-up steps. Labels are smoothed (=0.1) and a dropout mask (p=0.1) is applied. The five models described in the LSTM category have transformer counterparts which follow the same transformer base architecture. All systems are trained on a single GPU GTX TITAN using a batch size of 64 (LSTM) or 50 (transformer) sentences. 4 Results and Discussion In order to train the 10 NMT systems, we initialise the word embeddings following Artetxe et al. (2017) using a seed dictionary of 2.591 numerals automatically extracted from our Wikipedia editions, and feed the system directly with comparable articles. This avoids the n × m explosion of possible combinations of sentences, where n is the number of sentences Pin L1 and m in L2. In our approach, we input article ni × mj sentence pairs, that is, only all possible source–target sentence combinations within two articles linked by Wikipedia’s langlinks. Hence we miss the parallel sentences in non-linked articles but we win in speed. Articles are input in lots4 . For them, the appropriate representati"
P19-1178,D18-1399,0,0.149473,"-Bonet and Barr´on-Cede˜no (2017) for example. In a systematic study, Espa˜na-Bonet et al. (2017) show that cosine similarities between context vectors discriminate between parallel and non-parallel sentences already in the first stages of training. Other approaches perform max-pooling over encoder outputs (Schwenk, 2018; Artetxe and Schwenk, 2018) or calculate the mean of word embeddings (Bouamor and Sajjad, 2018) to extract pairs. On the other hand, unsupervised NMT is now achieving impressive results using large amounts of monolingual data and small parallel lexicons (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018). These systems rely on very strong language models and back-translation, and build complex architectures that combine denoising autoencoders, back-translation steps and shared encoders among languages. The most successful architectures also use SMT phrase tables, standalone or in combination with NMT (Lample et al., 2018b; Artetxe et al., 2018a). In our approach, we propose a new and simpler method without a priori parallel corpora. Our premise is that NMT systems —either sequence to sequence models with RNNs, transformers, or any architecture based on encoder–decoder mod"
P19-1178,J82-2005,0,0.736915,"Missing"
P19-1178,W15-3402,1,0.823597,"Missing"
P19-1178,D14-1179,0,0.103132,"Missing"
P19-1178,S17-2019,1,0.902774,"Missing"
P19-1178,Q17-1024,0,0.105086,"Missing"
P19-1178,P17-4012,0,0.0977833,"Missing"
P19-1178,P07-2045,0,0.00898842,"ments). We still use both representations and extend the 1829 number of candidates considered only for S=Ch , which is the most restrictive factor at the beginning of training. (iv) Low precision, high recall. Generalisation of the previous strategy where we make the method symmetric in source–target and Ch –Ce . 3 Experimental Setting Data. We use Wikipedia (WP) dumps1 in English (en) and French (f r), and pre-process the articles and split the text into sentences using the Wikitailor toolkit2 (Barr´on-Cede˜no et al., 2015). We further tokenise and truecase them using standard Moses scripts (Koehn et al., 2007) and apply a byte-pair encoding (Sennrich et al., 2016) of 100 k merge operations trained on the concatenation of English and French data. We also remove duplicates and discard sentences with more than 50 tokens for training the MT systems. We fix these settings as a comparison point for all the experiments even though smaller vocabularies and longer sentences might imply the extraction of more parallel sentences (see Section 4). We use newstest2012 for validation and newstest2014 for testing. WP dumps are used for two different purposes in our systems: (i) to calculate initial word embeddings"
P19-1178,P18-2037,0,0.370256,"cus on similarities estimated from NMT representations. The strength of NMT embeddings as semantic representations was first shown qualitatively in Sutskever et al. (2014); Ha et al. (2016) and Johnson et al. (2017), and used for estimating semantic similarities at sentence level in Espa˜na-Bonet and Barr´on-Cede˜no (2017) for example. In a systematic study, Espa˜na-Bonet et al. (2017) show that cosine similarities between context vectors discriminate between parallel and non-parallel sentences already in the first stages of training. Other approaches perform max-pooling over encoder outputs (Schwenk, 2018; Artetxe and Schwenk, 2018) or calculate the mean of word embeddings (Bouamor and Sajjad, 2018) to extract pairs. On the other hand, unsupervised NMT is now achieving impressive results using large amounts of monolingual data and small parallel lexicons (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018). These systems rely on very strong language models and back-translation, and build complex architectures that combine denoising autoencoders, back-translation steps and shared encoders among languages. The most successful architectures also use SMT phrase tables, standalone or in"
P19-1178,P16-1162,0,0.0994997,"the 1829 number of candidates considered only for S=Ch , which is the most restrictive factor at the beginning of training. (iv) Low precision, high recall. Generalisation of the previous strategy where we make the method symmetric in source–target and Ch –Ce . 3 Experimental Setting Data. We use Wikipedia (WP) dumps1 in English (en) and French (f r), and pre-process the articles and split the text into sentences using the Wikitailor toolkit2 (Barr´on-Cede˜no et al., 2015). We further tokenise and truecase them using standard Moses scripts (Koehn et al., 2007) and apply a byte-pair encoding (Sennrich et al., 2016) of 100 k merge operations trained on the concatenation of English and French data. We also remove duplicates and discard sentences with more than 50 tokens for training the MT systems. We fix these settings as a comparison point for all the experiments even though smaller vocabularies and longer sentences might imply the extraction of more parallel sentences (see Section 4). We use newstest2012 for validation and newstest2014 for testing. WP dumps are used for two different purposes in our systems: (i) to calculate initial word embeddings and (ii) as training corpus. In the first case, we use"
P19-1178,P18-1005,0,0.0923085,"˜no (2017) for example. In a systematic study, Espa˜na-Bonet et al. (2017) show that cosine similarities between context vectors discriminate between parallel and non-parallel sentences already in the first stages of training. Other approaches perform max-pooling over encoder outputs (Schwenk, 2018; Artetxe and Schwenk, 2018) or calculate the mean of word embeddings (Bouamor and Sajjad, 2018) to extract pairs. On the other hand, unsupervised NMT is now achieving impressive results using large amounts of monolingual data and small parallel lexicons (Lample et al., 2018a; Artetxe et al., 2018b; Yang et al., 2018). These systems rely on very strong language models and back-translation, and build complex architectures that combine denoising autoencoders, back-translation steps and shared encoders among languages. The most successful architectures also use SMT phrase tables, standalone or in combination with NMT (Lample et al., 2018b; Artetxe et al., 2018a). In our approach, we propose a new and simpler method without a priori parallel corpora. Our premise is that NMT systems —either sequence to sequence models with RNNs, transformers, or any architecture based on encoder–decoder models— already learn st"
S17-2019,S16-1081,0,0.0491886,"enough before the deadline and we could not include the results. 3 145 2017 Track 1 2 3 4 5 6 total L–L0 ar–ar ar–en es–es es–en en–en tr–en Instances 1, 081 2, 162 1, 555 1, 595 14, 778 0∗ 21, 171 Pctge. 5.11 10.21 7.34 7.53 69.80 0.00 100 3 For training, we used all the annotated datasets released both in the current and in previous editions.6 Table 1 shows the size of the different language collections. Note the important imbalance: there are more than ten times more instances available in en only than in the rest of languages. We used the test set from the 2016 edition (only in English) (Agirre et al., 2016) as our internal test set. Using the features in Sections 2.1 to 2.6, we train two regressors by: Sys1 learning one SVM per each language pair Sys2 learning one single SVM for all the language pairs together. We experiment with a third system using all the extensions of Section 2.7 on XGBoost. The purpose of this system is to analyse and compare different assumptions made for Sys1 and Sys2: Sys3 learning one single XGB for all the language pairs with an extended set of features. Table 1: Instances provided in the history of STS. (∗ No training data exists for this pair.) package (Mikolov et al"
S17-2019,Q17-1024,0,0.0640267,"Missing"
S17-2019,W15-3402,1,0.841459,"Missing"
S17-2019,S17-2001,0,0.0323469,"sed six binary features that mark the languages of each pair. lang1, lang2 and lang3 are set to 1 if s is written in either ar, en, or es, respectively. The other three features, lang4, lang5, and lang6, provide the same information for t. For instance, the value for the six features for a pair en–ar would be 0 1 0 1 0 0. Introduction The Semantic Textual Similarity (STS) task poses the following challenge. Let s and t be two text snippets. Determine the degree of equivalence α(s, t) |α ∈ [0, 5]. Whereas 0 represents complete independence, 5 reflects semantic equivalence. The current edition (Cer et al., 2017) includes the monolingual ar–ar, en–en, and es– es, as well as the cross-language ar–en, es– en, and tr–enlanguage pairs. We use the twoletter ISO 639-1 codes: ar=Arabic, en=English, es=Spanish, and tr=Turkish. Multilinguality is the premise of the Lump approach: we use representations which lie towards language-independence as we aim to be able to approach similar tasks on other languages, paying the least possible effort. Our regression model relies on different kinds of features, from simple length-based and lexical similarities to more sophisticated embeddings and deep neural net internal"
S17-2019,2009.mtsummit-posters.15,0,0.0273006,"l neural machine translation (NMT) system to obtain a representation in a common space for sentences in all the languages. We build the NMT system in the same philosophy of Johnson et al. (2016) using and adapting the Nematus engine (Sennrich et al., 2016). The multilingual system is able to translate between any combination of languages ar, en, and es. It was trained on 60 k parallel sentences (20 k per language pair) using 512-dimensional word embeddings, 1024 hidden units, a minibatch of 200 samples, and applying Adadelta optimisation. The parallel corpus includes data from United Nations (Rafalovitch and Dale, 2009), Common Crawl2 , News Commentary3 and IWSLT.4 We are not interested in the translations but in the context vectors output of the hidden layers of the encoder, as these are supposed to have learnt an interlingua representation of the input. We compute the cosine similarity between 2048dimensional context vectors from the internal representation when the encoder is fed with s and t. Two independent systems, one trained with words and another one trained with lemmas5 provide our two features lN M T and wN M T . Lexical Similarities (5 feats.) We compute cosine similarities between character n-gr"
S17-2019,1992.tmi-1.7,0,0.718314,"epresentations of s and t, with n = [2, 5] (2grm,. . .,5grm). The pre-processing in this case is casefolding and diacritics removal. The fifth feature cog is the cosine similarity computed over “pseudo-cognate” representations. From an NLP point of view, cognates are “words that are similar across languages” (Manning and Sch¨utze, 1999). We relax this concept and consider as pseudocognates any words in two languages that share prefixes. To do so, we discard tokens shorter than four characters, unless they contain nonalphabetical characters, and cut off the resulting tokens to four characters (Simard et al., 1992). This kind of representations is used on European languages with similar alphabets (McNamee and Mayfield, 2004; Simard et al., 1992). We apply Buckwalter transliteration to texts in ar and remove vowels from the snippets written in latin alphabets. For the pseudo-cognates computations, we use three characters instead of four. 2.4 Context Vectors in a Neural Machine Translation Engine (2 feats.) 2.6 Embeddings for Babel Synsets (2 feats.) BabelNet is a multilingual semantic network connecting concepts via Babel synsets (Navigli and Ponzetto, 2012). Each concept, or word, is identified by its I"
W12-0103,A00-1031,0,0.0906546,"n.html TRECs 2264 219 500 12116 219 2551 9,10,12,13,14,15,16 9,10,12 11 Tokens TrainL1 TrainL2 Experiments 5.1 A Table 1: Number of Questions and Answers in our data sets. The number of TREC evaluation from which are obtained is indicated. These three families of scores can be combined in several ways in order to produce a ranked list of answers. In Section 6 the combination methods are discussed. 5 Q Vocabulary Q A Q A 97028 91567 393978 373008 3232 540 32013 9130 Table 2: Statistics for the 12,116 Q-A pairs in the training corpus according to the annotation level. we use the TnT POS tagger (Brants, 2000), WordNet (Fellbaum, 1998), the YamCha chunker (Kudo and Matsumoto, 2003), the Stanford NERC (Finkel et al., 2005), and an in-house temporal expressions recogniser. Table 2 shows some statistics for the parallel corpus and the two different levels of annotation. From the SMT point of view the corpus is small in order to estimate the translation probabilities in a reliable way but, as stated before, Level2 representation diminishes the vocabulary considerably and alleviates the problem. 5.2 SMT system The statistical system is a state-of-the-art phrasebased SMT system trained on the previously"
W12-0103,J93-2003,0,0.0225473,"conclusions. 2 Translation Models in QA The use of machine translation in IR is not new. Berger and Lafferty (1999) firstly propose a probabilistic approach to IR based on methods of SMT. Under their perspective, the human user has an information need that is satisfied by an “ideal” theoretical document d from which the user draws important query words q. This process can be mirrored by a translation model: given the query q, they find the documents in the collection with words a most likely to translate to q. The key ingredient is the set of translation probabilities p(q|a) from IBM model 1 (Brown et al., 1993). In a posterior work, Berger et al. also introduce the formulation of the QA problem in terms of SMT (Berger et al., 2000). They estimate the likelihood that a given answer containing a word ai corresponds to a question containing word qj . This estimation relies on an IBM model 1. The method is tested with a collection of closeddomain Usenet and call-center questions, where each question must be paired with one of the recorded answers. Soricut and Brill (2004) implement a similar strategy but with a richer formulation and targeted to open-domain QA. Given a question Q, a web-search engine is"
W12-0103,N10-1080,0,0.0242295,"Missing"
W12-0103,P03-1003,0,0.185289,"text of answer searching in FAQ collections. The works we have described so far use archives of question-answer pairs as information sources. They are really doing document retrieval and sentence retrieval rather than question answering, because every document/sentence is known to be the answer of a question written in the form of an answer, and no further information extraction is necessary, they just select the best answer from a given pool of answers. The difference with a standard IR task is that these systems are not searching for relevant documents but for answer documents. In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. First, they use a standard IR engine to retrieve candidate sentences and process them with a constituent parser. Then, an elaborated process simplifies these parse trees converting them into sequences of relevant words and/or syntactic tags. This process reduces the length disparity between questions and answers. For the answer extraction, a special tag marking the position of the answer is sequentially added to all suitable positions in the sentence, thus yielding several candidate answers for each sentence. Fina"
W12-0103,P05-1045,0,0.00400226,"5.1 A Table 1: Number of Questions and Answers in our data sets. The number of TREC evaluation from which are obtained is indicated. These three families of scores can be combined in several ways in order to produce a ranked list of answers. In Section 6 the combination methods are discussed. 5 Q Vocabulary Q A Q A 97028 91567 393978 373008 3232 540 32013 9130 Table 2: Statistics for the 12,116 Q-A pairs in the training corpus according to the annotation level. we use the TnT POS tagger (Brants, 2000), WordNet (Fellbaum, 1998), the YamCha chunker (Kudo and Matsumoto, 2003), the Stanford NERC (Finkel et al., 2005), and an in-house temporal expressions recogniser. Table 2 shows some statistics for the parallel corpus and the two different levels of annotation. From the SMT point of view the corpus is small in order to estimate the translation probabilities in a reliable way but, as stated before, Level2 representation diminishes the vocabulary considerably and alleviates the problem. 5.2 SMT system The statistical system is a state-of-the-art phrasebased SMT system trained on the previously introduced corpus. Its development has been done using standard freely available software. The language model is e"
W12-0103,P07-2045,0,0.00741586,"estimate the translation probabilities in a reliable way but, as stated before, Level2 representation diminishes the vocabulary considerably and alleviates the problem. 5.2 SMT system The statistical system is a state-of-the-art phrasebased SMT system trained on the previously introduced corpus. Its development has been done using standard freely available software. The language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The model weights are optimised with Moses’ script of MERT against the BLEU evaluation metric. For the full model, we consider the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. 5.3 QA system The question answering system has three different modules as explained in Section 4. For the 25 T1 T50 MRR QA SR 0.006 (4) 0.066 (8) 0.206 (14) 0.538 (9) 0.024 (4) 0.142 (8) Upper bound 0.677 0.677 0.677 Table 3: Mean and standard deviation for 1000 realisations of the random baseline for QA"
W12-0103,P03-1004,0,0.00960184,"9,10,12 11 Tokens TrainL1 TrainL2 Experiments 5.1 A Table 1: Number of Questions and Answers in our data sets. The number of TREC evaluation from which are obtained is indicated. These three families of scores can be combined in several ways in order to produce a ranked list of answers. In Section 6 the combination methods are discussed. 5 Q Vocabulary Q A Q A 97028 91567 393978 373008 3232 540 32013 9130 Table 2: Statistics for the 12,116 Q-A pairs in the training corpus according to the annotation level. we use the TnT POS tagger (Brants, 2000), WordNet (Fellbaum, 1998), the YamCha chunker (Kudo and Matsumoto, 2003), the Stanford NERC (Finkel et al., 2005), and an in-house temporal expressions recogniser. Table 2 shows some statistics for the parallel corpus and the two different levels of annotation. From the SMT point of view the corpus is small in order to estimate the translation probabilities in a reliable way but, as stated before, Level2 representation diminishes the vocabulary considerably and alleviates the problem. 5.2 SMT system The statistical system is a state-of-the-art phrasebased SMT system trained on the previously introduced corpus. Its development has been done using standard freely av"
W12-0103,D08-1043,0,0.0197953,"he system must output a single sentence containing the answer to a factoid question. Murdock and Croft tackle the length disparity in question-answer pairs and show that this MT-based approach outperforms traditional query likelihood techniques. Riezler et al. (2007) define the problem of answer retrieval from FAQ and social Q/A websites as a query expansion problem. SMT is used to translate the original query terms to the language of the answers, thus obtaining an expanded list of terms usable in standard IR techniques. They also use SMT to perform question paraphrasing. In the same context, Lee et al. (2008) study methods for improving the translation quality removing noise from the parallel corpus. SMT can be also applied to sentence representations different than words. Cui et al. (2005) approach the task of passage retrieval for QA with translations of dependency parsing relations. They extract the sequences of relations that link each pair of words in the question and, using the IBM translation model 1, score their similarity to the relations extracted from the candidate passage. Thus, an approximate relation matching score is obtained. Surdeanu et al. (2011) extend the scope of this approach"
W12-0103,P04-1077,0,0.0437142,"are also transformed to the Level2 representation. Then, each candidate answer is replaced by the special ANSWER tag in the associated sentence, thus, each sentence has a unique ANSWER tag, as in the training examples. Finally, each candidate is evaluated assessing the similarity of the source sentence with the n-best translations. For this assessment we use two different metrics. One of them is a lexical metric commonly used in machine translation, BLEU (Papineni et al., 2002). A smoothed version is used to evaluate the pairs at sentence level yielding the score B. The other metric is ROUGE (Lin and Och, 2004), here named R. We use the skip-bigram overlapping measure with a maximum skip distance of 4 unigrams (ROUGE-S4). Contrary to BLEU, ROUGE-S does not require consecutive matches but is still sensitive to word order. Both BLEU and ROUGE are well-known metrics that are useful for finding partial matchings in long strings of words. Therefore it is an easy way of implementing an approximated pattern matching algorithm with off-the-shelf components. Although these scores can determine if a sentence is a candidate for asserting a certain property of a certain object, they do not have the power to dis"
W12-0103,P02-1038,0,0.0973426,"ation model and P (A) is the language model, and each of them can be understood as the sum of the probabilities for each of the segments or phrases that conform the sentence. The translation model quantifies the appropriateness of each segment of Q being answered by A; the language model is a measure of the fluency of the answer sentence and does not take into account which is the question. Since we are interested in identifying the concrete string that answers the question and not a full sentence, this probability is not as important as it is in the translation problem. The log-linear model (Och and Ney, 2002), a generalisation of the original noisy-channel approach (Eq. 1), estimates the final probability as the logarithmic sum of several terms that depend on both the question Q and the answer sentence A. Using just two of the features, the model reproduces the noisy-channel approach but written in this way one can include as many features as desired at the cost of introducing the same number of free parameters. The model in its traditional form includes 8 terms: A(Q) = Aˆ = argmaxA log P (A|Q) = + λlm log P (A) + λd log Pd (A, Q) + λlg log lex(Q|A) + λld log lex(A|Q) + λg log Pt (Q|A) + λd log Pt"
W12-0103,J03-1002,0,0.00327763,"o different levels of annotation. From the SMT point of view the corpus is small in order to estimate the translation probabilities in a reliable way but, as stated before, Level2 representation diminishes the vocabulary considerably and alleviates the problem. 5.2 SMT system The statistical system is a state-of-the-art phrasebased SMT system trained on the previously introduced corpus. Its development has been done using standard freely available software. The language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The model weights are optimised with Moses’ script of MERT against the BLEU evaluation metric. For the full model, we consider the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. 5.3 QA system The question answering system has three different modules as explained in Section 4. For the 25 T1 T50 MRR QA SR 0.006 (4) 0.066 (8) 0.206 (14) 0.538 (9) 0.024 (4) 0.142 (8) Upper bound 0.677 0.677 0."
W12-0103,P03-1021,0,0.0148374,"ies respectively, Pt (Q|A) the generative translation model, Pt (A|Q) the discriminative one, Pd (A, Q) the distortion model, and ph(A) and w(A) correspond to the phrase and word penalty models. We start by using this form for the answer probability and analyse the importance and validity of the terms in the experiments Section. The λ weights, which account for the relative importance of each feature in the log-linear probabilistic model, are commonly estimated by optimising the translation performance on a development set. For this optimisation one may use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference evaluation. Once the weights are determined and the probabilities estimated from a corpus of questionanswer pairs (a parallel corpus in this task), a decoder uses Eq. 2 to score the possible outputs and to find the best answer sentence given a question or, in general, an n-best list of answers. This formulation, although possible from an abstract point of view, is not feasible in practice. The corpus from which probabilities are estimated is finite, and therefore new questions may not be represented. There is no chance that SMT can generate"
W12-0103,P02-1040,0,0.0863815,"|A) the generative translation model, Pt (A|Q) the discriminative one, Pd (A, Q) the distortion model, and ph(A) and w(A) correspond to the phrase and word penalty models. We start by using this form for the answer probability and analyse the importance and validity of the terms in the experiments Section. The λ weights, which account for the relative importance of each feature in the log-linear probabilistic model, are commonly estimated by optimising the translation performance on a development set. For this optimisation one may use Minimum Error Rate Training (MERT) (Och, 2003) where BLEU (Papineni et al., 2002) is the reference evaluation. Once the weights are determined and the probabilities estimated from a corpus of questionanswer pairs (a parallel corpus in this task), a decoder uses Eq. 2 to score the possible outputs and to find the best answer sentence given a question or, in general, an n-best list of answers. This formulation, although possible from an abstract point of view, is not feasible in practice. The corpus from which probabilities are estimated is finite, and therefore new questions may not be represented. There is no chance that SMT can generate ex nihilo the knowledge necessary t"
W12-0103,P02-1006,0,0.0191822,"nd the candidate answer A. In this question, two types of constraints are expressed over the candidate answers. One is that the expected type of A is a kind of “television show.” The rest of the question indicates that “Karl Malden” is related to A as being “starred” by, and that “San Francisco” is a substring of A. Many factoid questions explicitly express an hyponymy relation about the answer type, and also several other relations describing its context (i.e. spatial, temporal, etc.). The QA problem can be approached from several points of view, ranging from simple surface pattern matching (Ravichandran and Hovy, 2002), to automated reasoning (Moldovan et al., 2007) or supercomputing (Ferrucci et al., 2010). In this work, we propose to use Statistical Machine Translation (SMT) for the task of factoid QA. Under this perspective, the answer is a translation of the question. It is not the first time that SMT is used for QA tasks, several works have been using translation models to determine the answers (Berger et al., 2000; Cui et al., 2005; Surdeanu et al., 2011). But to our knowledge this is the first 20 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguisti"
W12-0103,P07-1059,0,0.0810617,"used to retrieve 3-sentence-long answer texts from FAQ pages. These texts are later ranked with the likelihood of containing the answer to Q, and this likelihood is estimated via a noisy-channel architecture. The work of Murdock and Croft (2005) applies the same strategy to TREC data. They evaluate the TREC 2003 passage retrieval task. In this task, the system must output a single sentence containing the answer to a factoid question. Murdock and Croft tackle the length disparity in question-answer pairs and show that this MT-based approach outperforms traditional query likelihood techniques. Riezler et al. (2007) define the problem of answer retrieval from FAQ and social Q/A websites as a query expansion problem. SMT is used to translate the original query terms to the language of the answers, thus obtaining an expanded list of terms usable in standard IR techniques. They also use SMT to perform question paraphrasing. In the same context, Lee et al. (2008) study methods for improving the translation quality removing noise from the parallel corpus. SMT can be also applied to sentence representations different than words. Cui et al. (2005) approach the task of passage retrieval for QA with translations"
W12-0103,N04-1008,0,0.0220112,"collection with words a most likely to translate to q. The key ingredient is the set of translation probabilities p(q|a) from IBM model 1 (Brown et al., 1993). In a posterior work, Berger et al. also introduce the formulation of the QA problem in terms of SMT (Berger et al., 2000). They estimate the likelihood that a given answer containing a word ai corresponds to a question containing word qj . This estimation relies on an IBM model 1. The method is tested with a collection of closeddomain Usenet and call-center questions, where each question must be paired with one of the recorded answers. Soricut and Brill (2004) implement a similar strategy but with a richer formulation and targeted to open-domain QA. Given a question Q, a web-search engine is used to retrieve 3-sentence-long answer texts from FAQ pages. These texts are later ranked with the likelihood of containing the answer to Q, and this likelihood is estimated via a noisy-channel architecture. The work of Murdock and Croft (2005) applies the same strategy to TREC data. They evaluate the TREC 2003 passage retrieval task. In this task, the system must output a single sentence containing the answer to a factoid question. Murdock and Croft tackle th"
W12-0103,J11-2003,0,0.0624276,"xt (i.e. spatial, temporal, etc.). The QA problem can be approached from several points of view, ranging from simple surface pattern matching (Ravichandran and Hovy, 2002), to automated reasoning (Moldovan et al., 2007) or supercomputing (Ferrucci et al., 2010). In this work, we propose to use Statistical Machine Translation (SMT) for the task of factoid QA. Under this perspective, the answer is a translation of the question. It is not the first time that SMT is used for QA tasks, several works have been using translation models to determine the answers (Berger et al., 2000; Cui et al., 2005; Surdeanu et al., 2011). But to our knowledge this is the first 20 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics approach that uses a full Machine Translation system for generating answers. The paper is organised as follows: Section 2 reviews the previous usages of SMT in QA, Section 3 reports our theoretical approach to the task, Section 4 describes our QA system, Section 5 presents the experimental setting, Section 6 analyses the results and Section 7 dra"
W14-4015,P13-4033,1,0.886979,"on of this architecture with the aim to improve consistency and coherence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between wo"
W14-4015,P07-2045,0,0.00478793,"arding their accuracy when trying to predict related words (Section 3.1) and also regarding its possible effect within a translation system (Section 3.2). In both cases one observes that the quality of the translation and alignments previous to building the semantic models are bottlenecks for the final performance: part of the vocabulary, and therefore translation pairs, are lost in the training process. Future work includes studying different kinds of alignment heuristics. We plan to develop new features based on the semantic models to use them inside state-of-the-art SMT systems like Moses (Koehn et al., 2007) or discourse-oriented decoders like Docent (Hardmeier et al., 2013). 3.2 Cross-Lingual Lexical Substitution Another way to evaluate the semantic models is through the effect they have in translation. We implemented the Cross-Lingual Lexical Substitution task carried out in SemEval-2010 (Task2, 2010) 133 References Z. Gong, M. Zhang, and G. Zhou. 2011. Cache-based document-level statistical machine translation. In Proc. of the 2011 Conference on Empirical Methods in NLP, pages 909–919, UK. C. Hardmeier, J. Nivre, and J. Tiedemann. 2012. Document-wide decoding for phrase-based statistical machi"
W14-4015,W09-2412,0,0.0379861,"Missing"
W14-4015,tiedemann-2012-parallel,1,0.865268,"Missing"
W14-4015,2011.mtsummit-papers.13,0,0.179084,"erence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across languages. However, they are not able to ha"
W14-4015,D11-1084,0,\N,Missing
W14-4015,D12-1108,1,\N,Missing
W15-3402,W06-2810,0,0.207288,"Missing"
W15-3402,cui-etal-2008-corpus,0,0.201057,"rpus using IR techniques. They use the characteristic vocabulary of the domain (100 terms extracted from an external in-domain corpus) to query a Lucene search engine4 over the whole encyclopædia. Our approach is completely different: we try to get along with Wikipedia’s structure with a strategy to walk through the category graph departing from a root or pseudo-root category, which defines our domain of interest. We empirically set a threshold to stop exploring the graph such that the included categories most likely represent an entire domain (cf. Section 3). This approach is more similar to Cui et al. (2008), who explore the Wiki-Graph and score every category in order to assess its likelihood of belonging to the domain. Other tools are being developed to extract corpora from Wikipedia. Linguatools5 released a comparable corpus extracted from Wikipedias in 253 language pairs. Unfortunately, neither their tool nor the applied methodology description are available. CatScan26 is a tool that allows to explore and search categories recursively. The Accurat toolkit (Pinnis et al., 2012; S¸tef˘anescu, Dan and Ion, Radu and Hunsicker, Sabine, 2012)7 aligns comparable documents and extracts parallel sente"
W15-3402,W04-3208,0,0.0142898,"for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 3–13, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics 2 Background Sport Comparability in multilingual corpora is a fuzzy concept that has received alternative definitions without reaching an overall consensus (Rapp, 1995; Eagles Document Eag–Tcwg–Ctyp, 1996; Fung, 1998; Fung and Cheung, 2004; Wu and Fung, 2005; McEnery and Xiao, 2007; Sharoff et al., 2013). Ideally, a comparable corpus should contain texts in multiple languages which are similar in terms of form and content. Regarding content, they should observe similar structure, function, and a long list of characteristics: register, field, tenor, mode, time, and dialect (Maia, 2003). Nevertheless, finding these characteristics in real-life data collections is virtually impossible. Therefore, we attach to the following simpler four-class classification (Skadin¸a et al., 2010): (i) Parallel texts are true and accurate translati"
W15-3402,W95-0114,0,0.0936815,"tatistical machine translation engines for specific domains. Our experiments on the English– Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some othe"
W15-3402,P02-1040,0,0.0936058,"rallel corpora used to train the SMT systems (top rows) and of the sets used for development and test. CS Sc Sp Un Comp. Europarl 27.99 34.00 30.02 30.63 – c3g cog monoen ¯ S·len union 38.81 57.32 54.27 56.14 64.65 40.53 56.17 52.96 57.40 62.95 46.94 57.60 55.74 58.39 62.65 43.68 58.14 55.17 58.80 64.47 43.68 54.89 52.45 56.78 – Table 8: BLEU scores obtained on the Wikipedia test sets for the 20 specialised systems described in Section 5. A comparison column (Comp.) where all the systems are trained with corpora of the same size is also included (see text). against the BLEU evaluation metric (Papineni et al., 2002). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a lexicalised reordering. (i) Training systems with Wikipedia or Europarl for domain-specific translation. Table 8 shows the evaluation results on WPtest. All the specialised systems obtain significant improvements with respect to the Europarl system, regardless of their size. For instance, the worst specialised system (c3g with only 95,715 sentences for CS) outperforms by more than 10 points of BLEU the general Europarl translator. The mos"
W15-3402,W11-1212,0,0.211252,"ction (Erdmann et al., 2008), extraction of bilingual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 2008; Yasuda and Sumita, 2008). 3 Domain-Specific Comparable Corpora Extraction In this section we describe our proposal to extract domain-specific comparable corpora from Wikipedia. The input to the pipeline is the top category of the domain (e.g., Sport). The terminology used in this description is as follows. Let c be a Wikipedia category and c∗ be the top category of a domain. Let a be a Wikipedia article; a ∈ c if a contains c among its categories. Let G be the Wikipedia category graph. Vocabulary definition. The domain vocabula"
W15-3402,2005.mtsummit-papers.11,0,0.108055,"to be used for training SMT systems. Some standard parallel corpora have the same order of magnitude. For tasks other than MT, where the precision on the extracted pairs can be more important than the recall, one can obtain cleaner corpora by using a threshold that maximises precision instead of F1 . Evaluation: Statistical Machine Translation Task In this section we validate the quality of the obtained corpora by studying its impact on statistical machine translation. There are several parallel corpora for the English–Spanish language pair. We select as a general-purpose corpus Europarl v7 (Koehn, 2005), with 1.97M parallel sentences. The order of magnitude is similar to the largest corpus we have extracted from Wikipedia, so we can compare the results in a size-independent way. If our corpus extracted from Wikipedia was made up with parallel fragments of the desired domain, it should be the most adequate to translate these domains. If the quality of the parallel fragments was acceptable, it should also help when translating out-of-domain texts. In order to test these hypotheses we analyse three settings: (i) train SMT systems only with Wikipedia (WP) or Europarl (EP) to translate domain-spe"
W15-3402,J10-4005,0,0.0131287,"n Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 3–13, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics 2 Background"
W15-3402,J03-1002,0,0.00628859,"eliminate duplicates from this corpus, the size of the union is close to the sum of the individual corpora. This indicates that every similarity measure selects a different set of parallel fragments. Beside the specialised corpus for each domain, we build a larger corpus with all the data (Un). Again, duplicate fragments coming from articles belonging to more than one domain are removed. SMT systems are trained using standard freely available software. We estimate a 5-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with Moses (Koehn et al., 2007). We optimise the feature weights of the model with Minimum Error Rate Training (MERT) (Och, 2003) c3g cog monoen ¯ S·len union WPdev WPtest GNOME CS Sc Sp Un 95,715 182,283 210,664 120,835 577,428 723,760 1,213,965 1,367,169 956,346 3,847,381 334,828 451,324 461,237 389,975 1,181,664 883,366 1,430,962 1,638,777 1,160,977 4,948,241 300 500 1000 300 500 – 300 500 – 900 1500 – Table 7: Number of sentences of the Wikipedia parallel corpora used to train the SMT systems (top rows) and of the sets used for development"
W15-3402,P11-1133,0,0.0249799,"nally, the most related tool to ours: CorpusPedia8 extracts non-aligned, softly-aligned, and strongly-aligned comparable corpora from Wikipedia (Otero and L´opez, 2010). The difference with respect to our model is that they only consider the articles associated to one specific category and not to an entire domain. The inter-connection among Wikipedia editions in different languages has been exploited for multiple tasks including lexicon induction (Erdmann et al., 2008), extraction of bilingual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 2008; Yasuda and Sumita, 2008). 3 Domain-Specific Comparable Corpora Extraction In this section"
W15-3402,P11-2000,0,0.224925,"Missing"
W15-3402,P95-1050,0,0.107949,"m to train statistical machine translation engines for specific domains. Our experiments on the English– Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available;"
W15-3402,C10-1124,0,0.028803,"Spanish pair in the domains of Computer Science, Science, and Sports show that our in-domain translator performs significantly better than a generic one when translating in-domain Wikipedia articles. Moreover, we show that these corpora can help when translating out-of-domain texts. 1 Introduction Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. Comparable corpora were first used for extracting parallel lexicons (Rapp, 1995; Fung, 1995). Later they were used for feeding statistical machine translation (SMT) systems (Uszkoreit et al., 2010) and in multilingual retrieval models (Sch¨onhofen et al., 2007; Potthast et al., 2008). SMT systems estimate the statistical models from bilingual texts (Koehn, 2010). Since only the words that appear in the corpus can be translated, having a corpus of the right domain is important to have high coverage. However, it is evident that no large collections of parallel texts for all domains and language pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of"
W15-3402,1992.tmi-1.7,0,0.831818,"Missing"
W15-3402,I05-1023,0,0.0315766,"nguage pairs exist. In some cases, only general-domain parallel corpora are available; in some others there are no parallel resources at all. 1 http://en.wikipedia.org/wiki/Help: Category 3 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 3–13, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics 2 Background Sport Comparability in multilingual corpora is a fuzzy concept that has received alternative definitions without reaching an overall consensus (Rapp, 1995; Eagles Document Eag–Tcwg–Ctyp, 1996; Fung, 1998; Fung and Cheung, 2004; Wu and Fung, 2005; McEnery and Xiao, 2007; Sharoff et al., 2013). Ideally, a comparable corpus should contain texts in multiple languages which are similar in terms of form and content. Regarding content, they should observe similar structure, function, and a long list of characteristics: register, field, tenor, mode, time, and dialect (Maia, 2003). Nevertheless, finding these characteristics in real-life data collections is virtually impossible. Therefore, we attach to the following simpler four-class classification (Skadin¸a et al., 2010): (i) Parallel texts are true and accurate translations or approximate"
W15-3402,2009.mtsummit-posters.26,0,0.0424596,"comparable documents and extracts parallel sentences, lexicons, and named entities. Finally, the most related tool to ours: CorpusPedia8 extracts non-aligned, softly-aligned, and strongly-aligned comparable corpora from Wikipedia (Otero and L´opez, 2010). The difference with respect to our model is that they only consider the articles associated to one specific category and not to an entire domain. The inter-connection among Wikipedia editions in different languages has been exploited for multiple tasks including lexicon induction (Erdmann et al., 2008), extraction of bilingual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 200"
W15-3402,N10-1063,0,0.0207021,"ual dictionaries (Yu and Tsujii, 2009), and identification of particular translations (Chu et al., 2014; Prochasson and Fung, 2011). Different cross-language NLP tasks have particularly taken advantage of Wikipedia. Articles have been used for query translation (Sch¨onhofen et al., 2007) and crosslanguage semantic representations for similarity estimation (Cimiano et al., 2009; Potthast et al., 2008; Sorg and Cimiano, 2012). The extraction of parallel corpora from Wikipedia has been a hot topic during the last years (Adafre and de Rijke, 2006; Patry and Langlais, 2011; Plamada and Volk, 2012; Smith et al., 2010; Tom´as et al., 2008; Yasuda and Sumita, 2008). 3 Domain-Specific Comparable Corpora Extraction In this section we describe our proposal to extract domain-specific comparable corpora from Wikipedia. The input to the pipeline is the top category of the domain (e.g., Sport). The terminology used in this description is as follows. Let c be a Wikipedia category and c∗ be the top category of a domain. Let a be a Wikipedia article; a ∈ c if a contains c among its categories. Let G be the Wikipedia category graph. Vocabulary definition. The domain vocabulary represents the set of terms that better c"
W15-3402,zesch-etal-2008-extracting,0,0.105477,"Missing"
W15-3402,2012.eamt-1.37,0,0.108348,"Missing"
W15-3402,P07-2045,0,\N,Missing
W15-3402,P12-3016,0,\N,Missing
W15-3402,tiedemann-2012-parallel,0,\N,Missing
W15-3402,P03-1021,0,\N,Missing
W15-4908,W14-4012,0,0.0416542,"Missing"
W15-4908,P14-1129,0,0.0180384,"there are some works that try to use vector models trained using recurrent neural networks (RNN) to improve decoder outputs. For instance, in (Sundermeyer et al., 2014) they build two kinds of models at word level, one based on word alignments and other one phrase-based. The authors train RNNs to obtain their models and they use them to rerank n-best lists after decoding. They report improvements in BLEU and TER scores in several language pairs, but they are not worried about context issues of a document although they do take into account both sides of the translation: source and target. In (Devlin et al., 2014) they also present joint models that augment the NNLM with a source context window to introduce a new decoding feature. They ﬁnally present improvements in BLEU score for Arabic-English language pair and show a new technique to introduce this kind of models inside MT systems in a computationally efﬁcient way. These two last works prove the power of applying NN models as features inside MT systems. 3 Training monolingual and bilingual semantic models As we explained before, there are several works that use monolingual WVM as language models, 61 or the composition of monoligual models to build b"
W15-4908,D11-1084,0,0.587373,"ion in order to maintain the characteristics of the discourse. The evolution of the topic through a text is also an important feature to preserve. All these aspects can be used to improve the translation quality by trying to assure coherence throughout a document. Several recent works go on that direction. Some of them present postprocessing approaches making changes into a ﬁrst translation according to document-level information (Mart´ınez-Garcia et al., 2014a; Xiao et al., 2011). Others introduce the information within the decoder, by, for instance, implementing a topicbased cache approach (Gong et al., 2011; Xiong et al., 2015). The decoding methodology itself can be changed. This is the case of a document-oriented decoder, Docent (Hardmeier et al., 2013), which implements a search in the space of translations of a whole document. This framework allows us to consider features that apply at document level. One of the main goals of this paper is to take advantage of this capability to include semantic information at decoding time. We present here the usage of a semantic representation based on word embeddings as a language model within a document-oriented decoder. To do this, we trained a word vec"
W15-4908,P12-3024,1,0.877684,"Missing"
W15-4908,2010.iwslt-papers.10,0,0.051944,"he paper is organized as follows. A brief revision of the related work is done in Section 2. In Section 3, we describe our approach of using a bilingual word vector model as a language model. The model is compared to monolingual models and evaluated. We show and discuss the results of our experiments on the full translation task in Section 5. Finally, we draw the conclusions and deﬁne several lines of future work in Section 6. 2 Related Work In the last years, approaches to document-level translation have started to emerge. The earliest ones deal with pronominal anaphora within an SMT system (Hardmeier and Federico, 2010; Nagard and Koehn, 2010). These authors develop models that, with the help of coreference resolution methods, identify links among words in a text and use them for a better translation of pronouns. More recent approaches focus on topic cohesion. (Gong et al., 2011) tackle the problem by making available to the decoder the previous translations at decoding time using a cache system. In this way, one can bias the system towards the lexicon already used. (Xiong et al., 2015) also present a topic-based coherence improvement for an SMT system by trying to preserve the continuity of sentence topics"
W15-4908,W10-1737,0,0.0719471,"Missing"
W15-4908,D12-1108,0,0.211389,"Missing"
W15-4908,J03-1002,0,0.00952259,"α is the proportion of content words in the training corpus and � is a small ﬁxed probability, as described in (Hardmeier, 2014). The assumption is the same here as before, the better the choice, the closer the context vector will be to the vector representation of the evaluated word. The ﬁnal score for a document translation candidate is an average of the scores of its words. 5.2 Experimental Settings Our SMT baseline system is based on Moses. The translation system has been trained with the Europarl corpus in its version 7 for the Spanish– English language pair. We used the GIZA++ software (Och and Ney, 2003) to do the word 63 alignments. The language model is an interpolation of several 5-gram language models obtained using SRILM (Stolcke, 2002) with interpolated Kneser-Ney discounting on the target side of the Europarl corpus v7; United Nations; NewsCommentary 2007, 2008, 2009 and 2010; AFP, APW and Xinhua corpora as given by (Specia et al., 2013)3 The optimization of the weights is done with MERT (Och, 2003) against the BLEU measure on the NewsCommentary corpus of 2009. As in the previous section, our experiments are carried out over the NewsCommentary-2011 test set. We chose the newswire docum"
W15-4908,P13-4033,0,0.684855,". All these aspects can be used to improve the translation quality by trying to assure coherence throughout a document. Several recent works go on that direction. Some of them present postprocessing approaches making changes into a ﬁrst translation according to document-level information (Mart´ınez-Garcia et al., 2014a; Xiao et al., 2011). Others introduce the information within the decoder, by, for instance, implementing a topicbased cache approach (Gong et al., 2011; Xiong et al., 2015). The decoding methodology itself can be changed. This is the case of a document-oriented decoder, Docent (Hardmeier et al., 2013), which implements a search in the space of translations of a whole document. This framework allows us to consider features that apply at document level. One of the main goals of this paper is to take advantage of this capability to include semantic information at decoding time. We present here the usage of a semantic representation based on word embeddings as a language model within a document-oriented decoder. To do this, we trained a word vector model (WVM) using neural networks. As a ﬁrst approach, a monolingual model is used in analogy with the standard monolingual language models based o"
W15-4908,Y14-1004,0,0.363562,"ord vector models are more appropriate for the purpose of translation. The ﬁnal document-level translator incorporating the semantic model outperforms the basic Docent (without semantics) and also performs slightly over a standard sentencelevel SMT system in terms of ULC (the average of a set of standard automatic evaluation metrics for MT). Finally, we also present some manual analysis of the translations of some concrete documents. 1 Introduction Document-level information is usually lost during the translation process when using Statistical Machine Translation (SMT) sentence-based systems (Hardmeier, 2014; Webber, 2014). Cross-sentence dependencies are totally ignored, as they translate sentence by sentence without taking into account any document context when choosing the best translation. Some simple phenomena like c 2015 The authors. This article is licensed under a Creative � Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 59 coreferent pronouns outside a sentence cannot be properly translated in this way, which is already important because the correct translation of pronouns in a document confers a high level of coherence to the ﬁnal translation. Also, discourse connective"
W15-4908,P15-1001,0,0.0369739,"Missing"
W15-4908,P07-2045,0,0.0140381,"ng to the distribution of the word over the target document; and ﬁnally, generate the new translation tak60 ing into account the results of the ﬁrst two steps. These approaches report improvements in the ﬁnal translations but, in most of them. the improvements can only be seen through a detailed manual evaluation. When using automatic evaluation metrics like BLEU (Papineni et al., 2002), differences are not signiﬁcant. A document-oriented SMT decoder is presented in (Hardmeier et al., 2012; Hardmeier et al., 2013). The decoder is built on top of an open-source phrase-based SMT decoder, Moses (Koehn et al., 2007). The authors present a stochastic local search decoding method for phrase-based SMT systems which allows decoding complete documents. Docent starts from an initial state (translation) given by Moses and this one is modiﬁed by the application of a hill climbing strategy to ﬁnd a (local) maximum of the score function. The score function and some deﬁned change operations are the ones encoding the document-level information. One remarkable characteristic of this decoder, besides the change of perspective in the implementation from sentence-level to document-level, is that it allows the usage of a"
W15-4908,W04-3250,0,0.406098,"Missing"
W15-4908,C14-1017,0,0.033399,"Missing"
W15-4908,W14-4015,1,0.864402,"Missing"
W15-4908,N13-1090,0,0.0309364,"c Space Language Model (SSLM). In this case, the decoder uses the information of the word vector model to evaluate the adequacy of a word inside a translation by calculating the distance among the current word and its context. In the last years, several distributed word representation models have been introduced. Furthermore, distributed models have been successfully applied to several different NLP tasks. These models are able to capture and combine the semantic information of the text. An efﬁcient implementation of the Context Bag of Words (CBOW) and the Skipgram algorithms is presented in (Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013d). Within this implementation WVMs are trained using a neural network. These models proved to be robust and powerful to predict semantic relations between words even across languages. They are implemented inside the word2vec software package. However, they are not able to handle lexical ambiguity as they conﬂate word senses of polysemous words into one common representation. This limitation is already discussed in (Mikolov et al., 2013b) and in (Wolf et al., 2014), in which bilingual extensions of the word2vec architecture are also proposed. These"
W15-4908,P03-1021,0,0.0418603,"line system is based on Moses. The translation system has been trained with the Europarl corpus in its version 7 for the Spanish– English language pair. We used the GIZA++ software (Och and Ney, 2003) to do the word 63 alignments. The language model is an interpolation of several 5-gram language models obtained using SRILM (Stolcke, 2002) with interpolated Kneser-Ney discounting on the target side of the Europarl corpus v7; United Nations; NewsCommentary 2007, 2008, 2009 and 2010; AFP, APW and Xinhua corpora as given by (Specia et al., 2013)3 The optimization of the weights is done with MERT (Och, 2003) against the BLEU measure on the NewsCommentary corpus of 2009. As in the previous section, our experiments are carried out over the NewsCommentary-2011 test set. We chose the newswire documents as test set because typically they are documents with high consistency and coherence. Regarding the document-level decoder, we use Docent. The ﬁrst step in the Docent translation process is the output of our Moses baseline system. We set the initial Docent weights to be the same as the ones obtained with MERT for the Moses baseline. Finally, the word vector models used in the experiments of this sectio"
W15-4908,P02-1040,0,0.0958032,"ays within a same document. The aim is to incorporate document contexts into an existing SMT system following 3 steps. First, they identify the ambiguous words; then, they obtain a set of consistent translations for each word according to the distribution of the word over the target document; and ﬁnally, generate the new translation tak60 ing into account the results of the ﬁrst two steps. These approaches report improvements in the ﬁnal translations but, in most of them. the improvements can only be seen through a detailed manual evaluation. When using automatic evaluation metrics like BLEU (Papineni et al., 2002), differences are not signiﬁcant. A document-oriented SMT decoder is presented in (Hardmeier et al., 2012; Hardmeier et al., 2013). The decoder is built on top of an open-source phrase-based SMT decoder, Moses (Koehn et al., 2007). The authors present a stochastic local search decoding method for phrase-based SMT systems which allows decoding complete documents. Docent starts from an initial state (translation) given by Moses and this one is modiﬁed by the application of a hill climbing strategy to ﬁnd a (local) maximum of the score function. The score function and some deﬁned change operation"
W15-4908,P13-4014,0,0.0225964,"Missing"
W15-4908,D14-1003,0,0.0175693,"s well as in other areas. In short, NMT systems are build over a trained neural network that is able to output a translation given a source text in the input (Sutskever et al., 2014b; Sutskever et al., 2013; Bahdanau et al., 2014; Cho et al., 2014). However, these systems report some problems when translating unknown or rare words. We are aware of only few works that try to address this problem (Sutskever et al., 2014a; Jean et al., 2014). Furthermore, there are some works that try to use vector models trained using recurrent neural networks (RNN) to improve decoder outputs. For instance, in (Sundermeyer et al., 2014) they build two kinds of models at word level, one based on word alignments and other one phrase-based. The authors train RNNs to obtain their models and they use them to rerank n-best lists after decoding. They report improvements in BLEU and TER scores in several language pairs, but they are not worried about context issues of a document although they do take into account both sides of the translation: source and target. In (Devlin et al., 2014) they also present joint models that augment the NNLM with a source context window to introduce a new decoding feature. They ﬁnally present improveme"
W15-4908,D13-1176,0,0.168547,"Missing"
W15-4908,P15-1002,0,0.0355122,"Missing"
W15-4908,tiedemann-2012-parallel,0,0.0147478,"neously the semantic information associated to the source word and the information in the target side of the translation. In this way, we hope to better capture the semantic information that is implicitly given by translating a text. To better characterize ambiguous words for MT, for instance, we expect to be able to distinguish among the different meanings that the word desk can have when translated in Spanish: desk|mesa vs. desk|mostrador vs. desk|escritorio. 3.2 Settings The training set for our models is built from parallel corpora in the English-Spanish language pair available in Opus 1 (Tiedemann, 2012; Tiedemann, 2009). These corpora have been automatically aligned and therefore contain the aligment information necessary to build our bilingual models. We chose the one-to-one alignments to avoid noise and duplicities in the ﬁnal data. Table 1 shows the size of the speciﬁc data used: EuropalV7, United Nations, Multilingual United Nations, and Subtitles-2012. Monolingual models are also build with these corpora and therefore are comparable in size. With this corpus, the ﬁnal training set has 584 million words for English and 759 for Spanish. 1 http://opus.lingﬁl.uu.se/ Training Development Te"
W15-4908,2011.mtsummit-papers.13,0,0.260151,"onnectives are valuable because they mark the ﬂow of the discourse in a text. It is desirable to transfer them to the output translation in order to maintain the characteristics of the discourse. The evolution of the topic through a text is also an important feature to preserve. All these aspects can be used to improve the translation quality by trying to assure coherence throughout a document. Several recent works go on that direction. Some of them present postprocessing approaches making changes into a ﬁrst translation according to document-level information (Mart´ınez-Garcia et al., 2014a; Xiao et al., 2011). Others introduce the information within the decoder, by, for instance, implementing a topicbased cache approach (Gong et al., 2011; Xiong et al., 2015). The decoding methodology itself can be changed. This is the case of a document-oriented decoder, Docent (Hardmeier et al., 2013), which implements a search in the space of translations of a whole document. This framework allows us to consider features that apply at document level. One of the main goals of this paper is to take advantage of this capability to include semantic information at decoding time. We present here the usage of a semant"
W16-2336,D15-1041,0,0.0220783,"ings We look at this task as a bilinear prediction task as proposed by (Madhyastha et al., 2014). The proposed model makes use of word embeddings of both languages with no additional features. The basic function is formulated —the probability of a target word given a source word— as log-linear model and takes the following form: exp{φs˜(s)&gt; W φt˜(t)} Pr(t|s; W ) = P &gt; 0 t0 exp{φs˜(s) W φt˜(t )} Segments (1) As a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or part-of-speech tagging (Ling et al., 2015; Santos and Zadrozny, 2014). For our system we selected the best characterbased embedding architecture proposed by Kim et al. (Kim et al., 2016). The computation of the representation of each word starts with a characterbased embedding layer that associates each word (sequence of characters) with a sequence of vectors. This sequence of vectors is then processed with a set of 1D convolution filters of different lengths (from 1 to 7 characters) followed with a max pooling layer and two additional highway layers. The output of the second highway layer"
W16-2336,P03-1021,0,0.0143318,"these systems BTT (big translation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora for the Spanish–English language pair (i.e., UN corpora, Europarl corpora, News corpus, etc.2 ). This corpus appears as Quest in Table 1. For the monolingual corpus we use"
W16-2336,N03-1017,0,0.0306671,"system. http://www.statmt.org/wmt16 463 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 463–468, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 3 The Translation System Table 1: Size of the parallel (top) and monolingual (bottom) corpora used to train the translation systems The TALP-UPC translation system is built on three different components. We describe their theoretical basis in the following subsections. 3.1 Corpus Biomedical Quest Phrase-based SMT The standard phrase-based machine translation system (Koehn et al., 2003) focuses on finding the most probable target sentence given the source sentence. The phrase-based system has evolved from the noisy-channel to the log-linear model which combines a set of feature functions in the decoder, including the translation and language model, the reordering model and the lexical models. Although the phrase-based system is a commoditized technology used at the academic and commercial level, there are still many challenges to solve, such as OOVs. 3.2 Bio-mono/en Bio-mono/es Wikipedia/en Wikipedia/es 3.3 Words Vocab 6 6 0.3 · 106 0.5 · 106 1 · 10 13 · 106 0.1 · 106 0.01 ·"
W16-2336,padro-stanilovsky-2012-freeling,0,0.0258286,"Missing"
W16-2336,P02-1040,0,0.0955039,"ation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora for the Spanish–English language pair (i.e., UN corpora, Europarl corpora, News corpus, etc.2 ). This corpus appears as Quest in Table 1. For the monolingual corpus we use an English and Spanish Wikipedia dump3 ."
W16-2336,P07-2045,0,0.0331048,"small translation table). For more general systems, we also use the Quest data; we name these systems BTT (big translation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora for the Spanish–English language pair (i.e., UN corpora, Europarl corpora, News corpus, etc.2"
W16-2336,D15-1176,0,0.0263254,"n using Bilingual Word-Embeddings We look at this task as a bilinear prediction task as proposed by (Madhyastha et al., 2014). The proposed model makes use of word embeddings of both languages with no additional features. The basic function is formulated —the probability of a target word given a source word— as log-linear model and takes the following form: exp{φs˜(s)&gt; W φt˜(t)} Pr(t|s; W ) = P &gt; 0 t0 exp{φs˜(s) W φt˜(t )} Segments (1) As a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or part-of-speech tagging (Ling et al., 2015; Santos and Zadrozny, 2014). For our system we selected the best characterbased embedding architecture proposed by Kim et al. (Kim et al., 2016). The computation of the representation of each word starts with a characterbased embedding layer that associates each word (sequence of characters) with a sequence of vectors. This sequence of vectors is then processed with a set of 1D convolution filters of different lengths (from 1 to 7 characters) followed with a max pooling layer and two additional highway layers. Th"
W16-2336,2006.iwslt-papers.2,1,0.820112,"Missing"
W16-2336,K15-1031,0,0.0532749,"Missing"
W16-2336,C14-1017,1,0.845249,"rms of perplexity (Mikolov et al., 2010). They are also a good re-ranking option in tasks such as speech recognition and machine translation. However, the standard lookup-based word embeddings are limited to a finite-size vocabulary for both computational and sparsity reasons. Moreover, the orthographic representation of the words is completely ignored. The standard learning process is blind to the presence of stems, prefixes, suffixes and any other kind of affixes in words. Vocabulary Expansion using Bilingual Word-Embeddings We look at this task as a bilinear prediction task as proposed by (Madhyastha et al., 2014). The proposed model makes use of word embeddings of both languages with no additional features. The basic function is formulated —the probability of a target word given a source word— as log-linear model and takes the following form: exp{φs˜(s)&gt; W φt˜(t)} Pr(t|s; W ) = P &gt; 0 t0 exp{φs˜(s) W φt˜(t )} Segments (1) As a solution to those drawbacks, new alternative character-based word embeddings have been recently proposed for tasks as language modeling (Kim et al., 2016; Ling et al., 2015), parsing (Ballesteros et al., 2015) or part-of-speech tagging (Ling et al., 2015; Santos and Zadrozny, 201"
W16-2336,D13-1140,0,0.0225269,"er-based neural language model. Section 2 presents some related work to our approach. Next, Section 3 introduces the theoretical aspects of the system components and Section 4 the experiments. Finally, we justify our choice for the final submission and draw the conclusions in Section 5. 1 Related Work On the other hand, there have been several language models used for rescoring in SMT. For example, neural feed-forward language models (Schwenk et al., 2006) have been used to rescore both n-gram-based and phrase-based systems. Mikolov (2012) re-ranks n-best lists with recurrent neural networks. Vaswani et al. (2013) combine feed-forward language models, with rectified linear units and noise-contrastive estimation. Luong et al. (2015) propose to use deeper neural models which improve re-ranking. In this paper, we are using Kim et al. (2016) a characterbased language model to re-rank the output of the phrase-based system. http://www.statmt.org/wmt16 463 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 463–468, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 3 The Translation System Table 1: Size of the parallel (top) and m"
W16-2336,P15-2118,0,0.065368,"Missing"
W16-2336,N15-1176,0,0.031756,"Missing"
W16-2336,J03-1002,0,0.00913989,"in-domain system, we use only the biomedical data made available for the task (STT systems, small translation table). For more general systems, we also use the Quest data; we name these systems BTT (big translation table). For the in-domain system, a 5-gram language model is estimated on the target side of the corpus using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002) (SLM, small language model). For the extended systems, we use all the monolingual corpora available and the target side of the large parallel corpus (BLM, big language model). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007). The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric on devBio. Data Our main corpus is the compilation of the corpora assigned for the shared task, which was built using scientific publications gathered from the Scielo database. We focus on the Spanish–English language pair, for which the size of the corpora is summarised in Table 1. We further increase the vocabulary of the system by using standard parallel corpora"
W17-2617,N06-1003,0,0.136031,"Missing"
W17-2617,P11-2071,0,0.0597669,"Missing"
W17-2617,2005.mtsummit-papers.11,0,0.0490525,"Missing"
W17-2617,D12-1025,0,0.0398781,"Missing"
W17-2617,E14-1049,0,0.749708,"hile the authors were in TALP Research Center, Universitat Polit`ecnica de Catalunya, Barcelona. 139 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 139–145, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics embeddings using either sentence aligned or document aligned corpora (Bhattarai, 2012; Gouws et al., 2015; Koˇcisk´y et al., 2014). Our approach is significantly different as we obtain embeddings separately on monolingual corpora and then use supervision in the form of a small sparse bilingual dictionary, in some terms similar to Faruqui and Dyer (2014). We use a simple yet principled method to obtain a probabilistic conditional distribution of words directly and these probabilities allow us to expand the translation model for new words. The rest of the paper is organised as follows. Section 2 presents the log-bilinear softmax model, and its integration into an SMT system. The experimental work is described in Section 3. Finally, we conclude and sketch some avenues for future work. 2 nificantly large monolingual corpus and b) estimating W given a relatively small dictionary. That is, to learn W we use the source word to target word dictionar"
W17-2617,P14-2037,0,0.0364175,"Missing"
W17-2617,W15-1521,0,0.224732,"ght associated with feature hi (f, e) and λoov is the weight associated with the unseen word. 3 Empirical Analysis Quality of the Learned Embeddings. To understand the performance of the embedding projections in our model, we perform experiments to compute the top-10 accuracy of our models in the same setting provided in Upadhyay et al. (2016) for cross-lingual dictionary induction1 . The evaluation task judges how good cross-lingual embeddings are at detecting word pairs that are semantically similar across languages. Similarly to Upadhyay et al. (2016), we compare against BiSkip embeddings (Luong et al., 2015a), BiCVM (Hermann and Blunsom, 2014), BiCCA (Faruqui and Dyer, 2014) and BiVCD (Vulic and Moens, 2015). We experiment with English–German and English– French language pairs, so that we can induce the dictionaries for the five systems. As seen in Table 1, our full 300-dimensional embeddings perform better than the BiCCA-based model, whereas 100-dimensional compressed embedding perform slightly worse, but still are competitive. Since our model and BiCCA use similar supervision, we obtain similar results and differ in a similar way to those that use stronger supervision like BiCVM and BiSkip bas"
W17-2617,P15-1002,0,0.359781,"ght associated with feature hi (f, e) and λoov is the weight associated with the unseen word. 3 Empirical Analysis Quality of the Learned Embeddings. To understand the performance of the embedding projections in our model, we perform experiments to compute the top-10 accuracy of our models in the same setting provided in Upadhyay et al. (2016) for cross-lingual dictionary induction1 . The evaluation task judges how good cross-lingual embeddings are at detecting word pairs that are semantically similar across languages. Similarly to Upadhyay et al. (2016), we compare against BiSkip embeddings (Luong et al., 2015a), BiCVM (Hermann and Blunsom, 2014), BiCCA (Faruqui and Dyer, 2014) and BiVCD (Vulic and Moens, 2015). We experiment with English–German and English– French language pairs, so that we can induce the dictionaries for the five systems. As seen in Table 1, our full 300-dimensional embeddings perform better than the BiCCA-based model, whereas 100-dimensional compressed embedding perform slightly worse, but still are competitive. Since our model and BiCCA use similar supervision, we obtain similar results and differ in a similar way to those that use stronger supervision like BiCVM and BiSkip bas"
W17-2617,P08-2015,0,0.0889848,"Germany cristinae@dfki.de Introduction Data-driven machine translation systems are able to translate words that have been seen in the training parallel corpora, however translating unseen words is still a major challenge for even the best performing systems. The amount of parallel data is finite (and sometimes scarce) and, therefore, word types like named entities, domain specific content words, or infrequent terms are rare. This lack of information can potentially result in incomplete or erroneous translations. This problem has been actively studied in the field of machine translation (MT) (Habash, 2008; Daum´e III and Jagarlamudi, 2011; Marton et al., 2009; Rapp, 1999; Dou and Knight, ∗ This work was done while the authors were in TALP Research Center, Universitat Polit`ecnica de Catalunya, Barcelona. 139 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 139–145, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics embeddings using either sentence aligned or document aligned corpora (Bhattarai, 2012; Gouws et al., 2015; Koˇcisk´y et al., 2014). Our approach is significantly different as we obtain embeddings separately on monolingual cor"
W17-2617,C14-1017,1,0.929095,"ing W given a relatively small dictionary. That is, to learn W we use the source word to target word dictionary as training supervision. The dictionary can be a true bilingual dictionary or the word alignments generated by the SMT system, therefore, no additional resources to the training parallel corpus are needed. We learn W by minimizing the negative loglikelihood of the dictionary using a regularized (relaxed low-rank P regularization based) objective as: L(W ) = − e,f log(Pr(f |e; W )) + λkW kp . λ is the constant that controls the capacity of W . To find the optimum, we follow previous (Madhyastha et al., 2014b) work and use an optimization scheme based on Forward-Backward Splitting (FOBOS) (Singer and Duchi, 2009). We experiment with two regularization schemes, p = 2 or the `2 regularizer and p = ∗ or the `∗ (nuclear norm) regularizer. We find that both norms have approximately similar performance, however the trace norm regularized W has lower capacity and hence, smaller number of parameters. This is also observed by (Bach, 2008; Madhyastha et al., 2014a,b). In general, we can apply the ideas used by Mikolov et al. (2013b) to speed up the training as this model is equivalent to a softmax model. W"
W17-2617,P14-1006,0,0.0412608,"i (f, e) and λoov is the weight associated with the unseen word. 3 Empirical Analysis Quality of the Learned Embeddings. To understand the performance of the embedding projections in our model, we perform experiments to compute the top-10 accuracy of our models in the same setting provided in Upadhyay et al. (2016) for cross-lingual dictionary induction1 . The evaluation task judges how good cross-lingual embeddings are at detecting word pairs that are semantically similar across languages. Similarly to Upadhyay et al. (2016), we compare against BiSkip embeddings (Luong et al., 2015a), BiCVM (Hermann and Blunsom, 2014), BiCCA (Faruqui and Dyer, 2014) and BiVCD (Vulic and Moens, 2015). We experiment with English–German and English– French language pairs, so that we can induce the dictionaries for the five systems. As seen in Table 1, our full 300-dimensional embeddings perform better than the BiCCA-based model, whereas 100-dimensional compressed embedding perform slightly worse, but still are competitive. Since our model and BiCCA use similar supervision, we obtain similar results and differ in a similar way to those that use stronger supervision like BiCVM and BiSkip based embeddings. MT Data and System Set"
W17-2617,P08-1045,0,0.0653112,"Missing"
W17-2617,D09-1040,0,0.0291702,"ven machine translation systems are able to translate words that have been seen in the training parallel corpora, however translating unseen words is still a major challenge for even the best performing systems. The amount of parallel data is finite (and sometimes scarce) and, therefore, word types like named entities, domain specific content words, or infrequent terms are rare. This lack of information can potentially result in incomplete or erroneous translations. This problem has been actively studied in the field of machine translation (MT) (Habash, 2008; Daum´e III and Jagarlamudi, 2011; Marton et al., 2009; Rapp, 1999; Dou and Knight, ∗ This work was done while the authors were in TALP Research Center, Universitat Polit`ecnica de Catalunya, Barcelona. 139 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 139–145, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics embeddings using either sentence aligned or document aligned corpora (Bhattarai, 2012; Gouws et al., 2015; Koˇcisk´y et al., 2014). Our approach is significantly different as we obtain embeddings separately on monolingual corpora and then use supervision in the form of a small sp"
W17-2617,W13-2233,0,0.0491303,"Missing"
W17-2617,P16-1157,0,0.0149902,"U (Papineni et al., 2002) evaluation metric on the NewsCommentaries 20124 (NewsDev) set. We test our systems on the NewsCommentaries 2013 set (NewsTest) for an in-domain evaluation and on a test set P fb = argmaxf { λi log (hi (f, e)) + λoov log (Pr(f, e))} here, λi is the weight associated with feature hi (f, e) and λoov is the weight associated with the unseen word. 3 Empirical Analysis Quality of the Learned Embeddings. To understand the performance of the embedding projections in our model, we perform experiments to compute the top-10 accuracy of our models in the same setting provided in Upadhyay et al. (2016) for cross-lingual dictionary induction1 . The evaluation task judges how good cross-lingual embeddings are at detecting word pairs that are semantically similar across languages. Similarly to Upadhyay et al. (2016), we compare against BiSkip embeddings (Luong et al., 2015a), BiCVM (Hermann and Blunsom, 2014), BiCCA (Faruqui and Dyer, 2014) and BiVCD (Vulic and Moens, 2015). We experiment with English–German and English– French language pairs, so that we can induce the dictionaries for the five systems. As seen in Table 1, our full 300-dimensional embeddings perform better than the BiCCA-based"
W17-2617,P03-1021,0,0.0539721,"Missing"
W17-2617,P15-2118,0,0.0571708,"irical Analysis Quality of the Learned Embeddings. To understand the performance of the embedding projections in our model, we perform experiments to compute the top-10 accuracy of our models in the same setting provided in Upadhyay et al. (2016) for cross-lingual dictionary induction1 . The evaluation task judges how good cross-lingual embeddings are at detecting word pairs that are semantically similar across languages. Similarly to Upadhyay et al. (2016), we compare against BiSkip embeddings (Luong et al., 2015a), BiCVM (Hermann and Blunsom, 2014), BiCCA (Faruqui and Dyer, 2014) and BiVCD (Vulic and Moens, 2015). We experiment with English–German and English– French language pairs, so that we can induce the dictionaries for the five systems. As seen in Table 1, our full 300-dimensional embeddings perform better than the BiCCA-based model, whereas 100-dimensional compressed embedding perform slightly worse, but still are competitive. Since our model and BiCCA use similar supervision, we obtain similar results and differ in a similar way to those that use stronger supervision like BiCVM and BiSkip based embeddings. MT Data and System Settings. For estimating the monolingual WE, we use the CBOW algorith"
W17-2617,P02-1038,0,0.238749,"Missing"
W17-2617,J03-1002,0,0.00842725,"Missing"
W17-2617,D14-1015,0,0.0188398,"d Sheffield, S1 4DP, UK p.madhyastha@sheffield.ac.uk Abstract 2012; Irvine and Callison-Burch, 2013). Lexiconbased resources have been used for resolving unseen content words by exploiting a combination of monolingual and bilingual resources (Rapp, 1999; Callison-Burch et al., 2006; Zhao et al., 2015). In this context, distributed word representations, or word embeddings (WE), have been recently applied to resolve unseen word related problems (Mikolov et al., 2013b; Zou et al., 2013). In general, word representations capture rich linguistic relationships and several works (Gouws et al., 2015; Wu et al., 2014) try to use them to improve MT systems. However, very few approaches use them directly to resolve the out-of-vocabulary (OOV) problem in MT systems. Previous research in MT systems suggests that a significant number of named entities (NE) can be handled by using simple pre or post-processing methods, e.g., transliteration techniques (Hermjakob et al., 2008; Al-Onaizan and Knight, 2002). However, a change in domain results in a significant increase in the number of unseen content words for which simple pre or post-processing methods are sub-optimal (Zhang et al., 2012). Our work is inspired by"
W17-2617,P02-1040,0,0.0979948,"Missing"
W17-2617,P99-1067,0,0.0836133,"on systems are able to translate words that have been seen in the training parallel corpora, however translating unseen words is still a major challenge for even the best performing systems. The amount of parallel data is finite (and sometimes scarce) and, therefore, word types like named entities, domain specific content words, or infrequent terms are rare. This lack of information can potentially result in incomplete or erroneous translations. This problem has been actively studied in the field of machine translation (MT) (Habash, 2008; Daum´e III and Jagarlamudi, 2011; Marton et al., 2009; Rapp, 1999; Dou and Knight, ∗ This work was done while the authors were in TALP Research Center, Universitat Polit`ecnica de Catalunya, Barcelona. 139 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 139–145, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics embeddings using either sentence aligned or document aligned corpora (Bhattarai, 2012; Gouws et al., 2015; Koˇcisk´y et al., 2014). Our approach is significantly different as we obtain embeddings separately on monolingual corpora and then use supervision in the form of a small sparse bilingu"
W17-2617,P14-1011,0,0.0186198,"However, very few approaches use them directly to resolve the out-of-vocabulary (OOV) problem in MT systems. Previous research in MT systems suggests that a significant number of named entities (NE) can be handled by using simple pre or post-processing methods, e.g., transliteration techniques (Hermjakob et al., 2008; Al-Onaizan and Knight, 2002). However, a change in domain results in a significant increase in the number of unseen content words for which simple pre or post-processing methods are sub-optimal (Zhang et al., 2012). Our work is inspired by the recent advances (Zou et al., 2013; Zhang et al., 2014) in applications of word embeddings to the task of vocabulary expansion in the context of statistical machine translation (SMT). Our focus in this paper is to resolve unseen content words by using continuous word embeddings on both the languages and learn a model over a small seed lexicon to map the embedding spaces. To this extent, our work is similar to Ishiwatari et al. (2016) where the authors map distributional representations using a linear regression method similar to Mikolov et al. (2013b) and insert a new feature based on cosine similarity metric into the MT system. On the other hand,"
W17-2617,P16-1162,0,0.144712,"Missing"
W17-2617,N15-1176,0,0.0365308,"Missing"
W17-2617,N10-1063,0,0.0504076,"Missing"
W17-2617,D13-1141,0,0.0283509,"for Vocabulary Expansion in Machine Translation Pranava Swaroop Madhyastha∗ Department of Computer Science University of Sheffield Sheffield, S1 4DP, UK p.madhyastha@sheffield.ac.uk Abstract 2012; Irvine and Callison-Burch, 2013). Lexiconbased resources have been used for resolving unseen content words by exploiting a combination of monolingual and bilingual resources (Rapp, 1999; Callison-Burch et al., 2006; Zhao et al., 2015). In this context, distributed word representations, or word embeddings (WE), have been recently applied to resolve unseen word related problems (Mikolov et al., 2013b; Zou et al., 2013). In general, word representations capture rich linguistic relationships and several works (Gouws et al., 2015; Wu et al., 2014) try to use them to improve MT systems. However, very few approaches use them directly to resolve the out-of-vocabulary (OOV) problem in MT systems. Previous research in MT systems suggests that a significant number of named entities (NE) can be handled by using simple pre or post-processing methods, e.g., transliteration techniques (Hermjakob et al., 2008; Al-Onaizan and Knight, 2002). However, a change in domain results in a significant increase in the number of uns"
W17-2617,2006.amta-papers.25,0,0.0697341,"Missing"
W17-2617,W02-0505,0,\N,Missing
W17-2617,W05-0909,0,\N,Missing
W17-2617,P07-2045,0,\N,Missing
W19-5315,P07-2045,0,0.0843741,"n Quality Estimation (TQE) dataset for Indian languages (Nisarg et al., 2018), which essentially is the concatenation of two corpora by the Indian Languages Corpora Initiative, which focus on the health and tourism domain each. For development, we use the first 999 sentences from the English-Gujarati version of newsdev2019. Further, we report results on the final newstest2019 corpus. Pre-processing. All English corpora (excluding the evaluation corpora) undergo the same preprocessing. After being sentence split, the corpora are normalized, tokenized and truecased using standard Moses scripts (Koehn et al., 2007a). A byte-pair-encoding (BPE) (Sennrich et al., 2016b) of 40 k merge operations trained jointly on en–gu data respectively is applied accordingly. Duplicates are removed and sentences with more than 50 tokens are discarded. In order to enable a multilingual setup, language tokens indicating the designated target language are prepended to each source sentence. As the English–Gujarati setting is bilingual, this reduces to each Gujarati sentence starting with the language token <en>, and each English sentence with <gu>. Gujarati corpora are normalized and romanized Table 2 is 10 BLEU points belo"
W19-5315,W15-3402,1,0.884334,"Missing"
W19-5315,D16-1245,0,0.0838675,"Missing"
W19-5315,P14-5010,0,0.0053978,"Missing"
W19-5315,Q17-1024,0,0.0741193,"Missing"
W19-5315,P03-1021,0,0.0981224,"ion (SMT). We expect these systems to perform better when the number of parallel sentences is small. SMT systems are trained using standard freely available software. We estimate a 5-gram or 4-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002) depending on the language and the size of the monolingual corpus. Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007b). The optimisation of the feature weights of the model is done with Minimum Error Rate Training (MERT) (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a lexicalised reordering. Table 3: Size of the corpora used for the en–gu models. using the Indic NLP Library.14 The romanized corpora are then tokenized using Moses. As the romanization is case sensitive, no true-casing is performed. The shared BPE is applied. Cross-lingual word embeddings. We initialize the unsupervised NMT model using cross-lingual embeddings. These are trained u"
W19-5315,J03-1002,0,0.0208216,"–gu TQE en–gu WP Reference en–gu 7,807 10,650 107,637 50,000 18,033 Comparable WP Comparable en WP Comparable gu 546,924 143,120 3.3 The second family of systems we use in this setting is statistical machine translation (SMT). We expect these systems to perform better when the number of parallel sentences is small. SMT systems are trained using standard freely available software. We estimate a 5-gram or 4-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002) depending on the language and the size of the monolingual corpus. Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007b). The optimisation of the feature weights of the model is done with Minimum Error Rate Training (MERT) (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a lexicalised reordering. Table 3: Size of the corpora used for the en–gu models. using the Indic NLP Library.14 The romanized corpora are then tokenized using Moses. As the rom"
W19-5315,P18-4020,0,0.0461009,"Missing"
W19-5315,P02-1040,0,0.103632,"stems to perform better when the number of parallel sentences is small. SMT systems are trained using standard freely available software. We estimate a 5-gram or 4-gram language model using interpolated Kneser–Ney discounting with SRILM (Stolcke, 2002) depending on the language and the size of the monolingual corpus. Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2007b). The optimisation of the feature weights of the model is done with Minimum Error Rate Training (MERT) (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a lexicalised reordering. Table 3: Size of the corpora used for the en–gu models. using the Indic NLP Library.14 The romanized corpora are then tokenized using Moses. As the romanization is case sensitive, no true-casing is performed. The shared BPE is applied. Cross-lingual word embeddings. We initialize the unsupervised NMT model using cross-lingual embeddings. These are trained using monolingual data only. For the Engli"
W19-5315,P19-1178,1,0.886049,"Missing"
W19-5315,P17-4012,0,0.0167614,"rom each news outlet is shown in Table 3. Wikipedia (WP) is a popular source for comparable documents. In order to later extract paral8 9 Downloaded from https://dumps.wikimedia. org/ on January 2019. 10 https://github.com/cristinae/ WikiTailor 11 http://christos-c.com/bible/ 12 http://www.statmt.org/wmt19/ translation-task.html 13 http://opus.nlpl.eu/ September-November 2018 186 to provide a first model for back-translations as well as to train the final model submitted, the transformer is used in-between to extract additional data from Wikipedia. The transformer is trained using OpenNMT-py (Klein et al., 2017) and is defined as follows: 6layer encoder-decoder with 8-head self-attention and 2048-dim hidden feed-forward layers. Adam optimization with λ=2 and beta2=0.998; noam learning rate decay (as defined in Vaswani et al. (2017)) with 8000 warm-up steps. Labels are smoothed (=0.1) and a dropout mask (p=0.1) is applied. As is common for transformers, position encodings and Xavier parameter initialization (Glorot and Bengio, 2010) are used. # sentences Monolingual ssNewsCrawl en CommonCrawl gu NewsCrawl gu WP Edition gu 176,220,479 3,729,406 244,919 4,280,531 Crawled Divya Bhaskar gu News18 en News"
W19-5315,P16-1162,0,0.0672911,". Even though all the corpora made available for the shared task have document boundaries, ParaCrawl, for instance, has a mean of 1.06 sentences per document which makes it useless within our approach. 2.2 Small Large Corpus Monolingual corpora. We use a subset of the NewsCrawl corpus in English and German (years 2014, 2017 and a part of 2018, named as ssNewsCrawl in Table 1) to calculate word embeddings as explained in Section 2.3. We first use langdetect3 to extract only those sentences that are in the desired language and compile the final corpora to have a similar number of subword units (Sennrich et al., 2016a) in both languages and years (∼ 4. 109 ). The corpus is further cleaned, tokenised, truecased (with Moses scripts4 ) and BPEd (with subword-nmt5 ). The vocabulary of the BPE model depends on the system and is detailed in Section 2.3. Parallel corpora. Due to the restrictions explained in Section 2.1, we use the parallel corpora made available for the shared task in different proportions. Our base system uses CommonCrawl, 2.3 3 Neural Machine Translation Systems Our NMT systems are trained using the transformer architectures implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018). We"
