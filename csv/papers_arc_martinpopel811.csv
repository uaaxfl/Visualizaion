2021.wnut-1.38,Understanding Model Robustness to User-generated Noisy Texts,2021,-1,-1,2,0,226,jakub naplava,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"Sensitivity of deep-neural models to input noise is known to be a challenging problem. In NLP, model performance often deteriorates with naturally occurring noise, such as spelling errors. To mitigate this issue, models may leverage artificially noised data. However, the amount and type of generated noise has so far been determined arbitrarily. We therefore propose to model the errors statistically from grammatical-error-correction corpora. We present a thorough evaluation of several state-of-the-art NLP systems{'} robustness in multiple languages, with tasks including morpho-syntactic analysis, named entity recognition, neural machine translation, a subset of the GLUE benchmark and reading comprehension. We also compare two approaches to address the performance drop: a) training the NLP models with noised data generated by our framework; and b) reducing the input noise with external system for natural language correction. The code is released at https://github.com/ufal/kazitext."
2021.humeval-1.13,Detecting Post-Edited References and Their Effect on Human Evaluation,2021,-1,-1,3,0,6020,vvera kloudova,Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval),0,"This paper provides a quick overview of possible methods how to detect that reference translations were actually created by post-editing an MT system. Two methods based on automatic metrics are presented: BLEU difference between the suspected MT and some other good MT and BLEU difference using additional references. These two methods revealed a suspicion that the WMT 2020 Czech reference is based on MT. The suspicion was confirmed in a manual analysis by finding concrete proofs of the post-editing procedure in particular sentences. Finally, a typology of post-editing changes is presented where typical errors or changes made by the post-editor or errors adopted from the MT are classified."
2021.findings-emnlp.303,Do {UD} Trees Match Mention Spans in Coreference Annotations?,2021,-1,-1,1,1,227,martin popel,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"One can find dozens of data resources for various languages in which coreference - a relation between two or more expressions that refer to the same real-world entity - is manually annotated. One could also assume that such expressions usually constitute syntactically meaningful units; however, mention spans have been annotated simply by delimiting token intervals in most coreference projects, i.e., independently of any syntactic representation. We argue that it could be advantageous to make syntactic and coreference annotations convergent in the long term. We present a pilot empirical study focused on matches and mismatches between hand-annotated linear mention spans and automatically parsed syntactic trees that follow Universal Dependencies conventions. The study covers 9 datasets for 8 different languages."
2021.emnlp-main.801,Neural Machine Translation Quality and Post-Editing Performance,2021,-1,-1,2,0,3171,vilem zouhar,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high quality, which has become the state-of-the-art approach since then and also got adopted by most translation companies. Through an experimental study involving over 30 professional translators for English -{\textgreater} Czech translation, we examine the relationship between NMT performance and post-editing time and quality. Across all models, we found that better MT systems indeed lead to fewer changes in the sentences in this industry setting. The relation between system quality and post-editing time is however not straightforward and, contrary to the results on phrase-based MT, BLEU is definitely not a stable predictor of the time or final output quality."
2020.wmt-1.17,"Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the {UEDIN}-{CUNI} Submission to the {WMT} 2020 News Translation Task",2020,-1,-1,3,0,5732,ulrich germann,Proceedings of the Fifth Conference on Machine Translation,0,"We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech â English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU."
2020.wmt-1.28,{CUNI} {E}nglish-{C}zech and {E}nglish-{P}olish Systems in {WMT}20: Robust Document-Level Training,2020,-1,-1,1,1,227,martin popel,Proceedings of the Fifth Conference on Machine Translation,0,"We describe our two NMT systems submitted to the WMT 2020 shared task in English{\textless}-{\textgreater}Czech and English{\textless}-{\textgreater}Polish news translation. One system is sentence level, translating each sentence independently. The second system is document level, translating multiple sentences, trained on multi-sentence sequences up to 3000 characters long."
W19-5337,{E}nglish-{C}zech Systems in {WMT}19: Document-Level Transformer,2019,15,1,1,1,227,martin popel,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We describe our NMT systems submitted to the WMT19 shared task in EnglishâCzech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently, we split the document into possibly overlapping multi-sentence segments. In case of the T2T implementation, this {``}document-level{''}-trained system achieves a +0.6 BLEU improvement (p {\textless} 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we cannot draw any conclusions from this week evidence."
W19-5364,{CUNI} System for the {WMT}19 Robustness Task,2019,13,0,3,0,16474,jindvrich helcl,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,We present our submission to the WMT19 Robustness Task. Our baseline system is the Charles University (CUNI) Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain.
W18-6424,{CUNI} Transformer Neural {MT} System for {WMT}18,2018,0,12,1,1,227,martin popel,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We describe our NMT system submitted to the WMT2018 shared task in news translation. Our system is based on the Transformer model (Vaswani et al., 2017). We use an improved technique of backtranslation, where we iterate the process of translating monolingual data in one direction and training an NMT model for the opposite direction using synthetic parallel data. We apply a simple but effective filtering of the synthetic data. We pre-process the input sentences using coreference resolution in order to disambiguate the gender of pro-dropped personal pronouns. Finally, we apply two simple post-processing substitutions on the translated output. Our system is significantly (p {\textless} 0.05) better than all other English-Czech and Czech-English systems in WMT2018."
K18-2001,{C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to {U}niversal {D}ependencies,2018,0,1,3,0,5828,daniel zeman,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"Every year, the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2018, one of two tasks was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on test input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. This shared task constitutes a 2nd edition{---}the first one took place in 2017 (Zeman et al., 2017); the main metric from 2017 has been kept, allowing for easy comparison, also in 2018, and two new main metrics have been used. New datasets added to the Universal Dependencies collection between mid-2017 and the spring of 2018 have contributed to increased difficulty of the task this year. In this overview paper, we define the task and the updated evaluation methodology, describe data preparation, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems."
W17-0412,{U}dapi: Universal {API} for {U}niversal {D}ependencies,2017,3,3,1,1,227,martin popel,Proceedings of the {N}o{D}a{L}i{D}a 2017 Workshop on Universal Dependencies ({UDW} 2017),0,None
K17-3001,{C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to {U}niversal {D}ependencies,2017,28,32,2,0,5828,daniel zeman,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, the task was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe how the data sets were prepared, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems."
W16-6401,{M}oses {\\&} Treex Hybrid {MT} Systems Bestiary,2016,19,0,2,0.74822,14784,rudolf rosa,Proceedings of the 2nd Deep Machine Translation Workshop,0,None
W16-2301,Findings of the 2016 Conference on Machine Translation,2016,113,137,14,0,292,ondvrej bojar,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT quality), and an automatic post-editing task and bilingual document alignment task. This year, 102 MT systems from 24 institutions (plus 36 anonymized online systems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100-point scale assessments). The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries."
W16-2332,{SMT} and Hybrid systems of the {QTL}eap project in the {WMT}16 {IT}-task,2016,26,3,6,0,17856,rosa gaudio,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper presents the description of 12 systems submitted to the WMT16 IT-task, covering six different languages, namely Basque, Bulgarian, Dutch, Czech, Portuguese and Spanish. All these systems were developed under the scope of the QTLeap project, presenting a common strategy. For each language two different systems were submitted, namely a phrasebased MT system built using Moses, and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was implemented using TectoMT. For 4 of the 6 languages, the TectoMT-based system performs better than the Moses-based one."
W16-2334,Dictionary-based Domain Adaptation of {MT} Systems without Retraining,2016,8,4,4,0.74822,14784,rudolf rosa,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We describe our submission to the ITdomain translation task of WMT 2016. We perform domain adaptation with dictionary data on already trained MT systems with no further retraining. We apply our approach to two conceptually different systems developed within the QTLeap project: TectoMT and Moses, as well as Chimera, their combination. In all settings, our method improves the translation quality. Moreover, the basic variant of our approach is applicable to any MT system, including a black-box one."
L16-1296,Tools and Guidelines for Principled Machine Translation Development,2016,2,1,5,0,20841,nora aranberri,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This work addresses the need to aid Machine Translation (MT) development cycles with a complete workflow of MT evaluation methods. Our aim is to assess, compare and improve MT system variants. We hereby report on novel tools and practices that support various measures, developed in order to support a principled and informed approach of MT development. Our toolkit for automatic evaluation showcases quick and detailed comparison of MT system variants through automatic metrics and n-gram feedback, along with manual evaluation via edit-distance, error annotation and task-based feedback."
L16-1483,{QTL}eap {WSD}/{NED} Corpora: Semantic Annotation of Parallel Corpora in Six Languages,2016,9,5,5,0,16264,arantxa otegi,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This work presents parallel corpora automatically annotated with several NLP tools, including lemma and part-of-speech tagging, named-entity recognition and classification, named-entity disambiguation, word-sense disambiguation, and coreference. The corpora comprise both the well-known Europarl corpus and a domain-specific question-answer troubleshooting corpus on the IT domain. English is common in all parallel corpora, with translations in five languages, namely, Basque, Bulgarian, Czech, Portuguese and Spanish. We describe the annotated corpora and the tools used for annotation, as well as annotation statistics for each language. These new resources are freely available and will help research on semantic processing for machine translation and cross-lingual transfer."
2016.eamt-2.1,{T}ecto{MT} {--} a deep linguistic core of the combined Cimera {MT} system,2016,-1,-1,1,1,227,martin popel,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,0,None
W15-5711,Translation Model Interpolation for Domain Adaptation in {T}ecto{MT},2015,-1,-1,4,0.74822,14784,rudolf rosa,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-3009,New Language Pairs in {T}ecto{MT},2015,20,7,4,0.687831,2976,ondvrej duvsek,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"The TectoMT tree-to-tree machine translation system has been updated this year to support easier retraining for more translation directions. We use multilingual standards for morphology and syntax annotation and language-independent base rules. We include a simple, non-parametric way of combining TectoMTxe2x80x99s transfer model outputs. We submitted translations by the Englishto-Czech and Czech-to-English TectoMT pipelines to the WMT shared task. While the former offers a stable performance, the latter is completely new and will require more tuning and debugging."
W15-2111,Using Parallel Texts and Lexicons for Verbal Word Sense Disambiguation,2015,31,4,4,0.687831,2976,ondvrej duvsek,Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015),0,"We present a system for verbal Word Sense Disambiguation (WSD) that is able to exploit additional information from parallel texts and lexicons. It is an extension of our previous WSD method (Dusek et al., 2014), which gave promising results but used only monolingual features. In the follow-up work described here, we have explored two additional ideas: using English-Czech bilingual resources (as features only xe2x80x93 the task itself remains a monolingual WSD task), and using a xe2x80x9chybridxe2x80x9d approach, adding features extracted both from a parallel corpus and from manually aligned bilingual valency lexicon entries, which contain subcategorization information. Albeit not all types of features proved useful, both ideas and additions have led to significant improvements for both languages explored."
W14-3322,{CUNI} in {WMT}14: Chimera Still Awaits Bellerophon,2014,12,7,2,0,4973,alevs tamchyna,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We present our English!Czech and English!Hindi submissions for this yearxe2x80x99s WMT translation task. For English!Czech, we build upon last yearxe2x80x99s CHIMERA and evaluate several setups. English!Hindi is a new language pair for this year. We experimented with reverse self-training to acquire more (synthetic) parallel data and with modeling target-side morphology."
rosa-etal-2014-hamledt,{H}amle{DT} 2.0: Thirty Dependency Treebanks Stanfordized,2014,30,20,4,0.74822,14784,rudolf rosa,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present HamleDT 2.0 (HArmonized Multi-LanguagE Dependency Treebank). HamleDT 2.0 is a collection of 30 existing treebanks harmonized into a common annotation style, the Prague Dependencies, and further transformed into Stanford Dependencies, a treebank annotation style that became popular in recent years. We use the newest basic Universal Stanford Dependencies, without added language-specific subtypes. We describe both of the annotation styles, including adjustments that were necessary to make, and provide details about the conversion process. We also discuss the differences between the two styles, evaluating their advantages and disadvantages, and note the effects of the differences on the conversion. We regard the stanfordization as generally successful, although we admit several shortcomings, especially in the distinction between direct and indirect objects, that have to be addressed in future. We release part of HamleDT 2.0 freely; we are not allowed to redistribute the whole dataset, but we do provide the conversion pipeline."
W13-2216,{P}hrase{F}ix: Statistical Post-Editing of {T}ecto{MT},2013,24,6,2,0,29927,petra galuvsvcakova,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present two English-to-Czech systems that took part in the WMT 2013 shared task: TECTOMT and PHRASEFIX. The former is a deep-syntactic transfer-based system, the latter is a more-or-less standard statistical post-editing (SPE) applied on top of TECTOMT. In a brief survey, we put SPE in context with other system combination techniques and evaluate SPE vs. another simple system combination technique: using synthetic parallel data from TECTOMT to train a statistical MT system (SMT). We confirm that PHRASEFIX (SPE) improves the output of TECTOMT, and we use this to analyze errors in TECTOMT. However, we also show that extending data for SMT is more effective."
P13-1051,Coordination Structures in Dependency Treebanks,2013,31,22,1,1,227,martin popel,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Paratactic syntactic structures are notoriously difficult to represent in dependency formalisms. This has painful consequences such as high frequency of parsing errors related to coordination. In other words, coordination is a pending problem in dependency analysis of natural languages. This paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures. We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. In addition, empirical observations on convertibility between selected styles of representations are shown too."
W12-4205,Using Parallel Features in Parsing of Machine-Translated Sentences for Correction of Grammatical Errors,2012,32,6,4,0.667761,14784,rudolf rosa,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In this paper, we present two dependency parser training methods appropriate for parsing outputs of statistical machine translation (SMT), which pose problems to standard parsers due to their frequent ungrammaticality. We adapt the MST parser by exploiting additional features from the source language, and by introducing artificial grammatical errors in the parser training data, so that the training sentences resemble SMT output.n n We evaluate the modified parser on DEPFIX, a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input. Both parser modifications led to improvements in BLEU score; their combination was evaluated manually, showing a statistically significant improvement of the translation quality."
W12-3132,Formemes in {E}nglish-{C}zech Deep Syntactic {MT},2012,20,10,3,0.687831,2976,ondvrej duvsek,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"One of the most notable recent improvements of the TectoMT English-to-Czech translation is a systematic and theoretically supported revision of formemes---the annotation of morpho-syntactic features of content words in deep dependency syntactic structures based on the Prague tectogrammatics theory. Our modifications aim at reducing data sparsity, increasing consistency across languages and widening the usage area of this markup. Formemes can be used not only in MT, but in various other NLP tasks."
zeman-etal-2012-hamledt,{H}amle{DT}: To Parse or Not to Parse?,2012,24,34,3,0,5828,daniel zeman,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We propose HamleDT â HArmonized Multi-LanguagE Dependency Treebank. HamleDT is a compilation of existing dependency treebanks (or dependency conversions of other treebanks), transformed so that they all conform to the same annotation style. While the license terms prevent us from directly redistributing the corpora, most of them are easily acquirable for research purposes. What we provide instead is the software that normalizes tree structures in the data obtained by the user from their original providers."
bojar-etal-2012-joy,The Joy of Parallelism with {C}z{E}ng 1.0,2012,15,34,9,0.266636,292,ondvrej bojar,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"CzEng 1.0 is an updated release of our Czech-English parallel corpus, freely available for non-commercial research or educational purposes. In this release, we approximately doubled the corpus size, reaching 15 million sentence pairs (about 200 million tokens per language). More importantly, we carefully filtered the data to reduce the amount of non-matching sentence pairs. CzEng 1.0 is automatically aligned at the level of sentences as well as words. We provide not only the plain text representation, but also automatic morphological tags, surface syntactic as well as deep syntactic dependency parse trees and automatic co-reference links in both English and Czech. This paper describes key properties of the released resource including the distribution of text domains, the corpus data formats, and a toolkit to handle the provided rich annotation. We also summarize the procedure of the rich annotation (incl. co-reference resolution) and of the automatic filtering. Finally, we provide some suggestions on exploiting such an automatically annotated sentence-parallel corpus."
W11-2101,A Grain of Salt for the {WMT} Manual Evaluation,2011,15,41,3,0.325442,292,ondvrej bojar,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"The Workshop on Statistical Machine Translation (WMT) has become one of ACL's flagship workshops, held annually since 2006. In addition to soliciting papers from the research community, WMT also features a shared translation task for evaluating MT systems. This shared task is notable for having manual evaluation as its cornerstone. The Workshop's overview paper, playing a descriptive and administrative role, reports the main results of the evaluation without delving deep into analyzing those results. The aim of this paper is to investigate and explain some interesting idiosyncrasies in the reported results, which only become apparent when performing a more thorough analysis of the collected annotations. Our analysis sheds some light on how the reported results should (and should not) be interpreted, and also gives rise to some helpful recommendation for the organizers of WMT."
W11-2153,Influence of Parser Choice on Dependency-Based {MT},2011,18,9,1,1,227,martin popel,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,Accuracy of dependency parsers is one of the key factors limiting the quality of dependency-based machine translation. This paper deals with the influence of various dependency parsing approaches (and also different training data size) on the overall performance of an English-to-Czech dependency-based statistical translation system implemented in the Treex framework. We also study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of BLEU.
W10-1730,Maximum Entropy Translation Model in Dependency-Based {MT} Framework,2010,18,13,2,0.854143,7153,zdenvek vzabokrtsky,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"Maximum Entropy Principle has been used successfully in various NLP tasks. In this paper we propose a forward translation model consisting of a set of maximum entropy classifiers: a separate classifier is trained for each (sufficiently frequent) source-side lemma. In this way the estimates of translation probabilities can be sensitive to a large number of features derived from the source sentence (including non-local features, features making use of sentence syntactic structure, etc.). When integrated into English-to-Czech dependency-based translation scenario implemented in the TectoMT framework, the new translation model significantly outperforms the baseline model (MLE) in terms of BLEU. The performance is further boosted in a configuration inspired by Hidden Tree Markov Models which combines the maximum entropy translation model with the target-language dependency tree model."
W09-0422,{E}nglish-{C}zech {MT} in 2008,2009,15,13,4,0,292,ondvrej bojar,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,We describe two systems for English-to-Czech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach.
P09-2037,Hidden {M}arkov Tree Model in Dependency-based Machine Translation,2009,11,15,2,0,7153,zdenvek vzabokrtsky,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"We would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of dependency trees. In particular, we show that the transfer phase in a Machine Translation system based on tectogrammatical dependency trees can be seen as a task suitable for HMTM. When using the HMTM approach for the English-Czech translation, we reach a moderate improvement over the baseline."
