2020.aacl-main.6,P19-1033,1,0.652179,"categories, user profiles and features extracted from user behavior histories. They ranked candidate news based on the click scores computed by a neural factorization machine. However, these methods rely on manual feature engineering to build news and user representations, which usually necessitate massive expertise. In addition, handcrafted features may not be optimal in representing news content and user interest. In recent years, several news recommendation methods based on deep learning techniques are proposed (Okura et al., 2017; Khattar et al., 2018; Wang et al., 2018; Wu et al., 2019a; An et al., 2019; Wu et al., 2019b,c; Ge et al., 2020). For example, Okura et al. (2017) proposed to learn first news representations from news bodies using autoencoders, and then learn representations of users from their clicked news with a GRU network. Candidate news are ranked based on the click scores computed by the dot products between news and user representations. Wang et al. (2018) proposed to learn news representations from news titles and • To the best of our knowledge, this is the first work that explores to improve the sentiment diversity of news recommendation. • We propose a sentiment-aware new"
2020.aacl-main.6,D19-1671,1,0.675314,"Missing"
2020.aacl-main.6,D14-1162,0,0.0841738,"i=1 10 X i ), sci ), (8) sci ), i=1 where C is the number of candidate news in an impression, sci denotes the sentiment score of the candidate news with the i-th highest click score. In these metrics, higher scores indicate that the recommendation results are less diverse from the browsed news in their sentiment.9 We repeated each experiment 10 times and reported the average results over all impressions in terms of the recommendation performance and sentiment diversity. Following Wu et al. (2019b), in our experiments the word embeddings were initialized by the 300dimensional Glove embeddings (Pennington et al., 2014). The negative sampling ratio K was set to 4. Adam (Kingma and Ba, 2014) was chosen as the optimizer and the size of a minibatch was 30. In addition, the loss weights λ and µ were respectively set to 0.4 and 10. These hyperparameters were tuned on the validation set. To evaluate the performance of news recommendation, we use metrics including AUC, MRR, nDCG@5 and nDCG@108 . 4.2 Performance Evaluation We evaluate the recommendation performance and sentiment diversity of our approach by comparing it with several baseline methods, including: (1) LibFM (Rendle, 2012), a feature-based recommendatio"
2020.acl-main.267,D16-1171,0,0.0144082,"e representations to build the representations of an entire document. Although average pooling is computationally efficient, it cannot distinguish important contexts from unimportant ones, which may not be optimal for learning accurate text representations. There are also other popular pooling methods that can select salient features to learn more informative text representations, such as max pooling (Kim, 2014; Zhang et al., 2015) and attentive pooling (Yang et al., 2016), which are employed by many neural NLP methods (Collobert et al., 2011; Kim, 2014; Huang et al., 2012; Yang et al., 2016; Chen et al., 2016; Zhou et al., 2016; Du et al., 2017; Li et al., 2018; Wu et al., 2019a; Tao et al., 2019; Devlin et al., 2019; Wu et al., 2019b). For example, Collobert et al. (2011) proposed to learn representations of contexts within each window using feed forward neural networks, and used max pooling to build final text representations. Kim (2014) proposed to apply max pooling over time to the contextual word representations learned by multiple CNN filters. Huang et al. (2012) proposed to build representations of the entire document using the summation of word representations weighted by their TF-IDF scor"
2020.acl-main.267,P17-1055,0,0.0312344,"ing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average and attentive pooling and L∞ norm for max pooling, to build text representations"
2020.acl-main.267,N19-1423,0,0.215536,"ion (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average and attentive pooling and L∞ norm for max pooling, to build text representations, which may not be op"
2020.acl-main.267,P12-1092,0,0.0717581,"rage pooling to the sequence of sentence representations to build the representations of an entire document. Although average pooling is computationally efficient, it cannot distinguish important contexts from unimportant ones, which may not be optimal for learning accurate text representations. There are also other popular pooling methods that can select salient features to learn more informative text representations, such as max pooling (Kim, 2014; Zhang et al., 2015) and attentive pooling (Yang et al., 2016), which are employed by many neural NLP methods (Collobert et al., 2011; Kim, 2014; Huang et al., 2012; Yang et al., 2016; Chen et al., 2016; Zhou et al., 2016; Du et al., 2017; Li et al., 2018; Wu et al., 2019a; Tao et al., 2019; Devlin et al., 2019; Wu et al., 2019b). For example, Collobert et al. (2011) proposed to learn representations of contexts within each window using feed forward neural networks, and used max pooling to build final text representations. Kim (2014) proposed to apply max pooling over time to the contextual word representations learned by multiple CNN filters. Huang et al. (2012) proposed to build representations of the entire document using the summation of word represe"
2020.acl-main.267,D14-1181,0,0.0924972,"esentation vector from a collection of input feature vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average an"
2020.acl-main.267,P15-1098,0,0.128348,"of text representation. 1 The visualized weights of max pooling are summations of the maximum elements over time for each word. 2 https://github.com/wuch15/ACL2020-APLN 2 Related Work Neural networks are widely used to learn text representations from contexts (Peng et al., 2018). Pooling is usually an essential step in these methods to build contextual representations by summarizing the information of input features (LeCun et al., 2015). The simplest pooling method is average pooling, which is used in many approaches to construct text representations (Tang et al., 2014, 2015a,b). For example, Tang et al. (2015a) proposed to apply average pooling to the output of CNN filters to capture global contexts in a sentence. In addition, they also proposed to average the sentence representations learned by parallel CNN networks with different window sizes. In their another work (Tang et al., 2015b), they proposed to apply average pooling to the sequence of sentence representations to build the representations of an entire document. Although average pooling is computationally efficient, it cannot distinguish important contexts from unimportant ones, which may not be optimal for learning accurate text represen"
2020.acl-main.267,P14-1146,0,0.305955,"urately will facilitate many NLP methods (Ma et al., 2017). Introduction In recent years, neural network based methods are widely used in the natural language processing (NLP) field to learn text representations (Yang et al., 2016; Peters et al., 2018). In these methods, pooling is a core technique to build the text representation vector from a collection of input feature vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build th"
2020.acl-main.267,N19-1180,1,0.904864,"). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average and attentive pooling and L∞ norm for max pooling, to build text representations, which may not be optimal when handli"
2020.acl-main.267,D14-1162,0,0.0975115,"ts with different characteristics. The first one is AG’s News5 , which is a news topic classification dataset. Following (Zhang et al., 2015), we only use the title and description fields in this dataset. The second one is IMDB6 (Diao et al., 2014), which is a dataset with movie reviews and ratings. The third one is Amazon Electronics 4 In experiments on a machine with a GTX1080ti GPU, the computation of xp is accelerated by more than 10 times. 5 https://www.di.unipi.it/en/ 6 https://github.com/nihalb/JMARS In our experiments, the word embeddings were 300-dimensional and initialized by Glove (Pennington et al., 2014)7 . In our comparative experiments, the CNN networks had 400 filters, and their window size was 3. The dimension of LSTM hidden states was 200. The attention query vectors were 200-dimensional. The initial pooling norm p was set to 1, which is consistent with the vanilla attentive pooling. Adam (Kingma and Ba, 2014) was used as the optimizer, and the 7 We do not use language models such as ELMo and BERT since our work focuses on facilitating the pooling technique rather than boosting the performance of our approach against the state-of-the-art methods. 2965 Methods CNN-Avg CNN-Max CNN-Att CNN-"
2020.acl-main.267,N16-1174,0,0.84745,"kes Heavy Toll on Wildlife Max Pooling Fire on Queensland Island Takes Heavy Toll on Wildlife Attentive Pooling Fire on Queensland Island Takes Heavy Toll on Wildlife News Topic Classification Figure 1: The pooling weights of several different pooling methods on the representations produced by an LSTM network. Darker colors indicate higher weights. that can select salient features accurately will facilitate many NLP methods (Ma et al., 2017). Introduction In recent years, neural network based methods are widely used in the natural language processing (NLP) field to learn text representations (Yang et al., 2016; Peters et al., 2018). In these methods, pooling is a core technique to build the text representation vector from a collection of input feature vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson"
2020.acl-main.267,N18-1202,0,0.0275927,"ildlife Max Pooling Fire on Queensland Island Takes Heavy Toll on Wildlife Attentive Pooling Fire on Queensland Island Takes Heavy Toll on Wildlife News Topic Classification Figure 1: The pooling weights of several different pooling methods on the representations produced by an LSTM network. Darker colors indicate higher weights. that can select salient features accurately will facilitate many NLP methods (Ma et al., 2017). Introduction In recent years, neural network based methods are widely used in the natural language processing (NLP) field to learn text representations (Yang et al., 2016; Peters et al., 2018). In these methods, pooling is a core technique to build the text representation vector from a collection of input feature vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thu"
2020.acl-main.267,D15-1167,0,0.205443,"of text representation. 1 The visualized weights of max pooling are summations of the maximum elements over time for each word. 2 https://github.com/wuch15/ACL2020-APLN 2 Related Work Neural networks are widely used to learn text representations from contexts (Peng et al., 2018). Pooling is usually an essential step in these methods to build contextual representations by summarizing the information of input features (LeCun et al., 2015). The simplest pooling method is average pooling, which is used in many approaches to construct text representations (Tang et al., 2014, 2015a,b). For example, Tang et al. (2015a) proposed to apply average pooling to the output of CNN filters to capture global contexts in a sentence. In addition, they also proposed to average the sentence representations learned by parallel CNN networks with different window sizes. In their another work (Tang et al., 2015b), they proposed to apply average pooling to the sequence of sentence representations to build the representations of an entire document. Although average pooling is computationally efficient, it cannot distinguish important contexts from unimportant ones, which may not be optimal for learning accurate text represen"
2020.acl-main.267,D16-1024,0,0.33966,"vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average and attentive pooling and L∞ norm for max pooling, to build te"
2020.acl-main.331,P19-1033,1,0.648302,"//msnews.github.io. 1 Introduction Online news services such as Google News and Microsoft News have become important platforms for a large population of users to obtain news information (Das et al., 2007; Wu et al., 2019a). Massive news articles are generated and posted online every day, making it difficult for users to find interested news quickly (Okura et al., 2017). Personalized news recommendation can help users alleviate information overload and improve news reading experience (Wu et al., 2019b). Thus, it is widely used in many online news platforms (Li et al., 2011; Okura et al., 2017; An et al., 2019). In traditional recommender systems, users and items are usually represented using IDs, and their interactions such as rating scores are used to learn ID representations via methods like collaborative filtering (Koren, 2008). However, news recommendation has some special challenges. First, news articles on news websites update very quickly. New news articles are posted continuously, and existing news articles will expire in short time (Das et al., 2007). Thus, the cold-start problem is very severe in news recommendation. Second, news articles contain rich textual information such as title and"
2020.acl-main.331,N19-1423,0,0.0292461,"ove the performance of different neural text representation methods such as CNN and LSTM for news recommendation. It shows that selecting important words in news texts using attention can help learn more informative news representations. Another interesting finding is that the combination of LSTM and attention can achieve the best performance. However, to our best knowledge, it is not used in existing news recommendation methods. 5.3.2 Pre-trained Language Models Next, we explore whether the quality of news representation can be further improved by the pretrained language models such as BERT (Devlin et al., 2019), which have achieved huge success in different NLP tasks. We applied BERT to the news representation module of three state-of-theart news recommendation methods, i.e., NAML, LSTUR and NRMS. The results are summarized in Fig. 3. We find that by replacing the original word embedding module with the pre-trained BERT model, the performance of different news recommendation methods can be improved. It shows the BERT model pre-trained on large-scale corpus like Wikipedia can provide useful semantic information for news representation. We also find that fine-tuning the pre-trained BERT model with the"
2020.acl-main.331,D14-1162,0,0.0942926,"ify and compare the methods introduced in Section 4 on the MIND dataset. Since most of these news recommendation methods are based on news titles, for fair comparison, we only used news titles in experiments unless otherwise mentioned. We will explore the usefulness of different news texts such as body in Section 5.3.3. In order to simulate the practical news recommendation scenario where we always have unseen users not included in training data, we randomly sampled half of the users for training, and used all the users for test. For those methods that need word embeddings, we used the Glove (Pennington et al., 2014) as initialization. Adam was used as the optimizer. Since the non-clicked news are usually much more than the clicked news in each impression log, following (Wu et al., 2019b) we applied negative sampling technique to model training. All hyper-parameters were selected according to the results on the validation set. The metrics used in our experiments are AUC, MRR, nDCG@5 and nDCG@10, which are standard metrics for recommendation result evaluation. Each experiment was repeated 10 times. 3601 Overall LibFM DSSM Wide&Deep DeepFM DFM GRU DKN NPA NAML LSTUR NRMS Overlap Users Unseen Users AUC MRR n"
2020.acl-main.331,D16-1264,0,0.0417148,"and body. It is not appropriate to simply representing them using IDs, and it is important to understand their content from their texts (Kompan and Bielikov´a, 2010). Third, there is no explicit rating of news articles posted by users on news platforms. Thus, in news recommendation users’ interest in news is usually inferred from their click behaviors in an implicit way (Ilievski and Roy, 2013). A large-scale and high-quality dataset can significantly facilitate the research in an area, such as ImageNet for image classification (Deng et al., 2009) and SQuAD for machine reading comprehension (Rajpurkar et al., 2016). There are several public datasets for traditional recommendation tasks, such as Amazon dataset1 for product recommendation and MovieLens dataset2 for movie recommendation. Based on these datasets, many well-known recommendation methods have been developed. However, existing studies on news recommendation are much fewer, and many of them are conducted on proprietary datasets (Okura et al., 2017; Wang et al., 2018; Wu et al., 2019a). Although there are a few public datasets for news recommendation, they are usually in small size and most of them are not in English. Thus, a public 1 2 http://jm"
2020.acl-main.331,D19-1671,1,0.849233,"Missing"
2020.acl-main.331,N16-1174,0,0.052786,"dy (AMV) + Body + Cat. (AMV) + Body + Cat. + Ent. (AMV) AUC MRR nDCG@5 nDCG@10 66.22 64.17 66.32 67.07 67.09 67.23 67.38 67.50 67.60 31.92 30.49 31.88 32.34 32.40 32.41 32.37 32.43 32.51 34.53 32.81 34.42 34.98 35.03 35.04 35.12 35.21 35.24 40.23 38.57 40.22 40.74 40.80 40.83 40.79 40.96 41.03 Table 5: News representation with different news information. “Abs.”, “Cat.” and “Ent.” mean abstract, category and entity, respectively. Figure 3: BERT for news representation. ding (Avg-Emb), CNN, LSTM and multi-head selfattention (Self-Att). Since attention mechanism is an important technique in NLP (Yang et al., 2016), we also apply it to the aforementioned neural text representation methods. The results are in Table 4. We have several findings from the results. First, neural text representation methods such as CNN, Self-Att and LSTM can outperform traditional text representation methods like TF-IDF and LDA. This is because the neural text representation models can be learned with the news recommendation task, and they can capture the contexts of texts to generate better news representations. Second, Self-Att and LSTM outperform CNN in news representation. This is because multi-head self-attention and LSTM"
2020.acl-main.77,D14-1181,0,0.00292681,"linear transformations and a deep channel with multiple dense layers. The same features with LibFM are used for both channels; (4) DeepFM (Guo et al., 2017), combining factorization machines and deep neural networks with the same features as LibFM. Neural Recommendation Methods: Neural networks specially designed for news recommendation, including (1) DFM (Lian et al., 2018), a deep fusion model combining dense layers with different depths and using attention mechanism to select important features; (2) DKN (Wang et al., 2018), incorporating entity information in knowledge graphs with Kim CNN (Kim, 2014) to learn news representations and using news-level attention network to learn user representations; (3) GRU (Okura et al., 2017), using auto-encoders to represent news and a GRU network to represent users; (4) NRMS (Wu et al., 2019e), leveraging multi-head self-attentions for news and user representation learning; (5) HiFi Ark (Liu et al., 2019), summarizing user history into highly compact and complementary vectors as archives, and learning candidate-dependent user AUC 0.5661 0.5949 0.5812 0.5830 0.5861 0.6032 0.6102 0.6275 0.6027 0.6243 0.6359? 0.6258 0.6319 representation via attentive agg"
2020.acl-main.77,P19-1033,1,0.583815,"; Son et al., 2013; Li et al., 2014; Bansal et al., 2015). For example, Liu et al. (2010) used topic categories and interest features generated by a Bayesian model to build news and user representations. Son et al. (2013) extracted topic and location features from Wikipedia pages to build news representations for locationbased news recommendation. In recent years, deep learning based models have achieved better performance than traditional methods for news recommendation, due to their capabilities of distilling implicit semantic features in news content (Okura et al., 2017; Wang et al., 2018; An et al., 2019; Wu et al., 2019e,d). For example, Okura et al. (2017) learned news representations via denoising auto-encoders, then used recurrent neural networks to aggregate historical browsed 837 News-by-News Matching HDC, dilation=1 HDC, dilation=2 HDC, dilation=3 HDC, dilation=1 Historical Browsed News HDC, dilation=2 HDC, dilation=3 3D CNN ... ... ... ... ... HDC, dilation=1 3D Matching Image Q HDC, dilation=2 HDC, dilation=3 HDC, dilation=1 Candidate News HDC, dilation=2 HDC, dilation=3 Word Embedding Multi-grained News Representation News Representation Module Matching Matrices Aggregation Cross Ma"
2020.acl-main.77,D14-1162,0,0.0887772,"Missing"
2020.acl-main.77,P19-1110,1,0.784156,"Missing"
2020.acl-main.77,D19-1493,1,0.496606,"Missing"
2020.acl-main.77,D19-1671,1,0.64663,"Missing"
2020.acl-main.77,P18-1103,0,0.0211702,"}n×Ndk ×Nc , (3) where n denotes the total number of browsed news in user history, and each pixel Qk,i,j is defined as: Qk,i,j = [Mlk,c [i, j]]L l=0 . (4) Specifically, each pixel is a concatenated vector with L + 1 channels, indicating the matching degrees between a certain segment pair of the news content at different levels of granularity. As user’s click behaviors may be driven by personalized interests or temporary demands and events, different historical browsed news has different usefulness and representativeness for matching 839 and recommending the proper candidate news. Inspired by Zhou et al. (2018) in the issue of dialogue system, we resemble the compositional hierarchy of image recognition, and employ a layered 3D convolution & max-pooling neural network to identify the salient matching signals from the whole image. The 3D convolution is the extension of typical 2D convolution, whose filters and strides are 3D cubes. Formally, the higher-order pixel at (k, i, j) on the z-th feature map of the t-th layer is computed as: (t,z) Qk,i,j = ELU t−1H t−1R t−1 XW X X X z 0 w=0 h=0 r=0 4 , (5) where z 0 denotes each feature map of the previous layer, K(t,z) ∈ RWt ×Ht ×Rt is a 3D convolution kern"
2020.ccl-1.106,W17-4215,0,0.0257596,"na National Conference on Computational Linguistics, pages 1143-1154, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics 2 Related Work CC L 20 20 Automatic detection of clickbaits is important for online platforms to purify their web content and improve user experience. Traditional clickbait detection methods usually rely on handcrafted features to build representations of webpages (Chen et al., 2015; Biyani et al., 2016; Potthast et al., 2016; Chakraborty et al., 2016; Bourgonje et al., 2017; Cao et al., 2017; Indurthi and Oota, 2017; Gec¸kil et al., 2018). For example, Chen et al. (2015) proposed to represent news articles with semantic features (e.g., unresolved pronouns, affective words, suspenseful language and overuse numerals), syntax features (e.g., forward reference and reverse narrative) and image features (e.g., image placement and emotional content). In addition, they incorporate users’ behaviors on news, like reading time, sharing and commenting, to enhance news representation. They use various classification models like Naive Bayes and SVM to identify clickbaits base"
2020.ccl-1.106,D14-1162,0,0.0879086,"Missing"
2020.ccl-1.106,N16-1174,0,0.0595971,"fective neural architecture for context modeling. Thus, we apply two independent Transformers to learn hidden representations of words in title and body by modeling their contexts. We denote the hidden representation sequences of words in title and body as Et = [et1 , et2 , ..., etN ] and Eb = [eb1 , eb2 , ..., ebP ], respectively. Different words in a title or body may have different importance for modeling the content. For instance, the word “MUST” in Fig. 1 is more important than the word “About” in learning title representation for clickbait detection. Thus, we apply attention mechanisms (Yang et al., 2016) to select words in the title and body to form unified representations for them (denoted as et and eb ), which are respectively formulated as follows: 3.2 et = Attention([et1 , et2 , ..., etN ]), (1) eb = Attention([eb1 , eb2 , ..., ebP ]). (2) Style Modeling The style modeling module is used to capture the stylistic patterns in the title to better identify clickbaits. Usually, there are some common patterns on the style of clickbait titles. For example, many clickbaits use all-capital words (e.g., “MUST”, “NOT” and “THIS”), exclamation marks, and numeric characters to attract users’ attention"
2020.ccl-1.85,C18-1139,0,0.0222805,"as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach name"
2020.ccl-1.85,P19-1587,0,0.0193625,"e entity dictionary knowledge and contextual information into the NER model. Extensive experiments show our approach can effectively exploit entity dictionaries to improve the performance of various NER models and reduce their dependence on labeled data. 2 Related Work CC L 20 20 Named entity recognition is usually modeled as a sequence labeling problem (Wan et al., 2011). Many traditional NER methods are based on statistical sequence modeling methods, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Cohen and Sarawagi, 2004; Ratinov and Roth, 2009; Passos et al., 2014; Arora et al., 2019). Usually, a core problem in these methods is how to build the feature vector for each word, and these features are traditionally constructed via manual feature engineering (Ratinov and Roth, 2009). For example, Ratinov and Roth (2009) used many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted fea"
2020.ccl-1.85,Q16-1026,0,0.477774,"aims to extract entity names from texts and classify them into several pre-defined categories, such as person, location and organization (Levow, 2006). It is an important task in natural language processing, and a prerequisite for many downstream applications such as entity linking (Derczynski et al., 2015) and relation extraction (Lin et al., 2016; Luo et al., 2018; Zeng et al., 2018). Thus, NER is a hot research topic. In this paper, we focus on the English NER task. Many methods have been proposed for English NER, and most of them model this task as a word-level sequence labeling problem (Chiu and Nichols, 2016). For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for English NER. They used CNN to learn word representations from characters, LSTM to model the contexts of words, and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to annotate. When training data is scarce, their performance usually significantly declines (Peng et al., 2019). In addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, ma"
2020.ccl-1.85,D18-1217,0,0.0142394,"italization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach named TagLM for NER by pre-training a lang"
2020.ccl-1.85,J81-4005,0,0.704455,"Missing"
2020.ccl-1.85,N19-1423,0,0.37551,"o incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach named TagLM for NER by pre-training a language model on a large corpus to provide contextualized word r"
2020.ccl-1.85,D19-1096,0,0.0194451,"nowledge to design these dictionary based features, and these handcrafted features may not be optimal. Different from these methods, in our approach we introduce a term-level classification task to exploit the useful information in entity dictionary without manual feature engineering. We jointly train our model in both the NER and term classification tasks to enhance the performance of NER model in an end-to-end manner. There are also a few methods that explore to incorporate dictionary knowledge into Chinese NER models in an end-to-end manner by using graph neural networks (Sui et al., 2019; Gui et al., 2019). For example, Sui et al. (2019) propose a character-based collaborative graph neural network to learn the representations of characters and words matched by dictionaries from three word-character graphs, i.e., a containing graph that describes the connection between characters and matched words, a transition graph that builds the connections between characters and the nearest contextual matched words, and a Lattice graph that connects each word with its boundary characters. However, these methods mainly model the interactions between matched entities and their local contexts, while ignore the"
2020.ccl-1.85,C12-1071,0,0.115466,"CRF model for English NER. They used CNN to learn word representations from characters, LSTM to model the contexts of words, and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to annotate. When training data is scarce, their performance usually significantly declines (Peng et al., 2019). In addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NER (Liu et al., 2019; Magnolini et al., 2019) and most of them are based on dictionary matching features. For example, Wang et al. (2019) proposed to combine token matching features with token embeddings and LSTM outputs. However, in many cases"
2020.ccl-1.85,N16-1030,0,0.0305721,"many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) prop"
2020.ccl-1.85,W06-0115,0,0.0603833,"a context-dictionary attention. In addition, we propose an auxiliary term classification task to predict the types of the matched entity names, and jointly train it with the NER model to fuse both contexts and dictionary knowledge into NER. Extensive experiments on the CoNLL-2003 benchmark dataset validate the effectiveness of our approach in exploiting entity dictionaries to improve the performance of various NER models. CC L Named entity recognition (NER) aims to extract entity names from texts and classify them into several pre-defined categories, such as person, location and organization (Levow, 2006). It is an important task in natural language processing, and a prerequisite for many downstream applications such as entity linking (Derczynski et al., 2015) and relation extraction (Lin et al., 2016; Luo et al., 2018; Zeng et al., 2018). Thus, NER is a hot research topic. In this paper, we focus on the English NER task. Many methods have been proposed for English NER, and most of them model this task as a word-level sequence labeling problem (Chiu and Nichols, 2016). For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for English NER. They used CNN to learn word representations fro"
2020.ccl-1.85,D17-1282,0,0.0144347,"features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et"
2020.ccl-1.85,D18-1226,0,0.019237,"s, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach named TagLM for NER by"
2020.ccl-1.85,P16-1200,0,0.0384072,"contexts and dictionary knowledge into NER. Extensive experiments on the CoNLL-2003 benchmark dataset validate the effectiveness of our approach in exploiting entity dictionaries to improve the performance of various NER models. CC L Named entity recognition (NER) aims to extract entity names from texts and classify them into several pre-defined categories, such as person, location and organization (Levow, 2006). It is an important task in natural language processing, and a prerequisite for many downstream applications such as entity linking (Derczynski et al., 2015) and relation extraction (Lin et al., 2016; Luo et al., 2018; Zeng et al., 2018). Thus, NER is a hot research topic. In this paper, we focus on the English NER task. Many methods have been proposed for English NER, and most of them model this task as a word-level sequence labeling problem (Chiu and Nichols, 2016). For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for English NER. They used CNN to learn word representations from characters, LSTM to model the contexts of words, and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to a"
2020.ccl-1.85,P19-1524,0,0.0603402,"n addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NER (Liu et al., 2019; Magnolini et al., 2019) and most of them are based on dictionary matching features. For example, Wang et al. (2019) proposed to combine token matching features with token embeddings and LSTM outputs. However, in many cases entities are context-dependent. For instance, in Table 1, the word “Jordan” can be a person name or a location name in different contexts. Thus, it is not optimal to directly apply entity dictionaries to NER without considering the contexts. 1 https://www.geonames.org Proceedings of the 19th China National Conference on Computational Linguistics, pages 915-926, Hainan, Chi"
2020.ccl-1.85,W19-5807,0,0.0181192,"performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NER (Liu et al., 2019; Magnolini et al., 2019) and most of them are based on dictionary matching features. For example, Wang et al. (2019) proposed to combine token matching features with token embeddings and LSTM outputs. However, in many cases entities are context-dependent. For instance, in Table 1, the word “Jordan” can be a person name or a location name in different contexts. Thus, it is not optimal to directly apply entity dictionaries to NER without considering the contexts. 1 https://www.geonames.org Proceedings of the 19th China National Conference on Computational Linguistics, pages 915-926, Hainan, China, October 30 - Novermbe"
2020.ccl-1.85,E14-1048,0,0.0135247,"and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to annotate. When training data is scarce, their performance usually significantly declines (Peng et al., 2019). In addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NER (Liu et al., 2019; Magnolini et al., 2019) and most of them are based on dictionary matching features. For example, Wang et al. (2019) proposed to combine token matching features with token embeddings and LSTM outputs. However, in many cases entities are context-dependent. For instance, in Table 1, the word “Jordan” can be a person name or a location name in different c"
2020.ccl-1.85,W14-1609,0,0.142949,"n model to incorporate entity dictionary knowledge and contextual information into the NER model. Extensive experiments show our approach can effectively exploit entity dictionaries to improve the performance of various NER models and reduce their dependence on labeled data. 2 Related Work CC L 20 20 Named entity recognition is usually modeled as a sequence labeling problem (Wan et al., 2011). Many traditional NER methods are based on statistical sequence modeling methods, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Cohen and Sarawagi, 2004; Ratinov and Roth, 2009; Passos et al., 2014; Arora et al., 2019). Usually, a core problem in these methods is how to build the feature vector for each word, and these features are traditionally constructed via manual feature engineering (Ratinov and Roth, 2009). For example, Ratinov and Roth (2009) used many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing t"
2020.ccl-1.85,P19-1231,0,0.0191113,"we focus on the English NER task. Many methods have been proposed for English NER, and most of them model this task as a word-level sequence labeling problem (Chiu and Nichols, 2016). For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for English NER. They used CNN to learn word representations from characters, LSTM to model the contexts of words, and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to annotate. When training data is scarce, their performance usually significantly declines (Peng et al., 2019). In addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NE"
2020.ccl-1.85,P17-1161,0,0.341377,"istories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better perfo"
2020.ccl-1.85,N18-1202,0,0.21204,"4) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supe"
2020.ccl-1.85,W09-1119,0,0.57015,"d the term classification model to incorporate entity dictionary knowledge and contextual information into the NER model. Extensive experiments show our approach can effectively exploit entity dictionaries to improve the performance of various NER models and reduce their dependence on labeled data. 2 Related Work CC L 20 20 Named entity recognition is usually modeled as a sequence labeling problem (Wan et al., 2011). Many traditional NER methods are based on statistical sequence modeling methods, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Cohen and Sarawagi, 2004; Ratinov and Roth, 2009; Passos et al., 2014; Arora et al., 2019). Usually, a core problem in these methods is how to build the feature vector for each word, and these features are traditionally constructed via manual feature engineering (Ratinov and Roth, 2009). For example, Ratinov and Roth (2009) used many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word represe"
2020.ccl-1.85,P17-1194,0,0.0173022,"et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017)"
2020.ccl-1.85,S13-2058,0,0.0739984,"Missing"
2020.ccl-1.85,D19-1396,0,0.017795,"s rely on domain knowledge to design these dictionary based features, and these handcrafted features may not be optimal. Different from these methods, in our approach we introduce a term-level classification task to exploit the useful information in entity dictionary without manual feature engineering. We jointly train our model in both the NER and term classification tasks to enhance the performance of NER model in an end-to-end manner. There are also a few methods that explore to incorporate dictionary knowledge into Chinese NER models in an end-to-end manner by using graph neural networks (Sui et al., 2019; Gui et al., 2019). For example, Sui et al. (2019) propose a character-based collaborative graph neural network to learn the representations of characters and words matched by dictionaries from three word-character graphs, i.e., a containing graph that describes the connection between characters and matched words, a transition graph that builds the connections between characters and the nearest contextual matched words, and a Lattice graph that connects each word with its boundary characters. However, these methods mainly model the interactions between matched entities and their local context"
2020.ccl-1.85,W03-0419,0,0.540528,"Missing"
2020.ccl-1.85,I11-1096,0,0.0346375,"on, we propose an auxiliary term classification task to predict the types of the matched entity names in different contexts. Besides, we propose a unified framework to jointly train the NER model and the term classification model to incorporate entity dictionary knowledge and contextual information into the NER model. Extensive experiments show our approach can effectively exploit entity dictionaries to improve the performance of various NER models and reduce their dependence on labeled data. 2 Related Work CC L 20 20 Named entity recognition is usually modeled as a sequence labeling problem (Wan et al., 2011). Many traditional NER methods are based on statistical sequence modeling methods, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Cohen and Sarawagi, 2004; Ratinov and Roth, 2009; Passos et al., 2014; Arora et al., 2019). Usually, a core problem in these methods is how to build the feature vector for each word, and these features are traditionally constructed via manual feature engineering (Ratinov and Roth, 2009). For example, Ratinov and Roth (2009) used many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014)"
2020.ccl-1.85,I08-4016,0,0.0439182,"ctional pre-trained language model named BERT, which can empower downstream tasks like NER by using deep Transformers (Vaswani et al., 2017) to model contexts accurately. However, these neural network based methods heavily rely on labeled sentences to train NER models, which need heavy effort of manual annotation. In addition, their performance on recognizing entities which rarely or do not appear in labeled data is usually unsatisfactory (Wang et al., 2019). There are several approaches on utilizing entity dictionaries for named entity recognition (Cohen and Sarawagi, 2004; Lin et al., 2007; Yu et al., 2008; Rockt¨aschel et al., 2013; Passos et al., 2014; Song et al., 2015; Wang et al., 2019; Liu et al., 2019). In traditional methods, dictionaries are often incorporated as additional features. For example, Cohen et al. (2004) proposed to extract dictionary features based on entity matching and similarities, and they incorporated these features into an HMM based model. There are also a few methods to incorporate dictionary knowledge into neural NER models (Chiu and Nichols, 2016; Wang et al., 2019; Liu et al., 2019). For example, Wang et al. (2019) proposed to incorporate dictionaries into neural"
2020.ccl-1.85,N19-1342,0,0.0720066,"g features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach named TagLM for NER by pre-training a language model on a large corpus to provide"
2020.findings-emnlp.128,P19-1033,1,0.441834,"ssive news articles are posted online every day, users of online news services face heavy information overload (Zheng et al., 2018). Different users usually prefer different news information. Thus, personalized news recommendation, which aims to display news articles to users based on their personal interest, is a useful technique to improve user experience and has been widely used in many online news services (Wu et al., 2019b). The research of news recommendation has attracted many attentions from both academic and industrial fields (Okura et al., 2017; Wang et al., 2018; Lian et al., 2018; An et al., 2019; Wu et al., 2019a). Many news recommendation methods have been proposed in recent years (Wang et al., 2018; Wu et al., 2019b; Zhu et al., 2019b). These methods usually recommend news based on the matching between the news representation learned from news content and the user interest representation learned from historical user behaviors on news. For example, Okura et al. (2017) proposed to learn news representations from the content of news articles via autoencoder, and learn user interest representations from the clicked news articles via Gated Recurrent Unit (GRU) network. They ranked the c"
2020.findings-emnlp.128,W14-4012,0,0.0583608,"Missing"
2020.findings-emnlp.128,2020.acl-main.392,1,0.848151,"i.e., the strength of the noise), we can achieve a smaller privacy budget  which means better privacy protection. However, strong noise will hurt the accuracy of aggregated gradients. Thus, λ should be selected based on the trade-off between privacy protection and model performance. 4 Experiment 4.1 Dataset and Experimental Settings Our experiments were conducted on a public news recommendation dataset (named Adressa) collected from a Norwegian news website (Gulla et al., 2017) and another real-world dataset collected from Microsoft News4 (named MSN-News).5 For the Adressa dataset, following Hu et al. (2020), we used user logs in the first five days to construct 4 https://www.msn.com/en-us Our dataset and codes will be publicly available in https://github.com/JulySinceAndrew/FedNewsRec-EMNLPFindings-2020. 5 # users # news # impressions # positive behaviors # negative behaviors avg. # title length MSN-News 100,000 118,325 1,341,853 2,006,289 48,051,601 11.52 Adressa 528,514 16,004 2,411,187 6.60 Table 1: The statistical information of the dataset. users’ click history, used logs in the 6-th day for model training, and used logs in the 7-th day for model evaluation. Since the Adressa dataset does n"
2020.findings-emnlp.128,D19-1493,1,0.566924,"Missing"
2020.findings-emnlp.128,D19-1671,1,0.695969,"Missing"
2020.findings-emnlp.174,N19-1423,0,0.146638,"s users from user behaviors on the e-commerce platform based on their relevance to the candidate ads. Okura et al. (2017) proposed to use a GRU network for news recommendation, which models users from their clicked news. However, these methods mainly rely on sufficient labeled data to train user models, and their performance may be not optimal when training data is scarce. In addition, they only model task-specific user information and do not exploit the universal user information encoded in user behaviors. In recent years, pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and XLNET (Yang et al., 2019) have achieved great success in many NLP tasks, such as reading comprehension and machine translation. Many language models are pre-trained on a large unlabeled corpus via self-supervision tasks such as masked LM and next sentence prediction to model the contexts (Devlin et al., 2019). These language models can learn universal language representations from large unlabeled corpus and empower many different downstream tasks when the labeled data for these tasks is insufficient (Qiu et al., 2020). Motivated by pre-trained language models, in this paper we propose pre"
2020.findings-emnlp.174,N16-1174,0,0.0221708,"plete hyperparameter settings and analysis are included in supplements. To evaluate the performance of different methods, we used accuracy and macro F-score on the Demo dataset, and used AUC and AP scores on the CTR dataset. Each experiment was repeated 10 times independently. 3.2 Performance Evaluation In this section, we verify the effectiveness of our proposed PTUM method for user model pretraining. We choose several state-of-the-art user models and compare their performance with their variants pre-trained by our PTUM method. On the Demo dataset, the models to be compared include: (1) HAN (Yang et al., 2016), hierarchical attention network, which uses attentional LSTM to learn behavior and user representations. (2) HURA (Wu et al., 2019c), hierarchical user representation with attention model, which uses CNN and attention networks to learn behavior and user representations. (3) HSA (Wu et al., 2019b), using hierarchical multi-head self-attention to learn behavior and user representations. On the CTR dataset, the models to be compared include: (1) GRU4Rec (Hidasi et al., 2016), using GRU networks to learn behavior and user representations. (2) NativeCTR (An et al., 2019), using CNN and attention n"
2020.findings-emnlp.174,N18-1202,0,0.0189005,"CTR) prediction, which models users from user behaviors on the e-commerce platform based on their relevance to the candidate ads. Okura et al. (2017) proposed to use a GRU network for news recommendation, which models users from their clicked news. However, these methods mainly rely on sufficient labeled data to train user models, and their performance may be not optimal when training data is scarce. In addition, they only model task-specific user information and do not exploit the universal user information encoded in user behaviors. In recent years, pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and XLNET (Yang et al., 2019) have achieved great success in many NLP tasks, such as reading comprehension and machine translation. Many language models are pre-trained on a large unlabeled corpus via self-supervision tasks such as masked LM and next sentence prediction to model the contexts (Devlin et al., 2019). These language models can learn universal language representations from large unlabeled corpus and empower many different downstream tasks when the labeled data for these tasks is insufficient (Qiu et al., 2020). Motivated by pre-trained language models,"
2020.findings-emnlp.174,D19-1671,1,0.463046,"Missing"
2021.acl-long.423,P19-1033,1,0.862875,"News (Khattar et al., 2018; Das et al., 2007). To help users efficiently obtain their interested news information, personalized news recommendation technique that aims to recommend news according to user interests, is widely used by these platforms (Wu et al., 2020a; Liu et al., 2010; Lin et al., 2014). User interest modeling is a critical step for personalized news recommendation (Wu et al., 2021; Zheng et al., 2018; Wu et al., 2020c). Existing methods usually learn a single representation vector to model overall user interests from users’ clicked news (Okura et al., 2017; Wu et al., 2020b; An et al., 2019). For example, Okura et al. (2017) used a GRU network to model user interests from clicked news. They used the latest hidden state of GRU as the user interest representation. Wu et al. (2019e) used multi-head self-attention network to capture user interests, and used an attentive pooling network to obtain a unified user representation. However, user interest is usually diverse and multigrained. For example, as shown in Fig. 1, a user may have interest in movies, sports, finance and health at the same time. In addition, for users who are interested in sports, some of them may have general inter"
2021.acl-long.423,D14-1162,0,0.0893212,"ree weeks to construct test data. Since Feeds only contains topic label of news, we implement a simplified version of HieRec with only user- and topiclevel interest representations on Feeds. Besides, following Wu et al. (2020d), users in Feeds were anonymized via hash algorithms and de-linked from the production system to protect user privacy. Detailed information is summarized in Table 1. Next, we introduce experimental settings and hyper-parameters of HieRec. We use the first 30 words and 5 entities of news titles and users’ recent 50 clicked news in experiments. We adopt pre-trained glove (Pennington et al., 2014) word embeddings and TransE entity embeddings (Bordes et al., 2013) for initialization. In HieRec, the word and entity self-attention network output 400and 100-dimensional vectors, respectively. Besides, the unified news representation is 400-dimensional. Attention networks (i.e., φs (·), φt (·), and φg (·)) are implemented by single-layer dense networks. Besides, dimensions of topic and subtopic embeddings are 400, both of which are randomly initialized and fine-tuned. The hyper-parameters for combining different interest scores, i.e. λt and λs , are set to 0.15 and 0.7 respectively. Moreover"
2021.acl-long.423,2020.findings-emnlp.128,1,0.884748,"ained, which are difficult to be accurately modeled by a single user embedding. Different from these methods, we propose a hierarchical user interest modeling framework to model user interests in different aspects and granularities. In addition, we propose a hierarchical user interest matching framework to understand user interest in candidate news from different interest granularities for more accurate user interest targeting. 2 3 Related Work Personalized news recommendation is an important intelligent application and is widely studied in recent years (Bansal et al., 2015; Wu et al., 2019c; Qi et al., 2020; Ge et al., 2020). Existing methods usually model news from its content, model user interest from user’s clicked news, and recommend candidate news based on their relevance with user interests (Okura et al., 2017). For example, Okura et al. (2017) utilized an auto-encoder to learn news representations from news bodies. They applied a GRU network to capture user interests from the sequence of users’ historical clicks and used the last hidden state vector of GRU as user interest representation. Besides, they proposed to model relevance between user interest and candidate news based on the dot p"
2021.acl-long.423,2020.acl-main.77,1,0.699389,"ked news via a GRU network and long-term user interests from user-news interactions via user ID embeddings. (7) NRMS (Wu et al., 2019e): applying multi-head self-attention networks to learn news representations and user representations. (8) KRED (Liu et al., 2020): proposing a knowledge graph attention network to learn news representations from texts and entities of news titles. (9) GNewsRec (Hu et al., 2020): modeling short-term user interests from clicked news sequences via an attentive GRU network and long-term user interests from user-news click graph via a graph neural network. (10) FIM (Wang et al., 2020): modeling user interests in candidate news from semantic relevance of user’s clicked news and candidate news 2 https://github.com/JulySinceAndrew/HieRec via a 3-D CNN network. Each experiment is repeated 5 times. The average results and standard deviations are listed in Table 2, from which we have several observations. First, HieRec significantly outperforms other baseline methods which learn a single user embedding to model overall user interests, such as NRMS, NPA, and NAML. This is because user interests are usually diverse and multi-grained. However, it is difficult for a single represent"
2021.acl-long.423,P19-1110,1,0.821408,"Missing"
2021.acl-long.423,D19-1493,1,0.872578,"Missing"
2021.acl-long.423,D19-1671,1,0.877679,"Missing"
2021.acl-long.423,2020.aacl-main.6,1,0.882116,"experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation. 1 Figure 1: Click and non-click logs of an example user. Introduction Recently, massive people are habituated to reading news articles on online news platforms, such as Google News and Microsoft News (Khattar et al., 2018; Das et al., 2007). To help users efficiently obtain their interested news information, personalized news recommendation technique that aims to recommend news according to user interests, is widely used by these platforms (Wu et al., 2020a; Liu et al., 2010; Lin et al., 2014). User interest modeling is a critical step for personalized news recommendation (Wu et al., 2021; Zheng et al., 2018; Wu et al., 2020c). Existing methods usually learn a single representation vector to model overall user interests from users’ clicked news (Okura et al., 2017; Wu et al., 2020b; An et al., 2019). For example, Okura et al. (2017) used a GRU network to model user interests from clicked news. They used the latest hidden state of GRU as the user interest representation. Wu et al. (2019e) used multi-head self-attention network to capture user in"
2021.acl-long.423,2020.findings-emnlp.174,1,0.904662,"experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation. 1 Figure 1: Click and non-click logs of an example user. Introduction Recently, massive people are habituated to reading news articles on online news platforms, such as Google News and Microsoft News (Khattar et al., 2018; Das et al., 2007). To help users efficiently obtain their interested news information, personalized news recommendation technique that aims to recommend news according to user interests, is widely used by these platforms (Wu et al., 2020a; Liu et al., 2010; Lin et al., 2014). User interest modeling is a critical step for personalized news recommendation (Wu et al., 2021; Zheng et al., 2018; Wu et al., 2020c). Existing methods usually learn a single representation vector to model overall user interests from users’ clicked news (Okura et al., 2017; Wu et al., 2020b; An et al., 2019). For example, Okura et al. (2017) used a GRU network to model user interests from clicked news. They used the latest hidden state of GRU as the user interest representation. Wu et al. (2019e) used multi-head self-attention network to capture user in"
2021.acl-long.423,2020.acl-main.331,1,0.86944,"experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation. 1 Figure 1: Click and non-click logs of an example user. Introduction Recently, massive people are habituated to reading news articles on online news platforms, such as Google News and Microsoft News (Khattar et al., 2018; Das et al., 2007). To help users efficiently obtain their interested news information, personalized news recommendation technique that aims to recommend news according to user interests, is widely used by these platforms (Wu et al., 2020a; Liu et al., 2010; Lin et al., 2014). User interest modeling is a critical step for personalized news recommendation (Wu et al., 2021; Zheng et al., 2018; Wu et al., 2020c). Existing methods usually learn a single representation vector to model overall user interests from users’ clicked news (Okura et al., 2017; Wu et al., 2020b; An et al., 2019). For example, Okura et al. (2017) used a GRU network to model user interests from clicked news. They used the latest hidden state of GRU as the user interest representation. Wu et al. (2019e) used multi-head self-attention network to capture user in"
2021.acl-long.424,P19-1033,1,0.823497,"n visiting online news platforms (Wu et al., 2020d,b, 2021; Ge et al., 2020). Existing personalized news recommendation methods usually recommend news to a target user based on the matching between the content of candidate news and user interest inferred from previous behaviors (Zhu et al., 2019; Wu et al., 2019f). For example, Wu et al. (2019e) proposed to model news content from news title based on multi-head self-attention. In addition, they modeled user interest from the previously clicked news articles with multi-head self-attention to capture the relatedness between different behaviors. An et al. (2019) proposed to use CNN network to learn news embeddings from news titles and categories, and model both long-term and short-term user interests from news click behaviors. However, these personalized news recommendation methods usually have difficulties in making accurate recommendations to cold-start users, since the behaviors of these users are very sparse and it is difficult to model their interest (Trevisiol et al., 2014). Besides, these methods tend to recommend similar news with those users have read (Nguyen et al., 2014), which may hurt user experience and is not beneficial for them to rec"
2021.acl-long.424,D14-1162,0,0.0884303,"For both datasets, we randomly sample 500k impressions for model training, 100k impressions for validation, and 500k impressions for evaluation, respectively. The detailed statistics are listed in Table 1. Following previous works (Wu et al., 2019a; An et al., 2019), we use AUC, MRR, nDCG@5, and nDCG@10 to evaluate recommendation performance. MSN Feeds # News 161,013 4,117,562 # Users 490,522 98,866 # Impressions 1,100,000 1,100,000 # Clicks 1,675,084 2,384,976 Table 1: Statistics of the datasets. In our experiments, word embeddings are 300dimensional and initialized by the Glove embeddings (Pennington et al., 2014). The entity embeddings are 100-dimensional vectors pre-trained on knowledge tuples extracted from WikiData via TransE (Bordes et al., 2013). We use clicked and unclicked impressions in the recent one hour to compute the near real-time CTR. The recency and popularity embeddings are set to 100 dimensions and initialized randomly. All multi-head attention networks are set to have 20 attention heads and the output dimension of each head is 20. All gate networks are implemented by a two-layer dense network with 100-dimensional hidden vectors. Dropout approach (Srivastava et al., 2014) is applied t"
2021.acl-long.424,2020.findings-emnlp.128,1,0.744287,"er of views and comments (Yang, 2016; Tatar et al., 2014; Lee et al., 2010). For example, Yang (2016) proposed to use the frequency of views to measure news popularity. Tatar et al. (2014) proposed to predict news popularity based Related Work 2.1 Personalized News Recommendation Personalized news recommendation are widely used in online news platforms (Liu et al., 2010; Bansal et al., 2015; Wu et al., 2020d,c, 2019d). Existing personalized news recommendation methods usually rank candidate news for a target user based on the matching between news content and user interest (Wang et al., 2018; Qi et al., 2020; Wu et al., 2020a, 2019c). For example, Okura et al. (2017) learned news embeddings from news bodies via an auto-encoder and modeled user interests from the clicked news via a GRU network. The matching between news and user is formulated as the dot product of their embeddings. Wu et al. (2019e) used multi-head self-attention networks to generate 1 https://github.com/JulySinceAndrew/PP-Rec 2.2 5458 Popularity-based News Recommendation Figure 2: The overall framework of PP-Rec. on the number of comments of news via a linear model. Li et al. (2011) proposed to use the number of clicks on news to"
2021.acl-long.424,P19-1110,1,0.871435,"Missing"
2021.acl-long.424,D19-1493,1,0.883569,"Missing"
2021.acl-long.424,D19-1671,1,0.871688,"Missing"
2021.acl-long.424,2020.aacl-main.6,1,0.864496,"odeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation. 1 6.1 magnitude quake rattles Alaska Returning to normal is not simple for everyone Biden aims to rebuild and expand legal immigration Russia diplomat warns US ahead of summit Man accused of plotting Walmart attack arrested Black Wall Street was shattered 100 years ago Figure 1: Several example popular news. Introduction Personalized news recommendation is a useful technique to help users alleviate information overload when visiting online news platforms (Wu et al., 2020d,b, 2021; Ge et al., 2020). Existing personalized news recommendation methods usually recommend news to a target user based on the matching between the content of candidate news and user interest inferred from previous behaviors (Zhu et al., 2019; Wu et al., 2019f). For example, Wu et al. (2019e) proposed to model news content from news title based on multi-head self-attention. In addition, they modeled user interest from the previously clicked news articles with multi-head self-attention to capture the relatedness between different behaviors. An et al. (2019) proposed to use CNN network to l"
2021.acl-long.424,2020.findings-emnlp.174,1,0.792811,"odeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation. 1 6.1 magnitude quake rattles Alaska Returning to normal is not simple for everyone Biden aims to rebuild and expand legal immigration Russia diplomat warns US ahead of summit Man accused of plotting Walmart attack arrested Black Wall Street was shattered 100 years ago Figure 1: Several example popular news. Introduction Personalized news recommendation is a useful technique to help users alleviate information overload when visiting online news platforms (Wu et al., 2020d,b, 2021; Ge et al., 2020). Existing personalized news recommendation methods usually recommend news to a target user based on the matching between the content of candidate news and user interest inferred from previous behaviors (Zhu et al., 2019; Wu et al., 2019f). For example, Wu et al. (2019e) proposed to model news content from news title based on multi-head self-attention. In addition, they modeled user interest from the previously clicked news articles with multi-head self-attention to capture the relatedness between different behaviors. An et al. (2019) proposed to use CNN network to l"
2021.acl-long.424,2020.acl-main.331,1,0.920608,"odeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation. 1 6.1 magnitude quake rattles Alaska Returning to normal is not simple for everyone Biden aims to rebuild and expand legal immigration Russia diplomat warns US ahead of summit Man accused of plotting Walmart attack arrested Black Wall Street was shattered 100 years ago Figure 1: Several example popular news. Introduction Personalized news recommendation is a useful technique to help users alleviate information overload when visiting online news platforms (Wu et al., 2020d,b, 2021; Ge et al., 2020). Existing personalized news recommendation methods usually recommend news to a target user based on the matching between the content of candidate news and user interest inferred from previous behaviors (Zhu et al., 2019; Wu et al., 2019f). For example, Wu et al. (2019e) proposed to model news content from news title based on multi-head self-attention. In addition, they modeled user interest from the previously clicked news articles with multi-head self-attention to capture the relatedness between different behaviors. An et al. (2019) proposed to use CNN network to l"
2021.acl-short.107,N19-1423,0,0.0271475,"e use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of HiTransformer in long document modeling. 1 Introduction Transformer (Vaswani et al., 2017) is an effective architecture for text modeling, and has been an essential component in many state-of-the-art NLP models like BERT (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Wu et al., 2021). The standard Transformer needs to compute a dense self-attention matrix based on the interactions between each pair of tokens in text, where the computational complexity is proportional to the square of text length (Vaswani et al., 2017; Wu et al., 2020b). Thus, it is difficult for Transformer to model long documents efficiently (Child et al., 2019). There are several methods to accelerate Transformer for long document modeling (Wu et al., 2019; Kitaev et al., 2019; Wang et al., 2020; Qiu et al., 2020). One direction is using Transfo"
2021.acl-short.107,2020.acl-main.267,1,0.904363,"hmark datasets validate the efficiency and effectiveness of HiTransformer in long document modeling. 1 Introduction Transformer (Vaswani et al., 2017) is an effective architecture for text modeling, and has been an essential component in many state-of-the-art NLP models like BERT (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Wu et al., 2021). The standard Transformer needs to compute a dense self-attention matrix based on the interactions between each pair of tokens in text, where the computational complexity is proportional to the square of text length (Vaswani et al., 2017; Wu et al., 2020b). Thus, it is difficult for Transformer to model long documents efficiently (Child et al., 2019). There are several methods to accelerate Transformer for long document modeling (Wu et al., 2019; Kitaev et al., 2019; Wang et al., 2020; Qiu et al., 2020). One direction is using Transformer in a hierarchical manner to reduce sequence length, e.g., first learn sentence representations and then learn document representations from sentence representations (Zhang et al., 2019; Yang et al., 2020). However, the modeling of sentences is agnostic to the global document context, which may be suboptimal"
2021.acl-short.107,2020.acl-main.331,1,0.818817,"hmark datasets validate the efficiency and effectiveness of HiTransformer in long document modeling. 1 Introduction Transformer (Vaswani et al., 2017) is an effective architecture for text modeling, and has been an essential component in many state-of-the-art NLP models like BERT (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Wu et al., 2021). The standard Transformer needs to compute a dense self-attention matrix based on the interactions between each pair of tokens in text, where the computational complexity is proportional to the square of text length (Vaswani et al., 2017; Wu et al., 2020b). Thus, it is difficult for Transformer to model long documents efficiently (Child et al., 2019). There are several methods to accelerate Transformer for long document modeling (Wu et al., 2019; Kitaev et al., 2019; Wang et al., 2020; Qiu et al., 2020). One direction is using Transformer in a hierarchical manner to reduce sequence length, e.g., first learn sentence representations and then learn document representations from sentence representations (Zhang et al., 2019; Yang et al., 2020). However, the modeling of sentences is agnostic to the global document context, which may be suboptimal"
2021.acl-short.107,D14-1162,0,0.111777,"ments are conducted on three benchmark document modeling datasets. The first one is Amazon Electronics (He and McAuley, 2016) (denoted as Amazon), which is for product review rating prediction.3 The second one is IMDB (Diao et al., 2014), a widely used dataset for movie review rating prediction.4 The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale dataset for news intelligence.5 We use the content based news topic classification task on this dataset. The detailed dataset statistics are shown in Table 1. In our experiments, we use the 300-dimensional pre-trained Glove (Pennington et al., 2014) embeddings for initializing word embeddings. We use two Hi-Transformers layers in our approach and two Transformer layers in other baseline methods.6 We use attentive pooling (Yang et al., 2016) to implement the hierarchical pooling module. The hidden dimension is set to 256, i.e., 8 self-attention heads in total and the output dimension of each head is 32. Due to the limitation of GPU memory, the input sequence lengths of vanilla Transformer and its variants for long documents are 512 and 2048, respectively. The dropout (Srivastava et al., 2014) ratio is 0.2. The optimizer is Adam (Bengio an"
2021.acl-short.107,2020.findings-emnlp.232,0,0.0283646,"the-art NLP models like BERT (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Wu et al., 2021). The standard Transformer needs to compute a dense self-attention matrix based on the interactions between each pair of tokens in text, where the computational complexity is proportional to the square of text length (Vaswani et al., 2017; Wu et al., 2020b). Thus, it is difficult for Transformer to model long documents efficiently (Child et al., 2019). There are several methods to accelerate Transformer for long document modeling (Wu et al., 2019; Kitaev et al., 2019; Wang et al., 2020; Qiu et al., 2020). One direction is using Transformer in a hierarchical manner to reduce sequence length, e.g., first learn sentence representations and then learn document representations from sentence representations (Zhang et al., 2019; Yang et al., 2020). However, the modeling of sentences is agnostic to the global document context, which may be suboptimal because the local context within sentence is usually insufficient. Another direction is using a sparse self-attention matrix instead of a dense one. For example, Beltagy et al. (2020) proposed to combine local self-attention with a dilated sliding window"
2021.acl-short.107,N16-1174,0,0.0594138,"cond one is IMDB (Diao et al., 2014), a widely used dataset for movie review rating prediction.4 The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale dataset for news intelligence.5 We use the content based news topic classification task on this dataset. The detailed dataset statistics are shown in Table 1. In our experiments, we use the 300-dimensional pre-trained Glove (Pennington et al., 2014) embeddings for initializing word embeddings. We use two Hi-Transformers layers in our approach and two Transformer layers in other baseline methods.6 We use attentive pooling (Yang et al., 2016) to implement the hierarchical pooling module. The hidden dimension is set to 256, i.e., 8 self-attention heads in total and the output dimension of each head is 32. Due to the limitation of GPU memory, the input sequence lengths of vanilla Transformer and its variants for long documents are 512 and 2048, respectively. The dropout (Srivastava et al., 2014) ratio is 0.2. The optimizer is Adam (Bengio and LeCun, 2015), and the learning rate is 1e-4. The maximum training epoch is 3. The models are implemented using the Keras library with Tensorflow backend. The GPU we used is GeForce GTX 1080 Ti"
2021.emnlp-main.223,P19-1033,1,0.890505,"global model. Recently, Qi et al. (2020) proposed a FedRec method to train news recommendation models using federated learning. However, the model sizes of many existing news recommendation methods are large, especially their news models. For example, PLM-NR (Wu et al., 2021b) has 110.7M parameter in total, 110M of which are in the news model (BERT-Base version). With the explosion of online information, the large quantities of news generated every day may overwhelm users and make them difficult to find the news they are interested in. To tackle this problem, many news recommendation methods (An et al., 2019; Wang et al., 2020; Wu et al., 2019a; Qi 2814 1 https://gdpr-info.eu Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2814–2824 c November 7–11, 2021. 2021 Association for Computational Linguistics Thus, the communication and computation costs of FedRec can be too high for clients with rather limited computation resource. 2 In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation named Efficient-FedRec2 . In our framework, we decompose the news recommendation model into a large news model and a"
2021.emnlp-main.223,2021.ccl-1.108,0,0.0412765,"Missing"
2021.emnlp-main.223,N19-1423,0,0.0166652,"ura et al., 2017; Zhu et al., 2019; Qi et al., 2021b,a). They usually contain two core modules, i.e., user model and news model. For example, An et al. (2019) propose to use a CNN to learn contextual word embedding and an attention layer to select informative words. They combine long-term interests and short-term interests of users by using user id embeddings and a GRU network in user model. These methods learn news representations based on shallow NLP models, which is hard to well capture the news semantic information. Recently, pre-trained language models (PLM) achieve great success in NLP (Devlin et al., 2019; Liu et al., 2019; Bao et al., 2020). A few PLM-empowered news recommendation methods have been proposed. For example, Wu et al. (2021b) propose PLM-NR to empower news modeling by applying pre-trained language. They replace the news encoder in previous methods with pre-trained language models, and get stable improvement on news recommendation task. However, all the above methods train models based on centralized data, which is highly privacy-sensitive. Such kind of collections and analysis of private data have led to privacy concerns and risks (Shin et al., 2018). Besides, the adoption of som"
2021.emnlp-main.223,2020.acl-main.392,1,0.862375,"Missing"
2021.emnlp-main.223,2021.nodalida-main.3,0,0.0434679,"Missing"
2021.emnlp-main.223,2021.acl-long.424,1,0.791523,"in the union news set involved by a group of user behaviors to protect the click history of a specific user. We conduct plenty of experiments on two real-world datasets and the results show that our approach can effectively reduce the computation and communication cost on clients for federated news recommendation model training. Personalized news recommendation is an important technique to alleviate the information overloading problem and improve user reading experience. Many deep learning based recommendation methods have been proposed (Wu et al., 2019b; Okura et al., 2017; Zhu et al., 2019; Qi et al., 2021b,a). They usually contain two core modules, i.e., user model and news model. For example, An et al. (2019) propose to use a CNN to learn contextual word embedding and an attention layer to select informative words. They combine long-term interests and short-term interests of users by using user id embeddings and a GRU network in user model. These methods learn news representations based on shallow NLP models, which is hard to well capture the news semantic information. Recently, pre-trained language models (PLM) achieve great success in NLP (Devlin et al., 2019; Liu et al., 2019; Bao et al.,"
2021.emnlp-main.223,2020.findings-emnlp.128,1,0.748523,"ate user data has raised many concerns (Wu et al., 2019d). Moreover, due to the adoption of some data protection regulations such as GDPR1 , it might not be able to analyze centralized user data in the future. Federated learning (McMahan et al., 2017) is a privacy-preserving method to train models on the private data decentralized on a large number of clients. In federated learning, each user keeps a local copy of model, and compute local model gradients with their local private data. A central server coordinates the clients and aggregates local gradients to update the global model. Recently, Qi et al. (2020) proposed a FedRec method to train news recommendation models using federated learning. However, the model sizes of many existing news recommendation methods are large, especially their news models. For example, PLM-NR (Wu et al., 2021b) has 110.7M parameter in total, 110M of which are in the news model (BERT-Base version). With the explosion of online information, the large quantities of news generated every day may overwhelm users and make them difficult to find the news they are interested in. To tackle this problem, many news recommendation methods (An et al., 2019; Wang et al., 2020; Wu e"
2021.emnlp-main.223,2020.acl-main.77,1,0.800772,"ently, Qi et al. (2020) proposed a FedRec method to train news recommendation models using federated learning. However, the model sizes of many existing news recommendation methods are large, especially their news models. For example, PLM-NR (Wu et al., 2021b) has 110.7M parameter in total, 110M of which are in the news model (BERT-Base version). With the explosion of online information, the large quantities of news generated every day may overwhelm users and make them difficult to find the news they are interested in. To tackle this problem, many news recommendation methods (An et al., 2019; Wang et al., 2020; Wu et al., 2019a; Qi 2814 1 https://gdpr-info.eu Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2814–2824 c November 7–11, 2021. 2021 Association for Computational Linguistics Thus, the communication and computation costs of FedRec can be too high for clients with rather limited computation resource. 2 In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation named Efficient-FedRec2 . In our framework, we decompose the news recommendation model into a large news model and a lightweight user mo"
2021.emnlp-main.223,D19-1671,1,0.824733,"Missing"
2021.emnlp-main.223,2020.acl-main.331,1,0.793094,"protect the private information in user local gradients, we apply secure aggregation to aggregate gradients. In order to protect user interacted news history, we exchange the news representations in the union news set involved by a group of user behaviors. Experiments on two real-world datasets validate our method can effectively reduce both communication and computation cost on user side while keep the model performance. Ethical Statements User Information Protection in Dataset In this paper, we conduct experiments on two public datasets, i.e., MIND and Adressa. MIND dataset was released in (Wu et al., 2020b). It is a public English news recommendation dataset. In this dataset each user was de-linked from the production system when securely hashed into an anonymized ID using onetime salt mapping to protect user privacy. We have agreed with Microsoft Research License Terms7 before downloading this dataset and complied with these license terms when using this dataset. Adressa dataset was released in (Gulla et al., 2017). It is a public Norwegian news recommendation dataset. The users in this dataset are anonymized to protect user privacy. We follow the dataset license8 when using this dataset. Thu"
2021.findings-acl.387,N19-1423,0,0.228095,"jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching. In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MTBERT in compressing PLMs. 1 Introduction Pre-trained language models (PLMs) such as BERT and RoBERTa have achieved notable success in various NLP tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019). However, many PLMs have a huge model size and computational complexity, making it difficult to deploy them to low-latency and high-concurrence online systems or devices with limited computational resources (Jiao et al., 2020; Wu et al., 2021). Knowledge distillation is a widely used technique for compressing large-scale pre-trained language models (Sun et al., 2019; Wang et al., 2020). For example, Sanh et al. (2019) proposed DistilBERT to compress BERT by transferring knowledge from the soft labels predicted by the teacher model to student model with a"
2021.findings-acl.387,D13-1170,0,0.00424023,"ain higher weight. Following (Tang et al., 2019; Lu et al., 2020), we also incorporate gold labels to compute the taskspecific loss LT ask based on the predictions of the student model, i.e., LT ask = CE(y, ys ). The final loss function L for learning the student model is a summation of the multi-teacher hidden loss, multiteacher distillation loss and the task-specific loss, which is formulated as follows: L = LM T −Hid + LM T −Dis + LT ask . 3 (3) Experiments 3.1 Datasets and Experimental Settings We conduct experiments on three benchmark datasets with different sizes. The first one is SST2 (Socher et al., 2013), which is a benchmark for text sentiment classification. The second one is RTE (Bentivogli et al., 2009), which is a widely used dataset for natural language inference. The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale public English news dataset.5 We perform the news topic classification task on this dataset. The detailed statistics of the three datasets are shown in Table 1. In our experiments, we use the pre-trained 12layer BERT, RoBERTa and UniLM (Bao et al., 2020)6 models as the teachers to distill a 6-layer 5 6 https://msnews.github.io/ We used the UniLMv2 ver"
2021.findings-acl.387,D19-1441,0,0.533019,"sets validate the effectiveness of MTBERT in compressing PLMs. 1 Introduction Pre-trained language models (PLMs) such as BERT and RoBERTa have achieved notable success in various NLP tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019). However, many PLMs have a huge model size and computational complexity, making it difficult to deploy them to low-latency and high-concurrence online systems or devices with limited computational resources (Jiao et al., 2020; Wu et al., 2021). Knowledge distillation is a widely used technique for compressing large-scale pre-trained language models (Sun et al., 2019; Wang et al., 2020). For example, Sanh et al. (2019) proposed DistilBERT to compress BERT by transferring knowledge from the soft labels predicted by the teacher model to student model with a distillation loss. Jiao et al. (2020) proposed TinyBERT, which aligns the hidden states and the attention heatmaps between student and teacher models. These methods usually learn the student model from a single teacher model (Gou et al., 2020). However, the knowledge and supervision provided by a single teacher model may be insufficient to learn an accurate student model, and the student model may also i"
2021.findings-acl.387,2020.acl-main.267,1,0.901542,"ss function L for learning the student model is a summation of the multi-teacher hidden loss, multiteacher distillation loss and the task-specific loss, which is formulated as follows: L = LM T −Hid + LM T −Dis + LT ask . 3 (3) Experiments 3.1 Datasets and Experimental Settings We conduct experiments on three benchmark datasets with different sizes. The first one is SST2 (Socher et al., 2013), which is a benchmark for text sentiment classification. The second one is RTE (Bentivogli et al., 2009), which is a widely used dataset for natural language inference. The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale public English news dataset.5 We perform the news topic classification task on this dataset. The detailed statistics of the three datasets are shown in Table 1. In our experiments, we use the pre-trained 12layer BERT, RoBERTa and UniLM (Bao et al., 2020)6 models as the teachers to distill a 6-layer 5 6 https://msnews.github.io/ We used the UniLMv2 version. Dataset SST-2 RTE MIND #Train 67k 2.5k 102k #Dev 872 276 2.6k #Test 1.8k 3.0k 26k #Class 2 2 18 Table 1: The statistics of the three datasets. and a 4-layer student models respectively. We use the token embeddings"
2021.findings-acl.387,2021.findings-emnlp.280,1,0.717538,"ge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MTBERT in compressing PLMs. 1 Introduction Pre-trained language models (PLMs) such as BERT and RoBERTa have achieved notable success in various NLP tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019). However, many PLMs have a huge model size and computational complexity, making it difficult to deploy them to low-latency and high-concurrence online systems or devices with limited computational resources (Jiao et al., 2020; Wu et al., 2021). Knowledge distillation is a widely used technique for compressing large-scale pre-trained language models (Sun et al., 2019; Wang et al., 2020). For example, Sanh et al. (2019) proposed DistilBERT to compress BERT by transferring knowledge from the soft labels predicted by the teacher model to student model with a distillation loss. Jiao et al. (2020) proposed TinyBERT, which aligns the hidden states and the attention heatmaps between student and teacher models. These methods usually learn the student model from a single teacher model (Gou et al., 2020). However, the knowledge and supervisio"
2021.findings-acl.387,2020.findings-emnlp.372,0,0.608909,"the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MTBERT in compressing PLMs. 1 Introduction Pre-trained language models (PLMs) such as BERT and RoBERTa have achieved notable success in various NLP tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019). However, many PLMs have a huge model size and computational complexity, making it difficult to deploy them to low-latency and high-concurrence online systems or devices with limited computational resources (Jiao et al., 2020; Wu et al., 2021). Knowledge distillation is a widely used technique for compressing large-scale pre-trained language models (Sun et al., 2019; Wang et al., 2020). For example, Sanh et al. (2019) proposed DistilBERT to compress BERT by transferring knowledge from the soft labels predicted by the teacher model to student model with a distillation loss. Jiao et al. (2020) proposed TinyBERT, which aligns the hidden states and the attention heatmaps between student and teacher models. These methods usually learn the student model from a single teacher model (Gou et al., 2020). However, the knowle"
2021.findings-acl.387,2020.acl-main.331,1,0.924376,"ss function L for learning the student model is a summation of the multi-teacher hidden loss, multiteacher distillation loss and the task-specific loss, which is formulated as follows: L = LM T −Hid + LM T −Dis + LT ask . 3 (3) Experiments 3.1 Datasets and Experimental Settings We conduct experiments on three benchmark datasets with different sizes. The first one is SST2 (Socher et al., 2013), which is a benchmark for text sentiment classification. The second one is RTE (Bentivogli et al., 2009), which is a widely used dataset for natural language inference. The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale public English news dataset.5 We perform the news topic classification task on this dataset. The detailed statistics of the three datasets are shown in Table 1. In our experiments, we use the pre-trained 12layer BERT, RoBERTa and UniLM (Bao et al., 2020)6 models as the teachers to distill a 6-layer 5 6 https://msnews.github.io/ We used the UniLMv2 version. Dataset SST-2 RTE MIND #Train 67k 2.5k 102k #Dev 872 276 2.6k #Test 1.8k 3.0k 26k #Class 2 2 18 Table 1: The statistics of the three datasets. and a 4-layer student models respectively. We use the token embeddings"
2021.findings-acl.387,2021.ccl-1.108,0,0.0743917,"Missing"
2021.findings-acl.387,N16-1174,0,0.0726725,"atasets are shown in Table 1. In our experiments, we use the pre-trained 12layer BERT, RoBERTa and UniLM (Bao et al., 2020)6 models as the teachers to distill a 6-layer 5 6 https://msnews.github.io/ We used the UniLMv2 version. Dataset SST-2 RTE MIND #Train 67k 2.5k 102k #Dev 872 276 2.6k #Test 1.8k 3.0k 26k #Class 2 2 18 Table 1: The statistics of the three datasets. and a 4-layer student models respectively. We use the token embeddings and the first 4 or 6 Transformer layers of UniLM to initialize the parameters of the student model. The pooling layer is implemented by an attention network (Yang et al., 2016; Wu et al., 2020a). The temperature coefficient t is set to 1. The attention query dimension in the attentive pooling layer is 200. The optimizer we use is Adam (Bengio and LeCun, 2015). The teacher model learning rate is 2e-6 while the student model learning rate is 5e-6. The batch size is 64. Following (Jiao et al., 2020), we report the accuracy score on the SST-2 and RTE datasets. In addition, since the news topics in the MIND dataset are highly imbalanced, following (Wu et al., 2020b) we report both accuracy and macro-F1 scores. Each experiment is independently repeated 5 times and the av"
2021.findings-emnlp.124,P19-1033,1,0.897943,"mendation to encrypt the uploaded gradients. Qi et al. (2020) Personalized news recommendation is an important proposed to apply federated learning technique to research problem and has been widely studied in train neural news recommendation models on derecent years (Konstan et al., 1997; Wang and Blei, centralized user data. They used local differential 2011; Liu et al., 2010; Bansal et al., 2015; Wu et al., privacy technique (Ren et al., 2018) to protect the 2020b; Qi et al., 2021c; Wu et al., 2020d, 2021d; uploaded gradients from leaking user privacy. In Wang et al., 2020; Ge et al., 2020; An et al., 2019). brief, most of these methods focus on training a Existing news recommendation methods aim to recommendation model for ranking candidate news match candidate news content with user prefer- in a privacy-preserving way. However, how to genences mined from users’ historical behaviors (Khat- erate candidate news from news pool according tar et al., 2018; Wu et al., 2021b,f; Ge et al., 2020; to user interest and serve users with decentralized Qi et al., 2021a; Wu et al., 2019d; An et al., 2019). user behavior data are still unsolved problems. DifFor example, Okura et al. (2017) proposed to learn f"
2021.findings-emnlp.124,2021.acl-long.424,1,0.218472,"ool, which are further distributed to the user client at the ranking stage for personalized news display. In addition, we propose an interest decomposer-aggregator method with perturbation noise to better protect private user information encoded in user interest representations. Besides, we collaboratively train both recall and ranking models on the data decentralized in a large number of user clients in a privacy-preserving way. Experiments on two real-world news datasets show that our method can outperform baseline methods and effectively protect user privacy. 1 Introduction 2019c,b, 2020c; Qi et al., 2021a; Ge et al., 2020; Wu et al., 2021c). For example, Okura et al. (2017) employed a GRU network to build user embeddings from browsed news. Wu et al. (2019a) employed an attention network to build user embeddings by aggregating different clicked news. Both of them match candidate news and user interests via the inner product of their embeddings. In short, most of these methods rely on centralized storage of user behavior data to train models and serve users. However, user behavior data is usually highly privacysensitive (Chai et al., 2019), and centrally storing them may arouse users’ concerns"
2021.findings-emnlp.124,2020.findings-emnlp.128,1,0.832885,"19a) employed an attention network to build user embeddings by aggregating different clicked news. Both of them match candidate news and user interests via the inner product of their embeddings. In short, most of these methods rely on centralized storage of user behavior data to train models and serve users. However, user behavior data is usually highly privacysensitive (Chai et al., 2019), and centrally storing them may arouse users’ concerns on privacy leakage and violate some privacy protection regulations such as GDPR1 . A few methods explore to recommend news in a privacy-preserving way (Qi et al., 2020). For instance, Qi et al. (2020) proposed to store user data in user clients and applied federated learning technique (McMahan et al., 2017) to train news recommendation models on decentralized data. In general, these methods usually focus on developing privacy-preserving model training approaches based on decentralized user behavior data for ranking candidate news. However, how to generate candidate news and serve users in a privacy-preserving way remains an open problem. In this paper, we propose a unified news recommendation framework based on federated learning (named Uni-FedRec), which ca"
2021.findings-emnlp.124,2020.acl-main.77,1,0.886971,"u et al., 2021e; Pal et al., 2020), UniFedRec contains a recall stage for personalized candidate news generation and a ranking stage for candidate news ranking. In the recall stage, the user client first locally learns multiple interest representations from clicked news to model diverse Online news platforms usually rely on personalized news recommendation techniques to help users obtain their interested news information (Qi et al., 2021b; Wu et al., 2019c). Existing news recommendation models usually exploit users’ historical behavior data to model user interests for matching candidate news (Wang et al., 2020; Wu et al., 1438 1 https://gdpr-info.eu/ Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1438–1448 November 7–11, 2021. ©2021 Association for Computational Linguistics user interests. These representations are further uploaded to the server to recall a small number of candidate news (e.g., 100) from a large news pool. In the ranking stage, recalled candidate news are distributed to the user client and locally ranked for news personalized display. Bedsides, user interest representations may encode user privacy information. (Wu et al., 2020a). To protect private use"
2021.findings-emnlp.124,P19-1110,1,0.784467,"Missing"
2021.findings-emnlp.124,D19-1493,1,0.858774,"Missing"
2021.findings-emnlp.124,D19-1671,1,0.883767,"Missing"
2021.findings-emnlp.124,N19-1180,1,0.15532,"ggregator method with perturbation noise to better protect private user information encoded in user interest representations. Besides, we collaboratively train both recall and ranking models on the data decentralized in a large number of user clients in a privacy-preserving way. Experiments on two real-world news datasets show that our method can outperform baseline methods and effectively protect user privacy. 1 Introduction 2019c,b, 2020c; Qi et al., 2021a; Ge et al., 2020; Wu et al., 2021c). For example, Okura et al. (2017) employed a GRU network to build user embeddings from browsed news. Wu et al. (2019a) employed an attention network to build user embeddings by aggregating different clicked news. Both of them match candidate news and user interests via the inner product of their embeddings. In short, most of these methods rely on centralized storage of user behavior data to train models and serve users. However, user behavior data is usually highly privacysensitive (Chai et al., 2019), and centrally storing them may arouse users’ concerns on privacy leakage and violate some privacy protection regulations such as GDPR1 . A few methods explore to recommend news in a privacy-preserving way (Qi"
2021.findings-emnlp.124,2020.aacl-main.6,1,0.865683,"atching candidate news (Wang et al., 2020; Wu et al., 1438 1 https://gdpr-info.eu/ Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1438–1448 November 7–11, 2021. ©2021 Association for Computational Linguistics user interests. These representations are further uploaded to the server to recall a small number of candidate news (e.g., 100) from a large news pool. In the ranking stage, recalled candidate news are distributed to the user client and locally ranked for news personalized display. Bedsides, user interest representations may encode user privacy information. (Wu et al., 2020a). To protect private user information encoded in interest representations, we propose an interest decomposer-aggregator method with perturbation noise to synthesize interest representations with a group of basic interest embeddings. In addition, Uni-FedRec utilizes user data decentralized in a large number of user clients to collaboratively train the recall and ranking model in a privacy-preserving way. Extensive experiments on two real-world datasets verify that our method can significantly outperform baseline methods and effectively protect user privacy. In summary, our contributions are a"
2021.findings-emnlp.124,2020.findings-emnlp.174,1,0.838032,"atching candidate news (Wang et al., 2020; Wu et al., 1438 1 https://gdpr-info.eu/ Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1438–1448 November 7–11, 2021. ©2021 Association for Computational Linguistics user interests. These representations are further uploaded to the server to recall a small number of candidate news (e.g., 100) from a large news pool. In the ranking stage, recalled candidate news are distributed to the user client and locally ranked for news personalized display. Bedsides, user interest representations may encode user privacy information. (Wu et al., 2020a). To protect private user information encoded in interest representations, we propose an interest decomposer-aggregator method with perturbation noise to synthesize interest representations with a group of basic interest embeddings. In addition, Uni-FedRec utilizes user data decentralized in a large number of user clients to collaboratively train the recall and ranking model in a privacy-preserving way. Extensive experiments on two real-world datasets verify that our method can significantly outperform baseline methods and effectively protect user privacy. In summary, our contributions are a"
2021.findings-emnlp.124,2020.acl-main.331,1,0.838068,"Missing"
2021.findings-emnlp.280,N19-1423,0,0.234401,"horough expercompress a cumbersome teacher model into a iments on two real-world datasets with three lighter-weight student model by transferring usetasks show that NewsBERT can empower varful knowledge (Hinton et al., 2015; Kim and ious intelligent news applications with much Rush, 2016). It has been employed to compress smaller models. many huge pre-trained language models into much smaller versions and meanwhile keep most of the 1 Introduction original performance (Sanh et al., 2019; Sun et al., Pre-trained language models (PLMs) like 2019; Wang et al., 2020b; Jiao et al., 2020). For BERT (Devlin et al., 2019) and GPT (Radford example, Sanh et al. (2019) proposed a Distilet al., 2019) have achieved remarkable success in BERT approach, which learns the student model various NLP applications (Liu et al., 2019; Yang from the soft target probabilities of the teacher et al., 2019). These PLMs are usually in huge model by using a distillation loss with softmaxsize with hundreds of millions of parameters (Qiu temperature (Jang et al., 2016), and they regular3285 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3285–3295 November 7–11, 2021. ©2021 Association for Computational L"
2021.findings-emnlp.280,P84-1044,0,0.122405,"Missing"
2021.findings-emnlp.280,2020.findings-emnlp.372,0,0.634244,"earned by the teacher model. Thorough expercompress a cumbersome teacher model into a iments on two real-world datasets with three lighter-weight student model by transferring usetasks show that NewsBERT can empower varful knowledge (Hinton et al., 2015; Kim and ious intelligent news applications with much Rush, 2016). It has been employed to compress smaller models. many huge pre-trained language models into much smaller versions and meanwhile keep most of the 1 Introduction original performance (Sanh et al., 2019; Sun et al., Pre-trained language models (PLMs) like 2019; Wang et al., 2020b; Jiao et al., 2020). For BERT (Devlin et al., 2019) and GPT (Radford example, Sanh et al. (2019) proposed a Distilet al., 2019) have achieved remarkable success in BERT approach, which learns the student model various NLP applications (Liu et al., 2019; Yang from the soft target probabilities of the teacher et al., 2019). These PLMs are usually in huge model by using a distillation loss with softmaxsize with hundreds of millions of parameters (Qiu temperature (Jang et al., 2016), and they regular3285 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3285–3295 November 7–11, 2021. ©2021"
2021.findings-emnlp.280,D16-1139,0,0.0323948,"Missing"
2021.findings-emnlp.280,2021.ccl-1.108,0,0.0964318,"Missing"
2021.findings-emnlp.280,D14-1162,0,0.0890374,"o fitting the self-supervision tasks (e.g., masked token prediction) while the hidden representations of intermediate layers have better generalization ability, which is also validated by (Chi et al., 2021). Fourth, compared with TwinBERT, the results of TinyBERT and NewsBERT are usually better. This 4.2 Performance Evaluation may be because the TwinBERT method only distills In this section, we compare the performance of the teacher model based on the output soft labels, our NewsBERT approach with many baseline meth- while the other two methods can also align the hidods, including: (1) Glove (Pennington et al., 2014), den representations learned by intermediate layers, which is a widely used pre-trained word embed- which can help the student model better imitate the ding. We used Glove to initialize the word embed- teacher model. Fifth, our NewsBERT approach outdings in a Transformer (Vaswani et al., 2017) model performs all other compared baseline methods, and for news topic classification and the NRMS (Wu our further t-test results show the improvements are et al., 2019d) model for news recommendation. (2) significant at p < 0.01 (by comparing the models BERT (Devlin et al., 2019), a popular PLM with bi"
2021.findings-emnlp.280,D19-1441,0,0.313093,"rm many baseline methods for PLM distillation. The main contributions of this work include: • We propose a momentum distillation method which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for"
2021.findings-emnlp.280,2020.acl-main.195,0,0.0179758,"tion. The main contributions of this work include: • We propose a momentum distillation method which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for BERT compression named BERT-PKD, which d"
2021.findings-emnlp.280,2020.acl-main.77,1,0.902044,"technique that can learned by the teacher model. Thorough expercompress a cumbersome teacher model into a iments on two real-world datasets with three lighter-weight student model by transferring usetasks show that NewsBERT can empower varful knowledge (Hinton et al., 2015; Kim and ious intelligent news applications with much Rush, 2016). It has been employed to compress smaller models. many huge pre-trained language models into much smaller versions and meanwhile keep most of the 1 Introduction original performance (Sanh et al., 2019; Sun et al., Pre-trained language models (PLMs) like 2019; Wang et al., 2020b; Jiao et al., 2020). For BERT (Devlin et al., 2019) and GPT (Radford example, Sanh et al. (2019) proposed a Distilet al., 2019) have achieved remarkable success in BERT approach, which learns the student model various NLP applications (Liu et al., 2019; Yang from the soft target probabilities of the teacher et al., 2019). These PLMs are usually in huge model by using a distillation loss with softmaxsize with hundreds of millions of parameters (Qiu temperature (Jang et al., 2016), and they regular3285 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3285–3295 Novem"
2021.findings-emnlp.280,2021.findings-acl.188,0,0.0115905,"ropose a momentum distillation method which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for BERT compression named BERT-PKD, which distills the student model by learning from teacher’s outpu"
2021.findings-emnlp.280,P19-1110,1,0.873346,"Missing"
2021.findings-emnlp.280,D19-1671,1,0.892909,"Missing"
2021.findings-emnlp.280,2021.findings-acl.387,1,0.717538,"d which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for BERT compression named BERT-PKD, which distills the student model by learning from teacher’s output soft probabilities and hidden st"
2021.findings-emnlp.280,2020.acl-main.331,1,0.95966,"rich textual content (Wang et al., In our approach, we design a teacher-student 2020a). Thus, these applications would benefit a joint learning and distillation framework to collaboratively learn both teacher and student lot from the powerful language understanding abilmodels, where the student model can learn ity of PLMs if they could be incorporated in an from the learning experience of the teacher efficient way, which further has the potential to immodel. In addition, we propose a momentum prove the news reading experience of millions of distillation method by incorporating the gradiusers (Wu et al., 2020). ents of teacher model into the update of student model to better transfer the knowledge Knowledge distillation is a technique that can learned by the teacher model. Thorough expercompress a cumbersome teacher model into a iments on two real-world datasets with three lighter-weight student model by transferring usetasks show that NewsBERT can empower varful knowledge (Hinton et al., 2015; Kim and ious intelligent news applications with much Rush, 2016). It has been employed to compress smaller models. many huge pre-trained language models into much smaller versions and meanwhile keep most of"
2021.findings-emnlp.280,2020.emnlp-main.633,0,0.0112104,"istillation method which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for BERT compression named BERT-PKD, which distills the student model by learning from teacher’s output soft probabilit"
2021.findings-emnlp.280,N16-1174,0,0.0194224,".1 Teacher-Student Joint Learning and Distillation Framework several Transformer (Vaswani et al., 2017) layers. We assume that the teacher model has N K Transformer layers on the top of the embedding layer and the student model contains N Transformer layers on the embedding layer. Thus, the inference speed of the student model is approximately K times faster than the teacher. We first use the teacher and student models to separately process the input news text (denoted as x) through their Transformer layers and obtain the hidden representation of each token. We use a shared attentive pooling (Yang et al., 2016) layer (with parameter set Θp ) to convert the hidden representation sequences output by the teacher and student models into unified news embeddings, and finally use a shared dense layer (with parameter set Θd ) to predict the classification probability scores based on the news embedding. By sharing the parameters of the top pooling and dense layers, the student model can get richer supervision information from the teacher, and the teacher can also be aware of student’s learning status. Thus, the teacher and student can be reciprocally learned by sharing useful knowledge encoded by them, which"
2021.naacl-main.166,D15-1075,0,0.0274437,"tecture. 4 Experiments 4.1 Datasets and Experimental Settings Our experiments are conducted on five benchmark datasets for different tasks. Four of them are benchmark NLP datasets. The first one is AG’s News2 (denoted as AG), which is a news topic classification dataset. The second one is Amazon Electronics (He and McAuley, 2016) (denoted as Amazon), which is a dataset for review rating prediction. The third one is Stanford Sentiment Treebank (Socher et al., 2013) (denoted as SST). We use the binary classification version of this dataset. The fourth one is Stanford Natural Language Inference (Bowman et al., 2015) (SNLI) dataset, which is a widely used natural language inference dataset. The detailed statistics of these datasets are summarized in Table 1. In addition, we also conduct experiments on a benchmark news recommendation dataset named MIND (Wu et al., 2020c), aiming to validate the effectiveness of our approach in both text and user modeling. It contains the news impression logs of 1 million users from Microsoft News3 from October 12 to November 22, 2019. The training set contains the logs in the first five weeks except those on the last day which are used for validation. The rest logs are use"
2021.naacl-main.166,P19-1285,0,0.0499028,"Missing"
2021.naacl-main.166,N19-1423,0,0.167352,"anilla Transformer and its several variants. the precise information of the real distance, which may not be beneficial for the Transformer to cap1 Introduction ture word orders and the context relations. Transformer (Vaswani et al., 2017) has achieved In this paper, we propose a distance-aware Transhuge success in the NLP field in recent former (DA-Transformer), which can explicitly exyears (Kobayashi et al., 2020). It serves as the ba- ploit real token distance information to enhance sic architecture of various state-of-the-art models context modeling by leveraging the relative dislike BERT (Devlin et al., 2019) and GPT (Rad- tances between different tokens to re-scale the raw ford et al., 2019), and boosts the performance of attention weights before softmax normalization. 2059 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2059–2068 June 6–11, 2021. ©2021 Association for Computational Linguistics More specifically, since global and local context modeling usually have different distance preferences, we propose to learn a different parameter in different attention heads to weight the token distances,"
2021.naacl-main.166,2020.emnlp-main.574,0,0.0219409,"distance-aware way. DA-Transformer can effectively improve the However, the distance or relative position embedperformance of many tasks and outperform the dings used by these methods usually cannot keep vanilla Transformer and its several variants. the precise information of the real distance, which may not be beneficial for the Transformer to cap1 Introduction ture word orders and the context relations. Transformer (Vaswani et al., 2017) has achieved In this paper, we propose a distance-aware Transhuge success in the NLP field in recent former (DA-Transformer), which can explicitly exyears (Kobayashi et al., 2020). It serves as the ba- ploit real token distance information to enhance sic architecture of various state-of-the-art models context modeling by leveraging the relative dislike BERT (Devlin et al., 2019) and GPT (Rad- tances between different tokens to re-scale the raw ford et al., 2019), and boosts the performance of attention weights before softmax normalization. 2059 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2059–2068 June 6–11, 2021. ©2021 Association for Computational Linguistics Mor"
2021.naacl-main.166,N19-1238,0,0.0437208,"Missing"
2021.naacl-main.166,N18-2074,0,0.360734,"computed by ever, in contrast to recurrent and convolutional neuthe relevance between attention query and key. ral networks, it is difficult for vanilla TransformConcretely, in different self-attention heads ers to be aware of the token distances (Shaw et al., the relative distance between each pair of to2018), which are usually important cues for context kens is weighted by different learnable pamodeling. Thus, several works explored to incorrameters, which control the different preferporate token distance information into Transformer. ences on long- or short-term information of For example, Shaw et al. (2018) proposed to comthese heads. Since the raw weighted real disbine the embeddings of relative positions with attances may not be optimal for adjusting selfattention weights, we propose a learnable sigtention key and value in the self-attention network. moid function to map them into re-scaled coefThey restricted the maximum relative distance to ficients that have proper ranges. We first clip only keep the precise relative position information the raw self-attention weights via the ReLU within a certain distance. Yan et al. (2019) profunction to keep non-negativity and introduce posed a variant o"
2021.naacl-main.166,D13-1170,0,0.0029176,"ers, which are usually ignorable compared with the projection matrices (i) like WQ . Thus, our approach inherits the efficiency of the Transformer architecture. 4 Experiments 4.1 Datasets and Experimental Settings Our experiments are conducted on five benchmark datasets for different tasks. Four of them are benchmark NLP datasets. The first one is AG’s News2 (denoted as AG), which is a news topic classification dataset. The second one is Amazon Electronics (He and McAuley, 2016) (denoted as Amazon), which is a dataset for review rating prediction. The third one is Stanford Sentiment Treebank (Socher et al., 2013) (denoted as SST). We use the binary classification version of this dataset. The fourth one is Stanford Natural Language Inference (Bowman et al., 2015) (SNLI) dataset, which is a widely used natural language inference dataset. The detailed statistics of these datasets are summarized in Table 1. In addition, we also conduct experiments on a benchmark news recommendation dataset named MIND (Wu et al., 2020c), aiming to validate the effectiveness of our approach in both text and user modeling. It contains the news impression logs of 1 million users from Microsoft News3 from October 12 to Novembe"
2021.naacl-main.166,D19-1145,0,0.0199521,"nsformer portant basic neural architecture of various state-of- Instead of directly using the sinusoidal position the-art NLP models like BERT (Devlin et al., 2019) embedding (Vaswani et al., 2017) or the absolute and GPT (Radford et al., 2019). The core compo- position embedding (Devlin et al., 2019), several nent of Transformer is multi-head self-attention. It variants of the Transformer explore to use the relahas h attention heads, where the parameters in each tive positions to better model the distance between head are independent. For the i-th attention head, contexts (Shaw et al., 2018; Wang et al., 2019; Dai it takes a matrix H as the input. It first uses three et al., 2019; Yan et al., 2019). For example, Shaw 2060 et al. (2018) proposed to add the embeddings of relative positions to the attention key and value to capture the relative distance between two tokens. They only kept the precise distance within a certain range by using a threshold to clip the maximum distance to help generalize to long sequences. Dai et al. (2019) proposed Transformer-XL, which uses another form of relative positional encodings that integrate content-dependent positional scores and a global positional score into"
2021.naacl-main.166,D19-1671,1,0.849948,", 2014) embeddings for word embedding initialization.4 The number of attention head is 16, and the output dimension of each attention is 16. We use one Transformer layer in all experiments. On the AG, SST and SNLI datasets, we directly apply Transformer-based methods to the sentences. On the Amazon dataset, since reviews are usually long documents, we use Transformers in a hierarchical way by learning sentence representations from words via a word-level Transformer first and then learning document representations from sentences via a sentence-level Transformer. On the MIND dataset, following (Wu et al., 2019, 2020b) we also use a hierarchical model architecture that first learns representations of historical clicked news and candidate news from their titles with a word-level Transformer, then learns user representations from the representations of clicked news with a news-level Transformer, and final matches user and candidate news representations to compute click scores.5 We use the same model training strategy with negative sampling techniques as NRMS (Wu et al., 2019). On all datasets we use Adam (Kingma and Ba, 2015) as the optimization algorithm and the learning rate is 1e-3. On the AG, Amaz"
2021.naacl-main.166,2020.aacl-main.6,1,0.888833,"g & BNRist, Tsinghua University, Beijing 100084, China ‡ Microsoft Research Asia, Beijing 100080, China {wuchuhan15,wufangzhao}@gmail.com, yfhuang@tsinghua.edu.cn Abstract many tasks like text generation (Koncel-Kedziorski et al., 2019), machine translation (Vaswani et al., Transformer has achieved great success in the 2017), and reading comprehension (Xu et al., 2019). NLP field by composing various advanced Thus, the improvement on the Transformer archimodels like BERT and GPT. However, Transtecture would be beneficial for many NLP-related former and its existing variants may not be fields (Wu et al., 2020a). optimal in capturing token distances because A core component of Transformer is multi-head the position or distance embeddings used by these methods usually cannot keep the precise self-attention, which is responsible for modeling information of real distances, which may not the relations between contexts (Yang et al., 2019; be beneficial for modeling the orders and reGuo et al., 2019). However, self-attention is lations of contexts. In this paper, we proposition-agnostic since it does not distinguish the pose DA-Transformer, which is a distanceorders of inputs. Thus, in the vanilla Transf"
2021.naacl-main.166,2020.acl-main.331,1,0.858287,"g & BNRist, Tsinghua University, Beijing 100084, China ‡ Microsoft Research Asia, Beijing 100080, China {wuchuhan15,wufangzhao}@gmail.com, yfhuang@tsinghua.edu.cn Abstract many tasks like text generation (Koncel-Kedziorski et al., 2019), machine translation (Vaswani et al., Transformer has achieved great success in the 2017), and reading comprehension (Xu et al., 2019). NLP field by composing various advanced Thus, the improvement on the Transformer archimodels like BERT and GPT. However, Transtecture would be beneficial for many NLP-related former and its existing variants may not be fields (Wu et al., 2020a). optimal in capturing token distances because A core component of Transformer is multi-head the position or distance embeddings used by these methods usually cannot keep the precise self-attention, which is responsible for modeling information of real distances, which may not the relations between contexts (Yang et al., 2019; be beneficial for modeling the orders and reGuo et al., 2019). However, self-attention is lations of contexts. In this paper, we proposition-agnostic since it does not distinguish the pose DA-Transformer, which is a distanceorders of inputs. Thus, in the vanilla Transf"
2021.naacl-main.166,N19-1242,0,0.0558815,"Missing"
2021.naacl-main.166,D14-1162,0,0.108661,"19. The training set contains the logs in the first five weeks except those on the last day which are used for validation. The rest logs are used for test. The key statistics of this dataset are summarized in Table 2. Dataset AG Amazon SST SNLI # Train 108k 40k 8k 55k # Dev. 12k 5k 1k 10k # Test 7.6k 5k 2k 10k # Classes 4 5 2 2 Avg. len. 44 133 19 22 Table 1: Statistics of AG, Amazon, SST and SNLI datasets. # Users # News # Impressions 1,000,000 161,013 500,000 Avg. title len. # Click samples # Non-click samples 11.52 5,597,979 136,162,621 In our experiments, we use the 300-dimensional Glove (Pennington et al., 2014) embeddings for word embedding initialization.4 The number of attention head is 16, and the output dimension of each attention is 16. We use one Transformer layer in all experiments. On the AG, SST and SNLI datasets, we directly apply Transformer-based methods to the sentences. On the Amazon dataset, since reviews are usually long documents, we use Transformers in a hierarchical way by learning sentence representations from words via a word-level Transformer first and then learning document representations from sentences via a sentence-level Transformer. On the MIND dataset, following (Wu et a"
D19-1493,P19-1033,1,0.361303,"features extracted from the news reading, web browsing, and searching behaviors of users. However, these methods usually rely on manual feature engineering, which necessities massive domain knowledge and time to design. In addition, these methods cannot exploit contextual information in the behavior records of users, which may be insufficient to learn accurate user representations. 4875 In recent years, several deep learning based methods are proposed for news recommendation (Okura et al., 2017; Wang et al., 2018; Khattar et al., 2018; Kumar et al., 2017; Zheng et al., 2018; Wu et al., 2019b; An et al., 2019; Wu et al., 2019c,a; Zhu et al., 2019). For example, Okura et al. (2017) proposed to learn news representations from news bodies via denoising autoencoders, and learn user representations from the news browsing sequence via a GRU network. Wang et al. (2018) proposed to learn news representations from news titles via a knowledge-aware CNN network, which can incorporate useful entity information from knowledge graphs. Wu et al. (2019b) proposed to learn news representations from news titles, and apply personalized attention mechanism at both word- and news-level to generate representations of n"
D19-1493,D14-1162,0,0.0847221,"Missing"
D19-1493,D14-1181,0,0.00403986,"sentation, user representation and click predictor. Next, we introduce the details of each module. 3.1 News Representation Learning The news representation module is used to learn representations of news articles from their titles. It contains three layers. The first one is a word embedding layer, which is used to convert a news title from a sequence of words into a sequence of low-dimensional semantic vectors. Denote a news title with M words as [w1 , w2 , ..., wM ]. It is converted into a vector sequence [e1 , e2 , ..., eM ] via a pre-trained embedding matrix. The second one is a CNN layer (Kim, 2014). (1) r = M X αiw ci . (2) i=1 3.2 User Representation Learning The user representation module is used to learn the representations of users from different kinds of user behaviors, e.g., web searching, news click and webpage browsing. Since these user behaviors usually have different characteristics, simply aggregate them together into a long document may not be optimal for news representation. Thus, we propose an attentive multi-view learning framework to learn unified user representations by incorporating these three kinds of user behavior information as different views of news. The first vi"
D19-1493,P19-1110,1,0.320447,"Missing"
D19-1494,N19-1180,1,0.449453,"eview texts to enhance the learning of latent user and item factors from the rating matrix via non-negative matrix factorization (NMF). However, these methods only extract topics from reviews, and cannot effectively utilize the contexts and word orders in reviews, both of which are important for learning accurate user and item representations. In recent years, several deep learning based methods proposed to learn user and item representations from original review texts (Zhang et al., 2016; Zheng et al., 2017; Catherine and Cohen, 2017; Seo et al., 2017b,a; Chen et al., 2018; Tay et al., 2018; Wu et al., 2019). For example, Zheng et al. (2017) proposed a DeepCoNN approach to learn user and item representations from reviews via CNN networks. Catherine and Cohen (2017) proposed a TransNets approach to use CNNs to learn user and item representations. In their approach, these representations are regularized to be close to the representations of reviews from the target user-item pairs. Seo et al. (2017b) proposed to use CNN networks to learn user and item representations, and applied a word-level attention network to select important words. Chen et al. (2018) proposed to learn review representations usi"
D19-1671,P19-1033,1,0.301369,"l., 2010). Learning accurate representations of news and users are two core tasks in news recommendation (Okura et al., 2017). Several deep learning based methods have been proposed for these tasks (?Kumar et al., 2017; Khattar et al., 2018; 2 https://news.google.com/ https://www.msn.com/en-us/news James Harden's incredible heroics lift Rockets over Warriors in overtime Figure 1: Several news browsed by an example user. Orange and green dashed lines represent the interactions between words and news respectively. Introduction 1 Weather forecast This Week Wu et al., 2019b,c,a; Zhu et al., 2019; An et al., 2019). For example, Okura et al. (2017) proposed to learn news representations from news bodies via auto-encoders, and learn representations of users from their browsed news via GRU. However, GRU is quite time-consuming, and their method cannot capture the contexts of words. Wang et al. (2018) proposed to learn news representations from news titles via a knowledge-aware convolutional neural network (CNN), and learn representations of users based on the similarities between candidate news and their browsed news. However, CNN cannot capture the long-distance contexts of words, and their method cannot"
D19-1671,D14-1162,0,0.0862588,"Missing"
D19-1671,P19-1110,1,0.561777,"Missing"
D19-1671,W18-5909,1,0.785141,"6389–6394, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ??1 News Encoder The news encoder module is used to learn news representations from news titles. It contains three layers. The first one is word embedding, which is used to convert a news title from a sequence of words into a sequence of low-dimensional embedding vectors. Denote a news title with M words as [w1 , w2 , ..., wM ]. Through this layer it is converted into a vector sequence [e1 , e2 , ..., eM ]. The second layer is a word-level multi-head self-attention network (Vaswani et al., 2017; Wu et al., 2018). The interactions between words are important for learning news representations. For example, in the news title “Rockets Ends 2018 with a Win”, the interaction between “Rockets” and “Win” is useful for understanding this news, and such long-distance interactions usually cannot be captured by CNN. In addition, a word may interact with multiple words in the same news. For instance, in above example the word “Rockets” has interactions with both “Ends” and “Win”. Thus, we propose to use multi-head self-attention to learn contextual representations of words by capturing their interactions. The rep"
I17-4007,W10-0208,0,0.0351928,"@gmail.com Abstract value, such as dimensional sentiment analysis. Evaluating sentiment in valence-arousal (VA) space was first proposed by Ressel (1980). As shown in Figure 1, the valence dimension represents the degree of positive or negative sentiment, while the arousal dimension indicates the intensity of sentiment. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Yu et al., 2015; Wang et al., 2016) or texts(Kim et al., 2010; Paltoglou et al., 2013). Predicting valence-arousal ratings for words and phrases is very useful for constructing affective resources for dimensional sentiment analysis. Since the existing valence-arousal resources of Chinese are mainly in word-level and there is a lack of phrase-level ones, the Dimensional Sentiment Analysis for Chinese Phrases (DSAP) task aims to predict the valencearousal ratings for Chinese affective words and phrases automatically. In this task, we propose an approach using a densely connected LSTM network and word features to identify dimensional sentiment on valence a"
I17-4007,baccianella-etal-2010-sentiwordnet,0,0.0464147,"Missing"
I17-4007,N16-1066,0,0.0388707,"Missing"
I17-4007,P15-2129,0,0.093201,".edu.cn 2 Microsoft Research Asia, wufangzhao@gmail.com Abstract value, such as dimensional sentiment analysis. Evaluating sentiment in valence-arousal (VA) space was first proposed by Ressel (1980). As shown in Figure 1, the valence dimension represents the degree of positive or negative sentiment, while the arousal dimension indicates the intensity of sentiment. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Yu et al., 2015; Wang et al., 2016) or texts(Kim et al., 2010; Paltoglou et al., 2013). Predicting valence-arousal ratings for words and phrases is very useful for constructing affective resources for dimensional sentiment analysis. Since the existing valence-arousal resources of Chinese are mainly in word-level and there is a lack of phrase-level ones, the Dimensional Sentiment Analysis for Chinese Phrases (DSAP) task aims to predict the valencearousal ratings for Chinese affective words and phrases automatically. In this task, we propose an approach using a densely connected LSTM network and word features"
P16-1029,P11-1014,0,0.479648,"n explored. Glorot et al. (2011) proposed a sentiment domain adaptation method based on a deep learning technique, i.e., Stacked Denoising Auto-encoders. The core idea of their method is to learn a high-level representation that can capture generic concepts using the unlabeled data from multiple domains. Yoshida et al. (2011) proposed a probabilistic generative model for cross-domain sentiment classification with multiple source and target domains. In their method, each word is assigned three attributes, i.e., the domain label, the domain dependence/independence label, and sentiment polarity. Bollegala et al. (2011) proposed to construct a sentiment sensitive thesaurus for cross-domain sentiment classification using data from multiple source domains. This thesaurus is used to expand the feature vectors for both training and classification. However, the similarities between target domain and different source domains are not considered in these methods. In addition, although unlabeled data is utilized in these methods, the useful word-level sentiment knowledge in the unlabeled target domain data is not exploited. Related work Sentiment classification is widely known as a domain-dependent task, since differ"
P16-1029,P07-1056,0,0.817903,"eir sentiments in Book domain. In addition, the same word may convey different sentiments in different domains. For example, in Electronics domain “easy” is usually used in positive reviews, e.g., “this digital camera is easy to use.” However, it is frequently used as a negative word in Movie domain. For instance, “the ending of this movie is easy to guess.” Thus, the sentiment classifier trained in one domain usually cannot be applied to another domain directly (Pang and Lee, 2008). In order to tackle this problem, sentiment domain adaptation has been widely studied (Liu, 2012). For example, Blitzer et al. (2007) proposed to compute the correspondence among features from different domains using their associations with pivot features based on structural correspondence learning (SCL). Pan et al. (2010) proposed a spectral feature alignment (SFA) algorithm to align the domain-specific words from different domains in order to reduce the gap between source and target domains. However, all of these methods transfer sentiment information from only one source domain. When the source and target domains have significant difference in feature distributions, the adaptation performance will heavily decline. In som"
P16-1029,P11-1013,0,0.0816587,"fferent. For example, “boring” ∗ Corresponding author. 301 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 301–310, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics labels. Then the correspondence among features from source and target domains is computed using their associations with pivot features. In order to reduce the gap between source and target domains, Pan et al. (2010) proposed a spectral feature alignment (SFA) algorithm to align the domainspecific sentiment words from different domains into clusters. He et al. (2011) proposed to extract polarity-bearing topics using joint sentiment-topic (JST) model to expand the feature representations of texts from both source and target domains. Li et al. (2009) proposed to transfer sentiment knowledge from source domain to target domain using nonnegative matrix factorization. A common shortcoming of above methods is that if the source and target domains have significantly different distributions of sentiment expressions, then the domain adaptation performance will heavily decline (Li et al., 2013). adaptation. At the training stage, we extract two kinds of sentiment m"
P17-1156,P07-1056,0,0.203329,"the help of a small number of labeled samples which are selected and annotated in an active learning mode, as well as the domain-specific sentiment similarities among words mined from unlabeled samples of target domain. A unified model is proposed to fuse different types of sentiment information and train sentiment classifier for target domain. Extensive experiments on benchmark datasets show that our approach can train accurate sentiment classifier with less labeled samples. 1 Introduction Sentiment classification is widely known as a domain-dependent problem (Liu, 2012; Pang and Lee, 2008; Blitzer et al., 2007; Pan et al., 2010). This is because different domains usually have many different sentiment expressions. For example, “lengthy” and “boring” are popularly used in Book domain to express negative sentiment. However, they are rare in Kitchen appliance domain. Moreover, the same word or phrase may convey ∗ Corresponding author. different sentiments in different domains. For instance, “unpredictable” is frequently used to express positive sentiment in Movie domain (e.g., “The plot of this movie is fun and unpredictable”). However, it tends to be used as a negative word in Kitchen appliance domain"
P17-1156,P11-1014,0,0.0438223,"while t &lt; N do 7: t = t + 1. 8: Compute the uncertainty score of each sample xi in U (Eq. (4)). 9: Compute the informativeness score of each sample xi in U (Eq. (6)). 10: Select x∗ from U which has the highest informativeness score. 11: Annotate x∗ and obtain its sentiment label y. 12: L = L + {x∗ , y}, U = U − x∗ . 13: Update sentiment classifier w according to Eq. (7). 14: end while Datasets The dataset used in our experiments is the Amazon product review dataset1 collected by Blitzer et al. (2007), which is widely used in sentiment analysis and domain adaptation research (Pan et al., 2010; Bollegala et al., 2011). This dataset contains product reviews in four domains, i.e., Book, DVD, Electronics, and Kitchen appliances. In each domain, 1,000 positive and 1,000 negative reviews as well as a large number of unlabeled samples are included. The detailed statistics of this dataset are summarized in Table 1. positive negative unlabeled Book 1,000 1,000 973,194 DVD 1,000 1,000 122,438 Electronics 1,000 1,000 21,009 Kitchen 1,000 1,000 17,856 Table 1: The statistics of the Amazon dataset. Following many previous works (Blitzer et al., 2007; Bollegala et al., 2011), unigrams and bigrams were used to build fea"
P17-1156,D09-1062,0,0.777311,"ormation in sentiment lexicons to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode. Since the sentiment words in generalpurpose sentiment lexicons usually convey consistent sentiment polarities in different domains, and the actively selected labeled samples contain rich domain-specific sentiment information of target domain, our approach can effectively reduce the risk of negative transfer. The usefulness of labeled samples from target domain in sentiment domain adaptation has been observed by previous research works (Choi and Cardie, 2009; Chen et al., 2011; Li et al., 2013; Wu et al., 2016). For example, Choi and Cardie (2009) proposed to adapt a sentiment lexicon to a specific domain by exploiting both the relations among words which co-occur in the same sentiment expressions and the relations between words and labeled sentiment expressions. However, the 1702 labeled samples used in these methods are randomly selected, while in our approach we actively select informative samples from target domain to annotate. Thus, our approach has the potential to reduce the manual annotation effort. 2.2 Active Learning Active learning is"
P17-1156,D16-1057,0,0.0459369,"nt similarity score. Note r can be negative according to Eq. (1). Here that Si,j we focus on sentiment similarity rather than dissimilarity, and set all the negative values in Sr to r is [0, 1]. zero. The range of Si,j The second type of sentiment similarities are extracted according to the co-occurrence patterns among words. It is inspired by the observation that words frequently co-occurring with each other not only have a high probability to have similar semantics, but also tend to share similar sentiments (Turney, 2002; Velikovich et al., 2010; Yogatama and Smith, 2014; Tang et al., 2015; Hamilton et al., 2016). In this paper, we compute the co-occurrence between words in the context of document. Denote D as the set of all documents, and Ndi as the frequency of word i appearing in document d. Then, the sentiment similarity score between words i and j based on their co-occurrence patterns is defined as: c Si,j = P P d∈D d∈D min{Ndi , Ndj } max{Ndi , Ndj } + α2 , (2) where α2 is a positive smoothing parameter. If two words frequently co-occur with each other in many documents, then they will have a high sentiment similarity score according to Eq. (2). The c is also [0, 1]. Denote Sc ∈ RD×D range of Si"
P17-1156,P97-1023,0,0.0948276,"ntiment label. In this paper we focus on binary sentiment classification and yi ∈ {+1, −1}. In addition, we select log loss for f . Thus, f (xi , yi , w) = log(1 + exp(−yi wT xi )). Besides, we use S ∈ RD×D to represent the sentiment similarities among words extracted from unlabeled samples of target domain. 3.2 Domain-Specific Sentiment Similarities Next we introduce the extraction of domainspecific sentiment similarities among words from unlabeled samples of target domain. Two types of similarities are extracted in this paper. The first one is based on syntactic rules, which is inspired by (Hatzivassiloglou and McKeown, 1997; Huang et al., 2014; Wu and Huang, 2016). If two words have the same POS-tag such as adjective, verb, and adverb, and they are connected by coordinating conjunction “and” in the same sentence, then we regard they convey the same sentiment polarity. In 1703 addition, if two words are connected by adversative conjunction “but” and have the same POS-tag, then they are assumed to have opposite sentiment polarities. Denote Sr ∈ RD×D as the sentiment similarities extracted from unlabeled samples according to syntactic rules, and the similarity score between words i and j is defined as: r Si,j = s o"
P17-1156,P11-1013,0,0.082091,"erformance of directly applying a general sentiment classifier or a sentiment classifier trained in other domains to target domain is usually suboptimal. Since there are a large number of domains in user-generated content, it is impractical to manually annotate enough samples for each domain to train an accurate domain-specific sentiment classifier. Thus, sentiment domain adaptation, which transfers the sentiment classifier trained in a source domain with sufficient labeled data to a target domain with no or scarce labeled data, has been widely studied (Blitzer et al., 2007; Pan et al., 2010; He et al., 2011; Glorot et al., 2011). Existing sentiment domain adaptation methods are mainly based on transfer learning techniques. Many of them try to learn a new feature representation to augment or replace the original feature space in order to reduce the gap of sentiment feature distributions between source and target domains (Pan et al., 2010; Glorot et al., 2011). For example, Blitzer et al. (2007) proposed to learn a latent representation for domain-specific words from both source and target domains by using pivot features as bridge. The advantage of these methods is that no labeled data in target d"
P17-1156,W02-1011,0,0.03428,"nt lexicons to target domain is suboptimal. This is because there are many domain-specific sentiment expressions that are not covered by these general-purpose sentiment lexicons (Choi and Cardie, 2009). In addition, the performance of supervised sentiment classification methods such as SVM, LS, and LR is also 0.85 ASDA SVM 0.8 Accuracy i.e., MPQA (Wilson et al., 2005) and Bing Liu’s lexicon (Hu and Liu, 2004) for sentiment classification following the suggestions in (Hu and Liu, 2004); 2) SVM, LS, and LR, three popular supervised sentiment classification methods, i.e., support vector machine (Pang et al., 2002), least squares (Hu et al., 2013) and logistic regression (Wu et al., 2015); 3) ZIAL, the zero initialized active learning method (Cesa-Bianchi et al., 2006); 4) LIAL, the active learning method initialized by randomly selected labeled data (Settles, 2010); 5) SCL and SFA, two famous sentiment domain adaptation methods proposed in (Blitzer et al., 2007) and (Pan et al., 2010) respectively; 6) ILP, adapting sentiment lexicons to target domain via integer linear programming (Choi and Cardie, 2009); 7) AODA, the active online domain adaptation method (Rai et al., 2010); 8) ALCD, the active learni"
P17-1156,W10-0104,0,0.0338299,"Missing"
P17-1156,P02-1053,0,0.0375828,"me sentiment than opposite sentiments, then they will have a larger positive sentiment similarity score. Note r can be negative according to Eq. (1). Here that Si,j we focus on sentiment similarity rather than dissimilarity, and set all the negative values in Sr to r is [0, 1]. zero. The range of Si,j The second type of sentiment similarities are extracted according to the co-occurrence patterns among words. It is inspired by the observation that words frequently co-occurring with each other not only have a high probability to have similar semantics, but also tend to share similar sentiments (Turney, 2002; Velikovich et al., 2010; Yogatama and Smith, 2014; Tang et al., 2015; Hamilton et al., 2016). In this paper, we compute the co-occurrence between words in the context of document. Denote D as the set of all documents, and Ndi as the frequency of word i appearing in document d. Then, the sentiment similarity score between words i and j based on their co-occurrence patterns is defined as: c Si,j = P P d∈D d∈D min{Ndi , Ndj } max{Ndi , Ndj } + α2 , (2) where α2 is a positive smoothing parameter. If two words frequently co-occur with each other in many documents, then they will have a high senti"
P17-1156,N10-1119,0,0.0201386,"han opposite sentiments, then they will have a larger positive sentiment similarity score. Note r can be negative according to Eq. (1). Here that Si,j we focus on sentiment similarity rather than dissimilarity, and set all the negative values in Sr to r is [0, 1]. zero. The range of Si,j The second type of sentiment similarities are extracted according to the co-occurrence patterns among words. It is inspired by the observation that words frequently co-occurring with each other not only have a high probability to have similar semantics, but also tend to share similar sentiments (Turney, 2002; Velikovich et al., 2010; Yogatama and Smith, 2014; Tang et al., 2015; Hamilton et al., 2016). In this paper, we compute the co-occurrence between words in the context of document. Denote D as the set of all documents, and Ndi as the frequency of word i appearing in document d. Then, the sentiment similarity score between words i and j based on their co-occurrence patterns is defined as: c Si,j = P P d∈D d∈D min{Ndi , Ndj } max{Ndi , Ndj } + α2 , (2) where α2 is a positive smoothing parameter. If two words frequently co-occur with each other in many documents, then they will have a high sentiment similarity score acc"
P17-1156,H05-1044,0,0.144761,"0.7970 0.8329 0.8328 Table 2: Sentiment classification performance of different methods in different domains. Acc and Fscore represent accuracy and macro-averaged Fscore respectively. According to Table 2, the performance of directly applying sentiment lexicons to target domain is suboptimal. This is because there are many domain-specific sentiment expressions that are not covered by these general-purpose sentiment lexicons (Choi and Cardie, 2009). In addition, the performance of supervised sentiment classification methods such as SVM, LS, and LR is also 0.85 ASDA SVM 0.8 Accuracy i.e., MPQA (Wilson et al., 2005) and Bing Liu’s lexicon (Hu and Liu, 2004) for sentiment classification following the suggestions in (Hu and Liu, 2004); 2) SVM, LS, and LR, three popular supervised sentiment classification methods, i.e., support vector machine (Pang et al., 2002), least squares (Hu et al., 2013) and logistic regression (Wu et al., 2015); 3) ZIAL, the zero initialized active learning method (Cesa-Bianchi et al., 2006); 4) LIAL, the active learning method initialized by randomly selected labeled data (Settles, 2010); 5) SCL and SFA, two famous sentiment domain adaptation methods proposed in (Blitzer et al., 20"
P17-1156,P16-1029,1,0.845411,"t classification and yi ∈ {+1, −1}. In addition, we select log loss for f . Thus, f (xi , yi , w) = log(1 + exp(−yi wT xi )). Besides, we use S ∈ RD×D to represent the sentiment similarities among words extracted from unlabeled samples of target domain. 3.2 Domain-Specific Sentiment Similarities Next we introduce the extraction of domainspecific sentiment similarities among words from unlabeled samples of target domain. Two types of similarities are extracted in this paper. The first one is based on syntactic rules, which is inspired by (Hatzivassiloglou and McKeown, 1997; Huang et al., 2014; Wu and Huang, 2016). If two words have the same POS-tag such as adjective, verb, and adverb, and they are connected by coordinating conjunction “and” in the same sentence, then we regard they convey the same sentiment polarity. In 1703 addition, if two words are connected by adversative conjunction “but” and have the same POS-tag, then they are assumed to have opposite sentiment polarities. Denote Sr ∈ RD×D as the sentiment similarities extracted from unlabeled samples according to syntactic rules, and the similarity score between words i and j is defined as: r Si,j = s o Ni,j − Ni,j , o + Ni,j + α1 (1) s Ni,j s"
P19-1033,D14-1162,0,0.0991017,"Missing"
P19-1033,P82-1020,0,0.735736,"Missing"
P19-1033,D14-1181,0,0.00837424,"is the concatenation of user features and features of candidate news. • DeepFM (Guo et al., 2017), a widely used method that combines factorization machines and deep neural networks. We use the same features as LibFM. • Wide & Deep (Cheng et al., 2016), another deep learning based recommendation method that combines a wide channel and a deep channel. Again, the same features with LibFM are used for both channels. • DSSM (Huang et al., 2013), deep structured semantic model. The inputs are hashed words via character trigram, where all the browsed news titles are merged as query document. • CNN (Kim, 2014), using CNN with max pooling to learn news representations from the titles of browsed news by keeping the most salient features. 4.3 • DKN (Wang et al., 2018), a deep news recommendation model which contains CNN and candidate-aware attention on the news browsing histories. Effectiveness of Long- and Short-Term User Representation In this section, we conducted several experiments to explore the effectiveness of our approach in learning both long-term and short-term user representations. We compare the performance of our LSTUR methods with the long-term user representation model LTUR and the sho"
P19-1110,D14-1181,0,0.0248008,"The architecture of our basic neural news recommendation model is shown in Fig. 2. It consists of three major modules, i.e., news encoder, user encoder and click predictor. News Encoder. The news encoder module is used to learn representations of news from their titles. It contains three layers. The first one is word embedding, which converts a news title from a word sequence into a vector sequence. Denote a news title as [w1 , w2 , ..., wM ], where M is title length. It is converted into word vector sequence [e1 , e2 , ..., eM ] via a word embedding matrix. The second layer is a CNN network (Kim, 2014). Local contexts are important for understanding news titles. For example, in the news title “90th Birthday of Mickey mouse”, the local contexts of “mouse” such as “Mickey” is useful for inferring it is a comic character name. Thus, we use CNN to learn contextual word representations by capturing local contexts. The CNN layer takes the word vectors as input, and outputs the contextual word representations [c1 , c2 , ..., cM ]. Click Probability ?? � Click Predictor Dot ?? ???? ?? ???? ??2?? ???? ???? CNN ???? News Encoder ???? Word Embedding ??1 ???? ???? ???? Candidate News ??1?? ??1 News Enc"
P19-1110,D14-1162,0,0.0905198,"ANR-basic TANR* Table 1: Statistics of our dataset. AUC 0.5660 0.5689 0.6009 0.5735 0.5774 0.5860 0.5869 0.6102 0.6221 0.6289 MRR 0.2924 0.2956 0.3099 0.2989 0.3031 0.3034 0.3044 0.2811 0.3246 0.3315 nDCG@5 0.3015 0.3043 0.3261 0.3094 0.3122 0.3175 0.3184 0.3035 0.3487 0.3544 nDCG@10 0.3932 0.3955 0.4185 0.3996 0.4019 0.4067 0.4071 0.3952 0.4329 0.4392 Table 2: The results of different methods. *The improvement is significant at p < 0.01. Figure 4: Topic distributions in our dataset. In our experiments, word embeddings are 300dimensional and were initialized by the pretrained Glove embedding (Pennington et al., 2014). The CNN network has 400 filters, and their window sizes are 3. The negative sampling ratio K is 4 and the coefficient λ is 0.2. Adam (Kingma and Ba, 2014) is used as the optimization algorithm, and the batch size is 64. These hyperparameters were selected according to the validation set. The metrics used for result evaluation in our experiments include AUC, MRR, nDCG@5 and nDCG@10. We repeated each experiment 10 times and reported the average results. 3.2 Performance Evaluation We evaluate the performance of our TANR approach by comparing it with several baseline methods, including: (1) LibF"
P19-1344,S14-2051,0,0.710072,"on term extraction, and they take advantages of opinion label information to improve their performances. • MIN is an LSTM-based deep multi-task learning framework for ATE, opinion word extraction and sentimental sentence classification. It has two LSTMs equipped with extended memories, and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions (Li and Lam, 2017). • IHS R&D is the best system of laptop domain, and uses CRF with features extracted using named entity recognition, POS tagging, parsing, and semantic analysis (Chernyshevich, 2014). • CMLA is made up of multi-layer attention network, where each layer consists of a couple of attention with tensor operators. One attention is for extracting aspect terms, while the other is for extracting opinion terms (Wang et al., 2017). • NLANGP utilizes CRF with the word, name list and word cluster feature to tackle the task and obtains the best results in the restaurant domain. It also uses the output of a Recurrent Neural Network (RNN) as additional features to enhance their performances (Toh and Su, 2016). • WDEmb first learns embeddings of words and dependency paths based on the opt"
P19-1344,W14-4012,0,0.0400851,"Missing"
P19-1344,D14-1179,0,0.0449907,"Missing"
P19-1344,P17-1036,0,0.0610237,"ntiment analysis (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). In this paper, we only focus on the ATE task, and we solve this task by Seq2Seq learning which is often used in the generative task. We will introduce the recent study progresses in ATE and Seq2Seq learning. 4.1 Aspect Term Extraction Hu and Liu (2004) first propose to evaluate the sentiment of different aspects in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016;"
P19-1344,C10-1074,0,0.235204,"and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other r"
P19-1344,D17-1310,0,0.817565,"zza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they cannot read the whole sentence in advan"
P19-1344,D15-1168,0,0.765517,"nd their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they cannot read the whole"
P19-1344,D15-1166,0,0.0448049,"2014b; Sutskever et al., 2014), and first used in the field of machine translation. In addition, Cho et al. (2014a) improves the decoding by beam-search. However, vanilla Seq2Seq model performs worse in generating long sentences. The reason is that the encoder needs to compress the whole sentence into a fix length representation. To address this problem, Bahdanau et al. (2014) introduce an attention mechanism which selects important parts of the source sentence with respect to the previous hidden state in decoding the next state. Afterward, some studies focus on improving attention mechanism (Luong et al., 2015). So far, Seq2Seq models and attention mechanism have been applied to many fields such as dialog (Serban et al., 2016) generation, text summarization (Nallapati et al., 2016) and etc. In this paper, we first attempt to formalize the ATE as a sequence-to-sequence learning task because it can make full use of both the meaning of the sentence and label dependencies compared with existing methods. Furthermore, we design a position-aware attention model and gated unit networks to make Seq2Seq model better suit to this task. Generally, Seq2Seq model is timeconsuming in many fields because the target"
P19-1344,C10-2090,0,0.119147,"timent analysis, and aims at extracting all aspect terms present in the sentences (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). For example, given a restaurant review “The staff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is"
P19-1344,D10-1101,0,0.337486,"nd aims at extracting all aspect terms present in the sentences (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). For example, given a restaurant review “The staff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the"
P19-1344,K16-1028,0,0.199335,"ng label dependencies because they only use transition matrix to encourage valid label paths and discourage other paths (Collobert et al., 2011). As we know, the label of each word is conditioned on its previous label. For example, “O” is followed by “B/O” but not “I” in the B-I-O tagging schema. To the best of our knowledge, no neural networks based method utilizes the previous label to improve their performances directly. Recently, sequence to sequence (Seq2Seq) learning has been successfully applied to many generation tasks (Cho et al., 2014b; Sutskever et al., 2014; Bahdanau et al., 2014; Nallapati et al., 2016). Seq2Seq learning encodes a source sequence into a fixed-length vector based on which a decoder generates a target sequence. It just has the benefits of first collecting comprehensive information from the source text and then paying more attention to the generation of the target sequence. Thus, we propose to formalize the ATE task as a sequence-to-sequence learning problem, where 3538 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3538–3547 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the source and"
P19-1344,D14-1162,0,0.0893689,"N) model with GloVe embeddings to extract aspect-term (Xu et al., 2018). • BiLSTM-CNN-CRF is the state-of-the-art system for named entity recognition task, which adopts CNN and Bi-LSTM to learn character-level and word-level features respectively, and CRF is used to avoid the illegal transition between labels (Reimers and Gurevych, 2017). Baselines To evaluate the effectiveness of our approach, we compare our model with three groups of baselines. The first group of baselines utilizes conditional randomly fields (CRF): • CRF trains a CRF model with basic feature templates6 and word embeddings (Pennington et al., 2014) for ATE. The third group of baselines are joint methods for aspect term and opinion term extraction, and they take advantages of opinion label information to improve their performances. • MIN is an LSTM-based deep multi-task learning framework for ATE, opinion word extraction and sentimental sentence classification. It has two LSTMs equipped with extended memories, and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions (Li and Lam, 2017). • IHS R&D is the best system of laptop domain, and uses CRF with features extra"
P19-1344,P08-1036,0,0.0797458,"(ABSA) is a subfield of sentiment analysis (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). In this paper, we only focus on the ATE task, and we solve this task by Seq2Seq learning which is often used in the generative task. We will introduce the recent study progresses in ATE and Seq2Seq learning. 4.1 Aspect Term Extraction Hu and Liu (2004) first propose to evaluate the sentiment of different aspects in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014;"
P19-1344,S16-1002,0,0.319126,"Missing"
P19-1344,S16-1045,0,0.584081,"using named entity recognition, POS tagging, parsing, and semantic analysis (Chernyshevich, 2014). • CMLA is made up of multi-layer attention network, where each layer consists of a couple of attention with tensor operators. One attention is for extracting aspect terms, while the other is for extracting opinion terms (Wang et al., 2017). • NLANGP utilizes CRF with the word, name list and word cluster feature to tackle the task and obtains the best results in the restaurant domain. It also uses the output of a Recurrent Neural Network (RNN) as additional features to enhance their performances (Toh and Su, 2016). • WDEmb first learns embeddings of words and dependency paths based on the optimization objective formalized as w1 + r ≈ w2 , where w1 , w2 are words, r is the corresponding dependency path. Then, the learned embeddings of words and dependency paths are utilized as features in CRF for ATE (Yin et al., 2016). 5 https://github.com/facebookresearch/ fastText 6 https://sklearn-crfsuite.readthedocs. io/en/latest/ • RNCRF 8 learns structure features for each word from parse tree by Recursive Neural Networks, and the learned features are fed to CRF to decode the label for each word (Wang et al., 20"
P19-1344,S15-2082,0,0.487521,"Missing"
P19-1344,S14-2004,0,0.503276,"here labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word. The experimental results on two datasets show that Seq2Seq learning is effective in ATE accompanied with our proposed gated unit networks and position-aware attention mechanism. 1 Introduction Aspect term extraction (ATE) is a fundamental task in aspect-level sentiment analysis, and aims at extracting all aspect terms present in the sentences (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016). For example, given a restaurant review “The staff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development"
P19-1344,I08-1038,1,0.716497,"evaluate the sentiment of different aspects in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention mode"
P19-1344,D16-1059,0,0.763714,"taff is friendly, and their cheese pizza is delicious”, the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they can"
P19-1344,D09-1159,0,0.177878,"s appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other resources (Xu et al., 2018) to improve their performances. 4.2 Sequence-to-Sequence Learning Sequence-to-sequence model is a generative mo"
P19-1344,P18-2094,0,0.849769,"the ATE system should extract aspect terms “staff” and “cheese pizza”. Early works focus on detecting the pre-defined aspects in a sentence (Hu and Liu, 2004; Zhuang et al., 2006; Popescu and Etzioni, 2007). Then, some works regard ATE as a sequence labeling task and utilize Hidden Markov Model (Jin et al., 2009) or Conditional Random Fields (Jin et al., 2009; Ma and Wan, 2010; Jakob and Gurevych, 2010; Liu et al., 2013) to extract all possible aspect terms. With the development of deep learning techniques, neural networks based methods (Wang et al., 2016; Liu et al., 2015; Li and Lam, 2017; Xu et al., 2018) have achieved good performances in ATE task, and they still treat ATE as a sequence labeling problem and extract more useful features surrounding a word. Obviously, the overall meaning of the sentence is important to predict the label sequence. For example, the word memory should be an aspect term in the laptop review “The memory is enough for use.”, but it is not an aspect term in the sentence “The memory is sad for me.”. However, sequence labeling methods are not good at grasping the overall meaning of the whole sentence because they cannot read the whole sentence in advance. In addition, n"
P19-1344,J11-1002,0,0.267231,"s in a document, and all aspects are predefined artificially. The key step is to extract all possible aspects of a document (Zhuang et al., 2006; Popescu and Etzioni, 2007; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved goo"
P19-1344,P13-1161,0,0.0264517,"; Mei et al., 2007; Titov and McDonald, 2008; He et al., 2017). However, predefined aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other resources (Xu et al., 20"
P19-1344,D17-1035,0,0.0195966,"he second group of baselines employs neural networks methods to address the ATE problem: • Bi-LSTM applies different kinds of BiRNN (Elman/Jordan-type RNN) with different kinds of embeddings in the ATE task (Liu et al., 2015). • GloVe-CNN7 uses multi-layer Convolution Neural networks (CNN) model with GloVe embeddings to extract aspect-term (Xu et al., 2018). • BiLSTM-CNN-CRF is the state-of-the-art system for named entity recognition task, which adopts CNN and Bi-LSTM to learn character-level and word-level features respectively, and CRF is used to avoid the illegal transition between labels (Reimers and Gurevych, 2017). Baselines To evaluate the effectiveness of our approach, we compare our model with three groups of baselines. The first group of baselines utilizes conditional randomly fields (CRF): • CRF trains a CRF model with basic feature templates6 and word embeddings (Pennington et al., 2014) for ATE. The third group of baselines are joint methods for aspect term and opinion term extraction, and they take advantages of opinion label information to improve their performances. • MIN is an LSTM-based deep multi-task learning framework for ATE, opinion word extraction and sentimental sentence classificati"
P19-1344,P17-2023,0,0.0286843,"ned aspects may not cover all the aspects appearing in a document. Therefore, many works turn to extract all possible aspect terms in a document. The mainstream methods for aspect term extraction include the unsupervised method and supervised method. The typical unsupervised methods include bootstrapping (Wang and Wang, 2008), double propagation (Qiu et al., 2011) and others. The supervised methods contain Hidden Markov Model (Jin et al., 2009), Conditional Random Fields (Jakob and Gurevych, 2010; Li et al., 2010; Yang and Cardie, 2013; Chernyshevich, 2014; Toh and Su, 2016; Yin et al., 2016; Shu et al., 2017) and other approaches (Wu et al., 2009; Ma and Wan, 2010; Liu et al., 2013). With the developments of deep learning, neural networks based method such as recurrent NN (Liu et al., 2015; Li and Lam, 2017), recursive NN (Wang et al., 2016), convolution NN (Poria et al., 2016; Xu et al., 2018) and attention model (Wang et al., 2017) have achieved good performances in ATE. In addition, many works utilize multi-task learning (Yang and Cardie, 2013; Wang et al., 2016, 2017; Li et al., 2018) and other resources (Xu et al., 2018) to improve their performances. 4.2 Sequence-to-Sequence Learning Sequenc"
S18-1006,W14-2609,0,0.0318165,"@gmail.com Abstract weather #not happy” is non-ironic. Different approaches are proposed to recognize the complex irony in texts. Existing methods to detect irony are mainly based on rules or machine learning techniques (Joshi et al., 2017). Rules based methods usually depend on lexicons to identify irony (Khattri et al., 2015; Maynard and Greenwood, 2014). However, these methods cannot utilize the contextual information from texts. Traditional machine learning based methods such as SVM (Desai and Dave, 2016) are also effective in this task, but they usually need manually feature engineering (Barbieri et al., 2014). Recently, deep learning techniques are successfully applied to this task. For example, Ghosh et al. (2016) propose to use a CNN-LSTM model to classify the ironic and non-ironic tweets. Their method can significantly improve the classification performance without heavy feature engineering. However, existing methods are aimed to detect irony in tweets with explicit irony related hashtags. For example, tweets with #irony or #sarcasm hashtags are very likely to be ironic. Therefore, models may focus on these hashtags rather than the contextual information. To fill this gap, the SemEval-2018 task"
S18-1006,W16-0425,0,0.0609525,"epresentations of texts, and they will be used in three classification task. In addition, we incorporate several types of features to improve the model performance. Our model achieved an F-score of 70.54 (ranked 2/43) in the subtask A and 49.47 (ranked 3/29) in the subtask B. The experimental results validate the effectiveness of our system. 1 Introduction Figurative languages such as irony are widely used in web messages such as tweets to convey different sentiment. Identifying the ironic texts can help to understand the social web better and has many applications such as sentiment analysis (Ghosh and Veale, 2016). Irony detecting techniques are important to improve the performance of sentiment analysis. For example, the tweet “Monday mornings are my fave:)# not” is an irony with negative sentiment, but it will be probably classified as a positive one by a standard sentiment analysis model (Van Hee et al., 2016b). Thus, capturing the ironic information in texts is useful to predict sentiment more accurately (Van Hee et al., 2016a). However, determining whether a text is ironic is a challenging task since the the differences between ironic and non-ironic texts are usually subtle. For example, the tweet"
S18-1006,W15-4322,0,0.0715824,"Missing"
S18-1006,W15-2905,0,0.0127207,"n Liu1 , Zhigang Yuan1 and Yongfeng Huang1 1 Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University Beijing 100084, China 2 Microsoft Research Asia {wuch15,wu-sx15,ljx16,yuanzg14,yfhuang}@mails.tsinghua.edu.cn wufangzhao@gmail.com Abstract weather #not happy” is non-ironic. Different approaches are proposed to recognize the complex irony in texts. Existing methods to detect irony are mainly based on rules or machine learning techniques (Joshi et al., 2017). Rules based methods usually depend on lexicons to identify irony (Khattri et al., 2015; Maynard and Greenwood, 2014). However, these methods cannot utilize the contextual information from texts. Traditional machine learning based methods such as SVM (Desai and Dave, 2016) are also effective in this task, but they usually need manually feature engineering (Barbieri et al., 2014). Recently, deep learning techniques are successfully applied to this task. For example, Ghosh et al. (2016) propose to use a CNN-LSTM model to classify the ironic and non-ironic tweets. Their method can significantly improve the classification performance without heavy feature engineering. However, exist"
S18-1006,maynard-greenwood-2014-cares,0,0.0543699,"and Yongfeng Huang1 1 Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University Beijing 100084, China 2 Microsoft Research Asia {wuch15,wu-sx15,ljx16,yuanzg14,yfhuang}@mails.tsinghua.edu.cn wufangzhao@gmail.com Abstract weather #not happy” is non-ironic. Different approaches are proposed to recognize the complex irony in texts. Existing methods to detect irony are mainly based on rules or machine learning techniques (Joshi et al., 2017). Rules based methods usually depend on lexicons to identify irony (Khattri et al., 2015; Maynard and Greenwood, 2014). However, these methods cannot utilize the contextual information from texts. Traditional machine learning based methods such as SVM (Desai and Dave, 2016) are also effective in this task, but they usually need manually feature engineering (Barbieri et al., 2014). Recently, deep learning techniques are successfully applied to this task. For example, Ghosh et al. (2016) propose to use a CNN-LSTM model to classify the ironic and non-ironic tweets. Their method can significantly improve the classification performance without heavy feature engineering. However, existing methods are aimed to detec"
S18-1006,W17-5205,0,0.0499006,"Missing"
S18-1006,N13-1039,0,0.0670625,"Missing"
S18-1006,L16-1283,0,0.0734504,"Missing"
S18-1006,C16-1257,0,0.118572,"Missing"
S18-1006,S18-1005,0,0.170493,"Missing"
S18-1006,I17-4007,1,0.7889,"er will output the hidden representation H of texts. It will be concatenated with the sentiment features and the sentence embedding features. The sentiment features can provide additional sentiment information to detect irony, such as the sentiment polarity assigned by lexicons. The sentiment features are generated via the AffectiveTweets5 package in weka provided by Mohammad et al. (Mohammad and Bravo-Marquez, 2017). We use the TweetToLexiconFeatureVector (Bravo-Marquez et al., 2014) and TweetToSenIn order to address this problem, we propose a system2 based on a densely connected LSTM model (Wu et al., 2017) with multitask learning techniques. In our model, each LSTM layer will take all outputs of previous LSTM layers as input. Then different levels of contextual information can be learned at the same time. Our model is required to predict in three tasks simultaneously: 1) identifying the missing irony related hashtags; 2) classify ironic or non-ironic; 3) irony type classification. By using multitask learning strategy, the model can combine the information in the different tasks to improve the performance. The experimental results in both subtasks validate the effectiveness of our method. 2 Dens"
S18-1028,L16-1624,0,0.027715,"rporated into our model. The evaluation results show that our system outperform several baseline neural networks and can be further extended. reg). Our model outperforms several baseline neural networks, which proves that our model can identify the intensity of emotions and sentiment effectively. 2 Related Work Sentiment analysis in social media such as Twitter is an important task for opinion mining (Severyn and Moschitti, 2015). Traditional Twitter sentiment analysis methods mainly focus on identifying the polarities (Da Silva et al., 2014; dos Santos and Gatti, 2014) or emotion categories (Dini and Bittar, 2016) of tweets. However, it’s a difficult task to analysis the noisy tweets. They usually contain various nonstandard languages including emoticons, emojis, creatively spelled words and hash tags. In addition, these languages usually contain rich sentiment information. In order to capture such information, several lexicon-based methods are proposed. Nielsen et al. (2011) proposed to use a dictionary to incorporate emoticon information into tweet analysis models. Mohammad et al. proposed to use hash tags to identify emotion categories of tweets (2015). These lexicon-based methods are free from manu"
S18-1028,W15-4322,0,0.0688825,"Missing"
S18-1028,W17-5205,0,0.078575,"Missing"
S18-1028,W17-5207,0,0.076746,"orical and real-valued intensity of emotions or sentiment for English, Arabic, and Spanish (Mohammad et al., 2018). Existing approaches to analysis the intensity of emotions or sentiment are mainly based on lexicons and supervised learning. Lexicon-based methods usually rely on lexicons to assign the intensity scores of affective words in texts (Mohammad and Bravo-Marquez, 2017). However, these method can’t utilize the contextual information from texts. Supervised methods are mainly based on SVR (Madisetty and Desarkar, 2017), linear regression (John and Vechtomova, 2017) and neural networks (Goel et al., 2017; K¨oper et al., 2017). Usually neural network-based methods outperform SVR and linear regression-based methods siginificantly. Motivated by the successful applications of neural models in this task, we propose a system using a CNN-LSTM model with attention mechanism. Firstly, a tweet will be converted into a sequence of dense vectors by an embedding layer. Next, we use a Bi-LSTM layer to extract contextual information from them. The sequential features will be selected by an attention layer. Then we apply a CNN with different kernel sizes to extracting different local information. Thus, our m"
S18-1028,S18-1001,0,0.112862,"Missing"
S18-1028,W17-5235,0,0.0188507,"mEval-2018 Task 1 is aimed to identify the categorical and real-valued intensity of emotions or sentiment for English, Arabic, and Spanish (Mohammad et al., 2018). Existing approaches to analysis the intensity of emotions or sentiment are mainly based on lexicons and supervised learning. Lexicon-based methods usually rely on lexicons to assign the intensity scores of affective words in texts (Mohammad and Bravo-Marquez, 2017). However, these method can’t utilize the contextual information from texts. Supervised methods are mainly based on SVR (Madisetty and Desarkar, 2017), linear regression (John and Vechtomova, 2017) and neural networks (Goel et al., 2017; K¨oper et al., 2017). Usually neural network-based methods outperform SVR and linear regression-based methods siginificantly. Motivated by the successful applications of neural models in this task, we propose a system using a CNN-LSTM model with attention mechanism. Firstly, a tweet will be converted into a sequence of dense vectors by an embedding layer. Next, we use a Bi-LSTM layer to extract contextual information from them. The sequential features will be selected by an attention layer. Then we apply a CNN with different kernel sizes to extracting d"
S18-1028,N13-1039,0,0.0243735,"Missing"
S18-1028,S16-1004,0,0.0307022,"contextual information from texts. We apply attention techniques to selecting this information. A CNN layer with different kernel sizes is used to extract local features. The dense layers take the pooled CNN feature maps and predict the intensity scores. Our system achieves an average Pearson correlation score of 0.722 (ranked 12/48) in the emotion intensity regression task, and 0.810 in the valence regression task (ranked 15/38). It indicates that our system can be further extended. 1 Introduction Detecting the intensity of sentiment is an important task for fine-grained sentiment analysis (Kiritchenko et al., 2016; Mohammad and BravoMarquez, 2017). Intensity refers to the degree or amount of an emotion or degree of sentiment. For example, we can express our emotion by “very happy” or “a little angry”. The intensity can be analysis in multiple categories (i.e. low, moderate and high) or real-valued. Identifying the intensity information of sentiment has potential to applications such as electronic business, social computing and public health (Wilson, 2008). Twitter is a social platform which contains rich textual content. There have been many approaches to twitter sentiment analysis (Khan et al., 2015;"
S18-1028,C14-1008,0,0.0276401,"scores. In addition, several features are incorporated into our model. The evaluation results show that our system outperform several baseline neural networks and can be further extended. reg). Our model outperforms several baseline neural networks, which proves that our model can identify the intensity of emotions and sentiment effectively. 2 Related Work Sentiment analysis in social media such as Twitter is an important task for opinion mining (Severyn and Moschitti, 2015). Traditional Twitter sentiment analysis methods mainly focus on identifying the polarities (Da Silva et al., 2014; dos Santos and Gatti, 2014) or emotion categories (Dini and Bittar, 2016) of tweets. However, it’s a difficult task to analysis the noisy tweets. They usually contain various nonstandard languages including emoticons, emojis, creatively spelled words and hash tags. In addition, these languages usually contain rich sentiment information. In order to capture such information, several lexicon-based methods are proposed. Nielsen et al. (2011) proposed to use a dictionary to incorporate emoticon information into tweet analysis models. Mohammad et al. proposed to use hash tags to identify emotion categories of tweets (2015)."
S18-1028,W17-5206,0,0.0512007,"Missing"
S18-1063,W17-5205,0,0.0593053,"Missing"
S18-1063,E17-2017,0,0.180918,"Missing"
S18-1063,I17-4007,1,0.853953,"Missing"
S18-1063,S18-1003,0,0.0394778,"Missing"
S18-1063,N16-1174,0,0.13369,"Missing"
S18-1063,L16-1626,0,0.370853,"2018 Task 2: Residual CNN-LSTM Network with Attention for English Emoji Prediction Chuhan Wu1 , Fangzhao Wu2 , Sixing Wu1 , Zhigang Yuan1 , Junxin Liu1 and Yongfeng Huang1 1 Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University Beijing 100084, China 2 Microsoft Research Asia {wuch15,wu-sx15,yuanzg14,ljx16,yfhuang}@mails.tsinghua.edu.cn wufangzhao@gmail.com Abstract the semantics, usage or sentiment of emojis (Aoki and Uchida, 2011; Barbieri et al., 2016a,b,c; Ljubeˇsi´c and Fiˇser, 2016; Novak et al., 2015). For example, Barbieri et al. (2016b) explored the meaning and usage of emojis across different languages. Wijeratneet al. (2017) proposed to utilize the emoji sense definitions to improve the performance of emoji embedding model. However, these approaches cannot reveal the interplay between plain texts and emojis. In order to fill this gap, Barbieri et al. (2017) proposed a novel task to predict which emojis are evoked by text-based tweets. For example, given a tweet message “Love my coworkers ! @user”, a system is required to predict that emoji is associated with this tweet. Emojis are widely used by social media and social n"
S18-1063,W16-2610,0,0.0699456,"Missing"
S18-1157,O97-1002,0,0.252731,"tive. The CNNs are used to extract low-level features from the inputs. The MLP takes both the flatten CNN maps and inputs to predict the labels. The evaluation F-score of our system on the test set is 0.629 (ranked 15th), which indicates that our system still needs to be improved. However, the behaviours of our system in our experiments provide useful information, which can help to improve the collective understanding of this novel task. 1 Introduction Evaluating the similarity of words is an important task in semantic modeling. There have been different approaches based on corpus statistics (Jiang and Conrath, 1997; Mihalcea et al., 2006) and ontology (Seco et al., 2004; S´anchez et al., 2012). After an effective word representation proposed by mikolov et al (2013), word similarity can be evaluated based on word embedding weights (Levy and Goldberg, 2014). Usually higher cosine similarity of word embedding vectors indicates higher semantic similarity. However, existing semantic methods are not capable of discriminating similar words between each other without additional information. For example, it is easy for these models to tell “dog” and “puppy” is similar, but they can’t tell the differences between"
S18-1157,D14-1162,0,0.0942581,"or it will be set to 0. In this way, a 2-dim synset feature of each word can be obtained. We use the nltk tool(Bird et al., 2009) to generate the WordNet features. The features above are concatenated with word embedding as the input of MLP-CNN model. Word Embedding Since there are several out-of-vocabulary words in the dataset when using single pre-trained word embedding, we use three different embedding models to cover them. The three embedding models include pre-trained word2vec embedding1 provided by Mikolov et al. (Mikolov et al., 2013), the Glove embedding2 provided by Pennington et al. (Pennington et al., 2014) and the fastText embedding3 released by bojanowski et al. (Bojanowski et al., 2016). These embedding weights are all 300-dim. They are concatenated together as the representation of input words. 2.3 sigmoid 2.4 Word Feature Model Training and Ensemble Since the train set is unbalanced, we randomly select same numbers of positive (the attribute is discriminative) and negative (the attribute is not discriminative) samples from the train set every time. Thus, the training data we used in our experiments In our model, we use one-hot encoded POS tags and two binary features obtained by Word1 https"
S18-1157,I17-4007,1,0.844009,"Missing"
W18-0913,P16-2017,0,0.745683,"805 110 Proceedings of the Workshop on Figurative Language Processing, pages 110–114 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics M M - - - CRF or softmax Inference Bi-LSTM CNN zeros POS tag Word Cluster zeros Word embedding Embedding That take him by surprise Lemmatizing That takes him by surprise Figure 1: The architecture of our method. The final metaphor labels will be predicted by a CRF or softmax inference layer. is presented in Figure 1. We will introduce the details of modules in our model from bottom to top. We follow the approach proposed by Klebanov et al. (2016) to use the lemmatizing strategy. The first module in our model is a lemmatizer. This module is used to lemmatize the verbs in texts via a dictionary. The input is a text with a sequence of word, and output is the text with lemmatized words. Since verbs with different forms can share the same lemmas, using the lemmatized verbs in texts can simplify the semantic information and reduce the number of out-of-vocabulary words. We use the NLTK package (Bird et al., 2009) to transform the verbs into their lemmas. nated them with the word embeddings. We use the Stanford parser2 tool to obtain the POS"
W18-0913,W13-0904,0,0.0693244,"Missing"
W18-0913,P14-1045,0,0.015891,"nd they will be fine-tuned during model training. POS tags are useful in metaphor detecting task (Klebanov et al., 2014). Therefore, we also incorporate the one-hot encoded POS tags as additional features into our neural model, and concates∈S where S is the training set, and hs and ys are the hidden states and label sequence of sentence s. 2 111 https://nlp.stanford.edu/software/lex-parser.shtml Softmax: We use a dense layer with softmax activation function to predict the metaphor label sequences. Motivated by the cost-sensitive crossentropy (Santos-Rodr´ıguez et al., 2009; Yang et al., 2014; Muller et al., 2014), the loss function of our model is formulated as follows: LSof tmax = − N XX wyi yi log(yˆi ), 2.0 and 1.0 respectively. The class number of word cluster is set 50. The batch size is 50, and the max training epoch is set to 15. The optimizer we use is RMSProp in our experiment. The performance of both all POS testing and verbs testing subtasks is evaluated by precision, recall and F-score as a standard binary classification task. 3.2 (4) s∈S i=1 We compare the performance of the variants of our model and several baseline methods. The methods to be compared include: 1) CNN+CRF, using CNN to ex"
W18-0913,P14-1024,0,0.386334,"Missing"
W18-0913,W15-1405,0,0.373885,"Missing"
W18-0913,D11-1063,0,0.66098,"Missing"
W18-0913,W13-0908,0,0.0756097,"Missing"
W18-0913,I17-4007,1,0.880841,"Missing"
W18-0913,W14-2302,0,0.674251,"gineering, Tsinghua University, Beijing 100084 2 Microsoft Research Asia {wuch15,ybch14,wu-sx15,yuanzg14,yfhuang}@mails.tsinghua.edu.cn wufangzhao@gmail.com Abstract Existing computational approaches to detect metaphors are mainly based on lexicons (Mohler et al., 2013; Dodge et al., 2015) and supervised methods (Turney et al., 2011; Heintz et al., 2013; Klebanov et al., 2014, 2015, 2016). Lexiconbased methods are free from data annotation, but they are unable to detect novel metaphorical usages and capture the contextual information. Supervised methods such as logistic regression classifier (Klebanov et al., 2014) can capture richer metaphor information. However, they need sophisticated hand-crafted features. To improve the collective techniques on detecting metaphors, the metaphor shared task1 aims to detect both metaphorical verbs and metaphors with other POS. Given a sentence and their words with specific POS tags, systems are required to determine whether each word is a metaphor. We propose a CNN-LSTM model with CRF or weighted softmax classifier to address this task. Our model can take advantage of both long-range and local information by utilizing both LSTM and CNN layers. We propose to use a wei"
W18-0913,W15-1402,0,0.144713,"Missing"
W18-5909,C16-1084,0,0.197519,"Missing"
W18-5909,W18-5904,0,0.111538,"Missing"
