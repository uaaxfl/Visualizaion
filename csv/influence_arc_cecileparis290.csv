2015.mtsummit-papers.8,2012.eamt-1.57,1,0.940682,"ocuments drawing on existing wisdom about technical writing in Japanese. Since the study chiefly referred to writing guidelines intended for human understandability or readability, the overall efficacy of the rules with MT was not significant. Thus, there remains much room for investigating other patterns impactProceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 91 ing on MT performance within the municipal domain. O’Brien (2006) argued persuasively for the need to tune CL rule sets to language pair and MT system. The results of evaluation experiments by (Hartley et al., 2012) also suggested there were differences between rule-based machine translation (RBMT) and statistical machine translation (SMT) systems in terms of the impact of specific CL rules on their performance, although the MT systems as such were not the focus of their investigation. In short, it is still uncertain to what extent CL rules can be effectively generalised across MT systems or how much improvement can be attained if we compile rules specifically tuned to a given system. Practical deployment of CL requires that the readability of the source text (ST) should not be compromised in the interes"
2015.mtsummit-papers.8,J14-1005,0,0.0847801,"der of this paper is structured as follows. We describe related work in Section 2. In Section 3, we discuss how we constructed our CL rules, while Section 4 explains our experimental setup for human evaluation to assess the rules. We present our results accompanied with a discussion in Section 5. Section 6 concludes with implications for future work. 2 Related studies Controlled (natural) language or C(N)L is ‘a constructed language that is based on a certain natural language, being more restrictive concerning lexicon, syntax, and/or semantics while preserving most of its natural properties’ (Kuhn, 2014, p.123). A number of English CL rule sets have been proposed to improve MT performance as well as human comprehension, and they have been actually implemented, mainly in technical documentation (e.g., Kamprath et al., 1998; Nyberg et al., 2003). Evaluation experiments on CL for MT have also been undertaken to assess machine translatability and post-editing productivity (Pym, 1990; Bernth and Gdaniec, 2001; O’Brien and Roturier, 2007; Aikawa et al., 2007), showing evidence of the effectiveness of CL. In the case of Japanese CL, Nagao et al. (1984) devised a controlled grammar to syntactically"
2020.acl-main.520,C18-1139,0,0.0368698,"Missing"
2020.acl-main.520,S17-2091,0,0.0255184,"Missing"
2020.acl-main.520,D14-1082,0,0.0323506,"fatigue . Body Location General Feeling General Feeling Adverse drug event Figure 2: Examples involving Nested mentions. as an example, we can add two new entity types: ‘Body Location’ and ’General Feeling’, and then annotate ‘muscle pain and fatigue’ as a ‘Adverse drug event’ mention, ‘muscle’ as a ‘Body Location’ mention, and ‘pain’ and ‘fatigue’ as ‘General Feeling’ mentions (Figure 2). Then the discontinuous NER task can be converted into a Nested NER task. 3 Model Transition-based models, due to their high efficiency, are widely used for NLP tasks, such as parsing and entity recognition (Chen and Manning, 2014; Lample et al., 2016; Lou et al., 2017; Wang et al., 2018a). The model we propose for discontinuous NER is based on the shift-reduce parser (Watanabe and Sumita, 2015; Lample et al., 2016) that employs a stack to store partially processed spans and a buffer to store unprocessed tokens. The learning problem is then framed as: given the state of the parser, predict an action which is applied to change the state of the parser. This process is repeated until the parser reaches the end state (i.e., the stack and buffer are both empty). The main difference between our model and the ones in (Watanab"
2020.acl-main.520,P18-3006,1,0.916478,"biomedical natural language processing applications. In pharmacovigilance, it can be used to identify adverse drug events in consumer reviews in online medication forums, alerting medication developers, regulators and clinicians (Leaman et al., 2010; Sarker et al., 2015; Karimi et al., 2015b). In clinical settings, NER can be used to extract and summarize key information from electronic medical records such as conditions hidden in unstructured doctors’ notes (Feblowitz et al., 2011; Wang et al., 2018b). These applications require identification of complex mentions not seen in generic domains (Dai, 2018). Widely used sequence tagging techniques (flat model) encode two assumptions that do not always hold: (1) mentions do not nest or overlap, therefore each token can belong to at most one mention; and, (2) mentions comprise continuous sequences of tokens. Nested entity recognition addresses violations of the first assumption (Lu and Roth, 2015; Katiyar and Cardie, 2018; Sohrab and Miwa, 2018; Ringland et al., 2019). However, the violation of the second assumption is comparatively less studied and requires handling discontinuous mentions (see examples in Figure 1). In contrast to continuous ment"
2020.acl-main.520,N19-1149,1,0.892517,"Missing"
2020.acl-main.520,U17-1009,1,0.863225,"nto two categories: token level approach, based on sequence tagging techniques, and sentence level approach, where a combination of mentions within a sentence is jointly predicted (Dai, 2018). Token level approach Sequence tagging model takes a sequence of tokens as input and outputs a tag for each token, composed of a position indicator (e.g., BIO schema) and an entity type. The vanilla BIO schema cannot effectively represent discontinuous, overlapping mentions, therefore, some studies overcome this limitation via expanding the BIO tag set (Tang et al., 2013a; Metke-Jimenez and Karimi, 2016; Dai et al., 2017; Tang et al., 2018). In addition to BIO indicators, four new position indicators are introduced in (Metke-Jimenez and 1 Code available at GitHub: https://bit.ly/2XazEAO • O: mentions that have an interval at the current token; • X: mentions that end at the current token. Using this representation, a single entity mention can be represented as a path from node A to node X, incorporating at least one node of type B. Note that both token level and sentence level approaches predict first an intermediate representation of mentions (e.g., a sequence of tags in (MetkeJimenez and Karimi, 2016) and a"
2020.acl-main.520,N19-1423,0,0.00981965,"nizes entity mentions from a sentence. Note that, given one parser state, not all types of actions are valid. For example, if the stack does not contain any span, only SHIFT and OUT actions are valid because all other actions involve popping spans from the stack. We employ hard constraints that we only select the most likely action from valid actions. where ELMoi is the output representation of pretrained ELMo models (frozen) for the i-th token. These token representations c are directly used to represent tokens in the buffer. We also explore a variant that uses the output of pretrained BERT (Devlin et al., 2019) as token representations c, and fine-tune the BERT model. However, this finetuning approach with BERT does not achieve as good performance as feature extraction approach with ELMo (Peters et al., 2019). Following the work in (Dyer et al., 2015), we use Stack-LSTM to represent spans in the stack. That is, if a token is moved from the buffer to the stack, its representation is learned using: s0 = Stack-LSTM(sD . . . s1 ; cSHIFT ), where D is the number of spans in the stack. Once REDUCE related actions are applied, we use a multi-layer perceptron to learn the representation of the concatenated"
2020.acl-main.520,P15-1033,0,0.015402,"from the stack. We employ hard constraints that we only select the most likely action from valid actions. where ELMoi is the output representation of pretrained ELMo models (frozen) for the i-th token. These token representations c are directly used to represent tokens in the buffer. We also explore a variant that uses the output of pretrained BERT (Devlin et al., 2019) as token representations c, and fine-tune the BERT model. However, this finetuning approach with BERT does not achieve as good performance as feature extraction approach with ELMo (Peters et al., 2019). Following the work in (Dyer et al., 2015), we use Stack-LSTM to represent spans in the stack. That is, if a token is moved from the buffer to the stack, its representation is learned using: s0 = Stack-LSTM(sD . . . s1 ; cSHIFT ), where D is the number of spans in the stack. Once REDUCE related actions are applied, we use a multi-layer perceptron to learn the representation of the concatenated span. For example, the REDUCE action takes the representation of the top two spans in the stack: s0 and s1 , and produces a new span representation: ˜ s = WT [s0 ; s1 ] + b, where W and b denote the parameters for the composition function. The n"
2020.acl-main.520,D16-1003,0,0.0373305,"Missing"
2020.acl-main.520,D09-1015,0,0.0530283,"es components into discontinuous mentions based on a classifier’s decision was explored in recent work by Wang and Lu (2019). Discontinuous NER vs. Nested NER Although discontinuous mentions may overlap, we discriminate this overlapping from the one in nested NER. That is, if one mention is completely contained by the other, we call mentions involved nested entity mentions. In contrast, overlapping in discontinuous NER is usually that two mentions overlap, but no one is completely contained by the other. Most of existing nested NER models are built to tackle the complete containing structure (Finkel and Manning, 2009; Lu and Roth, 2015), and they cannot be directly used to identify overlapping mentions studied in this paper, nor mention the discontinuous mentions. However, we note that there is a possible perspective to solve discontinuous NER task by adding fine-grained entity types into the schema. Taking the second sentence in Figure 1 have much muscle pain and fatigue . Body Location General Feeling General Feeling Adverse drug event Figure 2: Examples involving Nested mentions. as an example, we can add two new entity types: ‘Body Location’ and ’General Feeling’, and then annotate ‘muscle pain and fa"
2020.acl-main.520,W10-1915,0,0.0154704,"us mentions, taken from the ShARe 13 (Pradhan et al., 2013) and CADEC (Karimi et al., 2015a) data sets, respectively. The first example contains a discontinuous mention ‘left atrium dilated’, the second example contains two mentions that overlap: ‘muscle pain’ and ‘muscle fatigue’ (discontinuous). Introduction Named Entity Recognition (NER) is a critical component of biomedical natural language processing applications. In pharmacovigilance, it can be used to identify adverse drug events in consumer reviews in online medication forums, alerting medication developers, regulators and clinicians (Leaman et al., 2010; Sarker et al., 2015; Karimi et al., 2015b). In clinical settings, NER can be used to extract and summarize key information from electronic medical records such as conditions hidden in unstructured doctors’ notes (Feblowitz et al., 2011; Wang et al., 2018b). These applications require identification of complex mentions not seen in generic domains (Dai, 2018). Widely used sequence tagging techniques (flat model) encode two assumptions that do not always hold: (1) mentions do not nest or overlap, therefore each token can belong to at most one mention; and, (2) mentions comprise continuous seque"
2020.acl-main.520,D15-1102,0,0.368239,"xtract and summarize key information from electronic medical records such as conditions hidden in unstructured doctors’ notes (Feblowitz et al., 2011; Wang et al., 2018b). These applications require identification of complex mentions not seen in generic domains (Dai, 2018). Widely used sequence tagging techniques (flat model) encode two assumptions that do not always hold: (1) mentions do not nest or overlap, therefore each token can belong to at most one mention; and, (2) mentions comprise continuous sequences of tokens. Nested entity recognition addresses violations of the first assumption (Lu and Roth, 2015; Katiyar and Cardie, 2018; Sohrab and Miwa, 2018; Ringland et al., 2019). However, the violation of the second assumption is comparatively less studied and requires handling discontinuous mentions (see examples in Figure 1). In contrast to continuous mentions which are often short spans of text, discontinuous mentions consist of components that are separated by intervals. Recognizing discontinuous mentions is particularly challenging as exhaustive enumeration of possible mentions, including discontinuous and overlapping spans, is exponential in sentence length. Existing approaches for discont"
2020.acl-main.520,D15-1166,0,0.0597315,"s1 . 3.2 3.1 Representation of the Parser State Given a sequence of N tokens, we first run a bidirectional LSTM (Graves et al., 2013) to derive the contextual representation of each token. Specif(1) Capturing Discontinuous Dependencies We hypothesize that the interactions between spans in the stack and tokens in the buffer are important factors in recognizing discontinuous mentions. Considering the example in Figure 3, a span in the 5863 stack (e.g., ‘muscle’) may need to combine with a future token in the buffer (e.g., ‘fatigue’). To capture this interaction, we use multiplicative attention (Luong et al., 2015) to let the span in the stack si learn which token in the buffer to attend, and thus a weighted sum of the representation of tokens in the buffer B: sai = a softmax(sT i Wi B)B. CADEC Avg mention L. Avg Disc.M L. Avg interval L. (2) 2.7 3.5 3.3 1.8 2.6 3.0 1.7 2.5 3.2 Discontinuous Mentions 2 components 650 (95.7) 1,026 (94.3) 1,574 (95.3) 3 components 27 ( 3.9) 62 ( 5.6) 76 ( 4.6) 4 components 2 ( 0.2) 0 ( 0.0) 0 ( 0.0) Selecting an Action Finally, we build the parser representation as the concatenation of the representation of top three spans from the stack (s0 , s1 , s2 ) and its attended r"
2020.acl-main.520,P16-1101,0,0.0337389,"labels the mention with entity type y. Stack have much muscle pain much muscle pain muscle pain muscle Predicted Action Buffer pain and fatigue and fatigue and fatigue and fatigue OUT OUT SHIFT SHIFT muscle pain and fatigue LEFTREDUCE muscle muscle pain and fatigue COMPLETE and fatigue OUT muscle muscle fatigue ically, for the i-th token in the sequence, its representation can be denoted as: h−−−−→ i ←−−−− c˜i = LSTM(t0 , . . . , ti ); LSTM(ti , . . . , tN−1 ) , where ti is the concatenation of the embeddings for the i-th token, its character level representation learned using a CNN network (Ma and Hovy, 2016). Pretrained contextual word representations have shown its usefulness on improving various NLP tasks. Here, we can also concatenate pretrained contextual word representations using ELMo (Peters et al., 2018) with c˜i , resulting in: SHIFT ci = [c˜i ; ELMoi ] , muscle fatigue REDUCE muscle fatigue COMPLETE Figure 3: An example sequence of transitions. Given the states of stack and buffer (blue highlighted), as well as the previous actions, predict the next action (i.e., LEFT-REDUCE) which is then applied to change the states of stack and buffer. • REDUCE pops the top two spans s0 and s1 from t"
2020.acl-main.520,H05-1124,0,0.326592,"ingland et al., 2019). However, the violation of the second assumption is comparatively less studied and requires handling discontinuous mentions (see examples in Figure 1). In contrast to continuous mentions which are often short spans of text, discontinuous mentions consist of components that are separated by intervals. Recognizing discontinuous mentions is particularly challenging as exhaustive enumeration of possible mentions, including discontinuous and overlapping spans, is exponential in sentence length. Existing approaches for discontinuous NER either suffer from high time complexity (McDonald et al., 2005) or ambiguity in translating intermediate representations into mentions (Tang et al., 2013a; MetkeJimenez and Karimi, 2016; Muis and Lu, 2016). In addition, current art uses traditional approaches that rely on manually designed features, which are tailored to recognize specific entity types. Also, these features usually do not generalize well in different genres (Leaman et al., 2015). Motivations The main motivation for recognizing discontinuous mentions is that they usually represent compositional concepts that differ from concepts represented by individual components. For example, the mentio"
2020.acl-main.520,D16-1008,0,0.532341,"(see examples in Figure 1). In contrast to continuous mentions which are often short spans of text, discontinuous mentions consist of components that are separated by intervals. Recognizing discontinuous mentions is particularly challenging as exhaustive enumeration of possible mentions, including discontinuous and overlapping spans, is exponential in sentence length. Existing approaches for discontinuous NER either suffer from high time complexity (McDonald et al., 2005) or ambiguity in translating intermediate representations into mentions (Tang et al., 2013a; MetkeJimenez and Karimi, 2016; Muis and Lu, 2016). In addition, current art uses traditional approaches that rely on manually designed features, which are tailored to recognize specific entity types. Also, these features usually do not generalize well in different genres (Leaman et al., 2015). Motivations The main motivation for recognizing discontinuous mentions is that they usually represent compositional concepts that differ from concepts represented by individual components. For example, the mention ‘left atrium dilated’ in the first example of Figure 1 describes a disorder which has its own CUI (Concept Unique Identi5860 Proceedings of"
2020.acl-main.520,N18-1079,0,0.0202103,"e key information from electronic medical records such as conditions hidden in unstructured doctors’ notes (Feblowitz et al., 2011; Wang et al., 2018b). These applications require identification of complex mentions not seen in generic domains (Dai, 2018). Widely used sequence tagging techniques (flat model) encode two assumptions that do not always hold: (1) mentions do not nest or overlap, therefore each token can belong to at most one mention; and, (2) mentions comprise continuous sequences of tokens. Nested entity recognition addresses violations of the first assumption (Lu and Roth, 2015; Katiyar and Cardie, 2018; Sohrab and Miwa, 2018; Ringland et al., 2019). However, the violation of the second assumption is comparatively less studied and requires handling discontinuous mentions (see examples in Figure 1). In contrast to continuous mentions which are often short spans of text, discontinuous mentions consist of components that are separated by intervals. Recognizing discontinuous mentions is particularly challenging as exhaustive enumeration of possible mentions, including discontinuous and overlapping spans, is exponential in sentence length. Existing approaches for discontinuous NER either suffer f"
2020.acl-main.520,N16-1030,0,0.0221744,"General Feeling General Feeling Adverse drug event Figure 2: Examples involving Nested mentions. as an example, we can add two new entity types: ‘Body Location’ and ’General Feeling’, and then annotate ‘muscle pain and fatigue’ as a ‘Adverse drug event’ mention, ‘muscle’ as a ‘Body Location’ mention, and ‘pain’ and ‘fatigue’ as ‘General Feeling’ mentions (Figure 2). Then the discontinuous NER task can be converted into a Nested NER task. 3 Model Transition-based models, due to their high efficiency, are widely used for NLP tasks, such as parsing and entity recognition (Chen and Manning, 2014; Lample et al., 2016; Lou et al., 2017; Wang et al., 2018a). The model we propose for discontinuous NER is based on the shift-reduce parser (Watanabe and Sumita, 2015; Lample et al., 2016) that employs a stack to store partially processed spans and a buffer to store unprocessed tokens. The learning problem is then framed as: given the state of the parser, predict an action which is applied to change the state of the parser. This process is repeated until the parser reaches the end state (i.e., the stack and buffer are both empty). The main difference between our model and the ones in (Watanabe and Sumita, 2015; L"
2020.acl-main.520,N18-1202,0,0.0147486,"e pain and fatigue LEFTREDUCE muscle muscle pain and fatigue COMPLETE and fatigue OUT muscle muscle fatigue ically, for the i-th token in the sequence, its representation can be denoted as: h−−−−→ i ←−−−− c˜i = LSTM(t0 , . . . , ti ); LSTM(ti , . . . , tN−1 ) , where ti is the concatenation of the embeddings for the i-th token, its character level representation learned using a CNN network (Ma and Hovy, 2016). Pretrained contextual word representations have shown its usefulness on improving various NLP tasks. Here, we can also concatenate pretrained contextual word representations using ELMo (Peters et al., 2018) with c˜i , resulting in: SHIFT ci = [c˜i ; ELMoi ] , muscle fatigue REDUCE muscle fatigue COMPLETE Figure 3: An example sequence of transitions. Given the states of stack and buffer (blue highlighted), as well as the previous actions, predict the next action (i.e., LEFT-REDUCE) which is then applied to change the states of stack and buffer. • REDUCE pops the top two spans s0 and s1 from the stack and concatenates them as a new span which is then pushed back to the stack. • LEFT-REDUCE is similar to the REDUCE action, except that the span s1 is kept in the stack. This action indicates the span"
2020.acl-main.520,W19-4302,0,0.0298239,"Missing"
2020.acl-main.520,P19-1510,1,0.820586,"such as conditions hidden in unstructured doctors’ notes (Feblowitz et al., 2011; Wang et al., 2018b). These applications require identification of complex mentions not seen in generic domains (Dai, 2018). Widely used sequence tagging techniques (flat model) encode two assumptions that do not always hold: (1) mentions do not nest or overlap, therefore each token can belong to at most one mention; and, (2) mentions comprise continuous sequences of tokens. Nested entity recognition addresses violations of the first assumption (Lu and Roth, 2015; Katiyar and Cardie, 2018; Sohrab and Miwa, 2018; Ringland et al., 2019). However, the violation of the second assumption is comparatively less studied and requires handling discontinuous mentions (see examples in Figure 1). In contrast to continuous mentions which are often short spans of text, discontinuous mentions consist of components that are separated by intervals. Recognizing discontinuous mentions is particularly challenging as exhaustive enumeration of possible mentions, including discontinuous and overlapping spans, is exponential in sentence length. Existing approaches for discontinuous NER either suffer from high time complexity (McDonald et al., 2005"
2020.acl-main.520,P15-1113,0,0.0284323,"pes: ‘Body Location’ and ’General Feeling’, and then annotate ‘muscle pain and fatigue’ as a ‘Adverse drug event’ mention, ‘muscle’ as a ‘Body Location’ mention, and ‘pain’ and ‘fatigue’ as ‘General Feeling’ mentions (Figure 2). Then the discontinuous NER task can be converted into a Nested NER task. 3 Model Transition-based models, due to their high efficiency, are widely used for NLP tasks, such as parsing and entity recognition (Chen and Manning, 2014; Lample et al., 2016; Lou et al., 2017; Wang et al., 2018a). The model we propose for discontinuous NER is based on the shift-reduce parser (Watanabe and Sumita, 2015; Lample et al., 2016) that employs a stack to store partially processed spans and a buffer to store unprocessed tokens. The learning problem is then framed as: given the state of the parser, predict an action which is applied to change the state of the parser. This process is repeated until the parser reaches the end state (i.e., the stack and buffer are both empty). The main difference between our model and the ones in (Watanabe and Sumita, 2015; Lample et al., 2016) is the set of transition actions. Watanabe and Sumita (2015) use SHIFT, REDUCE, UNARY, FINISH, and IDEA for the constituent pa"
2020.acl-main.520,P17-1114,0,0.188678,"Missing"
2020.acl-main.520,D18-1309,0,0.0288049,"Missing"
2020.acl-main.520,E17-1014,0,0.0261956,"The trained model which is most effective on the development set, measured using the F1 score, is used to evaluate the test set. 5 Baseline Models We choose one flat NER model which is strong at recognizing continuous mentions, and two discontinuous NER models as our baseline models: Flat model To train the flat model on our data sets, we use an off-the-shelf framework: Flair (Akbik et al., 2018), which achieves the state-of-the-art performance on CoNLL 03 data set. Recall that the flat model cannot be directly applied to data sets containing discontinuous mentions. Following the practice in (Stanovsky et al., 2017), we replace the discontinuous mention with the shortest span that fully covers it, and merge overlapping mentions into a single mention that covers both. Note that, different from (Stanovsky et al., 2017), we apply these changes only on the training set, but not on the development set and the test set. BIO extension model The original implementation in (Metke-Jimenez and Karimi, 2016) used a CRF model with manually designed features. We report their results on CADEC in Table 2 and reimplement a BiLSTM-CRF-ELMo model using their tag schema (denoted as ‘BIO Extension’ in Table 2). Graph-based m"
2020.acl-main.520,D19-1644,0,0.342869,"encoded using the same gold sequence of tags. We refer to a survey by (Dai, 2018) for more discussions on these models, and (Muis and Lu, 2016) for a theoretical analysis of ambiguity of these models. Similar to prior work, our proposed transitionbased model uses an intermediate representation (i.e., a sequence of actions). However, it does not suffer from this ambiguity issue. That is, the output sequence of actions can always be unambiguously decoded into mention outputs. The other two methods that focus on the discontinuous NER problem in literature are described in (McDonald et al., 2005; Wang and Lu, 2019). McDonald et al. (2005) solve the NER task as a structured multi-label classification problem. Instead of starting and ending indices, they represent each entity mention using the set of token positions that belong to the mention. This representation is flexible, as it allows mentions consisting of discontinuous tokens and does not require mentions to exclude each other. However, this method suffers from high time complexity. Tang et al. (2018) compare this representation with BIO variant schema proposed in (Metke-Jimenez and Karimi, 2016), and found that they achieve competitive F1 scores, a"
2020.acl-main.520,D18-1124,0,0.0606066,"n’ and ‘muscle fatigue’ (discontinuous). Introduction Named Entity Recognition (NER) is a critical component of biomedical natural language processing applications. In pharmacovigilance, it can be used to identify adverse drug events in consumer reviews in online medication forums, alerting medication developers, regulators and clinicians (Leaman et al., 2010; Sarker et al., 2015; Karimi et al., 2015b). In clinical settings, NER can be used to extract and summarize key information from electronic medical records such as conditions hidden in unstructured doctors’ notes (Feblowitz et al., 2011; Wang et al., 2018b). These applications require identification of complex mentions not seen in generic domains (Dai, 2018). Widely used sequence tagging techniques (flat model) encode two assumptions that do not always hold: (1) mentions do not nest or overlap, therefore each token can belong to at most one mention; and, (2) mentions comprise continuous sequences of tokens. Nested entity recognition addresses violations of the first assumption (Lu and Roth, 2015; Katiyar and Cardie, 2018; Sohrab and Miwa, 2018; Ringland et al., 2019). However, the violation of the second assumption is comparatively less studie"
2020.acl-main.520,E12-2021,0,\N,Missing
2020.alta-1.10,W19-1909,0,0.0647168,"Missing"
2020.alta-1.10,2020.trac-1.25,0,0.0313449,"Missing"
2020.alta-1.10,2020.findings-emnlp.151,1,0.832294,"Missing"
2020.alta-1.10,N19-1144,0,0.0481704,"Missing"
2020.coling-industry.14,D16-1084,0,0.027855,"Missing"
2020.coling-industry.14,N19-1423,0,0.00830786,"osts contributing to the SLO assessment, 2) risk classification, for identifying the different facet(s), or SLO risk(s), being discussed in the posts, and 3) risk-aware stance classification, for detecting stances in the posts that are specific to each SLO risk. The outcome from text analytic pipeline is fed into the SLO score computation component for converting the stances into numerical SLO scores. To train and evaluate the classifiers for each task above, we created both a silver standard and a gold standard dataset and employed state-of-the-art language understanding models such as BERT (Devlin et al., 2019). To monitor the derived SLO scores, a monitoring engine keeps track of the time series of the SLO scores of multiple organisations operating in a market. Specifically, it leverages Control Charts (Kan, 2002), a powerful tool for statistical process control. The monitoring engine discovers if an organisation is experiencing significant changes in its SLO score by contrasting its time series with a benchmark of the market. 147 We conduct several quantitative experiments to evaluate the performance of our classifiers, and thus the effectiveness of our text analysis pipeline. We then present a ca"
2020.coling-industry.14,E17-2068,0,0.0692815,"Missing"
2020.coling-industry.14,D14-1181,0,0.00529179,"Missing"
2020.coling-industry.14,S16-1003,0,0.1423,"e market (all the organisations together), and 3) the Social Feed, with the most recent social media posts (e.g., tweets) about the selected organisation, with both the stances and SLO risk categories (i.e., environment, social, and economic) identified. To carry out the SLO assessment, SIRTA extracts opinion information from posts published on social media (Twitter), along the different SLO facets, and then transforms that information into SLO scores. In contrast to many opinion mining systems that rely primarily on sentiment analysis, e.g., (Pang et al., 2008), we focus on stance detection (Mohammad et al., 2016a), which is more suitable for our task because it indicates whether someone is for, neutral or against a specific company, not just whether the surface sentiment of their posts is positive or negative. The novel aspect of SIRTA’s SLO assessment engine includes a specialised text classification pipeline (see the Text Analytic Pipeline on the bottom of Figure 1), where three chained text classification tasks are performed for opinion extraction: 1) relevance classification, for finding posts contributing to the SLO assessment, 2) risk classification, for identifying the different facet(s), or S"
2020.coling-industry.14,N13-1039,0,0.10275,"Missing"
2020.coling-industry.14,D14-1162,0,0.0862798,"Missing"
2020.coling-industry.14,S16-2021,0,0.0419775,"Missing"
2020.coling-industry.14,C18-1203,0,0.0341202,"Missing"
2020.findings-emnlp.151,W19-1909,0,0.0336225,"Missing"
2020.findings-emnlp.151,D19-1539,0,0.163784,"ease two pretrained BERT models trained on tweets and forum text, and we demonstrate the effectiveness of these two resources on a range of NLP data sets using social media text; and, (2) we investigate the correlation of source-target similarity and task accuracy using different domain-specific BERT models. We find that simple similarity measures can be used to nominate in-domain pretraining data (Figure 1). 2 Related Work Selecting data to pretrain BERT There are two known strategies: (1) collecting very large generic data, such as web crawl and news (Radford et al., 2019; Liu et al., 2019; Baevski et al., 2019); and, (2) selecting in-domain data, which we refer to as 1 We do not explicitly consider mode in this study, because all data used are written text. 1675 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1675–1681 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Standard approach * Pretrain BERT from scratch on generic data * Fine-tune BERT on target labeled data Target Labeled Data BERT Domain-speciﬁc BERT * Pretrain BERT from scratch on generic data * Continue pretraining BERT on domain-speciﬁc corpus (in-domain data) * Fine-tune BERT on ta"
2020.findings-emnlp.151,I13-1041,0,0.226271,"dels achieve the highest F1 score on 6 out of 8 target tasks that use social media text (Table 2). On CADEC (medications) and SemEval-14 laptop, SciBERT achieves the highest score due to the overlapping fields (i.e., medication and computer hardware, respectively). We note, however, that our Forum BERT achieves very close results. This demonstrates the effectiveness of our pretrained models on target tasks using social media text. To our surprise, on target tasks using tweets, forum BERT achieves better results than Twitter BERT on 3 classification tasks. On one hand, this may be explained by Baldwin et al. (2013)’s observation that forum text is the ‘median’ data, which is similar to all other types of 1677 4 Internet archive, Accessed 1 June 2020. Yelp Challenge, Accessed 1 June 2020. 6 Kaggle Twitter US Airline Sentiment Challenge 5 Target Text type Tweets Forum Non-social media Corpus Airline (C) BTC (N ) SMM4H-18 task3 (C) SMM4H-18 task4 (C) CADEC (N ) SemEval-14 laptop (N ) SemEval-14 restaurant (N ) SST-2 (C) EBM (N ) i2b2-10 (N ) JNLPBA (N ) Paper Field (C) BERT (3.3B) Bio (18B) Clinical (0.5B) Sci (3.1B) Twitter (0.9B) Forum (0.6B) 80.5± 0.3 78.0± 0.5 76.5± 0.9 89.4± 0.5 71.9± 0.6 81.1± 0.8 87"
2020.findings-emnlp.151,W04-1213,0,0.191194,"ying sentiment on movie reviews (Socher et al., 2013). In addition, we use four tasks that do not use social media text to investigate how our BERT models perform on out-of-domain target tasks: Paper Field: classifying the research topic based on the title of scholar articles about various fields (Beltagy et al., 2019); EBM: identifying intervention, outcome etc. on scholar articles about clinical trials (Nye et al., 2018); i2b2-10: identifying treatment, test and problem on clinical notes about health (Uzuner et al., 2011); JNLPBA: identifying RNA, DNA etc. on scholar articles about biology (Kim et al., 2004). 4.2 Results We observe that our BERT models achieve the highest F1 score on 6 out of 8 target tasks that use social media text (Table 2). On CADEC (medications) and SemEval-14 laptop, SciBERT achieves the highest score due to the overlapping fields (i.e., medication and computer hardware, respectively). We note, however, that our Forum BERT achieves very close results. This demonstrates the effectiveness of our pretrained models on target tasks using social media text. To our surprise, on target tasks using tweets, forum BERT achieves better results than Twitter BERT on 3 classification task"
2020.findings-emnlp.151,D19-1371,0,0.401017,"masked language model objective and then fine-tunes the model on the target task. We investigate the impact of the domain (i.e., the similarity between the underlying distribution of source and target data) of pretraining data on the effectiveness of pretrained models. We also propose a cost-effective way to select pretraining data. Recent studies on domain-specific BERT models, which are pretrained on specialty source data, empirically show that, when in-domain data is used for pretraining, target task performance can be improved (Lee et al., 2019; Alsentzer et al., 2019; Huang et al., 2019; Beltagy et al., 2019). These publicly available domain-specific BERT models are valuable to the NLP community. However, the selection of in-domain data usually resorts to intuition, which varies across NLP practitioners (Dai et al., 2019). According to Halliday and Hasan (1989), the context specific usage of language is affected by three factors: field (the subject matter being discussed), tenor (the relationship between the participants in the discourse and their purpose) and mode (communication medium, e.g., ‘spoken’ or ‘written’).1 Generally, the selection of pretraining data in existing domain-specific BERT mo"
2020.findings-emnlp.151,W06-1615,0,0.212092,"on larger data leads to better pretrained models. For example, Baevski et al. (2019) empirically show that the average GLUE score (Wang et al., 2019) can increase from lower than 80 to higher than 81 when the size of pretraining data increases from 562 million to 18 billion tokens. Our study uses the second strategy. However, we select our pretraining data from the tenor perspective rather than the field. A summary of the source data used in these domain-specific BERT models can be found in Table 1. Finding in-domain data Our study relates to the literature on investigating domain similarity (Blitzer et al., 2006; Ben-David et al., 2007; Ruder and Plank, 2017) and text similarity (Mihalcea et al., 2006; Pavlick et al., 2015; Kusner et al., 2015). Our work is also inspired by the study by Dai et al. (2019) on the impact of source data on pretrained LSTM-based models (i.e., ELMo) and by Van Asch and Daelemans (2010) on the correlation between similarity and accuracy loss of POS taggers. Model Original BERT Books and encyclopedia articles, various fields BioBERT (Lee et al., Scholar articles on biology 2019) ClinicalBERT (Alsentzer Nursing and physician notes on et al., 2019) hospital admission SciBERT ("
2020.findings-emnlp.151,N19-1149,1,0.917395,"data on the effectiveness of pretrained models. We also propose a cost-effective way to select pretraining data. Recent studies on domain-specific BERT models, which are pretrained on specialty source data, empirically show that, when in-domain data is used for pretraining, target task performance can be improved (Lee et al., 2019; Alsentzer et al., 2019; Huang et al., 2019; Beltagy et al., 2019). These publicly available domain-specific BERT models are valuable to the NLP community. However, the selection of in-domain data usually resorts to intuition, which varies across NLP practitioners (Dai et al., 2019). According to Halliday and Hasan (1989), the context specific usage of language is affected by three factors: field (the subject matter being discussed), tenor (the relationship between the participants in the discourse and their purpose) and mode (communication medium, e.g., ‘spoken’ or ‘written’).1 Generally, the selection of pretraining data in existing domain-specific BERT models is based on the field rather than the tenor. For example, BioBERT (Lee et al., 2019) and SciBERT (Beltagy et al., 2019) are both pretrained on scholar articles, but on different fields (biology and computer scien"
2020.findings-emnlp.151,C16-1111,0,0.0274135,"to achieve state-of-the-art performance on these data sets. Our BERT results follow the standard twostage approach of finetuning the pretrained model. Domain-specific BERTs add a stage in the middle: finetuning BERT on domain-specific unlabeled data (cf. Figure 1). 4.1 Target Tasks We use eight target tasks with their text sampled from Twitter and forums, to examine whether our BERT models can lead to improvements, compared to the original BERT. These tasks are Airline6 : classifying sentiment on tweets about major U.S. airlines; BTC: identifying location, person, and organization on tweets (Derczynski et al., 2016); SMM4H-18: classifying whether the user reports an adverse drug events (task3) (Weissenbacher et al., 2018), or intends to receive a seasonal influenza vaccine (task4) on tweets about health (Joshi et al., 2018); CADEC: identifying adverse drug events etc. on reviews about medications (Karimi et al., 2015); SemEval-14: identifying product or service attributes on reviews about laptops and restaurants (Pontiki et al., 2014); SST: classifying sentiment on movie reviews (Socher et al., 2013). In addition, we use four tasks that do not use social media text to investigate how our BERT models perf"
2020.findings-emnlp.151,N19-1423,0,0.0470476,"e language variety, we pretrain two models on tweets and forum text respectively, and empirically demonstrate the effectiveness of these two resources. In addition, we investigate how similarity measures can be used to nominate in-domain pretraining data. We publicly release our pretrained models at https://bit.ly/35RpTf0. 1 Introduction Sequence transfer learning (Ruder, 2019), that pretrains language representations on unlabeled text (source) and then adapts these representations to a supervised task (target), has demonstrated its effectiveness on a range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Approaches vary in model, pretraining objective, pretraining data and adaptation strategy. We consider a widely used method, BERT (Devlin et al., 2019). It pretrains a transformer-based model using a masked language model objective and then fine-tunes the model on the target task. We investigate the impact of the domain (i.e., the similarity between the underlying distribution of source and target data) of pretraining data on the effectiveness of pretrained models. We also propose a cost-effective way to select pretraining data. Recent studies on domain-specific BERT model"
2020.findings-emnlp.151,2020.acl-main.740,0,0.052046,"Missing"
2020.findings-emnlp.151,W11-2123,0,0.0699158,"inate in-domain source data? 5.1 Measuring Similarity We use three measures of the similarity between source and target data. We then observe whether 1678 PPL 5.2 1.0 TVC 0.8 0.6 TTR 0.4 0.2 JSD PPL TVC TTR Figure 2: Correlation between different similarity measures and diversity measure and the improvement (∆) due to domain-specific BERT models. these similarity values correlate with the usefulness of pretrained models in § 5.2. Language model perplexity (PPL) has been used to provide a proxy to estimate corpus similarity (Baldwin et al., 2013). We construct KneserNey smoothed 3-gram models (Heafield, 2011) on source data and use the perplexity of target data relative to these language models as the similarity between source and target data. Jensen-Shannon divergence (JSD), based on term distributions, has been successfully used for domain adaptation (Ruder and Plank, 2017). We first measure the probability of each term (up to 3-gram) in source and target data, separately. Then, we use the Jensen-Shannon divergence between these two probability distributions as the similarity between source and target data. Target vocabulary covered (TVC) measures the percentage of the target vocabulary present"
2020.findings-emnlp.151,W18-5911,1,0.835263,"on domain-specific unlabeled data (cf. Figure 1). 4.1 Target Tasks We use eight target tasks with their text sampled from Twitter and forums, to examine whether our BERT models can lead to improvements, compared to the original BERT. These tasks are Airline6 : classifying sentiment on tweets about major U.S. airlines; BTC: identifying location, person, and organization on tweets (Derczynski et al., 2016); SMM4H-18: classifying whether the user reports an adverse drug events (task3) (Weissenbacher et al., 2018), or intends to receive a seasonal influenza vaccine (task4) on tweets about health (Joshi et al., 2018); CADEC: identifying adverse drug events etc. on reviews about medications (Karimi et al., 2015); SemEval-14: identifying product or service attributes on reviews about laptops and restaurants (Pontiki et al., 2014); SST: classifying sentiment on movie reviews (Socher et al., 2013). In addition, we use four tasks that do not use social media text to investigate how our BERT models perform on out-of-domain target tasks: Paper Field: classifying the research topic based on the title of scholar articles about various fields (Beltagy et al., 2019); EBM: identifying intervention, outcome etc. on sc"
2020.findings-emnlp.151,2021.ccl-1.108,0,0.0601863,"Missing"
2020.findings-emnlp.151,2020.emnlp-demos.2,0,0.0504136,"Missing"
2020.findings-emnlp.151,P18-1019,0,0.0261697,"ts etc. on reviews about medications (Karimi et al., 2015); SemEval-14: identifying product or service attributes on reviews about laptops and restaurants (Pontiki et al., 2014); SST: classifying sentiment on movie reviews (Socher et al., 2013). In addition, we use four tasks that do not use social media text to investigate how our BERT models perform on out-of-domain target tasks: Paper Field: classifying the research topic based on the title of scholar articles about various fields (Beltagy et al., 2019); EBM: identifying intervention, outcome etc. on scholar articles about clinical trials (Nye et al., 2018); i2b2-10: identifying treatment, test and problem on clinical notes about health (Uzuner et al., 2011); JNLPBA: identifying RNA, DNA etc. on scholar articles about biology (Kim et al., 2004). 4.2 Results We observe that our BERT models achieve the highest F1 score on 6 out of 8 target tasks that use social media text (Table 2). On CADEC (medications) and SemEval-14 laptop, SciBERT achieves the highest score due to the overlapping fields (i.e., medication and computer hardware, respectively). We note, however, that our Forum BERT achieves very close results. This demonstrates the effectiveness"
2020.findings-emnlp.151,P15-2070,0,0.0245962,"Missing"
2020.findings-emnlp.151,S14-2004,0,0.0347605,"o the original BERT. These tasks are Airline6 : classifying sentiment on tweets about major U.S. airlines; BTC: identifying location, person, and organization on tweets (Derczynski et al., 2016); SMM4H-18: classifying whether the user reports an adverse drug events (task3) (Weissenbacher et al., 2018), or intends to receive a seasonal influenza vaccine (task4) on tweets about health (Joshi et al., 2018); CADEC: identifying adverse drug events etc. on reviews about medications (Karimi et al., 2015); SemEval-14: identifying product or service attributes on reviews about laptops and restaurants (Pontiki et al., 2014); SST: classifying sentiment on movie reviews (Socher et al., 2013). In addition, we use four tasks that do not use social media text to investigate how our BERT models perform on out-of-domain target tasks: Paper Field: classifying the research topic based on the title of scholar articles about various fields (Beltagy et al., 2019); EBM: identifying intervention, outcome etc. on scholar articles about clinical trials (Nye et al., 2018); i2b2-10: identifying treatment, test and problem on clinical notes about health (Uzuner et al., 2011); JNLPBA: identifying RNA, DNA etc. on scholar articles a"
2020.findings-emnlp.151,N19-5004,0,0.0427155,"are pretrained on in-domain data. Often, the pretraining data used in these models are selected based on their subject matter, e.g., biology or computer science. Given the range of applications using social media text, and its unique language variety, we pretrain two models on tweets and forum text respectively, and empirically demonstrate the effectiveness of these two resources. In addition, we investigate how similarity measures can be used to nominate in-domain pretraining data. We publicly release our pretrained models at https://bit.ly/35RpTf0. 1 Introduction Sequence transfer learning (Ruder, 2019), that pretrains language representations on unlabeled text (source) and then adapts these representations to a supervised task (target), has demonstrated its effectiveness on a range of NLP tasks (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Approaches vary in model, pretraining objective, pretraining data and adaptation strategy. We consider a widely used method, BERT (Devlin et al., 2019). It pretrains a transformer-based model using a masked language model objective and then fine-tunes the model on the target task. We investigate the impact of the domain (i.e., the similar"
2020.findings-emnlp.151,D17-1038,0,0.127757,"ls. For example, Baevski et al. (2019) empirically show that the average GLUE score (Wang et al., 2019) can increase from lower than 80 to higher than 81 when the size of pretraining data increases from 562 million to 18 billion tokens. Our study uses the second strategy. However, we select our pretraining data from the tenor perspective rather than the field. A summary of the source data used in these domain-specific BERT models can be found in Table 1. Finding in-domain data Our study relates to the literature on investigating domain similarity (Blitzer et al., 2006; Ben-David et al., 2007; Ruder and Plank, 2017) and text similarity (Mihalcea et al., 2006; Pavlick et al., 2015; Kusner et al., 2015). Our work is also inspired by the study by Dai et al. (2019) on the impact of source data on pretrained LSTM-based models (i.e., ELMo) and by Van Asch and Daelemans (2010) on the correlation between similarity and accuracy loss of POS taggers. Model Original BERT Books and encyclopedia articles, various fields BioBERT (Lee et al., Scholar articles on biology 2019) ClinicalBERT (Alsentzer Nursing and physician notes on et al., 2019) hospital admission SciBERT (Beltagy et al., Scholar articles on biology and"
2020.findings-emnlp.151,D13-1170,0,0.00446968,"t on tweets about major U.S. airlines; BTC: identifying location, person, and organization on tweets (Derczynski et al., 2016); SMM4H-18: classifying whether the user reports an adverse drug events (task3) (Weissenbacher et al., 2018), or intends to receive a seasonal influenza vaccine (task4) on tweets about health (Joshi et al., 2018); CADEC: identifying adverse drug events etc. on reviews about medications (Karimi et al., 2015); SemEval-14: identifying product or service attributes on reviews about laptops and restaurants (Pontiki et al., 2014); SST: classifying sentiment on movie reviews (Socher et al., 2013). In addition, we use four tasks that do not use social media text to investigate how our BERT models perform on out-of-domain target tasks: Paper Field: classifying the research topic based on the title of scholar articles about various fields (Beltagy et al., 2019); EBM: identifying intervention, outcome etc. on scholar articles about clinical trials (Nye et al., 2018); i2b2-10: identifying treatment, test and problem on clinical notes about health (Uzuner et al., 2011); JNLPBA: identifying RNA, DNA etc. on scholar articles about biology (Kim et al., 2004). 4.2 Results We observe that our BE"
2020.findings-emnlp.151,W10-2605,0,0.0778888,"Missing"
2020.findings-emnlp.151,W18-5904,0,0.0216206,"e approach of finetuning the pretrained model. Domain-specific BERTs add a stage in the middle: finetuning BERT on domain-specific unlabeled data (cf. Figure 1). 4.1 Target Tasks We use eight target tasks with their text sampled from Twitter and forums, to examine whether our BERT models can lead to improvements, compared to the original BERT. These tasks are Airline6 : classifying sentiment on tweets about major U.S. airlines; BTC: identifying location, person, and organization on tweets (Derczynski et al., 2016); SMM4H-18: classifying whether the user reports an adverse drug events (task3) (Weissenbacher et al., 2018), or intends to receive a seasonal influenza vaccine (task4) on tweets about health (Joshi et al., 2018); CADEC: identifying adverse drug events etc. on reviews about medications (Karimi et al., 2015); SemEval-14: identifying product or service attributes on reviews about laptops and restaurants (Pontiki et al., 2014); SST: classifying sentiment on movie reviews (Socher et al., 2013). In addition, we use four tasks that do not use social media text to investigate how our BERT models perform on out-of-domain target tasks: Paper Field: classifying the research topic based on the title of scholar"
2021.acl-long.415,N19-1357,0,0.0207755,"and are mostly applied to models with differentiable functions. Further, they may be sensitive 1 This approach does not aim to improve the transparency (Lipton, 2018) of the black-box model. to randomized model initializations or permuted data labels (Adebayo et al., 2018), which is undesirable. These methods can be computationally heavy in the case of complex black-box models (Wu and Ong, 2021), e.g., BERT (Devlin et al., 2018). Attention-based methods (Wiegreffe and Pinter, 2019) can only be applied to Transformer-based models (Vaswani et al., 2017), and their effectiveness is questionable (Jain and Wallace, 2019; Serrano and Smith, 2019). Perturbation-based methods approximate feature importance by observing changes in a model’s outcome after a feature is changed. They either consider changes in performance as an indicator of feature importance directly (Martens and Provost, 2014; Zeiler and Fergus, 2014; Schwab and Karlen, 2019), or they employ a higher-order approximation of the decision boundary (Ribeiro et al., 2016; Lundberg and Lee, 2017). Perturbation-based methods are typically computationally inefficient for explaining high-dimensional data, and they suffer from high variance due to perturba"
2021.acl-long.415,2020.emnlp-main.747,0,0.0277781,"statistical significance. Detailed precision and recall values of positive reviews appear in Appendix G. Faithfulness. We select the top-K important words generated by an explanation method and compute the precision, recall and F1 against the human-annotated rationales. It is worth noting that our L2E explainer is not supervised by human rationales directly. Instead, we use the same experimental setup as in Section 4.5 to ensure the L2E explainer is learning from the baseline algorithms rather than the human rationales. Table 4 displays the average values over all test instances. As noted by Carton et al. (2020), the rationales in the original dataset are not exhaustively identified by human annotators. For a particular event, we expect to observe a lower precision than recall, since the black-box model might still be able to utilize the words not being annotated in addition to the words annotated by a human. The results in Table 4 align with this hypothesis. For instance, besides LRP for the positive reviews and Kernel SHAP for both reviews, all baselines and the corresponding L2E have higher recall than precision. Furthermore, L2E outperforms the corresponding baseline A significantly in most cases"
2021.acl-long.415,2020.acl-main.409,0,0.0207685,"high-dimensional data, and they suffer from high variance due to perturbation randomness (Slack et al., 2020; Chen et al., 2019). Model-based Approaches. These approaches train the explainer with an objective function to improve efficiency at test time. The closest work to ours is by Schwab and Karlen (2019), who train an explainer using a causality-based explanation algorithm. However, these approaches do not learn from arbitrary algorithms or discretize feature weights — the high variation of continuous weights may impair the ability to capture the commonalities in an explanation algorithm. Jain et al. (2020) discretize the weights produced by an existing method, but they use these weights to build a faithful classifier for an underlying blackbox model, rather than using them to explain the model directly. Other works train a classifier and an explainer jointly in order to incorporate explainability directly into the classifier (Lei et al., 2016; Camburu et al., 2018). Unlike these approaches, we do not change the classifier or require an expensive process to collect human rationales, as done in (Camburu et al., 2018). Lastly, a few works use information-theoretic objectives to train an explainer"
2021.clpsych-1.5,N19-1423,0,0.0203429,"Missing"
2021.clpsych-1.5,W16-6208,0,0.0250832,"lue (stddev) across the five runs. Label category Precision Recall F1 Affection 0.62 (0.005) 0.65 (0.005) 0.63 (0.005) Anger 0.57 (0.005) 0.57 (0.004) 0.57 (0.000) Fear 0.54 (0.005) 0.49 (0.005) 0.52 (0.005) Happiness 0.58 (0.000) 0.59 (0.004) 0.58 (0.005) Sadness 0.54 (0.005) 0.60 (0.000) 0.56 (0.004) Surprise 0.52 (0.004) 0.47 (0.004) 0.49 (0.004) Figure 3: Confusion matrix for BERT model’s predictions. Numbers correspond to mean values (stddev) across the five runs. vlin et al., 2019).12 The model’s standard lexicon is manually augmented with emojis, using emoji2vec pre-trained embeddings (Eisner et al., 2016). We use the following hyperparameters. The maximum sequence length for the BERT tokeniser is set at 128. The learning rate is 3·10−5 . The batch size is 512 (spread over 4 GPUs). The number of epochs is 2, with checkpoints every 150 batches. The best checkpoint (as measured by macro F1) is saved. We train a separate model on each random subset. Macro F1 score ranges from 0.560 to 0.562, with a mean of 0.561 and a standard deviation of 0.001. Table 3 shows F1-score by class, and Figure 3 shows the confusion matrix for the model’s predictions; in both cases the values are averaged across the fi"
2021.clpsych-1.5,S12-1033,0,0.0351579,"Joy, Sadness, Disgust, Shame, Guilt) and to answer questions about their physiological and psychological state during these emotion episodes. Overall, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus s"
2021.clpsych-1.5,P17-1067,0,0.0154623,"all, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus specifically geared towards sharing one’s emotions, unlike Twitter or Facebook, which support many other activities. This makes Vent particularly interesting for"
2021.clpsych-1.5,W17-1612,0,0.0287794,". As before, the category of Surprise appears to be less clearly connected with the texts properties: the classifier made the biggest number of mistakes on it, and these mistakes were relatively evenly spread across the other 5 categories. To better understand the classifier’s performance, we visually inspect 60 random sentences (10 per label category) in which the classifier made a wrong prediction. Given that the variability between the models in the five runs is small, we only examine predictions from a single model with the best macro F1 score. Table 4 shows the results. As recommended by Benton et al. (2017), all specific examples are rephrased to protect users privacy. In the majority of the vents (45), the label assigned by the classifier is consistent with the text. Common reasons for the mistakes include lack of context which would allow to clearly differentiate between several possible affective states (e.g., Affection and Happiness, or Anger and Sadness); multiple emotions clearly expressed in the text (in some cases the classifier did capture one of the emotions, while the label reflected another). In a minority of cases, it is not immediately clear whether the labels fit the text (8 cases"
2021.clpsych-1.5,C18-1179,0,0.0361885,"Missing"
2021.clpsych-1.5,W13-1602,0,0.0244196,"s about their physiological and psychological state during these emotion episodes. Overall, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus specifically geared towards sharing one’s emotions, unlike Twitter or"
2021.clpsych-1.5,roberts-etal-2012-empatweet,0,0.028901,"sgust, Shame, Guilt) and to answer questions about their physiological and psychological state during these emotion episodes. Overall, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus specifically geared towa"
2021.clpsych-1.5,P16-1148,0,0.0217899,"these emotion episodes. Overall, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus specifically geared towards sharing one’s emotions, unlike Twitter or Facebook, which support many other activities. This makes Ven"
2021.emnlp-main.233,2020.emnlp-main.255,0,0.011346,"ring LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 102 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at https://github.com/situsnow/LLE. 1 Introduction 2021); perturbation-based methods, which observe changes in model performance after feature perturbation (Schwab and Karlen, 2019; Kim et al., 2020), or approximate the local decision boundary through perturbed samples (Ribeiro et al., 2016; Lundberg and Lee, 2017); and model-based methods, which train an explainer model by optimizing an explanation-meritorious objective,1 such as robustness/stability (Lakkaraju et al., 2020; AlvarezMelis and Jaakkola, 2018) that requires similar examples to have similar explanations. All these methods aim to explain static black-box models, whereas explaining dynamic ones, as in the lifelong learning (LL) (Silver et al., 2013) setting, is under-explored. We propose a Lifelong Explanation (LLE) approach t"
C08-2020,J01-3004,0,0.0200188,"tion increases and delivery channels become more diverse in terms of space requirements (e.g., web browsers, email, PDAs). We, as humans, address this problem by shortening our sentences or restricting the content we include. We achieve the former by manipulating vocabulary and syntax. This requires careful attention to the text at sentence level and often does not reclaim significant amount of space. 2 Related Work NLG systems have exploited the discourse structure for a number of tasks – e.g., to generate appropriate cue phrases (e.g., Scott and de Souza, 1990) or reason about layout (e.g., Bateman et al., 2001). Our system uses the discourse structure to reason about how much to realise to fit a specific space. It produces one discourse tree that is then realised for different delivery channels, each with its own space requirements. Like other systems (e.g., Moore and Paris, 1993), our system specifies the RST relations (Mann and Thompson, 1988) that hold between text spans during discourse planning. It then exploits the RST principle of nuclearity to decide what to realise. The intuition is that nuclei are important while satellites can be dropped. This intuition has been exploited in some systems"
C08-2020,J93-4004,1,0.768157,"and syntax. This requires careful attention to the text at sentence level and often does not reclaim significant amount of space. 2 Related Work NLG systems have exploited the discourse structure for a number of tasks – e.g., to generate appropriate cue phrases (e.g., Scott and de Souza, 1990) or reason about layout (e.g., Bateman et al., 2001). Our system uses the discourse structure to reason about how much to realise to fit a specific space. It produces one discourse tree that is then realised for different delivery channels, each with its own space requirements. Like other systems (e.g., Moore and Paris, 1993), our system specifies the RST relations (Mann and Thompson, 1988) that hold between text spans during discourse planning. It then exploits the RST principle of nuclearity to decide what to realise. The intuition is that nuclei are important while satellites can be dropped. This intuition has been exploited in some systems to produce summaries (e.g., Sparck-Jones, 1993; Marcu, 1998). Our purpose is different, however. We do not aim to produce a summary but a text that fits into some space requirements, © CSIRO 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0"
C16-2008,W02-2117,1,0.549993,"Missing"
C16-2008,J14-1005,0,0.0222758,"TUAL, which aims to help writers create multilingual texts. The highlighted feature of the system is that it enables machine translation (MT) to generate outputs appropriate to their functional context within the target document. Our system is operational online, implementing core mechanisms for document structuring and controlled writing. These include a topic template and a controlled language authoring assistant, linked to our statistical MT system. 1 Introduction For improved machine translatability, a wide variety of controlled language (CL) rule sets have been proposed (Kittredge, 2003; Kuhn, 2014). Evidence of reduced post-editing costs when a CL is employed is provided (Bernth and Gdaniec, 2001; O’Brien and Roturier, 2007), and several controlled authoring support tools, such as Acrolinx1 and MAXIT2 , have been developed. The fundamental limitation of the CLs proposed hitherto is, however, that they are defined at the level of the sentence rather than at the level of the document (Hartley and Paris, 2001). In fact, the notion of functional document element (see Section 2.1) does figure in some CL rule sets. ASD Simplified Technical English (ASD, 2013), for example, specifies writing p"
C16-2008,2003.eamt-1.10,0,0.0917482,"pic template is the core interface for authoring self-contained topics in a structured manner. The left pane in Figure 3 provides the basic DITA Task topic structure for composing municipal procedural documents. • CL authoring assistant analyses each sentence in the text box and highlights any segment that violates a local CL rule or controlled terminology, together with diagnostic comments and suggestions for rewriting (shown at bottom centre in Figure 3) (Miyata et al., 2016). In addition, we have implemented a preliminary rewriting support function with several of the features advocated by Mitamura et al. (2003). For a particular CL-noncompliant segment, the function offers alternative expressions; clicking one of the suggestions automatically replaces the offending segment in the text box above. • Pre-translation processing automatically modifies source segments in the background following transformation rules defined for each functional element, and then MT produces the translation and back-translation at the same time. 3 We used a Japanese morphological analyser MeCab. http://taku910.github.io/mecab/ 37 MT and back translation DITA task topic CL authoring assistant Figure 3: Task topic template fo"
C16-2008,2015.mtsummit-papers.8,1,0.857596,"Missing"
C96-2124,J95-1002,1,\N,Missing
C96-2124,J93-4004,1,\N,Missing
D08-1057,N04-1015,0,0.199313,"ith [host families]2 in the districts; A total or partial destruction of over 3,000 homes in Dili affecting at least 14,000 IDPs Figure 2: Examples of the pattern hDisplacedPersons[1], HostingCommunities[2]i. ure 2 showed that mentions of the plight of internationally displaced persons are often followed by descriptions of the impact on the host communities that look after them. In this particular example, this is realised lexically in the co-occurrences of the words displaced and host. Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. However, to our knowledge, using word co-occurrences in this manner to represent schematic knowledge for the purposes of selecting content in a statistically-generated summary sentence has not previously been explored. This paper seeks to determine whether or not such patterns exist in homogeneous data; and furthermore, whether such patterns can be used to better select words from auxiliary sentences. In particular, we propose the “Seed and Grow” approach for this task. The results show that even simple modelling"
D08-1057,J05-3002,0,0.176208,"mations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identified by finding the common elements in the input sentences. We see this as an example of conservation. In our work, this general proposition is equivalent to the core information for the summary sentence before the incorporation of supplementary material. In contrast to both compression and conservation work, we focus on augmenting th"
D08-1057,J96-1002,0,0.0370589,"Missing"
D08-1057,D07-1001,0,0.0182264,"nformation. Roughly, text-to-text transformations fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be r"
D08-1057,P02-1057,0,0.0720033,"Missing"
D08-1057,J05-4004,0,0.0500513,"Missing"
D08-1057,J95-2003,0,0.0080821,"ns fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identifie"
D08-1057,P03-1069,0,0.0983791,"ing in camps and with [host families]2 in the districts; A total or partial destruction of over 3,000 homes in Dili affecting at least 14,000 IDPs Figure 2: Examples of the pattern hDisplacedPersons[1], HostingCommunities[2]i. ure 2 showed that mentions of the plight of internationally displaced persons are often followed by descriptions of the impact on the host communities that look after them. In this particular example, this is realised lexically in the co-occurrences of the words displaced and host. Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. However, to our knowledge, using word co-occurrences in this manner to represent schematic knowledge for the purposes of selecting content in a statistically-generated summary sentence has not previously been explored. This paper seeks to determine whether or not such patterns exist in homogeneous data; and furthermore, whether such patterns can be used to better select words from auxiliary sentences. In particular, we propose the “Seed and Grow” approach for this task. The results show"
D08-1057,N03-1020,0,0.12957,"e what words were actually chosen in the summary sentence of the aligned sentence tuple. We are specifically interested in open-class words, and so a stopword list of closed-class words is used to filter the sentences in each test case. We evaluate against the set of open-class words in the human-authored summary sentence using recall and precision metrics. Recall is the size of the intersection of the selected and gold-standard sets, normalised by the length of the gold-standard sentence (in words). This recall metric is similar to the ROUGE-1 metric, the unigram version of the ROUGE metric (Lin and Hovy, 2003) used in the Document Understanding Conferences2 (DUC). Precision is the size of the intersection normalised by the number of words selected. We also report the F-measure, which is the harmonic mean of the recall and precision scores. Recall, precision and F-measure are measured at various values of n ranging from 1 to the number of open-class words in the gold-standard summary sentence for a particular test case. For the purposes of evaluation, differences in tokens due to morphology were explored crudely via the use of Porter’s stemming algorithm. However, the results from stemming are not t"
D08-1057,J91-1002,0,0.0540133,"which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identified by finding the common elements in the inpu"
D08-1057,P05-1009,0,0.0682517,"Missing"
D08-1057,P08-2033,1,0.823543,"least one auxiliary sentence). Of the 580 cases, 50 cases were set aside for testing. The remaining 530 cases were used for training. Statistics for the training portion of the sentence augmentation set are provided in Table 1. In this paper, aligned sentence tuples are obtained via manual annotation. Automatic construction of these sentence-level alignments is possible and has been explored by Jing and McKeown (1999). We also envisage using tools for scoring sentence similarity (for example, see Hatzivassiloglou et al. (2001)) for automatically constructing them; this is the focus of work by Wan and Paris (2008). 3 http://ochaonline3.un.org/humanitarianappeal/index.htm 549 5.2 The Baselines Three baselines were used in this work: the random, tf-idf and position baselines. A random word selector shows what performance might be achieved in the absence of any linguistic knowledge. We also sorted all words in the aligned source sentences by their weighted tf-idf scores. This baseline selects words in order until the desired word limit is reached. This baseline is referred to as the tf-idf baseline. Finally, we selected words based on their sentence order, choosing first those words from the key sentence."
D08-1057,I05-5012,1,0.846344,"Missing"
D09-1096,N07-1041,0,0.0236858,"Missing"
D09-1096,W04-1008,0,0.0704116,"benefit from better knowledge of this message structure, facilitating focus on relevant content in specific parts of a message. In particular, access to zone information would allow email classification, summarisation and analysis tools to separate or filter out ‘noise’ and focus on the content in specific zones of a message that are relevant to the application at hand. Email contact mining tools such as that developed by Culotta et al. (2004), for example, might access the email signature zone, while tools that attempt to identify tasks or action items in email (e.g., (Bellotti et al., 2003; Corston-Oliver et al., 2004; Bennett 919 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 919–928, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP content (bottom-posting) or interleaved with the quoted content (inline replying). Confounding the issue further is that users are able to configure their email client to suit their individual tastes, and can change both the syntax of quoting and their quoting style (top, bottom or inline replying) on a permessage basis. To address these challenges, in this paper we describe Zebra, our email zone classification system. First we de"
D09-1096,J97-1003,0,0.427287,"Missing"
D09-1096,W97-0304,0,\N,Missing
E09-1097,C00-1007,0,0.0278773,"reinserted. Since the LMO baseline reduces to bigram generation when concatenating single words, we test a second language model baseline which always uses a 4-gram window size. A Viterbi-like generator with a 4-gram model and a beam of 100 is used to generate a sequence. For this baseline, referred to as the Viterbi baseline, base noun phrases were separated into their constituent words and included in the input word set. 6 Related Work 6.1 Statistical Surface Realisers The work in this paper is similar to research in statistical surface realisation (for example, Langkilde and Knight (1998); Bangalore and Rambow (2000); Filippova and Strube (2008)). These start with a semantic representation for which a specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal. The task in this paper is different as it is a text-to-text scenario and does not begin with a representation of semantics. 5.4 Results The results are presented in Table 2. Significance was measured using the sign test and the sampling method outlined in (Collins et al., 2005). We will examine the results in the PTB-LM column first. The gain of 10 BLEU points by the LMO baseline over the Viter"
E09-1097,P99-1071,0,0.00878955,"with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections, such as English. Approaches for other language types, for example German, have been explored (Filippova and Strube, 2007). 6.2 Text-to-Text Generation As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation. In this work, sentences presenting the same information, for example multiple news articles describing the same event, are merged to form a single summary by aligning repeated words and phrases across sentences. Other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do. Work on phrasebased statistical machine translation has been applied to paraphrase generation (Bannard and Callison-Burch, 2005) and multi-sentence alignment in summarisation (Daum´e III and Marcu, 2004). These ap"
E09-1097,W07-2216,0,0.0153048,"ely if called on an 1 Adapted from (McDonald et al., 2005) and . The difference concerns the direction of the edge and the edge weight function. We have also folded the function ‘contract’ in McDonald et al. (2005) into the main algorithm. Again following that work, we treat the function s as a data structure permitting storage of updated edge weights. probLM O (w0 . . . wj ) http://www.ce.rit.edu/˜ sjyeec/dmst.html = j−k−1 Y probM LE (wi+k |wi . . . wi+k−1 ) i=0 (3) 854 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 surrounding tree context. This makes the search for an optimal tree an NP-hard problem (McDonald and Satta, 2007) as all possible trees must be considered to find an optimal solution. Consequently, we must choose a heuristic search algorithm for finding the locally optimum spanning tree. By representing argument positions that can be filled only once, we allow modifiers to compete for argument positions and vice versa. The CLE algorithm only considers this competition in one direction. In line 3 of Algorithm 1, only heads compete for modifiers, and thus the solution will be sub-optimal. In Wan et al. (2007), we showed that introducing a model of argument positions into a greedy spanning tree algorithm ha"
E09-1097,H05-1066,0,0.586947,". This is not surprising as these methods are unable to model grammaticality at the sentence level, unless the size of n is sufficiently large. In practice, the lack of sufficient training data means that n is often smaller than the average sentence length. Even if data exists, increasing the size of n corresponds to a higher degree polynomial complexity search for the best word sequence. In response, we introduce an algorithm for searching for the best word sequence in a way that attempts to model grammaticality at the sentence level. Mirroring the use of spanning tree algorithms in parsing (McDonald et al., 2005), we present an approach to statistical sentence generation. Given a set of scrambled words, the approach searches for the most probable dependency tree, as defined by some corpus, such that it contains each word of the input set. The tree is then traversed to obtain the final word ordering. In particular, we present two spanning tree algorithms. We first adapt the Chu-Liu-Edmonds (CLE) algorithm (see Chu and Liu (1965) and Edmonds (1967)), used in McDonald et al. (2005), to include a basic argument model, added to keep track of linear precedence between heads and modifiers. While our adapted"
E09-1097,P07-1044,1,0.690541,"surement of grammaticality, we use the string regeneration task. Beginning with a human-authored sentence with its word order randomised, the goal is to regenerate the original sentence. Success is indicated by the proportion of the original sentence regenerated, as measured by any string comparison method: in our case, using the BLEU metric (Papineni et al., 2002). One benefit to this evaluation is that content selection, as a factor, is held constant. Specifically, the probability of word selection is uniform for all words. 3 Alternative grammaticality measures have been developed recently (Mutton et al., 2007). We are currently exploring the use of this and other metrics. 4 This would correspond to the use of a chunking algorithm or a named-entity recogniser to find noun phrases that could be re-used for sentence generation. 857 Algorithms Viterbi baseline LMO baseline CLE AB PTB-LM 14.9 24.3 26.4 33.6 AB: the dow at this point was down about 35 points CLE: was down about this point 35 points the dow at LMO: was this point about at down the down 35 points Viterbi: the down 35 points at was about this point down BLLIP-LM 18.0 26.0 26.8 33.7 Original: at this point, the dow was down about 35 points F"
E09-1097,P04-1030,0,0.0114709,"nal (WSJ) with human annotations of syntactic structures. Dependency events were sourced from the events file of the Collins parser package, which contains the dependency events found in training sections 2-22 of the corpus. Development was done on section 00 and testing was performed on section 23. A 4-gram language model (LM) was also obtained from the PTB training data, referred to as PTB-LM. Additionally, a 4-gram language model was obtained from a subsection of the BLLIP’99 Corpus (LDC number: LDC2000T43) containing three years of WSJ data from 1987 to 1989 (Charniak et al., 1999). As in Collins et al. (2004), the 1987 portion of the BLLIP corpus containing 20 million words was also used to create a language model, referred to here as BLLIP-LM. Ngram models were smoothed using Katz’s method, backing off to smaller values of n. For this evaluation, tokenisation was based on that provided by the PTB data set. This data set also delimits base noun phrases (noun phrases without nested constituents). Base noun phrases were treated as single tokens, and the rightmost word assumed to be the head. For the algorithms tested, the input set for any test case consisted of the single tokens identified by the P"
E09-1097,P05-1066,0,0.0320327,"this paper is similar to research in statistical surface realisation (for example, Langkilde and Knight (1998); Bangalore and Rambow (2000); Filippova and Strube (2008)). These start with a semantic representation for which a specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal. The task in this paper is different as it is a text-to-text scenario and does not begin with a representation of semantics. 5.4 Results The results are presented in Table 2. Significance was measured using the sign test and the sampling method outlined in (Collins et al., 2005). We will examine the results in the PTB-LM column first. The gain of 10 BLEU points by the LMO baseline over the Viterbi baseline shows the performance improvement that can be gained when reinserting the base noun phrases. 858 structures whilst searching for the best word ordering. As a result, an argument model is needed to identify linguistically plausible spanning trees. We treated the alignment of modifiers to head words as a bipartite graph matching problem. This is similar to work in semantic role labelling by Pad´o and Lapata (2006). The alignment of answers to question types as a sema"
E09-1097,P06-1146,0,0.0168536,"Missing"
E09-1097,P96-1025,0,0.199273,"rs. Adopting an approach similar to Johnson (2007), we look at the direction (left or right) of the head with respect to the modifier; we consequently define a set D = {l, r} to represent this. Set D represents the linear precedence of the words in the dependency relation; consequently, it partially approximates the distinction between syntactic roles like subject and object. Each edge has a pair of associated weights, one for each direction, defined by the function s : E × D → R, based on a probabilistic model of dependency relations. To calculate the edge weights, we adapt the definition of Collins (1996) to use direction rather than relation type (represented in the original as triples of non-terminals). Given a corpus, for some edge e = (u, v) ∈ E and direction d ∈ D, we calculate the edge weight as: not always correspond with a linguistically valid dependency tree, primarily because it does not attempt to ensure that words in the tree have plausible numbers of arguments. We propose an alternative dependencyspanning tree algorithm which uses a more fine-grained argument model representing argument positions. To find the best modifiers for argument positions, we treat the attachment of edges"
E09-1097,P02-1040,0,0.0950057,"density up to a fixed maximum, in this case 7 argument positions, and assume zero probability beyond that. 5 Evaluation 5.1 String Generation Task The best-performing word ordering algorithm is one that makes fewest grammatical errors. As a surrogate measurement of grammaticality, we use the string regeneration task. Beginning with a human-authored sentence with its word order randomised, the goal is to regenerate the original sentence. Success is indicated by the proportion of the original sentence regenerated, as measured by any string comparison method: in our case, using the BLEU metric (Papineni et al., 2002). One benefit to this evaluation is that content selection, as a factor, is held constant. Specifically, the probability of word selection is uniform for all words. 3 Alternative grammaticality measures have been developed recently (Mutton et al., 2007). We are currently exploring the use of this and other metrics. 4 This would correspond to the use of a chunking algorithm or a named-entity recogniser to find noun phrases that could be re-used for sentence generation. 857 Algorithms Viterbi baseline LMO baseline CLE AB PTB-LM 14.9 24.3 26.4 33.6 AB: the dow at this point was down about 35 poin"
E09-1097,W04-3216,0,0.0625091,"Missing"
E09-1097,D07-1002,0,0.0191291,"ain of 10 BLEU points by the LMO baseline over the Viterbi baseline shows the performance improvement that can be gained when reinserting the base noun phrases. 858 structures whilst searching for the best word ordering. As a result, an argument model is needed to identify linguistically plausible spanning trees. We treated the alignment of modifiers to head words as a bipartite graph matching problem. This is similar to work in semantic role labelling by Pad´o and Lapata (2006). The alignment of answers to question types as a semantic role labelling task using similar methods was explored by Shen and Lapata (2007). Our work is also strongly related to that of Wong and Mooney (2007) which constructs symbolic semantic structures via an assignment process in order to provide surface realisers with input. Our approach differs in that we do not begin with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections,"
E09-1097,P05-1009,0,0.127674,"ame information, for example multiple news articles describing the same event, are merged to form a single summary by aligning repeated words and phrases across sentences. Other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do. Work on phrasebased statistical machine translation has been applied to paraphrase generation (Bannard and Callison-Burch, 2005) and multi-sentence alignment in summarisation (Daum´e III and Marcu, 2004). These approaches typically use n-gram models to find the best word sequence. The WIDL formalism (Soricut and Marcu, 2005) was proposed to efficiently encode constraints that restricted possible word sequences, for example dependency information. Though similar, our work here does not explicitly represent the word lattice. For these text-to-text systems, the order of elements in the generated sentence is heavily based on the original order of words and phrases in the input sentences from which lattices are built. Our approach has the benefit of considering all possible orderings of words, corresponding to a wider range of paraphrases, provided with a suitable dependency model is available. 7 Conclusions In this p"
E09-1097,P07-1041,0,0.0565153,"semantic structures via an assignment process in order to provide surface realisers with input. Our approach differs in that we do not begin with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections, such as English. Approaches for other language types, for example German, have been explored (Filippova and Strube, 2007). 6.2 Text-to-Text Generation As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation. In this work, sentences presenting the same information, for example multiple news articles describing the same event, are merged to form a single summary by aligning repeated words and phrases across sentences. Other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do. Work on phrasebased statistical machine translation has been applied to parap"
E09-1097,D08-1019,0,0.0364206,"eline reduces to bigram generation when concatenating single words, we test a second language model baseline which always uses a 4-gram window size. A Viterbi-like generator with a 4-gram model and a beam of 100 is used to generate a sequence. For this baseline, referred to as the Viterbi baseline, base noun phrases were separated into their constituent words and included in the input word set. 6 Related Work 6.1 Statistical Surface Realisers The work in this paper is similar to research in statistical surface realisation (for example, Langkilde and Knight (1998); Bangalore and Rambow (2000); Filippova and Strube (2008)). These start with a semantic representation for which a specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal. The task in this paper is different as it is a text-to-text scenario and does not begin with a representation of semantics. 5.4 Results The results are presented in Table 2. Significance was measured using the sign test and the sampling method outlined in (Collins et al., 2005). We will examine the results in the PTB-LM column first. The gain of 10 BLEU points by the LMO baseline over the Viterbi baseline shows the perform"
E09-1097,P07-1022,0,0.0163055,"irected edges. For each sentence w = w1 . . . wn , we define the digraph Gw = (Vw , Ew ) where Vw = {w0 , w1 , . . . , wn }, with w0 a dummy root vertex, and Ew = {(u, v)|u ∈ Vw , v ∈ Vw  {w0 }}. The graph is fully connected (except for the root vertex w0 which is only fully connected outwards) and is a representation of possible dependencies. For an edge (u, v), we refer to u as the head and v as the modifier. We extend the original formulation of McDonald et al. (2005) by adding a notion of argument positions for a word, providing points to attach modifiers. Adopting an approach similar to Johnson (2007), we look at the direction (left or right) of the head with respect to the modifier; we consequently define a set D = {l, r} to represent this. Set D represents the linear precedence of the words in the dependency relation; consequently, it partially approximates the distinction between syntactic roles like subject and object. Each edge has a pair of associated weights, one for each direction, defined by the function s : E × D → R, based on a probabilistic model of dependency relations. To calculate the edge weights, we adapt the definition of Collins (1996) to use direction rather than relati"
E09-1097,N07-1022,0,0.0160194,"hows the performance improvement that can be gained when reinserting the base noun phrases. 858 structures whilst searching for the best word ordering. As a result, an argument model is needed to identify linguistically plausible spanning trees. We treated the alignment of modifiers to head words as a bipartite graph matching problem. This is similar to work in semantic role labelling by Pad´o and Lapata (2006). The alignment of answers to question types as a semantic role labelling task using similar methods was explored by Shen and Lapata (2007). Our work is also strongly related to that of Wong and Mooney (2007) which constructs symbolic semantic structures via an assignment process in order to provide surface realisers with input. Our approach differs in that we do not begin with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections, such as English. Approaches for other language types, for example Ge"
E09-1097,W98-1426,0,\N,Missing
E09-1097,P05-1074,0,\N,Missing
E09-1097,W00-1401,0,\N,Missing
I05-5012,W98-1426,0,\N,Missing
I05-5012,C00-1007,0,\N,Missing
I05-5012,P98-1035,0,\N,Missing
I05-5012,C98-1035,0,\N,Missing
I05-5012,P96-1025,0,\N,Missing
I05-5012,P99-1071,0,\N,Missing
I13-1084,W02-1011,0,0.0183365,"nternational Joint Conference on Natural Language Processing, pages 712–718, Nagoya, Japan, 14-18 October 2013. in the same sentence, with differing results associated with each intervention. Keeping the end-use of this task in mind, we model the problem as a binary classification problem. We use the approach proposed by Niu et al. (2005) as a benchmark approach for comparison, and also use some of the features proposed by them. The majority of the work related to polarity classification has been carried out outside the medical domain, under various umbrella terms such as: sentiment analysis (Pang et al., 2002; Pang and Lee, 2004), semantic orientation (Turney, 2002), opinion mining (Pang and Lee, 2008), subjectivity (Lyons, 1981) and many more. All these terms refer to the general method of extracting polarity from text (Taboada et al., 2010). The pioneering work in sentiment analysis by Pang et al. (2002) utilised machine learning models to predict sentiments in text, and their approach showed that SVM classifiers (Vapnik, 1995) trained using bagof-words features produced good accuracies. Following this work, such classification approaches have been applied to texts of various granularities: docu"
I13-1084,D07-1111,0,0.0116414,"ariant, we detect the negations using the same approach as (Niu et al., 2005). In their simplistic approach, the authors use the no keyword as a negation word and use that for detecting negated concepts. To extract the features, all the sentences in the data set are first parsed by the Apple Pie parser6 to get phrase information. Then, in a sentence containing the word no, the noun phrase containing no is extracted. Every word in this noun phrase except no itself is attached a ‘NO’ tag. We use a similar approach, but instead of the Apple Pie parser, we use the GENIA Dependency Parser (GDep)7 (Sagae and Tsujii, 2007), since it has been shown to give better performance with medical text. For the second variant, we use the negation terms mentioned in the BioScope corpus8 (Vincze et al., 2008), and apply the same strategy as before, using the GDep parser again. For the third variant, we use the same approach using the negation terms from NegEx (Chapman et al., 2001). (v) PIBOSO Category of Sentences Our analysis of the QSpec summary sentences suggested that the class of a sentence may be related to the presence of polarity in the sentence. For example, a sentence classified as Outcome is more likely to conta"
I13-1084,J96-2004,0,0.0870153,"a number of cases, although a sentence is polarised, it does not mention an intervention. Such sentences were annotated of this paper without adding any intervention to the context. In this manner, we annotated a total of 589 sentences from the QSpec summaries associated with the 33 questions. If a sentence contained more than one intervention, we added an annotated instance for each intervention. A subset of the QSpec sentences, 124 in total, were annotated by the second author of this paper and these annotations were used to measure agreement among the annotators. We used the Cohen’s Kappa (Carletta, 1996) measure to compute interannotator agreement. We obtained an agreement of κ = 0.85, which can be regarded as almost perfect agreement (Landis and Koch, 1977). Following the annotation process, we compared the annotations of the single document summary sentences with the bottom-line summary annotations. Given that a summary sentence has been annotated to be of positive polarity with an intervention in context, we first checked if the drug name (or a generalisation of it) is also mentioned in the bottom-line summary. If yes, we checked the polarity of the bottom-line summary. In this 4 Automatic"
I13-1084,U11-1014,1,0.765966,"Missing"
I13-1084,U12-1011,1,0.894979,"Missing"
I13-1084,J07-1005,0,0.0956606,"Missing"
I13-1084,P02-1053,0,0.0324013,"pages 712–718, Nagoya, Japan, 14-18 October 2013. in the same sentence, with differing results associated with each intervention. Keeping the end-use of this task in mind, we model the problem as a binary classification problem. We use the approach proposed by Niu et al. (2005) as a benchmark approach for comparison, and also use some of the features proposed by them. The majority of the work related to polarity classification has been carried out outside the medical domain, under various umbrella terms such as: sentiment analysis (Pang et al., 2002; Pang and Lee, 2004), semantic orientation (Turney, 2002), opinion mining (Pang and Lee, 2008), subjectivity (Lyons, 1981) and many more. All these terms refer to the general method of extracting polarity from text (Taboada et al., 2010). The pioneering work in sentiment analysis by Pang et al. (2002) utilised machine learning models to predict sentiments in text, and their approach showed that SVM classifiers (Vapnik, 1995) trained using bagof-words features produced good accuracies. Following this work, such classification approaches have been applied to texts of various granularities: documents, sentences, and phrases. Research has also focused o"
I13-1084,U11-1012,0,0.0362355,"Missing"
I13-1084,W08-0606,0,0.0635282,"Missing"
I13-1084,P04-1035,0,0.0435478,"Conference on Natural Language Processing, pages 712–718, Nagoya, Japan, 14-18 October 2013. in the same sentence, with differing results associated with each intervention. Keeping the end-use of this task in mind, we model the problem as a binary classification problem. We use the approach proposed by Niu et al. (2005) as a benchmark approach for comparison, and also use some of the features proposed by them. The majority of the work related to polarity classification has been carried out outside the medical domain, under various umbrella terms such as: sentiment analysis (Pang et al., 2002; Pang and Lee, 2004), semantic orientation (Turney, 2002), opinion mining (Pang and Lee, 2008), subjectivity (Lyons, 1981) and many more. All these terms refer to the general method of extracting polarity from text (Taboada et al., 2010). The pioneering work in sentiment analysis by Pang et al. (2002) utilised machine learning models to predict sentiments in text, and their approach showed that SVM classifiers (Vapnik, 1995) trained using bagof-words features produced good accuracies. Following this work, such classification approaches have been applied to texts of various granularities: documents, sentences, and"
I13-1084,J09-3003,0,0.0373885,"vity (Lyons, 1981) and many more. All these terms refer to the general method of extracting polarity from text (Taboada et al., 2010). The pioneering work in sentiment analysis by Pang et al. (2002) utilised machine learning models to predict sentiments in text, and their approach showed that SVM classifiers (Vapnik, 1995) trained using bagof-words features produced good accuracies. Following this work, such classification approaches have been applied to texts of various granularities: documents, sentences, and phrases. Research has also focused on classifying polarities relative to contexts (Wilson et al., 2009). However, only limited research has taken place on applying polarity classification techniques on complex domains such as the medical domain (Niu et al., 2005; Sarker et al., 2011). Our aim is to investigate the possibility of using sentence-level polarity classification to generate bottom-line, evidence-based summaries. While there has been some research on automatic summarisation in this domain (Lin and DemnerFushman, 2007; Niu et al., 2006; Sarker et al., 2013; Cao et al., 2011), to the best of our knowledge, there is no system that currently produces bottom-line, evidence-based summaries"
I13-1084,J11-2001,0,\N,Missing
J88-3006,P86-1016,0,0.0280091,"ions for users who fall between the extremes of novice and expert. This means TAILOR is able to generate descriptions to a whole range of users, rather that just for an a priori set of user stereotypes. 1.1 PREVIOUS WORK ON USER MODELING IN QUESTION ANSWERING PROGRAMS In studying the factors involved in tailoring the content of an answer to a user, research to date has focused mainly on the problems of inferring and using user goals, plans, and beliefs (Appelt 1982, 1985; Carberry 1983, and this issue; McKeown et al. 1985), recognizing and dealing with misconceptions (Kaplan 1982; McCoy 1983; McCoy 1986, and this issue; Quilici et al., this issue), and superposing various stereotypes (Rich 1979). The issue addressed here differs from these because we are not concerned with the users&apos; goals in asking the question, nor with correcting their view of the domain, but rather with providing an answer that is optimally informative (without being overwhelming) given how much the user knows about the domain. We are not interested in building a user model using stereotypes (as was Rich), but in determining an answer based on a user model involving user types. As in McCoy (1986, and this issue), we are"
J88-3006,P87-1014,1,0.789975,"Missing"
J88-3006,P85-1029,1,0.535927,"Missing"
J88-3006,J88-3004,0,\N,Missing
J88-3006,J88-3005,0,\N,Missing
J93-4004,J86-3001,0,0.833781,"(called action summaries) encoded in N O A H - s t y l e plan operators (Sacerdoti 1977) to generate these hypotheses. T h e o r e m - p r o v i n g is then used to determine if a series of p r o p o s e d actions will have the desired effect on the hearer's mental state. The systems that have been built within 653 Computational Linguistics Volume 19, Number 4 this framework to date (Cohen 1978; Appelt 1985) plan short (one- or two-sentence) texts to achieve the speakers' goal(s). In this approach, the intentional structure describing the speaker's purposes and the relationships between them (Grosz and Sidner 1986) is explicitly represented. However, this approach does not represent or use rhetorical knowledge about how speech acts may be combined into larger bodies of coherent text to achieve a speaker's goals. It assumes that appropriate axioms could be added to generate longer texts, and that the text produced will be coherent as a byproduct of the planning process. However, this has not been demonstrated. Moreover, we believe that building a system to produce multisentential texts directly from the logics proposed by proponents of this approach would prove to be computationally infeasible. We see tw"
J93-4004,P84-1076,0,0.0293139,"rease the hearer's ability to perform the action (ENABLEMENT),how much of this information to present, in what order, at what level of detail, etc. Moreover, the theory states that the nucleus and satellite portions of a text may occur in any order, relations may occur any number of times, and a nucleus or satellite may be expanded into a text employing any other relation at any point. In order to use RST, a text generation system must have control strategies that dictate how to find such knowledge in the knowledge base, when and what relations should occur, how many times, and in what order. Mann (1984) suggested that goal pursuit methods used in artificial intelligence could be applied to RST for text generation. Schemata can be viewed as means for achieving the goals stated as their effects, and the constraints on relations as a kind of precondition to using a particular schema. However, much work must be done to formalize the constraints and effects of the RST relations and schemata in order to use RST in a text generation system. One attempt at formalization was made by Hovy (1991), who operationalized a subset of the RST relation definitions for use as plan operators in a text structuri"
J93-4004,J92-4007,1,0.64877,"n a MOTIVATION relation to the nucleus. 8 This is because the knowledge base search originally retrieved this as the information that is most relevant to helping this hearer identify the Phillips screwdriver. 667 Computational Linguistics Volume 19, Number 4 Because of this dichotomy between the two classes of RST relations, we conclude that any approach to discourse structure that relies solely on rhetorical relations or predicates and does not explicitly encode information about intentions is inadequate for handling dialogues. Hovy's (1991) approach suffers from this problem. 9 Moreover, as Moore and Pollack (1992) argue, a straightforward approach to revising such an operationalization of RST by modifying subject matter operators to indicate associated intentions cannot succeed. Such an approach &quot;presumes a one-to-one mapping between the ways in which information can be related and the ways in which intentions combine into a coherent plan to affect a hearer's mental state.&quot; We have just shown examples indicating that no such mapping exists. 5. A Text Planner for Advisory Dialogues In this section we present a text planner that constructs explanations based on the intentions of the speaker at each step"
J93-4004,J88-3006,1,0.828219,"dual parts of the text on the hearer, as well as how the parts relate to one another rhetorically. We present a text planner that records this information and show how the resulting structure is used to respond appropriately to a follow-up question. 1. I n t r o d u c t i o n Explanation systems must p r o d u c e multisentential texts, including justifications of their actions, descriptions of their problem-solving strategies, and definitions of the terms they use. Previous research in natural language generation has s h o w n that schemata of rhetorical predicates (McKeown 1985; McCoy 1989; Paris 1988) or rhetorical relations (Hovy 1991) can be used to capture the structure of coherent multisentential texts. Schemata are scriptlike entities that encode standard patterns of discourse structure. Associating a schema with a communicative goal allows a system to generate a text that achieves the goal. However, we have found that schemata are insufficient as a discourse model for advisory dialogues. Although they encode standard patterns of discourse structure, schemata do not include a representation of the intended effects of the components of a schema, nor h o w these intentions are related t"
J93-4004,W90-0112,0,0.0142145,"hetorical predicates or relations for natural language generation. Third, as illustrated by the two alternative operators for achieving a MOTIVATION subgoal shown in Figures 15 or 16, in our plan language we can represent very general strategies that are applicable across domains, as well as very specific strategies that may be necessary to handle the idiosyncratic language used in a particular domain. While one may argue that the operator in Figure 16 could be replaced by a more general, domain-independent operator, this does not obviate the need for domain-specific communication strategies. Rambow (1990) argues that domain-specific communication knowledge must be used (whether implicitly or explicitly) in all planned communica676 Johanna D. Moore and C6cile L. Paris Planning Text for Advisory Dialogs tion, and advocates that domain communication knowledge be represented explicitly. In our plan language, some types of domain-specific communication strategies can be represented in plan operators. When there are multiple operators capable of achieving a given effect, the constraint mechanism controls which operators are deemed appropriate in a given context. Note, however, that we do not wish to"
J93-4004,E91-1003,0,0.00657889,"Missing"
N10-1142,P09-1032,0,0.055028,"has been processed to remove duplicate messages and to normalise sender and recipient names, resulting in just over 250,000 email messages. No attachments are included. Our request classifier training data is drawn from a collection of 664 messages that were selected at random from the Enron corpus. Each message was annotated by three annotators, with overall kappa agreement of 0.681. From the full dataset of 664 messages, we remove all messages where annotators disagreed for training and evaluating our request classifier, in order to mitigate the effects of annotation noise, as discussed in (Beigman and Klebanov, 2009). The unanimously agreed data set used for training consists of 505 email messages. 4.2 • whether the message contains any sentences that end with a question mark; and • binary word unigram and word bigram features for n-grams that occur at least three times across the training set. Before generating n-gram features, we normalise the message text as shown in Table 1, in a manner similar to Carvalho and Cohen (2006). We also add tokens marking the start and end of sentences, detected using a modified version of Scott Piao’s sentence splitter (Piao et al., 2002), and tokens marking the start and"
N10-1142,W06-3406,0,0.0590319,"sages, we remove all messages where annotators disagreed for training and evaluating our request classifier, in order to mitigate the effects of annotation noise, as discussed in (Beigman and Klebanov, 2009). The unanimously agreed data set used for training consists of 505 email messages. 4.2 • whether the message contains any sentences that end with a question mark; and • binary word unigram and word bigram features for n-grams that occur at least three times across the training set. Before generating n-gram features, we normalise the message text as shown in Table 1, in a manner similar to Carvalho and Cohen (2006). We also add tokens marking the start and end of sentences, detected using a modified version of Scott Piao’s sentence splitter (Piao et al., 2002), and tokens marking the start and end of the message. Symbol Used Pattern numbers day pronoun-object pronoun-subject filetype multi-dash multi-underscore Any sequence of digits Day names or abbreviations Objective pronouns: me, her, him, us, them Subjective pronouns: I, we, you, he, she, they .doc, .pdf, .ppt, .txt, .xls, .rtf 3 or more sequential ‘-’ characters 3 or more sequential ‘ ’ characters Table 1: Normalisation applied to n-gram features"
N10-1142,W04-3240,0,0.118205,"Missing"
N10-1142,W04-1008,0,0.014809,"Missing"
N10-1142,U08-1009,1,0.933378,"cuss in Section 4. A distinction can be drawn between messagelevel identification—i.e., the task of determining whether an email message contains a request — and utterance-level identification—i.e., determining precisely where and how the request is expressed. In this paper, we focus on the task of message-level identification, since utterance-level identification is a significantly more problematic task: it is often the case that, while we might agree that a message contains a request or commitment, it is much harder to determine the precise extent of the text that conveys this request (see (Lampert et al., 2008b) for a detailed discussion of some of the issues here). 3 Related Work Our request classification work builds on influential ideas proposed by Winograd and Flores (1986) in taking a language/action perspective and identifying speech acts in email. While this differs from the approach of most currently-used email systems, which routinely treat the content of email messages as homogeneous bags-of-words, there is a growing body of research applying ideas from Speech Act Theory (Austin, 1962; Searle, 1969) to analyse and enhance email communication. Khosravi and Wilks (1999) were among the first"
N10-1142,D09-1096,1,0.931459,"ferent functional parts, which we call email zones, and then using this information to consider only content from certain parts of a message for request classification, would improve request classifi984 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 984–992, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics cation performance. To test this hypothesis, we developed an SVMbased automated email zone classifier configured with graphic, orthographic and lexical features; this is described in more detail in (Lampert et al., 2009). Section 5 describes how we improve request classification performance using this email zone classifier. Section 6 summarises the performance of our request classifiers, with and without automated email zoning, along with an analysis of the contribution of lexical features to request classification, discussion of request classification learning curves, and a detailed error analysis that explores the sources of request classification errors. Finally, in Section 7, we offer pointers to future work and some concluding remarks. 2 Background and Motivation Previous research has established that us"
N10-1142,scerri-etal-2008-evaluating,0,0.0249841,"e-level, marking emails as requests if they contain one or more request utterances. As noted earlier, we define a request as an utterance from the email sender that places an obligation on a recipient to schedule an action (e.g., add to a calendar or task list), perform an action, or respond. Requests may be conditional or unconditional in terms of the obligation they impose on the recipient. Conditional requests require action only if a stated condition is satisfied. Previous annotation experiments have shown that conditional requests are an important phenomena and occur frequently in email (Scerri et al., 2008; Lampert et al., 2008a). Requests may also be phrased as either a direct or indirect speech act. Although some linguists distinguish between speech acts that require a physical response and those that require a verbal or information response, e.g., (Sinclair and Coulthard, 1975), we follow Searle’s approach and make no such distinction. We thus consider questions requiring an informational response to be requests, since they place an obliga987 tion on the recipient to answer.1 Additionally, there are some classes of request which have been the source of systematic human disagreement in our pr"
N19-1149,C18-1139,0,0.0197782,"expertise needed for labelling or even due to legal barriers. Researchers working on such tasks usually spend a considerable amount of effort and resources on collecting useful external data sources and investigating how to transfer knowledge to their target tasks (Qi et al., 2009; Kim et al., 2017). Recent transfer learning techniques make the most of limited labelled data by incorporating word vectors or LMs pretrained on a large amount of unlabelled data. This produces dramatic improvements over a range of NLP tasks where appropriate unlabelled data is available (Peters et al., 2017, 2018; Akbik et al., 2018; Devlin et al., 2019). However, there is still a lack of systematic study on how to select appropriate data to pretrain word Our overarching goal is to develop a costeffective approach that, given a NER data set, nominates the most suitable source data to pretrain word vectors or LMs from several options. Our approach builds on the hypothesis that the more similar the source data is to the target data, the better the pretrained models are, all other aspects (such as source data size) being equal. We propose using target vocabulary covered rate and language model perplexity to select pretraini"
N19-1149,K18-1028,0,0.0212591,"ere m is the number of sentences in the target data set, and P (DTi ) is the probability assigned by 2 Our focus is on transferring through pretrained models using one single source and we do not consider multilingual similarity. the language model trained on the source data to the i-th sentence from the target data set, whose sentence length is Ni . PPL is token-based, similar to TVC, but also captures surface structure. We therefore propose PPL as a proxy to measure tenor as well as field. 4.3 Word Vector Variance Pretrained word vectors capture semantic and syntactic regularities of words (Artetxe et al., 2018). The variance of a word vector that is first trained on the source data and then on the target data can reflect the difference of linguistic regularities between the two data sets. Intuitively, if the context words around a given word are very different in the source and target data, then the word vector of this word learned from the source will be updated more than those words whose context words are similar between source and target. Therefore, we use Word Vector Variance (WVV) as another combined measure of tenor and field. To calculate word vector variance, we first train word vectors on"
N19-1149,S17-2091,0,0.0582749,"e (2015) investigate different methods to transfer knowledge to supervised recurrent neural networks. They establish that a pretrained recurrent LM can improve the generalization ability of the supervised models. They use unlabelled data from Amazon reviews to pretrain the LM and find that it can improve classification accuracy on the Rotten Tomatoes data set. Joshi et al. (2018) empirically showed that, for their vaccination behaviour detection task on twitter data, LMs pretrained on a small amount of movie reviews outperform the ones pretrained on large size of Wikipedia data. Peters et al. (2017) successfully inject the information captured by a bidirectional LM into a sequence tagger, and extend this approach to other NLP tasks (Peters et al., 2018). Our work is based on (Peters et al., 2018) and investigates the impact of pretraining data on the effectiveness of pretrained LMs for downstream NER tasks. Transfer Learning While our study falls into the paradigm of semi-supervised learning, we distinguish ourselves from other studies in transfer learning. One sub-area of transfer learning is domain adaptation, which aims to learn transferable representation from a source domain and app"
N19-1149,W06-1615,0,0.331219,"nformation captured by a bidirectional LM into a sequence tagger, and extend this approach to other NLP tasks (Peters et al., 2018). Our work is based on (Peters et al., 2018) and investigates the impact of pretraining data on the effectiveness of pretrained LMs for downstream NER tasks. Transfer Learning While our study falls into the paradigm of semi-supervised learning, we distinguish ourselves from other studies in transfer learning. One sub-area of transfer learning is domain adaptation, which aims to learn transferable representation from a source domain and apply it to a target domain (Blitzer et al., 2006; Yang and Eisenstein, 2015). The question in domain adaptation is usually framed as ‘Given a source and a target, how to transfer?’. In contrast, the question we address is ‘Given a specific target, which source to choose from?’. The other sub-area of transfer learning is transferring from multiple sources (Yin and Sch¨utze, 2015; Li et al., 2018). Our work focuses, instead, on the selection of a single external data source. Our work is inspired by the methodology proposed by Johnson et al. (2018) where they predict a system’s accuracy using larger training data from its performance on much s"
N19-1149,D15-1075,0,0.029453,"to quantify different aspects of similarity between source pretraining and target task data. We demonstrate that these measures are good predictors of the usefulness of pretrained models for Named Entity Recognition (NER) over 30 data pairs. Results also suggest that pretrained LMs are more effective and more predictable than pretrained word vectors, but pretrained word vectors are better when pretraining data is dissimilar. 1 Introduction Modern neural architectures for NLP are highly effective when provided a large amount of labelled training data (Zhang et al., 2015; Conneau et al., 2017; Bowman et al., 2015). However, a large labelled data set is not always readily accessible due to the high cost of expertise needed for labelling or even due to legal barriers. Researchers working on such tasks usually spend a considerable amount of effort and resources on collecting useful external data sources and investigating how to transfer knowledge to their target tasks (Qi et al., 2009; Kim et al., 2017). Recent transfer learning techniques make the most of limited labelled data by incorporating word vectors or LMs pretrained on a large amount of unlabelled data. This produces dramatic improvements over a"
N19-1149,J06-1003,0,0.0631231,"t systematic study to investigate LMs pretrained on various data sources.1 • We find that it is important to consider tenor as well as field when selecting pretraining data, contrary to human intuitions. • We show that models pretrained on a modest amount of similar data outperform pretrained models that take weeks to train over very large generic data. 2 Related Work Text Similarity Word similarity following the hypothesis that similar words tend to occur in similar contexts (Harris, 1954) is well studied and forms the foundation of neural word embedding architectures. Hill et al. (2015) and Budanitsky and Hirst (2006) evaluate functional similarity (as in school versus college) and associative similarity (as in school versus teacher) captured by semantic models, respectively. Pavlick et al. (2015) study sentence-level similarity, using entailment relation, vector embedding and stylistic variation measures. Kusner et al. (2015) propose Word Mover’s Distance to measure the similarity between documents and evaluate on document classification tasks. We extend the study of similarity to corpus-level, and focus on its implication on unsupervised pretraining. Pretrained Word Vectors The effectiveness of pretraine"
N19-1149,W16-2922,0,0.278119,"ilarity Measures to Select Pretraining Data for NER Xiang Dai1,2 Sarvnaz Karimi1 Ben Hachey2,3 Cecile Paris1 1 CSIRO Data61, Sydney, Australia 2 University of Sydney, Sydney, Australia 3 Digital Health CRC, Sydney, Australia {dai.dai,sarvnaz.karimi,cecile.paris}@csiro.au ben.hachey@gmail.com Abstract vectors or LMs. We observe a range of heuristic strategies in the literature: (1) collecting a large amount of generic data, e.g., web crawl (Pennington et al., 2014; Mikolov et al., 2018); (2) selecting data from a similar field (the subject matter of the content being discussed), e.g., biology (Chiu et al., 2016; Karimi et al., 2017); and, (3) selecting data from a similar tenor (the participants in the discourse, their relationships to each other, and their purposes), e.g., Twitter, or online forums (Li et al., 2017; Chronopoulou et al., 2019). In all these settings, the decision is based on heuristics and varies according to the individual’s experience. We also conducted a pilot study that suggests that the practitioner’s intuition is to prioritise field over tenor (see Section 3). Word vectors and Language Models (LMs) pretrained on a large amount of unlabelled data can dramatically improve variou"
N19-1149,N19-1213,0,0.0273661,"i.dai,sarvnaz.karimi,cecile.paris}@csiro.au ben.hachey@gmail.com Abstract vectors or LMs. We observe a range of heuristic strategies in the literature: (1) collecting a large amount of generic data, e.g., web crawl (Pennington et al., 2014; Mikolov et al., 2018); (2) selecting data from a similar field (the subject matter of the content being discussed), e.g., biology (Chiu et al., 2016; Karimi et al., 2017); and, (3) selecting data from a similar tenor (the participants in the discourse, their relationships to each other, and their purposes), e.g., Twitter, or online forums (Li et al., 2017; Chronopoulou et al., 2019). In all these settings, the decision is based on heuristics and varies according to the individual’s experience. We also conducted a pilot study that suggests that the practitioner’s intuition is to prioritise field over tenor (see Section 3). Word vectors and Language Models (LMs) pretrained on a large amount of unlabelled data can dramatically improve various Natural Language Processing (NLP) tasks. However, the measure and impact of similarity between pretraining data and target task data are left to intuition. We propose three cost-effective measures to quantify different aspects of simil"
N19-1149,W04-1213,0,0.188419,"6) and consisting of around 28K Good or Featured articles from Wikipedia. These articles are reviewed by human editors, and they are selected based on the writing quality. We refer to this data set as Wiki. Yelp An online forum where customers can write reviews about local businesses. We use data released in round 12 of the Yelp Data set Challenge and select the first 2 out of 6 million reviews. 6 Table 1: List of the source data sets. Target data sets Six NER data sets are used as target data: CADEC (Karimi et al., 2015), CoNLL2003 (Sang and Meulder, 2003), CRAFT (Bada et al., 2012), JNLPBA (Collier and Kim, 2004), ScienceIE (Augenstein et al., 2017) and WetLab (Kulkarni et al., 2018). Details of these target data are listed in Table 2. We choose these data sets based on two considerations: 1. NER is a popular structured NLP task. Using NER, we want to observe how the similarity between source and target data may affect the effectiveness of different pretrained word vectors and LMs on downstream tasks. 2. NER is highly sensitive to word representations, because the model needs to make token level decisions. That is, each token needs to be assigned a proper label. Past studies have shown that removing p"
N19-1149,E17-1104,0,0.0183208,"ost-effective measures to quantify different aspects of similarity between source pretraining and target task data. We demonstrate that these measures are good predictors of the usefulness of pretrained models for Named Entity Recognition (NER) over 30 data pairs. Results also suggest that pretrained LMs are more effective and more predictable than pretrained word vectors, but pretrained word vectors are better when pretraining data is dissimilar. 1 Introduction Modern neural architectures for NLP are highly effective when provided a large amount of labelled training data (Zhang et al., 2015; Conneau et al., 2017; Bowman et al., 2015). However, a large labelled data set is not always readily accessible due to the high cost of expertise needed for labelling or even due to legal barriers. Researchers working on such tasks usually spend a considerable amount of effort and resources on collecting useful external data sources and investigating how to transfer knowledge to their target tasks (Qi et al., 2009; Kim et al., 2017). Recent transfer learning techniques make the most of limited labelled data by incorporating word vectors or LMs pretrained on a large amount of unlabelled data. This produces dramati"
N19-1149,W11-2123,0,0.0273606,"robability to any sequence of words &lt; w1 , · · · , wN > using chain rule of probability: p(w1 , w2 , · · · , wN ) = N Y p(wi |w1w−1 ), i=1 where N is the length of the sequence and w1i−1 are all words before word wi . In practice, this equation can be simplified by n-gram models based on Markov Assumption: p(w1 , w2 , · · · , wN ) = N Y i−1 p(wi |wi−n+1 ), i=1 i−1 where wi−n+1 represents only n preceding words of wi . To make the model generalize better, smoothing techniques can be used to assign non-zero probabilities to unseen events. In this study, we use Kneser-Ney smoothed 5-gram models (Heafield, 2011). To measure the similarity between two data sets using language modeling, we first train the language model on the source data, then evaluate it on the target data using perplexity to represent the degree of similarity. The intuition is that, if the model finds a sentence very unlikely (dissimilar from the data where this language model is trained on), it will assign a low probability and therefore high perplexity. The summed up perplexity (PPL) is then: P P L(DS , DT ) = m X P (DTi ) − N1 i , i=1 where m is the number of sentences in the target data set, and P (DTi ) is the probability assig"
N19-1149,J15-4004,0,0.0167221,"ledge, this is the first systematic study to investigate LMs pretrained on various data sources.1 • We find that it is important to consider tenor as well as field when selecting pretraining data, contrary to human intuitions. • We show that models pretrained on a modest amount of similar data outperform pretrained models that take weeks to train over very large generic data. 2 Related Work Text Similarity Word similarity following the hypothesis that similar words tend to occur in similar contexts (Harris, 1954) is well studied and forms the foundation of neural word embedding architectures. Hill et al. (2015) and Budanitsky and Hirst (2006) evaluate functional similarity (as in school versus college) and associative similarity (as in school versus teacher) captured by semantic models, respectively. Pavlick et al. (2015) study sentence-level similarity, using entailment relation, vector embedding and stylistic variation measures. Kusner et al. (2015) propose Word Mover’s Distance to measure the similarity between documents and evaluate on document classification tasks. We extend the study of similarity to corpus-level, and focus on its implication on unsupervised pretraining. Pretrained Word Vector"
N19-1149,P18-2072,0,0.0282189,"h aims to learn transferable representation from a source domain and apply it to a target domain (Blitzer et al., 2006; Yang and Eisenstein, 2015). The question in domain adaptation is usually framed as ‘Given a source and a target, how to transfer?’. In contrast, the question we address is ‘Given a specific target, which source to choose from?’. The other sub-area of transfer learning is transferring from multiple sources (Yin and Sch¨utze, 2015; Li et al., 2018). Our work focuses, instead, on the selection of a single external data source. Our work is inspired by the methodology proposed by Johnson et al. (2018) where they predict a system’s accuracy using larger training data from its performance on much smaller pilot data. However, we aim to predict the usefulness of pretrained models for target tasks from the similarity between the source pretraining data and the target task data. Named Entity Recognition Our work builds on the literature on deep neural networks applied to sequence tagging tasks. Architectures based on 1461 Figure 1: Likert scale ratings from NLP and ML practitioners (N = 30) for the statement ‘Unsupervised pretraining on S would be useful for supervised named entity recognition l"
N19-1149,W18-5911,1,0.728214,"similarity between source data and target task data on the effectiveness of pretrained word vectors for NER tasks. Our observations are a useful supplement to the literature as a practitioners’ guide. Pretrained Language Models Dai and Le (2015) investigate different methods to transfer knowledge to supervised recurrent neural networks. They establish that a pretrained recurrent LM can improve the generalization ability of the supervised models. They use unlabelled data from Amazon reviews to pretrain the LM and find that it can improve classification accuracy on the Rotten Tomatoes data set. Joshi et al. (2018) empirically showed that, for their vaccination behaviour detection task on twitter data, LMs pretrained on a small amount of movie reviews outperform the ones pretrained on large size of Wikipedia data. Peters et al. (2017) successfully inject the information captured by a bidirectional LM into a sequence tagger, and extend this approach to other NLP tasks (Peters et al., 2018). Our work is based on (Peters et al., 2018) and investigates the impact of pretraining data on the effectiveness of pretrained LMs for downstream NER tasks. Transfer Learning While our study falls into the paradigm of"
N19-1149,W17-2342,1,0.705385,"Select Pretraining Data for NER Xiang Dai1,2 Sarvnaz Karimi1 Ben Hachey2,3 Cecile Paris1 1 CSIRO Data61, Sydney, Australia 2 University of Sydney, Sydney, Australia 3 Digital Health CRC, Sydney, Australia {dai.dai,sarvnaz.karimi,cecile.paris}@csiro.au ben.hachey@gmail.com Abstract vectors or LMs. We observe a range of heuristic strategies in the literature: (1) collecting a large amount of generic data, e.g., web crawl (Pennington et al., 2014; Mikolov et al., 2018); (2) selecting data from a similar field (the subject matter of the content being discussed), e.g., biology (Chiu et al., 2016; Karimi et al., 2017); and, (3) selecting data from a similar tenor (the participants in the discourse, their relationships to each other, and their purposes), e.g., Twitter, or online forums (Li et al., 2017; Chronopoulou et al., 2019). In all these settings, the decision is based on heuristics and varies according to the individual’s experience. We also conducted a pilot study that suggests that the practitioner’s intuition is to prioritise field over tenor (see Section 3). Word vectors and Language Models (LMs) pretrained on a large amount of unlabelled data can dramatically improve various Natural Language Pro"
N19-1149,N18-2016,0,0.0134344,". These articles are reviewed by human editors, and they are selected based on the writing quality. We refer to this data set as Wiki. Yelp An online forum where customers can write reviews about local businesses. We use data released in round 12 of the Yelp Data set Challenge and select the first 2 out of 6 million reviews. 6 Table 1: List of the source data sets. Target data sets Six NER data sets are used as target data: CADEC (Karimi et al., 2015), CoNLL2003 (Sang and Meulder, 2003), CRAFT (Bada et al., 2012), JNLPBA (Collier and Kim, 2004), ScienceIE (Augenstein et al., 2017) and WetLab (Kulkarni et al., 2018). Details of these target data are listed in Table 2. We choose these data sets based on two considerations: 1. NER is a popular structured NLP task. Using NER, we want to observe how the similarity between source and target data may affect the effectiveness of different pretrained word vectors and LMs on downstream tasks. 2. NER is highly sensitive to word representations, because the model needs to make token level decisions. That is, each token needs to be assigned a proper label. Past studies have shown that removing pretrained word vectors from a tagging system results in a large drop in"
N19-1149,N19-1423,0,0.0515695,"labelling or even due to legal barriers. Researchers working on such tasks usually spend a considerable amount of effort and resources on collecting useful external data sources and investigating how to transfer knowledge to their target tasks (Qi et al., 2009; Kim et al., 2017). Recent transfer learning techniques make the most of limited labelled data by incorporating word vectors or LMs pretrained on a large amount of unlabelled data. This produces dramatic improvements over a range of NLP tasks where appropriate unlabelled data is available (Peters et al., 2017, 2018; Akbik et al., 2018; Devlin et al., 2019). However, there is still a lack of systematic study on how to select appropriate data to pretrain word Our overarching goal is to develop a costeffective approach that, given a NER data set, nominates the most suitable source data to pretrain word vectors or LMs from several options. Our approach builds on the hypothesis that the more similar the source data is to the target data, the better the pretrained models are, all other aspects (such as source data size) being equal. We propose using target vocabulary covered rate and language model perplexity to select pretraining data. We also intro"
N19-1149,Q15-1016,0,0.0407536,"emantic models, respectively. Pavlick et al. (2015) study sentence-level similarity, using entailment relation, vector embedding and stylistic variation measures. Kusner et al. (2015) propose Word Mover’s Distance to measure the similarity between documents and evaluate on document classification tasks. We extend the study of similarity to corpus-level, and focus on its implication on unsupervised pretraining. Pretrained Word Vectors The effectiveness of pretrained word vectors mainly depends on three factors: source data, training algorithm, and its hyper-parameters. Turian et al. (2010) and Levy et al. (2015) systematically compare count-based distributional models and distributed neural embedding models. They find that both models can improve the performance of downstream tasks. Chiu et al. (2016) identify the most influential hyper-parameters of neural embedding methods. They also investigate the impact of the source data size and find that larger pretraining data do not necessarily produce better word vectors for biomedical NER. Our work regarding pretrained word vectors is conducted using skip-gram model with default hyper-parameter setting (Mikolov et al., 2013), and our focus is on the impac"
N19-1149,N18-2076,0,0.0195456,"-supervised learning, we distinguish ourselves from other studies in transfer learning. One sub-area of transfer learning is domain adaptation, which aims to learn transferable representation from a source domain and apply it to a target domain (Blitzer et al., 2006; Yang and Eisenstein, 2015). The question in domain adaptation is usually framed as ‘Given a source and a target, how to transfer?’. In contrast, the question we address is ‘Given a specific target, which source to choose from?’. The other sub-area of transfer learning is transferring from multiple sources (Yin and Sch¨utze, 2015; Li et al., 2018). Our work focuses, instead, on the selection of a single external data source. Our work is inspired by the methodology proposed by Johnson et al. (2018) where they predict a system’s accuracy using larger training data from its performance on much smaller pilot data. However, we aim to predict the usefulness of pretrained models for target tasks from the similarity between the source pretraining data and the target task data. Named Entity Recognition Our work builds on the literature on deep neural networks applied to sequence tagging tasks. Architectures based on 1461 Figure 1: Likert scale"
N19-1149,P16-1101,0,0.0820573,"nce (Huang et al., 2015; Lample et al., 2016). Experimental Setup To investigate the impact of source data on pretrained word vectors and LMs, we pretrain word vectors and LMs on different sources separately, then observe how the effectiveness of these pretrained models varies in different NER data sets. We use the BiLSTM-CRF model, a state-of-theart model for sequence tagging tasks, as a supervised model for the target NER task. We follow the architecture proposed in (Lample et al., 2016), except that we use two BiLSTM-layers and employ a CNN network to learn character-level representations (Ma and Hovy, 2016). Micro average F1 score is used to evaluate the performance of the tagger (Sang and Meulder, 2003). Word vectors are pretrained using word2vec with its default hyper-parameter setting (Mikolov et al., 2013). In different experiments, we only replace the word embedding weights initialized by word vectors pretrained on different source data, then make these weights trained jointly with other model parameters. The baseline is denoted as None in Table 3, where word embedding weights are randomly initialized. LMs are pretrained using the architecture proposed by Jozefowicz et al. (2016) with hyper"
N19-1149,L18-1008,0,0.0612806,"Missing"
N19-1149,P10-1040,0,0.0666309,"us teacher) captured by semantic models, respectively. Pavlick et al. (2015) study sentence-level similarity, using entailment relation, vector embedding and stylistic variation measures. Kusner et al. (2015) propose Word Mover’s Distance to measure the similarity between documents and evaluate on document classification tasks. We extend the study of similarity to corpus-level, and focus on its implication on unsupervised pretraining. Pretrained Word Vectors The effectiveness of pretrained word vectors mainly depends on three factors: source data, training algorithm, and its hyper-parameters. Turian et al. (2010) and Levy et al. (2015) systematically compare count-based distributional models and distributed neural embedding models. They find that both models can improve the performance of downstream tasks. Chiu et al. (2016) identify the most influential hyper-parameters of neural embedding methods. They also investigate the impact of the source data size and find that larger pretraining data do not necessarily produce better word vectors for biomedical NER. Our work regarding pretrained word vectors is conducted using skip-gram model with default hyper-parameter setting (Mikolov et al., 2013), and ou"
N19-1149,C18-1327,0,0.0143662,"Likert scale ratings from NLP and ML practitioners (N = 30) for the statement ‘Unsupervised pretraining on S would be useful for supervised named entity recognition learning on T.’ Target data T is described as ‘Online forum posts about medications,’ source data S1 as ‘Research papers about biology and health,’ and source data S2 as ‘Online reviews about restaurants, hotels, barbers, mechanics, etc.’ different combinations of convolutional and recurrent neural networks have achieved state-of-the-art results on many NER tasks. A detailed review and comparison of these methods can be found in (Yang et al., 2018). Our experiments on the usefulness of pretrained word vectors and pretrained LMs for NER tasks are based on one variant proposed by Lample et al. (2016). 3 What Human Intuition Indicates Results of a survey capturing intuition regarding selection of pretraining data across 30 NLP or machine learning practitioners is shown in Figure 1. Participants were provided short descriptions of the target data set T, and two possible source data sets S1 and S2 as • T: Online forum posts about medications; • S1: Research papers about biology and health; • S2: Online reviews about restaurants, hotels, barb"
N19-1149,N15-1069,0,0.0545953,"a bidirectional LM into a sequence tagger, and extend this approach to other NLP tasks (Peters et al., 2018). Our work is based on (Peters et al., 2018) and investigates the impact of pretraining data on the effectiveness of pretrained LMs for downstream NER tasks. Transfer Learning While our study falls into the paradigm of semi-supervised learning, we distinguish ourselves from other studies in transfer learning. One sub-area of transfer learning is domain adaptation, which aims to learn transferable representation from a source domain and apply it to a target domain (Blitzer et al., 2006; Yang and Eisenstein, 2015). The question in domain adaptation is usually framed as ‘Given a source and a target, how to transfer?’. In contrast, the question we address is ‘Given a specific target, which source to choose from?’. The other sub-area of transfer learning is transferring from multiple sources (Yin and Sch¨utze, 2015; Li et al., 2018). Our work focuses, instead, on the selection of a single external data source. Our work is inspired by the methodology proposed by Johnson et al. (2018) where they predict a system’s accuracy using larger training data from its performance on much smaller pilot data. However,"
N19-1149,K15-1021,0,0.0605467,"Missing"
N19-1149,P15-2070,0,0.0639074,"Missing"
N19-1149,Q16-1005,0,0.0185445,"ilar field to the target data, they tend to share similar vocabulary. Conversely, vocabularies are different from each other if source and target are from different fields. Imagine data sets about medications and restaurants. Those who select pretraining data from a similar tenor believe that tenor may impact the writing style of text. Imagine the participants in online reviews and scientific papers, their relationships to each other, their purposes and how these affect text style, including punctuation, lexical normalization, politeness, emotiveness and so on (Lee, 2001; Solano-Flores, 2006; Pavlick and Tetreault, 2016). Below, we detail different measures based on these intuitions to quantify different aspects of similarity between two data sets. 4.1 Target Vocabulary Covered The first measure is simply the percentage of the target vocabulary that is also present in the source data. An extremely dissimilar example is that of different languages. They have a totally different vocabulary and are considered dissimilar, even if 1462 they are written in a similar style and talking about the same subject 2 . We propose Target Vocabulary Covered (TVC) as a measure of field, calculated as |VDS ∩ VDT | , |VDT | T V"
N19-1149,D14-1162,0,0.0800018,"Missing"
N19-1149,P17-1161,0,0.141531,"le due to the high cost of expertise needed for labelling or even due to legal barriers. Researchers working on such tasks usually spend a considerable amount of effort and resources on collecting useful external data sources and investigating how to transfer knowledge to their target tasks (Qi et al., 2009; Kim et al., 2017). Recent transfer learning techniques make the most of limited labelled data by incorporating word vectors or LMs pretrained on a large amount of unlabelled data. This produces dramatic improvements over a range of NLP tasks where appropriate unlabelled data is available (Peters et al., 2017, 2018; Akbik et al., 2018; Devlin et al., 2019). However, there is still a lack of systematic study on how to select appropriate data to pretrain word Our overarching goal is to develop a costeffective approach that, given a NER data set, nominates the most suitable source data to pretrain word vectors or LMs from several options. Our approach builds on the hypothesis that the more similar the source data is to the target data, the better the pretrained models are, all other aspects (such as source data size) being equal. We propose using target vocabulary covered rate and language model perp"
N19-1149,N18-1202,0,0.127385,"t LM can improve the generalization ability of the supervised models. They use unlabelled data from Amazon reviews to pretrain the LM and find that it can improve classification accuracy on the Rotten Tomatoes data set. Joshi et al. (2018) empirically showed that, for their vaccination behaviour detection task on twitter data, LMs pretrained on a small amount of movie reviews outperform the ones pretrained on large size of Wikipedia data. Peters et al. (2017) successfully inject the information captured by a bidirectional LM into a sequence tagger, and extend this approach to other NLP tasks (Peters et al., 2018). Our work is based on (Peters et al., 2018) and investigates the impact of pretraining data on the effectiveness of pretrained LMs for downstream NER tasks. Transfer Learning While our study falls into the paradigm of semi-supervised learning, we distinguish ourselves from other studies in transfer learning. One sub-area of transfer learning is domain adaptation, which aims to learn transferable representation from a source domain and apply it to a target domain (Blitzer et al., 2006; Yang and Eisenstein, 2015). The question in domain adaptation is usually framed as ‘Given a source and a targ"
P17-2075,D12-1135,0,0.023926,"preferences), and, consequently, social media data is now a valuable resource. Accordingly, social media analytics have received much attention among researchers and companies (Wan and Paris, 2014; Valdes et al., 2015; Zubiaga et al., 2016). Inferring demographic characteristics from social media is a useful mechanism to gain a better understanding of a cohort and one’s audience, and to facilitate interacting with that audience. Many researchers have studied ways to infer demographic attributes of Twitter users, such as age (Mislove et al., 2011; Mohammady Ardehaly and Culotta, 2015), gender (Filippova, 2012; Taniguchi et al., 2015), occupation (Preot¸iucPietro et al., 2015; Kim et al., 2016), location (Jurgens et al., 2015; Jayasinghe et al., 2016) or politi471 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 471–477 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2075 4 1 1 softmax h1 RNU1 2 3 2 x1 3 h3 h2 RNU2 x2 5 6 6 (a) Graph 4 5 (b) Tree RNU3 x3 h6 h4 RNU6 x6 h5 RNU4 x4 RNU5 x5 (c) GRNN Figure 1: An illustration of GRNN construction steps: (a) The graph"
P17-2075,N15-1019,0,0.0611061,"Missing"
P17-2075,P82-1020,0,0.842806,"Missing"
P17-2075,W16-3929,1,0.832421,"ttention among researchers and companies (Wan and Paris, 2014; Valdes et al., 2015; Zubiaga et al., 2016). Inferring demographic characteristics from social media is a useful mechanism to gain a better understanding of a cohort and one’s audience, and to facilitate interacting with that audience. Many researchers have studied ways to infer demographic attributes of Twitter users, such as age (Mislove et al., 2011; Mohammady Ardehaly and Culotta, 2015), gender (Filippova, 2012; Taniguchi et al., 2015), occupation (Preot¸iucPietro et al., 2015; Kim et al., 2016), location (Jurgens et al., 2015; Jayasinghe et al., 2016) or politi471 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 471–477 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2075 4 1 1 softmax h1 RNU1 2 3 2 x1 3 h3 h2 RNU2 x2 5 6 6 (a) Graph 4 5 (b) Tree RNU3 x3 h6 h4 RNU6 x6 h5 RNU4 x4 RNU5 x5 (c) GRNN Figure 1: An illustration of GRNN construction steps: (a) The graph consisting of six vertices; (b) The tree converted from the graph using breadth-first search; (c) The GRNN constructed from the tree. The targe"
P17-2075,P15-1169,0,0.0694201,"Missing"
P17-2075,P15-1132,0,0.0145586,"g a RNN for each tree. Specifically, we construct a tree T = (VT , ET ) of depth d rooted at vt using a breadth-first search algorithm from G. VT and ET are the sets of vertices and edges in the tree. (v, w) ∈ ET denotes an edge from a parent vertex v to a child vertex w. RNNs are deep learning models that recursively compose the vector of a parent unit from those of child units over a given structure in topological order (Pollack, 1990). They have shown to be very effective for various natural language processing (NLP) tasks, capturing syntactic and semantic composition (Socher et al., 2011; Qian et al., 2015). In this section, we describe the framework of Graph RNNs (GRNNs) (Xu et al., 2017) to classify the vertices of a graph. This framework allows us to infer the demographic characteristics of social media users. We formally define the problem of Twitter vertex classification as follows: A Twitter social network is defined as G = (V, E), where V is the set of vertices (users) and E is the set of edges (relationships) between the vertices. Each edge e ∈ E is an ordered pair e = (vi , vj ), where vi , vj ∈ V , that is unweighted1 (wij = 0) but di2.2 Word Embeddings Let Si = {w1 , w2 , ..., wR } be"
P17-2075,D14-1121,0,0.0849921,"Missing"
P17-2075,W15-2814,0,0.0296183,", consequently, social media data is now a valuable resource. Accordingly, social media analytics have received much attention among researchers and companies (Wan and Paris, 2014; Valdes et al., 2015; Zubiaga et al., 2016). Inferring demographic characteristics from social media is a useful mechanism to gain a better understanding of a cohort and one’s audience, and to facilitate interacting with that audience. Many researchers have studied ways to infer demographic attributes of Twitter users, such as age (Mislove et al., 2011; Mohammady Ardehaly and Culotta, 2015), gender (Filippova, 2012; Taniguchi et al., 2015), occupation (Preot¸iucPietro et al., 2015; Kim et al., 2016), location (Jurgens et al., 2015; Jayasinghe et al., 2016) or politi471 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 471–477 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2075 4 1 1 softmax h1 RNU1 2 3 2 x1 3 h3 h2 RNU2 x2 5 6 6 (a) Graph 4 5 (b) Tree RNU3 x3 h6 h4 RNU6 x6 h5 RNU4 x4 RNU5 x5 (c) GRNN Figure 1: An illustration of GRNN construction steps: (a) The graph consisting of six vertic"
P17-2075,P14-1018,0,0.036142,"Missing"
P18-2123,D16-1084,0,0.335324,"Missing"
P18-2123,D16-1053,0,0.0196148,"arry the core idea of a stance-bearing sentence but also tend to be recurring in the corpus. First, to capture the recurrences of the domain aspects, a simple way is to make every input sentence be consumed by this layer (see Figure 1), so that the layer parameters are shared across the corpus for being stimulated by all appearances of the domain aspects. Then, we utilize self-attention to signal the core parts of a stance-bearing sentence. Self-attention is an attention mechanism for selecting specific parts of a sequence by relating its elements at different positions (Vaswani et al., 2017; Cheng et al., 2016). In our case, the self-attention process is based on the assumption that the core parts of a sentence are those that are compatible with the semantics of the entire sentence. To this end, we introduce a compatibility function to score the semantic compatibility between the encoded seThere are two inputs in CrossNet: a stance-bearing sentence P and a descriptive target T (e.g, climate change is concern in Table 1). We use word embeddings (Mikolov et al., 2013) to represent each word in the input as a dense vector. The output of this layer are two sequences of vectors P = {p1 , ..., p|P |} and"
P18-2123,D16-1105,0,0.02201,"Missing"
P18-2123,I13-1191,0,0.0951881,"the proposed model can find useful domain-specific information from a stancebearing sentence and that the classification performance is improved in certain domains. Introduction Stance classification is the task of automatically identifying users’ positions about a specific target from text (Mohammad et al., 2017). Table 1 shows an example of this task, where the stance of the sentence is recognized as favorable on the target climate change is concern. Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest (Hasan and Ng, 2013; Mohammad et al., 2016; Ebrahimi et al., 2016). This implies that a new classifier has to be built from scratch on a well-prepared set of ground-truth data whenever predictions are needed for an unseen target. An alternative to this approach is to conduct a cross-target classification, where the classifier is adapted from different but related targets (Augenstein et al., 2016), which allows benefiting from the knowledge of existing targets. For example, in our project we are interested in online users’ stances on the approvals of particular mining projects in the country. It might be useful t"
P18-2123,P82-1020,0,0.818442,"Missing"
P18-2123,S16-1003,0,0.199848,"Missing"
P18-2123,D14-1162,0,0.079589,"Missing"
P19-1108,W18-3713,0,0.0129492,"and the tweet is predicted to not be a PHM. If the figurative usage prediction is literal, then the prediction from the PHM detection module is returned. We refer to this approach as ‘+Pipeline’. 2. Feature Augmentation Approach augments PHM detection with figurative usage features. Therefore, the figurative label and the linguistic features from figurative usage detection are concatenated as figurative usage features ad passed through a convolution layer. The two are then concatenated in a dense layer to make the prediction. The approach is illustrated in Figure 3. This approach is based on Dasgupta et al. (2018), where they augment additional features to word embeddings of words in a document. We refer to this approach as ‘+FeatAug’. In +Pipeline, the figurative label guides whether or not PHM detection will be called. In +FeatAug, the label becomes one of the features. For both the approaches, the figurative label is determined by producing the literal usage score and then applying an empirically determined threshold. We experimentally determine if using the literal usage score performs better than using the LDA-based estimator (See Section 4.3). 4.1 Experiment Setup Dataset We report our results on"
P19-1108,N15-1184,0,0.0491512,"Missing"
P19-1108,W15-3821,0,0.060689,"Missing"
P19-1108,N15-1169,0,0.0296133,"Missing"
P19-1108,N13-1097,0,0.0939878,"Missing"
P19-1108,D18-1199,0,0.228364,"report that either the author or someone they know is experiencing a health condition or a symptom (Lamb et al., 2013). For example, the sentence ‘I have been coughing since morning’ is a PHM, while ‘Having a cough for three weeks or more could be a sign of cancer’ is not. The former reports that the author has a cough while, in the latter, the author provides information about coughs in general. Past work in PHM detection uses classification-based approaches with human-engineered features (Lamb To address the question, we use a state-ofthe-art approach that detects idiomatic usage of words (Liu and Hwa, 2018). Given a word and a sentence, the approach identifies if the word is used in a figurative or literal sense in the sentence. We refer to this module as ‘figurative usage detection’. We experiment with alternative ways to combine figurative usage detection with PHM detection, and report results on a manually labeled dataset of tweets. 2 Motivation As the first step, we ascertain if the volume of figurative usage of symptom words warrants such attention. Therefore, we randomly selected 200 tweets (with no duplicates and retweets) posted in November 2018, each containing either ‘cough’ or ‘breath"
P19-1108,P14-5010,0,0.00247159,"peline approach. figurative/literal distribution which indicates the probability of a word to be either figurative or literal, and a document-figurative/literal distribution which gives a predictive score for a document to be literal or figurative. To obtain the literal usage score, we generate the literal usage representation using word2vec similarity learned from the Sentiment140 tweet dataset (Go et al., 2009). We use two sets of linguistic features, as reported in Liu and Hwa (2018): the presence of subordinate clauses and part-of-speech tags of neighbouring words, using Stanford CoreNLP (Manning et al., 2014). We adapt the abstractness feature in their paper to health-relatedness (i.e., the presence of health-related words). The intuition is that tweets which contain more health-related words are more likely to be using the symptom words in a literal sense instead of figurative. Therefore, the abstractness feature in the original paper is converted to domain relatedness and captured using the presence of health-related words. We consider the symptom word as the target word. It must be noted that we do not have or use figurative labels in the dataset except for the sample used to report the efficac"
P19-1108,D13-1145,0,0.0702514,"Missing"
P19-1108,D14-1162,0,0.080737,"Missing"
P19-1108,E09-1086,0,0.044109,"Missing"
P19-1460,W11-0702,0,0.0534204,"λ X θ2 (4) θ∈Θ where N is the size of training set, y ∈ {Agree, Disagree, N either} is the ground-truth label indicator for each class, and yˆ is the predicted class probability. λ is the coefficient for L2 -regularisation. Θ denotes the set of all trainable parameters in our model. Minimising Eq. 4 encourages the comparison results between the extracted reasons from P and Q to be stance-predictive. 3 Related Work Our work is mostly related to the task of detecting agreement and disagreement in online discussions. Recent studies have mainly focused on classifying (dis)agreement in dialogues (Abbott et al., 2011; Wang and Cardie, 2014; Misra and Walker, 2013; Allen et al., 2014). In these studies, various features (e.g., structural, linguistic) and/or specialised lexicons are proposed to recognise (dis)agreement in different dialogic scenarios. In contrast, we detect stance (dis)agreement between independent utterances where dialogic features are absent. Stance classification has recently received much attention in the opinion mining community. Different approaches have been proposed to classify stances of individual utterances in ideological forums (Murakami and Raymond, 2010; Somasundaran and Wiebe"
P19-1460,P12-1042,0,0.0340808,"nts. We propose a reason comparing network (RCN) to leverage reason information for stance comparison. Empirical results on a well-known stance corpus show that our method can discover useful reason information, enabling it to outperform several baselines in stance (dis)agreement detection. 1 Introduction Agreement and disagreement naturally arise when peoples’ views, or “stances”, on the same topics are exchanged. Being able to identify the convergence and divergence of stances is valuable to various downstream applications, such as discovering subgroups in a discussion (Hassan et al., 2012; Abu-Jbara et al., 2012), improving recognition of argumentative structure (Lippi and Torroni, 2016), and bootstrapping stance classification with (dis)agreement side information (Sridhar et al., 2014; Ebrahimi et al., 2016). Previous efforts on (dis)agreement detection are confined to the scenario of natural dialogues (Misra and Walker, 2013; Wang and Cardie, 2014; Sridhar et al., 2015; Rosenthal and McKeown, 2015), where dialogic structures are used to create a conversational context for (dis)agreement inference. However, nondialogic stance-bearing utterances are also very common in real-world scenarios. For exampl"
P19-1460,D14-1124,0,0.0157885,"agree, N either} is the ground-truth label indicator for each class, and yˆ is the predicted class probability. λ is the coefficient for L2 -regularisation. Θ denotes the set of all trainable parameters in our model. Minimising Eq. 4 encourages the comparison results between the extracted reasons from P and Q to be stance-predictive. 3 Related Work Our work is mostly related to the task of detecting agreement and disagreement in online discussions. Recent studies have mainly focused on classifying (dis)agreement in dialogues (Abbott et al., 2011; Wang and Cardie, 2014; Misra and Walker, 2013; Allen et al., 2014). In these studies, various features (e.g., structural, linguistic) and/or specialised lexicons are proposed to recognise (dis)agreement in different dialogic scenarios. In contrast, we detect stance (dis)agreement between independent utterances where dialogic features are absent. Stance classification has recently received much attention in the opinion mining community. Different approaches have been proposed to classify stances of individual utterances in ideological forums (Murakami and Raymond, 2010; Somasundaran and Wiebe, 2010; Gottopati et al., 2013; Qiu et al., 2015) and social media ("
P19-1460,D16-1084,0,0.0291168,"Missing"
P19-1460,E17-1024,0,0.0172807,"he scenario of natural dialogues (Misra and Walker, 2013; Wang and Cardie, 2014; Sridhar et al., 2015; Rosenthal and McKeown, 2015), where dialogic structures are used to create a conversational context for (dis)agreement inference. However, nondialogic stance-bearing utterances are also very common in real-world scenarios. For example, in social media, people can express stances autonomously, without the intention of initiating a discussion (Mohammad et al., 2016). There are also corpora built with articles containing many self-contained stance-bearing utterances (Ferreira and Vlachos, 2016; Bar-Haim et al., 2017). Studying how to detect (dis)agreement between such independent stance-bearing utterances has several benefits: 1) pairing these utterances can lead to a larger (dis)agreement corpus for training a potentially richer model for (dis)agreement detection; 2) the obtained pairs enable training a distance-based model for opinion clustering and subgroup mining; 3) it is applicable to the aforementioned non-dialogic stance corpora; and 4) it encourages discovering useful signals for (dis)agreement detection beyond dialogic features (e.g., the reason information studied in this work). In this work, w"
P19-1460,W14-2107,0,0.165917,"e outlawed, only Freedom to have a gun is outlaws will have guns. same as freedom of speech. (Stance: Against) (Stance: Against) Class Label: Agree Table 1: The task of detecting stance (dis)agreement between utterances towards a topic of discussion. It has been observed that when expressing 4665 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4665–4671 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics stances, people usually back up their stances with specific explanations or reasons (Hasan and Ng, 2014; Boltuzic and Snajder, 2014). These reasons are informative about which stance is taken, because they give more details on how a stance is developed. However, simply comparing the reasons may not be sufficient to predict stance (dis)agreement, as sometimes people can take the same stance but give different reasons (e.g., the points outlaws having guns and freedom of speech mentioned in Table 1). One way to address this problem is to make the reasons stancecomparable, so that the reason comparison results can be stance-predictive. In this paper, in order to leverage reason information for detecting stance (dis)agreement,"
P19-1460,D15-1075,0,0.014247,"Du et al., 2017; Mohammad et al., 2017). In our work, we classify (dis)agreement relationships between a pair of stance-bearing utterances. Reason information has been found useful in argumentation mining (Lippi and Torroni, 2016), where studies leverage stance and reason signals for various argumentation tasks (Hasan and Ng, 2014; Boltuzic and Snajder, 2014; Sobhani et al., 2015). We study how to exploit the reason information to better understand the stance, thus addressing a different task. Our work is also related to the tasks on textual relationship inference, such as textual entailment (Bowman et al., 2015), paraphrase detection (Yin and Sch¨utze, 2015), and question answering (Wang et al., 2016). Unlike the textual relationships addressed in those tasks, the relationships between utterances expressing stances do not necessarily contain any rephrasing or entailing semantics, but they do carry discourse signals (e.g., reasons) related to stance expressing. 4 4.1 Experiments Setup Dataset: The evaluation of our model requires a corpus of agreed/disagreed utterance pairs. For this, we adapted a popular corpus for stance detection, i.e., a collection of tweets expressing stances from SemEval-2016 Ta"
P19-1460,D16-1053,0,0.066242,"Missing"
P19-1460,D16-1105,0,0.0328326,"Missing"
P19-1460,N16-1138,0,0.0206686,"detection are confined to the scenario of natural dialogues (Misra and Walker, 2013; Wang and Cardie, 2014; Sridhar et al., 2015; Rosenthal and McKeown, 2015), where dialogic structures are used to create a conversational context for (dis)agreement inference. However, nondialogic stance-bearing utterances are also very common in real-world scenarios. For example, in social media, people can express stances autonomously, without the intention of initiating a discussion (Mohammad et al., 2016). There are also corpora built with articles containing many self-contained stance-bearing utterances (Ferreira and Vlachos, 2016; Bar-Haim et al., 2017). Studying how to detect (dis)agreement between such independent stance-bearing utterances has several benefits: 1) pairing these utterances can lead to a larger (dis)agreement corpus for training a potentially richer model for (dis)agreement detection; 2) the obtained pairs enable training a distance-based model for opinion clustering and subgroup mining; 3) it is applicable to the aforementioned non-dialogic stance corpora; and 4) it encourages discovering useful signals for (dis)agreement detection beyond dialogic features (e.g., the reason information studied in thi"
P19-1460,D13-1191,0,0.247787,"nd Cardie, 2014; Misra and Walker, 2013; Allen et al., 2014). In these studies, various features (e.g., structural, linguistic) and/or specialised lexicons are proposed to recognise (dis)agreement in different dialogic scenarios. In contrast, we detect stance (dis)agreement between independent utterances where dialogic features are absent. Stance classification has recently received much attention in the opinion mining community. Different approaches have been proposed to classify stances of individual utterances in ideological forums (Murakami and Raymond, 2010; Somasundaran and Wiebe, 2010; Gottopati et al., 2013; Qiu et al., 2015) and social media (Augenstein et al., 4667 2016; Du et al., 2017; Mohammad et al., 2017). In our work, we classify (dis)agreement relationships between a pair of stance-bearing utterances. Reason information has been found useful in argumentation mining (Lippi and Torroni, 2016), where studies leverage stance and reason signals for various argumentation tasks (Hasan and Ng, 2014; Boltuzic and Snajder, 2014; Sobhani et al., 2015). We study how to exploit the reason information to better understand the stance, thus addressing a different task. Our work is also related to the t"
P19-1460,I13-1191,0,0.0570302,"Missing"
P19-1460,D14-1083,0,0.064632,"terance2: If guns are outlawed, only Freedom to have a gun is outlaws will have guns. same as freedom of speech. (Stance: Against) (Stance: Against) Class Label: Agree Table 1: The task of detecting stance (dis)agreement between utterances towards a topic of discussion. It has been observed that when expressing 4665 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4665–4671 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics stances, people usually back up their stances with specific explanations or reasons (Hasan and Ng, 2014; Boltuzic and Snajder, 2014). These reasons are informative about which stance is taken, because they give more details on how a stance is developed. However, simply comparing the reasons may not be sufficient to predict stance (dis)agreement, as sometimes people can take the same stance but give different reasons (e.g., the points outlaws having guns and freedom of speech mentioned in Table 1). One way to address this problem is to make the reasons stancecomparable, so that the reason comparison results can be stance-predictive. In this paper, in order to leverage reason information for dete"
P19-1460,D12-1006,0,0.027396,"t stance (dis)agreements. We propose a reason comparing network (RCN) to leverage reason information for stance comparison. Empirical results on a well-known stance corpus show that our method can discover useful reason information, enabling it to outperform several baselines in stance (dis)agreement detection. 1 Introduction Agreement and disagreement naturally arise when peoples’ views, or “stances”, on the same topics are exchanged. Being able to identify the convergence and divergence of stances is valuable to various downstream applications, such as discovering subgroups in a discussion (Hassan et al., 2012; Abu-Jbara et al., 2012), improving recognition of argumentative structure (Lippi and Torroni, 2016), and bootstrapping stance classification with (dis)agreement side information (Sridhar et al., 2014; Ebrahimi et al., 2016). Previous efforts on (dis)agreement detection are confined to the scenario of natural dialogues (Misra and Walker, 2013; Wang and Cardie, 2014; Sridhar et al., 2015; Rosenthal and McKeown, 2015), where dialogic structures are used to create a conversational context for (dis)agreement inference. However, nondialogic stance-bearing utterances are also very common in real-wo"
P19-1460,P15-2088,0,0.0219529,"Missing"
P19-1460,W13-4006,0,0.0977303,"disagreement naturally arise when peoples’ views, or “stances”, on the same topics are exchanged. Being able to identify the convergence and divergence of stances is valuable to various downstream applications, such as discovering subgroups in a discussion (Hassan et al., 2012; Abu-Jbara et al., 2012), improving recognition of argumentative structure (Lippi and Torroni, 2016), and bootstrapping stance classification with (dis)agreement side information (Sridhar et al., 2014; Ebrahimi et al., 2016). Previous efforts on (dis)agreement detection are confined to the scenario of natural dialogues (Misra and Walker, 2013; Wang and Cardie, 2014; Sridhar et al., 2015; Rosenthal and McKeown, 2015), where dialogic structures are used to create a conversational context for (dis)agreement inference. However, nondialogic stance-bearing utterances are also very common in real-world scenarios. For example, in social media, people can express stances autonomously, without the intention of initiating a discussion (Mohammad et al., 2016). There are also corpora built with articles containing many self-contained stance-bearing utterances (Ferreira and Vlachos, 2016; Bar-Haim et al., 2017). Studying how to detect (dis)agre"
P19-1460,S16-1003,0,0.0486474,"Missing"
P19-1460,P16-2022,0,0.0391715,"Missing"
P19-1460,N12-1072,0,0.188056,"Missing"
P19-1460,C10-2100,0,0.0410754,"g (dis)agreement in dialogues (Abbott et al., 2011; Wang and Cardie, 2014; Misra and Walker, 2013; Allen et al., 2014). In these studies, various features (e.g., structural, linguistic) and/or specialised lexicons are proposed to recognise (dis)agreement in different dialogic scenarios. In contrast, we detect stance (dis)agreement between independent utterances where dialogic features are absent. Stance classification has recently received much attention in the opinion mining community. Different approaches have been proposed to classify stances of individual utterances in ideological forums (Murakami and Raymond, 2010; Somasundaran and Wiebe, 2010; Gottopati et al., 2013; Qiu et al., 2015) and social media (Augenstein et al., 4667 2016; Du et al., 2017; Mohammad et al., 2017). In our work, we classify (dis)agreement relationships between a pair of stance-bearing utterances. Reason information has been found useful in argumentation mining (Lippi and Torroni, 2016), where studies leverage stance and reason signals for various argumentation tasks (Hasan and Ng, 2014; Boltuzic and Snajder, 2014; Sobhani et al., 2015). We study how to exploit the reason information to better understand the stance, thus addressi"
P19-1460,P16-1122,0,0.0157854,"s between a pair of stance-bearing utterances. Reason information has been found useful in argumentation mining (Lippi and Torroni, 2016), where studies leverage stance and reason signals for various argumentation tasks (Hasan and Ng, 2014; Boltuzic and Snajder, 2014; Sobhani et al., 2015). We study how to exploit the reason information to better understand the stance, thus addressing a different task. Our work is also related to the tasks on textual relationship inference, such as textual entailment (Bowman et al., 2015), paraphrase detection (Yin and Sch¨utze, 2015), and question answering (Wang et al., 2016). Unlike the textual relationships addressed in those tasks, the relationships between utterances expressing stances do not necessarily contain any rephrasing or entailing semantics, but they do carry discourse signals (e.g., reasons) related to stance expressing. 4 4.1 Experiments Setup Dataset: The evaluation of our model requires a corpus of agreed/disagreed utterance pairs. For this, we adapted a popular corpus for stance detection, i.e., a collection of tweets expressing stances from SemEval-2016 Task 6. It contains tweets with stance labels (Favour, Against, and None) on five topics, i.e"
P19-1460,D16-1244,0,0.0621936,"Missing"
P19-1460,W14-2617,0,0.728826,"rise when peoples’ views, or “stances”, on the same topics are exchanged. Being able to identify the convergence and divergence of stances is valuable to various downstream applications, such as discovering subgroups in a discussion (Hassan et al., 2012; Abu-Jbara et al., 2012), improving recognition of argumentative structure (Lippi and Torroni, 2016), and bootstrapping stance classification with (dis)agreement side information (Sridhar et al., 2014; Ebrahimi et al., 2016). Previous efforts on (dis)agreement detection are confined to the scenario of natural dialogues (Misra and Walker, 2013; Wang and Cardie, 2014; Sridhar et al., 2015; Rosenthal and McKeown, 2015), where dialogic structures are used to create a conversational context for (dis)agreement inference. However, nondialogic stance-bearing utterances are also very common in real-world scenarios. For example, in social media, people can express stances autonomously, without the intention of initiating a discussion (Mohammad et al., 2016). There are also corpora built with articles containing many self-contained stance-bearing utterances (Ferreira and Vlachos, 2016; Bar-Haim et al., 2017). Studying how to detect (dis)agreement between such inde"
P19-1460,D14-1162,0,0.0878186,"Missing"
P19-1460,P11-2065,0,0.080218,"Missing"
P19-1460,W15-4625,0,0.0175468,"e same topics are exchanged. Being able to identify the convergence and divergence of stances is valuable to various downstream applications, such as discovering subgroups in a discussion (Hassan et al., 2012; Abu-Jbara et al., 2012), improving recognition of argumentative structure (Lippi and Torroni, 2016), and bootstrapping stance classification with (dis)agreement side information (Sridhar et al., 2014; Ebrahimi et al., 2016). Previous efforts on (dis)agreement detection are confined to the scenario of natural dialogues (Misra and Walker, 2013; Wang and Cardie, 2014; Sridhar et al., 2015; Rosenthal and McKeown, 2015), where dialogic structures are used to create a conversational context for (dis)agreement inference. However, nondialogic stance-bearing utterances are also very common in real-world scenarios. For example, in social media, people can express stances autonomously, without the intention of initiating a discussion (Mohammad et al., 2016). There are also corpora built with articles containing many self-contained stance-bearing utterances (Ferreira and Vlachos, 2016; Bar-Haim et al., 2017). Studying how to detect (dis)agreement between such independent stance-bearing utterances has several benefi"
P19-1460,W15-0509,0,0.014965,"es have been proposed to classify stances of individual utterances in ideological forums (Murakami and Raymond, 2010; Somasundaran and Wiebe, 2010; Gottopati et al., 2013; Qiu et al., 2015) and social media (Augenstein et al., 4667 2016; Du et al., 2017; Mohammad et al., 2017). In our work, we classify (dis)agreement relationships between a pair of stance-bearing utterances. Reason information has been found useful in argumentation mining (Lippi and Torroni, 2016), where studies leverage stance and reason signals for various argumentation tasks (Hasan and Ng, 2014; Boltuzic and Snajder, 2014; Sobhani et al., 2015). We study how to exploit the reason information to better understand the stance, thus addressing a different task. Our work is also related to the tasks on textual relationship inference, such as textual entailment (Bowman et al., 2015), paraphrase detection (Yin and Sch¨utze, 2015), and question answering (Wang et al., 2016). Unlike the textual relationships addressed in those tasks, the relationships between utterances expressing stances do not necessarily contain any rephrasing or entailing semantics, but they do carry discourse signals (e.g., reasons) related to stance expressing. 4 4.1 E"
P19-1460,N15-1091,0,0.0810396,"Missing"
P19-1460,W10-0214,0,0.046862,"s (Abbott et al., 2011; Wang and Cardie, 2014; Misra and Walker, 2013; Allen et al., 2014). In these studies, various features (e.g., structural, linguistic) and/or specialised lexicons are proposed to recognise (dis)agreement in different dialogic scenarios. In contrast, we detect stance (dis)agreement between independent utterances where dialogic features are absent. Stance classification has recently received much attention in the opinion mining community. Different approaches have been proposed to classify stances of individual utterances in ideological forums (Murakami and Raymond, 2010; Somasundaran and Wiebe, 2010; Gottopati et al., 2013; Qiu et al., 2015) and social media (Augenstein et al., 4667 2016; Du et al., 2017; Mohammad et al., 2017). In our work, we classify (dis)agreement relationships between a pair of stance-bearing utterances. Reason information has been found useful in argumentation mining (Lippi and Torroni, 2016), where studies leverage stance and reason signals for various argumentation tasks (Hasan and Ng, 2014; Boltuzic and Snajder, 2014; Sobhani et al., 2015). We study how to exploit the reason information to better understand the stance, thus addressing a different task. Our work"
P19-1460,P15-1012,0,0.0758935,"s, or “stances”, on the same topics are exchanged. Being able to identify the convergence and divergence of stances is valuable to various downstream applications, such as discovering subgroups in a discussion (Hassan et al., 2012; Abu-Jbara et al., 2012), improving recognition of argumentative structure (Lippi and Torroni, 2016), and bootstrapping stance classification with (dis)agreement side information (Sridhar et al., 2014; Ebrahimi et al., 2016). Previous efforts on (dis)agreement detection are confined to the scenario of natural dialogues (Misra and Walker, 2013; Wang and Cardie, 2014; Sridhar et al., 2015; Rosenthal and McKeown, 2015), where dialogic structures are used to create a conversational context for (dis)agreement inference. However, nondialogic stance-bearing utterances are also very common in real-world scenarios. For example, in social media, people can express stances autonomously, without the intention of initiating a discussion (Mohammad et al., 2016). There are also corpora built with articles containing many self-contained stance-bearing utterances (Ferreira and Vlachos, 2016; Bar-Haim et al., 2017). Studying how to detect (dis)agreement between such independent stance-bearing"
P19-1460,W14-2715,0,0.533664,"cover useful reason information, enabling it to outperform several baselines in stance (dis)agreement detection. 1 Introduction Agreement and disagreement naturally arise when peoples’ views, or “stances”, on the same topics are exchanged. Being able to identify the convergence and divergence of stances is valuable to various downstream applications, such as discovering subgroups in a discussion (Hassan et al., 2012; Abu-Jbara et al., 2012), improving recognition of argumentative structure (Lippi and Torroni, 2016), and bootstrapping stance classification with (dis)agreement side information (Sridhar et al., 2014; Ebrahimi et al., 2016). Previous efforts on (dis)agreement detection are confined to the scenario of natural dialogues (Misra and Walker, 2013; Wang and Cardie, 2014; Sridhar et al., 2015; Rosenthal and McKeown, 2015), where dialogic structures are used to create a conversational context for (dis)agreement inference. However, nondialogic stance-bearing utterances are also very common in real-world scenarios. For example, in social media, people can express stances autonomously, without the intention of initiating a discussion (Mohammad et al., 2016). There are also corpora built with article"
P19-1510,W04-2705,0,\N,Missing
P19-1510,J93-2004,0,\N,Missing
P19-1510,sekine-etal-2002-extended,0,\N,Missing
P19-1510,D09-1015,0,\N,Missing
P19-1510,W07-1009,0,\N,Missing
P19-1510,W03-0419,0,\N,Missing
P19-1510,J96-2004,0,\N,Missing
P19-1510,P07-1031,1,\N,Missing
P19-1510,D15-1102,0,\N,Missing
P19-1510,N16-1030,0,\N,Missing
P19-1510,P17-1114,0,\N,Missing
P19-1510,D17-1276,0,\N,Missing
P19-1510,N18-1079,0,\N,Missing
P19-1510,N18-1131,0,\N,Missing
P19-1510,P18-3006,1,\N,Missing
P19-1510,D18-1124,0,\N,Missing
P19-1510,D18-1309,0,\N,Missing
P19-1510,D18-1019,0,\N,Missing
P19-1510,E12-2021,0,\N,Missing
P85-1029,J80-3003,0,\N,Missing
P87-1014,H86-1019,0,\N,Missing
P87-1014,C86-1137,0,\N,Missing
P87-1014,P85-1029,1,\N,Missing
P87-1014,P84-1065,1,\N,Missing
P89-1025,P85-1007,0,0.02541,"ntentional, attentional, and rhetorical structure of the generated text. In addition, since the expert system explanation facility is intended to be used by many different users, the text planner takes knowledge about the user into account. In our system, the user model contains the user's domain goals and the knowledge he is assumed to have about the domain. THE PLAN LANGUAGE In our plan language, intentional goals are represented in terms of the effects the speaker intends his utterance to have on the hearer. Following Hovy (1988a), we use the terminology for expressing beliefs developed by Cohen and Levesque (1985) in their theory of rational interaction, but have found the need to extend the terminology to represent the types of intentional goals necessary for the kinds of responses desired in an advisory setting. Although Cohen and Levesque have subsequently retracted some aspects of their theory of rational interaction (Cohen and Levesque, 1987), the utilityof their notation for our purposes remains unaffected, as argued in (Hovy, 1989).2 aPEA recommends transformations that improve the 'style' of the user's code. It does not attempt to understand the content of the user's program. 2Space limitations"
P89-1025,P88-1020,0,0.613791,"RPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. 203 DIALOGUES&quot; C~cile L. Paris USC/information Sciences Institute 4676 Admiralty Way Marina del Key, CA 90292-6695, USA the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical and attentional information to produce coherent text (McKeown, 1985, McCoy, 1985, Paris, 1988b), omitting intentional information, or conflating intentional and rhetorical information (Hovy, 1988b). No text generation system records or reasons about the rhetorical, the attentional, as well as the intentional structures of the texts it produces. In this paper, we argue that to successfully participate in an explanation dialogue, a generation system must maintain the kinds of information outlined by Grosz and Sidner as well as an explicit representation of the rhetorical structure of the texts it generates. We present a text planner that builds a detailed text plan, containing the intentional, attentional, and rhetorical structures of the responses it produces. The main focus of this pa"
P89-1025,J88-3006,1,0.244126,"The research described in this paper was supported by the Defense Advanced Research Projects Agency (DARPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. 203 DIALOGUES&quot; C~cile L. Paris USC/information Sciences Institute 4676 Admiralty Way Marina del Key, CA 90292-6695, USA the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical and attentional information to produce coherent text (McKeown, 1985, McCoy, 1985, Paris, 1988b), omitting intentional information, or conflating intentional and rhetorical information (Hovy, 1988b). No text generation system records or reasons about the rhetorical, the attentional, as well as the intentional structures of the texts it produces. In this paper, we argue that to successfully participate in an explanation dialogue, a generation system must maintain the kinds of information outlined by Grosz and Sidner as well as an explicit representation of the rhetorical structure of the texts it generates. We present a text planner that builds a detailed text plan, containing the inten"
P89-1025,J86-3001,0,\N,Missing
P96-1026,W94-0308,1,0.837463,"that the patterns of realisations uncovered in our analysis follow the principle of good technical writing practice known as the minimalist approach, e.g., (Carroll, 1994; Hammond, 1994). Moreover, we observe that our corpus does not exhibit shortcomings identified in a Systemic Functional analysis of English software manuals (Plum et al., 1990), such as a high incidence of agentless passive and a failure to distinguish the function of informing from that of instructing. Other work has focused on the cross-linguistic realisations of two specific semantic relations (generation and enablement) (Delin et al., 1994; Delia et 197 al., 1996), in a more general corpus of instructions for household appliances. Our work focuses on the single application domain of software instructions. However, it takes into consideration the whole task structure and looks at the realisation of semantic elements as found in the knowledge base, instead of two semantic relations not explicitly present in the underlying semantic model. 10 Conclusion In this paper we have shown how genre and task structure provide two essential sources of control over the text generation process. Genre does so by constraining the selection of th"
P96-1026,C96-1050,1,0.873674,"Missing"
P96-1026,J82-2006,0,0.144411,"Furthermore, we know that Macintosh documentation undergoes thorough local quality control. It certainly conforms to the principles of good documentation established by current research on technical documentation and on the needs of end-users, e.g., (Carroll, 1994; Hammond, 1994), in that it supplies clear and concise information for the task at hand. Finally, we have been assured by French users of the software that they consider this particular manual to be well written and to bear no unnatural trace of its origins. Technical manuals within a specific domain constitute a sublanguage, e.g., (Kittredge, 1982; Sager et al., 1980). An important defining property of a sublanguage is that of closure, both lexieal and syntactic. Lexical closure has been demonstrated by, for example, (Kittredge, 1987), who shows that after as few as the first 2000 words of a sublanguage text, the number of new word types increases little if at all. Other work, e.g., (Biber, 1988; Biber, 1989) and (Grishman and Kittredge, 1986) illustrates the property of syntactic closure, which means that generally available constructions just do not occur in this or that sublanguage. In the light of these results, we considered a cor"
P96-1026,W94-0307,0,0.164563,"two chapters which provide the user with generic instructions for performing relevant tasks, and descriptions of the commands available within MacWrite. The overlap in information between the two chapters offers opportunities to observe differences in the linguistic expressions of the same task structure elements in different contexts. features Our lexico-grammatical coding was done using the networks and features of the Nigel grammar (Halliday, 1985). We focused on four main concerns, guided by previous work on instructional texts, e.g., (Lehrberger, 1986; Plum et at., 1990; Ghadessy, 1993; Kosseim and Lapalme, 1994). • Relations between processes: to determine whether textual cohesion was achieved through conjunctives or through relations implicit in the task structure elements. Among the features considered were clause dependency and conjunction type. • Agency: to see whether the actor performing or enabling a particular action is clearly identified, and whether the reader is explicitly addressed. We coded here for features such as voice and agent types. • Mood, modality and polarity: to find out the extent to which actions are presented to the reader as being desirable, possible, mandatory, or prohibit"
U03-1012,P00-1041,0,0.0212721,"vation behind discovering segments in a text is that a sentence extraction summary should choose the most representative sentence for each segment, resulting in a comprehensive summary. In the view of Gong and Liu (2001), segments form the main themes of a document. They present a theme interpretation of the SVD analysis, as it is used for discourse segmentation, upon which our use of the technique is based. However, Gong and Liu use SVD for creating sentence extraction summaries, not for generating a single sentence summary by re-using words. In subsequent work to Witbrock and Mittal (1999), Banko et al. (2000) describe the use of information about the position of words within four quarters of the source document. The headline candidacy score of a word is weighted by its position in one of the quarters. We interpret this use of position information as a means of guiding the generation of a headline towards the central theme of the document, which for news articles typically occurs in the first quarter. SVD potentially offers a more general mechanism for handling the discovery of the central themes and their positions within the document. Jin et al. (2002) have also examined a statistical model for h"
U03-1012,W97-0704,0,0.0468846,"generates summaries greater in length than a sentence. We direct the reader to Paice (1990) for an overview of summarisation based on sentence extraction. Examples of recent systems include Kupiec et al. (1995) and Brandow et al. (1995). For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing. Hybrid methods for abstract-like summarisation, which combine statistical and symbolic approaches, have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summarisation has been explored by a number of researchers (see for example, Witbrock and Mittal, 1999; Zajic et al., 2002). Interestingly, in the work of Witbrock and Mittal (1999), the selection of words for inclusion in the headline is decided solely on the basis of corpus statistics and does not use statistical information about the distribution of words in the document itself. Our work differs in that we utilise an SVD analysis to provide information about the document to be summarized, specifically its main theme. Discourse segmentation for sentence extracti"
U03-1012,J00-2011,0,0.290873,"mentation, where sentences naturally cluster in terms of their ‘aboutness’. $ is about, a characteristic described by Borko as being indicative. 5 Using Singular Value Decomposition for Content Selection As an alternative to the Conditional probability, we examine the use of SVD in determining the Content Selection probability. Before we outline the procedure for basing this probability on SVD, we will first outline our interpretation of the SVD analysis, based on that of Gong and Liu (2001). Our description is not intended to be a comprehensive explanation of SVD, and we direct the reader to Manning and Schütze (2000) for a description of how SVD is used in information retrieval. Conceptually, when used to analyse documents, SVD can discover relationships between word co-occurrences in a corpus of text. For example, in the context of information retrieval, this provides one way to retrieve additional documents that contain synonyms of query terms, where synonymy is defined by similarity of word co-occurrences. By discovering patterns in word co-occurrences, SVD also provides information that can be used to cluster documents based on similarity of themes. In the context of single document summarisation, we"
U03-1012,J98-3005,0,0.171898,"ppropriate, given that the performance does not differ considerably. In such a situation, a collection of documents is only necessary for collecting bigram statistics. 7 Related Work As the focus of this paper is on statistical singlesentence summarisation we will not focus on preceding work which generates summaries greater in length than a sentence. We direct the reader to Paice (1990) for an overview of summarisation based on sentence extraction. Examples of recent systems include Kupiec et al. (1995) and Brandow et al. (1995). For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing. Hybrid methods for abstract-like summarisation, which combine statistical and symbolic approaches, have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summarisation has been explored by a number of researchers (see for example, Witbrock and Mittal, 1999; Zajic et al., 2002). Interestingly, in the work of Witbrock and Mittal (1999), the selection of words for inclusion in the headline is decided solely on the basis of corpus"
U03-1012,W03-1202,1,0.166119,"ence summaries using SVD. In Section 6, we present our experimental design in which we evaluated our approach, along with the results and corresponding discussion. Section 7, provides an overview of related work. Finally, in Section 8, we present our conclusions and future work. 2 Searching for a Probable Headline We re-implemented the work described in Witbrock and Mittal (1999) to provide a single sentence summarisation mechanism. For full details of their approach, we direct the reader to their paper (Witbrock and Mittal, 1999). For an overview of our implementation of their algorithm, see Wan et al. (2003). For convenience, a brief description is presented here. In a search, n words are selected on the basis of the two criteria. Conceptually, the task is twofold. Witbrock and Mittal (1999) label these two tasks as Content Selection and Realisation. Each criterion is scored probabilistically, whereby the probability is estimated by prior collection of corpus statistics. To estimate Content Selection probability for each word, we use the Maximum Likelihood Estimate (MLE). In an offline training stage, the system counts the number of times a word is used in a headline, with the condition that it o"
U04-1009,P88-1020,0,0.116511,", the first paragraph is the nucleus, while the other two paragraphs are satellites. Content Content Presentation Presentation Figure 2: Content and presentation planning for two different devices For reasons of simplicity and modularity, we explicitly maintain a conceptual and architectural separation between the different processes of content planning, presentation planning and surface realisation. This separation is closely analogous to the distinction between document planning, micro planning and surface realisation adopted by many natural language generation systems (e.g., McKeown, 1985; Hovy 1988; Moore and Paris, 1993). We exploited this generation paradigm to build several prototypes of information delivery systems, in particular one in the travel domain (cf., Wilkinson et al., 2000; Paris et al., 2001; Paris, 2002) and one in the corporate domain, where users received a brochure about CSIRO tailored to their interest and needs (cf. Paris et al., 2003). In these prototypes, the process was exactly as illustrated in Figure 2. Through these prototypes, however, we have now come to recognise that, during the presentation planning stage, we must distinguish between two types of decision"
U04-1009,J93-4004,1,0.796185,"the “information assembly” step. Figure 1: Adapting the presentation of information to suit different devices Finally, we briefly introduce the last stage of the information delivery process, the realisation. The paper ends with some concluding remarks. 2 The Myriad Architecture The core of the Myriad architecture is our information planning engine, which we call the Virtual Document Planner (VDP) (Colineau et al., 2004b). The VDP is based on a typical Natural Language Generation (NLG) architecture, where the linguistic resources are separate from the planning engine. The VDP is based on the Moore and Paris (1993) text planner, and, as in that text planner, the resources are represented as plans. The VDP works essentially as follows. Given a top level communicative goal (an overall purpose for presenting information), the engine uses a library of discourse rules (plan operators) to select and organise the content. Then, a library of presentation plans is employed to make local presentation decisions about how to present content. The output of the discourse planning stage is a discourse tree, which is then augmented (extended) during the presentation planning stage. This is shown schematically in Figure"
U04-1012,W98-1402,0,0.0279048,"le from the FOCAL scenario. 2.1 have contributed to the progresses made in this domain:1 Background Studies in natural language generation have considerably influenced the research directions in multimedia information presentation, in particular on the issues of how to represent the global discourse structure, and how to organise and integrate each source of information in relation to the others. Several important notions Starting from these notions, the generation of multimedia information presentations has been considered by many researchers (e.g., André and Rist, 1990; 1993; Maybury, 1993; Bateman et al., 1998; Green et al., 1998; Mittal et al., 1998) as a goal-directed activity that starts from a communicative goal (i.e., a presentation intent), which is then further refined into communicative acts. Indeed, based on studies done in linguistics and philosophy (e.g., Austin 1962; Searle, 1969), in discourse (e.g., Grosz and Sidner, 1986) and in text planning (e.g., Hovy, 1988; Arens et al., 1993; Moore and Paris, 1993), the multimedia generation community has built on the idea that the internal organisation of a discourse or a presentation is composed of a hierarchy of communicative acts, each act s"
U04-1012,W03-2110,1,0.774132,"rmation can be displayed. A number of modalities and media are available to display the information to the end-user. These include visual display mechanisms, such as 3-D virtual batttlespace, and spoken dialogue interaction with virtual conversational characters (VCCs) that allow presentation of information through speech as well as through textual Dominique Estival Defence Science & Technology Organisation (DSTO) Human Systems Integration Group Command and Control Division Edinburgh SA 5111, Australia Dominique.Estival@dsto.defence.gov.au displays (Taplin et al. 2001; Broughton et al., 2002; Estival et al., 2003). While these have so far been studied and implemented somewhat independently of each other, ultimately all the different available means to present the information to the end-user must work together and be combined into a coherent whole; otherwise, the result would be very confusing to the user. From the delivery perspective (as opposed to the input fusion aspect) with which we are concerned here, FOCAL can be considered as an instance of an intelligent multimedia presentation (IMMP) system (see Bordegoni et al., 1997 for a reference model). 1.2 An IMMP Architecture for FOCAL The framework fo"
U04-1012,W98-0210,0,0.0216845,"ario. 2.1 have contributed to the progresses made in this domain:1 Background Studies in natural language generation have considerably influenced the research directions in multimedia information presentation, in particular on the issues of how to represent the global discourse structure, and how to organise and integrate each source of information in relation to the others. Several important notions Starting from these notions, the generation of multimedia information presentations has been considered by many researchers (e.g., André and Rist, 1990; 1993; Maybury, 1993; Bateman et al., 1998; Green et al., 1998; Mittal et al., 1998) as a goal-directed activity that starts from a communicative goal (i.e., a presentation intent), which is then further refined into communicative acts. Indeed, based on studies done in linguistics and philosophy (e.g., Austin 1962; Searle, 1969), in discourse (e.g., Grosz and Sidner, 1986) and in text planning (e.g., Hovy, 1988; Arens et al., 1993; Moore and Paris, 1993), the multimedia generation community has built on the idea that the internal organisation of a discourse or a presentation is composed of a hierarchy of communicative acts, each act supporting a specific"
U04-1012,J86-3001,0,0.140136,"e and integrate each source of information in relation to the others. Several important notions Starting from these notions, the generation of multimedia information presentations has been considered by many researchers (e.g., André and Rist, 1990; 1993; Maybury, 1993; Bateman et al., 1998; Green et al., 1998; Mittal et al., 1998) as a goal-directed activity that starts from a communicative goal (i.e., a presentation intent), which is then further refined into communicative acts. Indeed, based on studies done in linguistics and philosophy (e.g., Austin 1962; Searle, 1969), in discourse (e.g., Grosz and Sidner, 1986) and in text planning (e.g., Hovy, 1988; Arens et al., 1993; Moore and Paris, 1993), the multimedia generation community has built on the idea that the internal organisation of a discourse or a presentation is composed of a hierarchy of communicative acts, each act supporting a specific communicative goal that contributes to the whole. It has then extended this principle to multimedia material. Thus, as pointed out by Maybury (1993, p.61): “As text can be viewed as consisting of a hierarchy of intentions, similarly, multimedia communication can be viewed as consisting of linguistic and graphic"
U04-1012,P88-1020,0,0.313676,"ence model for FOCAL. We conclude in Section 4 with a short discussion of the evaluation to be undertaken. 2 • the notion of discourse structure and the generation of multi-sentential texts, as embodied, for example in (McKeown, 1985a; 1985b; Moore and Paris, 1993); • the notion of coherence and the rhetorical dependencies between discourse parts, as defined, for example, in Rhetorical Structure Theory (RST) (Mann and Thompson, 1988); and finally, • the hierarchical planning approach as a means to structure and to represent a discourse goal hierarchy and the relationships between them, as in (Hovy, 1988; Moore and Paris, 1993) inter alia. IMMP Systems Intelligent multimedia presentation systems (IMMP) are characterised by their capacity to automate the design of multimedia presentations. IMMP systems typically base their design decisions on explicit representations of diverse knowledge, and combine mechanisms and techniques that select, organise and coordinate relevant information across appropriate media. Such systems present the advantages to be: • Adaptable and flexible by generating on-thefly multimedia presentations of various combinations of information and media characteristics; • Con"
U04-1012,J98-3004,0,0.0321905,"ibuted to the progresses made in this domain:1 Background Studies in natural language generation have considerably influenced the research directions in multimedia information presentation, in particular on the issues of how to represent the global discourse structure, and how to organise and integrate each source of information in relation to the others. Several important notions Starting from these notions, the generation of multimedia information presentations has been considered by many researchers (e.g., André and Rist, 1990; 1993; Maybury, 1993; Bateman et al., 1998; Green et al., 1998; Mittal et al., 1998) as a goal-directed activity that starts from a communicative goal (i.e., a presentation intent), which is then further refined into communicative acts. Indeed, based on studies done in linguistics and philosophy (e.g., Austin 1962; Searle, 1969), in discourse (e.g., Grosz and Sidner, 1986) and in text planning (e.g., Hovy, 1988; Arens et al., 1993; Moore and Paris, 1993), the multimedia generation community has built on the idea that the internal organisation of a discourse or a presentation is composed of a hierarchy of communicative acts, each act supporting a specific communicative goal th"
U04-1012,J93-4004,1,0.862988,"dia presentation has grown from notions and systems developed in natural language generation. We then describe the process of generating multimedia presentations, and, in particular, an approach to integrate coherently multimedia content, with examples from the FOCAL scenario. In Section 3 we describe the design for an IMMP architecture based on the reference model for FOCAL. We conclude in Section 4 with a short discussion of the evaluation to be undertaken. 2 • the notion of discourse structure and the generation of multi-sentential texts, as embodied, for example in (McKeown, 1985a; 1985b; Moore and Paris, 1993); • the notion of coherence and the rhetorical dependencies between discourse parts, as defined, for example, in Rhetorical Structure Theory (RST) (Mann and Thompson, 1988); and finally, • the hierarchical planning approach as a means to structure and to represent a discourse goal hierarchy and the relationships between them, as in (Hovy, 1988; Moore and Paris, 1993) inter alia. IMMP Systems Intelligent multimedia presentation systems (IMMP) are characterised by their capacity to automate the design of multimedia presentations. IMMP systems typically base their design decisions on explicit rep"
U04-1012,W04-0609,1,0.81816,"ue mode). When the system is answering a user’s query, the DM agent has to understand the user’s query in order to identify what is the user’s information need.5 This requires the DM agent to have access to domain knowledge, e.g., an ontology of the 4 We are currently collaborating with UniSA on the design of a Media Selection agent and a Media Presentation agent for these two layers. 5 In briefing mode, the DM agent generates the information need, while in dialogue mode, the information need comes from the input devices, whose output is sent to the Input Fuser and then to the DM. domain (see Nowak et al., 2004), to ensure that the query makes sense (i.e., is syntactically and semantically well-structured). Then, the DM’s aim is to determine a communicative goal which answers this information need and to send this goal to the MMP agent. The communicative goal thus constitutes the input to the MMP agent. From this input, the MMP selects the appropriate discourse strategies to be developed (e.g., &quot;explain mission&quot;). Once the MMP receives a communicative goal, it can develop a discourse plan to satisfy this goal. The discourse plan aims at selecting the relevant content and organising it. A set of queri"
U06-1007,W04-3240,0,0.244905,"Missing"
U06-1007,J00-3003,0,0.26356,"Missing"
U06-1007,W04-1008,0,0.0607951,"Missing"
U06-1007,A97-1011,0,0.00863765,"exical or contentless utterances; terms of address or salutation. 2nd person; verb implies an attribute or ability of the other; terms of evaluation. 2nd person; verb implies internal experience or volitional action. Table 4: VRM form criteria from (Stiles, 1992) The intuition for including the utterance length as a feature is that different VRMs are often associated with longer or shorter utterances - e.g., Acknowledgement utterances are often short, while Edifications are often longer. To compute our utterance features, we made use of the Connexor Functional Dependency Grammar (FDG) parser (Tapanainen and Jarvinen, 1997) for grammatical analysis and to extract syntactic dependency information for the words in each utterance. We also used the morphological tags assigned by Connexor. This information was used to calculate utterance features as follows: • Utterance Length: The number of words in the utterance. • First Word: The first word in each utterance, represented as a series of independent boolean features (one for each unique first word present in the corpus). • Last Token: The last token in each utterance – either the final punctuation (if present) or the final word in the utterance. As for the First Wor"
U06-1013,C02-1150,0,0.0948066,"Missing"
U06-1019,P05-1074,0,0.019642,"known performance for this corpus. We also examine the strengths and weaknesses of dependency based features and conclude that they may be useful in more accurately classifying cases of False Paraphrase. 1 Introduction In recent years, interest has grown in paraphrase generation methods. The use of paraphrase generation tools has been envisaged for applications ranging from abstract-like summarisation (see for example, Barzilay and Lee (2003), Daum´e and Marcu (2005), Wan et al. (2005)), questionanswering (for example, Marsi and Krahmer (2005)) and Machine Translation Evaluation (for example, Bannard and Callison-Burch (2005) and Yves Lepage (2005)). These approaches all employ a loose definition of paraphrase attributable to Dras (1999), who defines a ‘paraphrase pair’ operationally to be “a pair of units of text deemed to be interchangeable”. Notably, such a definition of paraphrase lends itself easily to corpora based methods. Furthermore, what the more modern approaches share is the fact that often they generate new paraphrases from raw text not semantic representations. The generation of paraphrases from raw text is a specific type of what is commonly referred to as text-to-text generation (Barzilay and Lee,"
U06-1019,W05-1612,0,0.0149558,"Missing"
U06-1019,N03-1003,0,0.0184087,"Missing"
U06-1019,U03-1014,0,0.036602,"Missing"
U06-1019,P96-1025,0,0.0188936,"Missing"
U06-1019,P02-1040,0,0.0775439,"equence overlap, where tokenisation is delimited by white space. We considered unigram overlap and explored two metrics, recall (feature 1) and precision (feature 2), where a precision score is defined as: precision = word-overlap(sentence1 , sentence2 ) word-count(sentence1 ) and recall is defined as: recall = word-overlap(sentence1 , sentence2 ) word-count(sentence2 ) nexor parser2 which provides lemmatisation information. For both sentences, each original word is replaced by its lemma. We then calculated our unigram precision and recall scores as before (features 3 and 4). The Bleu metric (Papineni et al., 2002), which uses the geometric average of unigram, bigram and trigram precision scores, is implemented as feature 5. The score was obtained using the original Bleu formula3 with a brevity penalty set to 1 (that is, the brevity penalty is ignored). Note that in our usage, there is only one ’reference’ sentence. By reversing which sentence was considered the ‘test’ sentence and which was considered the ‘reference’, a recall version of Bleu was obtained (feature 6). Lemmatised versions provided features 7 and 8. Finally, because of the bi-directionality property of paraphrase, the F-Measure4 , which"
U06-1019,W05-1203,0,0.0338777,"Missing"
U06-1019,C04-1051,0,0.253524,"Missing"
U06-1019,W06-1603,0,0.516548,"Missing"
U06-1019,I05-5012,1,0.793999,"nd sentence was generated from an input news article statistically using a four-gram language model and a probabilistic word selection module. Although other paraphrase generation approaches differ in their underlying mechanisms1 , most generate a novel sentence that cannot be found verbatim in the input text. The generated second sentence of the example is intended to be a paraphrase of the article headline. One might be convinced that the first exam1 The details of the generation algorithm used for this example are peripheral to the focus of this paper and we direct the interested reader to Wan et al. (2005) for more details. Proceedings of the 2006 Australasian Language Technology Workshop (ALTW2006), pages 131–138. 131 2 Paraphrase Classification and Related Work Example 1: Original Headline: European feeds remain calm on higher dollar. Generated Sentence: The European meals and feeds prices were firm on a stronger dollar; kept most buyers in this market. Example 2: Original Headline: India’s Gujral says too early to recognise Taleban. Generated Sentence: Prime Minister Inder Kumar Gujral of India and Pakistan to recognise the Taleban government in Kabul. Figure 1: Two examples of generated nov"
U06-1019,I05-5008,0,0.0109142,"examine the strengths and weaknesses of dependency based features and conclude that they may be useful in more accurately classifying cases of False Paraphrase. 1 Introduction In recent years, interest has grown in paraphrase generation methods. The use of paraphrase generation tools has been envisaged for applications ranging from abstract-like summarisation (see for example, Barzilay and Lee (2003), Daum´e and Marcu (2005), Wan et al. (2005)), questionanswering (for example, Marsi and Krahmer (2005)) and Machine Translation Evaluation (for example, Bannard and Callison-Burch (2005) and Yves Lepage (2005)). These approaches all employ a loose definition of paraphrase attributable to Dras (1999), who defines a ‘paraphrase pair’ operationally to be “a pair of units of text deemed to be interchangeable”. Notably, such a definition of paraphrase lends itself easily to corpora based methods. Furthermore, what the more modern approaches share is the fact that often they generate new paraphrases from raw text not semantic representations. The generation of paraphrases from raw text is a specific type of what is commonly referred to as text-to-text generation (Barzilay and Lee, 2003). As techniques fo"
U06-1019,U05-1023,0,0.485398,"Missing"
U06-1019,I05-5003,0,\N,Missing
U08-1009,W04-3240,0,0.209927,"Missing"
U08-1009,W04-1008,0,0.581779,"y existing definitions deal only with requests and ignore conditionality (which is very common) as a feature (for example, (Camino et al., 1998; Khosravi and Wilks, 1999; Leuski, 2004)). Others define requests and commitments in terms of specific conversation states, requiring the creation of multiple categories for the same speech act in different stages of a conversation. Often not all of the many combinations of speech acts and conversation states are modeled, resulting in uncodable utterances (Cohen et al., 2004; Goldstein and Sabin, 2006). Previous work on utterance-level classification (Corston-Oliver et al., 2004) relied on short, simple definitions and canonical examples. These lack the detail and clarity required for unambiguous classification of the complex requests and commitments we find in real-world email. Since our review of related work, Scerri et al. (2008) have noted some similar concerns. Unfortunately, their interannotator agreement for requests and commitments remains low; we believe this could be improved through the careful consideration of the edge cases we outline in this paper. Conditionality is an important part of our definitions. Conditional requests and commitments require action"
U08-1009,scerri-etal-2008-evaluating,0,0.0779736,", requiring the creation of multiple categories for the same speech act in different stages of a conversation. Often not all of the many combinations of speech acts and conversation states are modeled, resulting in uncodable utterances (Cohen et al., 2004; Goldstein and Sabin, 2006). Previous work on utterance-level classification (Corston-Oliver et al., 2004) relied on short, simple definitions and canonical examples. These lack the detail and clarity required for unambiguous classification of the complex requests and commitments we find in real-world email. Since our review of related work, Scerri et al. (2008) have noted some similar concerns. Unfortunately, their interannotator agreement for requests and commitments remains low; we believe this could be improved through the careful consideration of the edge cases we outline in this paper. Conditionality is an important part of our definitions. Conditional requests and commitments require action only if a stated condition is satisfied. Our early annotation experiments, summarised in Section 3 and detailed in (Lampert et al., 2007), show that annotators require guidance about how to classify conditional requests and commitments to achieve even moder"
U08-1017,J93-4004,1,0.768795,"Theory (RST) (Mann and Thompson, 1988). The systems then reason about this discourse tree to allow them to participate in a dialogue (e.g., Moore and Swartout, 1989), generate appropriate cue phrases to link two spans of text (e.g., Scott and de Souza, 1990) or reason about layout (e.g., Bateman et al., 2001). See Taboada and Mann (2006) for other applications. Our system uses discourse trees to reason about how much content to express in order to fill some specific available space. Other systems have performed reasoning at the discourse structure level to control how much text is generated. Moore and Paris (1993), for example, allowed their discourse planner to be set in terse or verbose mode to produce short or long texts. Their approach thus constrained the length of the generated content at the onset of the process. However, this can be overly restrictive. In contexts such as ours, for example, in which similar content must be delivered via several media, it is desirable to produce one discourse tree that can then be delivered appropriately to the different delivery channels and conform to different space constraints. During discourse planning, our system specifies the RST relations that hold betwe"
U08-1017,C94-1056,0,0.101444,"Missing"
U08-1017,J00-2005,0,0.024997,"levance scores to text nodes in the discourse structure to produce documents of variable length. While our approach is similar to O’Donnell’s in that respect, his approach required individual sentences to be manually marked up with rhetorical relations. This allowed his system to manipulate the text at or below the sentence level, although repair had to occur after the process to ensure coherence and grammaticality. O’Donnell’s approach was thus close to traditional NLG systems that build text from first principles and are able to vary the amount of text at the lexico-grammatical level (e.g., Reiter, 2000). Like most NLG-based approaches to constraining the length of a text, we use greedy algorithms to cut content. Vander Linden (2008) reports on an alternate approach that used dynamic programming. His work has not yet been evaluated, so it is unclear how valuable it could be in our context. consults a repository of text fragments to assemble the relevant information. The fragments, written by the marketing team of our organisation, are self contained and comprised of typically one paragraph, two at most. 3 SciFly integrates all the relevant fragments into a coherent whole (see Figure 1) using"
U08-1017,W08-1125,0,0.0175855,"Donnell’s in that respect, his approach required individual sentences to be manually marked up with rhetorical relations. This allowed his system to manipulate the text at or below the sentence level, although repair had to occur after the process to ensure coherence and grammaticality. O’Donnell’s approach was thus close to traditional NLG systems that build text from first principles and are able to vary the amount of text at the lexico-grammatical level (e.g., Reiter, 2000). Like most NLG-based approaches to constraining the length of a text, we use greedy algorithms to cut content. Vander Linden (2008) reports on an alternate approach that used dynamic programming. His work has not yet been evaluated, so it is unclear how valuable it could be in our context. consults a repository of text fragments to assemble the relevant information. The fragments, written by the marketing team of our organisation, are self contained and comprised of typically one paragraph, two at most. 3 SciFly integrates all the relevant fragments into a coherent whole (see Figure 1) using meta-data describing each fragment. The meta-data chosen is that envisaged for a new content management system for our organisation’"
U08-1017,P89-1025,1,\N,Missing
U11-1014,W10-3104,0,0.0199652,"2010). We apply SVMs in our experiments and compare its performance with some other popular classifiers. Another important aspect of our work is negation detection. Negated terms in medical text usually indicate the presence or absence of specific medical findings. Additionally, they may also indicate the polarity of the outcome presented in a medical article (e.g., drug X shows no improvement for patients suffering from condition Y). Recent research work has shown that information on the polarity of phraselevel assertions does not improve performance in a document level classification task (Goldstein and Uzuner, 2010). However, statistics based on the presence/absence of negations have not been incorporated for text classification in this domain. Negation identification has shown to markedly improve performance of medical information retrieval systems. Therefore, there has been a significant amount of work on automatic negation detection techniques in the medical domain, such as the works of Elkin et al. (2005) and Huang et al. (2007) . Rokach et al. (2008) provides a detailed survey of negation detection techniques for the medical domain. A popular and simple negation detection approach is NegEx (Chapman"
U11-1014,J07-1005,0,0.128657,"egarding the efficacy of drug X for condition Y). Manually assessing the outcomes presented by multiple medical papers on a given topic is a time-consuming task and often cannot be efficiently performed at point of care (Ely et al., 1999). Hence, there is a strong need for automatic outcome polarity identification techniques to aid the decision making process of practitioners. 1.1 Motivation In order to appease the problem of information overload faced by medical domain experts, research has focused on information retrieval, automatic summarisation and question answering of medical documents (Lin and Demner-Fushman, 2007; Fiszman et al., 2009). Intelligent text processing systems that perform automatic summarisation and question answering for this domain can benefit significantly from techniques that can automatically detect the polarity of outcomes presented in documents. Such techniques will be particularly useful for multidocument summarisation, where the detection of contradictory or consistent outcomes presented in separate documents is vital. Furthermore, recent research on Abeed Sarker, Diego Moll´ a and C´ecile Paris. 2011. Outcome Polarity Identification of Medical Papers. In Proceedings of Australas"
U11-1014,P04-1035,0,0.370601,"Missing"
U11-1014,W02-1011,0,0.0228866,"Missing"
U11-1014,P02-1053,0,0.0228385,"Missing"
U11-1014,J11-2001,0,\N,Missing
U12-1011,J06-1003,0,0.0267495,"idea of the coverage upper limits. 4.1.2 matched CUIs. That is, besides considering direct matches between CUIs, we also consider similarities among concepts when performing this calculation. This is important because often bottomline summaries contain generic terms representing the more speciﬁc concepts in the source texts (e.g., the generic term anti-depressant in the bottomline summary to represent paroxetine, amitriptyline and so on). The concept similarity between two concepts gives a measure of their semantic relatedness or how close two concepts are within a speciﬁc domain or ontology (Budanitsky and Hirst, 2006). In our concept coverage computation, each CUI in a receives a score of 1.0 if it has an exact match with the CUIs in Da . For each unmatched CUI in a, its concept similarity value with each unmatched concept in Da is computed and the maximum similarity value is chosen as the score for that concept. To compute the similarity between two concepts, we use the similarity measure proposed by Jiang and Conrath (1997). The authors apply a corpus-based method that works in conjunction with lexical taxonomies to calculate semantic similarities between terms, and the approach has been shown to agree w"
U12-1011,O97-1002,0,0.124092,"itriptyline and so on). The concept similarity between two concepts gives a measure of their semantic relatedness or how close two concepts are within a speciﬁc domain or ontology (Budanitsky and Hirst, 2006). In our concept coverage computation, each CUI in a receives a score of 1.0 if it has an exact match with the CUIs in Da . For each unmatched CUI in a, its concept similarity value with each unmatched concept in Da is computed and the maximum similarity value is chosen as the score for that concept. To compute the similarity between two concepts, we use the similarity measure proposed by Jiang and Conrath (1997). The authors apply a corpus-based method that works in conjunction with lexical taxonomies to calculate semantic similarities between terms, and the approach has been shown to agree well with human judgements. We use the implementation provided by McInnes et al. (2009), and scale the scores so that they fall within the range [0.0,1.0), with 0.0 indicating no match and 0.99 representing near perfect match (theoretically). The direct match score or maximum similarity score of each CUI in a are added and divided by m to give the ﬁnal concept coverage score. 4.1.3 Comparison of Coverage Scores Ou"
U12-1011,J07-1005,0,0.594251,"actice timeconsuming. Research has shown that practitioners generally spend about 2 minutes to search for evidence (Ely et al., 2000). Consequently, practitioners often fail to provide evidence-based answers to clinical queries, particularly at point of care (Ely et al., 1999; Ely et al., 2002). The research ﬁndings strongly motivate the need for endto-end medical text summarisation systems. 2.2 Summarisation for EBM As already mentioned, the task of automatic text summarisation is particularly challenging for the medical domain because of the vast amount of domain-speciﬁc knowledge required (Lin and Demner-Fushman, 2007) and the highly complex domain-speciﬁc terminologies and semantic relationships (Athenikos and Han, 2009). Text processing systems in this domain generally use the Uniﬁed Medical Language System (UMLS)2 , which is a repository of biomedical vocabularies developed by the US National Library of Medicine. It covers over 1 million biomedical concepts and terms from various vocabularies, semantic categories for the concepts and both hierRelated Work Evidence Based Medicine There is a good amount of published work on EBM practice, which is deﬁned by Sackett et al. (1996) as “the conscientious, expli"
U12-1011,N03-1020,0,0.0919442,"ce in making decisions about the care of individual patients”. The goal of EBM is to improve the quality of patient 2 80 http://www.nlm.nih.gov/research/umls/ 2.3 archical and non-hierarchical relationships among the concepts (Aronson, 2001). In the UMLS vocabulary, each medical concept is represented using a Concept Unique Identiﬁer (CUI). Related concepts are grouped into generic categories called semantic types. Our analysis heavily relies on the CUIs and semantic types of medical terms. The most important research related to automatic evaluation of summarisation systems is perhaps that by Lin and Hovy (2003) . The authors propose a set of metrics called Recall-Oriented Understudy for Gisting Evaluation (ROUGE) that have become very much the standard for automatic summary evaluation. The intent of the ROUGE measures is to ﬁnd the similarity between automatically generated summaries and reference summaries and it has been shown that ROUGE scores of summaries have a high correlation with human evaluations. We incorporate some ROUGE statistics in our analysis. ROUGE has also been used for analysis tasks in automatic text summarisation, such as the analysis of extractive summarisation provided by Ceyl"
U12-1011,W04-1013,0,0.0174424,"Missing"
U12-1011,W11-1605,0,0.0136205,"vided by Ceylan et al. (2011) . The authors use ROUGE to show that the combination of all possible extractive summaries follow a long-tailed gaussian distribution, causing most summarisation systems to achieve scores that are generally close to the mean and making it difﬁcult for systems to achieve very high scores. This analysis of extractive summaries has opened a new direction for relative comparison of summarisation systems and the approach has been used in recent work (Sarker et al., 2012). Another recent analysis work on text summarisation, similar to the one we present here, is that by Louis and Nenkova (2011), who show that human-generated summaries generally contain a large proportion of generic content along with speciﬁc content. From the perspective of our research, this means that some of the disagreement between different summarisers, in terms of content, may be attributed to dissimilar generic (stylistic) content that are not contained in the source documents, rather than dissimilar query-speciﬁc content. There has been some progress in research for EBM text summarisation (i.e., query-focused summarisation of published medical texts) in recent years. Lin and Demner-Fushman (2007) showed the"
U12-1011,U11-1012,0,0.23621,"Missing"
U13-1009,P99-1071,0,0.0619903,"be unranked (or tied). However, one could also employ a weighting scheme like TF or TF.IDF to rank the words. To help make the query more specific, we also only keep queries that are longer than 2 words. We hypothesise that any experimental context that is useful in generating a query will be repeated in the adjacent posts. The advantage to this approach is its simplicity, we do not need to employ computationally expensive methods to identify in advance the set of posts in a blog that corresponds to a single research goal. We borrow from work in multi-document summarisation (for example, see Barzilay et al. (1999)) which treats words mentioned in multiple texts (in this case, both posts) as being particularly important in capturing background information. 5 5.1 Preparing the Bibliography Gold Standards We used the three bibliographies volunteered by the users: L, R and D. The bibliographic files required preprocessing to convert them into sets of PubMed references, against which we compare our suggested references. The bibliographies were originally provided in EndNote format. Each EndNote file was converted into plain text, where each bibligraphic entry was transformed into a reference, one reference"
U13-1009,P08-1093,0,0.0800846,"Missing"
U13-1009,N09-1066,0,0.0573788,"Missing"
U13-1009,W07-1008,0,0.014103,"ata at the university for the participants who have volunteered to trial. To provide links to relevant scientific literature from the ELN entries, we instrumented Labhttp://www.labtrove.org/ 63 Trove such that, as an ELN user reads a blog entry (that he or she is entitled to read), a list of automatically detected chemical entities are presented following the main text entry. These entities are detected using the OSCAR tool for Named Entity Recognition in chemistry literature (version 4 (Jessop et al., 2011)). For a description of earlier OSCAR versions, see Corbett and Murray-Rust (2006) and Corbett et al. (2007)).3 In this deployed version, to automatically suggest scientific literature, we use the chemical named entities as queries which are sent to the PubMed Entrez Application Programming Interface (API). This API provides references from the PubMed repository of scientific literature, bibligraphic details and abstracts for references matching the query. We modified the blog display page to provide extra linked data. A screenshot of the CSIRO plugin is presented in Figure 2. Within the interface, the user can decide whether or not to view extra linked data that we have associated with the blog tex"
U13-1009,P94-1002,0,0.0409195,"y be the case that more than one post is required for this purpose, or that the simple adjacency of posts is not sufficient for capturing the context of the overaching research goals in general. If the latter, we could first segment the blog into portions, where each portion represents a linguistically coherent set of text describing laboratory tasks that correlates to some larger research goal. We could then generate a query for each segment. For this task, we might employ text segmentation approaches which use dramatic changes in vocabulary to signify a new topical segment (for example, see Hearst (1994) as the seminal work in such text segmentation approaches). This might also hopefully improve recall since retrieval would be based on segments and not blog posts. Interestingly, the evaluation results suggest that the blogs might themselves be different. For example, the suggested references for ELN Blog R consistently under-performs compared to Blog L and D. This could be because there are fewer entries in Blog R. As we are using only three blogs (limited by the number of bibliographies we were provided), our results might be heavily affected by the individual variations in the blogs. Ideall"
U13-1009,P08-2033,1,0.839646,"Missing"
U13-1009,P13-2102,0,0.0160738,"not overly successful. Recall that our hypothesis was that what was common between two adjacent posts would be important. Even when using the experimental context with other methods (M3), we only observed a slight benefit. There are a number alternative approaches to using a larger experimental context. Perhaps it might be the differences and not the similarities between the posts that are more useful as query 9 68 We thank the anonymous reviewers for this suggestion. We could also consider a looser evaluation which examines articles commonly cited by the suggested references, as is done in (Jha et al., 2013). This would allow the ability to detect older seminal articles that we may not be able to recover using generated queries if that seminal work uses vocabulary that is different to contemporary research. Appropriately handling these by counting them as matched if one or more suggested references cite them may help provide a better understanding of the performance of the system. Finally, we are now collecting user interface data with which to conduct user studies. By analysing cases where the user clicked the PubMed links based on the abstract of the suggested reference, we may be able to learn"
U16-1010,D11-1052,0,0.0196056,"l, can, may, might, could, would, should, will, must. 4.1 Stylistic Features Following related work in examining stylistic linguistic features in analysing the language of mental health discussions (for example, Kumar et al. (2015) and Coppersmith et al. (2015a)), we examine a set of features that capture the linguistics attributes associated with the style of writing, such as orthography or words that have a strong syntactic element like pronouns. Similar features have been proven successful in sentiment analysis domain (for example, Mohammad et al. (2013) look at part-of-speech features and Brody and Diakopoulos (2011) examines orthographic features). The features we explored are as follows: • Part-of-Speech (POS) features The counts for POS tags provided by the CMU Twitter NLP tool (Gimpel et al., 2011). 4.2 Social Media Features The Twitter Application Programming Interface (API)6 provides additional metadata in addition to the message content. Some of these features capture elements of the social environment of the Twitter user posting the message, such as the size of their Twitter community (through the follower • Generic Text Attributes The number of chars, tokens in the Twitter message. 6 For full doc"
U16-1010,W16-0316,0,0.0927164,"negation, certainty and quantifier and have been used by Coppersmith et al. (2014) and De Choudhury et al. (2013) to study mental health signals in Twitter. Coppersmith et al. (2015a) employ the features to characterise mental illness, such Attention Deficit/Hyperactivity Disorder (ADHD) and Seasonal Affective Disorder (SAD). These have also been applied to other data sources besides Twitter. For analyses of text on suicide ideation, Matykiewicz et al. (2009), uses LIWC to study suicide notes of suicide completers. Kumar et al. (2015) look at Reddit discussions following a celebrity suicide. Cohan et al. (2016) use the features to categorise mental health forum data in the 2016 CL Psych shared task. In addition to LIWC, other stylistic features are possible. For example, Pestian et al. (2010) examines the use of readability metrices, such as the Flesch and Kincaid readability scores. Liakata et al. (2012) describe the role of features such as grammatical subject and object, grammatical triples, and negation in detecting emotion in the i2b2 dataset. Social media metadata features have also preTable 2: Accuracy and macro-F1 scores for different variants of our baseline. treatment of social media conve"
U16-1010,W14-3207,0,0.0187529,"rk that examines different facets of text studied that help to characterise mental illness, with a particular focus on work on detecting suicide ideation. We can characterise features used as being: (i) stylistic, or (ii) social media metadata: The stylistic features for analysing suiciderelated text often uses features from the Linguistics Inquirer Word Count (LIWC) (Tausczik and Pennebaker, 2010). LIWC provides features such as articles, auxiliary verbs, conjunctions, adverbs, personal pronouns, prepositions, functional words, assent, negation, certainty and quantifier and have been used by Coppersmith et al. (2014) and De Choudhury et al. (2013) to study mental health signals in Twitter. Coppersmith et al. (2015a) employ the features to characterise mental illness, such Attention Deficit/Hyperactivity Disorder (ADHD) and Seasonal Affective Disorder (SAD). These have also been applied to other data sources besides Twitter. For analyses of text on suicide ideation, Matykiewicz et al. (2009), uses LIWC to study suicide notes of suicide completers. Kumar et al. (2015) look at Reddit discussions following a celebrity suicide. Cohan et al. (2016) use the features to categorise mental health forum data in the"
U16-1010,W15-1204,0,0.0455371,"Missing"
U16-1010,W16-0311,0,0.108994,"ared much in common, albeit for different data sets. In this paper, significant results are in bold font. We found that using a larger n-gram size did not help, decreasing the macro-F1 score to 57.7. We suspect this is due to the short nature of Twitter. Using the CMU tool provided a small improvement in macro-F1 (59.0), which we attribute to Twokenise’s more comprehensive treatment of social media text conventions. We note that character n-grams have also been explored in the literature, as a means to abstract beyond the noisy nature of social media. This has been experimented in the past by Coppersmith et al. (2016) and Malmasi et al. (2016). We focus on unigram features here to allow a straightforward comparison with the previously published results for the dataset. In the remainder of this paper, as our baseline, we use our re-implementation of the O’Dea et al. (2015) classifier, using the Twokenise tool to create unigram features. 4 Features used in Suicide-related Research 5 https://github.com/myleott/ark-twokenize-py 96 https://www.i2b2.org/NLP/Coreference/Call.php • Orthographic This feature group includes the number of all-upper-letter word, alllower-letter word, words starting with upper letter,"
U16-1010,P11-2008,0,0.0880812,"Missing"
U16-1010,Y15-1064,0,0.113625,"lassifier, using the Twokenise tool to create unigram features. 4 Features used in Suicide-related Research 5 https://github.com/myleott/ark-twokenize-py 96 https://www.i2b2.org/NLP/Coreference/Call.php • Orthographic This feature group includes the number of all-upper-letter word, alllower-letter word, words starting with upper letter, words containing continuously repeated letters and ratio of all uppercase to all lowercase words in one tweet. viously been explored in the analysis of mental health related content. For example, metadata such as the time of post has previously been studied by Huang et al. (2015) and De Choudhury et al. (2013). Interestingly, De Choudhury et al. (2013) link time of posting to an insomnia index. De Choudhury et al. (2013) also examines Twitter discussions, looking at the proportion of reply posts and the fraction of retweets as features. Related features are possible with other data sources besides Twitter. For example, Cohan et al. (2016) examine the role of discussion thread length for forum data. A more complex set of features derived from the social media platform are network-related features. Colombo et al. (2016) perform social network analysis and examine the fr"
U16-1010,W16-0314,0,0.0893586,"r different data sets. In this paper, significant results are in bold font. We found that using a larger n-gram size did not help, decreasing the macro-F1 score to 57.7. We suspect this is due to the short nature of Twitter. Using the CMU tool provided a small improvement in macro-F1 (59.0), which we attribute to Twokenise’s more comprehensive treatment of social media text conventions. We note that character n-grams have also been explored in the literature, as a means to abstract beyond the noisy nature of social media. This has been experimented in the past by Coppersmith et al. (2016) and Malmasi et al. (2016). We focus on unigram features here to allow a straightforward comparison with the previously published results for the dataset. In the remainder of this paper, as our baseline, we use our re-implementation of the O’Dea et al. (2015) classifier, using the Twokenise tool to create unigram features. 4 Features used in Suicide-related Research 5 https://github.com/myleott/ark-twokenize-py 96 https://www.i2b2.org/NLP/Coreference/Call.php • Orthographic This feature group includes the number of all-upper-letter word, alllower-letter word, words starting with upper letter, words containing continuou"
U16-1010,W09-1323,0,0.0253579,"nd Pennebaker, 2010). LIWC provides features such as articles, auxiliary verbs, conjunctions, adverbs, personal pronouns, prepositions, functional words, assent, negation, certainty and quantifier and have been used by Coppersmith et al. (2014) and De Choudhury et al. (2013) to study mental health signals in Twitter. Coppersmith et al. (2015a) employ the features to characterise mental illness, such Attention Deficit/Hyperactivity Disorder (ADHD) and Seasonal Affective Disorder (SAD). These have also been applied to other data sources besides Twitter. For analyses of text on suicide ideation, Matykiewicz et al. (2009), uses LIWC to study suicide notes of suicide completers. Kumar et al. (2015) look at Reddit discussions following a celebrity suicide. Cohan et al. (2016) use the features to categorise mental health forum data in the 2016 CL Psych shared task. In addition to LIWC, other stylistic features are possible. For example, Pestian et al. (2010) examines the use of readability metrices, such as the Flesch and Kincaid readability scores. Liakata et al. (2012) describe the role of features such as grammatical subject and object, grammatical triples, and negation in detecting emotion in the i2b2 dataset"
U16-1010,S13-2053,0,0.0232776,"s, have, has, going, gonna, was, were, did, had, gone, shall, can, may, might, could, would, should, will, must. 4.1 Stylistic Features Following related work in examining stylistic linguistic features in analysing the language of mental health discussions (for example, Kumar et al. (2015) and Coppersmith et al. (2015a)), we examine a set of features that capture the linguistics attributes associated with the style of writing, such as orthography or words that have a strong syntactic element like pronouns. Similar features have been proven successful in sentiment analysis domain (for example, Mohammad et al. (2013) look at part-of-speech features and Brody and Diakopoulos (2011) examines orthographic features). The features we explored are as follows: • Part-of-Speech (POS) features The counts for POS tags provided by the CMU Twitter NLP tool (Gimpel et al., 2011). 4.2 Social Media Features The Twitter Application Programming Interface (API)6 provides additional metadata in addition to the message content. Some of these features capture elements of the social environment of the Twitter user posting the message, such as the size of their Twitter community (through the follower • Generic Text Attributes T"
U16-1010,W16-0324,0,0.0200785,"tic content, we can explore methods that capitalise on this. Our aim here is to understand the feasibility of data acquisition approaches based on responses. In exploring the role of the text in responses for suicide ideation detection, our work is similar to the recent 2016 CL Psych shared task, where forum discussions were the main source data. As a result, many participants explored the discussion as extra text context from which to derive features. For example, Malmasi et al. (2016) used the discussion structure to look at the posts preceding and following the discussion post in question. Pink et al. (2016) look at concatenations of discussion reply chains as a source of features. We used a similar approach in this work, except that we focus on the much shorter Twitter discussions. We incorporate information about the discussion context by examining the responses to the Twitter post in question, or the “triggering post”. When using the additional context of discussion responses, the feature representation of the triggering post can be augmented with feature representations based on the text of the responses. Given the results of the preceding section, we focus on unigram features for responses."
U17-1009,I11-1142,0,0.0197958,"ets, with different entity and document types. We are also interested in comparing these methods on nested entities which CADEC and i2b2 2009 did not contain. There are other methods that we should investigate, including joint models. Incorporating medical ontologies such as SNOMED CT and MedDRA can also potentially inform a system that deal with biomedical concepts, and in particular adverse events. Other potential methods to investigate are those that incorporate syntactic and semantic parsing of the sentences as well as tree-structure of the entities into account [Finkel and Manning, 2009, Dinarelli and Rosset, 2011, 2012]. 5.2 Complex Named Entities In both the CADEC and i2b2 datasets, drug names rarely have overlapping or discontinuous properties. In contrast, it is common for ADEs to be discontinuous or overlapped. 925 out of 6318 ADEs in CADEC are overlapped, while 675 ones are discontinuous. In this section, we focus on the analysis of the effectiveness of the different methods on identifying these discontinuous or overlapped ADEs. For the first set of experiments, we separate entities based on their complexity: overlapping, discontinuous, continuous multiword, and simple (single word). We then eval"
U17-1009,W07-1009,0,0.093097,"Missing"
U17-1009,dinarelli-rosset-2012-tree-structured,0,0.0456463,"Missing"
U17-1009,D09-1015,0,0.271857,"ing other biomedical datasets, with different entity and document types. We are also interested in comparing these methods on nested entities which CADEC and i2b2 2009 did not contain. There are other methods that we should investigate, including joint models. Incorporating medical ontologies such as SNOMED CT and MedDRA can also potentially inform a system that deal with biomedical concepts, and in particular adverse events. Other potential methods to investigate are those that incorporate syntactic and semantic parsing of the sentences as well as tree-structure of the entities into account [Finkel and Manning, 2009, Dinarelli and Rosset, 2011, 2012]. 5.2 Complex Named Entities In both the CADEC and i2b2 datasets, drug names rarely have overlapping or discontinuous properties. In contrast, it is common for ADEs to be discontinuous or overlapped. 925 out of 6318 ADEs in CADEC are overlapped, while 675 ones are discontinuous. In this section, we focus on the analysis of the effectiveness of the different methods on identifying these discontinuous or overlapped ADEs. For the first set of experiments, we separate entities based on their complexity: overlapping, discontinuous, continuous multiword, and simple"
U17-1009,P05-1045,0,0.011615,"ependencies in the tokens and therefore should recognise multiword entities well. We directly apply this method to our problem with all the features that are proposed including character and word level features, to examine its effectiveness in our problem. HI- Continuation of concept, for discontinuous and overlapping spans. The label for the token exausted and depressed in the previous example are both HI-ADE. 4.2 Sequence Labelling: CRF and Bi-LSTM Model 5 We used two different sequence labelling methods: one based on conditional random fields as implemented in Stanford NER (version 3.8.0) [Finkel et al., 2005] and another based on a deep learning method implemented in NeuroNER [Dernoncourt et al., 2017a]. NeuroNER uses Bidirectional LSTM (Bi-LSTM) neural networks. It contains three layers: a character-enhanced tokenembedding layer, a label prediction layer, and a label sequence optimisation layer [Dernoncourt et al., 2017b]. BiLSTMs are known to take into account context from both left and right of a token, and they can handle sequences of variable size. Word embeddings can be provided as input to NeuroNER. To create the embedding, the documents are first tokenised using the spaCy tokeniser 2 . A"
U17-1009,D17-2017,0,0.0199244,"tly apply this method to our problem with all the features that are proposed including character and word level features, to examine its effectiveness in our problem. HI- Continuation of concept, for discontinuous and overlapping spans. The label for the token exausted and depressed in the previous example are both HI-ADE. 4.2 Sequence Labelling: CRF and Bi-LSTM Model 5 We used two different sequence labelling methods: one based on conditional random fields as implemented in Stanford NER (version 3.8.0) [Finkel et al., 2005] and another based on a deep learning method implemented in NeuroNER [Dernoncourt et al., 2017a]. NeuroNER uses Bidirectional LSTM (Bi-LSTM) neural networks. It contains three layers: a character-enhanced tokenembedding layer, a label prediction layer, and a label sequence optimisation layer [Dernoncourt et al., 2017b]. BiLSTMs are known to take into account context from both left and right of a token, and they can handle sequences of variable size. Word embeddings can be provided as input to NeuroNER. To create the embedding, the documents are first tokenised using the spaCy tokeniser 2 . A token will be taken as input of the word embedding layer, and its vector representation will be"
U17-1009,W17-2342,1,0.656275,"mplexity of the named entities. These methods are employed using a strategy called one-vs-all in which we train separate models for each entity type. For example, a model to identify drug is created yielding only two kinds of results: drug and not-a-drug. Different methods of generating word embeddings using Wikipedia, MEDLINE, same corpus, and random embeddings are investigated. In our experiments they all generate similar results given our embeddings were used in a dynamic setting. Dynamic embeddings are re-calculated during the training phase. This is in line with the findings reported in [Karimi et al., 2017]. In the experimental results, we report on random word embeddings. To evaluate, we run 10-fold cross-validation and report the average scores. Evaluation metrics used here are precision, recall, and F-score, all calculated based on the exact matches of extracted entities with the gold data. 4.3 Non-Sequence Labelling A recent work by Xu et al. [2017] propose a nonsequence labelling based on the FOFE (FixedSize Ordinally-Forgetting Encoding) representa2 https://spacy.io/ (Version 2.0, accessed 15 Nov 2017) 83 Dataset Entity Drug CADEC ADE i2b2 Drug Method Precision Recall F-Score CRF Bi-LSTM"
U17-1009,L16-1530,0,0.0245672,". readness and sweeling, 5. itching, and 6. abominal cramps Continuous, overlapping pain in knee (overlapping with pain in foot) Discontinuous, non-overlapping liver blood test mildly elevated Discontinuous, overlapping pain in foot Sentence Disorentatation, trouble brathing, extreme hot, readness and sweeling, itching, later abominal cramps. pain in knee and foot. My Liver blood test are also mildly elevated. Pain in knee and foot. Table 1: Examples of complexities in entities. Note the misspellings and irregular text in the sentences. biomedical domain where these entities are more popular [Kilicoglu et al., 2016]. In this paper, we evaluate three NER methods, one most popular and two most recent ones, for their capabilities in extracting the complex entities that exist in our noisy dataset of reports of adverse drug events. Our aim is to identify which of these methods more accurately extracts these entities, and whether the differences in complexity or type of entities guide what method to choose. 2 NER is traditionally seen as a sequence labelling task. One of the most competitive models is Conditional Random Fields (CRF). It is applied in a number of NER systems, such as Stanford NER. Finkel et al"
U17-1009,W17-2316,0,0.0203238,"Missing"
U17-1009,W10-1915,0,0.512813,"well as with cascading and joint learning models. Their method, however, is unable to identify nested entities of the same type. Finkel and Manning [2009] propose a discriminative parsingbased method for nested named entity recognition, employing CRFs as its core. work. However they do not provide any insight on extracting complex entities (if there were any) in their datasets. 2.2 Concept Extraction for Pharmacovigilance Safety signal detection for pharmacovigilance from medical literature, electronic health records and medical forums have been studied in the past decade [Kuhn et al., 2010, Leaman et al., 2010, Benton et al., 2011, Liu and Chen, 2013, Karimi et al., 2015c, Henriksson et al., 2015, Zhao et al., 2015, Pierce et al., 2017]. One of the problems in generating such signals from text is the extraction of relevant concepts, such as medications, adverse events, and patient information. Named entity recognition therefore has been studied as one of the methods to extract these relevant concepts. Nikfarjam et al. [2015] investigate ADE extraction from a medical forum, DailyStrength, and Twitter, using Conditional Random Fields (CRFs). To train the CRFs, they use word embeddings as one of the f"
U17-1009,P17-1114,0,0.160472,"Missing"
U17-1009,P15-2081,0,0.0309036,"0 ( 0.00) Discontinuous, overlapping 1 ( 0.05) 593 ( 9.38) 0 ( 0.00) Continuous, non-overlapping 1797 (99.83) 5311 (84.06) 8850 (100.00) Continuous, overlapping 1 ( 0.05) 332 ( 5.25) 0 ( 0.00) Multiword 141 ( 7.83) 4574 (72.40) 2181 (24.64) Single word 1659 (92.17) 1744 (27.60) 6669 (75.36) Table 2: Overall statistics of the number of entities and their breakdown based on their complexity in the datasets. Numbers in brackets are percentages. and depressed, two ADEs feeling exausted and feeling depressed overlap and share one common token feeling, so the label of token feeling is HB-ADE. tion [Zhang et al., 2015] which they call FOFENER. This is using a local detection approach where the left and right contexts of tokens created using FOFE are represented to a deep feedforward neural network. This method is very powerful in capturing immediate dependencies in the tokens and therefore should recognise multiword entities well. We directly apply this method to our problem with all the features that are proposed including character and word level features, to examine its effectiveness in our problem. HI- Continuation of concept, for discontinuous and overlapping spans. The label for the token exausted an"
U19-1020,W16-2917,0,0.0507377,"Missing"
U19-1020,W18-5911,1,0.884519,"Missing"
U19-1020,N13-1097,0,0.0212999,"Missing"
U19-1020,E17-1015,0,0.0146851,"MTL has been applied in different kinds of tasks. Zou et al. (2018) predict influenza counts based on search counts for different geographical regions - however, they do not use a neural architecture. The task in itself is similar to Pair 1 in our experiments. Chowdhury et al. (2018) use MTL for pharmacovigilance, where each tweet is labeled with adverse drug reaction and indication labels. This is similar to the drug usage detection task in our experiments. For this, they use bi-directional LSTM as the shared layer, in addition to other task-specific layers before and after the shared layer. Benton et al. (2017) use MTL for prediction of mental health signals. Their architecture uses multi-layer perceptrons as shared layers. Bingel and Søgaard (2017) use bi-directional LSTM as the shared layer and compares different pairs of NLP tasks. In contrast, we experiment with three alternatives of shared sentence representations. The above are classification formulations for health informatics. MTL has also been used for other tasks such as biomedical entity extraction (Crichton et al., 2017), non-textual data based on medical tests to predict disease progression (Zhou et al., 2011) and so on. We use datasets"
U19-1020,P17-1001,0,0.105214,"help each other in terms of how similar the participating datasets or tasks are. In this paper, we examine the utility of Multi-Task Learning (MTL) for several pairs of health informatics tasks that are related in different ways. MTL pertains to the class of learning algorithms that jointly train predictors for more than one task. In Natural Language Processing (NLP) research, MTL using deep learning has been used either to learn shared representations for related tasks, or to deal with limited labeled datasets (Xue et al., 2007; Zhang and Yeung, 2012; Søgaard and Goldberg, 2016; Ruder, 2017; Liu et al., 2017) for a variety of NLP problems such as sentiment analysis (Huang et al., 2013; Mishra et al., 2017). Most of this work that uses MTL presents architectures utilising multiple shared and task-specific layers. In contrast, we wish to see if the benefit comes from the simplistic notion of ‘learning these classifiers together’. Therefore, we use a basic architecture for our MTL experiments consisting of a single shared layer and single task-specific layers, and experiment with different alternatives for the shared layer. This simplicity allows us to understand the benefit of MTL in comparison with"
U19-1020,E17-2026,0,0.0758614,"lems: one problem influences the probability of output of the other. Through our experiments with simple architectures for popular tasks in health informatics, we examine the question: ‘Does multi-task learning always help?’ 2 Related Work MTL has been applied to a variety of text classification tasks (Søgaard and Goldberg, 2016; Xue et al., 2007; Ruder, 2017; Zhang and Yeung, 2012; Liu et al., 2017). The impact of task relatedness on MTL has been explored in case of statistical prediction models (Zhang and Yeung, 2012; Ben-David and Schuller, 2003). In the case of deep learning-based models, Bingel and Søgaard (2017) show how fundamental NLP tasks (such as MWE detection, POS tagging and so on) of different complexities perform when paired. (Mishra et al., 2017) use MTL for two related tasks in opinion mining: sentiment classificaton and sarcasm classification. (Wu and Huang, 2016) use MTL for personalisation of sentiment classification where global and local classifiers are jointly learned. A survey of MTL approaches using deep learning is by (Ruder, 2017). In the context of health informatics, MTL has been applied in different kinds of tasks. Zou et al. (2018) predict influenza counts based on search cou"
U19-1020,P17-1035,0,0.0666567,"we examine the utility of Multi-Task Learning (MTL) for several pairs of health informatics tasks that are related in different ways. MTL pertains to the class of learning algorithms that jointly train predictors for more than one task. In Natural Language Processing (NLP) research, MTL using deep learning has been used either to learn shared representations for related tasks, or to deal with limited labeled datasets (Xue et al., 2007; Zhang and Yeung, 2012; Søgaard and Goldberg, 2016; Ruder, 2017; Liu et al., 2017) for a variety of NLP problems such as sentiment analysis (Huang et al., 2013; Mishra et al., 2017). Most of this work that uses MTL presents architectures utilising multiple shared and task-specific layers. In contrast, we wish to see if the benefit comes from the simplistic notion of ‘learning these classifiers together’. Therefore, we use a basic architecture for our MTL experiments consisting of a single shared layer and single task-specific layers, and experiment with different alternatives for the shared layer. This simplicity allows us to understand the benefit of MTL in comparison with Single-Task Learning (STL) for different configurations of shared layers, for task pairs that are"
U19-1020,D14-1162,0,0.0821623,"has been reported in the paper or its derivative papers, to the best of our knowledge. Since these datasets have been reported in past papers, we use Tweepy3 to download the datasets of tweets using their identifiers. To implement the deep learning models, we use Keras (Chollet, 2015), with the Adam optimiser and binary crossentropy as the loss function during training, with 3 http://www.tweepy.org/; Last accessed on 3rd September, 2019. a dropout of 0.25 and number of units for intermediate layers as 25. We use word embeddings with 200 dimensions, pre-trained on a Twitter corpus using GLoVe (Pennington et al., 2014). These embeddings have been trained on 2 billion tweets with 27 billion tokens. The general outline of our experimentation is a comparison of MTL with the equivalent singletask learning (STL) version. The corresponding STL architecture is shown in Figure 2. This architecture is identical to MTL, except that it separately learns the classifiers for the two tasks. The STL version uses one dense layer to obtain the classification output after the embedding layer and a layer to capture the semantic representation (the equivalent of the shared layer in MTL). For all our experiments, we report aver"
U19-1020,P16-2038,0,0.154212,"f these datasets or classification tasks help each other in terms of how similar the participating datasets or tasks are. In this paper, we examine the utility of Multi-Task Learning (MTL) for several pairs of health informatics tasks that are related in different ways. MTL pertains to the class of learning algorithms that jointly train predictors for more than one task. In Natural Language Processing (NLP) research, MTL using deep learning has been used either to learn shared representations for related tasks, or to deal with limited labeled datasets (Xue et al., 2007; Zhang and Yeung, 2012; Søgaard and Goldberg, 2016; Ruder, 2017; Liu et al., 2017) for a variety of NLP problems such as sentiment analysis (Huang et al., 2013; Mishra et al., 2017). Most of this work that uses MTL presents architectures utilising multiple shared and task-specific layers. In contrast, we wish to see if the benefit comes from the simplistic notion of ‘learning these classifiers together’. Therefore, we use a basic architecture for our MTL experiments consisting of a single shared layer and single task-specific layers, and experiment with different alternatives for the shared layer. This simplicity allows us to understand the b"
U19-1020,W18-5904,0,0.0677063,"Missing"
W02-2117,W94-0307,0,0.07767,"Missing"
W02-2117,J97-1004,0,0.0149898,"Stede, 1992; Kosseim & Lapalme, 1994; Paris & Vander Linden, 1996; Power et al., 1998), demonstrating that existing technology is adequate for generating draft instructions. However, only a few of these projects have been formally evaluated, e.g., (Hartley et al., 2000), and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness. This tends to be the case, in fact, for evaluations of NLG systems in general. People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts - e.g., (Lester & Porter, 1997; Callaway & Lester, 2001), without measuring the actual impact of the texts on their intended users. The evaluation of the STOP system (Reiter et al., 2001) is a notable exception to this trend. STOP produced texts tailored to individual smokers intended to convince them to stop smoking. As an evaluation, the researchers performed a largescale study of how effective the generated texts were at achieving this goal. Rather than checking the output text for errors, or comparing its fluency with that of hand-written text, they compared how often readers of STOP&apos;s individually tailored texts actua"
W02-2117,J93-4004,1,0.626255,"ure 3: Hypertext output displayed in a Netscape browser - the mailing labels task When satisfied with the task model, the technical writer exports the model to the generator for the on-line help to be generated. In Isolde, the technical writer refines the input and controls the generation of instructions through Tamot. A task model example, shown in Tamot, is presented in Figure 2. This window includes a tree-structured representation of the task hierarchy on the left, and a graphical representation of the tasks on the right. The Instruction generator uses: (1) the Moore & Paris text planner (Moore & Paris, 1993); (2) a sentence planner implemented as extensions to the text planner; and (3) the KPML development environment and lexico-grammatical resources (Bateman, 1997). This system is implemented as a LISP server. It plans the instructions using discourse plans that handle any task model configuration, including sequences, compositions and Boolean connectors. It plans hypertext links and can integrate canned text with generated text. Style parameters have also been implemented, giving the technical writers some control at the discourse or sentence level. For example, writers can decide to produce a"
W02-2117,P01-1057,0,0.167936,"t instructions. However, only a few of these projects have been formally evaluated, e.g., (Hartley et al., 2000), and the evaluations performed have focussed on the fluency and grammaticality of the output text rather than on its effectiveness. This tends to be the case, in fact, for evaluations of NLG systems in general. People are asked to rate the acceptability of the generated texts or to compare them to human-authored texts - e.g., (Lester & Porter, 1997; Callaway & Lester, 2001), without measuring the actual impact of the texts on their intended users. The evaluation of the STOP system (Reiter et al., 2001) is a notable exception to this trend. STOP produced texts tailored to individual smokers intended to convince them to stop smoking. As an evaluation, the researchers performed a largescale study of how effective the generated texts were at achieving this goal. Rather than checking the output text for errors, or comparing its fluency with that of hand-written text, they compared how often readers of STOP&apos;s individually tailored texts actually stopped smoking as compared to readers of generic, generated texts. Thus, the evaluation assessed the relative effectiveness of tailored and generic text"
W03-1202,P00-1041,0,0.0570253,"vation behind discovering segments in a text is that a sentence extraction summary should choose the most representative sentence for each segment, resulting in a comprehensive summary. In the view of Gong and Liu (2001), segments form the main themes of a document. They present a theme interpretation of the SVD analysis, as it is used for discourse segmentation, upon which our use of the technique is based. However, Gong and Liu use SVD for creating sentence extraction summaries, not for generating a single sentence summary by re-using words. In subsequent work to Witbrock and Mittal (1999), Banko et al. (2000) describe the use of information about the position of words within four quarters of the source document. The headline candidacy score of a word is weighted by its position in one of quarters. We interpret this use of position information as a means of guiding the generation of a headline towards the central theme of the document, which for news articles typically occurs in the first quarter. SVD potentially offers a more general mechanism for handling the discovery of the central themes and their positions within the document. Jin et al. (2002) have also examined a statistical model for headl"
W03-1202,W97-0704,0,0.072912,"g abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction 1 Theme is a term that is used in many ways by many researchers, and generally without any kind of formal definition. Our use of the term here is akin to the notion that underlies work on text segmentation, where sentences naturally cluster in terms of their ‘aboutness’. and natural language processing. Hybrid methods for abstract-like summarisation which combine statistical and symbolic approaches have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summarisation has been explored by a number of researchers (see for example, Witbrock and Mittal, 1999; Zajic et al., 2002). We build on the approach employed by Witbrock and Mittal (1999) which we will describe in more detail in Section 3. Interestingly, in the work of Witbrock and Mittal (1999), the selection of words for inclusion in the headline is decided solely on the basis of corpus statistics and does not use statistical information about the distribution of words in the document itself. Our work differs in that we utilise an SVD analysis to provide inform"
W03-1202,J00-2011,0,0.557111,"th probabilities, directing the search towards the more probable word sequences first. The use of repeated words in the path is not permitted. 5.2 Using Singular Value Decomposition for Content Selection As an alternative to the Conditional probability, we examine the use of SVD in determining the Content Selection probability. Before we outline the procedure for basing this probability on SVD, we will first outline our interpretation of the SVD analysis, based on that of Gong and Liu (2001). Our description is not intended to be a comprehensive explanation of SVD, and we direct the reader to Manning and Schütze (2000) for a description of how SVD is used in information retrieval. Conceptually, when used to analyse documents, SVD can discover relationships between word co-occurrences in a collection of text. For example, in the context of information retrieval, this provides one way to retrieve additional documents that contain synonyms of query terms, where synonymy is defined by similarity of word co-occurrences. By discovering patterns in word co-occurrences, SVD also provides information that can be used to cluster documents based on similarity of themes. In the context of single document summarisation,"
W03-1202,J98-3005,0,0.0911491,"erated headline using SVD: “singapore shares fall” Figure 2. The headline generated using an SVDbased word selection criterion. The movement in share price is correct. 4 Related Work As the focus of this paper is on statistical singlesentence summarisation we will not focus on preceding work which generates summaries greater in length than a sentence. We direct the reader to Paice (1990) for an overview of summarisation based on sentence extraction. Examples of recent systems include Kupiec et al. (1995) and Brandow et al. (1995). For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction 1 Theme is a term that is used in many ways by many researchers, and generally without any kind of formal definition. Our use of the term here is akin to the notion that underlies work on text segmentation, where sentences naturally cluster in terms of their ‘aboutness’. and natural language processing. Hybrid methods for abstract-like summarisation which combine statistical and symbolic approaches have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summar"
W03-1202,X98-1026,0,\N,Missing
W06-1419,W02-2117,1,0.823161,"odify/update the system to the new requirements? In asking these questions, we believe it is also useful to decouple a specific system and its underlying architecture, and ask the appropriate questions to both. 3 Usability Evaluations of NLG Systems When talking about evaluation of NLG systems, we should also remember that usability evaluations are crucial, as they can confirm the usefulness of a system for its purpose and look at the impact of the generated text on its intended audience. There has been an increasing number of such evaluations – e.g., (Reiter et al., 2001, Paris et al., 2001, Colineau et al., 2002, Kushniruk et al., 2002, Elhadad et al., 2005) – and we should continue to encourage them as well as develop and share methodologies (and pitfalls) for performing these evaluations. It is interesting, in fact, to note that communities that have emphasized common task and corpus evaluations, such as the IR community, are now turning their attention to stakeholder-based evaluations such as task-based evaluations. In looking at ways to evaluate NLG systems, we might again enlarge our view beyond reader/listener-oriented usability evaluations, as readers are not the only persons potentially affec"
W06-1419,P01-1057,0,0.0146449,"ch effort and time would be required to modify/update the system to the new requirements? In asking these questions, we believe it is also useful to decouple a specific system and its underlying architecture, and ask the appropriate questions to both. 3 Usability Evaluations of NLG Systems When talking about evaluation of NLG systems, we should also remember that usability evaluations are crucial, as they can confirm the usefulness of a system for its purpose and look at the impact of the generated text on its intended audience. There has been an increasing number of such evaluations – e.g., (Reiter et al., 2001, Paris et al., 2001, Colineau et al., 2002, Kushniruk et al., 2002, Elhadad et al., 2005) – and we should continue to encourage them as well as develop and share methodologies (and pitfalls) for performing these evaluations. It is interesting, in fact, to note that communities that have emphasized common task and corpus evaluations, such as the IR community, are now turning their attention to stakeholder-based evaluations such as task-based evaluations. In looking at ways to evaluate NLG systems, we might again enlarge our view beyond reader/listener-oriented usability evaluations, as readers"
W09-3606,P08-2033,1,0.801696,"nt for the cited document: Participants indicated that this was useful in appraising the cited article. These pieces of information were commonly identified as useful in helping readers make value judgements about the cited work. This is perhaps an artifact of the biomedical domain, where research has a critical nature and concerns health and medical issues. 5.2 A Contextualised Preview To generate the contextualised preview of the cited document, the system finds the set of sentences that relate to the citation context, employing approaches for summarising documents that exploit anchor text (Wan and Paris, 2008). Following Spark Jones (1998), we specify the purpose of the contextualised summary along particular dimensions, indicated here in italics: • The situation is tied to a particular context of use: an in-browser summary triggered by a citation and its citing context. • An audience of expert researchers is assumed. • The intended usage of the summary is one of preview. We assume that the reader is making a relevance judgement as to whether or not to download (and, if necessary, buy) the cited document. Specifically, the information presented should help the reader determine the level of trust to"
W09-3606,C08-1087,0,0.0172234,"al. (1995); in particular, our work focuses on browsing scenarios. Toms (2000) presents a study of browsing behaviour over electronic texts and examines the differences between searching and The genre of academic literature, and the development of technologies to support researchers as users, has been studied by several groups working in automatic text summarisation. Teufel and Moens (2002) describe a summarisation approach that extracts text from documents and highlights the rhetorical role that an extract plays within the originating document (for example, stating the Aim of an experiment). Qazvinian and Radev (2008) present an approach to summarising academic documents based on finding citation contexts in the entire set of published literature for the document in question. Both approaches, however, treat the cited document in isolation of the read46 ing context and do not actively support the reading task. 3 their own reasons. In addition, we also asked them about the frequency of their literature browsing activity. The main section of the questionnaire consisted of a series of questions, corresponding to the issues we wanted to explore: Understanding How Researchers Browse through Scientific Literature"
W09-3606,J02-4002,0,0.0152721,"constructing lists of domain-specific key words which may correspond well to user interests. However, we are interested in relating information needs to user tasks in scenarios in which there is no explicit query, as in Bystrm et al. (1995); in particular, our work focuses on browsing scenarios. Toms (2000) presents a study of browsing behaviour over electronic texts and examines the differences between searching and The genre of academic literature, and the development of technologies to support researchers as users, has been studied by several groups working in automatic text summarisation. Teufel and Moens (2002) describe a summarisation approach that extracts text from documents and highlights the rhetorical role that an extract plays within the originating document (for example, stating the Aim of an experiment). Qazvinian and Radev (2008) present an approach to summarising academic documents based on finding citation contexts in the entire set of published literature for the document in question. Both approaches, however, treat the cited document in isolation of the read46 ing context and do not actively support the reading task. 3 their own reasons. In addition, we also asked them about the freque"
W12-3710,baccianella-etal-2010-sentiwordnet,0,0.08963,"Missing"
W12-3710,W11-0703,0,0.0233023,"ntent of posts and then applied a similar graph clustering algorithm for partitioning participants into supporting and opposing parties. By combining both text and link information, this approach was demonstrated to outperform the method proposed by Agrawal et al. (2003). Due to the nature of clustering mechanisms, the output of these methods are two user parties, in each of which users most agree or disagree with each other. However, users’ positions in the two parties do not necessarily correspond to the global position with respect to the main issue in a debate, which is our interest here. Balasubramanyan and Cohen (2011) proposed a computational method to classify sentiment polarity in blog comments and predict the polarity based on the topics discussed in a blog post. Finally, Somasundaran and Wiebe (2010) explored the utility of sentiment and arguing opinions in ideological debates and applied a support vector machine based approach for classifying stances of individual posts. In our work, we focus on classifying people’s global positions on a main issue by exploiting and aggregating local positions expressed in individual posts. 3 Our Proposed Method To infer support or opposition positions with respect to"
W12-3710,P04-1085,0,0.783873,"ational speech. Thomas et al. (2006) presented a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates. This method showed that the classification of participants’ positions can be improved by introducing the constraint that a single speaker retains the same position during one debate. Wang et al. (2011) presented a conditional random field based approach for detecting agreement/disagreement between speakers in English broadcast conversations. Galley et al. (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. These differ from our work in that the speakers are assumed to be present all the time during the conversation, and therefore, user speech models can be built, and their dependencies can be explored to facilitate agreement and disagreement classification. Our aggregation technique does, however, presuppose consistency of opinions, in a similar way to Thomas et al. (2006). There has been other related work which aims to analyse informal texts for opinion mining"
W12-3710,C10-2100,0,0.430483,"sion at all times. They may enter or exit the online discussion at any point, so it is not appropriate to use models assuming continued conversation. In addition, most discussions in online debates are essentially dialogic; participants could choose to implicitly respond to a previous post, or explicitly quote some content from an earlier post and make a response. Therefore, an assumption has to be made about what a participant’s post is in response to, particularly when an explicit quote is not present; in most cases, a post is assumed to be in response to the most recent post in the thread (Murakami and Raymond, 2010). In this paper, we address the problem of detecting users’ positions with respect to the main topic in online debates; we call this the global position of users on an issue. It is inappropriate to identify each user’s global position with respect to a main topic directly, because most expressions of opinion are made not Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 61–69, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics for the main topic but for posts in a local context. This poses a difficulty"
W12-3710,W10-0214,0,0.118216,"ch was demonstrated to outperform the method proposed by Agrawal et al. (2003). Due to the nature of clustering mechanisms, the output of these methods are two user parties, in each of which users most agree or disagree with each other. However, users’ positions in the two parties do not necessarily correspond to the global position with respect to the main issue in a debate, which is our interest here. Balasubramanyan and Cohen (2011) proposed a computational method to classify sentiment polarity in blog comments and predict the polarity based on the topics discussed in a blog post. Finally, Somasundaran and Wiebe (2010) explored the utility of sentiment and arguing opinions in ideological debates and applied a support vector machine based approach for classifying stances of individual posts. In our work, we focus on classifying people’s global positions on a main issue by exploiting and aggregating local positions expressed in individual posts. 3 Our Proposed Method To infer support or opposition positions with respect to the seed post, we propose a three-step method. First, we consider each post in its local context and build a local classifier to classify each pair of posts as agreeing with each other or n"
W12-3710,W06-1639,0,0.801132,"mers’ online discussions, companies can better understand customers’ reviews about their products or services. For government agencies, it could help gather public opinions about policies, legislation, laws, or elections. For social science, it can assist scientists to understand a breadth of social phenomena from online observations of large numbers of individuals. Despite the potentially wide range of applications, understanding participants’ positions in online debates remains a difficult task. One reason is that online conversations are very dynamic in nature. Unlike spoken conversations (Thomas et al., 2006; Wang et al., 2011), users in online debates are not guaranteed to participate in a discussion at all times. They may enter or exit the online discussion at any point, so it is not appropriate to use models assuming continued conversation. In addition, most discussions in online debates are essentially dialogic; participants could choose to implicitly respond to a previous post, or explicitly quote some content from an earlier post and make a response. Therefore, an assumption has to be made about what a participant’s post is in response to, particularly when an explicit quote is not present;"
W12-3710,P11-2065,0,0.366974,"ons, companies can better understand customers’ reviews about their products or services. For government agencies, it could help gather public opinions about policies, legislation, laws, or elections. For social science, it can assist scientists to understand a breadth of social phenomena from online observations of large numbers of individuals. Despite the potentially wide range of applications, understanding participants’ positions in online debates remains a difficult task. One reason is that online conversations are very dynamic in nature. Unlike spoken conversations (Thomas et al., 2006; Wang et al., 2011), users in online debates are not guaranteed to participate in a discussion at all times. They may enter or exit the online discussion at any point, so it is not appropriate to use models assuming continued conversation. In addition, most discussions in online debates are essentially dialogic; participants could choose to implicitly respond to a previous post, or explicitly quote some content from an earlier post and make a response. Therefore, an assumption has to be made about what a participant’s post is in response to, particularly when an explicit quote is not present; in most cases, a po"
W15-3707,W11-3712,0,0.0453234,"d it. Related Work There has been much work in using Twitter to predict the outcome of an election e.g., (O’Connor et al., 2010), as well as critiques of such approaches (Gayo-Avello et al., 2011) and explorations of sentiment for prediction (Tumasjan et al., 2010). Our work focuses on different types of media, specifically news and Twitter data. There are several investigations of media which take into account the diversity of platforms and data types. For example, some have examined the effect of different information sources on public discussion, e.g., (Scharl and Weichselbraun, 2006) and (Ahmad et al., 2011). (Declerck, 2013) mentions that it would be interesting to characterise the public discussion topics for an election. In this work, we assume that these topics are provided a priori and show how a ranking of election topics is possible. Further afield from election-focused research, (Liu et al., 2011) also utilise embedded links in Twitter but for the purposes of generating summaries of events (see also (Nichols et al., 2012) and (van Oorschot et al., 2012)). Here, we examine how our ranking of issues based on embedded links compares with that of an official commentary, rather than generating"
W15-3707,W13-2712,0,0.0192363,"re has been much work in using Twitter to predict the outcome of an election e.g., (O’Connor et al., 2010), as well as critiques of such approaches (Gayo-Avello et al., 2011) and explorations of sentiment for prediction (Tumasjan et al., 2010). Our work focuses on different types of media, specifically news and Twitter data. There are several investigations of media which take into account the diversity of platforms and data types. For example, some have examined the effect of different information sources on public discussion, e.g., (Scharl and Weichselbraun, 2006) and (Ahmad et al., 2011). (Declerck, 2013) mentions that it would be interesting to characterise the public discussion topics for an election. In this work, we assume that these topics are provided a priori and show how a ranking of election topics is possible. Further afield from election-focused research, (Liu et al., 2011) also utilise embedded links in Twitter but for the purposes of generating summaries of events (see also (Nichols et al., 2012) and (van Oorschot et al., 2012)). Here, we examine how our ranking of issues based on embedded links compares with that of an official commentary, rather than generating event summaries."
W15-3707,W11-0709,0,0.0288456,"media, specifically news and Twitter data. There are several investigations of media which take into account the diversity of platforms and data types. For example, some have examined the effect of different information sources on public discussion, e.g., (Scharl and Weichselbraun, 2006) and (Ahmad et al., 2011). (Declerck, 2013) mentions that it would be interesting to characterise the public discussion topics for an election. In this work, we assume that these topics are provided a priori and show how a ranking of election topics is possible. Further afield from election-focused research, (Liu et al., 2011) also utilise embedded links in Twitter but for the purposes of generating summaries of events (see also (Nichols et al., 2012) and (van Oorschot et al., 2012)). Here, we examine how our ranking of issues based on embedded links compares with that of an official commentary, rather than generating event summaries. 3 4 To obtain election issues, we use a number of different online commentaries about the election. These sources were: (1) news articles from prominent news companies6 7 ; (2) issues extracted from a Vote Compass8 questionnaire by the Vox Populi company; and (3) Wikipedia9 . For our"
W15-3707,W06-1706,0,0.0111568,"th a link to the tweet that referenced it. Related Work There has been much work in using Twitter to predict the outcome of an election e.g., (O’Connor et al., 2010), as well as critiques of such approaches (Gayo-Avello et al., 2011) and explorations of sentiment for prediction (Tumasjan et al., 2010). Our work focuses on different types of media, specifically news and Twitter data. There are several investigations of media which take into account the diversity of platforms and data types. For example, some have examined the effect of different information sources on public discussion, e.g., (Scharl and Weichselbraun, 2006) and (Ahmad et al., 2011). (Declerck, 2013) mentions that it would be interesting to characterise the public discussion topics for an election. In this work, we assume that these topics are provided a priori and show how a ranking of election topics is possible. Further afield from election-focused research, (Liu et al., 2011) also utilise embedded links in Twitter but for the purposes of generating summaries of events (see also (Nichols et al., 2012) and (van Oorschot et al., 2012)). Here, we examine how our ranking of issues based on embedded links compares with that of an official commentar"
W16-0313,C14-1048,0,0.0184711,"t the CLPsych 2016 Shared Task Sunghwan Mac Kim1 , Yufei Wang2∗, Stephen Wan1 and C´ecile Paris1 1 Data61, CSIRO, Sydney, Australia 2 School of Information Technology and Electrical Engineering, The University of Queensland, Brisbane, Australia Mac.Kim@csiro.au, Yufei.Wang1@uq.net.au, Stephen.Wan@csiro.au, Cecile.Paris@csiro.au Abstract representation that has been widely applied to NLP. We also investigated the use of post embeddings, which have recently attracted much attention as feature vectors for representing text (Zhou et al., 2015; Salehi et al., 2015). Here, as in other related work (Guo et al., 2014), the post embeddings are learned from the unlabelled data as features for supervised classifiers. (ii) Our exploration of text granularity focuses on classifiers for sentences as well as posts. For the sentence-level classifiers, a post is split into sentences as the basic unit of annotation using a sentence segmenter. (iii) To explore the granularity of labels indicating concern, we note that the data includes a set of 12 FG labels representing factors that assist in deciding on whether a post is concerning or not. These are in addition to 4 CG labels. This paper describes the Data61-CSIRO t"
W16-0313,N15-1099,0,0.0628885,"Missing"
W16-0313,P15-1014,0,0.0309993,"t al., 2009) to segment sentences for the sentence-level classifiers, producing 4,305 sentences from the 947 posts. 3.2 Features We used two types of feature representations for the text: TF-IDF and post embeddings. The TF-IDF feature vectors of unigrams were generated from the labelled dataset, whereas the embeddings were obtained using both labelled and unlabelled dataset using sent2vec (Le and Mikolov, 2014). We obtained the embeddings for the whole post directly instead of combining the embeddings for the individual words of the post due to the superior performance of document embeddings (Sun et al., 2015; Tang et al., 2015). In our preliminary investigations, we explored various kinds of features such as bi- and trigrams, metadata from the posts (such as the number of views of a post or the author’s affiliation with ReachOut) and orthographic features (for example, the presence of emoticons, punctuation, etc.), but we did not obtain any performance benefits with respect to intrinsic evaluations on the training data. 3.3 Text Pre-processing FG label allClear followupBye supporting underserved currentMildDistress followupOk pastDistress angryWithForumMember angryWithReachout currentAcuteDistres"
W16-0313,D15-1167,0,0.0228805,"gment sentences for the sentence-level classifiers, producing 4,305 sentences from the 947 posts. 3.2 Features We used two types of feature representations for the text: TF-IDF and post embeddings. The TF-IDF feature vectors of unigrams were generated from the labelled dataset, whereas the embeddings were obtained using both labelled and unlabelled dataset using sent2vec (Le and Mikolov, 2014). We obtained the embeddings for the whole post directly instead of combining the embeddings for the individual words of the post due to the superior performance of document embeddings (Sun et al., 2015; Tang et al., 2015). In our preliminary investigations, we explored various kinds of features such as bi- and trigrams, metadata from the posts (such as the number of views of a post or the author’s affiliation with ReachOut) and orthographic features (for example, the presence of emoticons, punctuation, etc.), but we did not obtain any performance benefits with respect to intrinsic evaluations on the training data. 3.3 Text Pre-processing FG label allClear followupBye supporting underserved currentMildDistress followupOk pastDistress angryWithForumMember angryWithReachout currentAcuteDistress followupWorse cris"
W16-0313,P15-1025,0,0.0686713,"Missing"
W16-5611,J96-1002,0,0.11294,"Missing"
W16-5611,D12-1135,0,0.0324489,"Missing"
W16-5611,P15-1169,0,0.127402,"Missing"
W16-6206,P14-2030,0,0.124155,"Missing"
W16-6206,J92-4003,0,0.584848,"lusters We can easily access a large-scale unlabelled dataset using the Twitter API, supplementing our dataset, to apply unsupervised machine learning methods to help in social role tagging. Previous work showed that word clusters derived from an unlabelled dataset can improve the performance of many NLP applications (Koo et al., 2008; Turian et al., 2010; Spitkovsky et al., 2011; Kong et al., 2014). This finding motivates us to use a similar approach to improve tagging performance for Twitter profiles. Two clustering techniques are employed to generate the cluster features: Brown clustering (Brown et al., 1992) and K-means clustering (MacQueen, 1967). The Brown clustering algorithm induces a hierarchy of words from an unannotated corpus, and it allows us to directly map words to clusters. Word embeddings induced from a neural network are often useful representations of the meaning of words, encoded as distributional vectors. Unlike Brown clustering, word embeddings do not have any form of clusters by default. K-means clustering is thus used on the resulting word vectors. Each word is mapped to the unique cluster ID to which it was assigned, and these cluster identifiers were used as features. Bit st"
W16-6206,D12-1135,0,0.0319338,"e increasingly turning to social media as a cheap and large-volume source of real-time data, supplementing “traditional” data sources such as interviews and questionnaires. For these fields, being able to examine demographic factors can be a key part of analyses. However, demographic characteristics are not always available on social media data. Consequently, there has been a growing body of work investigating methods to estimate a variety of demographic characteristics from social media data, such as gender and age on Twitter and Facebook (Mislove et al., 2011; Sap et al., 2014) and YouTube (Filippova, 2012). In this work we focus on estimating social roles, an under-explored area. In social psychology literature, Augoustinos et al. (2014) provide an overview of schemata for social roles, which includes achieved roles based on the choices of the individual (e.g., writer or artist) and ascribed roles based on the inherent traits of an individual (e.g., teenager or schoolchild). Social roles can represent a variety of categories including gender roles, family roles, occupations, and hobbyist roles. Beller et al. (2014) have explored a set of social roles (e.g., occupation-related and familyrelated"
W16-6206,P08-1109,0,0.0453298,"Missing"
W16-6206,D14-1108,0,0.0599271,"Missing"
W16-6206,P08-1068,0,0.128946,"Missing"
W16-6206,W03-0430,0,0.130537,"lly, we employ a first-order linear chain CRF, in which the preceding word (and its features) is incorporated as context in the labelling task. In this task, each word is tagged with one of two labels: social roles are tagged with R (for “role”), whereas the other words are tagged by O (for “other”). The social role tagger uses two categories of features: (i) basic lexical features and (ii) word cluster features. The first category captures lexical cues that may be indicative of a social role. These features include morphological, syntactic, orthographic and regular expression-based features (McCallum and Li, 2003; Finkel et al., 2008). The second captures semantic similarities, as illustrated in Tables 4 and 5 (Section 3). To use Brown clusters in CRFs, we use eight bit string representations of different lengths to create features representing the ancestor clusters of the word. For word2vec clusters, the cluster identifiers are used as features in CRFs. If a word is Model KWS CRFs Feature Basic + Brown + W2V + (Brown+W2V) Precision 0.659 0.830 0.859 0.837 0.863 Recall 0.759 0.648 0.708 0.660 0.712 F1 0.690 0.725 0.774 0.736 0.779 Table 6: 10-fold cross-validation macro-average results on the annotate"
W16-6206,N13-1090,0,0.0378958,"d tokenised using whitespace and punctuation as delimiters. To obtain the Brown clusters, we use a publicly available toolkit, wcluster5 to generate 1,000 clusters with the minimum occurrence of 40, yielding 47,167 word types. The clusters are hierarchically structured as a binary tree. Each word belongs to one cluster, and the path from the word to the root of the tree can be represented as a bit string. These can be truncated to refer to clusters higher up in the tree. To obtain word embeddings, we used the skipgram model as implemented in word2vec6 , a neural network toolkit introduced by (Mikolov et al., 2013), to generate a 300-dimension word vector based on a 10-word context window size. We then used K-means clustering on the resulting 47,167 word vectors (k=1,000). Each word was mapped to the unique cluster ID to which it was assigned. Tables 4 and 5 show some examples of Brown clusters and word2vec clusters respectively, for three social roles: writer, teacher and musician. We note that similar types of social roles are grouped into the same clusters in both methods. For instance, orchestrator and saxophonist are in the same cluster containing musician. Both clusters are able to capture 5 https"
W16-6206,D14-1121,0,0.029877,"cientists and media analysts are increasingly turning to social media as a cheap and large-volume source of real-time data, supplementing “traditional” data sources such as interviews and questionnaires. For these fields, being able to examine demographic factors can be a key part of analyses. However, demographic characteristics are not always available on social media data. Consequently, there has been a growing body of work investigating methods to estimate a variety of demographic characteristics from social media data, such as gender and age on Twitter and Facebook (Mislove et al., 2011; Sap et al., 2014) and YouTube (Filippova, 2012). In this work we focus on estimating social roles, an under-explored area. In social psychology literature, Augoustinos et al. (2014) provide an overview of schemata for social roles, which includes achieved roles based on the choices of the individual (e.g., writer or artist) and ascribed roles based on the inherent traits of an individual (e.g., teenager or schoolchild). Social roles can represent a variety of categories including gender roles, family roles, occupations, and hobbyist roles. Beller et al. (2014) have explored a set of social roles (e.g., occupat"
W16-6206,D11-1118,0,0.0608279,"Missing"
W16-6206,P10-1040,0,0.138769,"Missing"
W18-5911,D14-1162,0,0.0816606,"ilarity: For each word, we obtain similarity with ‘receive, ‘get’ and ‘take’, and use the highest similarity as this feature. We use pre-trained embeddings from Mikolov et al. (2013). This is to allow presence of words related to the act of receiving to be used as a signal for prediction; 1. Sentence Vector: 200 dimensions; Logistic Regression. (SV) 2. Dense Neural Network: 64 dimensions, 1 inter. layer + 5 epochs (DNN) 3. BiLSTM: GloVe840B + 3 epochs + 50 lstm units + 0.25 dropout (BiLSTM) 6. Sentence Vector: A sentence vector is computed as an average of word vectors using GloVe embeddings (Pennington et al., 2014); 4. CNN: GloVe840B + 5 epochs + 50 filters + 2 filter length + 0.75 dropout (CNN) 5. LSTM-LM: We pre-train a language model on the training dataset with a 3-layer LSTM. We then build a softmax layer on top of this pretrained LSTM, and fine-tune the neural network with supervision (Howard and Ruder, 2018). 7. Length: Number of characters and words; 8. Emotion: Word counts of each emotion category as given by SenticNet (Cambria et al., 2014). The combination of classifiers, misclassification costs and features has been experimentally validated. All models are implemented using TensorFlow (Abadi"
W18-5911,W17-5801,0,0.0297903,"shi1 Xiang Dai1,2 Sarvnaz Karimi1 Ross Sparks1 C´ecile Paris1 C Raina MacIntyre3 1 CSIRO Data61, Sydney, Australia, 2 University of Sydney, Sydney, Australia 3 The University of New South Wales, Sydney, Australia {aditya.joshi,dai.dai,sarvnaz.karimi}@csiro.au {ross.sparks,cecile.paris}@csiro.au, r.macintyre@unsw.edu.au Abstract cine. On the contrary, ‘Vaccines drastically reduce risks of infection’ is negative because the tweet describes vaccines but does not report a vaccine being administered. Past work in vaccination behaviour detection uses n-grams as features of a statistical classifier (Skeppstedt et al., 2017; Huang et al., 2017). However, alternatives to n-grams have shown promise in several Natural Language Processing (NLP) tasks. Therefore, we compare three typical NLP approaches for vaccination behaviour detection: rule-based, statistical and deep learning techniques. Our submissions to the shared task use statistical and deep learning-based text classification. The systems are trained on a concatenation of the training and the validation set. The work reported in this paper ranked first among nine teams, as communicated by the shared task committee. Vaccination behaviour detection deals with"
W18-5911,P04-3031,0,0.0352858,"ount Table 1: Features of the statistical approach. 2.2 support vector machine are summarised in Table 1. These features are: Rule-based Approach Since vaccination behaviour detection may appear to be only about detecting administration of a vaccine, we implement a na¨ıve method to detect vaccination behaviour. Our rule-based approach looks for words indicating ‘receive’ (without negation) to predict vaccination behaviour as follows: 1. Uni/Bigrams: Boolean; 2. Special Characters: A feature each indicating four special characters ?, #, @, ! 3. POS: Count of each POS tag using NLTK POS tagger (Bird and Loper, 2004). This feature follows the intuition that presence of certain POS tags such as verbs may serve as signals; 1. If a tweet contains one among the words ‘give’, ‘take’, ‘taking’, ‘gave’, ‘giving’, ‘get’, ‘getting’, ‘took’, ‘receive’ or ‘received’ and no negation word, predict the tweet as positive. 4. Negation: Presence of a negation word. This is to serve as a negation feature where, although the act of receiving a vaccine is mentioned, the negation word changes the output class; 2. Else, predict the tweet as negative. 2.3 Deep Learning-based Approach We experiment with five typical deep learnin"
W18-5911,P18-1031,0,0.02039,"r: 200 dimensions; Logistic Regression. (SV) 2. Dense Neural Network: 64 dimensions, 1 inter. layer + 5 epochs (DNN) 3. BiLSTM: GloVe840B + 3 epochs + 50 lstm units + 0.25 dropout (BiLSTM) 6. Sentence Vector: A sentence vector is computed as an average of word vectors using GloVe embeddings (Pennington et al., 2014); 4. CNN: GloVe840B + 5 epochs + 50 filters + 2 filter length + 0.75 dropout (CNN) 5. LSTM-LM: We pre-train a language model on the training dataset with a 3-layer LSTM. We then build a softmax layer on top of this pretrained LSTM, and fine-tune the neural network with supervision (Howard and Ruder, 2018). 7. Length: Number of characters and words; 8. Emotion: Word counts of each emotion category as given by SenticNet (Cambria et al., 2014). The combination of classifiers, misclassification costs and features has been experimentally validated. All models are implemented using TensorFlow (Abadi et al., 2016). The parameters are experimentally determined. 44 Approach F-Score Accuracy Approach F-Score Accuracy Skeppstedt et al. (2017) Huang et al. (2017) 76.84 77.64 87.01 87.65 Statistical LSTM-LM 86.06 88.74 85.71 89.44 Statistical Rule-based SV DNN BiLSTM CNN LSTM-LM 80.75 40.48 77.87 78.74 78."
W18-5911,P11-1015,0,0.021916,"Missing"
W19-5015,C18-1139,0,0.0246372,"attention mechanism to incorporate information about the order and the collection of words (Vaswani et al., 2017). The pre-trained model of USE that returns a vector of 512 dimensions is also available on Tensorflow Hub; (C) Neural-Net Language Model (NNLM) by Bengio et al. (2003): The model simultaneously learns representations of words and probability functions for word sequences, allowing it to capture semantics of a sentence. We use a pre-trained model available on Tensorflow Hub, that is trained on the English Google News 200B corpus, and computes a vector of 128 dimensions; (D) FLAIR by Akbik et al. (2018): This library by Zalando research3 uses character-level language models to learn contextualised representations. We use the pooling option to create sentence vectors. This is a concatenation of GloVe embeddings and the forward/backward language model. The resultant is a vector of 4196 dimensions. Table 1 refers to the four configurations as ELMo, USE, NNLM and FLAIR respectively. Table 2: Dataset statistics. pus from an unrelated domain (news and general, in the case of Word2Vec and GloVe respectively), they may not capture the semantics of the domain of the specific classification problem. T"
W19-5015,D11-1145,0,0.0126458,"ns used in our experiments. natives that have been used for several text classification problems in NLP: word-based representations and context-based representations. They are summarised in Table 1, and described in the following subsections. and Priego, 2017) and sentence similarity detection (Fu et al., 2016). In terms of the biomedical domain, word embedding-based features have been used for entity extraction in biomedical corpora (Yadav et al., 2017) or clinical information extraction (Kholghi et al., 2016). Several approaches for personal health mention classification have been reported (Aramaki et al., 2011; Lamb et al., 2013a; Yin et al., 2015). Aramaki et al. (2011) use bag-of-words as features for personal health mention classification. Lamb et al. (2013a) use linguistic features including coarse topic-based features, while Yin et al. (2015) use features based on parts-of-speech and dependencies for a statistical classifier. Feng et al. (2018) compare statistical classifiers with deep learning-based classifiers for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et a"
W19-5015,S16-1098,0,0.0354512,"Missing"
W19-5015,K15-1027,0,0.0306613,"Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al., 2015), sarcasm detection (Joshi et al., 2016), sentiment analysis (Zhang et al., 2015; Tkachenko et al., 2018), co-reference resolution (Simova and Uszkoreit, 2017), grammatical error correction (Chou et al., 2016), emotion intensity determination (Buscaldi 1 Content words refers to all words except stop words. 135 Proceedings of the BioNLP 2019 workshop, pages 135–141 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Word-based Context-based Representation Details A tweet vector is the average of the vectors of the content words in the tweet. Word2Vec PreTrain, Vect"
W19-5015,W17-5236,0,0.0297775,"ov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al., 2015), sarcasm detection (Joshi et al., 201"
W19-5015,W16-2917,0,0.0246815,"100) 0.070) 0.116) 0.7814 (σ: 0.02) 0.8155 (σ: 0.030) 0.7495 (σ: 0.020) 0.7896 (σ: 0.031) (B) Context-based Representations ELMo USE NNLM FLAIR 1024 512 128 4196 0.8010 (σ: 0.8164 (σ: 0.8520 (σ: 0.8000 (σ: 0.021) 0.008) 0.006) 0.021) 0.7724 (σ: 0.7790 (σ: 0.7610 (σ: 0.7667 (σ: Table 3: Comparison of five word-based representations with four context-based representations; Average accuracy with standard deviation (σ) indicated in brackets. 5 detect whether or not a tweet describes the usage of a medicinal drug (‘I took some painkillers this morning’, for example). We use the dataset provided by Jiang et al. (2016); (C) Personal Health Mention classification (PHMC): A personal health mention is a person’s report about their illness. We use the dataset provided by Robinson et al. (2015). For example ‘I have been sick for a week now’ is a personal health mention while ‘Rollercoasters can make you sick’ is not. It must be noted that IIC involves influenza while the PHMC dataset covers a set of illnesses as described later. We first present a quantitative evaluation to compare the two types of representations. Following that, we analyse sources of errors. 5.1 Quantitative Evaluation We compare word-based an"
W19-5015,D16-1104,1,0.84456,"mbeddings’) are dense, real-valued vectors that capture semantics of concepts (Mikolov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems:"
W19-5015,D18-2029,0,0.0160858,"IIC DUC PHMC 9,006 (2,306) 13,409 (3,167) 2,661 (1,304) words in the sentence, they compute a vector for sentences on the whole, by taking into account the order of words and the set of co-occurring words. We experiment with four deep contextualised vectors: (A) Embeddings from Language Models (ELMo) by Peters et al. (2018): ELMo uses character-based word representations and bidirectional LSTMs. The pre-trained model computes a contextualised vector of 1024 dimensions. ELMo is available in the Tensorflow Hub2 , a repository of machine learning modules; (B) Universal Sentence Encoder (USE) by Cer et al. (2018): The encoder uses a Transformer architecture that uses attention mechanism to incorporate information about the order and the collection of words (Vaswani et al., 2017). The pre-trained model of USE that returns a vector of 512 dimensions is also available on Tensorflow Hub; (C) Neural-Net Language Model (NNLM) by Bengio et al. (2003): The model simultaneously learns representations of words and probability functions for word sequences, allowing it to capture semantics of a sentence. We use a pre-trained model available on Tensorflow Hub, that is trained on the English Google News 200B corpus"
W19-5015,W16-4910,0,0.0292297,"e, real-valued vectors that capture semantics of concepts (Mikolov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation"
W19-5015,U16-1003,0,0.0689249,"Missing"
W19-5015,Y15-1013,0,0.021879,"s for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word embedding clusters as features for adverse drug reaction detection. 3 3.1 Word-based Representations A word-based representation of a tweet combines word embeddings of the content words in the tweet. We use the average of the word embeddings of content words in the tweet. Average of word embeddings have been used for different NLP tasks (De Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has been pre-trained on a corpus of tweet"
W19-5015,C18-1152,0,0.01679,"mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word embedding clusters as features for adverse drug reaction detection. 3 3.1 Word-based Representations A word-based representation of a tweet combines word embeddings of the content words in the tweet. We use the average of the word embeddings of content words in the tweet. Average of word embeddings have been used for different NLP tasks (De Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has been pre-trained on a corpus of tweets with 27 billion tokens"
W19-5015,N13-1097,0,0.122449,"ents. natives that have been used for several text classification problems in NLP: word-based representations and context-based representations. They are summarised in Table 1, and described in the following subsections. and Priego, 2017) and sentence similarity detection (Fu et al., 2016). In terms of the biomedical domain, word embedding-based features have been used for entity extraction in biomedical corpora (Yadav et al., 2017) or clinical information extraction (Kholghi et al., 2016). Several approaches for personal health mention classification have been reported (Aramaki et al., 2011; Lamb et al., 2013a; Yin et al., 2015). Aramaki et al. (2011) use bag-of-words as features for personal health mention classification. Lamb et al. (2013a) use linguistic features including coarse topic-based features, while Yin et al. (2015) use features based on parts-of-speech and dependencies for a statistical classifier. Feng et al. (2018) compare statistical classifiers with deep learning-based classifiers for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word"
W19-5015,N15-1184,0,0.0722086,"Missing"
W19-5015,E17-1109,0,0.124235,"ddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al., 2015), sarcasm detection (Joshi et al., 2016), sentiment analysis (Zhang et al., 2015; Tkac"
W19-5015,W18-4002,0,0.0190136,"th deep learning-based classifiers for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word embedding clusters as features for adverse drug reaction detection. 3 3.1 Word-based Representations A word-based representation of a tweet combines word embeddings of the content words in the tweet. We use the average of the word embeddings of content words in the tweet. Average of word embeddings have been used for different NLP tasks (De Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has b"
W19-5015,W18-4414,0,0.0190502,"sed classifiers for personal health mention detection. In terms of detecting drug-related content in text, there has been work on detecting adverse drug reactions (Karimi et al., 2015). Nikfarjam et al. (2015) use word embedding clusters as features for adverse drug reaction detection. 3 3.1 Word-based Representations A word-based representation of a tweet combines word embeddings of the content words in the tweet. We use the average of the word embeddings of content words in the tweet. Average of word embeddings have been used for different NLP tasks (De Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has been pre-traine"
W19-5015,S15-2094,0,0.030168,"ns (also known as ‘embeddings’) are dense, real-valued vectors that capture semantics of concepts (Mikolov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used fo"
W19-5015,D14-1162,0,0.0829765,"Boom et al., 2016; Yoon et al., 2018; Orasan, 2018; Komatsu et al., 2015; Ettinger et al., 2018). As in past work, words that were not learned in the embeddings are dropped during the computation of the tweet vector. We experiment with three kinds of word embeddings: 1. Pre-trained Embeddings: Denoted as Word2Vec PreTrained and GloVe PreTrained in Table 1, we use pre-trained embeddings of words learned from large text corpora: (A) Word2Vec by Mikolov et al. (2013): This has been pre-trained on a corpus of news articles with 300 million tokens, resulting in 300dimensional vectors; (B) GloVe by Pennington et al. (2014): This has been pre-trained on a corpus of tweets with 27 billion tokens, resulting in 200-dimensional vectors. Representations A tweet vector is a distributed representation of a tweet, and is computed for every tweet in the training set. The tweet vector along with the output label is then used to train the statistical classification model. The intuition is that the tweet vector captures the semantics of the tweet and, as a result, can be effectively used for classification. To obtain tweet vectors, we experiment with two alter2. Embeddings Trained on The Training Split: It may be argued tha"
W19-5015,N18-1202,0,0.0398235,"of the tweet and, as a result, can be effectively used for classification. To obtain tweet vectors, we experiment with two alter2. Embeddings Trained on The Training Split: It may be argued that, since the pretrained embeddings are learned from a cor136 Classification # tweets (# true tweets) IIC DUC PHMC 9,006 (2,306) 13,409 (3,167) 2,661 (1,304) words in the sentence, they compute a vector for sentences on the whole, by taking into account the order of words and the set of co-occurring words. We experiment with four deep contextualised vectors: (A) Embeddings from Language Models (ELMo) by Peters et al. (2018): ELMo uses character-based word representations and bidirectional LSTMs. The pre-trained model computes a contextualised vector of 1024 dimensions. ELMo is available in the Tensorflow Hub2 , a repository of machine learning modules; (B) Universal Sentence Encoder (USE) by Cer et al. (2018): The encoder uses a Transformer architecture that uses attention mechanism to incorporate information about the order and the collection of words (Vaswani et al., 2017). The pre-trained model of USE that returns a vector of 512 dimensions is also available on Tensorflow Hub; (C) Neural-Net Language Model (N"
W19-5015,simova-uszkoreit-2017-word,0,0.0311253,"ors that capture semantics of concepts (Mikolov et al., 2013). When learned from a large corpus, embeddings of related words are expected to be closer than those of unrelated words. When a statistical classifier is trained, distributed representations of textual units (such as sentences or documents) in the training set can be used as feature representations of the textual unit. This technique of statistical classification that uses embeddings as features has been shown to be useful for many Natural Language Processing (NLP) problems (Zhang et al., 2015; Joshi et al., 2016; Chou et al., 2016; Simova and Uszkoreit, 2017; Fu et al., 2016; Buscaldi and Priego, 2017) and biomedical NLP problems (Yadav et al., 2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al."
W19-5015,P18-1112,0,0.0143197,"2017; Kholghi et al., 2016). In this paper, we experiment with three classification problems in health informatics: influenza infection classification, drug usage classification and For these classification problems, we compare five approaches that use word-based representations with four approaches that use context-based representations. 2 Related Work Distributed representations as features for statistical classification have been used for many NLP problems: semantic relation extraction (Hashimoto et al., 2015), sarcasm detection (Joshi et al., 2016), sentiment analysis (Zhang et al., 2015; Tkachenko et al., 2018), co-reference resolution (Simova and Uszkoreit, 2017), grammatical error correction (Chou et al., 2016), emotion intensity determination (Buscaldi 1 Content words refers to all words except stop words. 135 Proceedings of the BioNLP 2019 workshop, pages 135–141 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Word-based Context-based Representation Details A tweet vector is the average of the vectors of the content words in the tweet. Word2Vec PreTrain, Vectors of the content words are obtained from preGloVe PreTrain trained embeddings from Word2Vec & GloVe res"
W93-0224,J92-4007,0,0.0274761,"y it is used tbr. The constraints they represent are not well defined, may even differ depending on tile surrounding context, and may encompass several different aspects of the discourse. • As already pointed out, these relations represent difl~rent types of constraints. This has also been discussed by other researchers [ 1-3, 5-8]. Conllating all these faclors by using one term ""rhetorical relations"" leads to contusion. • Few theories allow Ik~rseveral relations to hold at the same time. ~ Yet. semantic constraints, relations between communicative goals and textual relations usually co-occur [2, 7]. In fact, not only do they co-occur between two units of text, but they may also give rise to different non-isomorphic structures. However, all these relations (or constraints) contribute to generating an appropriate text and all should be represented and taken into account. Furthermore, several constraints of the same type might be present. • To do an analysis in terms of rhetorical relations such as RST, one is often implicitly also doing an intentional analysis. For example, consider the sentence: (1) if you cook tonight, (2) I'll take you out to the movies. I Notable exceptions ale theori"
W94-0306,J86-2003,0,0.0181891,"Missing"
W94-0308,W93-0203,1,0.701901,"Missing"
W94-0308,P92-1016,0,0.0503603,"Missing"
W96-0504,J93-4004,1,0.835617,"Missing"
W96-0504,C96-2124,1,0.818333,"Missing"
