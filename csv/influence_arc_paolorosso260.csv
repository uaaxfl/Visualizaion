2016.gwc-1.47,C10-3010,0,0.068065,"Missing"
2016.gwc-1.47,2006.bcs-1.2,0,0.0845691,"that each can substitute others in a sentence without changing its general meaning. Therefore, in Natural Language Processing (NLP), various applications used this information, especially Query Expansion (QE), Information Retrieval (IR) and Question Answering (QA) systems. Thus, the development of new WordNets targeting new languages and dialects and/or enriching existing ones, witnessed regular experiences and research works. Arabic, as a Semitic language spoken by around 300 million people worldwide, is concerned by these experiences as well as the use of Arabic WordNet (AWN) (Felbaum 1998; Elkateb et al., 2006) in recent NLP applications such as information retrieval (Abbache et al., 2014) Elearning of Arabic (Karkar et al., 2015), semantic-based applications (Bouhriz et al., 2015), conceptual search (Al-Zoghby and Shaalan 2015), etc. After the first release of AWN, There were many attempts to enrich its content (Al khalifa and Rodriguez 2009; Rodriguez et al., 2008a; 2008b). Nevertheless, the gap between AWN and the Arabic language as well as other similar WNs remained one of the limitations for its use. Also, some particularities of the Arabic language, including Broken Plurals (BP), has not been"
2016.gwc-1.47,elkateb-etal-2006-building,0,0.471986,"that each can substitute others in a sentence without changing its general meaning. Therefore, in Natural Language Processing (NLP), various applications used this information, especially Query Expansion (QE), Information Retrieval (IR) and Question Answering (QA) systems. Thus, the development of new WordNets targeting new languages and dialects and/or enriching existing ones, witnessed regular experiences and research works. Arabic, as a Semitic language spoken by around 300 million people worldwide, is concerned by these experiences as well as the use of Arabic WordNet (AWN) (Felbaum 1998; Elkateb et al., 2006) in recent NLP applications such as information retrieval (Abbache et al., 2014) Elearning of Arabic (Karkar et al., 2015), semantic-based applications (Bouhriz et al., 2015), conceptual search (Al-Zoghby and Shaalan 2015), etc. After the first release of AWN, There were many attempts to enrich its content (Al khalifa and Rodriguez 2009; Rodriguez et al., 2008a; 2008b). Nevertheless, the gap between AWN and the Arabic language as well as other similar WNs remained one of the limitations for its use. Also, some particularities of the Arabic language, including Broken Plurals (BP), has not been"
2016.gwc-1.47,W11-0142,0,0.0605877,"Missing"
2016.gwc-1.47,rodriguez-etal-2008-arabic,0,0.0993837,"hing existing ones, witnessed regular experiences and research works. Arabic, as a Semitic language spoken by around 300 million people worldwide, is concerned by these experiences as well as the use of Arabic WordNet (AWN) (Felbaum 1998; Elkateb et al., 2006) in recent NLP applications such as information retrieval (Abbache et al., 2014) Elearning of Arabic (Karkar et al., 2015), semantic-based applications (Bouhriz et al., 2015), conceptual search (Al-Zoghby and Shaalan 2015), etc. After the first release of AWN, There were many attempts to enrich its content (Al khalifa and Rodriguez 2009; Rodriguez et al., 2008a; 2008b). Nevertheless, the gap between AWN and the Arabic language as well as other similar WNs remained one of the limitations for its use. Also, some particularities of the Arabic language, including Broken Plurals (BP), has not been sufficiently addressed. Indeed, the morphological analysis of BP is not an easy task in NLP since they are irregular forms of plurals, and cannot be identified using patterns. Making these BP forms in a resource such as AWN is much helpful for the developers of NLP applications. In previous research (Abouenour et al., 2013), we presented a new content that has"
2020.coling-main.116,W19-7723,1,0.837527,"el, it’s just the law of the strongest Italian Nuovo governo monti: dal bunga bunga al banca banca... New Monti’s Governmet: from bunga bunga to bank bank... REUTERS ANALISI Crisi, di corsa verso il governo Monti REUTERS ANALYSIS Crisis, running towards Monti’s Government not ironic not ironic not ironic not Table 2: Examples from the datasets. Provided that the availability of morphological and syntactic knowledge is crucial for performing the experiments described in the rest of the paper, we needed to obtain a representation of all the datasets in UD format. With the exception of TWITTIRÒ (Cignarella et al., 2019), a subset of the IronITA dataset already available as a UD treebank (see the current release in the UD official repository3 ), we obtained the dependency-based annotation for the other corpora by applying the UDPipe4 pipeline (for tokenization, PoS-tagging and parsing). Figure 1 provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained"
2020.coling-main.116,S19-2197,1,0.824824,"and their interaction in several NLP tasks, showing their effectiveness. For example, Sidorov et al. (2012) exploited syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information specifically for irony detection. Among them we cite Cignarella and Bosco (2019), who employed a SVC combined with shallow features based on morphology and dependency syntax outperforming strong baselines in the IroSvA 2019 irony detection shared task in Spanish variants (Ortega-Bueno et al., 2019). This paper aims to go one step further focusing for the first time on the development of syntax-aware irony d"
2020.coling-main.116,S15-2080,1,0.750861,"e directions for future work. 2 Related work The identification of irony and the description of pragmatic and linguistic devices that trigger it has always been a controversial topic (Grice, 1975; Sperber and Wilson, 1981; Utsumi, 1996), a challenge for both humans and automatic tools. This motivates the interest for irony of the NLP community, the organization of tasks regarding its processing and the subsequent creation of benchmarks. In the last decade several shared tasks have been organized in order to advance with the techniques regarding the automatic processing of figurative language (Ghosh et al., 2015), and more recently irony and sarcasm detection have been gaining greater attention. Being irony a pragmatic device inherently related to culture and language, the task captured the attention of various researchers, dealing with different languages and mostly focusing on social media texts. The presence of irony for Italian has been investigated -among other things- in the SENTIPOLC task of the EVALITA 2014 (Basile et al., 2014) and 2016 (Barbieri et al., 2016) and at IronITA (Cignarella et al., 2018). It was addressed for French within the context of DEFT 2017 (Benamara et al., 2017), for Eng"
2020.coling-main.116,P15-2124,0,0.0315116,"of the phenomenon itself, that may assume a large variety of merged forms and facets. Several semantic and syntactic devices can be used to express irony (e.g., analogy, oxymoron, paradox, euphemism and rhetorical questions), and a variety of different linguistic elements can cause the incongruity, determine the clash and play the role of irony triggers within a text. Most computational research in irony detection is applied on social media texts and focuses primarily on content-based processing of the linguistic information using semantic or pragmatic devices, neglecting syntax. For example, Joshi et al. (2015) propose a method for sarcasm detection in tweets as the task of detecting incongruity between an observed and an expected word, while Karoui et al. (2015) rely on external common knowledge to the utterance shared by author and reader. However, in addition to these pragmatic devices, some hints about the role of syntax can be found in linguistic literature where the deviation from syntactic norms has been reported as a possible trigger of the phenomenon. For instance, Michaelis and Feng (2015) explore the usage of split interrogatives and of topic-comment utterances in English, arguing they bo"
2020.coling-main.116,2020.lrec-1.500,0,0.0286436,"ns occurring in texts has been moreover confirmed by Mahler et al. (2017), where artificial syntactic manipulations are applied, such as those based on negations and adverbs, with the purpose of fooling sentiment analysis systems. Some research already explored different kinds of syntactic features and their interaction in several NLP tasks, showing their effectiveness. For example, Sidorov et al. (2012) exploited syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information specifically for irony detection. Among them we cite Cignarella and Bosco (2019), who employed a SVC combined with sh"
2020.coling-main.116,P15-2106,0,0.0190749,"(e.g., analogy, oxymoron, paradox, euphemism and rhetorical questions), and a variety of different linguistic elements can cause the incongruity, determine the clash and play the role of irony triggers within a text. Most computational research in irony detection is applied on social media texts and focuses primarily on content-based processing of the linguistic information using semantic or pragmatic devices, neglecting syntax. For example, Joshi et al. (2015) propose a method for sarcasm detection in tweets as the task of detecting incongruity between an observed and an expected word, while Karoui et al. (2015) rely on external common knowledge to the utterance shared by author and reader. However, in addition to these pragmatic devices, some hints about the role of syntax can be found in linguistic literature where the deviation from syntactic norms has been reported as a possible trigger of the phenomenon. For instance, Michaelis and Feng (2015) explore the usage of split interrogatives and of topic-comment utterances in English, arguing they both have a double function (they express two distinct communicative acts) and that one of those is precisely that of conveying an ironic meaning (e.g. “Who"
2020.coling-main.116,P14-2050,0,0.356056,"forward binary classification task on irony detection, that is, the task for which literature offers baselines and fair-sized annotated datasets for a variety of languages. For addressing the task, we performed a set of experiments where several models were implemented exploiting classical machine learning algorithms, deep learning architectures and state-of-the-art language models implemented with the Python libraries scikit-learn6 and keras7 . We tested different sets of pre-trained word embeddings to initialize the neural models, namely fastText8 and a dependency-based word2vec proposed by Levy and Goldberg (2014) (word2vecf ). The latter was trained on the concatenation of all the treebanks available in the UD repository for each considered language. In order to combine these methods with syntactic features inspired by Sidorov et al. (2014), we used data where not only a binary annotation for irony is applied, but also a morphological and syntactic analysis is available (see § 4). 4.1 Pre-processing and Features We stripped all the URLs and we normalized all characters to lowercase letters, as it is often done before the application of sentiment analysis tools. We investigated the use of novel feature"
2020.coling-main.116,C18-1159,0,0.0498333,"syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information specifically for irony detection. Among them we cite Cignarella and Bosco (2019), who employed a SVC combined with shallow features based on morphology and dependency syntax outperforming strong baselines in the IroSvA 2019 irony detection shared task in Spanish variants (Ortega-Bueno et al., 2019). This paper aims to go one step further focusing for the first time on the development of syntax-aware irony detection systems in a multilingual perspective (English, Spanish, French and Italian), providing an in-depth inve"
2020.coling-main.116,N18-1088,0,0.0576339,"syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information specifically for irony detection. Among them we cite Cignarella and Bosco (2019), who employed a SVC combined with shallow features based on morphology and dependency syntax outperforming strong baselines in the IroSvA 2019 irony detection shared task in Spanish variants (Ortega-Bueno et al., 2019). This paper aims to go one step further focusing for the first time on the development of syntax-aware irony detection systems in a multilingual perspective (English, Spanish, French and Italian), providing an in-depth inve"
2020.coling-main.116,N18-4006,0,0.0195055,"quality of the morpho-syntactic annotation might provide. The Italian dataset is, in fact, the only one amongst the four ones that has been manually checked and corrected and whose quality could be considered superior compared to the resources used for the other languages. Furthermore, Italian is the only language configuration in which the dependency-based word embeddings have been trained on UD-based text from the same textual genre (tweets). To the best of our knowledge, dependency-based word embeddings were not used before in the detection of irony; on the other hand, in a previous work, MacAvaney and Zeldes (2018) expand the findings of Levy and Goldberg (2014) exploring the effectiveness of various dependency-based word embeddings on domain similarity, word similarity, and two other downstream tasks in English (question-type classification and named entity recognition.). They found that embeddings trained with UD contexts excel only in some tasks but not always improve the performance. 1353 It is our belief that fine-grained syntactic information such as the features we implemented in § 4.1, when added to an already robust architecture, could capture important structures of language and therefore boos"
2020.coling-main.116,W17-5405,0,0.0208141,"Missing"
2020.coling-main.116,P13-2017,0,0.0368726,"t release in the UD official repository3 ), we obtained the dependency-based annotation for the other corpora by applying the UDPipe4 pipeline (for tokenization, PoS-tagging and parsing). Figure 1 provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained on the EWT treebank (Silveira et al., 2014), that for Spanish on both GSD-Spanish (McDonald et al., 2013) and ANCORA corpora (Taulé et al., 2008). The model for Italian – for the remaining part of IronITA5 – was trained on POSTWITA (Sanguinetti et al., 2018) and ISDT treebanks (Simi et al., 2014), while that for French on the GSDFrench corpus (McDonald et al., 2013). We are aware that there actually exists a Twitter treebank for English, i.e. TWEEBANK V2 (Liu et al., 2018b), but it is not fully compliant with the UD format 3 https://github.com/UniversalDependencies/UD_Italian-TWITTIRO. http://ufal.mff.cuni.cz/udpipe. 5 Approximately 1,400 out 4,849 tweets from IronITA are also part of the TWITTIR"
2020.coling-main.116,2020.lrec-1.497,0,0.0484969,"Missing"
2020.coling-main.116,W19-7811,0,0.011322,"dge extracted from treebanks can be usefully exploited for addressing the irony detection task. In particular, they pave the way for a further investigation where the combination of a dependency-based syntactic approach and state-of-the-art neural models can be explored. In the future, in order to validate our findings, we will propose a novel and wider experimental setting, where more languages are included, e.g. testing our approach on other languages for which both ironyannotated datasets and UD resources are available, such as Arabic (Ghanem et al., 2020; Seddah et al., 2020) or German21 (Rehbein et al., 2019). Acknowledgments The work of C. Bosco and V. Basile was partially funded by Progetto di Ateneo/CSP 2016 Immigrants, Hate and Prejudice in Social Media (S1618_L2_BOSC_01). The work of M. Sanguinetti is funded by PRIN 2017 (2019-2022) project HOPE - High quality Open data Publishing and Enrichment. The work of P. Rosso was partially funded by the Spanish MICINN under the research project MISMIS-FAKEnHATE on Misinformation and Miscommunication in social media: FAKE news and HATE speech(PGC2018-096212-B-C31) and by the Generalitat Valenciana under the project DeepPattern (PROMETEO/2019/121). Than"
2020.coling-main.116,L18-1279,1,0.84297,"kenization, PoS-tagging and parsing). Figure 1 provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained on the EWT treebank (Silveira et al., 2014), that for Spanish on both GSD-Spanish (McDonald et al., 2013) and ANCORA corpora (Taulé et al., 2008). The model for Italian – for the remaining part of IronITA5 – was trained on POSTWITA (Sanguinetti et al., 2018) and ISDT treebanks (Simi et al., 2014), while that for French on the GSDFrench corpus (McDonald et al., 2013). We are aware that there actually exists a Twitter treebank for English, i.e. TWEEBANK V2 (Liu et al., 2018b), but it is not fully compliant with the UD format 3 https://github.com/UniversalDependencies/UD_Italian-TWITTIRO. http://ufal.mff.cuni.cz/udpipe. 5 Approximately 1,400 out 4,849 tweets from IronITA are also part of the TWITTIRÒ corpus, already available in UD. 4 1349 root conj punct ccomp ccomp cop punct cc obj cop Spero sia colite . Ma ho paura sia amore . I hope it’s colitis"
2020.coling-main.116,2020.acl-main.107,0,0.0189529,"hesis that morpho-syntactic knowledge extracted from treebanks can be usefully exploited for addressing the irony detection task. In particular, they pave the way for a further investigation where the combination of a dependency-based syntactic approach and state-of-the-art neural models can be explored. In the future, in order to validate our findings, we will propose a novel and wider experimental setting, where more languages are included, e.g. testing our approach on other languages for which both ironyannotated datasets and UD resources are available, such as Arabic (Ghanem et al., 2020; Seddah et al., 2020) or German21 (Rehbein et al., 2019). Acknowledgments The work of C. Bosco and V. Basile was partially funded by Progetto di Ateneo/CSP 2016 Immigrants, Hate and Prejudice in Social Media (S1618_L2_BOSC_01). The work of M. Sanguinetti is funded by PRIN 2017 (2019-2022) project HOPE - High quality Open data Publishing and Enrichment. The work of P. Rosso was partially funded by the Spanish MICINN under the research project MISMIS-FAKEnHATE on Misinformation and Miscommunication in social media: FAKE news and HATE speech(PGC2018-096212-B-C31) and by the Generalitat Valenciana under the project De"
2020.coling-main.116,silveira-etal-2014-gold,0,0.0569576,"Missing"
2020.coling-main.116,simi-etal-2014-less,1,0.831725,"provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained on the EWT treebank (Silveira et al., 2014), that for Spanish on both GSD-Spanish (McDonald et al., 2013) and ANCORA corpora (Taulé et al., 2008). The model for Italian – for the remaining part of IronITA5 – was trained on POSTWITA (Sanguinetti et al., 2018) and ISDT treebanks (Simi et al., 2014), while that for French on the GSDFrench corpus (McDonald et al., 2013). We are aware that there actually exists a Twitter treebank for English, i.e. TWEEBANK V2 (Liu et al., 2018b), but it is not fully compliant with the UD format 3 https://github.com/UniversalDependencies/UD_Italian-TWITTIRO. http://ufal.mff.cuni.cz/udpipe. 5 Approximately 1,400 out 4,849 tweets from IronITA are also part of the TWITTIRÒ corpus, already available in UD. 4 1349 root conj punct ccomp ccomp cop punct cc obj cop Spero sia colite . Ma ho paura sia amore . I hope it’s colitis . But I fear it’s love . VERB AUX NOUN"
2020.coling-main.116,D13-1170,0,0.00799628,"t. The fact that the expression of opinion or the production of irony are sensitive to syntactic variations occurring in texts has been moreover confirmed by Mahler et al. (2017), where artificial syntactic manipulations are applied, such as those based on negations and adverbs, with the purpose of fooling sentiment analysis systems. Some research already explored different kinds of syntactic features and their interaction in several NLP tasks, showing their effectiveness. For example, Sidorov et al. (2012) exploited syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information speci"
2020.coling-main.116,taule-etal-2008-ancora,0,0.0304766,"we obtained the dependency-based annotation for the other corpora by applying the UDPipe4 pipeline (for tokenization, PoS-tagging and parsing). Figure 1 provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained on the EWT treebank (Silveira et al., 2014), that for Spanish on both GSD-Spanish (McDonald et al., 2013) and ANCORA corpora (Taulé et al., 2008). The model for Italian – for the remaining part of IronITA5 – was trained on POSTWITA (Sanguinetti et al., 2018) and ISDT treebanks (Simi et al., 2014), while that for French on the GSDFrench corpus (McDonald et al., 2013). We are aware that there actually exists a Twitter treebank for English, i.e. TWEEBANK V2 (Liu et al., 2018b), but it is not fully compliant with the UD format 3 https://github.com/UniversalDependencies/UD_Italian-TWITTIRO. http://ufal.mff.cuni.cz/udpipe. 5 Approximately 1,400 out 4,849 tweets from IronITA are also part of the TWITTIRÒ corpus, already available in UD. 4 134"
2020.coling-main.116,C96-2162,0,0.260223,"is surveyed, while Section 3 describes the data and experimental settings we followed. In Section 4, we provide a description of the experiments we performed, and, in the last section, we discuss the experiments and provide some final remarks through an error analysis. We conclude the paper with some insights about the impact of morpho-syntax information and suggest possible directions for future work. 2 Related work The identification of irony and the description of pragmatic and linguistic devices that trigger it has always been a controversial topic (Grice, 1975; Sperber and Wilson, 1981; Utsumi, 1996), a challenge for both humans and automatic tools. This motivates the interest for irony of the NLP community, the organization of tasks regarding its processing and the subsequent creation of benchmarks. In the last decade several shared tasks have been organized in order to advance with the techniques regarding the automatic processing of figurative language (Ghosh et al., 2015), and more recently irony and sarcasm detection have been gaining greater attention. Being irony a pragmatic device inherently related to culture and language, the task captured the attention of various researchers, d"
2020.coling-main.116,S18-1005,0,0.0382354,"Missing"
2020.lrec-1.627,W14-2609,0,0.0202177,"usefulness of this representation for developing computational models of irony to be used for training purposes. Keywords: Twitter, Italian, Irony, Irony Activators, Syntax, Universal Dependencies 1. Introduction Irony detection proves to be an important task in the domain of Sentiment Analysis, due to the fact that the presence of ironic devices in a text may reverse the polarity of an opinion, which can be expressed, for example, with positive words but intending a negative meaning, therefore undermining the performance of Sentiment Analysis systems (Bosco et al., 2013; Reyes et al., 2013; Barbieri et al., 2014; Ghosh et al., 2015; Hernández Farías et al., 2015; Joshi et al., 2015). As a matter of fact, the recognition of irony and the identification of pragmatic and linguistic devices that activate it have always been a controversial topic (Grice, 1975; Grice, 1978; Sperber and Wilson, 1981), and a challenging task for both human annotators and automatic tools. Additionally, the automatic treatment of such phenomenon is further complicated by the co-occurrence of similar forms of speech such as sarcasm or satire (Hernández Farías and Rosso, 2016; Joshi et al., 2017; Ravi and Ravi, 2017; Zhang et al"
2020.lrec-1.627,L18-1664,1,0.870382,"Missing"
2020.lrec-1.627,W19-7723,1,0.924417,"he cases the corpora exploited in the tasks mentioned above are featured by a simple indication of the presence/absence of irony, but the interest for annotation going beyond this point is attested e.g. by (Cignarella et al., 2018a), which describes an Italian corpus of tweets (TWITTIRÒ), annotated with a fine-grained tagset for irony that was presented in Karoui et al. (2015). The same resource has been recently released under the Universal Dependencies project after the application of the UD morpho-syntactic format, based on dependency syntax (Nivre et al., 2016), thus creating TWITTIRÒ UD (Cignarella et al., 2019a). The starting point for this paper is TWITTIRÒ - UD. Recently, we proposed for this corpus to add a new level of semantic information by explicitly tagging irony activators at token level (Cignarella et al., 2019b) and the present research is a direct follow-up of this last work. Here the annotation of the activators has been applied on the full dataset, consisting of 1,424 tweets, and analyzed according to the perspectives described in the following sections. We present the annotation process that underlies the enrichment of the TWITTIRÒ - UD corpus that, to the best of our knowledge, is o"
2020.lrec-1.627,S15-2080,1,0.92103,"resentation for developing computational models of irony to be used for training purposes. Keywords: Twitter, Italian, Irony, Irony Activators, Syntax, Universal Dependencies 1. Introduction Irony detection proves to be an important task in the domain of Sentiment Analysis, due to the fact that the presence of ironic devices in a text may reverse the polarity of an opinion, which can be expressed, for example, with positive words but intending a negative meaning, therefore undermining the performance of Sentiment Analysis systems (Bosco et al., 2013; Reyes et al., 2013; Barbieri et al., 2014; Ghosh et al., 2015; Hernández Farías et al., 2015; Joshi et al., 2015). As a matter of fact, the recognition of irony and the identification of pragmatic and linguistic devices that activate it have always been a controversial topic (Grice, 1975; Grice, 1978; Sperber and Wilson, 1981), and a challenging task for both human annotators and automatic tools. Additionally, the automatic treatment of such phenomenon is further complicated by the co-occurrence of similar forms of speech such as sarcasm or satire (Hernández Farías and Rosso, 2016; Joshi et al., 2017; Ravi and Ravi, 2017; Zhang et al., 2019) and by the"
2020.lrec-1.627,P15-2124,0,0.0176182,"irony to be used for training purposes. Keywords: Twitter, Italian, Irony, Irony Activators, Syntax, Universal Dependencies 1. Introduction Irony detection proves to be an important task in the domain of Sentiment Analysis, due to the fact that the presence of ironic devices in a text may reverse the polarity of an opinion, which can be expressed, for example, with positive words but intending a negative meaning, therefore undermining the performance of Sentiment Analysis systems (Bosco et al., 2013; Reyes et al., 2013; Barbieri et al., 2014; Ghosh et al., 2015; Hernández Farías et al., 2015; Joshi et al., 2015). As a matter of fact, the recognition of irony and the identification of pragmatic and linguistic devices that activate it have always been a controversial topic (Grice, 1975; Grice, 1978; Sperber and Wilson, 1981), and a challenging task for both human annotators and automatic tools. Additionally, the automatic treatment of such phenomenon is further complicated by the co-occurrence of similar forms of speech such as sarcasm or satire (Hernández Farías and Rosso, 2016; Joshi et al., 2017; Ravi and Ravi, 2017; Zhang et al., 2019) and by the text domain. The application of tools for irony dete"
2020.lrec-1.627,P15-2106,0,0.12896,"essage. The Universal Dependencies (UD) format is used to encode the morpho-syntactic features of the irony activators, and a UD-based Italian treebank is exploited as a case study. After a brief overview of the shared tasks and the corpora developed for irony detection, in this paper we describe more in detail TWITTIRÒ - UD along with its annotation layers, focusing in particular on the one that includes irony activators. 2. Background and Motivation The present work is part of a wider joint project regarding irony detection. Together with other research groups working on English and French (Karoui et al., 2015), in the past years we have developed a fine-grained annotation devoted to the implementation of models for irony and irony-aware systems in a multilingual perspective. The main aim of this project is the investigation of the suitability and usefulness of a fine-grained annotation for the development of computational models of irony and figurative language devices. As far as irony detection is concerned, several evaluation exercises were organized for different languages in the last few years. The Italian EVALITA campaign included a first pilot task on irony detection in 2014, within the SENTI"
2020.lrec-1.627,E17-1025,1,0.84119,"[spero, sia, colite, .] and T 2 = [ho, sia, amore]. While, from a dependency relation viewpoint T1 and T2 are connected by means of: T 1 → T 2 = [T 1 → ccomp → conj → ccomp → T 2]. Such information could be exploited as feature in the implementation of automatic systems for the detection of irony, but it could also be useful to gain new insights on patterns that may underlie the activation of irony. 4.2. Annotation Guidelines In social media such as Twitter, contrasts in ironic tweets often consist of at least two propositions (but also simple words), which are in contradiction to each other (Karoui et al., 2017). This contradiction can be at a verbal or situational level. As we described in Section 3.1., the two elements in contrasts, i.e. T1 and T2, can be both part of the internal context of an utterance (that is explicitly lexicalized), or one is present and the other one implied. Starting from this principle – and exploiting the fact that our data are tokenized as per the Universal Dependencies format – we annotate T1 and T2 at a token level such that: Spero sia colite . Ma ho paura sia amore . T1 T2 • T1 and T2 can be tokens of any type: no specific constraints are given on the morpho-syntactic"
2020.lrec-1.627,L16-1262,0,0.0696921,"Missing"
2020.lrec-1.627,K17-3009,0,0.0171274,"istribution throughout the corpus. ANALOGY EUPHEMISM CONTEXT SHIFT OXYMORON PARADOX HYPERBOLE FALSE ASSERTION RHETORICAL QUESTION OTHER TOTAL IMPLICIT EXPLICIT TOTAL 55 10 – – 7 117 19 57 206 74 185 277 74 – 202 141 261 84 185 277 81 117 221 198 1,424 Table 1: Irony types and irony categories in TWITTIRÒ UD . Cells are with hyphen sign when they represent a category either inherently implicit or explicit. 3.2. Syntax In order to add the syntactic annotation layer to the dataset, we applied the full pipeline of tokenization, lemmatization, PoS-tagging and dependency parsing provided by UDPipe (Straka and Straková, 2017), obtaining a representation in CoNLL-U format of the 1,424 tweets8 . # sent_id = ___________________________ # twittiro = EXPLICIT OXYMORON / PARADOX # text = #labuonascuola è avere una scuola.9 The parser output was also manually revised by two independent annotators. Below, we provide an example of annotated tree: 7 For a detailed description of the categories and their profound pragmatic meaning, please refer to (Cignarella et al., 2018a). 8 The TWITTIRÒ - UD treebank is freely available at: https://github.com/UniversalDependencies/ UD_Italian-TWITTIRO. 9 Translation: #thegoodschool is to"
2020.lrec-1.627,S18-1005,0,0.0372578,"Missing"
2020.semeval-1.172,baccianella-etal-2010-sentiwordnet,0,0.0495204,"in (Online), December 12, 2020. 2 Background Sentiment classification is the task of detecting whether a textual item (e.g., a product review, a blog post, an editorial, etc.) expresses a POSITIVE or a NEGATIVE opinion in general or about a given entity, e.g., a product, a person, a political party, or a policy (Nakov et al., 2016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention t"
2020.semeval-1.172,Q17-1010,0,0.0565339,"token is replaced with a TOPIC token. For example, the tokens # and LoveIsLove are merged and replaced with a TOPIC token. • Emoji’s with text were divided into two tokens. For example, he, becomes he and ,. • If a token contains more than one emoji, each emoji was considered as a token. For example, ,/ becomes , and /. Embeddings: Following Collobert et al. (2011), a lot of authors argued that word embedding plays a vital role to improve natural language task performance. Hence, we experimented the use of word embeddings to improve the performance of our proposed models. Using the fastText (Bojanowski et al., 2017), we prepared two embedding models: Skip-gram and Cbow. After empirically evaluating the performance on validation set, the embeddings‘ dimensionality was set to 300 for all the embeddings. The embeddings are trained on training data using the parameters: lr=0.05, context window=5, epochs=5, minimal number of word occurences=5, dimensionality=300. Experiment: We carried out two experiments with similar settings except different word embedding approaches: Skip-gram for SkipGRun, and Cbow for CbowRun. Hyper-parameters: After evaluating the model performance on the validation data, the optimal va"
2020.semeval-1.172,J81-4005,0,0.486121,"Missing"
2020.semeval-1.172,N19-1423,0,0.03792,"we pre-processed the CM tweets and proposed a Recurrent Convolutional Neural Network for the sentiment analysis of CM tweets. We submitted two runs and obtaining promising results: our best run obtained 0.691 of F1 averaged across the positives, negatives and the neutral. We observed that the proposed architecture occasionally strives to separate the positive and negative polarities from the neutral and vice versa. For future work, we will explore the performance of our model with larger corpora against the testing set. Also, we would like to investigate other embedding choices such as BERT (Devlin et al., 2019). Moreover, due to the impact that irony and sarcasm have on sentiment analysis (Hern´andez Farıas and Rosso, 2016) it would be interesting to apply deep learning techniques to detect irony (Zhang et al., 2019) but in a code-mixed scenario. Acknowledgements The research work of the first four authors was supported by ERA-Net CHIST-ERA LIHLITH Project funded by ANR (France) project ANR-17-CHR2-0001-03. The research work of the last author was partially funded by the Spanish MICINN under the project MISMIS-FAKEnHATE on Misinformation and Miscommunication in social media: FAKE news and HATE speec"
2020.semeval-1.172,C16-1234,0,0.0267187,"were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research work have been carried out in particular Hindi-English CM data with different approaches: lexicon lookup (Sharma et al., 2015), sub-word with CNN-LSTM (Joshi et al., 2016), Siamese networks (Choudhary et al., 2018), dual Encoder Network with features (Lal et al., 2019). Lai et al. (2015) proposed Recurrent Convolutional Network for text classification which is a foundational task in many NLP applications. We followed this model in our task. 3 System overview We are inspired by the model proposed in (Lai et al., 2015) particularly proposed for the text classification task. The proposed model takes sequence of CM words as input and provides sentiment polarity class as output. The recurrent structure of the proposed model captures the contextual information during"
2020.semeval-1.172,P19-2052,0,0.0137829,", 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research work have been carried out in particular Hindi-English CM data with different approaches: lexicon lookup (Sharma et al., 2015), sub-word with CNN-LSTM (Joshi et al., 2016), Siamese networks (Choudhary et al., 2018), dual Encoder Network with features (Lal et al., 2019). Lai et al. (2015) proposed Recurrent Convolutional Network for text classification which is a foundational task in many NLP applications. We followed this model in our task. 3 System overview We are inspired by the model proposed in (Lai et al., 2015) particularly proposed for the text classification task. The proposed model takes sequence of CM words as input and provides sentiment polarity class as output. The recurrent structure of the proposed model captures the contextual information during the learning of the word representation, and the max-pooling layer identifies the key CM words. I"
2020.semeval-1.172,S13-2053,0,0.0654071,"016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research work have been carried out in particular Hindi-English CM data with different approaches: lexicon lookup (Sharma et al., 2015), sub-word with CNN-LSTM (Joshi et al., 2016), Siamese networks (Choudhary et al., 2018), dual Encoder Network with features (Lal et"
2020.semeval-1.172,S16-1001,0,0.0290725,"tivecommons.org/licenses/by/4.0/. The code of this work is available at https://github.com/somnath-banerjee/Code-Mixed_ SentimentAnalysis. 1 https://code-switching.github.io/2020/ 1281 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1281–1287 Barcelona, Spain (Online), December 12, 2020. 2 Background Sentiment classification is the task of detecting whether a textual item (e.g., a product review, a blog post, an editorial, etc.) expresses a POSITIVE or a NEGATIVE opinion in general or about a given entity, e.g., a product, a person, a political party, or a policy (Nakov et al., 2016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Moha"
2020.semeval-1.172,pak-paroubek-2010-twitter,0,0.100366,"policy (Nakov et al., 2016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research work have been carried out in particular Hindi-English CM data with different approaches: lexicon lookup (Sharma et al., 2015), sub-word with CNN-LSTM (Joshi et al., 2016), Siamese networks (Choudhary et al., 2018), dual Encoder Networ"
2020.semeval-1.172,S15-2078,0,0.0267216,"s the task of detecting whether a textual item (e.g., a product review, a blog post, an editorial, etc.) expresses a POSITIVE or a NEGATIVE opinion in general or about a given entity, e.g., a product, a person, a political party, or a policy (Nakov et al., 2016). Classifying tweets according to sentiment has many applications in political science, social sciences, market research, and many others (Mart´ınez-C´amara et al., 2014; Mejova et al., 2015). Although initially sentiment identification was focused on newswire text (Baccianella et al., 2010), later research turned towards social media (Rosenthal et al., 2015). Since 2013, a sentiment classification task on Twitter data have been organized in SemEval campaigns. Most of the earlier approaches to this problem were based on hand crafted features and sentiment lexicons (Pak and Paroubek, 2010; Mohammad et al., 2013). These features were then used as input to classifiers (e.g., Support Vector Machines). However, such approaches required extensive domain knowledge, were laborious to define, and can lead to incomplete or over-specific features. Recently, researchers pay their attention to sentiment polarity detection on CM data. However, a few research wo"
2020.semeval-1.209,2020.lrec-1.758,0,0.0399332,"Missing"
2020.semeval-1.209,N19-1423,0,0.036497,"re-trained for a large number of languages. In the experiments, the proposed method for English texts is compared with other approaches to analyze the relevance of the architecture used. Furthermore, simple models for the other languages are evaluated to compare them with the proposed one. The experimental results show that the model based on BERT outperforms other approaches. The main contribution of this work lies in this study, despite not obtaining the first positions in most cases of the competition ranking. 1 Introduction BERT, the Bidirectional Encoder Representations for Transformers (Devlin et al., 2019), is a model producing context representations that leverage on language model pre-training. It is based on transformers (Vaswani et al., 2017), which are models that process words in relation to all other words in a sentence, rather than word by word in order. That is, as opposed to directional models, which read the text input sequentially (forward and/or backward), the transformer reads the entire sequence of words at once. This characteristic allows the model to learn the context of a word based on all of its surroundings. Hence, BERT models can consider the full context of a word by looki"
2020.semeval-1.209,S19-2081,0,0.0284664,"roposed system, respectively. Experiments and results are then discussed in Section 4. Finally, we present our conclusions with a summary of our findings in Section 5. 2 Related Work Related tasks to abusive language analysis, and particularly the offensive language detection, have attracted significant attention during last years to prevent this kind of online behaviour. This is evidenced by different works (Waseem et al., 2017; Malmasi and Zampieri, 2018; Vidgen and Derczynski, 2020; Tekiroglu et al., 2020), and the organization of different workshops and shared tasks (Kumar et al., 2018; I Orts, 2019; Mandl et al., 2019; Zampieri et al., 2019b; Bosco et al., 2018; Basile et al., 2019). In general, models in OffensEval 2019 used different approaches, from traditional machine learning such as Support Vector Machines and Logistic Regression, to deep learning such as Convolutional Neural Networks and Recurrent Neural Networks, some of them including attention mechanisms. Moreover, some system employed BERT and reached top-performance in the competition (Zampieri et al., 2019b). In the present work, we propose a system based on BERT and MBERT, analyzing its parameters. 3 Dataset A multilingual"
2020.semeval-1.209,W18-4401,0,0.013058,"in the task and the proposed system, respectively. Experiments and results are then discussed in Section 4. Finally, we present our conclusions with a summary of our findings in Section 5. 2 Related Work Related tasks to abusive language analysis, and particularly the offensive language detection, have attracted significant attention during last years to prevent this kind of online behaviour. This is evidenced by different works (Waseem et al., 2017; Malmasi and Zampieri, 2018; Vidgen and Derczynski, 2020; Tekiroglu et al., 2020), and the organization of different workshops and shared tasks (Kumar et al., 2018; I Orts, 2019; Mandl et al., 2019; Zampieri et al., 2019b; Bosco et al., 2018; Basile et al., 2019). In general, models in OffensEval 2019 used different approaches, from traditional machine learning such as Support Vector Machines and Logistic Regression, to deep learning such as Convolutional Neural Networks and Recurrent Neural Networks, some of them including attention mechanisms. Moreover, some system employed BERT and reached top-performance in the competition (Zampieri et al., 2019b). In the present work, we propose a system based on BERT and MBERT, analyzing its parameters. 3 Dataset"
2020.semeval-1.209,2020.lrec-1.629,0,0.046886,"hanisms. Moreover, some system employed BERT and reached top-performance in the competition (Zampieri et al., 2019b). In the present work, we propose a system based on BERT and MBERT, analyzing its parameters. 3 Dataset A multilingual dataset with five languages is provided for the task. Therefore, a corpus of annotated texts have been released for each of the following languages: English (Rosenthal et al., 2020), Arabic 1 2 Multilingual Bert Readme Document https://sites.google.com/site/offensevalsharedtask/home 1606 (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020). The tagset matches the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a) that was used in OffensEval 2019. Then, the task is divided into three subtasks according to the tag hierarchy. So that, the 3 subtasks are developed for English, while for the rest of the 4 languages, only the first one is developed. The subtask A aims to discriminate between offensive and non-offensive text. Therefore, every text is assigned one of the two following labels: Offensive (OFF) and Not Offensive (NOT). On the one hand, offensive texts include insults,"
2020.semeval-1.209,2020.lrec-1.430,0,0.0206399,"tworks, some of them including attention mechanisms. Moreover, some system employed BERT and reached top-performance in the competition (Zampieri et al., 2019b). In the present work, we propose a system based on BERT and MBERT, analyzing its parameters. 3 Dataset A multilingual dataset with five languages is provided for the task. Therefore, a corpus of annotated texts have been released for each of the following languages: English (Rosenthal et al., 2020), Arabic 1 2 Multilingual Bert Readme Document https://sites.google.com/site/offensevalsharedtask/home 1606 (Mubarak et al., 2020), Danish (Sigurbergsson and Derczynski, 2020), Greek (Pitenis et al., 2020) and Turkish (C¸o¨ ltekin, 2020). The tagset matches the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a) that was used in OffensEval 2019. Then, the task is divided into three subtasks according to the tag hierarchy. So that, the 3 subtasks are developed for English, while for the rest of the 4 languages, only the first one is developed. The subtask A aims to discriminate between offensive and non-offensive text. Therefore, every text is assigned one of the two following labels: Offensive (OFF) and Not Offensive (NOT). On the one hand, of"
2020.semeval-1.209,2020.acl-main.110,0,0.0116227,"sents general ideas of related works. Then, Sections 3 and 4 describes the dataset used in the task and the proposed system, respectively. Experiments and results are then discussed in Section 4. Finally, we present our conclusions with a summary of our findings in Section 5. 2 Related Work Related tasks to abusive language analysis, and particularly the offensive language detection, have attracted significant attention during last years to prevent this kind of online behaviour. This is evidenced by different works (Waseem et al., 2017; Malmasi and Zampieri, 2018; Vidgen and Derczynski, 2020; Tekiroglu et al., 2020), and the organization of different workshops and shared tasks (Kumar et al., 2018; I Orts, 2019; Mandl et al., 2019; Zampieri et al., 2019b; Bosco et al., 2018; Basile et al., 2019). In general, models in OffensEval 2019 used different approaches, from traditional machine learning such as Support Vector Machines and Logistic Regression, to deep learning such as Convolutional Neural Networks and Recurrent Neural Networks, some of them including attention mechanisms. Moreover, some system employed BERT and reached top-performance in the competition (Zampieri et al., 2019b). In the present work,"
2020.semeval-1.209,W17-3012,0,0.0221678,"sing the MBERT model instead. The paper is organized as follows. Section 2 presents general ideas of related works. Then, Sections 3 and 4 describes the dataset used in the task and the proposed system, respectively. Experiments and results are then discussed in Section 4. Finally, we present our conclusions with a summary of our findings in Section 5. 2 Related Work Related tasks to abusive language analysis, and particularly the offensive language detection, have attracted significant attention during last years to prevent this kind of online behaviour. This is evidenced by different works (Waseem et al., 2017; Malmasi and Zampieri, 2018; Vidgen and Derczynski, 2020; Tekiroglu et al., 2020), and the organization of different workshops and shared tasks (Kumar et al., 2018; I Orts, 2019; Mandl et al., 2019; Zampieri et al., 2019b; Bosco et al., 2018; Basile et al., 2019). In general, models in OffensEval 2019 used different approaches, from traditional machine learning such as Support Vector Machines and Logistic Regression, to deep learning such as Convolutional Neural Networks and Recurrent Neural Networks, some of them including attention mechanisms. Moreover, some system employed BERT and reached"
2020.semeval-1.209,D19-1077,0,0.0203741,"rvised dataset for a target task. Therefore, it is eliminated the need for engineering a specific architecture for a task. This approach has advanced the state-of-the-art performances in many natural language processing tasks ranging from sequence classification to question answering. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1605 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1605–1614 Barcelona, Spain (Online), December 12, 2020. Multilingual BERT (MBERT)1 (Wu and Dredze, 2019) is a language model pre-trained on Wikipedia text from 104 languages in the same way as BERT for English. Therefore, not only is a contextual model, but the training does not requires supervision. That is, no alignment among the languages is done, but rather in the model the tokens from different languages share an embedding space and a single encoder. There are no cross-lingual objectives specifically designed nor any cross-lingual data, like parallel corpora. However, MBERT produces a representation that seems to generalize well from a cross-lingual perspective for a variety of downstream t"
2020.semeval-1.209,N19-1144,0,0.0204798,"Missing"
2020.semeval-1.209,S19-2010,0,0.0326754,"Missing"
2021.clpsych-1.24,W17-5802,0,0.0261089,"public health matter (World Health Orcourse and physical health. It includes among its ganization, 2012), have received attention in pretypical symptomatology an intense fear of gaining vious research in computational studies as well. weight or becoming fat and a distortion of one’s The majority of research has focused on the study body image (APA, 2014). of depression (De Choudhury et al., 2013; Eich224 Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology, pages 224–236 June 11, 2021. ©2021 Association for Computational Linguistics staedt et al., 2018; Abd Yusof et al., 2017; Yazdavar et al., 2017), but other mental illnesses have also been studied, including generalized anxiety disorder (Shen and Rudzicz, 2017), schizophrenia (Mitchell et al., 2015), post-traumatic stress disorder (Coppersmith et al., 2014, 2015), risks of suicide (O’dea et al., 2015), and self-harm (Losada et al., 2019; Yang et al., 2016). For anorexia, there are very few studies approaching the problem from a computational perspective. To our knowledge, the only publicly available social media dataset dedicated to anorexia is the eRisk dataset (Losada et al., 2019). The winners of eRisk’s shar"
2021.clpsych-1.24,N19-1151,0,0.0580955,"Missing"
2021.clpsych-1.24,C18-1126,0,0.0429245,"Missing"
2021.clpsych-1.24,W14-3207,0,0.0361012,"ll. weight or becoming fat and a distortion of one’s The majority of research has focused on the study body image (APA, 2014). of depression (De Choudhury et al., 2013; Eich224 Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology, pages 224–236 June 11, 2021. ©2021 Association for Computational Linguistics staedt et al., 2018; Abd Yusof et al., 2017; Yazdavar et al., 2017), but other mental illnesses have also been studied, including generalized anxiety disorder (Shen and Rudzicz, 2017), schizophrenia (Mitchell et al., 2015), post-traumatic stress disorder (Coppersmith et al., 2014, 2015), risks of suicide (O’dea et al., 2015), and self-harm (Losada et al., 2019; Yang et al., 2016). For anorexia, there are very few studies approaching the problem from a computational perspective. To our knowledge, the only publicly available social media dataset dedicated to anorexia is the eRisk dataset (Losada et al., 2019). The winners of eRisk’s shared task on anorexia detection (Mohammadi et al., 2019) used a hierarchical attention network and obtain a state-of-the-art F1 score of 0.71. In (Cohan et al., 2018) the authors introduce a dataset annotated for multiple mental disorders"
2021.clpsych-1.24,W15-1204,0,0.0615746,"Missing"
2021.clpsych-1.24,1995.mtsummit-1.1,0,0.0310311,"the study of body in modernity as an unfinished explanatory activity as a result of a greater feelmaterial, as “a place of self-presentation” (Le Breing of helplessness and fear. ton, 2011). This body of research could be applied to the study of anorexia nervosa without forgetting 1 Introduction and Previous Work the enormous symbolism of the act of eating. It is well established that eating with others (Dunbar, Anorexia nervosa (AN) is a type of eating disor2017) and eating the same food as the others is a der that leads to multiple psychiatric and somatic major symbol of social integration (Harris, 1971; complications and constitutes a major public health Young et al., 1971). concern. It involves a restriction of energy intake in relation to needs, leading to significantly low Mental health disorders in general, as a very body weight in relation to age, sex, developmental significant public health matter (World Health Orcourse and physical health. It includes among its ganization, 2012), have received attention in pretypical symptomatology an intense fear of gaining vious research in computational studies as well. weight or becoming fat and a distortion of one’s The majority of research has"
2021.clpsych-1.24,N19-1357,0,0.0298591,"neural network on one datapoint for each user, so as to ensure attention weights consistently correspond to the same part of the post history for each training example. The plot shows a general increasing importance for users suffering from anorexia: posts in the end of a user’s history are more heavily weighted. This is an interesting finding, since intuition, supported by findings such as those presented in the previous sections related to emotion evolution, would suggest a user’s activity on social media becomes increasingly indicative of their mental state as time goes by. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have questioned whether attention mechanisms necessarily help with the interpretability of neural network predictions. We further explore additional techniques in order to interpret the representations learned by the model. Attention is a mechanism frequently used in recurrent neural networks in order to weigh the parts of the input sequence differently according to their 3.2 User Embeddings importance for prediction. Attention weights are learned by the network, and thus can be used as We continue explaining the model’s behavior by a mean"
2021.clpsych-1.24,2021.ccl-1.108,0,0.0297412,"Missing"
2021.clpsych-1.24,W15-1202,0,0.0199004,"of gaining vious research in computational studies as well. weight or becoming fat and a distortion of one’s The majority of research has focused on the study body image (APA, 2014). of depression (De Choudhury et al., 2013; Eich224 Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology, pages 224–236 June 11, 2021. ©2021 Association for Computational Linguistics staedt et al., 2018; Abd Yusof et al., 2017; Yazdavar et al., 2017), but other mental illnesses have also been studied, including generalized anxiety disorder (Shen and Rudzicz, 2017), schizophrenia (Mitchell et al., 2015), post-traumatic stress disorder (Coppersmith et al., 2014, 2015), risks of suicide (O’dea et al., 2015), and self-harm (Losada et al., 2019; Yang et al., 2016). For anorexia, there are very few studies approaching the problem from a computational perspective. To our knowledge, the only publicly available social media dataset dedicated to anorexia is the eRisk dataset (Losada et al., 2019). The winners of eRisk’s shared task on anorexia detection (Mohammadi et al., 2019) used a hierarchical attention network and obtain a state-of-the-art F1 score of 0.71. In (Cohan et al., 2018) the authors in"
2021.clpsych-1.24,D14-1162,0,0.0851736,"vel encoder, which produces a representa- mapping words of the English vocabulary to 64 tion of a post, and a user-level encoder, which lexico-syntactic features of different kinds, with generates a representation of a user’s post history. high quality associations curated by human exThe post-level encoder and the user-level encoder perts, capturing different levels of language: inare modelled as LSTMs. The word sequences en- cluding style (through syntactic categories), emocoded using embeddings initialized with GloVe tions (through affect categories) and topics (such pre-trained embeddings (Pennington et al., 2014) as money, health or religion). and passed to the LSTM are then concatenated with Emotions and sentiment. We dedicate a few the other features to form the hierarchical post en- features to representing emotional content in our coding. The obtained representation is passed to texts, since the emotional state of a user is known the user-encoder LSTM, which is connected to the to be highly correlated with her mental health. output layer. We use the train/test split provided by Aside from the sentiment and emotion categories the shared task organizers, done at the user level, in the LIWC lexicon,"
2021.clpsych-1.24,P19-1282,0,0.0175363,"tapoint for each user, so as to ensure attention weights consistently correspond to the same part of the post history for each training example. The plot shows a general increasing importance for users suffering from anorexia: posts in the end of a user’s history are more heavily weighted. This is an interesting finding, since intuition, supported by findings such as those presented in the previous sections related to emotion evolution, would suggest a user’s activity on social media becomes increasingly indicative of their mental state as time goes by. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have questioned whether attention mechanisms necessarily help with the interpretability of neural network predictions. We further explore additional techniques in order to interpret the representations learned by the model. Attention is a mechanism frequently used in recurrent neural networks in order to weigh the parts of the input sequence differently according to their 3.2 User Embeddings importance for prediction. Attention weights are learned by the network, and thus can be used as We continue explaining the model’s behavior by a means to interpreting its dec"
2021.clpsych-1.24,W17-3107,0,0.0219013,"etypical symptomatology an intense fear of gaining vious research in computational studies as well. weight or becoming fat and a distortion of one’s The majority of research has focused on the study body image (APA, 2014). of depression (De Choudhury et al., 2013; Eich224 Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology, pages 224–236 June 11, 2021. ©2021 Association for Computational Linguistics staedt et al., 2018; Abd Yusof et al., 2017; Yazdavar et al., 2017), but other mental illnesses have also been studied, including generalized anxiety disorder (Shen and Rudzicz, 2017), schizophrenia (Mitchell et al., 2015), post-traumatic stress disorder (Coppersmith et al., 2014, 2015), risks of suicide (O’dea et al., 2015), and self-harm (Losada et al., 2019; Yang et al., 2016). For anorexia, there are very few studies approaching the problem from a computational perspective. To our knowledge, the only publicly available social media dataset dedicated to anorexia is the eRisk dataset (Losada et al., 2019). The winners of eRisk’s shared task on anorexia detection (Mohammadi et al., 2019) used a hierarchical attention network and obtain a state-of-the-art F1 score of 0.71."
2021.clpsych-1.24,D19-1002,0,0.0129691,"as to ensure attention weights consistently correspond to the same part of the post history for each training example. The plot shows a general increasing importance for users suffering from anorexia: posts in the end of a user’s history are more heavily weighted. This is an interesting finding, since intuition, supported by findings such as those presented in the previous sections related to emotion evolution, would suggest a user’s activity on social media becomes increasingly indicative of their mental state as time goes by. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have questioned whether attention mechanisms necessarily help with the interpretability of neural network predictions. We further explore additional techniques in order to interpret the representations learned by the model. Attention is a mechanism frequently used in recurrent neural networks in order to weigh the parts of the input sequence differently according to their 3.2 User Embeddings importance for prediction. Attention weights are learned by the network, and thus can be used as We continue explaining the model’s behavior by a means to interpreting its decisions. In our mod- analyzing"
2021.clpsych-1.24,N16-1174,0,0.0395074,"image (APA, 2014). of depression (De Choudhury et al., 2013; Eich224 Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology, pages 224–236 June 11, 2021. ©2021 Association for Computational Linguistics staedt et al., 2018; Abd Yusof et al., 2017; Yazdavar et al., 2017), but other mental illnesses have also been studied, including generalized anxiety disorder (Shen and Rudzicz, 2017), schizophrenia (Mitchell et al., 2015), post-traumatic stress disorder (Coppersmith et al., 2014, 2015), risks of suicide (O’dea et al., 2015), and self-harm (Losada et al., 2019; Yang et al., 2016). For anorexia, there are very few studies approaching the problem from a computational perspective. To our knowledge, the only publicly available social media dataset dedicated to anorexia is the eRisk dataset (Losada et al., 2019). The winners of eRisk’s shared task on anorexia detection (Mohammadi et al., 2019) used a hierarchical attention network and obtain a state-of-the-art F1 score of 0.71. In (Cohan et al., 2018) the authors introduce a dataset annotated for multiple mental disorders including anorexia. Another study (Amini and Kosseim) on the explainability of anorexia detection mode"
2021.clpsych-1.24,N16-1000,0,0.206249,"Missing"
2021.eacl-main.56,aker-etal-2017-simple,0,0.0119796,"ork, none of these works considers the sequence of affective information in text; instead, they feed the entire news articles as one segment into their models. In contrast, the aim of our work is to evaluate this source of information, using a neural architecture. • We build a novel fake news dataset, called MultiSourceFake, that is collected from a large set of websites and annotated on the basis of the joint agreement of a set of news sources. 2 Related Work Previous work on fake news detection is mainly divided into two main lines, namely with a focus on social media (Zubiaga et al., 2015; Aker et al., 2017; Ghanem et al., 2019) or online news articles (Tausczik and Pennebaker, 2010; Horne and Adali, 2017; Rashkin et al., 2017; Barr´on-Cedeno et al., 2019). In this work we focus on the latter one. Factchecking (Karadzhov et al., 2017; Zlatkova et al., 2019; Shu et al., 2019a) is another closely related research topic. However, fact-checking targets only short texts (that is, claims) and focuses on using external resources (e.g. Web, knowledge sources) to verify the factuality of the news. The focus in previous work on fake news detection is mainly on proposing new feature sets. Horne and Adali ("
2021.eacl-main.56,C18-1244,0,0.0222144,"t statements can unmask the fabrication. On the other hand, in fake news articles the authors exploit the length of the news to conceal their fabricated story. This fact exposes the readers to be emotionally manipulated while reading longer texts that have several imprecise or fabricated plots. The flow of information has been investigated for different tasks: Reagan et al. (2016) studied the emotional arcs in stories in order to understand complex emotional trajectories; Maharjan et al. (2018) model the flow of emotions over a book and quantify its usefulness for predicting success in books; Kar et al. (2018) explore the problem of creating tags for movies from plot synopses using emotions. Unlike previous works (Rashkin et al., 2017; Shu et al., 2018; Castelo et al., 2019; Ghanem et al., 2020) that discarded the chronological order of events in news articles, in this work we propose a model that takes into account the affective changes in texts to detect fake news. We hypothesize that fake news has a different distribution of affective information across the text compared to real news, e.g. more fear emotion in the first part of the article or more overall offensive terms, etc. Therefore, modelin"
2021.eacl-main.56,karadzhov-etal-2017-fully,0,0.110113,"7; Ghanem et al., 2019) or online news articles (Tausczik and Pennebaker, 2010; Horne and Adali, 2017; Rashkin et al., 2017; Barr´on-Cedeno et al., 2019). In this work we focus on the latter one. Factchecking (Karadzhov et al., 2017; Zlatkova et al., 2019; Shu et al., 2019a) is another closely related research topic. However, fact-checking targets only short texts (that is, claims) and focuses on using external resources (e.g. Web, knowledge sources) to verify the factuality of the news. The focus in previous work on fake news detection is mainly on proposing new feature sets. Horne and Adali (2017) present a set of content-based features, including readability (number of unique words, SMOG readability measure, etc.), stylistic (frequency of partof-speech tags, number of stop words, etc.) and psycholinguistic features (i.e., several categories from the LIWC dictionary (Tausczik and Pennebaker, 2010)). When these features are fed into a Support Vector Machine (SVM) classifier and applied, for instance, to the task of distinguishing satire from real news, they obtain high accuracies. Using the same features for the task of fake news detection, however, results in somewhat lower scores. P´e"
2021.eacl-main.56,N18-2042,0,0.03477,"Missing"
2021.eacl-main.56,W10-0204,0,0.0182268,"evance of the affective information with respect to the topics. For this, we concatenate the topic summarized vector vtopic with the representation vector vaffect , aimed at capturing the affective information extracted from each segment (Section 3.2). vconcat = vtopic ⊕ vaffect To merge the different representations and capture their joint interaction in each segment, the model processes the produced concatenated vector vconcat with another fully connected layer: • Emotions: We use emotions as features to detect their change among articles’ segments. For that we use the NRC emotions lexicon (Mohammad and Turney, 2010) that contains ∼14K words labeled using the eight Plutchik’s emotions (8 Features). • Sentiment: We extract the sentiment from the text, positive and negative, again using the NRC lexicon (Mohammad and Turney, 2010) (2 Features). • Morality: We consider cue words from the Moral Foundations Dictionary2 (Graham et al., 2009) where words are assigned to one (or more) of the following categories: care, harm, fairness, unfairness (cheating), loyalty, betrayal, authority, subversion, sanctity and degradation (10 Features). • Imageability: We use a list of words rated by their degree of abstractness"
2021.eacl-main.56,C18-1287,0,0.0551735,"Missing"
2021.eacl-main.56,D17-1317,0,0.0787607,"to conceal their fabricated story. This fact exposes the readers to be emotionally manipulated while reading longer texts that have several imprecise or fabricated plots. The flow of information has been investigated for different tasks: Reagan et al. (2016) studied the emotional arcs in stories in order to understand complex emotional trajectories; Maharjan et al. (2018) model the flow of emotions over a book and quantify its usefulness for predicting success in books; Kar et al. (2018) explore the problem of creating tags for movies from plot synopses using emotions. Unlike previous works (Rashkin et al., 2017; Shu et al., 2018; Castelo et al., 2019; Ghanem et al., 2020) that discarded the chronological order of events in news articles, in this work we propose a model that takes into account the affective changes in texts to detect fake news. We hypothesize that fake news has a different distribution of affective information across the text compared to real news, e.g. more fear emotion in the first part of the article or more overall offensive terms, etc. Therefore, modeling the flow of such information may help discriminating fake from real news. Our model consists of two main sub-modules, topic-b"
2021.eacl-main.56,N16-1174,0,0.056493,"nformation by searching Twitter for users who shared news. Out of the whole collected information, we use only the textual information of news articles, which is the part we are interested in. 683 Baselines. To evaluate the performance of our model, we use a combination of fake news detection models and deep neural network architectures: • CNN, LSTM: We use CNN and LSTM models and validate their performance when treating each document as one fragment. We experiment with different hyper-parameters and report results for the ones that performed best on the validation set. • HAN: The authors of (Yang et al., 2016) proposed a Hierarchical Attention Networks (HAN) model for long document classification. The proposed model consists of two levels of attention mechanisms, i.e., word and sentence attention. The model splits each document into sentences and learns sentence representations from words. • BERT: is a text representation model that showed superior performance on multiple natural language processing (NLP) benchmarks (Devlin et al., 2019). We use the pre-trained bertbase-uncased version which has 12-layers and yields output embeddings with a dimension of size 768. We feed the hidden representation o"
2021.eacl-main.56,N16-1000,0,0.0992272,"might be less harmful than news articles. They may have some eye-catching terms that aim to manipulate the readers’ emotions (Chakraborty et al., 2016). In many cases, the identification of this kind of exaggeration in short statements can unmask the fabrication. On the other hand, in fake news articles the authors exploit the length of the news to conceal their fabricated story. This fact exposes the readers to be emotionally manipulated while reading longer texts that have several imprecise or fabricated plots. The flow of information has been investigated for different tasks: Reagan et al. (2016) studied the emotional arcs in stories in order to understand complex emotional trajectories; Maharjan et al. (2018) model the flow of emotions over a book and quantify its usefulness for predicting success in books; Kar et al. (2018) explore the problem of creating tags for movies from plot synopses using emotions. Unlike previous works (Rashkin et al., 2017; Shu et al., 2018; Castelo et al., 2019; Ghanem et al., 2020) that discarded the chronological order of events in news articles, in this work we propose a model that takes into account the affective changes in texts to detect fake news. W"
2021.eacl-main.56,D19-1216,0,0.0156223,"architecture. • We build a novel fake news dataset, called MultiSourceFake, that is collected from a large set of websites and annotated on the basis of the joint agreement of a set of news sources. 2 Related Work Previous work on fake news detection is mainly divided into two main lines, namely with a focus on social media (Zubiaga et al., 2015; Aker et al., 2017; Ghanem et al., 2019) or online news articles (Tausczik and Pennebaker, 2010; Horne and Adali, 2017; Rashkin et al., 2017; Barr´on-Cedeno et al., 2019). In this work we focus on the latter one. Factchecking (Karadzhov et al., 2017; Zlatkova et al., 2019; Shu et al., 2019a) is another closely related research topic. However, fact-checking targets only short texts (that is, claims) and focuses on using external resources (e.g. Web, knowledge sources) to verify the factuality of the news. The focus in previous work on fake news detection is mainly on proposing new feature sets. Horne and Adali (2017) present a set of content-based features, including readability (number of unique words, SMOG readability measure, etc.), stylistic (frequency of partof-speech tags, number of stop words, etc.) and psycholinguistic features (i.e., several categories"
2021.semeval-1.37,S19-2007,1,0.884797,"Missing"
2021.semeval-1.37,2020.emnlp-demos.2,0,0.0156645,"the classification module. The first one is based on a Siamese neural network and the second one relies on fusing representations and training a linear regression model. 3.1 Encoder Modules The Encoder plays an important role because it is concerned with learning an abstract representation that vanishes the colinearity between its features 298 and compresses the textual information on a single dense vector. In our proposal, the encoders are based on Transformer models (TM), specifically on RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2020) architectures. Moreover, we employed BERTweet (Nguyen et al., 2020) which is based on the structure and pre-training procedure like RoBERTa, but using an English tweets corpus that makes it easier to fine-tune on NLP tasks where the texts are short and informal. For fine-tuning the TM-based encoders we add up an intermediate layer that receives the vectors from the output sequence of the TM. On this sequence of vectors, we explore three variants for selecting the best way of representing the message: i) the vector in the first position (associated to the CLS token), ii) the normalized sum of all vector in the sequence, and iii) the vector in the last position"
2021.semeval-1.37,N18-1202,0,0.0152711,"al., 2018). For this purpose, we constructed an embedding matrix whose features were based on an affective information set proposed by (Far´ıas et al., 2016) containing basic emotions (i.e., sadness, surprise, fear, etc). The embedding vectors involved 52 components between binary and no binary values, and the vocabulary was built from the affective resources, hence words not expressing emotional charge at all were encoded with the null vector. The information obtained from this embedding for a message was fed into an BiLSTM (Bidirectional Long Short Term Memory) architecture similar to ELMO (Peters et al., 2018). Deeper BiLSTM output was fed, to an intermediate layer to condense the information passed later to the output layers. For training this model we used a multitask approach focusing on predicting how offensive a message is, as well as how funny it is, by using the data provided for HaHackathon. Linear Regression As we can be observed in Figure 3, the encoding provided by transformer models differ regarding the space region in which the offensive features are projected. We can infer that one representation helps the others by providing information not captured simultaneously. Figure 3: Scatter"
2021.semeval-1.37,P18-1031,0,0.0188375,"-based encoders we add up an intermediate layer that receives the vectors from the output sequence of the TM. On this sequence of vectors, we explore three variants for selecting the best way of representing the message: i) the vector in the first position (associated to the CLS token), ii) the normalized sum of all vector in the sequence, and iii) the vector in the last position of the sequence. On this layer, we stacked an output layer that makes the final prediction for the targeted task. For that purpose, we follow the strategy proposed in the Universal Language Model FineTuning (ULMFiT) (Howard and Ruder, 2018). For each layer of the TM a different learning rate is set up, increasing it using a multiplier while the neural network gets deeper. This multiplier increases 0.1 points from a layer Li to another Li+1 . We use this dynamic learning rate to keep most information from the pre-training at shallow layers and biasing the deeper ones to learn about the specific tasks. On the humor predicting subtask the BERTweet encoder was employed, whereas for offensiveness rating the three TMs were considered and trained using a multitask learning strategy for predicting offensive scores and irony together. Pa"
2021.semeval-1.37,C18-1244,0,0.0183238,"r (Bishop, 1995) in the regression subtask, which is less sensitive to outliers than the standard mean squared error. It is defined as follows: P (|y − yˆ|)kc ErrorM inkowski = (3) n Where y is the label for one example, yˆ is the predicted value, n the number of examples and kc the Minkowski coefficient which we set to 1.4 empirically. The Affective Features Conversely to the three representations above, the Affective Features representation was obtained from a word-level recurrent neural network, trying to capture how affective information from different dimensions flows along the messages (Kar et al., 2018). For this purpose, we constructed an embedding matrix whose features were based on an affective information set proposed by (Far´ıas et al., 2016) containing basic emotions (i.e., sadness, surprise, fear, etc). The embedding vectors involved 52 components between binary and no binary values, and the vocabulary was built from the affective resources, hence words not expressing emotional charge at all were encoded with the null vector. The information obtained from this embedding for a message was fed into an BiLSTM (Bidirectional Long Short Term Memory) architecture similar to ELMO (Peters et"
2021.semeval-1.37,2021.ccl-1.108,0,0.0318373,"Missing"
2021.semeval-1.37,2021.semeval-1.9,0,0.0269534,"om the context in which they are produced. Additional knowledge from gestures, prosody features, visual content, situational environment and sociocultural rules play an important role in how humans properly understand the real meaning behind funny and hateful contents. All this makes humor recognition and offensiveness detection challenging tasks within Natural Language Processing (NLP) and HumanComputer Interaction (HCI). On this line, the Task 7, HaHackathon: Detecting and Rating Humor and Offense at SemEval-2021 aims at computationally recognizing humor and offensiveness in English tweets (Meaney et al., 2021). In this paper we describe the systems used by the RoMa team in the shared task on Detecting and Rating Humor and Offense (HaHackathon) at SemEval 2021. Our systems rely on data representations learned through fine-tuned neural language models. Particularly, we explore two distinct architectures. The first one is based on a Siamese Neural Network (SNN) combined with a graph-based clustering method. The SNN model is used for learning a latent space where instances of humor and non-humor can be distinguished. The clustering method is applied to build prototypes of both classes which are used fo"
2021.semeval-1.37,S18-1005,0,0.0566405,"Missing"
2021.semeval-1.37,W16-5618,0,0.0379339,"ersitat Polit`ecnica de Val`encia Valencia Spain prosso@dsic.upv.es Abstract Keles and Niall McCrae and Annmarie Grealish , 2020). Most of these harmful contents are often masquerade as innocent jokes or simply as a funny content. Therefore, it is crucial to shed light on the commonalities and differences between both phenomena in order to properly addressing the challenge of computationally distinguishing humorous messages from aggressive or offensive ones. Recognizing humorous and offensive utterances on written messages is a very difficult task for human beings and even more for computers (Waseem, 2016). These difficulties increase when the textual messages are isolated from the context in which they are produced. Additional knowledge from gestures, prosody features, visual content, situational environment and sociocultural rules play an important role in how humans properly understand the real meaning behind funny and hateful contents. All this makes humor recognition and offensiveness detection challenging tasks within Natural Language Processing (NLP) and HumanComputer Interaction (HCI). On this line, the Task 7, HaHackathon: Detecting and Rating Humor and Offense at SemEval-2021 aims at"
2021.semeval-1.37,S19-2010,0,0.0261888,"The SiaNet model and the Ridge Regression model are fed with information of the messages extracted through their respective encoder modules, this makes our first effort focused on tuning them for obtaining the best representation. For both approaches, SiaNet and Ridge Regressor, the encoders were optimized using the RMSprop method (Hinton et al., 2012). Firstly, for obtaining the multi-viewed representation of the messages, the RoBERTa, BERTweet and XLNet encoders were fine-tuned using (MLM) unsupervised learning. For that, we considered three additional related-datasets (Basile et al., 2019; Zampieri et al., 2019; Van Hee et al., 2018) and the HaHackathon dataset itself. Afterwards, since the multi-viewed representation was constructed for rating offensiveness, the three models were trained specifically for this regression task by exploring two main ideas based on multitask learning strategy (MTL): i) The first one aims at capturing the Strategy HAHA Irony No MTL 0.70 0.65 0.81 0.75 0.63 0.67 0.69 0.68 0.70 Table 1: MTL strategies for offensiveness rating subtask. HAHA refers to MTL with all HaHackathon subtasks and Irony refers to MTL with irony detection task yields our best result at predicting whe"
barron-cedeno-etal-2010-corpus,clough-etal-2002-building,0,\N,Missing
barron-cedeno-etal-2010-corpus,P02-1020,0,\N,Missing
buscaldi-rosso-2006-mining,P02-1054,0,\N,Missing
buscaldi-rosso-2006-mining,P04-3018,0,\N,Missing
buscaldi-rosso-2008-geo,H05-1046,0,\N,Missing
buscaldi-rosso-2008-geo,W03-0107,0,\N,Missing
C10-1005,J93-2003,0,0.047958,"ient for the CL-ESA data requirements. 3.2 In this model an estimation of how likely is that d′ is a translation of dq is performed. It is based on the adaptation of the Bayes rule for MT: p(d′ |dq ) = p(d′ ) p(dq |d′ ) . p(dq ) (1) As p(dq ) does not depend on d′ , it is neglected. From an MT point of view, the conditional probability p(dq |d′ ) is known as translation model probability and is computed on the basis of a statistical bilingual dictionary. p(d′ ) is known as language model probability; it describes the target language L′ in order to obtain grammatically acceptable translations (Brown et al., 1993). Translating dq into L′ is not the concern of this method, rather it focuses on retrieving texts written in L′ which are potential translations of dq . Therefore, Barr´on-Cede˜no et al. (2008) proposed replacing the language model (the one used in T+MA) by that known as length model. This model depends on text’s character lengths instead of language structures. Multiple translations from d into L′ are possible, and it is uncommon to find a pair of translated texts d and d′ such that |d |= |d′ |. Nevertheless, the length of such translations is closely related to a translation length factor. I"
C10-1005,clough-etal-2002-building,0,0.11682,"1 where µ and σ are the mean and the standard deviation of the character lengths between translations of texts from L into L′ . If the length of d′ is not the expected given dq , it receives a low qualification. The translation model probability is defined as: p(d |d′ ) = YX p(x, y), In other Information Retrieval tasks a plethora of corpora is available for experimental and comparison purposes. However, plagiarism implies an ethical infringement and, to the best of our knowledge, there is no corpora of actual cases available, other than some seminal efforts on creating corpora of text reuse (Clough et al., 2002), artificial plagiarism (Potthast et al., 2009), and simulated plagiarism (Clough and Stevenson, 2010). The problem is worse for cross-language plagiarism. Therefore, in our experiments we use two parallel corpora: Software, an en-eu translation memory of software manuals generously supplied by Elhuyar Fundazioa5 ; and Consumer, a corpus extracted from a consumer oriented magazine that includes articles written in Spanish along with their Basque, Catalan, and Galician translations6 (Alc´azar, 2006). Software includes 288, 000 parallel sentences; 8.66 (6.83) words per sentence in the English (B"
C10-1005,P07-2045,0,0.00738927,"Missing"
C10-1005,N04-1034,0,0.0137599,"Missing"
C10-1005,J03-1002,0,0.00763746,"Missing"
C10-1005,steinberger-etal-2006-jrc,0,0.0300396,"m of CLPD in Basque, with source texts written in Spanish (the co-official language of the Figure 1: First sentences from the Wikipedia articles “Party of European Socialists” (en),“Partido Socialista Europeo” (es), and “Europako Alderdi Sozialista” (eu) (Wikipedia, 2010b). 100 words are contained in the en, es and eu articles, respectively). Of high relevance is that the two corpora used in this work were manually constructed by translating English and Spanish text into Basque. In the experiments carried out by Potthast et al. (2010), which inspired our work, texts from the JCRAcquis corpus (Steinberger et al., 2006) and Wikipedia were used. The first one is a multilingual corpus with no clear definition of source and target languages, whereas in Wikipedia no specific relationship exists between the different languages in which a topic may be broached. In some cases (cf. Fig. 1) they are clearly co-derived, but in others they are completely independent. CLPD has been investigated just recently, mainly by adapting models formerly proposed for cross-language information retrieval. This is the case of cross-language explicit semantic analysis (CL-ESA), proposed by Potthast et al. (2008). In this case the com"
C10-1005,barron-cedeno-etal-2010-corpus,1,\N,Missing
C10-2115,ambati-etal-2010-active,0,0.0315161,"Missing"
C10-2115,N03-1003,0,0.0248306,"Missing"
C10-2115,clough-etal-2002-building,0,0.519814,"Missing"
C12-2043,P04-3024,0,0.0356753,"ries in the training data using a particular feature value individually to rank documents is considered as the importance score s( f i ). Usually, the features which can not better discriminate between relevance classes do not add more knowledge for the learning algorithm. This discrimination can be better captured by measuring the divergence of the feature on relevance classes. In order to estimate the divergence of a feature over the relevance classes, we use KL divergence. KL divergence has successfully been used for the feature selection methods for classification problems (Coetzee, 2005; Schneider, 2004). Because of the intuitive differences between the classification and ranking, we adapt and call it as expected divergence which, to the best of our knowledge, is novel. Classes in ranking are ordinal relevance levels, while they are unordered categories in case of classification. Hence, we boost the divergence of a feature over distant relevance classes by the expected divergence. For example, consider a 5-scale relevance system with relevance classes ri ∈ R where i ∈ {0, 1, .., 4}, the divergence of a feature over r0 and r4 is far more important than that over r0 and r1 . The expected diverg"
C14-1116,P11-1030,1,0.860458,"Missing"
C14-1116,E09-1039,0,0.300969,"styles of text. Obtaining such corpora is a challenging task since most authorship attribution studies focus on a single domain. We have found two datasets that meet our criteria, one having both cross-topic and cross-genre flavor, and the other having only cross-topic flavor. The first corpus contains communication samples from 21 authors in six genres (Email, Essay, Blog, Chat, Phone Interview, and Discussion) on six topics (Catholic Church, Gay Marriage, War in Iraq, Legalization of Marijuana, Privacy Rights, and Sex Discrimination), which we call dataset 1. This dataset was obtained from Goldstein-Stewart et al. (2009). Using this dataset, it is possible to see how the performance of cross-topic AA changes across different genres. Another corpus is composed of texts published in The Guardian daily newspaper written by 13 authors in one genre on four topics (dataset 2) due Stamatatos et al. (2013). It contains opinion articles (comments) about World, U.K., Culture, and Politics. Table 1 shows some statistics about the datasets. Corpus #authors #genres #topics Dataset 1 Dataset 2 21 13 6 1 6 4 avg #docs/author 36 64 avg #sentences/doc 31.7 53 avg #words/doc 600 1034 Table 1: Some statistics about dataset 1 an"
D08-1030,W03-2201,0,0.0628018,"ic Content Extraction (ACE)1 , the task of NER has garnered significant attention within the natural language processing (NLP) community. ACE has facilitated evaluation for different languages creating standardized test sets and evaluation metrics. NER systems are typically enabling sub-tasks within 1 http://www.nist.gov/speech/tests/ace/2004/doc/ace04evalplan-v7.pdf large NLP systems. The quality of the NER system has a direct impact on the quality of the overall NLP system. Evidence abound in the literature in areas such as Question Answering, Machine Translation, and Information Retrieval (Babych and Hartley, 2003; Ferr´andez et al., 2004; Toda and Kataoka, 2005). The most prominent NER systems approach the problem as a classification task: identifying the named entities (NE) in the text and then classifying them according to a set of designed features into one of a predefined set of classes (Bender et al., 2003). The number of classes differ depending on the data set. To our knowledge, to date, the approach is always to model the problem with a single set of features for all the classes simultaneously. This research, diverges from this view. We recognize that different classes are sensitive to differi"
D08-1030,W03-0420,0,0.181789,"/www.nist.gov/speech/tests/ace/2004/doc/ace04evalplan-v7.pdf large NLP systems. The quality of the NER system has a direct impact on the quality of the overall NLP system. Evidence abound in the literature in areas such as Question Answering, Machine Translation, and Information Retrieval (Babych and Hartley, 2003; Ferr´andez et al., 2004; Toda and Kataoka, 2005). The most prominent NER systems approach the problem as a classification task: identifying the named entities (NE) in the text and then classifying them according to a set of designed features into one of a predefined set of classes (Bender et al., 2003). The number of classes differ depending on the data set. To our knowledge, to date, the approach is always to model the problem with a single set of features for all the classes simultaneously. This research, diverges from this view. We recognize that different classes are sensitive to differing features. Hence, in this study, we aspire to discover the optimum feature set per NE class. We approach the NER task from a multi-classification perspective. We create a classifier for each NE class independently based on an optimal feature set, then combine the different classifiers for a global NER"
D08-1030,farber-etal-2008-improving,0,0.561935,"Missing"
D08-1030,P05-1071,0,0.257377,"Missing"
D08-1030,W03-0419,0,\N,Missing
D14-1153,D13-1151,0,0.0264657,"t copying from any source, e.g. the case of a student who asked someone else to write for him parts of his essay or thesis. Hence, the task of detecting plagiarism intrinsically is to identify, in the given suspicious document, the fragments that are not consistent with the rest of the text in terms of writing style. The automatic analysis of the writing style is an important component of many NLP applications. For some of them, when analyzing the style, a document is considered as a whole, which is the case of the authorship identification (Stamatatos, 2009a) and the authorship verification (Koppel and Seidman, 2013). For other applications, a document is perceived as a set of fragments, for each of them the writing style needs to be analyzed individually. Examples of such applications include: paragraph authorship clustering (Brooke and Hirst, 2012), authorial Salim Chikhi MISC Lab Constantine 2 University, Algeria slchikhi@yahoo.com segmentation of multi-author documents (Akiva and Koppel, 2013), detection of stylistic inconsistencies between consecutive paragraphs (Graham et al., 2005) and plagiarism direction identification (Grozea and Popescu, 2010). For intrinsic plagiarism detection, it is crucial"
D14-1153,C10-2115,1,0.652958,"Missing"
dubey-etal-2014-enrichment,C04-1151,0,\N,Missing
dubey-etal-2014-enrichment,P99-1067,0,\N,Missing
dubey-etal-2014-enrichment,J05-4003,0,\N,Missing
dubey-etal-2014-enrichment,D10-1025,0,\N,Missing
dubey-etal-2014-enrichment,P06-1011,0,\N,Missing
dubey-etal-2014-enrichment,I13-1163,0,\N,Missing
dubey-etal-2014-enrichment,barker-gaizauskas-2012-assessing,0,\N,Missing
dubey-etal-2014-enrichment,W11-3501,0,\N,Missing
E14-1044,Q13-1013,0,0.034221,"rpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, representation of the textual item at hand (be it a sentence or a document). Following this trend, in this paper we provide a knowledge-based representation of documents which goes beyond the lexical surface of text, while at the same time avoiding the need for training in a cross-language setting. To achieve this we leverage a multilingual semantic network, i.e., BabelNet, to obtain language-independent representations, which contain concepts together with semantic relations bet"
E14-1044,D09-1092,0,0.111807,"Missing"
E14-1044,J05-4003,0,0.0385517,"retrieval and categorization. 1 Introduction The huge amount of text that is available online is becoming ever increasingly multilingual, providing an additional wealth of useful information. Most of this information, however, is not easily accessible to the majority of users because of language barriers which hamper the cross-lingual search and retrieval of knowledge. Today’s search engines would benefit greatly from effective techniques for the cross-lingual retrieval of valuable information that can satisfy a user’s needs by not only providing (Landauer and Littman, 1994) and translating (Munteanu and Marcu, 2005) relevant results into different languages, but also by reranking the results in a language of interest on the basis of the importance of search results in other languages. Vector-based models are typically used in the literature for representing documents both in monolingual and cross-lingual settings (Manning et al., 2008). However, because of the large size of the vocabulary, having each term as a component of the vector makes the document representation very sparse. To address this issue several approaches to dimensionality reduction have been proposed, such as Principal Component Analysis"
E14-1044,W07-0711,0,0.0843323,"comparable document in the other language. We compared KBSim against the state-of-the-art supervised models S2Net, OPCA, CCA, and CLLSI (cf. Section 2). In contrast to these models, KBSim does not need a training step, so we applied it directly to the testing partition. In addition we also included the results of CL-ESA7 , CL-C3G8 and two simple vector-based models which translate all documents into English on a word-by-word basis and compared them using cosine similarity: the first model (CosSimE ) uses a statistical dictionary trained with Europarl using Wavelet-Domain Hidden Markov Models (He, 2007), a model similar to IBM Model 4; the second model (CosSimBN ) instead uses Algorithm 1 to translate the vectors with BabelNet. 0 KBSim(d, d ) = c(G)Sg (G, G ) + (1 − c(G))Sv (~ vLL0 , ~ vLL0 ), (8) where c(G) is an interpolation factor calculated as the edge density of knowledge graph G: c(G) = |E(G)| . |V (G)|(|V (G) |− 1) (9) Note that, using the factor c(G) to interpolate the two similarities in Eq. 8, we determine the relevance for the knowledge graphs and the multilingual vectors in a dynamic way. Indeed, c(G) makes the contribution of graph similarity depend on the richness of the knowl"
E14-1044,W11-0329,0,0.13096,"puter Science Sapienza Universit`a di Roma, Italy {francosalvador,navigli}@di.uniroma1.it 2 Natural Language Engineering Lab - PRHLT Research Center Universitat Polit`ecnica de Val`encia, Spain {mfranco,prosso}@dsic.upv.es Abstract 1994), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multi"
E14-1044,D10-1025,0,0.0880472,"1 Department of Computer Science Sapienza Universit`a di Roma, Italy {francosalvador,navigli}@di.uniroma1.it 2 Natural Language Engineering Lab - PRHLT Research Center Universitat Polit`ecnica de Val`encia, Spain {mfranco,prosso}@dsic.upv.es Abstract 1994), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador e"
E14-1044,W05-0822,0,0.0257559,"419 Model S2Net KBSim OPCA CosSimE CosSimBN CCA CL-LSI CL-ESA CL-C3G Dimension 2000 N/A 2000 N/A N/A 1500 5000 15000 N/A Accuracy 0.7447 0.7342 0.7255 0.7033 0.7029 0.6894 0.5302 0.2660 0.2511 MMR 0.7973 0.7750 0.7734 0.7467 0.7550 0.7378 0.6130 0.3305 0.3025 KBSim Full MT CosSimBN OPCA CCA CL-LSI CosSimE N/A 50 N/A 100 150 5000 N/A EN News Accuracy 0.8189 0.8483 0.8023 0.8412 0.8388 0.8401 0.8046 ES News Accuracy 0.6997 0.6484 0.6737 0.5954 0.5323 0.5105 0.4481 ble categories. In addition, each dataset of news is translated into the other four languages using the Portage translation system (Sadat et al., 2005). As a result, we have five different multilingual datasets, each containing source news documents in one language and four sets of translated documents in the other languages. Each of the languages has an independent vocabulary. Document vectors in the collection are created using TFIDFbased weighting. and CL-LSI, OPCA performs better thanks to its improved projection method using a noise covariance matrix, which enables it to obtain the main components in a low-dimensional space. CL-C3G and CL-ESA obtain the lowest results. Considering that English and Spanish do not have many lexical simila"
E17-2089,D14-1142,0,0.184612,"n categories generally recognised with keywords alone. In addition, the cross-domain variant has the additional difficulty of using a different vocabulary among domains. This problem is usually drawn by means of domain adaptation techniques (Ben-David et al., 2007). Most of these techniques exploit pivot features that allow to map vocabularies among domains. String kernels are known for their good performance in text classification (Lodhi et al., 2002). Recent works with this representation demonstrated its excellent capacity to capture lexical peculiarities of text (Popescu and Grozea, 2012; Ionescu et al., 2014). In this work we study the single and cross-domain polarity classification tasks from the string kernels perspective. The research questions we aim to answer are: The polarity classification task aims at automatically identifying whether a subjective text is positive or negative. When the target domain is different from those where a model was trained, we refer to a cross-domain setting. That setting usually implies the use of a domain adaptation method. In this work, we study the single and cross-domain polarity classification tasks from the string kernels perspective. Contrary to classical"
E17-2089,W02-1011,0,0.0171186,"t features, we do not use the target domain for training. Our approach detects the lexical peculiarities that characterise the text polarity and maps them into a domain independent space by means of kernel discriminant analysis. Experimental results show state-of-the-art performance in single and cross-domain polarity classification. 1 Introduction The polarity classification task, also known as (binary) polarity or sentiment categorisation, aims at identifying whether a subjective text is positive or negative depending on the overall sentiment detected. Single domain polarity classification (Pang et al., 2002) refers to the standard text classification setting (Sebastiani, 2002). The cross-domain level (Blitzer et al., 2007) refers to classify a different domain from that or those where a model was trained. These tasks have become especially important for business purposes. The vastness and accessibility of the Internet produced a new generation of event and product reviewers. These reviewers employ channels such as blogs, fora or social media. In consequence, companies are highly interested into identifying reviewers’ opinions on, for • What is the performance of string kernels for single and cros"
E17-2106,P14-1062,0,0.110174,"N-gram embeddings are fed to convolutional and max pooling layers, and the final classification is done via a softmax layer applied to the final text representation (_: whitespace in the input). As can be seen from Figure 1, we use a convolutional layer with different widths w, allowing us to capture patterns that involve everything from morphemes to words. We then pool the resulting feature maps f by max-over-time pooling (Collobert et al., 2011), to obtain yk , the maximum value of each feature map fk : tional approach to CNN that uses either a sequence of words or a sequence of characters (Kalchbrenner et al., 2014; Kim, 2014; Collobert et al., 2011; Zhang et al., 2015). This CNN captures local interactions at the character level, which are then aggregated to learn high-level patterns for modeling the style of an author. The main contributions of this paper are: yk = max fk [i], k = 1 . . . m i where m is the number of feature maps. This allows us to represent the text by its most important features, independent of their position. After pooling and concatenating the feature representations yk , we obtain a compact representation of the text. Finally, this representation is passed through a fully connect"
E17-2106,P15-1150,0,0.102148,"Missing"
E17-2106,D15-1167,0,0.0390136,"Missing"
E17-2106,N16-1082,0,0.0198297,"k by Bagnall (2015) uses a multi-headed Recurrent Neural Network (RNN) character language model that gives a set of next character probabilities for each author at every step of the model. This was the best-performing system for the PAN 2015 author identification task with a macro-averaged area under the curve (AUC) of 0.628 (Stamatatos et al., 2015). Despite the promising results that CNNs and RNNs show, the results are not interpretable and few of these works attempt to analyze what the networks are actually learning. We try to get an insight into our model by using the saliency analysis by Li et al. (2016). We have also devised our own method of finding out the input n-grams that are overall most important to the model. As a solution to the problem of AA of short texts, we propose a neural network architecture that is able to learn the representation of the text starting from the character sequence. Our architecture is a CNN that uses a sequence of character n-grams as input. This contrasts with the tradiWe present a model to perform authorship attribution of tweets using Convolutional Neural Networks (CNNs) over character n-grams. We also present a strategy that improves model interpretability"
E17-2106,D13-1193,0,0.232759,"ting Systems and Industrial Engineering Dept. Universidad Nacional de Colombia Bogotá, Colombia {ssierral, fagonzalezo}@unal.edu.co Manuel Montes-y-Gómez Thamar Solorio Instituto Nacional de Dept. of Computer Science Astrofısica, Optica y Electronica University of Houston Puebla, Mexico Houston, TX, 77004 mmontesg@ccc.inoep.mx solorio@cs.uh.edu Abstract ods that dealt with AA of short text. However, we were able to find research in AA using traditional as well as related approaches. Character and word n-grams have been used as the core of many authorship attribution systems (Stamatatos, 2009; Schwartz et al., 2013; Layton et al., 2010). Character and word n-grams help determine the author of a document by capturing the syntax and style of an author. Considering deep learning approaches, we found one other work that uses CNNs for authorship attribution (Rhodes, 2015). However, they use word representations for larger texts rather than character representation for short texts. Additionally, work by Bagnall (2015) uses a multi-headed Recurrent Neural Network (RNN) character language model that gives a set of next character probabilities for each author at every step of the model. This was the best-perform"
J13-4005,C10-1005,1,0.811149,"Missing"
J13-4005,N03-1003,0,0.271833,"d guidelines used for its annotation are available at http://clic.ub.edu/corpus/en/paraphrases-en. The subsets of the MSRP and WRPA corpora annotated with the same typology are also available at this Web site. 10 http://staffwww.dcs.shef.ac.uk/people/T.Cohn/paraphrase corpus.html. 11 http://wicopaco.limsi.fr/. 925 Computational Linguistics Volume 39, Number 4 or meaning-altering. There also exist works where the focus is not to build a paraphrase corpus, but to create a paraphrase extraction or generation system, which ends up in also building a paraphrase collection, such as Barzilay and Lee (2003). Plagiarism detection experts are starting to turn their attention to paraphrasing. Burrows, Potthast, and Stein (2012) built the Webis Crowd Paraphrase Corpus by crowd-sourcing more than 4,000 manually simulated samples of paraphrase plagiarism.12 In order to create feasible mechanisms for crowd-sourcing paraphrase acquisition, they built a classifier to reject bad instances of paraphrase plagiarism (e.g., cases of verbatim plagiarism). These crowd-sourced instances are similar to the cases of simulated plagiarism in the PAN-PC-10 corpus, and hence the P4P (see the following). P4P was built"
J13-4005,P01-1008,0,0.0241186,"Missing"
J13-4005,P99-1071,0,0.0313701,"Missing"
J13-4005,clough-etal-2002-building,0,0.778948,"Missing"
J13-4005,J08-4005,0,0.0117388,"Missing"
J13-4005,I05-5002,0,0.0158835,"Missing"
J13-4005,max-wisniewski-2010-mining,0,0.0182925,"Missing"
J13-4005,C10-2115,1,0.925888,"Missing"
L16-1683,P10-2052,1,0.86414,"Missing"
L16-1683,A92-1018,0,0.394621,"Missing"
L16-1683,W00-0730,0,0.0673491,"Missing"
L18-1615,W14-1214,0,0.241936,"Missing"
L18-1615,W11-1603,0,0.00995647,"English and Spanish. At the beginning of 2016, the Newsela corpora contained around 2,000 original news articles in English and around 250 original news articles in Spanish (both with their corresponding manually simplified versions at four different simplification levels). The current state-of-the-art systems for automatic sentencealignment of original and manually simplified text are the Greedy Structural WikNet (GSWN) method (Hwang et al., 2015) used for sentence-alignment of original and simple English Wikipedia, and the HMM-based (using Hidden Markov Model and Viterbi algorithm) method (Bott and Saggion, 2011) used for sentence-alignment of the Spanish Simplext corpus (Saggion et al., 2015). The HMMbased method can be applied to any language as it does not require any language-specific resources. It is based on two hypothesis: (H1) that the original order of information is preserved, and (H2) that every ‘simple’ sentence has a corresponding ‘original’ sentence. The GSWN method does not assume H1 or H2, but it only allows for ‘1-1’ sentence alignments (which is very restricting for TS) and it is language-dependent as it requires the English Wiktionary2 . In this paper, we present a freely available"
L18-1615,W12-2910,0,0.199518,"offsets of ‘complex’ text snippets that correspond to the previous and the next aligned ‘simple’ snippets. For instance, if L = {(s1 , c4 ), (s3 , c7 )} and U = {s2 }, then the search space for the alignments of s2 is reduced to {c4 ...c7 }. We denote the MST-LIS alignment strategy by adding ‘*’ to the name of the similarity method (e.g. C3G*). simpler versions (at different levels of simplification) for each of them, and sentence-aligned them with seven different alignment strategies offered by the CATS tool: C3G, C3G*, CWASA, CWASA*, WAVG, WAVG*, C3G-2step, and the HMM-based alignment tool (Bott et al., 2012). Then we asked two native speakers of English (first trained on additional 3 original articles and their corresponding simplified versions) and two native speakers of Spanish (first trained in the same manner) to classify the obtained sentence pairs (a total of approx. 3,500 sentence-pairs for each language) in one of the four classes: 2.2.1. Modeling Sentence Splitting and Compression In both alignment strategies (MST and MST-LIS), we allow the same original sentence to be aligned with multiple simple sentences, in order to allow for modeling both sentence splitting and sentence compression"
L18-1615,N15-1022,0,0.242225,"ntences and their manual simplifications. The parallel TS corpus for Brazilian Portuguese, compiled for the purposes of the PorSimples project (Alu´ısio et al., 2008) contains around 4,500 aligned sentences, and the parallel TS corpus for Spanish, compiled for the purposes of the Simplext project (Saggion et al., 2015) contains only around 1,000 aligned sentences. The largest existing TS comparable corpora is the English Wikipedia – Simple English Wikipedia (EW–SEW), consisting of 170,000 sentence pairs (Kauchak, 2013), or 150,000 full matches and 130,000 partial matches in the newer version (Hwang et al., 2015). In both cases, the sentences were automatically aligned from comparable English Wikipedia and Simple English Wikipedia articles. However, the use of EW–SEW dataset for modeling TS has been disputed (Amancio and ˇ Specia, 2014; Stajner et al., 2015; Xu et al., 2015) for several reasons: (1) the simplified articles are not necessarily direct simplifications of the original articles; (2) the quality of simplifications is not checked; (3) the dataset does not cover sentence splitting which is one of the most common operations in text simplification. The Newsela corpora1 of document-aligned news"
L18-1615,P13-1151,0,0.145199,"TS is the scarcity and limited size of parallel TS corpora which would contain original sentences and their manual simplifications. The parallel TS corpus for Brazilian Portuguese, compiled for the purposes of the PorSimples project (Alu´ısio et al., 2008) contains around 4,500 aligned sentences, and the parallel TS corpus for Spanish, compiled for the purposes of the Simplext project (Saggion et al., 2015) contains only around 1,000 aligned sentences. The largest existing TS comparable corpora is the English Wikipedia – Simple English Wikipedia (EW–SEW), consisting of 170,000 sentence pairs (Kauchak, 2013), or 150,000 full matches and 130,000 partial matches in the newer version (Hwang et al., 2015). In both cases, the sentences were automatically aligned from comparable English Wikipedia and Simple English Wikipedia articles. However, the use of EW–SEW dataset for modeling TS has been disputed (Amancio and ˇ Specia, 2014; Stajner et al., 2015; Xu et al., 2015) for several reasons: (1) the simplified articles are not necessarily direct simplifications of the original articles; (2) the quality of simplifications is not checked; (3) the dataset does not cover sentence splitting which is one of th"
L18-1615,P15-2135,1,0.888286,"Missing"
L18-1615,P17-2016,1,0.914231,"pecia, 2014; Stajner et al., 2015; Xu et al., 2015) for several reasons: (1) the simplified articles are not necessarily direct simplifications of the original articles; (2) the quality of simplifications is not checked; (3) the dataset does not cover sentence splitting which is one of the most common operations in text simplification. The Newsela corpora1 of document-aligned news texts, manually simplified at four different simplification levels have been freely available for a few years for research purposes. These corpora have several advantages over the EW– ˇ SEW dataset (Xu et al., 2015; Stajner et al., 2017): (1) simplified texts present direct simplifications of the original articles; (2) simplification was performed by trained human editors, following strict guidelines; (3) by sentencealigning those corpora one can get training material for simplifications at various levels, i.e. train different simplification models depending on the intended reader group; and (4) they provide comparable training material in two languages, English and Spanish. At the beginning of 2016, the Newsela corpora contained around 2,000 original news articles in English and around 250 original news articles in Spanish ("
moreau-etal-2010-evaluation,gravier-etal-2004-ester,0,\N,Missing
moreau-etal-2010-evaluation,mostefa-etal-2006-evaluation,1,\N,Missing
moreau-etal-2010-evaluation,galliano-etal-2006-corpus,0,\N,Missing
P10-2052,D08-1063,1,0.824843,", 2004), the author uses projections from English into Arabic to bootstrap a sense tagging system for Arabic as well as a seed Arabic WordNet through projection. In (Hwa et al., 2002), the authors report promising results of inducing Chinese dependency trees from English. The obtained model outperformed the baseline. More recently, in (Chen and Ji, 2009), the authors report their comparative study between monolingual and cross-lingual bootstrapping. Finally, in Mention Detection (MD), a task which includes NER and adds the identification and classification of nominal and pronominal mentions, (Zitouni and Florian, 2008) show the impact of using a MT system to enhance the performance of an Arabic MD model. The authors report an improvement of up to 1.6F when the baseline system uses lexical features only. Unlike the work we present here, their approach requires the availability of an accurate MT system which is a more expensive process. the all the features except the syntagmatic ones (All-Synt.) contrasted against the system including the semantic features, i.e. All the features, per class All . The baseline results, FreqBaseline, assigns a test token the most frequent tag observed for it in the gold trainin"
P10-2052,W03-2201,0,0.0192083,"highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute). 1 2 Introduction Named Entity Recognition (NER) has earned an important place in Natural Language Processing (NLP) as an enabling process for other tasks. When explicitly taken into account, research shows that it helps such applications achieve better performance levels (Babych and Hartley, 2003; Thompson and Dozier, 1997). NER is defined as the computational identification and classification of Named Entities (NEs) in running text. For instance, consider the following text: Barack Obama is visiting the Middle East. A NER system should be able to identify Barack Obama and Middle East as NEs and classify them as Person (PER) and Geo-Political Entity (GPE), respectively. The class-set used to tag NEs may vary according to user needs. In this research, we adopt the Automatic Content Extraction (ACE) 2007 nomenclature1 . According to (Nadeau and Sekine, 2007), optimization of the feature"
P10-2052,D08-1030,1,0.902274,"class-set used to tag NEs may vary according to user needs. In this research, we adopt the Automatic Content Extraction (ACE) 2007 nomenclature1 . According to (Nadeau and Sekine, 2007), optimization of the feature set is the key component in enhancing the performance of a global NER system. In this paper we investigate the possibility of building a high performance Arabic NER system by using a large space of available feature sets that go beyond the explored shallow feature sets used to date in the literature for Arabic NER. 1 Our Approach We use our state-of-the-art NER system described in (Benajiba et al., 2008) as our baseline system (BASE) since it yields, to our knowledge, the best performance for Arabic NER . BASE employs Support Vector Machines (SVMs) and Conditional Random Fields (CRFs) as Machine Learning (ML) approaches. BASE uses lexical, syntactic and morphological features extracted using highly accurate automatic Arabic POS-taggers. BASE employs a multi-classifier approach where each classifier is tagging a NE class separately. The feature selection is performed by using an incremental approach selecting the top n features (the features are ranked according to their individual impact) at"
P10-2052,W09-2209,0,0.0116509,"81.42 76.07 54.49 81.82 75.92 55.65 81.31 75.30 57.30 81.73 75.67 58.11 Table 2: Final Results obtained with selected features contrasted against all features combined works, they augment training data from parallel data for training supervised systems. In (Diab, 2004), the author uses projections from English into Arabic to bootstrap a sense tagging system for Arabic as well as a seed Arabic WordNet through projection. In (Hwa et al., 2002), the authors report promising results of inducing Chinese dependency trees from English. The obtained model outperformed the baseline. More recently, in (Chen and Ji, 2009), the authors report their comparative study between monolingual and cross-lingual bootstrapping. Finally, in Mention Detection (MD), a task which includes NER and adds the identification and classification of nominal and pronominal mentions, (Zitouni and Florian, 2008) show the impact of using a MT system to enhance the performance of an Arabic MD model. The authors report an improvement of up to 1.6F when the baseline system uses lexical features only. Unlike the work we present here, their approach requires the availability of an accurate MT system which is a more expensive process. the all"
P10-2052,P02-1033,1,0.803187,"Missing"
P10-2052,P05-1071,0,0.0608239,"ganization NE class (ORG) lexica; 4. POS-tag and Base Phrase Chunk (BPC): automatically tagged using AMIRA (Diab et al., 2007) which yields Fmeasures for both tasks in the high 90’s; 5. Morphological features: automatically tagged using the Morphological Analysis and Disambiguation for Arabic (MADA) tool to extract information about gender, number, person, definiteness and ashttp://www.nist.gov/speech/tests/ace/index.htm 281 Proceedings of the ACL 2010 Conference Short Papers, pages 281–285, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics pect for each word (Habash and Rambow, 2005); 6. Capitalization: derived as a side effect from running MADA. MADA chooses a specific morphological analysis given the context of a given word. As part of the morphological information available in the underlying lexicon that MADA exploits. As part of the information present, the underlying lexicon has an English gloss associated with each entry. More often than not, if the word is a NE in Arabic then the gloss will also be a NE in English and hence capitalized. We devise an extended Arabic NER system (EXTENDED) that uses the same architecture as BASE but employs additional features to thos"
P10-2052,P03-1058,0,0.0362183,"Missing"
P10-2052,W97-0315,0,0.583534,"R system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute). 1 2 Introduction Named Entity Recognition (NER) has earned an important place in Natural Language Processing (NLP) as an enabling process for other tasks. When explicitly taken into account, research shows that it helps such applications achieve better performance levels (Babych and Hartley, 2003; Thompson and Dozier, 1997). NER is defined as the computational identification and classification of Named Entities (NEs) in running text. For instance, consider the following text: Barack Obama is visiting the Middle East. A NER system should be able to identify Barack Obama and Middle East as NEs and classify them as Person (PER) and Geo-Political Entity (GPE), respectively. The class-set used to tag NEs may vary according to user needs. In this research, we adopt the Automatic Content Extraction (ACE) 2007 nomenclature1 . According to (Nadeau and Sekine, 2007), optimization of the feature set is the key component in"
P17-2016,J03-1002,0,0.0481646,"Missing"
P17-2016,W10-1607,0,0.0716558,"Missing"
P17-2016,W16-4912,0,0.0987186,"in {sanja,simone,heiner}@informatik.uni-mannheim.de marc.franco@symanto.net, prosso@prhlt.upv.es 1 Abstract (EW–SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (Hwang et al., 2015). For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches. The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings (Glavaˇs ˇ and Stajner, 2015; Paetzold and Specia, 2016) seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning. However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far. The Newsela corpora1 offers over 2,000 original news articles in English and around 250 in Spanish, manually simplified to 3–4 different complexity levels following strict guidelines (Xu et al., 2015). Although it was suggested that it has better quality than the EW–SEW corpus (Xu et al., 2015), Newsela has not yet been used for training end-to-end ATS syste"
P17-2016,W11-1603,0,0.346565,"ish is the English Wikipedia – Simple English Wikipedia 1 Freely available: https://newsela.com/data/ 97 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2016 3 able software;2 (2) compare the performances of lexically- and semantically-based alignment methods across various text complexity levels; (3) test the hypothesis that the original order of information is preserved during manual simplification (Bott and Saggion, 2011) by offering customized MST-LIS alignment strategy (Section 3.1); and (4) show that the new sentence-alignments lead to the state-of-the-art ATS systems even in a basic phrase-based statistical machine translation (PBSMT) approach to text simplifications. 2 Approach Having a set of ‘simple’ text snippets S and a set of ‘complex’ text snippets C, we offer two strategies (Section 3.1) to obtain the alignments (si , cj ), where si ∈ S, cj ∈ C. Each alignment strategy, in turn, can use one of the three methods (Section 3.2) to calculate similarity scores between text snippets (either paragraphs or"
P17-2016,P15-2011,1,0.868371,"Missing"
P17-2016,P08-1040,0,0.0631219,"even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems. 1 Introduction Automated text simplification (ATS) tries to automatically transform (syntactically, lexically and/or semantically) complex sentences into their simpler variants without significantly altering the original meaning. It has attracted much attention recently as it could make texts more accessible to wider audiences (Alu´ısio and Gasperin, 2010; Saggion et al., 2015), and used as a pre-processing step, improve performances of various NLP tasks and systems (Vickrey and Koller, 2008; Evans, 2011; ˇ Stajner and Popovi´c, 2016). However, the state-of-the-art ATS systems still do not reach satisfying performances and require ˇ some human post-editing (Stajner and Popovi´c, 2016). While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences (Speˇ cia, 2010; Stajner et al., 2015), probably due to the limited size or bad quality of parallel TS corpora used for training. The largest existing sentence-aligned TS dataset for English is the English Wikipedia – Simpl"
P17-2016,N15-1022,0,0.197548,"ext Simplification Systems 1 ˇ Sanja Stajner , Marc Franco-Salvador2,3 , Simone Paolo Ponzetto1 , Paolo Rosso3 , Heiner Stuckenschmidt1 DWS Research Group, University of Mannheim, Germany 2 Symanto Research, Nuremberg, Germany 3 PRHLT Research Center, Universitat Polit`ecnica de Val`encia, Spain {sanja,simone,heiner}@informatik.uni-mannheim.de marc.franco@symanto.net, prosso@prhlt.upv.es 1 Abstract (EW–SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (Hwang et al., 2015). For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches. The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings (Glavaˇs ˇ and Stajner, 2015; Paetzold and Specia, 2016) seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning. However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far. The Newsela corpora1 offers over 2,000 original news articl"
P17-2016,P15-2135,1,0.883593,"xts more accessible to wider audiences (Alu´ısio and Gasperin, 2010; Saggion et al., 2015), and used as a pre-processing step, improve performances of various NLP tasks and systems (Vickrey and Koller, 2008; Evans, 2011; ˇ Stajner and Popovi´c, 2016). However, the state-of-the-art ATS systems still do not reach satisfying performances and require ˇ some human post-editing (Stajner and Popovi´c, 2016). While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences (Speˇ cia, 2010; Stajner et al., 2015), probably due to the limited size or bad quality of parallel TS corpora used for training. The largest existing sentence-aligned TS dataset for English is the English Wikipedia – Simple English Wikipedia 1 Freely available: https://newsela.com/data/ 97 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2016 3 able software;2 (2) compare the performances of lexically- and semantically-based alignment meth"
P17-2016,P07-2045,0,0.0103568,"3G-2s), which first aligns paragraphs using the best paragraph-alignment method (C3G) and then within each paragraph align sentences with the best sentence-alignment method (C3G), led to more good+partial alignments than the ‘direct’ sentence-alignment C3G method. 5 Extrinsic Evaluation Finally, we test our new English Newsela (C3G2s) sentence-alignments (both for the neighbouring levels – neighb. and for all levels – all) and Newsela sentence-alignments for neighboring levels obtained with HMM-method6 (Bott and Saggion, 2011) in the ATS task using standard PBSMT models7 in the Moses toolkit (Koehn et al., 2007). We vary the training dataset and the corpus used to build language models (LMs), while keeping always the same 2,000 sentence pairs for tuning (Xu et al., 2016) and the first 70 sentence 8 Both freely available from: https://github.com/ cocoxu/simplification/ 9 We use the output of the original SBMT (Xu et al., 2016) ˇ and LightLS (Glavaˇs and Stajner, 2015) systems, obtained from the authors. 10 Those cases in which the two annotators did not agree are additionally evaluated by a third annotator to obtain majority. 6 Given that the performance of the HMM-method was poor for non-neighboring"
P17-2016,W16-3411,1,0.914076,"Missing"
P17-2016,N03-1017,0,0.0759083,"Missing"
P17-2016,Q16-1029,0,0.186509,"he HMM method on the paragraph-alignment 0–1 task, there is still significant difference in the obtained scores (in some cases, our methods led to good matches whereas the HMM only led to partial matches). 99 Sentence 0–1 0–4 3–4 C3G 98.3 56.1 81.1 C3G* 96.7 54.7 78.8 CWASA 98.3 45.3 79.7 CWASA* 96.1 42.1 76.4 WAVG 97.8 56.1 79.7 WAVG* 96.1 50.0 79.7 C3G-2s 98.5 57.8 83.5 HMM 86.2 25.2 65.6 Method pairs of their test set8 for our human evaluation. Using that particular test set allow us to compare our (PBSMT) systems with the output of the stateof-the-art syntax-based MT (SBMT) system for TS (Xu et al., 2016) which is not freely available. We compare: (1) the performance of the standard PBSMT model which uses only the already available EW–SEW dataset (Hwang et al., 2015) with the performances of the same PBSMT models but this time using the combination of the EW–SEW dataset and our newly-created Newsela datasets; (2) the latter PBSMT models (which use both EW–SEW and new Newsela datasets) against the state-of-the-art supervised ATS system (Xu et al., 2016), and one of the recently proposed unsupervised lexical simplification systems, the LightLS ˇ system (Glavaˇs and Stajner, 2015).9 We perform th"
panicheva-etal-2010-personal,W02-1011,0,\N,Missing
perez-estruch-etal-2017-learning,D11-1120,0,\N,Missing
reyes-etal-2010-evaluating,E06-2031,0,\N,Missing
reyes-etal-2010-evaluating,W04-2214,0,\N,Missing
reyes-etal-2010-evaluating,P05-3029,0,\N,Missing
reyes-etal-2010-evaluating,W02-1011,0,\N,Missing
reyes-etal-2010-evaluating,strapparava-valitutti-2004-wordnet,0,\N,Missing
reyes-etal-2010-evaluating,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
roshchina-etal-2012-evaluating,W11-3711,1,\N,Missing
S07-1096,W04-2406,0,0.015899,"hown to improve the baseline results if we carefully select the external resource to use, with a priori knowledge of the domain and the broadness of the corpus (wide or narrow domain). Evenmore, we have to be sure that the Lexical Data Base (LDB) has been suitable constructed. Due to the last facts, we consider that the use of a self automatically constructed LDB (using the same test corpora), may be of high benefit. This assumption is based on the intrinsic properties extracted from the corpus itself. Our proposal is related somehow with the investigations presented in (Sch¨ utze, 1998) and (Purandare and Pedersen, 2004), where words are also expanded with co-ocurrence terms for word sense discrimination. The main difference consists in the use of the same corpora for constructing the co-ocurrence list. Following we describe the self term expansion method used and, thereafter, the results obtained in the task #2 of Semeval 2007 competition. 431 2 The Self Term Expansion Method In literature, co-ocurrence terms is the most common technique used for automatic construction of LDBs (Grefenstette, 1994; Frakes and Baeza-Yates, 1992). A simple approach may use n-grams, which allows to predict a word from previous w"
S07-1096,S07-1002,0,0.0217506,"Missing"
S07-1096,W06-3814,0,0.0241954,", 2003) for our experiments, defining the average of similarities among all the sentences for a given ambiguous word as the stop criterion for this clustering method. The input similarity matrix for the clustering method was calculated by using the Jaccard coefficient. 3 Evaluation The task organizers decided to use two different measures for evaluating the runs submitted to the task. The first measure is called unsupervised one, and it is based on the Fscore measure. Whereas the second measure is called supervised recall. For further information on how these measures are calculated refer to (Agirre et al., 2006a; Agirre et al., 2006b). Since these measures give conflicting information, two different evaluation results are reported in this paper. In Table 2 we may see our ranking and the Fscore measure obtained (UPV-SI). We also show the best and worst team Fscores; as well as the 432 total average and two baselines proposed by the task organizers. The first baseline (Baseline1) assumes that each ambiguous word has only one sense, whereas the second baseline (Baseline2) is a random assignation of senses. We are ranked as third place and our results are better scored than the other teams except for th"
S07-1096,W06-1669,0,0.0145781,", 2003) for our experiments, defining the average of similarities among all the sentences for a given ambiguous word as the stop criterion for this clustering method. The input similarity matrix for the clustering method was calculated by using the Jaccard coefficient. 3 Evaluation The task organizers decided to use two different measures for evaluating the runs submitted to the task. The first measure is called unsupervised one, and it is based on the Fscore measure. Whereas the second measure is called supervised recall. For further information on how these measures are calculated refer to (Agirre et al., 2006a; Agirre et al., 2006b). Since these measures give conflicting information, two different evaluation results are reported in this paper. In Table 2 we may see our ranking and the Fscore measure obtained (UPV-SI). We also show the best and worst team Fscores; as well as the 432 total average and two baselines proposed by the task organizers. The first baseline (Baseline1) assumes that each ambiguous word has only one sense, whereas the second baseline (Baseline2) is a random assignation of senses. We are ranked as third place and our results are better scored than the other teams except for th"
S07-1096,W95-0105,0,0.0496642,"n some context. In this approach, every word close to the one we would like to determine its correct sense is expanded with its different senses by using the WordNet lexicon (Fellbaum, 1998). Then, an overlapping factor is calculated in order to determine the correct sense of the ambiguous word. Different other approaches have made use of a similar procedure. By using dictionaries, the proposals presented in (Lesk, 1986; Wilks et al., 1990; Nancy and V´eronis, 1990) are the most sucessful in WSD. Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. Finally, in (Sussna, 1993; Resnik, 1995; Banerjee and Pedersen, 2002) the use of lexicons in WSD has been investigated. Although in some cases the knowledge resource seems not to be used strictly for term expansion, the aplication of co-occurrence terms is included in their algorithms. Like in information retrieval, the application of term expansion in WSD by using corelated terms has shown to improve the baseline results if we carefully select the external resource to use, with a priori knowledge of the domain and the broadness of the corpus (wide or narrow domain). Evenmore, we have to be sure that the Lexical Data Base (LDB) has"
S07-1096,J98-1004,0,0.0587661,"lated terms has shown to improve the baseline results if we carefully select the external resource to use, with a priori knowledge of the domain and the broadness of the corpus (wide or narrow domain). Evenmore, we have to be sure that the Lexical Data Base (LDB) has been suitable constructed. Due to the last facts, we consider that the use of a self automatically constructed LDB (using the same test corpora), may be of high benefit. This assumption is based on the intrinsic properties extracted from the corpus itself. Our proposal is related somehow with the investigations presented in (Sch¨ utze, 1998) and (Purandare and Pedersen, 2004), where words are also expanded with co-ocurrence terms for word sense discrimination. The main difference consists in the use of the same corpora for constructing the co-ocurrence list. Following we describe the self term expansion method used and, thereafter, the results obtained in the task #2 of Semeval 2007 competition. 431 2 The Self Term Expansion Method In literature, co-ocurrence terms is the most common technique used for automatic construction of LDBs (Grefenstette, 1994; Frakes and Baeza-Yates, 1992). A simple approach may use n-grams, which allow"
S07-1096,C92-2070,0,0.102974,"using a external knowledge database for determining the correct sense of a word given in some context. In this approach, every word close to the one we would like to determine its correct sense is expanded with its different senses by using the WordNet lexicon (Fellbaum, 1998). Then, an overlapping factor is calculated in order to determine the correct sense of the ambiguous word. Different other approaches have made use of a similar procedure. By using dictionaries, the proposals presented in (Lesk, 1986; Wilks et al., 1990; Nancy and V´eronis, 1990) are the most sucessful in WSD. Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. Finally, in (Sussna, 1993; Resnik, 1995; Banerjee and Pedersen, 2002) the use of lexicons in WSD has been investigated. Although in some cases the knowledge resource seems not to be used strictly for term expansion, the aplication of co-occurrence terms is included in their algorithms. Like in information retrieval, the application of term expansion in WSD by using corelated terms has shown to improve the baseline results if we carefully select the external resource to use, with a priori knowledge of the domain and the broadness of the corpus (wide"
S07-1097,W04-0820,1,0.879435,"Missing"
S07-1097,magnini-cavaglia-2000-integrating,0,0.04237,"Missing"
S07-1097,C96-1005,0,\N,Missing
S07-1097,W04-0860,0,\N,Missing
S13-1033,S12-1094,0,0.0652848,"Missing"
S13-1033,J06-3003,0,\N,Missing
S13-1033,S12-1080,0,\N,Missing
S15-2080,C10-1113,1,0.0610957,"Missing"
S16-1126,P98-1013,0,0.503337,"nts, question vs. related questions, or question vs. comments of related questions — with a set of similarities computed at two different levels: lexical and semantic. This representation allows us to estimate the relatedness between text pairs in terms of what is explicitly stated and what it means. Our lexical similarities employ representations such as word and character n-grams, and bag-of-words (BOW). The semantic similarities include the use of distributed word bidirectional alignments, distributed representations of text, knowledge graphs, and frames from the FrameNet lexical database (Baker et al., 1998). This type of dual representations have been successfully employed for question answering by the highest performing system in the previous edition of this Se814 Proceedings of SemEval-2016, pages 814–821, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics mEval task (Tran et al., 2015). Other Natural Language Processing (NLP) tasks such as cross-language document retrieval and categorization also benefited from similar representations (Franco-Salvador et al., 2014). In this task, if the question or comment includes multiple text fields, e.g. body and sub"
S16-1126,S15-2035,0,0.137294,"features such as syntactic relations and distributed word representations. Similarly to our work, the highest performing approach employed a combination of lexical and semanticbased similarity measures (Tran et al., 2015). Its semantic features included the use of probabilistic topic models, translation obfuscation-based alignments, and pre-computed distributed representations of words both generated with the word2vec1 and GloVe2 toolkits. Their lexical features included BOW, word alignments, and noun matching. They employed a regression model for classification. Another interesting approach, Hou et al. (2015), included textual features — word lengths and punctuation — in addition to syntactical-based features — Part-ofSpeech (PoS) tags. In this work we aim at differentiating from the other approaches by enhancing our ranking model with new similarity measures. These include the use of knowledge graphs obtained using the largest multilingual semantic network — BabelNet — frames from the FrameNet lexical database, and bidirectional distributed word alignments. 3 Lexical and Semantic-based Community Question Answering In this section we detailed the system that we designed for this CQA task. First in"
S16-1126,W10-1201,0,0.0229261,"Missing"
S16-1126,E14-1044,1,0.141883,"Missing"
S16-1126,W15-5403,1,0.650561,"ted the normalized count of common ngrams(n=1,2,3) between two texts. 3.1.2 Semantic Features The semantic features that we employed are the following: • Distributed representations of texts. We used the continuous Skip-gram model (Mikolov et al., 2013) of the word2vec toolkit to generate distributed representations of the words of the complete English Wikipedia.4 Next, for each text, e.g. question or comment, we averaged its word vectors in order to have a single representation of its content as this setting has shown good results in other NLP tasks (e.g. for language variety identification (Franco-Salvador et al., 2015a) and discriminating similar languages (Franco-Salvador et al., 2015b)). Finally, the similarity between texts, e.g. question vs. comment, is estimated using the cosine similarity. 3 http://www.nltk.org/ We used 200-dimensional vectors, context windows of size 10, and 20 negative words for each sample. 4 816 • Distributed word alignments. The use of word alignment strategies has been employed in the past for textual semantic relatedness (Hassan and Mihalcea, 2011). Tran et al. (2015) employed distributed representations to align the words of the question with the words of the comment. A more"
S16-1126,S15-2047,0,0.0705957,"Missing"
S16-1126,S16-1083,0,0.0963816,". 1 Introduction The key role that the Internet plays today for our society benefited the dawn of thousands of new Web social activities. Among those, forums emerged with special relevance following the paradigm of the Community Question Answering (CQA). These type of social networks allow people to post a question to other users of that community. The usage is simple, without much restrictions, and infrequently moderated. The popularity of CQA is a strong indicator that users receive some good and valuable answers. Details of the SemEval 2016 Task 3 on CQA can be found in the overview paper (Nakov et al., 2016). In this work we evaluate the three English-related Task 3 subtasks on CQA. We first represent each instance to rank — question versus (vs.) comments, question vs. related questions, or question vs. comments of related questions — with a set of similarities computed at two different levels: lexical and semantic. This representation allows us to estimate the relatedness between text pairs in terms of what is explicitly stated and what it means. Our lexical similarities employ representations such as word and character n-grams, and bag-of-words (BOW). The semantic similarities include the use o"
S16-1126,P02-1006,0,0.16122,"Missing"
S16-1126,S15-2038,0,0.258086,"employ representations such as word and character n-grams, and bag-of-words (BOW). The semantic similarities include the use of distributed word bidirectional alignments, distributed representations of text, knowledge graphs, and frames from the FrameNet lexical database (Baker et al., 1998). This type of dual representations have been successfully employed for question answering by the highest performing system in the previous edition of this Se814 Proceedings of SemEval-2016, pages 814–821, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics mEval task (Tran et al., 2015). Other Natural Language Processing (NLP) tasks such as cross-language document retrieval and categorization also benefited from similar representations (Franco-Salvador et al., 2014). In this task, if the question or comment includes multiple text fields, e.g. body and subject, similarities are estimated using all possible combinations (see Section 3.2). Finally, the ranking of instances is performed using a state-of-the-art machine-learned ranking algorithm: SVMrank . 2 Related Work Automatic question answering has been a popular interest of research in NLP from the beginning of the Internet"
S16-1126,C98-1013,0,\N,Missing
S18-1086,E14-3007,0,0.0270979,"olitecnica de Valencia, Spain prosso@dsic.upv.es Abstract 2013). Several approaches have been proposed to detect irony, where most of them have turned the problem into a binary classification task using a set of features. (Carvalho et al., 2009) proposed one of the first works on irony detection. They worked on the identification of a set of patterns to identify ironic sentences. The adopted features were the use of punctuation marks and emoticons. (Reyes et al., 2013) proposed a model that employed four types of conceptual features: signatures, unexpectedness, style and emotional scenarios. (Barbieri and Saggion, 2014) proposed a model using lexical features, such as frequency of rare and common terms, synonyms, adjectives, emoticons, punctuation marks, positive and negative terms. Their results showed that the most important features are structure, frequency and synonyms for detecting irony in multiple datasets. (Karoui et al., 2015) presented a model to detect irony using a vector composed of six main groups of features: surface features (such as punctuation marks), sentiment (positive and negative words), sentiment shifter (positive and negative words in the scope of an intensifier), shifter (presence a"
S18-1086,W15-5409,1,0.74386,"tection classification task. LDR was proposed for different application where its discriminative statistical features proved to be efficient for classification purposes. Therefore, in this task, we investigated the LDR efficiency in irony detection, where the implicit meaning of a sentence is required to identify the correct class type. We proposed the low dimensional representation (LDR) in (Rangel et al., 2017a). LDR has been used in multiple author profiling tasks (Rangel et al., 2017b; Litvinova et al., 2017), and especially in language variety identification (FrancoSalvador et al., 2015; Fabra et al., 2015). The key aspect of the LDR is the use of weights to represent the probability of each term belonging to each one of the different language varieties. In our approach we built a vector of features from a matrix of terms weights. Starting from a set of training documents, with using Tf-Idf weighting scheme, we built a matrix of terms weights where each row represents Tf-Idf terms weights of a document (specifically this row of term weights represents a unique class C of its document), and each column corresponds to a specific term. Therefore, we obtained another matrix where each term weight wa"
S18-1086,Y13-1035,0,0.0641392,"Missing"
S18-1086,P15-2106,0,0.0226938,"on of a set of patterns to identify ironic sentences. The adopted features were the use of punctuation marks and emoticons. (Reyes et al., 2013) proposed a model that employed four types of conceptual features: signatures, unexpectedness, style and emotional scenarios. (Barbieri and Saggion, 2014) proposed a model using lexical features, such as frequency of rare and common terms, synonyms, adjectives, emoticons, punctuation marks, positive and negative terms. Their results showed that the most important features are structure, frequency and synonyms for detecting irony in multiple datasets. (Karoui et al., 2015) presented a model to detect irony using a vector composed of six main groups of features: surface features (such as punctuation marks), sentiment (positive and negative words), sentiment shifter (positive and negative words in the scope of an intensifier), shifter (presence a negation word or reporting speech verbs), opposition (sentiment opposition or contrast between a subjective and an objective proposition) and internal contextual (the presence of personal pronouns). (Reyes et al., 2012) also studied the effect of multiple features to distinguish ironic and non-ironic tweets messages. The"
S18-1086,E17-1025,0,0.0722211,"terms weights of a document (specifically this row of term weights represents a unique class C of its document), and each column corresponds to a specific term. Therefore, we obtained another matrix where each term weight was built using the ratio between the weights of 3 Experiments and Results During the experiments, LDR was tested using different classifiers. In the following sections we will illustrate the experiments that we carried out. For the evaluation, we used both accuracy and macro-average F-score. Moreover, the error anal532 Dataset # of tweets # Positive # Negative SemEval2018 (Karoui et al., 2017) (Pt´acˇ ek et al., 2014) 3,834 1,911 1,923 540 540 - 67,779 18,889 48890 with the DecisionStump classifier. To improve the results, we adopted the Majority Vote (MV) algorithm using the results that were generated by the other two datasets with the training subset, each combined with the training part of task A subset in the 10-fold cross-validation. To note, none of the datasets improves the results more than using the task A subset independently. In spite of that, combining with Karoui et al. dataset achieved its highest results with Multilayer Perceptron classifier, and with Ptacek et al."
S18-1086,C14-1022,0,0.0755843,"Missing"
S18-1086,S18-1005,0,0.0481659,"Missing"
S18-1086,P15-1100,0,0.0717586,"Missing"
S18-1097,E17-1025,0,0.0204295,"ted with an ironic or non-ironic sense. A widely exploited measure in SA for developing lexica is the Pointwise Mutual Information (PMI) (Church and Hanks, 1990). We decided to adopt a similar strategy to generate two lists of terms associated to ironic and non-ironic senses. As starting point we took advantage of a set of corpora from the state of the art in irony detection (henceforth benchmark-corpora). The datasets we used are described in (Reyes et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Pt´acˇ ek et al., 2014; Mohammad et al., 2015; Ghosh et al., 2015; Sulis et al., 2016; Karoui et al., 2017). Overall, more than 165,000 tweets were used to generate the lists of words: ironic terms and nonironic terms. We calculate the PMI score for each term2 in the benchmark-corpora. After that, we selected only those terms with a PMI score greater than zero. In order to determine the class of an instance we assigned a vote (v) for each word (w) in a given tweet (t). First, we filter out the stopwords in each tweet. Then, we search for the most similar term in each of the lists in order to determine whether w is more related to an ironic or nonironic sense. Mainly we compute a score that indicate"
S18-1097,W14-2609,0,0.114542,"rice (1975), stating that the function of irony is to effectively communicate the opposite of the literal interpretation a given utterance. Nowadays, with the growing interest in irony detection, there are several approaches1 for addressing such an interesting task. Probably, the most widely used is that exploiting characteristics extracted from the text (such as n-grams, punctuation marks, part-of-speech labels, among others) on its own (Riloff et al., 2013; Pt´acˇ ek et al., 2014). Inherent aspects of irony such as its very subjective component have also been considered (Reyes et al., 2013; Barbieri et al., 2014; Hern´andez Far´ıas 2 Method Description In order to determine the presence of ironic content in tweets, we propose an ensemble of different methods, namely, a bag-of-words and word embeddings classifiers, as well as a voting scheme based on a list of potentially ironic and non-ironic terms. 2.1 Individual classifiers Ironic/nonironic Orientation (irO) This approach attempts to capture the ironic and non-ironic connotation of the words in a tweet in order to identify the presence of ironic content. Building a lexicon for irony detection is not a trivial task. It has been recognized in (Nozza"
S18-1097,J90-1003,0,0.212284,"2016) that a lexicon for irony detection can be derived by using a huge amount of data. 1 For a more comprehensive overview of irony detection, see (Joshi et al., 2017). 594 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 594–599 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics To develop a lexicon for irony detection it is needed to calculate how much a word could be associated with an ironic or non-ironic sense. A widely exploited measure in SA for developing lexica is the Pointwise Mutual Information (PMI) (Church and Hanks, 1990). We decided to adopt a similar strategy to generate two lists of terms associated to ironic and non-ironic senses. As starting point we took advantage of a set of corpora from the state of the art in irony detection (henceforth benchmark-corpora). The datasets we used are described in (Reyes et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Pt´acˇ ek et al., 2014; Mohammad et al., 2015; Ghosh et al., 2015; Sulis et al., 2016; Karoui et al., 2017). Overall, more than 165,000 tweets were used to generate the lists of words: ironic terms and nonironic terms. We calculate the PMI score fo"
S18-1097,S15-2080,1,0.829633,"lculate how much a word could be associated with an ironic or non-ironic sense. A widely exploited measure in SA for developing lexica is the Pointwise Mutual Information (PMI) (Church and Hanks, 1990). We decided to adopt a similar strategy to generate two lists of terms associated to ironic and non-ironic senses. As starting point we took advantage of a set of corpora from the state of the art in irony detection (henceforth benchmark-corpora). The datasets we used are described in (Reyes et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Pt´acˇ ek et al., 2014; Mohammad et al., 2015; Ghosh et al., 2015; Sulis et al., 2016; Karoui et al., 2017). Overall, more than 165,000 tweets were used to generate the lists of words: ironic terms and nonironic terms. We calculate the PMI score for each term2 in the benchmark-corpora. After that, we selected only those terms with a PMI score greater than zero. In order to determine the class of an instance we assigned a vote (v) for each word (w) in a given tweet (t). First, we filter out the stopwords in each tweet. Then, we search for the most similar term in each of the lists in order to determine whether w is more related to an ironic or nonironic sens"
S18-1097,W16-0425,0,0.0137154,"elia Irazu´ Hern´andez Far´ıas1 , Fernando S´anchez-Vega1 Manuel Montes-y-G´omez1,2 , and Paolo Rosso2 1 ´ Instituto Nacional de Astrof´ısica, Optica y Electr´onica (INAOE), Mexico 2 PRHLT Research Center, Universitat Polit`ecnica de Val`encia, Spain {dirazuherfa,fer.callotl,mmontesg}@inaoep.mx prosso@dsic.upv.es Abstract et al., 2016). Other methods have opted for taking advantage of information coming from the context in which a given utterance is produced (Rajadesingan et al., 2015). There are also some approaches exploiting deep learning techniques and word embeddings (Poria et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016; Nozza et al., 2016). A less explored strategy for addressing irony detection is the use of ensemble methods. Fersini et al. (2015) and Liu et al. (2014) compared the performance of ensemble approaches against traditional classifiers; the best results were obtained by the ensemble strategy setting. In this paper we describe our participation to the SemEval-2018 Task 3: Irony detection in English tweets (Van Hee et al., 2018). The INAOEUPV system explores the use of an ensemble approach that considers different combinations of three methods. The main contribution of our app"
S18-1097,C16-1151,0,0.0185885,"tection in Twitter Delia Irazu´ Hern´andez Far´ıas1 , Fernando S´anchez-Vega1 Manuel Montes-y-G´omez1,2 , and Paolo Rosso2 1 ´ Instituto Nacional de Astrof´ısica, Optica y Electr´onica (INAOE), Mexico 2 PRHLT Research Center, Universitat Polit`ecnica de Val`encia, Spain {dirazuherfa,fer.callotl,mmontesg}@inaoep.mx prosso@dsic.upv.es Abstract et al., 2016). Other methods have opted for taking advantage of information coming from the context in which a given utterance is produced (Rajadesingan et al., 2015). There are also some approaches exploiting deep learning techniques and word embeddings (Poria et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016; Nozza et al., 2016). A less explored strategy for addressing irony detection is the use of ensemble methods. Fersini et al. (2015) and Liu et al. (2014) compared the performance of ensemble approaches against traditional classifiers; the best results were obtained by the ensemble strategy setting. In this paper we describe our participation to the SemEval-2018 Task 3: Irony detection in English tweets (Van Hee et al., 2018). The INAOEUPV system explores the use of an ensemble approach that considers different combinations of three methods. The main"
S18-1097,C14-1022,0,0.150002,"so, 2016). There are several disciplines studying irony from different perspectives. The most prevalent definition is that from Grice (1975), stating that the function of irony is to effectively communicate the opposite of the literal interpretation a given utterance. Nowadays, with the growing interest in irony detection, there are several approaches1 for addressing such an interesting task. Probably, the most widely used is that exploiting characteristics extracted from the text (such as n-grams, punctuation marks, part-of-speech labels, among others) on its own (Riloff et al., 2013; Pt´acˇ ek et al., 2014). Inherent aspects of irony such as its very subjective component have also been considered (Reyes et al., 2013; Barbieri et al., 2014; Hern´andez Far´ıas 2 Method Description In order to determine the presence of ironic content in tweets, we propose an ensemble of different methods, namely, a bag-of-words and word embeddings classifiers, as well as a voting scheme based on a list of potentially ironic and non-ironic terms. 2.1 Individual classifiers Ironic/nonironic Orientation (irO) This approach attempts to capture the ironic and non-ironic connotation of the words in a tweet in order to id"
S18-1097,D13-1066,0,0.086198,"Missing"
S18-1097,D16-1104,0,0.0230041,"Far´ıas1 , Fernando S´anchez-Vega1 Manuel Montes-y-G´omez1,2 , and Paolo Rosso2 1 ´ Instituto Nacional de Astrof´ısica, Optica y Electr´onica (INAOE), Mexico 2 PRHLT Research Center, Universitat Polit`ecnica de Val`encia, Spain {dirazuherfa,fer.callotl,mmontesg}@inaoep.mx prosso@dsic.upv.es Abstract et al., 2016). Other methods have opted for taking advantage of information coming from the context in which a given utterance is produced (Rajadesingan et al., 2015). There are also some approaches exploiting deep learning techniques and word embeddings (Poria et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016; Nozza et al., 2016). A less explored strategy for addressing irony detection is the use of ensemble methods. Fersini et al. (2015) and Liu et al. (2014) compared the performance of ensemble approaches against traditional classifiers; the best results were obtained by the ensemble strategy setting. In this paper we describe our participation to the SemEval-2018 Task 3: Irony detection in English tweets (Van Hee et al., 2018). The INAOEUPV system explores the use of an ensemble approach that considers different combinations of three methods. The main contribution of our approach lies on the us"
S18-1097,S18-1005,0,0.0413805,"Missing"
S18-1105,de-albornoz-etal-2012-sentisense,0,0.0236835,"Missing"
S18-1105,W14-2609,0,0.316948,"sic.upv.es Abstract ally said (Sperber and Wilson, 1986). Situational irony, instead refers to a contradictory or unexpected outcome of events (Lucariello, 2014). In Twitter we can find many examples both of verbal irony and of posts where users describe aspects of an ironic situation. Most of the proposed approaches to the automatic detection of irony in social media (Riloff et al., 2013; Buschmeier et al., 2014; Pt´acˇ ek et al., 2014)take advantage of lexical factors such as n-grams, punctuation marks, among others. Information related to affect has been also exploited (Reyes et al., 2013; Barbieri et al., 2014; Hern´andez Far´ıas et al., 2015). Other scholars proposed methods exploiting the context surrounding an ironic utterance (Wallace et al., 2015; Karoui et al., 2015). Recently, also deep learning techniques have been applied (Nozza et al., 2016; Poria et al., 2016). This paper describes our participation in the SemEval-2018 Task 3. The aim of this task is to identify ironic tweets. ValenTO exploited an extended version of emotIDM (Hern´andez Far´ıas et al., 2016), an irony detection model based mainly on affective information. In particular, we experimented the use of a wide range of affectre"
S18-1105,W14-2608,0,0.0231219,"ica (INAOE) Mexico Viviana Patti Dip. di Informatica University of Turin Italy Paolo Rosso PRHLT Research Center Universitat Polit`ecnica de Val`encia Spain dirazuherfa@hotmail.com patti@di.unito.it prosso@dsic.upv.es Abstract ally said (Sperber and Wilson, 1986). Situational irony, instead refers to a contradictory or unexpected outcome of events (Lucariello, 2014). In Twitter we can find many examples both of verbal irony and of posts where users describe aspects of an ironic situation. Most of the proposed approaches to the automatic detection of irony in social media (Riloff et al., 2013; Buschmeier et al., 2014; Pt´acˇ ek et al., 2014)take advantage of lexical factors such as n-grams, punctuation marks, among others. Information related to affect has been also exploited (Reyes et al., 2013; Barbieri et al., 2014; Hern´andez Far´ıas et al., 2015). Other scholars proposed methods exploiting the context surrounding an ironic utterance (Wallace et al., 2015; Karoui et al., 2015). Recently, also deep learning techniques have been applied (Nozza et al., 2016; Poria et al., 2016). This paper describes our participation in the SemEval-2018 Task 3. The aim of this task is to identify ironic tweets. ValenTO e"
S18-1105,P15-2106,0,0.0154113,"witter we can find many examples both of verbal irony and of posts where users describe aspects of an ironic situation. Most of the proposed approaches to the automatic detection of irony in social media (Riloff et al., 2013; Buschmeier et al., 2014; Pt´acˇ ek et al., 2014)take advantage of lexical factors such as n-grams, punctuation marks, among others. Information related to affect has been also exploited (Reyes et al., 2013; Barbieri et al., 2014; Hern´andez Far´ıas et al., 2015). Other scholars proposed methods exploiting the context surrounding an ironic utterance (Wallace et al., 2015; Karoui et al., 2015). Recently, also deep learning techniques have been applied (Nozza et al., 2016; Poria et al., 2016). This paper describes our participation in the SemEval-2018 Task 3. The aim of this task is to identify ironic tweets. ValenTO exploited an extended version of emotIDM (Hern´andez Far´ıas et al., 2016), an irony detection model based mainly on affective information. In particular, we experimented the use of a wide range of affectrelated features for characterizing the presence of ironic content, covering different facets of affect, from sentiment to finer-grained emotions. Most theorist (Grice,"
S18-1105,D14-1125,0,0.0318181,"Missing"
S18-1105,L16-1283,0,0.0462458,"Missing"
S18-1105,S18-1005,0,0.0397377,"Missing"
S18-1105,C16-1151,0,0.0192356,"ironic situation. Most of the proposed approaches to the automatic detection of irony in social media (Riloff et al., 2013; Buschmeier et al., 2014; Pt´acˇ ek et al., 2014)take advantage of lexical factors such as n-grams, punctuation marks, among others. Information related to affect has been also exploited (Reyes et al., 2013; Barbieri et al., 2014; Hern´andez Far´ıas et al., 2015). Other scholars proposed methods exploiting the context surrounding an ironic utterance (Wallace et al., 2015; Karoui et al., 2015). Recently, also deep learning techniques have been applied (Nozza et al., 2016; Poria et al., 2016). This paper describes our participation in the SemEval-2018 Task 3. The aim of this task is to identify ironic tweets. ValenTO exploited an extended version of emotIDM (Hern´andez Far´ıas et al., 2016), an irony detection model based mainly on affective information. In particular, we experimented the use of a wide range of affectrelated features for characterizing the presence of ironic content, covering different facets of affect, from sentiment to finer-grained emotions. Most theorist (Grice, 1975; Wilson and Sperber, 1992; Alba-Juez and Attardo, 2014) recognized, indeed, the important role"
S18-1105,P15-1100,0,0.0247323,"ucariello, 2014). In Twitter we can find many examples both of verbal irony and of posts where users describe aspects of an ironic situation. Most of the proposed approaches to the automatic detection of irony in social media (Riloff et al., 2013; Buschmeier et al., 2014; Pt´acˇ ek et al., 2014)take advantage of lexical factors such as n-grams, punctuation marks, among others. Information related to affect has been also exploited (Reyes et al., 2013; Barbieri et al., 2014; Hern´andez Far´ıas et al., 2015). Other scholars proposed methods exploiting the context surrounding an ironic utterance (Wallace et al., 2015; Karoui et al., 2015). Recently, also deep learning techniques have been applied (Nozza et al., 2016; Poria et al., 2016). This paper describes our participation in the SemEval-2018 Task 3. The aim of this task is to identify ironic tweets. ValenTO exploited an extended version of emotIDM (Hern´andez Far´ıas et al., 2016), an irony detection model based mainly on affective information. In particular, we experimented the use of a wide range of affectrelated features for characterizing the presence of ironic content, covering different facets of affect, from sentiment to finer-grained emotions."
S18-1105,Y13-1035,0,0.0647493,"Missing"
S18-1105,D13-1066,0,0.0784187,"Missing"
S18-1105,H05-1044,0,0.0633483,"(verbs, adverbs, nouns, adjectives), emoticons, up3.2 Our Proposal percase characters, among others. Affective Features. They are organized in three We decided to participate to the shared task by sub-groups representing different facets of affect: using emotIDM. By analyzing the training data, Sentiment-Related Features (Sent). Hu&Liu an interesting characteristic was found: 857 out (Hu and Liu, 2004), General Inquirer (Stone and of 3,834 tweets contain an URL. From these Hunt, 1963), EffectWordNet (Choi and Wiebe, tweets, 265 were belonging to the ironic class, 2014), Subjectivity lexicon (Wilson et al., 2005), while 592 were labeled as non-ironic. Notice and EmoLex (Mohammad and Turney, 2013), that, in (Hern´andez-Farias et al., 2014), the authors AFINN, SWN, Semantic Orientation lexicon found a similar behavior regarding URL informa(Taboada and Grieve, 2004), and SenticNet (SN) tion in the dataset provided by the organizers of (Cambria et al., 2014). SentiPOLC-2014 (Basile et al., 2014). FurtherEmotional Categories (eCat). EmoLex, more, Barbieri et al. (2014) exploited a feature EmoSenticNet (Poria et al., 2013), SentiSense for alerting the existence of an URL in a tweet; (Carrillo de Albornoz et"
S19-2007,D14-1162,0,0.0807958,"Missing"
S19-2007,L18-1443,1,0.764724,"Imm. 40.50 59.50 32.10 67.90 50.31 46.69 Test Women 42.00 58.00 94.94 5.06 92.56 7.44 Table 2: Distribution percentages across sets and categories for Spanish data. The percentages for the target and aggressiveness categories are computed on the total number of hateful tweets. to collect tweets: (1) monitoring potential victims of hate accounts, (2) downloading the history of identified haters and (3) filtering Twitter streams with keywords, i.e. words, hashtags and stems. Regarding the keyword-driven approach, we employed both neutral keywords (in line with the collection strategy applied in Sanguinetti et al. (2018)), derogatory words against the targets, and highly polarized hashtags, in order to collect a corpus for reflecting also on the subtle but important differences between HS, offensiveness (Wiegand et al., 2018) and stance (Taul´e et al., 2017). The keywords that occur more frequently in the collected tweets are: migrant, refugee, #buildthatwall, bitch, hoe, women for English, and inmigra-, arabe, sudaca, puta, callate, perra for Spanish4 . The entire HatEval dataset is composed of 19,600 tweets, 13,000 for English and 6,600 for Spanish. They are distributed across the targets as follows: 9,091"
S19-2007,E17-2068,0,0.0671744,"Missing"
S19-2104,W17-3007,0,0.0233162,"Missing"
S19-2104,W17-3013,0,0.0588329,"Missing"
S19-2104,D14-1181,0,0.010268,"Missing"
S19-2104,W17-3006,0,0.0376252,"Missing"
S19-2104,D14-1162,0,0.0820222,"n state as → − ← − the concatenation of these two hˆt = [ ht , ht ]. The idea of this Bi-LSTM is to capture long-range and backwards dependencies. Methodology and Data The corpus provided by the organizers consists of 14,100 tweets in English. The data collection methods used to compile the dataset used in OffensEval is described in Zampieri et al. (2019a). The first step is the preprocessing of the tweets, where texts are cleaned. All emoticons, hashtag and urls are removed. Then, the texts are represented as vectors with word embedding vectors. We used the pre-trained word vectors of Glove (Pennington et al., 2014), trained on 2 billion words from Twitter. The method proposed in this work is based on an architecture that sequentially obtains the output for each of the subtasks. In the first level we use a model whose input is the word embeddings of a tweet and the output is a vector (r vector) that is taken as a compact representation of the input and is used in the following steps. For the model, two types of networks have been used. In a first approach a Recurrent Neural Network (RNN) is used, and as a second approximation a Convolutional Neuronal Network (CNN). These two models are described below. 3"
S19-2104,W17-1101,0,0.0290848,"llying, among other abusive behavior. To increase the research in this areas, several workshops have been organized, such as ALW1 and TRAC2 . Recently, OffensEval3 (Zampieri et al., 2019b), which is a shared task at the SemEval-20194 workshop has been launched on the research community. The aim of OffensEval is to deal with offensive language detection in 2 Related Work Some approaches have been proposed to tackle the problem of offensive language detection. It is the case of recent works (Waseem et al., 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018) and surveys (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018). There are even studies on languages other than English such as (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Many of the last approaches rely on neural network models. For instance, the work of (Ganesan 1 https://sites.google.com/site/abusivelanguageworkshop2017/ et al., 2018) presents a Multi-Layer Feedforward 2 https://sites.google.com/view/trac1/home 3 Neural Networks. Moreover, (Park and Fung, https://competitions.codalab.org/competitions/20011 4 2017) proposes to implement three models based http://alt.qcri.org/semeval2019/index.php?id="
S19-2104,W17-3003,0,0.0169169,"as ALW1 and TRAC2 . Recently, OffensEval3 (Zampieri et al., 2019b), which is a shared task at the SemEval-20194 workshop has been launched on the research community. The aim of OffensEval is to deal with offensive language detection in 2 Related Work Some approaches have been proposed to tackle the problem of offensive language detection. It is the case of recent works (Waseem et al., 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018) and surveys (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018). There are even studies on languages other than English such as (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Many of the last approaches rely on neural network models. For instance, the work of (Ganesan 1 https://sites.google.com/site/abusivelanguageworkshop2017/ et al., 2018) presents a Multi-Layer Feedforward 2 https://sites.google.com/view/trac1/home 3 Neural Networks. Moreover, (Park and Fung, https://competitions.codalab.org/competitions/20011 4 2017) proposes to implement three models based http://alt.qcri.org/semeval2019/index.php?id=tasks 582 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 582–586 Mi"
S19-2104,W17-3012,0,0.0516721,"ive language in texts from the social media is an essential task for security, the prevention of cyber-bullying, among other abusive behavior. To increase the research in this areas, several workshops have been organized, such as ALW1 and TRAC2 . Recently, OffensEval3 (Zampieri et al., 2019b), which is a shared task at the SemEval-20194 workshop has been launched on the research community. The aim of OffensEval is to deal with offensive language detection in 2 Related Work Some approaches have been proposed to tackle the problem of offensive language detection. It is the case of recent works (Waseem et al., 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018) and surveys (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018). There are even studies on languages other than English such as (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Many of the last approaches rely on neural network models. For instance, the work of (Ganesan 1 https://sites.google.com/site/abusivelanguageworkshop2017/ et al., 2018) presents a Multi-Layer Feedforward 2 https://sites.google.com/view/trac1/home 3 Neural Networks. Moreover, (Park and Fung, https://competitions.codalab.org/c"
S19-2104,N19-1144,0,0.106498,"portant media of personal and commercial communication. In this scenario, some users take advantage of the anonymity of this kind of communication, using this to engage in behaviour that many of them would not consider in real life. Therefore, much of the offensive language is widespread in social networks. Then, studying offensive language in texts from the social media is an essential task for security, the prevention of cyber-bullying, among other abusive behavior. To increase the research in this areas, several workshops have been organized, such as ALW1 and TRAC2 . Recently, OffensEval3 (Zampieri et al., 2019b), which is a shared task at the SemEval-20194 workshop has been launched on the research community. The aim of OffensEval is to deal with offensive language detection in 2 Related Work Some approaches have been proposed to tackle the problem of offensive language detection. It is the case of recent works (Waseem et al., 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018) and surveys (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018). There are even studies on languages other than English such as (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene"
S19-2104,S19-2010,0,0.11376,"portant media of personal and commercial communication. In this scenario, some users take advantage of the anonymity of this kind of communication, using this to engage in behaviour that many of them would not consider in real life. Therefore, much of the offensive language is widespread in social networks. Then, studying offensive language in texts from the social media is an essential task for security, the prevention of cyber-bullying, among other abusive behavior. To increase the research in this areas, several workshops have been organized, such as ALW1 and TRAC2 . Recently, OffensEval3 (Zampieri et al., 2019b), which is a shared task at the SemEval-20194 workshop has been launched on the research community. The aim of OffensEval is to deal with offensive language detection in 2 Related Work Some approaches have been proposed to tackle the problem of offensive language detection. It is the case of recent works (Waseem et al., 2017; ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018) and surveys (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018). There are even studies on languages other than English such as (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene"
S19-2135,W17-3001,0,0.026054,"a superior and a subordinate compared with conversations between friends and some groups of slurs are more acceptable then others. In (Pamungkas et al., 2018) it was shown that the results of the model based on Support Vector Machine were quite good and in the research (Frenda et al., 2018) the ensemble of models allow to achieve a high level of accuracy. In work (Shushkevich and Cardiff, 2018) it was presented the ensemble of Logistic Regression. Support Vector Machine and Naive Bayes model which shown quite good results. Expanding the point that offensive speech is heterogeneous, the work (Clarke and Grieve, 2017) presented results which showed that there is a difference between racist and sexist posts: the sexist messages were more interactive (more personal) and more attitudinal (with authors opinion) than racist ones. From this article we can make a conclusion that the most popular linguistic feature in offensive language are question marks and question DO (when a sentence stars with the word do). It is necessary to add that models based on neural networks show good results of offensive language recognition, as it was shown in (Badjatiya et al., 2017), where the authors created the model based on Lo"
S19-2135,P12-2018,0,0.0549416,"ndividual target of offenses then about group and other targets. - The model based on Logistic Regression (LR) (Wright, 1995; Genkin et al., 2007), this type of classifiers apply an exponential function to a lineal combination of objects extracted from the data. - The model based on Naive Bayes (NB), whose advantages are an absence of big training dataset and speed calculations requirement (Hi and Li, 2007). 5 - The model presented an interpolation between LR and NB with 0.25 coefficient of interpolation as a form of regulation: trust NB unless the LR. This type of interpolation was shown in (Wang and Manning, 2012) where NB was combined with Support Vector Machine, but in our case the combination LR+NB worked better. Conclusion To sum up, we created an ensemble of models, which allow as to achieve quite good results being placed 25th out of a total of 65 participants. We showed that the idea of blending simple models based on Logistic Regression, Naive Bayes and Support Vector Machine gives a perspective in the area of hate speech recognition in the identification of the target of offensive messages. - The model based on Support Vector Machine (SVM), the effectiveness of which in the work with texts was"
S19-2135,N19-1144,0,0.025568,"on was for three different groups: This problem is receiving increasing amounts of attention and researchers are making attempts to build systems capable of recognizing such kinds of aggressive speech, offenses and insults in social networks. - Individual, when the target of the offensive post was a person; - Group, when the target of the offensive message was a group of people considered as a unit; This article presented our approach to hate speech detection, which we used for the challenge SemEval-2019 Task 6: OffensEval - Identifying and Categorizing Offensive Language in Social ;;; Media (Zampieri et al., 2019a),(Zampieri et al., 2019b). - Other, when the target of the offensive tweet did not belong to any of the previous categories (e.g., a situation, an event, or another issue). There are two datasets in English and in Spanish languages for analysis, and our team worked with English only. The training dataset included 13200 tweets, 4400 of them were offensive ones, 3876 messages were labeled as ’Target Insult and Threats’ and 524 ones as ’Untargeted’. We The task consisted of three sub-tasks and proposed to investigate the data extracted from Twitter for creating a classification system. 770 Proc"
S19-2135,S19-2010,0,0.0264936,"on was for three different groups: This problem is receiving increasing amounts of attention and researchers are making attempts to build systems capable of recognizing such kinds of aggressive speech, offenses and insults in social networks. - Individual, when the target of the offensive post was a person; - Group, when the target of the offensive message was a group of people considered as a unit; This article presented our approach to hate speech detection, which we used for the challenge SemEval-2019 Task 6: OffensEval - Identifying and Categorizing Offensive Language in Social ;;; Media (Zampieri et al., 2019a),(Zampieri et al., 2019b). - Other, when the target of the offensive tweet did not belong to any of the previous categories (e.g., a situation, an event, or another issue). There are two datasets in English and in Spanish languages for analysis, and our team worked with English only. The training dataset included 13200 tweets, 4400 of them were offensive ones, 3876 messages were labeled as ’Target Insult and Threats’ and 524 ones as ’Untargeted’. We The task consisted of three sub-tasks and proposed to investigate the data extracted from Twitter for creating a classification system. 770 Proc"
S19-2197,aker-etal-2017-simple,0,0.177214,"). Stance Detection (SD) consists in automatically determining whether the author of a text is in favour, against, or neutral towards a given target, i.e. statement, event, person or organization, and it is generally indicated as TARGETSPECIFIC STANCE CLASSIFICATION (Mohammad et al., 2016). Another type of stance classification, more general-purpose, is the OPEN STANCE CLASSIFI CATION task, usually indicated with the acronym SDQC, by referring to the four categories exploited for indicating the attitude of a message with respect to the rumour: Support (S), Deny (D), Query (Q) and Comment (C) (Aker et al., 2017). Target-specific stance classification is especially suitable for analyses about a specific product or political actor, being the target given as already extracted, e.g. from conversational cues. On this regard several shared tasks have been organized in recent years: see for instance SemEval-2016 Task 6 (Mohammad et al., 2017) considering six commonly known targets in the United States, and StanceCat at IberEval-2017 on stance and gender detection in tweets on the matter of the Independence of Catalonia (Taul´e et al., 2017). On the other hand, the open stance classification, (i.e. the task"
S19-2197,S17-2080,0,0.29964,"Missing"
S19-2197,D14-1125,0,0.0614709,"Missing"
S19-2197,de-albornoz-etal-2012-sentisense,0,0.0552493,"Missing"
S19-2197,C18-1158,0,0.022227,"easonable to consider the application of SDQC techniques for accomplishing rumour analysis tasks. A first shared task, concerning SDQC applied to rumor detection, has been organized at SemEval-2017, i.e RumorEval 2017 (Derczynski et al., 2017). Furthermore, several research works have analyzed the open issue of the impact of rumors in social media (Resnick et al., 2014; Zubiaga et al., 2015, 2018), for instance exploiting linguistic features (Ghanem et al., 2018). Such a kind of approaches may be also found in works which deal with the problems of Fake News Detection (Ciampaglia et al., 2015; Hanselowski et al., 2018). Furthermore, a rumor is defined as a “circulating story of questionable veracity, which is apparently credible but hard to verify, and produces sufficient scepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2015). Concerning veracity identification, increasingly advanced systems and annotation schemas have been developed to support the analysis of rumour veracity and misinformation in text (Qazvinian et al., 2011; Kumar and Geethakumari, 2014; Zhang et al., 2015). 3 Description of the task The RumorEval task is articulated in the following sub-tasks: Ta"
S19-2197,S16-1003,0,0.130035,"Missing"
S19-2197,W10-0204,0,0.0164778,": emotional, sentiment, lexical, stylistic, meta-structural and Twitter-based. Furthermore, we introduced a novel set of syntax-based features. Emotional Features - We exploited several emotional resources in order to build features for our system. Three lexica: (a) EmoSenticNet, a lexicon that assigns six WordNet Affect emotion labels to SenticNet concepts (Poria et al., 2013); (b) the NRC Emotion Lexicon, a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive) (Mohammad and Turney, 2010); 1126 and (c) SentiSense, an easily scalable conceptbased affective lexicon for Sentiment Analysis (De Albornoz et al., 2012). We also exploited two tools: (d) Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (Fast et al., 2016); and (e) LIWC a text analysis dictionary that counts words in psychologically meaningful categories (Pennebaker et al., 2001). Sentiment Features - Our sentiment features were modeled exploiting sentiment resources such as: (a) SentiStrength, a sentiment strength detection program which uses a lexical approa"
S19-2197,D11-1147,0,0.335938,"n The problem of rumor detection lately is attracting considerable attention, also considering the very fast diffusion of information that features social media platforms. In particular rumors are facilitated by large users’ communities, where also expert journalists are unable to keep up with the huge volume of online generated information and to decide whether a news is a hoax (Procter et al., 2013; Webb et al., 2016; Zubiaga et al., 2018). Rumour stance classification is the task that intends to classify the type of contribution to the rumours expressed by different posts of a same thread (Qazvinian et al., 2011) according to a set of given categories: supporting, denying, querying or simply commenting on the rumour. For instance, referring to Twitter, once a tweet that introduces a rumour is detected (the “source tweet”), all the tweets having a reply relationship with it, (i.e. being part of the same thread), are collected to be classified. Our participation to this task is mainly focused on the investigation of linguistic features of social Related work The RumorEval 2019 shared task involves two tasks: Task A (rumour stance classification) and Task B (verification). Stance Detection (SD) consists"
S19-2197,W18-5510,1,0.840587,"dia or streaming news analysis. Provided that attitudes around a claim can act as proxies for its veracity, and not only of its controversiality, it is reasonable to consider the application of SDQC techniques for accomplishing rumour analysis tasks. A first shared task, concerning SDQC applied to rumor detection, has been organized at SemEval-2017, i.e RumorEval 2017 (Derczynski et al., 2017). Furthermore, several research works have analyzed the open issue of the impact of rumors in social media (Resnick et al., 2014; Zubiaga et al., 2015, 2018), for instance exploiting linguistic features (Ghanem et al., 2018). Such a kind of approaches may be also found in works which deal with the problems of Fake News Detection (Ciampaglia et al., 2015; Hanselowski et al., 2018). Furthermore, a rumor is defined as a “circulating story of questionable veracity, which is apparently credible but hard to verify, and produces sufficient scepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2015). Concerning veracity identification, increasingly advanced systems and annotation schemas have been developed to support the analysis of rumour veracity and misinformation in text (Qazvini"
sidorov-etal-2010-english,J90-2002,0,\N,Missing
sidorov-etal-2010-english,J03-1002,0,\N,Missing
sidorov-etal-2010-english,atserias-etal-2006-freeling,0,\N,Missing
W04-0820,magnini-cavaglia-2000-integrating,0,\N,Missing
W09-0808,W02-0507,0,\N,Missing
W11-1715,atserias-etal-2006-freeling,0,0.020879,"Missing"
W11-1715,W04-2214,0,0.00618194,"7, are given in Table 2. 4.3 Table 2: Statistics of the most frequent POS-grams. Funny profiling Irony takes advantage of humor aspects to produce its effect. This category intends to characterize the documents in terms of humorous properties. In order to represent this category, we selected some of the best humor features reported in the literature: stylistic features, human centeredness, and keyness. The stylistic features, according to the experiments reported in (Mihalcea and Strapparava, 2006), were obtained by collecting all the words labeled with the tag “sexuality” in WordNet Domains (Bentivogli et al., 2004). The second feature focuses on social relationships. In order to retrieve these words, the elements registered in WordNet (Miller, 1995), which belong to the synsets relation, relationship and relative, were retrieved. The last feature is represented by obtaining the keyness value of the words (cf. (Reyes et al., 2009)). This value is calculated comparing the word frequencies in the ironic documents against their frequencies in a reference corpus. Google N-grams (Brants and Franz, 2006) was 121 Order Sequences 2-grams 3-grams 4-grams 5-grams 6-grams 7-grams 300 298 282 159 39 65 Examples dt n"
W11-1715,P09-2041,0,0.0149122,"nderly irony in such structures. In contrast, Carvalho et al. (2009) suggested some clues for automatically identifying ironic sentences by means of identifying features such as emoticons, onomatopoeic expressions, punctuation and quotation marks. Furthermore, there are others approaches which are focused on particular devices such as sarcasm and satire, rather than on the whole concept of irony. For instance, Tsur et al. (2010) and Davidov et al. (2010) address the problem of finding linguistic elements that mark the use of sarcasm in online product reviews and tweets, respectively. Finally, Burfoot and Baldwin (2009) explore the task of automatic satire detection by evaluating features related to headline elements, offensive language and slang. 3.1 Evaluation Corpus Due to the scarce work on automatic irony processing, and to the intrinsic features of irony, it is quite difficult and subjective to obtain a corpus with ironic data. Therefore, we decided to rely on the wisdom of the crowd and use a collection of customer reviews from the Amazon web site. These reviews are considered as ironic by customers, as well as by many journalists, both in mass and social media. According to such means, all these revi"
W11-1715,W10-3110,0,0.0154483,"ith “physical” words, supposes a great challenge, even from a linguistic analysis, because it points to social and cognitive layers quite difficult to be computationally represented. One of the communicative phenomena which better represents this problem is irony. According to Wilson 118 and Sperber (2007), irony is essentially a communicative act which expresses an opposite meaning of what was literally said. Due to irony is common in texts that express subjective and deeply-felt opinions, its presence represents a significant obstacle to the accurate analysis of sentiment in such texts (cf. Councill et al. (2010)). In this research work we aim at gathering a set of discriminating elements to represent irony. In particular, we focus on analyzing a set of customer reviews (posted on the basis of an online viral effect) in order to obtain a set of key components to face the task of irony detection. This paper is organized as follows. Section 2 introduces the theoretical problem of irony. Section 3 presents the related work as well as the evaluation corpus. Section 4 describes our model and the experiments that were performed. Section 5 assesses the model and presents the discussion of the results. Finall"
W11-1715,D09-1063,0,0.0138973,"298 282 159 39 65 Examples dt nn; nn in; jj nn; nn nn dt nn in; dt jj nn; jj nn nn nn in dt nn; vb dt jj nn vbd dt vbg nn jj nnp vbd dt vbg nn jj nns vbd dt vbg nn jj fd used as the reference corpus. Only the words whose keyness was ≥ 100 were kept. 4.4 Positive/Negative Profiling As we have already pointed out, one of the most important properties of irony relies on the communication of negative information through positive one. This category intends to be an indicator about the correlation between positive and negative elements in the data. The Macquarie Semantic Orientation Lexicon (MSOL) (Saif et al., 2009) was used to label the data. This lexicon contains 76,400 entries (30,458 positive and 45,942 negative ones). 4.5 Affective Profiling In order to enhance the quality of the information related to the expression of irony, we considered to represent information linked to psychological layers. The affective profiling category is an attempt to characterize the documents in terms of words which symbolize subjective contents such as emotions, feelings, moods, etc. The WordNet-Affect resource (Strapparava and Valitutti, 2004) was employed for obtaining the affective terms. This resource contains 11 c"
W11-1715,strapparava-valitutti-2004-wordnet,0,0.0235134,"ive and negative elements in the data. The Macquarie Semantic Orientation Lexicon (MSOL) (Saif et al., 2009) was used to label the data. This lexicon contains 76,400 entries (30,458 positive and 45,942 negative ones). 4.5 Affective Profiling In order to enhance the quality of the information related to the expression of irony, we considered to represent information linked to psychological layers. The affective profiling category is an attempt to characterize the documents in terms of words which symbolize subjective contents such as emotions, feelings, moods, etc. The WordNet-Affect resource (Strapparava and Valitutti, 2004) was employed for obtaining the affective terms. This resource contains 11 classes to represent affectiveness. According to the authors, these classes represent how speakers convey affective meanings by means of selecting certain words and not others. 4.6 Pleasantness Profiling The last category is an attempt to represent ideal cognitive scenarios to express irony. This means that, like words, the contexts in which irony appears are enormous. Therefore, since it is impossible to make out all the possibilities, we pretend to define a schema to represent favorable and unfavorable ironic contexts"
W11-1715,C96-2162,0,0.935996,"ndirect negation (Giora, 1995). On the basis of some pragmatic frameworks, authors focus on certain fine-grained aspects of this term. For instance, Grice (1975) conProceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 118–124, c 24 June, 2011, Portland, Oregon, USA 2011 Association for Computational Linguistics siders that an utterance is ironic if it intentionally violates some conversational maxims. Wilson and Sperber (2007) assume that verbal irony must be understood as echoic; i.e. as a distinction between use and mention. Utsumi (1996), in contrast, suggests an ironic environment which causes a negative emotional attitude. According to these points of view, the elements to conceive a verbal expression as ironic point to different ways of explaining the same underlying concept of opposition, but specially note, however, that most of them rely on literary studies (Attardo, 2007); thus, their computational formalization is quite challenging. Furthermore, consider that people have their own concept of irony, which often does not match with the rules suggested by the experts. For instance, consider the following expressions retr"
W12-0413,W11-1713,0,0.107175,"Missing"
W12-0413,W97-0703,0,0.581705,"of chat interactions with sexually explicit content: 1. Predator/Other (a) Predator/Victim (victim is underage) (b) Predator/Volunteer posing as a children (c) Predator/Law enforcement officer posing as a child Our Approach We believe that lexical chains are appropriate to model the fixated discourse of the predators chats. 2. Adult/Adult (consensual relationship) 4.1 Lexical Chains A lexical chain is a sequence of semantically related terms (Morris and Hirst, 1991). It has applications in many tasks including Word Sense Disambiguation (WSD) (Galley and McKeown, 2003) and Text Summarization (Barzilay and Elhadad, 1997). To estimate semantic similarity we used two metrics: the similarity of Leacock and Chodorow (Leacock and Chodorow, 2003), and that of Resnik (Resnik, 1995). Leacock and Chodorow’s semantic similarity measure is defined as: SimL&Ch (c1 , c2 ) = −log Data The most interesting from our research point of view is data of the type 1(a), but obtaining such data is not easy. However, the data of type 1(b) is freely available at the web site www.perverted-justice.com (PJ). For our study, we have extracted chat logs from the perverted-justice website. Since the victim is not real, we considered only t"
W12-0413,J91-1002,0,0.249021,"im 13 in march and not yet? i lied a little bit b4 Predator: its all cool Predator: i can lick hard 4 5 Pendar (2007) has summarized the possible types of chat interactions with sexually explicit content: 1. Predator/Other (a) Predator/Victim (victim is underage) (b) Predator/Volunteer posing as a children (c) Predator/Law enforcement officer posing as a child Our Approach We believe that lexical chains are appropriate to model the fixated discourse of the predators chats. 2. Adult/Adult (consensual relationship) 4.1 Lexical Chains A lexical chain is a sequence of semantically related terms (Morris and Hirst, 1991). It has applications in many tasks including Word Sense Disambiguation (WSD) (Galley and McKeown, 2003) and Text Summarization (Barzilay and Elhadad, 1997). To estimate semantic similarity we used two metrics: the similarity of Leacock and Chodorow (Leacock and Chodorow, 2003), and that of Resnik (Resnik, 1995). Leacock and Chodorow’s semantic similarity measure is defined as: SimL&Ch (c1 , c2 ) = −log Data The most interesting from our research point of view is data of the type 1(a), but obtaining such data is not easy. However, the data of type 1(b) is freely available at the web site www.p"
W12-3208,C10-1005,1,0.877947,"Missing"
W12-3208,C10-1048,0,0.130789,"r of them do not differ at the core and there have been many approaches to it (Bendersky and Croft, 2009; Hoad and Zobel, 2003; Seo and Croft, 2008). The text reuse can also be present in the cross-language environment (Barr´on-Cede˜no et al., 2010; Potthast et al., 2011a). Since few years, PAN organises competitions at CLEF3 (PAN@CLEF) on plagiarism detection (Potthast et al., 2010; Potthast et al., 2011b) and at FIRE4 (PAN@FIRE) on cross-language text reuse (Barr´on-Cede˜no et al., 2011). In the past, there has been an attempt to identify the plagiarism among the papers of ACL anthology in (HaCohen-Kerner et al., 2010), but it mainly aims to propose a new strategy to identify the plagiarism and uses the anthology as the corpus. In this study, we are concerned about the verbatim reuse and that too in large amount, only. We identify such strong text reuse cases in two spans of five years papers of ACL (conference and workshops) and analyse them to notice the trends in the past and the present based on their year of publication, paper type and the authorship. The detection method along with the subsection of the ACL anthology used are described in Section 2. Section 3 contains the details of the carried experi"
W12-3717,baccianella-etal-2010-sentiwordnet,0,0.0242188,"of different authors, thereby obtaining 68 files. We have also used the subset the of NPS chat corpus (Forsythand and Martell, 2007), though it is not of type 2. We have extracted chat lines only for those adult authors who had more than 30 lines written. Finally the dataset consisted of 65 authors. From each dataset we have left 20 files for testing. 6 Experiments To distinguish between predators and not predators we used a Naive Bayes classifier, already successfully utilized for analyzing chats by previous research (Lin, 2007). To extract positive and negative words, we used SentiWordNet (Baccianella et al., 2010). The features borrowed from McGhee et al. (2011), were detected with the list of words authors made available for us. Imperative sentences were detected as affirmative sentences starting with verbs. Emoticons were captured with simple regular expressions. Our dataset is imbalanced, the majority of the chat logs are from PJ. To make the experimental data more balanced, we have created 5 subsets of PJ corFeature Class Emotional Markers Features borrowed from McGhee et al. (2011) Features helpful to detect neuroticism level Features derived from pedophile’s psychological profile Other Feature Po"
W12-3717,W97-0703,0,0.0717515,"Missing"
W12-3717,W12-0413,1,0.710427,"Missing"
W12-3717,J91-1002,0,0.240367,"ges of personal and reflexive pronouns and modal obligation verbs (have to, has to, had to, must, should, mustn’t, and shouldn’t). We consider the use of imperative sentences and emoticons to capture the predators tendencies to be dominant and copy childrens’ behaviour respectively. The study of Egan et al. (Egan et al., 2011) has revealed several recurrent themes that appear in PJ chats. Among them, fixated discourse: the unwillingness of the predator to change the topic. In (Bogdanova et al., 2012) we present experiments on modeling the fixated discourse. We have constructed lexical chains (Morris and Hirst, 1991) starting with the anchor word “sex” in the first WordNet meaning: “sexual activity, sexual practice, sex, sex activity (activities associated with sexual intercourse)”. We have finally used as a feature the length of the lexical chain constructed with the Resnik similarity measure (Resnik, 1995) with the threshold = 0.7. The full list of features is presented in Table 1. 5 Datasets Pendar (2007) has summarized the possible types of chat interactions with sexually explicit content: 1. Predator/Other (a) Predator/Victim (victim is underaged) (b) Predator/Volunteer posing as a children 114 The m"
W12-3717,S07-1013,0,0.0243579,"use of positive words is expected. On the other hand, as it was described earlier, pedophiles tend to be emotionally unstable and prone to lose temper, hence they might 113 Predator: Predator: Predator: Predator: Predator: Predator: Predator: Predator: Predator: hello r u there thnx a lot thanx a lot u just wast my time drive down there can u not im any more Here the offender is angry because the pseudovictim did not call him: Predator: u didnt call Predator: i m angry with u Therefore, we have decided to use markers of basic emotions as features. At the SemEval 2007 task on “Affective Text” (Strapparava and Mihalcea, 2007) the problem of fine-grained emotion annotation was defined: given a set of news titles, the system is to label each title with the appropriate emotion out of the following list: ANGER, DISGUST, FEAR, JOY, SADNESS, SURPRISE. In this research work we only use the percentages of the markers of each emotion. We have also borrowed several features from McGhee et al. (2011): • Percentage of approach words. Approach words include verbs such as come and meet and such nouns as car and hotel. • Percentage of relationship words. These words refer to dating (e.g. boyfriend, date). • Percentage of family"
W12-3717,strapparava-valitutti-2004-wordnet,0,0.1358,"Missing"
W12-3717,W11-1713,0,0.0639967,"Missing"
W13-1606,P09-2078,0,0.0634405,"so Donato Hern´andez Fusilier1,2 Manuel Montes-y-G´omez Natural Language Laboratorio de Tecnolog´ıas Rafael Guzm´an Cabrera Engineering Lab., ELiRF. del Lenguaje. Divisi´on de Ingenier´ıas 2 Instituto Nacional de Campus Irapuato-Salamanca. Universitat Polit`ecnica de 1 ´ Val`encia Universidad de Guanajuato Astrof´ısica, Optica y Electr´onica. Spain. Mexico. Mexico. {donato,guzmanc}@ugto.mx mmontesg@inaoep.mx Abstract to respond promptly to their clients’ expectations. It is not surprising that opinion mining technologies have been witnessed a great interest in recent years (Zhou et al., 2008; Mihalcea and Strapparava, 2009). Research in this field has been mainly oriented to problems such as opinion extraction (Liu B., 2012) and polarity classification (Reyes and Rosso., 2012). However, because of the current trend about the growing number of online reviews that are fake or paid by companies to promote their products or damage the reputation of competitors, the automatic detection of opinion spam has emerged as a highly relevant research topic (Jindal et al., 2010; Jindal and Liu, 2008; Lau et al., 2011; Wu et al., 2010; Ott et al., 2011; Sihong et al., 2012). Nowadays a large number of opinion reviews are poste"
W13-1606,P11-1032,0,0.653724,"tnessed a great interest in recent years (Zhou et al., 2008; Mihalcea and Strapparava, 2009). Research in this field has been mainly oriented to problems such as opinion extraction (Liu B., 2012) and polarity classification (Reyes and Rosso., 2012). However, because of the current trend about the growing number of online reviews that are fake or paid by companies to promote their products or damage the reputation of competitors, the automatic detection of opinion spam has emerged as a highly relevant research topic (Jindal et al., 2010; Jindal and Liu, 2008; Lau et al., 2011; Wu et al., 2010; Ott et al., 2011; Sihong et al., 2012). Nowadays a large number of opinion reviews are posted on the Web. Such reviews are a very important source of information for customers and companies. The former rely more than ever on online reviews to make their purchase decisions and the latter to respond promptly to their clients’ expectations. Due to the economic importance of these reviews there is a growing trend to incorporate spam on such sites, and, as a consequence, to develop methods for opinion spam detection. In this paper we focus on the detection of deceptive opinion spam, which consists of fictitious op"
W14-3306,W11-2156,1,0.605388,"Missing"
W14-3306,P07-2045,0,0.00412561,"educed dimension vector-space model, which is constructed either by means of standard latent semantic analysis or using deep representation as decribed in section 3. Introduction This paper describes the joint participation of the Instituto Polit´ecnico Nacional (IPN) and the Universitat Polit`ecnica de Valencia (UPV) in cooperation with Institute for Infocomm Research (I2R) on the 9th Workshop on Statistical Machine Translation (WMT 2014). In particular, our participation was in the English-to-Hindi translation task. Our baseline system is an standard phrasebased SMT system built with Moses (Koehn et al., 2007). Starting from this system we propose to introduce a source-context feature function inspired by previous works (R. Costa-juss`a and Banchs, 2011; Banchs and Costa-juss`a, 2011). The main novelty of this work is that the source-context feature is computed in a new deep representation. The rest of the paper is organized as follows. Section 2 presents the motivation of this semantic feature and the description of how the source context feature function is added to Moses. Section 3 explains how both the latent semantic indexing and deep representation of sentences are used to better compute simi"
W14-3306,W11-1014,1,\N,Missing
W15-2909,I13-1039,0,0.0160468,"jee and Chua, 2014) the authors presented the results of a logistic regression model using 13 different independent variables: complexity, reading difficulty, adjective, article, noun, preposition, adverb, verb, pronoun, personal pronoun, positive cues, perceptual words and future tense. In (Ren et al., 2014) a semi-supervised model called mixing population and individual property PU learning, is presented. The model is then incorporated to a SVM classifier. In (Ott et al., 2011) the authors used the 80 dimensions of LIWC2007, unigrams and bigrams as set of features with a SVM classifier. In (Feng and Hirst, 2013), profile alignment compatibility features combined with unigrams, bigrams and syntactic production rules were proposed for representing the opinion spam corpus. Then, a multivariate performance measures version of SVM classifier (named SVMperf ) was trained. In (Hern´andez Fusilier et al., 2015b) the authors studied two different representations: character n-grams and word n-grams. In particular, the best results were obtained with a Na¨ıve Bayes classifier using character 4 and 5 grams as features. Model Accuracy 10 fold cross-validation (Banerjee and Chua, 2014) (Ren et al., 2014) Our appro"
W15-2909,P12-2034,0,0.0378021,"Missing"
W15-2909,E14-1030,0,0.0145857,"n, quartiles, median, etc.) among populations of numerical data, without any assumptions about the underlying statistical distribution of the data. 63 al., 2015b) and the results were similar (no statistically significant difference was found), but our low dimensionality representation makes our approach more efficient. For future work we plans to investigate another emotion/sentiment features in order to study the contributions in tasks of deception detection of opinion spam. Also we are interesting to test our model with other corpora related to opinion spam as the one recently proposed in (Fornaciari and Poesio, 2014). Acknowledgments The research work of the first author has been partially funded by CONICET (Argentina). The work of the second author was done in the framework of the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems, DIANAAPPLICATIONS-Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-C02-01) research project, and the WIQ-EI IRSES project (grant no. 269180) within the FP 7 Marie Curie People Framework on Web Information Quality Evaluation Initiative. The first author would like to thank Donato Hern´andez Fusilier for kindly providing the results of his met"
W15-2909,P11-1032,0,0.259171,"learning variant for the same task, concluding the appropriateness of their approach for detecting opinion spam. In this paper we study the feasibility of the application of different features for representing safely information about clues related to fake reviews. We focus our study in a variant of the stylistic feature character n-grams named character n-grams in tokens. We also study an emotion-based feature and a linguistic processes feature based on LIWC variables. We evaluated the proposed features with a Support Vector Machines (SVM) classifier using a corpus of 1600 reviews of hotels (Ott et al., 2011; Ott et al., 2013). We show an experimental study evaluating the single features and combining them with the intention to obtain better features. After that previous study, we selected the one with we obtained the best results and made direct and indirect comparisons with some other methods. The obtained results show that the proposed features can capture information from the contents of the reviews and the writing style allowing to obtain classification results as good as with traditional character n-grams but with a lower dimensionality of representation. The rest of the paper is organized"
W15-2909,N13-1053,0,0.174481,"or the same task, concluding the appropriateness of their approach for detecting opinion spam. In this paper we study the feasibility of the application of different features for representing safely information about clues related to fake reviews. We focus our study in a variant of the stylistic feature character n-grams named character n-grams in tokens. We also study an emotion-based feature and a linguistic processes feature based on LIWC variables. We evaluated the proposed features with a Support Vector Machines (SVM) classifier using a corpus of 1600 reviews of hotels (Ott et al., 2011; Ott et al., 2013). We show an experimental study evaluating the single features and combining them with the intention to obtain better features. After that previous study, we selected the one with we obtained the best results and made direct and indirect comparisons with some other methods. The obtained results show that the proposed features can capture information from the contents of the reviews and the writing style allowing to obtain classification results as good as with traditional character n-grams but with a lower dimensionality of representation. The rest of the paper is organized as follows. Section"
W15-2909,D14-1055,0,0.0315945,"Missing"
W15-5403,W14-4204,0,0.145435,"Missing"
W15-5403,W14-5904,0,0.0791197,"Missing"
W15-5403,W14-5307,0,0.0737505,"Missing"
W15-5403,W15-5401,0,0.0869715,"Missing"
W15-5409,W14-5316,0,0.240436,"Missing"
W15-5409,W14-5317,0,0.102574,"Missing"
W15-5409,P12-3005,0,0.110355,"Missing"
W15-5409,W14-5904,0,0.0943188,"Missing"
W15-5409,W14-5315,0,0.23319,"Missing"
W15-5409,W14-4204,0,0.10107,"Missing"
W15-5409,W14-5314,0,0.243932,"Missing"
W15-5409,W13-1706,0,0.0552087,"Missing"
W15-5409,W14-5318,0,0.0917034,"Missing"
W15-5409,W14-5307,0,0.437239,"Missing"
W15-5409,W15-5401,0,0.0899334,"Missing"
W18-1605,C12-1025,1,0.921286,"which is used to evaluate the model. To evaluate the task, we have proposed a statistical embedding representation that we have compared with common single-layer approaches based on n-grams, obtaining encouraging results. CH ID L1 1,139 1,103 22,549 1,102 1,143 8 Others 23,931 10,997 16,102 10,998 23,923 74 Table 1: Number of documents in each corpus. L1 corresponds to the documents written by authors of the native language to be identified. Others comprise all the documents written by authors of the other native languages in the corpus. 3 Low Dimensionality Statistical Embedding As shown in (Brooke and Hirst, 2012; Ionescu et al., 2014), single-layer representations such as n-grams are able to obtain competitive results in a cross-corpus scenario. However, n-grams use to be filtered in order to reduce dimensionality, and generally the most frequent ones are selected. Nevertheless, omitting some of the rarest terms is fairly common and necessary for top performance. We propose a Low Dimensionality Statistical Embedding (LDSE) to represent the documents on the basis of the probability distribution of the occurrence of all their terms in the different languages, i.e. L1. Furthermore, LDSE represents texts"
W18-1605,W13-1725,1,0.886192,"Missing"
W18-1605,D14-1142,0,0.0173944,"e the model. To evaluate the task, we have proposed a statistical embedding representation that we have compared with common single-layer approaches based on n-grams, obtaining encouraging results. CH ID L1 1,139 1,103 22,549 1,102 1,143 8 Others 23,931 10,997 16,102 10,998 23,923 74 Table 1: Number of documents in each corpus. L1 corresponds to the documents written by authors of the native language to be identified. Others comprise all the documents written by authors of the other native languages in the corpus. 3 Low Dimensionality Statistical Embedding As shown in (Brooke and Hirst, 2012; Ionescu et al., 2014), single-layer representations such as n-grams are able to obtain competitive results in a cross-corpus scenario. However, n-grams use to be filtered in order to reduce dimensionality, and generally the most frequent ones are selected. Nevertheless, omitting some of the rarest terms is fairly common and necessary for top performance. We propose a Low Dimensionality Statistical Embedding (LDSE) to represent the documents on the basis of the probability distribution of the occurrence of all their terms in the different languages, i.e. L1. Furthermore, LDSE represents texts without the need of us"
W18-1605,J16-3005,0,0.0416635,"Missing"
W18-1605,N15-1160,0,0.0360884,"Missing"
W18-1605,W17-5007,0,0.0418799,"Missing"
W18-1605,W13-1706,0,0.0388044,"Missing"
W18-1605,C12-1158,0,0.076426,"Missing"
W18-5510,S17-2082,0,0.0608371,"Missing"
W18-5510,C18-1158,0,0.1433,"Missing"
W18-5510,P17-2102,0,0.0691362,"Missing"
W18-5510,S17-2083,0,0.012012,"automatic perspective. Fake news gained large attention recently from the natural language processing 1 https://www.theguardian.com/media/2016/dec/18/whatis-fake-news-pizzagate 2 http://fever.ai/task.html 66 Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 66–71 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics ter was the source for rumors datasets. Therefore, the proposed approaches for rumors focused more on the propagation of tweets (ex. retweet ratio (Enayet and El-Beltagy, 2017)) and the writing style of the tweets (Kochkina et al., 2017). trieved evidence (in case there are many) should be labeled as ”Supported”, ”Refuted” or ”NotEnoughInfo” (if there isn’t sufficient evidence to either support or refute it). The given attention to fake news and rumors detection in the literature is more than the one gained by detecting worthy claims. The orientation in these works was towards inferring these worthy claims using linguistic and stylistic aspects (Ghanem et al., 2018c; Hassan et al., 2015). 2 3 3.1 Stance Detection in FNC-1 Task Given a pair of text fragments (title and article) obtained from news, the task goal is to estimate"
W18-5510,K15-1032,0,0.0601701,"Missing"
W18-5510,S16-1003,0,0.211175,"Missing"
W19-7723,albogamy-ramsay-2017-universal,0,0.0310713,"he UD-Italian treebank (Simi et al., 2014) and PoSTWITA-UD (Sanguinetti et al., 2017; Sanguinetti et al., 2018). The former entails standard texts drawn from newspapers, legal codes and Wikipedia, the latter texts from social media. The genre of social media texts can be a bottleneck for morphological and syntactic analysis, but some experiments are reported in literature about parsing this type of data, see e.g. (Foster et al., 2011) and (Kong et al., 2014), who introduce the dependency parser TWEEBOPARSER and TWEEBANK, a Twitter treebank later extended in TWEEBANK V 2 (Liu et al., 2018). In Albogamy and Ramsay (2017) an Arabic dependency treebank of tweets is converted in the UD format, while in (Blodgett et al., 2018) a treebank of tweets in African-American English is created, and in Bhat et al. (2018) a UD treebank of HindiEnglish is created focusing on syntactic aspects of code-switching. Finally, addressing the morphological analysis of social media, the task organized in the 2016’s edition of EVALITA2 can be cited. In this edition of the evaluation campaign for NLP and speech tools for Italian, a task about PoS-tagging of social media texts has been organized (Bosco et al., 2016) which was centered"
W19-7723,P18-1131,0,0.196739,"Missing"
W19-7723,de-marneffe-etal-2014-universal,0,0.0943198,"Missing"
W19-7723,E17-1025,1,0.923715,"` In this paper we introduce a novel Twitter treebank for Italian, i.e. TWITTIRO-UD. The data come from a resource originally developed for training and testing irony detection systems, also exploited as a benchmark for the Italian irony detection task held in EVALITA 20181 (Cignarella et al., 2018b). In order to pave the way towards collecting evidences about the relationships between syntax and semantic knowledge involved in SA tasks we are developing this project of annotation which encompasses in ` TWITTIRO-UD both the fine-grained annotation for irony applied in a multilingual setting in Karoui et al. (2017) and that morphological and syntactic provided by Universal Dependencies (UD). An alike resource will allow us to extract morphological and syntactic features to be used to improve the performance in irony and stance detection tasks (Duric and Song, 2012; Sidorov et al., 2014). The UD resources available for Italian and social media meaningfully helped us in the morphological and syntactic analysis of the dataset (Bosco et al., 2014; Sanguinetti et al., 2017; Sanguinetti et al., 2018). This paper is organized as follows. The next section briefly surveys the literature about Italian social medi"
W19-7723,D14-1108,0,0.077505,"Missing"
W19-7723,N18-1088,0,0.0670694,"ce, are two: namely, the UD-Italian treebank (Simi et al., 2014) and PoSTWITA-UD (Sanguinetti et al., 2017; Sanguinetti et al., 2018). The former entails standard texts drawn from newspapers, legal codes and Wikipedia, the latter texts from social media. The genre of social media texts can be a bottleneck for morphological and syntactic analysis, but some experiments are reported in literature about parsing this type of data, see e.g. (Foster et al., 2011) and (Kong et al., 2014), who introduce the dependency parser TWEEBOPARSER and TWEEBANK, a Twitter treebank later extended in TWEEBANK V 2 (Liu et al., 2018). In Albogamy and Ramsay (2017) an Arabic dependency treebank of tweets is converted in the UD format, while in (Blodgett et al., 2018) a treebank of tweets in African-American English is created, and in Bhat et al. (2018) a UD treebank of HindiEnglish is created focusing on syntactic aspects of code-switching. Finally, addressing the morphological analysis of social media, the task organized in the 2016’s edition of EVALITA2 can be cited. In this edition of the evaluation campaign for NLP and speech tools for Italian, a task about PoS-tagging of social media texts has been organized (Bosco et"
W19-7723,W17-6526,1,0.844708,"this project of annotation which encompasses in ` TWITTIRO-UD both the fine-grained annotation for irony applied in a multilingual setting in Karoui et al. (2017) and that morphological and syntactic provided by Universal Dependencies (UD). An alike resource will allow us to extract morphological and syntactic features to be used to improve the performance in irony and stance detection tasks (Duric and Song, 2012; Sidorov et al., 2014). The UD resources available for Italian and social media meaningfully helped us in the morphological and syntactic analysis of the dataset (Bosco et al., 2014; Sanguinetti et al., 2017; Sanguinetti et al., 2018). This paper is organized as follows. The next section briefly surveys the literature about Italian social media UD resources. Section 3 introduces the dataset used for our project and describes the various 1 http://www.evalita.it/2018 annotation steps. In Sections 4 we discuss the creation of the gold standard set, and we highlight the findings of a quantitative analysis. Finally, in Section 5 we draw some considerations on the current state of the project and give some insights on future work. 2 Related Work In recent years UD have become the standard for syntactic"
W19-7723,L18-1279,1,0.567636,"which encompasses in ` TWITTIRO-UD both the fine-grained annotation for irony applied in a multilingual setting in Karoui et al. (2017) and that morphological and syntactic provided by Universal Dependencies (UD). An alike resource will allow us to extract morphological and syntactic features to be used to improve the performance in irony and stance detection tasks (Duric and Song, 2012; Sidorov et al., 2014). The UD resources available for Italian and social media meaningfully helped us in the morphological and syntactic analysis of the dataset (Bosco et al., 2014; Sanguinetti et al., 2017; Sanguinetti et al., 2018). This paper is organized as follows. The next section briefly surveys the literature about Italian social media UD resources. Section 3 introduces the dataset used for our project and describes the various 1 http://www.evalita.it/2018 annotation steps. In Sections 4 we discuss the creation of the gold standard set, and we highlight the findings of a quantitative analysis. Finally, in Section 5 we draw some considerations on the current state of the project and give some insights on future work. 2 Related Work In recent years UD have become the standard for syntactic annotation (De Marneffe et"
W19-7723,simi-etal-2014-less,1,0.91562,"e highlight the findings of a quantitative analysis. Finally, in Section 5 we draw some considerations on the current state of the project and give some insights on future work. 2 Related Work In recent years UD have become the standard for syntactic annotation (De Marneffe et al., 2014; Nivre et al., 2016) and the repository of UD projects enlarges by the day, also including data for under-resourced languages and less studied varieties, see e.g. Wang et al. (2017). As far as Italian is concerned, the main UD resources, that we exploited as reference, are two: namely, the UD-Italian treebank (Simi et al., 2014) and PoSTWITA-UD (Sanguinetti et al., 2017; Sanguinetti et al., 2018). The former entails standard texts drawn from newspapers, legal codes and Wikipedia, the latter texts from social media. The genre of social media texts can be a bottleneck for morphological and syntactic analysis, but some experiments are reported in literature about parsing this type of data, see e.g. (Foster et al., 2011) and (Kong et al., 2014), who introduce the dependency parser TWEEBOPARSER and TWEEBANK, a Twitter treebank later extended in TWEEBANK V 2 (Liu et al., 2018). In Albogamy and Ramsay (2017) an Arabic depen"
W19-7723,K17-3009,0,0.0971805,"Missing"
W19-7723,K17-3001,0,0.307221,"r we introduce a novel Twitter treebank for Italian, i.e. TWITTIRO-UD. The data come from a resource originally developed for training and testing irony detection systems, also exploited as a benchmark for the Italian irony detection task held in EVALITA 20181 (Cignarella et al., 2018b). In order to pave the way towards collecting evidences about the relationships between syntax and semantic knowledge involved in SA tasks we are developing this project of annotation which encompasses in ` TWITTIRO-UD both the fine-grained annotation for irony applied in a multilingual setting in Karoui et al. (2017) and that morphological and syntactic provided by Universal Dependencies (UD). An alike resource will allow us to extract morphological and syntactic features to be used to improve the performance in irony and stance detection tasks (Duric and Song, 2012; Sidorov et al., 2014). The UD resources available for Italian and social media meaningfully helped us in the morphological and syntactic analysis of the dataset (Bosco et al., 2014; Sanguinetti et al., 2017; Sanguinetti et al., 2018). This paper is organized as follows. The next section briefly surveys the literature about Italian social medi"
W19-7723,P17-1159,0,0.0898847,"roject and describes the various 1 http://www.evalita.it/2018 annotation steps. In Sections 4 we discuss the creation of the gold standard set, and we highlight the findings of a quantitative analysis. Finally, in Section 5 we draw some considerations on the current state of the project and give some insights on future work. 2 Related Work In recent years UD have become the standard for syntactic annotation (De Marneffe et al., 2014; Nivre et al., 2016) and the repository of UD projects enlarges by the day, also including data for under-resourced languages and less studied varieties, see e.g. Wang et al. (2017). As far as Italian is concerned, the main UD resources, that we exploited as reference, are two: namely, the UD-Italian treebank (Simi et al., 2014) and PoSTWITA-UD (Sanguinetti et al., 2017; Sanguinetti et al., 2018). The former entails standard texts drawn from newspapers, legal codes and Wikipedia, the latter texts from social media. The genre of social media texts can be a bottleneck for morphological and syntactic analysis, but some experiments are reported in literature about parsing this type of data, see e.g. (Foster et al., 2011) and (Kong et al., 2014), who introduce the dependency"
