1997.iwpt-1.16,P93-1005,0,0.0808478,"Missing"
1997.iwpt-1.16,J93-1002,0,0.171651,"d associates a probability with each parse derivation, given by the product of the probability of each change included in the derivation. Further, they also described an algorithm to handle this model within the GLR parsing framework, gaining parse efficiency. However, since their probabilistic model in itself is not intimately coupled with the GLR parsing algorithm, their model needs an additional complex algorithm for training. On the other hand, Briscoe and Carroll proposed the distribution of probabilities directly to each action in an LR table to realize mildly context-sensitive parsing' [3] . Their model overcomes the drawback of context insensitivity of PCFGs by estimating the probability of each LR parsing action according to its left (i.e. LR parse state) and right context (i.e. next input symbol) . The probability of each parse derivation is computed as the product of the probability assigned to each action included in the derivation. Unlike the approach of 1 By ""a mildly context-sensitive model"" , we mean a model that is moderately more context-sensitive than context-free models such as PCFGs, but not a fully context-sensitive one, which would b e intractable in both train"
1997.iwpt-1.16,P96-1025,0,0.0949265,"Missing"
1997.iwpt-1.16,W96-0112,0,0.0992357,"Missing"
1997.iwpt-1.16,E91-1004,0,0.038428,"Missing"
1997.iwpt-1.16,C92-2066,0,0.046663,"Missing"
1997.iwpt-1.16,1995.iwpt-1.26,0,0.157225,"Missing"
1999.mtsummit-1.80,C94-1093,0,0.0216485,"rser can work using any tag set and grammar, and using the same input bracketing, we obtain corpus that shares partial syntactic structure. 1 2 2.1 Introduction The ready availability of large corpora, especially bracketed corpora, facilitates corpus-based research such as probabilistic parsing. However each corpus has its own part-of-speech tag sets and notation schemes. Corpusbased markup schemes can become customized to a specific corpus, and incompatible with other corpora with different tag sets or notation schemes. A number of morphological information mapping methods have been proposed [5] [6] [1]. Mapping systems have rewrite rules that are derived automatically or manually, and map part-of-speech tags word by word. But mapping between part-of-speech tags, for example noun to pronoun, cannot be performed because of the large numbers of words with multiple parts-of-speech. Conventional rewrite rules consider - 543- Our Method Two Layered Grammar First of all, let us look at examples of a parse tree. (Figs. 1,2) These two trees describe possible syntactic structures of the Japanese sentence kare ga watashi ni atarashii jisho wo kure ta “He gave me a new dictionary”, where the au"
2001.mtsummit-papers.28,E99-1024,0,0.0261657,"Missing"
2001.mtsummit-papers.28,C00-2102,0,0.0346535,"Missing"
2001.mtsummit-papers.28,P94-1013,0,0.11332,"Missing"
2001.mtsummit-papers.28,P95-1026,0,0.0291228,"Missing"
2007.tmi-papers.12,W06-3123,0,0.0528538,"ability Model Marcu and Wong (2002) proposed a joint probability model. It models how source and target sentences are simultaneously generated by concepts. Many of the phrase-based SMT models require word-level alignments for extracting phrases from combinations of the alignments. On the other hand, their training method can learn word and phrase alignments at the same time for searching for optimal alignments among possible partial word sequences in sentence pairs. There was a report that the joint probability model achieved better performance on SMT, especially for smallsized training data (Birch et al., 2006). The formulation of Marcu-Wong model can be simply extended to non-parallel corpora by adding a means of handling monolingual phrases appearing independently of any counterpart. The search for optimal phrase alignments in their training method can be 1 http://www.nhk.or.jp/english/ straightforwardly viewed as ﬁnding the parallel parts in a comparable document pairs. Therefore, we choose to employ their joint probability model for comparable corpora. The main diﬃculty of the extension is the arbitrariness of deciding how many portions in each of the document pairs should be considered as unrel"
2007.tmi-papers.12,P05-1032,0,0.022937,"r to those of the Marcu-Wong method: 1. Initialize distributions. 2. For each document pair, produce an initial alignment by linking phrases so as to create bilingual or monolingual concepts that have high t for all words in the document pairs. Then hillclimb towards the Viterbi alignment by breaking and merging concepts, swapping words between concepts, and moving words across concepts, so as to maximize the product of t. 3. Update distributions with the results of hillclimbing in step 2. 4. Iterate step 2.–3. several times. We use a suﬃx array data structure for counting phrase occurrences (Callison-Burch et al., 2005), so we don’t need to select only the limited number of high-frequency n-grams as phrase candidates. In the following sections we give a detailed explanation of our extensions to the steps of the Marcu-Wong method. 3.2.1 Initializing Distributions t-distribution We deﬁne a phrase as a continuous sequence of zero or more words which does not extend more than one sentence. Under this deﬁnition, a document consisting of w words and s non-empty sentences can be) ( partitioned into i non-empty phrases in w−s i−s ways, because the document has w ¡ s partitionable word boundaries and i ¡ s times of p"
2007.tmi-papers.12,W02-1018,0,0.0275806,"ents (Munteanu and Marcu, 2006). To detect the parallel parts, most of these studies required good statistical bilingual dictionaries, which are extracted from parallel corpora. Here we face “the chicken or the egg” problem. Previous studies use preexisting parallel corpora as bootstraps to prepare dictionaries, but it would be better to obtain lexical translation knowledge and extract parallel parts (eliminate unrelated parts) from comparable corpora simultaneously without parallel corpora. In this paper, we propose an extension of the phrase-based joint probability model for SMT proposed by Marcu and Wong (2002). Our method can extract phrase alignments directly from comparable document pairs, without preexisting dictionaries or preprocessing of training data such as splitting it into sentences or extracting parallel parts. To prevent from producing alignments between unrelated phrases while searching for optimal alignments, we check each alignment as to 95 Original Japanese script: Script translated into English: 1: 地震が続いている伊豆諸島できょう午前六時四十二分頃強い地震があり式 根島で震度五弱を観測しました。 (There was a strong earthquake in the Izu Islands at 6:42 this morning, and the quake was measured the intensity of ﬁve-minus on the Jap"
2007.tmi-papers.12,W04-3243,0,0.0192735,"btain the initial cooccurrence distribution o. As in the calculation of the tdistribution, we only need to prepare the total counts before EM training. 3.2.2 Producing Alignments with Log-Likelihood Ratio (LLR) Checking To produce the alignments in step 2, we statistically check the bilingual concepts by usa c B(a|a+b, a+b )B(c|c+d, c+d ) = 2 log B(a|a+b, B(k|n, p) = (n) k a+c a+c )B(c|c+d, a+b+c+d ) a+b+c+d pk (1 ¡ p)n−k ~e ¬~e f~ ¬f~ a b c d cooccurrence count matrix ing log-likelihood ratio (LLR) statistics (Dunning, 1993) so as to produce only concepts of reliably correlated phrase pairs (Moore, 2004; Munteanu and Marcu, 2006). Note that monolingual concepts are all available without checking. The checking procedure for a concept (~e, f~) is as follows: 1. Prepare the o of the following pairs: o(~e, f~), o(~e, ¬f~) (total counts for ~e and any phrases except f~), o(¬~e, f~) and o(¬~e, ¬f~). Then calculate the LLR(~e, f~) by using the formula in Figure 2. 2. If the LLR(~e, f~) exceeds the threshold, the occurrences of ~e and f~ are considered to be reliably correlated. The correlation can be classiﬁed as positive if both ad ¡ bc > 0 in the matrix in Figure 2 and t(~e, f~) > t(~e, φ) · t(φ,"
2007.tmi-papers.12,W02-1037,0,0.846734,"and Marcu, 2006). To detect the parallel parts, most of these studies required good statistical bilingual dictionaries, which are extracted from parallel corpora. Here we face “the chicken or the egg” problem. Previous studies use preexisting parallel corpora as bootstraps to prepare dictionaries, but it would be better to obtain lexical translation knowledge and extract parallel parts (eliminate unrelated parts) from comparable corpora simultaneously without parallel corpora. In this paper, we propose an extension of the phrase-based joint probability model for SMT proposed by Marcu and Wong (2002). Our method can extract phrase alignments directly from comparable document pairs, without preexisting dictionaries or preprocessing of training data such as splitting it into sentences or extracting parallel parts. To prevent from producing alignments between unrelated phrases while searching for optimal alignments, we check each alignment as to 95 Original Japanese script: Script translated into English: 1: 地震が続いている伊豆諸島できょう午前六時四十二分頃強い地震があり式 根島で震度五弱を観測しました。 (There was a strong earthquake in the Izu Islands at 6:42 this morning, and the quake was measured the intensity of ﬁve-minus on the Jap"
2007.tmi-papers.12,P06-1011,0,0.267251,"Missing"
2007.tmi-papers.12,P99-1067,0,0.088952,"that a larger corpus size made coverage better and phrase lengths longer but did not change the precision by much. This means that LLR checking guarantees the correctness of phrasal alignments according to the LLR thresholds. 4 The asymptotic distribution of LLR statistics will follows χ2 (1), so if the LLR score of a phrase pair exceeds a threshold whose χ2 (1) probability is p, the phrase pair is considered to be correlated with an ap5 Related Work The studies on acquiring translation knowledge from non-parallel corpora started with extracting lexical translations (e.g. (Fung and Yee, 1998; Rapp, 1999)). To ﬁnd translations, they generally exploit the tendency that equivalent words have similar contextual words in corpora of diﬀerent languages. These methods are powerful in terms of their applicability even to unrelated bilingual corpora, but they provide very poor coverage. Extracting parallel segments of longer than lexical level from non-parallel corpora have been studied afterward. As for the challenges to exploit comparable corpora, there have been some eﬀorts on extracting parallel sentences (Zhao and Vogel, 2002; Munteanu and Marcu, 2002). Both studied used a statistical bilingual di"
2007.tmi-papers.12,J93-1003,0,0.065292,"ual phrase pairs described in the next section. We consider a pair of bilingual phrases ~e and f~ in a document pair (e, f ) to be cooccurring phrases if they are potentially generable by a bilingual concept; i.e. the pair is generated by a bilingual concept, or each of the pair is separately generated by a monolingual concept. In addition, we assume that only smaller number of cooccurrences between a and b are observed when ~e (we call each of them e~1 , . . . , e~a ) in e appears a times and f~ (we call each of them f~1 , . . . , f~b ) in f appears Figure 2: Log-Likelihood Ratio Statistics (Dunning, 1993) b−1 a−n b times. There are (a + n=1 c=1 c) ways of alignments between (e, f ) where the same number of ~e and f~ are generated from monolingual concepts in each side of the document pair (assuming a > b), so the cooccurrence counts for a pair (~e, f~) cooccurring in (e, f ) can be calculated as follows: ∑ ( 1+ a+ ∑b−1 ∑a−n ) n=1 ab c=1 c ∑ £ a ∑ b ∑ A(we ¡ le , se + δei , wf ¡ lf , sf + δfj ) i=1 j=1 A(we , se , wf , sf ) . (5) We collect the counts of each document pair in a corpus to obtain the initial cooccurrence distribution o. As in the calculation of the tdistribution, we only need to"
2007.tmi-papers.12,C04-1151,0,0.210014,"llel Takenobu Tokunaga‡ ‡ Department of Computer Science Tokyo Institute of Technology Tokyo, JAPAN 152-8552 take@cl.cs.titech.ac.jp corpora, comparable corpora are much easier to build from commonly available documents, such as news article pairs describing the same event in diﬀerent languages. Recently, many studies on automatic acquisition of parallel parts from noisy non-parallel corpora have been conducted to acquire larger training corpora for statistical machine translation (SMT). One of the recent studies tried to ﬁnd parallel sentences (Zhao and Vogel, 2002; Munteanu and Marcu, 2002; Fung and Cheung, 2004), and another tried to extract sub-sentential parallel fragments (Munteanu and Marcu, 2006). To detect the parallel parts, most of these studies required good statistical bilingual dictionaries, which are extracted from parallel corpora. Here we face “the chicken or the egg” problem. Previous studies use preexisting parallel corpora as bootstraps to prepare dictionaries, but it would be better to obtain lexical translation knowledge and extract parallel parts (eliminate unrelated parts) from comparable corpora simultaneously without parallel corpora. In this paper, we propose an extension of t"
2007.tmi-papers.12,P98-1069,0,0.0641852,"ons 3 and 6, we see that a larger corpus size made coverage better and phrase lengths longer but did not change the precision by much. This means that LLR checking guarantees the correctness of phrasal alignments according to the LLR thresholds. 4 The asymptotic distribution of LLR statistics will follows χ2 (1), so if the LLR score of a phrase pair exceeds a threshold whose χ2 (1) probability is p, the phrase pair is considered to be correlated with an ap5 Related Work The studies on acquiring translation knowledge from non-parallel corpora started with extracting lexical translations (e.g. (Fung and Yee, 1998; Rapp, 1999)). To ﬁnd translations, they generally exploit the tendency that equivalent words have similar contextual words in corpora of diﬀerent languages. These methods are powerful in terms of their applicability even to unrelated bilingual corpora, but they provide very poor coverage. Extracting parallel segments of longer than lexical level from non-parallel corpora have been studied afterward. As for the challenges to exploit comparable corpora, there have been some eﬀorts on extracting parallel sentences (Zhao and Vogel, 2002; Munteanu and Marcu, 2002). Both studied used a statistical"
2020.coling-main.396,N18-1118,0,0.193954,"lish ↔ German translation tasks. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) has made great progress in translating sentences in isolation. In contrast to isolated sentences, sentences in documents cannot be correctly translated without context outside the current sentence (L¨aubli et al., 2018). To address this problem, various context-aware NMT models have been developed to exploit preceding and/or succeeding sentences in source- and/or target-side languages as context (Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Voita et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018). Bawden et al. (2018) used NMT models with source- and target-side contexts in the previous sentence, and reported that the source-side context was effective for improving the translation quality. However, the target-side context led to lower quality even though they highlighted the importance of the target-side context. Agrawal et al. (2018) also pointed out that the use of the target-side context increases the risk of error propagation. We"
2020.coling-main.396,2012.eamt-1.60,0,0.527138,"tion, and over-translation not included in the ground truth sentence. Even if there are no translation errors in the predicted sentence, there is a bias of translationese that is outputs of machine translation tend to be simpler and more standardized than human-translated texts (Toral, 2019). Therefore, the predicted target-side sentence tends to have lower diversity than the ground truth sentence. This different use, called exposure bias (Bengio et al., 2015; Ranzato et al., 2016), led to lower translation quality. The following example, extracted from the IWSLT2017 Japanese-English dataset (Cettolo et al., 2012), presents the previous target-side ground truth and predicted sentences (ground truth context and predicted context) in addition to the source sentence and target sentences (reference and vanilla NMT output). This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. License details: http: 4483 Proceedings of the 28th International Conference on Computational Linguistics, pages 4483–4494 Barcelona, Spain (Online), December 8-13, 2020 Ground truth context: She lays eggs, she feeds the larvae – so an ant starts as an egg, then i"
2020.coling-main.396,W18-1820,0,0.0253658,"he frequency threshold for the vocabulary filter was set to 35. For the concatenation-based context-aware NMT, we used the encoder and decoder of the transformer model (Vaswani et al., 2017), which was a state-of-the-art NMT model. The transformer model contains a multi-headed attention mechanism, applied as self-attention, and a position-wise fully connected feed-forward network. The encoder converted the received source-language sentence into a sequence of continuous representations, and the decoder generated the target-language sentence. We implemented this system with the Sockeye toolkit (Hieber et al., 2018). For the multi-encoder context-aware NMT, we also used the transformer model and implemented it by modifying the architecture as shown in (Littell et al., 2019). All models were trained on an Nvidia P100 Tesla GPU. In training each model, we applied stochastic gradient descent (SGD) with Adam (Kingma and Ba, 2015) as the optimizer, using a learning rate of 0.0002, multiplied by 0.7 after every eight checkpoints. We set the batch size to 5000 tokens and the maximum sentence length to 200 BPE units for the concatenation-based context-aware NMT and to 100 BPE units for the multi-encoder context-"
2020.coling-main.396,D16-1132,0,0.0259148,"Examples. We show the output examples of the Japanese-to-English task with the TED Talk corpus in Table 5. “The previous ground-truth target sentence” and “The previous predicted target sentence” show sentences used as context information. “The current sentence” shows a source sentence to be translated. The multi-encoder context-aware NMTs were used for the four context-aware methods. Japanese sentences of the examples include zero anaphora, where pronouns can be omitted when they are pragmatically or grammatically inferable from intra- and inter-sentential context (Okumura and Tamura, 1996; Iida et al., 2016). The first example (#1) is the simplest case. The previous and the current sentences are same for the English-side (“We choose to go to the moon.”). On the other hand, for the Japanese-side, the subject pronoun of the current sentence (“we”) is omitted as zero anaphora. Due to zero anaphora, the Single-Sentence NMT failed to translate the current sentence. In contrast, the other four context-aware NMTs succeeded in translating the current sentence. From this example, it appears that the context-aware NMTs try to resolve the zero anaphora and are effective to translate documents correctly. The"
2020.coling-main.396,D19-6503,0,0.0465523,"ial token and discarding preceding tokens. 2.2 Multi-encoder Context-aware NMT A multi-encoder context-aware NMT models encode the previous sentence and the current sentence separately, and the two generated vectors are combined to form a single vector for the input to the decoder. In this model, there are two types of options: where to integrate and how to integrate. First, there are two choices for “where to integrate”: outside or inside the decoder. We used the approach of the integration inside the decoder with a parallel attention because it has the best reported results as described in (Kim et al., 2019). This method relates the context to the target history independently of the current source-side sentence and makes the decoding speed faster. Second, there are three choices for “how to integrate” as shown in (Bawden et al., 2018): an attention concatenation, an attention gate or a hierarchical attention. We used the attention gate because it achieved the best results in exploratory experiments. The attention gate makes a single vector c with the previous sentence vector cr and the current sentence vector cs as the below equations: g = tanh(Wr cr + Ws cs ) + bg , (1) c = g ⊙ (Wt cr ) + (1 − g"
2020.coling-main.396,C18-1050,0,0.0242998,"Vaswani et al., 2017) has made great progress in translating sentences in isolation. In contrast to isolated sentences, sentences in documents cannot be correctly translated without context outside the current sentence (L¨aubli et al., 2018). To address this problem, various context-aware NMT models have been developed to exploit preceding and/or succeeding sentences in source- and/or target-side languages as context (Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Voita et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018). Bawden et al. (2018) used NMT models with source- and target-side contexts in the previous sentence, and reported that the source-side context was effective for improving the translation quality. However, the target-side context led to lower quality even though they highlighted the importance of the target-side context. Agrawal et al. (2018) also pointed out that the use of the target-side context increases the risk of error propagation. We considered that the reason for the low quality was a gap between a training and an inference when using target-side context in"
2020.coling-main.396,D18-1512,0,0.0628951,"Missing"
2020.coling-main.396,P17-2031,0,0.0419044,"Missing"
2020.coling-main.396,W19-5326,0,0.0188859,"r model (Vaswani et al., 2017), which was a state-of-the-art NMT model. The transformer model contains a multi-headed attention mechanism, applied as self-attention, and a position-wise fully connected feed-forward network. The encoder converted the received source-language sentence into a sequence of continuous representations, and the decoder generated the target-language sentence. We implemented this system with the Sockeye toolkit (Hieber et al., 2018). For the multi-encoder context-aware NMT, we also used the transformer model and implemented it by modifying the architecture as shown in (Littell et al., 2019). All models were trained on an Nvidia P100 Tesla GPU. In training each model, we applied stochastic gradient descent (SGD) with Adam (Kingma and Ba, 2015) as the optimizer, using a learning rate of 0.0002, multiplied by 0.7 after every eight checkpoints. We set the batch size to 5000 tokens and the maximum sentence length to 200 BPE units for the concatenation-based context-aware NMT and to 100 BPE units for the multi-encoder context-aware NMT. For the other hyperparameters of the models, we used the default Sockeye parameter values. We applied early stopping with a patience of 32. Decoding w"
2020.coling-main.396,P18-1118,0,0.013563,"e translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) has made great progress in translating sentences in isolation. In contrast to isolated sentences, sentences in documents cannot be correctly translated without context outside the current sentence (L¨aubli et al., 2018). To address this problem, various context-aware NMT models have been developed to exploit preceding and/or succeeding sentences in source- and/or target-side languages as context (Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Voita et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018). Bawden et al. (2018) used NMT models with source- and target-side contexts in the previous sentence, and reported that the source-side context was effective for improving the translation quality. However, the target-side context led to lower quality even though they highlighted the importance of the target-side context. Agrawal et al. (2018) also pointed out that the use of the target-side context increases the risk of error propagation. We considered that the reason for the low quality was a gap between"
2020.coling-main.396,N19-1313,0,0.0160291,"kever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) has made great progress in translating sentences in isolation. In contrast to isolated sentences, sentences in documents cannot be correctly translated without context outside the current sentence (L¨aubli et al., 2018). To address this problem, various context-aware NMT models have been developed to exploit preceding and/or succeeding sentences in source- and/or target-side languages as context (Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Voita et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018). Bawden et al. (2018) used NMT models with source- and target-side contexts in the previous sentence, and reported that the source-side context was effective for improving the translation quality. However, the target-side context led to lower quality even though they highlighted the importance of the target-side context. Agrawal et al. (2018) also pointed out that the use of the target-side context increases the risk of error propagation. We considered that the reason for the low quality was a gap between a training and an in"
2020.coling-main.396,D18-1325,0,0.016178,"7) has made great progress in translating sentences in isolation. In contrast to isolated sentences, sentences in documents cannot be correctly translated without context outside the current sentence (L¨aubli et al., 2018). To address this problem, various context-aware NMT models have been developed to exploit preceding and/or succeeding sentences in source- and/or target-side languages as context (Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Voita et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018). Bawden et al. (2018) used NMT models with source- and target-side contexts in the previous sentence, and reported that the source-side context was effective for improving the translation quality. However, the target-side context led to lower quality even though they highlighted the importance of the target-side context. Agrawal et al. (2018) also pointed out that the use of the target-side context increases the risk of error propagation. We considered that the reason for the low quality was a gap between a training and an inference when using target-side context in the model. At the training"
2020.coling-main.396,2020.lrec-1.445,1,0.528639,"t from 2. k is a hyper-parameter to control the speed of convergence where k ≥ 1 depends on the baseline translation quality. The higher the translation quality, the smaller the value of k (set near to one). Under the above procedure, the model learns to handle the noise and be more robust against the noise in the inference. 4 Experiments To evaluate our method, we experimented on two document-level parallel datasets in two language pairs for machine translation. 4.1 Data We used two parallel corpora as follows. • News (Japanese ↔ English). We used a content-equivalent translated news corpus (Mino et al., 2020). The corpus was low-noise parallel data made by manually translating Japanese news articles into English in a content-equivalent manner. We removed news titles from development and test sets. • TED Talk (Japanese ↔ English, English ↔ German). We used the IWSLT 2017 (Cettolo et al., 2012) datasets based on the TED Talks where each talk is considered a document. We used the “train” set for training and the “dev2010” set for validation for all the tasks. The “tst2014” was used for testing English ↔ Japanese tasks and the “tst2015” was used for testing English ↔ German tasks. Table 1 shows the st"
2020.coling-main.396,W18-6307,0,0.0314817,"Missing"
2020.coling-main.396,P11-2093,0,0.0190331,"d for testing English ↔ German tasks. Table 1 shows the statistics of each corpus. 4.2 Systems Our method can be implemented to various types of context-aware NMT systems. In this paper, we applied our method to the concatenation-based context-aware NMT model (Figure 1 (a)) and the multiencoder context-aware NMT model (Figure 1 (b)). Though various numbers of sentences can be utilized as context in each model, we used only one previous sentence for each model to reduce memory consumption. 4486 4.3 Settings We used the Moses toolkit1 to clean and tokenize the English and German data and KyTea (Neubig et al., 2011) to tokenize the Japanese data. Then, we used a vocabulary of 48K units based on a joint bytepair encoding (BPE) (Sennrich et al., 2016) for the source and target. The frequency threshold for the vocabulary filter was set to 35. For the concatenation-based context-aware NMT, we used the encoder and decoder of the transformer model (Vaswani et al., 2017), which was a state-of-the-art NMT model. The transformer model contains a multi-headed attention mechanism, applied as self-attention, and a position-wise fully connected feed-forward network. The encoder converted the received source-language"
2020.coling-main.396,C96-2147,0,0.260008,"encoder context-aware NMT. Examples. We show the output examples of the Japanese-to-English task with the TED Talk corpus in Table 5. “The previous ground-truth target sentence” and “The previous predicted target sentence” show sentences used as context information. “The current sentence” shows a source sentence to be translated. The multi-encoder context-aware NMTs were used for the four context-aware methods. Japanese sentences of the examples include zero anaphora, where pronouns can be omitted when they are pragmatically or grammatically inferable from intra- and inter-sentential context (Okumura and Tamura, 1996; Iida et al., 2016). The first example (#1) is the simplest case. The previous and the current sentences are same for the English-side (“We choose to go to the moon.”). On the other hand, for the Japanese-side, the subject pronoun of the current sentence (“we”) is omitted as zero anaphora. Due to zero anaphora, the Single-Sentence NMT failed to translate the current sentence. In contrast, the other four context-aware NMTs succeeded in translating the current sentence. From this example, it appears that the context-aware NMTs try to resolve the zero anaphora and are effective to translate docu"
2020.coling-main.396,P02-1040,0,0.107725,"ontext-aware NMT. For the other hyperparameters of the models, we used the default Sockeye parameter values. We applied early stopping with a patience of 32. Decoding was performed through beam search with a beam size of 5, and we did not apply ensemble decoding with multiple models, although this could have improved the translation quality. The hyperparameter k of our proposed method in Equation (3) was set to 2. To evaluate the translation quality, we trained five models with different seeds and used the median BLEU score of the five translation results. We calculated case-insensitive BLEU (Papineni et al., 2002) scores by using multi-bleu.perl2 . 4.4 Baseline Methods To measure the effectiveness of our proposed approach, we consider the following baselines. • Sentence-level translation (Single-Sentence). To compare translations with and without context information beyond the current sentence, we used a single sentence translation as the baseline: noncontext-aware translation. The Single-Sentence method was implemented using the transformer model. • Context-aware translation with a source-side previous sentence (Src-Context), a target-side previous ground truth sentence (Trg-Context GT), and a target-"
2020.coling-main.396,P16-1162,0,0.0338119,"s types of context-aware NMT systems. In this paper, we applied our method to the concatenation-based context-aware NMT model (Figure 1 (a)) and the multiencoder context-aware NMT model (Figure 1 (b)). Though various numbers of sentences can be utilized as context in each model, we used only one previous sentence for each model to reduce memory consumption. 4486 4.3 Settings We used the Moses toolkit1 to clean and tokenize the English and German data and KyTea (Neubig et al., 2011) to tokenize the Japanese data. Then, we used a vocabulary of 48K units based on a joint bytepair encoding (BPE) (Sennrich et al., 2016) for the source and target. The frequency threshold for the vocabulary filter was set to 35. For the concatenation-based context-aware NMT, we used the encoder and decoder of the transformer model (Vaswani et al., 2017), which was a state-of-the-art NMT model. The transformer model contains a multi-headed attention mechanism, applied as self-attention, and a position-wise fully connected feed-forward network. The encoder converted the received source-language sentence into a sequence of continuous representations, and the decoder generated the target-language sentence. We implemented this syst"
2020.coling-main.396,W17-4811,0,0.33155,"in English ↔ Japanese and English ↔ German translation tasks. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) has made great progress in translating sentences in isolation. In contrast to isolated sentences, sentences in documents cannot be correctly translated without context outside the current sentence (L¨aubli et al., 2018). To address this problem, various context-aware NMT models have been developed to exploit preceding and/or succeeding sentences in source- and/or target-side languages as context (Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Voita et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018). Bawden et al. (2018) used NMT models with source- and target-side contexts in the previous sentence, and reported that the source-side context was effective for improving the translation quality. However, the target-side context led to lower quality even though they highlighted the importance of the target-side context. Agrawal et al. (2018) also pointed out that the use of the target-side context increases the risk of e"
2020.coling-main.396,W19-6627,0,0.0477646,"when using target-side context in the model. At the training phase, the ground truth sentences (references) were used as the target-side context, and at the inference phase, the sentences predicted by a translation model were used. The predicted sentence includes translation errors such as mistranslation, under-translation, and over-translation not included in the ground truth sentence. Even if there are no translation errors in the predicted sentence, there is a bias of translationese that is outputs of machine translation tend to be simpler and more standardized than human-translated texts (Toral, 2019). Therefore, the predicted target-side sentence tends to have lower diversity than the ground truth sentence. This different use, called exposure bias (Bengio et al., 2015; Ranzato et al., 2016), led to lower translation quality. The following example, extracted from the IWSLT2017 Japanese-English dataset (Cettolo et al., 2012), presents the previous target-side ground truth and predicted sentences (ground truth context and predicted context) in addition to the source sentence and target sentences (reference and vanilla NMT output). This work is licensed under a Creative Commons Attribution 4."
2020.coling-main.396,P18-1117,0,0.0936047,"tion tasks. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) has made great progress in translating sentences in isolation. In contrast to isolated sentences, sentences in documents cannot be correctly translated without context outside the current sentence (L¨aubli et al., 2018). To address this problem, various context-aware NMT models have been developed to exploit preceding and/or succeeding sentences in source- and/or target-side languages as context (Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Voita et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018). Bawden et al. (2018) used NMT models with source- and target-side contexts in the previous sentence, and reported that the source-side context was effective for improving the translation quality. However, the target-side context led to lower quality even though they highlighted the importance of the target-side context. Agrawal et al. (2018) also pointed out that the use of the target-side context increases the risk of error propagation. We considered that the"
2020.coling-main.396,D19-1081,0,0.0140121,"uction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) has made great progress in translating sentences in isolation. In contrast to isolated sentences, sentences in documents cannot be correctly translated without context outside the current sentence (L¨aubli et al., 2018). To address this problem, various context-aware NMT models have been developed to exploit preceding and/or succeeding sentences in source- and/or target-side languages as context (Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Voita et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018). Bawden et al. (2018) used NMT models with source- and target-side contexts in the previous sentence, and reported that the source-side context was effective for improving the translation quality. However, the target-side context led to lower quality even though they highlighted the importance of the target-side context. Agrawal et al. (2018) also pointed out that the use of the target-side context increases the risk of error propagation. We considered that the reason for the low q"
2020.coling-main.396,D17-1301,0,0.104717,"previous approaches in English ↔ Japanese and English ↔ German translation tasks. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) has made great progress in translating sentences in isolation. In contrast to isolated sentences, sentences in documents cannot be correctly translated without context outside the current sentence (L¨aubli et al., 2018). To address this problem, various context-aware NMT models have been developed to exploit preceding and/or succeeding sentences in source- and/or target-side languages as context (Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Voita et al., 2019; Maruf and Haffari, 2018; Maruf et al., 2019; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018). Bawden et al. (2018) used NMT models with source- and target-side contexts in the previous sentence, and reported that the source-side context was effective for improving the translation quality. However, the target-side context led to lower quality even though they highlighted the importance of the target-side context. Agrawal et al. (2018) also pointed out that the use of the target-side c"
2020.coling-main.396,P19-1426,0,0.0392117,"Missing"
2020.coling-main.396,N16-1004,0,0.0230879,"is, and ellipsis consistencies in the current sentence. Furthermore, the context can resolve anaphoric pronouns and other discourse characteristics. Most contextaware NMT systems are modified to take additional source- and/or target-side context as their input. Recent work in the context-aware NMT can be classified into multi-input models and multi-encoder models. Tiedemann and Scherrer (2017) proposed concatenating the previous and the current source sentences with a special token (such as BREAK ) as multi-input to a translation model. Bawden et al. (2018) extended multi-encoder NMT systems (Zoph and Knight, 2016; Libovick´y and Helcl, 2017; Wang et al., 2017) to exploit the previous source sentence as the context. These systems combine the information from the current sentence and the context of the source-side with either concatenation, gating or hierarchical attention. Kim et al. (2019) investigated the difference in a multi-input model and two types of multi-encoder NMT models, where the first method is an integration outside the decoder, which combines encoder representations of all input sentences before being fed to the decoder, and another method is to integrate each encoder representation ins"
2020.lrec-1.445,D19-5617,0,0.122148,"size of the corpus and trying different approaches besides human translation from scratch. Concerning its availability, we are in the process of negotiating with the content holders and public agencies to release the corpus. 3618 Table 4: Attached tags in the four corpora for each method. Domain-tag method for domain adaptation Content-equivalent corpus Automatic-alignment corpus Back-translated corpus (CE-NMT) Back-translated corpus (AA-NMT) Single tag (Kobus et al., 2017) <CE&gt; <AA&gt; <CE&gt; <AA&gt; Single tag with back-translation (Vaibhav et al., 2019) <CE&gt; <AA&gt; <BT-CE&gt; <BT-AA&gt; Two types of tags (Berard et al., 2019a) <CE&gt; <AA&gt; <BT&gt; <CE&gt; <BT&gt; <AA&gt; <NS-S&gt; <CE-T&gt; <NO-BT&gt; <NO-AN&gt; <NS-S&gt; <NS-T&gt; <NO-BT&gt; <AN&gt; <CE-S&gt; <NS-T&gt; <BT&gt; <NO-AN&gt; <NS-S&gt; <NS-T&gt; <BT&gt; <AN&gt; Multi-tag method (proposed) 3. Extension of Domain Adaptation for NMT In this section, we describe the corpora we used in our experiments. We then discuss problems in training NMT models with existing methods and propose a new method to solve those problems. 3.1. Training Corpora We used four corpora for training NMT models. Table 3 lists the numbers of sentences and words in each corpus. In addition to the content-equivalent corpus described in section 2"
2020.lrec-1.445,W19-5361,0,0.116492,"size of the corpus and trying different approaches besides human translation from scratch. Concerning its availability, we are in the process of negotiating with the content holders and public agencies to release the corpus. 3618 Table 4: Attached tags in the four corpora for each method. Domain-tag method for domain adaptation Content-equivalent corpus Automatic-alignment corpus Back-translated corpus (CE-NMT) Back-translated corpus (AA-NMT) Single tag (Kobus et al., 2017) <CE&gt; <AA&gt; <CE&gt; <AA&gt; Single tag with back-translation (Vaibhav et al., 2019) <CE&gt; <AA&gt; <BT-CE&gt; <BT-AA&gt; Two types of tags (Berard et al., 2019a) <CE&gt; <AA&gt; <BT&gt; <CE&gt; <BT&gt; <AA&gt; <NS-S&gt; <CE-T&gt; <NO-BT&gt; <NO-AN&gt; <NS-S&gt; <NS-T&gt; <NO-BT&gt; <AN&gt; <CE-S&gt; <NS-T&gt; <BT&gt; <NO-AN&gt; <NS-S&gt; <NS-T&gt; <BT&gt; <AN&gt; Multi-tag method (proposed) 3. Extension of Domain Adaptation for NMT In this section, we describe the corpora we used in our experiments. We then discuss problems in training NMT models with existing methods and propose a new method to solve those problems. 3.1. Training Corpora We used four corpora for training NMT models. Table 3 lists the numbers of sentences and words in each corpus. In addition to the content-equivalent corpus described in section 2"
2020.lrec-1.445,W19-5206,0,0.0355505,"Missing"
2020.lrec-1.445,P17-2061,0,0.0211328,"corpora, to train NMT systems. The content-equivalent corpus was nonnoisy. In contrast, the automatic-alignment corpus was noisy because of the two factors: first, that the target-side English sentences were generated through news writing, and second, that the Japanese and English sentences were aligned automatically. Furthermore, the back-translated corpus was noisy because the source-side Japanese sentences were generated by an English→Japanese NMT system. Table 1 lists the feature differences between the corpora. To exploit corpora with these different features, a method of domain tagging (Chu et al., 2017; Kobus et al., 2017), which is a domain-adaptation technique, can be applied. Unfortunately, existing domain-tag methods cannot sufficiently express the differences between these corpora because of the lack of tag information. For example, though the target-side sentences in both the automatic-alignment corpus and the back-translated corpus come from the same domain of original news, as listed in Table 1, the existing methods cannot express this difference. Such features of the target-side sentences are significant for controlling the output sentences. To solve this problem, we developed a mu"
2020.lrec-1.445,W18-1820,0,0.0760458,"sed on a joint bytepair encoding (BPE) (Sennrich et al., 2016b) for the source and target. For the translation model, we used the encoder and decoder of the transformer model (Vaswani et al., 2017), which is a state-of-the-art NMT model. The transformer model uses a multi-headed attention mechanism, applied as selfattention, and a position-wise fully connected feed-forward network. The encoder converted the received sourcelanguage sentence into a sequence of continuous representations, and the decoder generated the target-language sentence. We implemented our systems with the Sockeye toolkit (Hieber et al., 2018) and trained them on an 4 3620 https://github.com/moses- smt/ mosesdecoder Table 5: Japanese→English translation results without domain-adaptation. Training corpora Data size Content-equivalent corpus 0.22M Automatic-alignment corpus 0.29M Content-equivalent corpus, automatic-alignment corpus 0.51M Content-equivalent corpus, automatic-alignment corpus, back-translated corpora 1.57M Table 6: Japanese→English translation results with domain-adaptation. BLEU score Domain-adaptation method No tag 20.36 Single tag 22.41 Single tag with back-translation 24.19 Two types of tags 24.25 Proposed method"
2020.lrec-1.445,kobus-etal-2017-domain,0,0.169573,"NMT systems. The content-equivalent corpus was nonnoisy. In contrast, the automatic-alignment corpus was noisy because of the two factors: first, that the target-side English sentences were generated through news writing, and second, that the Japanese and English sentences were aligned automatically. Furthermore, the back-translated corpus was noisy because the source-side Japanese sentences were generated by an English→Japanese NMT system. Table 1 lists the feature differences between the corpora. To exploit corpora with these different features, a method of domain tagging (Chu et al., 2017; Kobus et al., 2017), which is a domain-adaptation technique, can be applied. Unfortunately, existing domain-tag methods cannot sufficiently express the differences between these corpora because of the lack of tag information. For example, though the target-side sentences in both the automatic-alignment corpus and the back-translated corpus come from the same domain of original news, as listed in Table 1, the existing methods cannot express this difference. Such features of the target-side sentences are significant for controlling the output sentences. To solve this problem, we developed a multi-tag method that c"
2020.lrec-1.445,W04-3250,0,0.118344,"s, we used the default Sockeye parameter values. We applied early stopping with a patience of 32. Decoding was performed through beam search with a beam size of 5, and we did not apply ensemble decoding with multiple models, although this could have improved the translation quality. To evaluate the translation quality, we trained five models with different seeds, and we used the median BLEU score of the five translation results. We calculated casesensitive BLEU (Papineni et al., 2002) scores by using multi-bleu.perl 5 . We used a statistical significance test with paired bootstrap resampling (Koehn, 2004), and the threshold was set to p = 0.05. 4.2. Results 4.2.1. Effectiveness of the Content-Equivalent Corpus Table 5 summarizes the experimental results of NMT models with different combinations of the four corpora, as listed in the first column. The NMT model trained with only the content-equivalent corpus achieved a BLEU score of 20.93, which was the best score for Japanese→English news translation without using a domain-adaptation technique. Comparing the content-equivalent corpus and the automaticalignment corpus, though the amount of data in the contentequivalent corpus was lower, the BLEU"
2020.lrec-1.445,2015.iwslt-evaluation.11,0,0.0224026,"ese direction, adapted to the content-equivalent and automatic-alignment corpora, respectively. Then, we translated all the English monolingual news data into Japanese with each of the two NMT models. We call these corpora the “back-translated corpus (CE-NMT)” and the “back-translated corpus (AA-NMT).” Though the amount of parallel data was increased through the back-translation, the augmented data included noise on the source side (Japanese) because of the use of imperfect translation results by the NMT system. ever, to have degraded performance when trained with out-of-domain or noisy data (Luong and Manning, 2015; Belinkov and Bisk, 2018). Domain adaptation, which refers to the domain shift between the in-domain (same domain as the test set) and out-of-domain data, is one technique to address this problem. Kobus et al. (2017) and Chu et al. (2017) proposed a domain-adaptation technique for multi-domain NMT, which consists of inserting a domain tag into each source-side data entry to specify the domain of the corpus. Furthermore, Berard et al. (2019b) proposed two types of tags, a corpus tag and a type tag, also called a noise tag. The corpus tag indicates the domain of each corpus, as in Kobus et al."
2020.lrec-1.445,P11-2093,0,0.0262696,"e proposed domainadaptation method, we used both the existing methods and the proposed method to train NMT models with the four corpora. 4.1. Datasets and Setup We used the parallel corpora described in section 3.1. Among these parallel corpora, we made a test set (size 2.0K) from the content-equivalent corpus because the automatic-alignment corpus includes noises, as shown in Figure 1. We used the remaining data in the contentequivalent corpus as training data. All of the datasets were preprocessed as follows. We used the Moses toolkit 4 to clean and tokenize the English data and used KyTea (Neubig et al., 2011) to tokenize the Japanese data. Then, we used a vocabulary of 32K units based on a joint bytepair encoding (BPE) (Sennrich et al., 2016b) for the source and target. For the translation model, we used the encoder and decoder of the transformer model (Vaswani et al., 2017), which is a state-of-the-art NMT model. The transformer model uses a multi-headed attention mechanism, applied as selfattention, and a position-wise fully connected feed-forward network. The encoder converted the received sourcelanguage sentence into a sequence of continuous representations, and the decoder generated the targe"
2020.lrec-1.445,P02-1040,0,0.109883,"nts. We set the batch size to 5000 tokens and the maximum sentence length to 99 BPE units. For the other hyperparameters of the models, we used the default Sockeye parameter values. We applied early stopping with a patience of 32. Decoding was performed through beam search with a beam size of 5, and we did not apply ensemble decoding with multiple models, although this could have improved the translation quality. To evaluate the translation quality, we trained five models with different seeds, and we used the median BLEU score of the five translation results. We calculated casesensitive BLEU (Papineni et al., 2002) scores by using multi-bleu.perl 5 . We used a statistical significance test with paired bootstrap resampling (Koehn, 2004), and the threshold was set to p = 0.05. 4.2. Results 4.2.1. Effectiveness of the Content-Equivalent Corpus Table 5 summarizes the experimental results of NMT models with different combinations of the four corpora, as listed in the first column. The NMT model trained with only the content-equivalent corpus achieved a BLEU score of 20.93, which was the best score for Japanese→English news translation without using a domain-adaptation technique. Comparing the content-equival"
2020.lrec-1.445,P16-1009,0,0.318337,"ple of a Japanese-English parallel sentence pair with noise. The underlined parts are not in the other language. call it a “content-equivalent corpus.” Because the amount of content-equivalent corpus data is insufficient for developing a high-quality Japanese→English NMT system for news, we constructed two more Japanese-English parallel news corpora with different features. One was made with an automatic sentence alignment method, as in the case of existing parallel news corpora with noise, and we call this an “automatic-alignment corpus.” The other was made with a back-translation technique (Sennrich et al., 2016a) to leverage monolingual English news articles, and we call this a “back-translated corpus.” Thus, we used three different types of parallel data, from the content-equivalent, 3616 Table 1: Feature differences between corpora. Corpus name Source Target Content-equivalent corpus Original news Content-equivalent news Automaticalignment corpus Original news Original news (with noise between source and target sides) Back-translated corpus MT output Original news automatic-alignment, and back-translated corpora, to train NMT systems. The content-equivalent corpus was nonnoisy. In contrast, the au"
2020.lrec-1.445,P16-1162,0,0.393585,"ple of a Japanese-English parallel sentence pair with noise. The underlined parts are not in the other language. call it a “content-equivalent corpus.” Because the amount of content-equivalent corpus data is insufficient for developing a high-quality Japanese→English NMT system for news, we constructed two more Japanese-English parallel news corpora with different features. One was made with an automatic sentence alignment method, as in the case of existing parallel news corpora with noise, and we call this an “automatic-alignment corpus.” The other was made with a back-translation technique (Sennrich et al., 2016a) to leverage monolingual English news articles, and we call this a “back-translated corpus.” Thus, we used three different types of parallel data, from the content-equivalent, 3616 Table 1: Feature differences between corpora. Corpus name Source Target Content-equivalent corpus Original news Content-equivalent news Automaticalignment corpus Original news Original news (with noise between source and target sides) Back-translated corpus MT output Original news automatic-alignment, and back-translated corpora, to train NMT systems. The content-equivalent corpus was nonnoisy. In contrast, the au"
2020.lrec-1.445,2007.mtsummit-papers.63,0,0.126736,"-alignment corpus <BT-CE&gt; : for the back-translated corpus (CE-NMT) <BT-AA&gt; : for the back-translated corpus (AANMT) • A method with two types of tags (Berard et al., 2019b) used the following tags: <BT&gt; : for the back-translated corpora (noise tag) <CE&gt; : for the content-equivalent corpus (corpus tag) <AA&gt; : for the automatic-alignment corpus (corpus tag) 3.2. Existing Domain-Adaptation Methods Using Tags The naive approach for NMT training is to use the entire corpora. NMT systems have been shown, how3 One project member (c.f. Acknowledgments) applied an automatic sentence alignment method (Utiyama and Isahara, 2007) to the bilingual data in Table 2. Thus, we could fully leverage the results to make an automatic-alignment corpus. A tag was attached to the top of a source-side sentence. The first three rows in Table 4 list the attached tags for the three 3619 <CE-S&gt;: for corpora with source-side sentences back-translated with CE-NMT existing domain-adaptation methods when implemented in the four corpora. 3.3. Problems of Existing Domain-Adaptation Methods Using Tag The existing methods using tags were proposed to distinguish corpora with different features for training. An NMT model using tags can learn th"
2020.lrec-1.445,N19-1190,0,0.0376672,"Missing"
2020.lrec-1.445,D17-1147,0,0.0371563,"Missing"
2020.lrec-1.445,C18-1269,0,0.0311604,"Missing"
2020.lrec-1.854,D17-1141,0,0.0226897,"other aspects of how it is connected to the argument. Discourse structure provides not only a means to reorder sentences automatically, but it also ex1 The concept behind a major claim has also variously been referred to as main stance or conclusion in the literature. plains the ways which the improved texts are better than the original ones. Therefore, the annotation of discourse structure is a prerequisite for a sentence order analysis. In its final application, an analysis of both the discourse structure and sentence order is also helpful for downstream tasks ˇ such as essay assessment (Al Khatib et al., 2017; Snajder et al., 2019) and education (Iida and Tokunaga, 2014; Cullen et al., 2018; Matsumura and Sakamoto, 2019) Since the texts we wish to annotate are argumentative, we employ the approach from the argument mining field. In the NLP community, argument mining is an emerging area2 aimed to analyse argumentative texts from a multidisciplinary perspective, including logic, rhetoric and language (Lippi and Torroni, 2016). It aims to provide structured data for computational models of argument and reasoning engines (Lippi and Torroni, 2016). Traditionally, the annotation of argumentative discour"
2020.lrec-1.854,L18-1304,0,0.0385498,"Missing"
2020.lrec-1.854,J86-3001,0,0.811577,"s based on standard web technologies and can be easily customised to other annotation schemes, it can be easily used by anybody. Apart from the project it was originally designed for, in which hundreds of texts were annotated by three annotators, TIARA has already been adopted by a second discourse annotation study, which uses it in the teaching of argumentation. Keywords: annotation tool, discourse annotation, discourse relations, sentence reordering, teaching of argumentation 1. Introduction There are many important aspects in writing such as grammar, mechanics, writing style and coherence (Grosz and Sidner, 1986; Lee and Webster, 2012). Out of these aspects, textual coherence, as an aspect of discourse structure, is extremely important. It concerns how sentences or other discourse units form a flow of meaning (Grosz and Sidner, 1986), and has been extensively analysed in the past (Jacobs et al., 1981; Grosz et al., 1995; Mann and Thompson, 1988; Wolf and Gibson, 2005; Garing, 2014). Our longterm goal is to automatically reorder sentences in argumentative essays so that their coherence is improved, as well as providing an explanation for the changes made. Therefore, the analysis of the order of senten"
2020.lrec-1.854,J95-2003,0,0.64692,"which uses it in the teaching of argumentation. Keywords: annotation tool, discourse annotation, discourse relations, sentence reordering, teaching of argumentation 1. Introduction There are many important aspects in writing such as grammar, mechanics, writing style and coherence (Grosz and Sidner, 1986; Lee and Webster, 2012). Out of these aspects, textual coherence, as an aspect of discourse structure, is extremely important. It concerns how sentences or other discourse units form a flow of meaning (Grosz and Sidner, 1986), and has been extensively analysed in the past (Jacobs et al., 1981; Grosz et al., 1995; Mann and Thompson, 1988; Wolf and Gibson, 2005; Garing, 2014). Our longterm goal is to automatically reorder sentences in argumentative essays so that their coherence is improved, as well as providing an explanation for the changes made. Therefore, the analysis of the order of sentence in original texts (which we call first drafts) and in their improved versions is necessary. A tool is needed to support these annotations, and we believe that it can be designed in such a way that it is useful for language and argument education in general. Existing theory of text coherence stipulate that the"
2020.lrec-1.854,iida-tokunaga-2014-building,1,0.490454,"ourse structure provides not only a means to reorder sentences automatically, but it also ex1 The concept behind a major claim has also variously been referred to as main stance or conclusion in the literature. plains the ways which the improved texts are better than the original ones. Therefore, the annotation of discourse structure is a prerequisite for a sentence order analysis. In its final application, an analysis of both the discourse structure and sentence order is also helpful for downstream tasks ˇ such as essay assessment (Al Khatib et al., 2017; Snajder et al., 2019) and education (Iida and Tokunaga, 2014; Cullen et al., 2018; Matsumura and Sakamoto, 2019) Since the texts we wish to annotate are argumentative, we employ the approach from the argument mining field. In the NLP community, argument mining is an emerging area2 aimed to analyse argumentative texts from a multidisciplinary perspective, including logic, rhetoric and language (Lippi and Torroni, 2016). It aims to provide structured data for computational models of argument and reasoning engines (Lippi and Torroni, 2016). Traditionally, the annotation of argumentative discourse structure consists of two main steps. The first of these is"
2020.lrec-1.854,kaplan-etal-2010-annotation,1,0.849305,"eordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too much). Task-specific tools are often simply better for the annotation process and can lead to a better inter-annotator agreement (Sonntag and Stede, 2014). This paper presents TIARA,3 a new client-side tool for annotating discourse structure and sentence reordering to support our goal."
2020.lrec-1.854,W15-0501,0,0.420687,"iates them into argumentative or non-argumentative components (Stab and Gurevych, 2014; Lippi and Torroni, 2016). Argumentative components (ACs) can be further classified according to their rhetorical function in the discourse, e.g., into major claim, claim and premise (Stab and Gurevych, 2014). The second step is argumentative discourse structure prediction, which links ACs and labels the links in order to form the structured representation of the text. All ACs must be connected to the structure, while non-ACs remain unconnected. Links can be directed (Stab and Gurevych, 2014) or undirected (Kirschner et al., 2015). A new discourse annotation study often has specific, so far unserved needs, and we are no exception. Taking an empirical approach to the task of sentence reordering, we 2 The interest of the NLP community towards this is proven by Argument Mining Workshop series at ACL conferences. Readers may refer to Lippi and Torroni (2016) and Lytos et al. (2019) as an overview of this field. 6912 need two kinds of annotation: (1) discourse structure and (2) sentence reordering. This will allow us to analyse and correlate the discourse characteristics of the first drafts and their improved versions. Whil"
2020.lrec-1.854,P12-2049,0,0.0246766,"technologies and can be easily customised to other annotation schemes, it can be easily used by anybody. Apart from the project it was originally designed for, in which hundreds of texts were annotated by three annotators, TIARA has already been adopted by a second discourse annotation study, which uses it in the teaching of argumentation. Keywords: annotation tool, discourse annotation, discourse relations, sentence reordering, teaching of argumentation 1. Introduction There are many important aspects in writing such as grammar, mechanics, writing style and coherence (Grosz and Sidner, 1986; Lee and Webster, 2012). Out of these aspects, textual coherence, as an aspect of discourse structure, is extremely important. It concerns how sentences or other discourse units form a flow of meaning (Grosz and Sidner, 1986), and has been extensively analysed in the past (Jacobs et al., 1981; Grosz et al., 1995; Mann and Thompson, 1988; Wolf and Gibson, 2005; Garing, 2014). Our longterm goal is to automatically reorder sentences in argumentative essays so that their coherence is improved, as well as providing an explanation for the changes made. Therefore, the analysis of the order of sentence in original texts (wh"
2020.lrec-1.854,D17-1019,0,0.0205955,"While there are publicly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too much). Task-specific too"
2020.lrec-1.854,L16-1371,0,0.0187,"nd Y are potentially confusing when annotators often change the links labelled with X to Y (and vice versa). TIARA does this by logging the actions performed by annotators in each annotation-file. (d) Ease of use, installation and deployment Ease of use and installation for annotators is often prioritised for annotation design, but we believe that deployment is equally important. Not every project owner is tech-savvy; for them, an annotation tool that is hard to deploy is practically unusable. In contrast, tools that are usable without deployment and may run at client-side, such as EasyTree5 (Little and Tratz, 2016), are able to reach and help as many potential users as possible, including those who have no knowledge in programming. Therefore, TIARA shares the same principle. Users only need a web browser and the TIARA package. This tool is written in javascript, html and css. We use JsPlumb6 and Treant-js7 as the visualisation libraries. We understand that the necessity of deployment (server-side) is often coupled with file and/or annotation management features (Yimam et al., 2013), and this is important in a large annotation project. While the current version of TIARA does not actively support such ann"
2020.lrec-1.854,C04-1108,0,0.0459512,"eir improved versions. While there are publicly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too m"
2020.lrec-1.854,prasad-etal-2008-penn,0,0.111939,"re easy to customise, offering the flexibility to accomodate a wide range of annotation tasks. However, BRAT and WebAnno were originally designed for morphological, syntactic and semantic annotations (i.e., rather local word or phrase-level annotation). While they support link display and could thus theoretically be used for discourse annotation, the visual display of links appears as drawn directly on top of text. This style of display has already been identified by others as a source of confusion for argumentation and discourse annotation projects (Kirschner et al., 2015). PDTB annotator10 (Prasad et al., 2008) also falls into the class of annotation tools designed for local relations. When it comes to the display of larger-scale hierarchical or graphical structure of discourse, this falls entirely outside the purview of these tools. Annotation tools which are specifically aimed at visualising larger-scale and more global discourse structure have also been developed, e.g., RSTTool11 (Mann and Thompson, 1988), TreeAnno12 (De Kuthy et al., 2018), OVA13 (Janier et al., 2014), DiGAT14 (Kirschner et al., 8 Figure 3: Example of TIARA’s configuration script (written in javascript). (h) Saving annotation Us"
2020.lrec-1.854,W19-4405,0,0.184556,"ks ACs and labels the links in order to form the structured representation of the text. All ACs must be connected to the structure, while non-ACs remain unconnected. Links can be directed (Stab and Gurevych, 2014) or undirected (Kirschner et al., 2015). A new discourse annotation study often has specific, so far unserved needs, and we are no exception. Taking an empirical approach to the task of sentence reordering, we 2 The interest of the NLP community towards this is proven by Argument Mining Workshop series at ACL conferences. Readers may refer to Lippi and Torroni (2016) and Lytos et al. (2019) as an overview of this field. 6912 need two kinds of annotation: (1) discourse structure and (2) sentence reordering. This will allow us to analyse and correlate the discourse characteristics of the first drafts and their improved versions. While there are publicly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no exist"
2020.lrec-1.854,sonntag-stede-2014-grapat,0,0.281546,"they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too much). Task-specific tools are often simply better for the annotation process and can lead to a better inter-annotator agreement (Sonntag and Stede, 2014). This paper presents TIARA,3 a new client-side tool for annotating discourse structure and sentence reordering to support our goal. We outline our annotation needs (i.e., target domain, annotation scheme) in Section 2, and describe how these requirements translate to design considerations and features of the tool in Section 3. Section 4 shows how it sits among other annotation tools. Section 5 outlines how TIARA benefits other domains such as education, i.e., in the teaching of argumentative writing. Finally, Section 6 concludes this paper and describes what can be improved in the future. 2."
2020.lrec-1.854,C14-1142,0,0.723504,"munity, argument mining is an emerging area2 aimed to analyse argumentative texts from a multidisciplinary perspective, including logic, rhetoric and language (Lippi and Torroni, 2016). It aims to provide structured data for computational models of argument and reasoning engines (Lippi and Torroni, 2016). Traditionally, the annotation of argumentative discourse structure consists of two main steps. The first of these is argument component detection. This step determines the boundaries of discourse units (segmentation) and differentiates them into argumentative or non-argumentative components (Stab and Gurevych, 2014; Lippi and Torroni, 2016). Argumentative components (ACs) can be further classified according to their rhetorical function in the discourse, e.g., into major claim, claim and premise (Stab and Gurevych, 2014). The second step is argumentative discourse structure prediction, which links ACs and labels the links in order to form the structured representation of the text. All ACs must be connected to the structure, while non-ACs remain unconnected. Links can be directed (Stab and Gurevych, 2014) or undirected (Kirschner et al., 2015). A new discourse annotation study often has specific, so far u"
2020.lrec-1.854,J17-3005,0,0.350654,"ntence reordering, we 2 The interest of the NLP community towards this is proven by Argument Mining Workshop series at ACL conferences. Readers may refer to Lippi and Torroni (2016) and Lytos et al. (2019) as an overview of this field. 6912 need two kinds of annotation: (1) discourse structure and (2) sentence reordering. This will allow us to analyse and correlate the discourse characteristics of the first drafts and their improved versions. While there are publicly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory pow"
2020.lrec-1.854,E12-2021,0,0.127336,"Missing"
2020.lrec-1.854,J05-2005,0,0.376165,"n. Keywords: annotation tool, discourse annotation, discourse relations, sentence reordering, teaching of argumentation 1. Introduction There are many important aspects in writing such as grammar, mechanics, writing style and coherence (Grosz and Sidner, 1986; Lee and Webster, 2012). Out of these aspects, textual coherence, as an aspect of discourse structure, is extremely important. It concerns how sentences or other discourse units form a flow of meaning (Grosz and Sidner, 1986), and has been extensively analysed in the past (Jacobs et al., 1981; Grosz et al., 1995; Mann and Thompson, 1988; Wolf and Gibson, 2005; Garing, 2014). Our longterm goal is to automatically reorder sentences in argumentative essays so that their coherence is improved, as well as providing an explanation for the changes made. Therefore, the analysis of the order of sentence in original texts (which we call first drafts) and in their improved versions is necessary. A tool is needed to support these annotations, and we believe that it can be designed in such a way that it is useful for language and argument education in general. Existing theory of text coherence stipulate that the order of sentences mirrors the intentional struc"
2020.lrec-1.854,P19-1067,0,0.0174254,"ly available argumentative essay corpora in which some aspect of discourse structure has been annotated (Peldszus and Stede, 2016; Stab and Gurevych, 2017), sentence reordering annotation is the problem: no ready-made corpora with sentence reordering annotation exists, at least not for our target domain of student essays, and no existing annotation tool supports sentence reordering, as we will show in Section 4. There are however some studies that explain how to order sentences to generate coherent texts in NLP applications, (Barzilay et al., 2002; Okazaki et al., 2004; Li and Jurafsky, 2017; Xu et al., 2019). However, they operate on different, non-argumentative texts such as news, and because they are statistical, they lack the explanatory power we need for educational purposes. Although general-purpose annotation tools exist (Kaplan et al., 2010; Stenetorp et al., 2012), the modification of an existing annotation tool is still often not realistic due to many real-life constraints (e.g. the time involved in modification rather than fresh implementation, the availability of documentation and the entire redesign necessary when annotation needs diverge too much). Task-specific tools are often simpl"
2020.lrec-1.854,P13-4001,0,0.0544213,"Missing"
2020.lrec-1.876,J12-3002,0,0.0284288,"repancy. For instance, the maps have different labels for the same landmark, and landmarks appear only in one of the maps. The most significant difference between the maps is that a route is marked only in the giver’s map. The goal of the task is to replicate the giver’s route in the follower’s map only through dialogue. Map Task is an asymmetric task where participants have different roles. In an asymmetric task, a participant with abundant information tells another participant what to do. There have been dialogue corpora collected through asymmetric tasks in the past. The Fruit Cart Corpus (Aist et al., 2012) was collected by a two-person task performed on a PC, where one participant (director) instructs another participant (actor) to place objects on a displayed map and to change their colour. Both participants share the displayed map. The ArtWalk Task (Liu et al., 2016) assigns two par7088 ticipants different roles: a director and a follower, where the follower walks around the real town to find the public art following the director’s instructions via Skype. In these asymmetric tasks, the number of participant’s utterances is often unbalanced due to their different roles. The followers tend to s"
2020.lrec-1.876,L16-1432,0,0.0248965,"ess of gamification in annotation tasks. Vannella et al. (2014) proposed video games for a validation task. They created games for the purpose of word validation in extended WordNet synsets, where they compared the data collected by crowdsourcing with the equivalent collected from the games. They showed that gamebased validation leads to higher quality result at a lower cost. These studies adopted gamification for the validation and annotation of existing data, while we aim at collecting new dialogue data by gamification. There are several attempts at collecting dialogue data by gamification. Asher et al. (2016) implemented a chat system on an online multi-player board game and constructed a multi-party conversation corpus. The chat log was annotated by novice and expert annotators. They labelled dialogue acts and discourse structure in the environment without gamification. Manuvinakurike and DeVault (2015) presented a browser-game that collects spoken dialogue data via crowdsourcing. The goal of this two-player game was to identify a target image among eight pictures displayed on each player’s screen. The aspect of dialogue collection was gamified in these games. However, they cannot use their games"
2020.lrec-1.876,bunt-etal-2012-iso,0,0.0353347,"ct is a representation of the speaker’s intention for each utterance in dialogues. It is considered as primary information for dialogue structure and is common as an annotation label for such structure (Core and Allen, 1997; Stolcke et al., 2000). Compared to other shallow information such as dependency and POS, the dialogue act is more dependent on the annotator’s interpretation. Hence, self-annotation should be more suitable for annotating the correct dialogue acts than the shallow information mentioned above. On our platform, we created a simple label set based on the ISO 24617-2 standard (Bunt et al., 2012), which is shown in Table 1. The ISO standard label set is exhaustive and multidimensional. We simplify it to eight labels for introducing annotation into the game. It is well known that human’s short-term memory has a size of around seven chunks (Miller, 1956). Considering this fact, we reduced the number of dialogue act labels to eight. We tried to reduce the player’s cognitive load at the cost of granularity of dialogue act categories. 7087 Dialogue Act Description QUESTION An utterance to ask or confirm something to the partner. An utterance to make the partner do something, or to show the"
2020.lrec-1.876,L16-1734,0,0.0174862,"mod” mechanism, we can extend the game system to add functions for data collection. (4) Minecraft is one of the most popular video games in the world, with over 112 million active players. Abundant players make the collection of large amount of data easier. (5) Minecraft is a multi-player game. Players from different locations can play in the same virtual world through the Internet. Minecraft has been gathering interest as a platform for various applications including virtual agents (Johnson et al., 2016; Gray et al., 2019), human behaviour analysis (M¨uller et al., 2015), and dialogue tasks (Dumont et al., 2016; Narayan-Chen et al., 2019). These studies showed the usefulness of Minecraft as a flexible platform. Platform Architecture Overview Our platform provides a virtual world in which a specific task is performed by players through dialogue and functions to collect various information along with dialogue logs. Players access to a Minecraft server run by the data collector via their Minecraft client. All players start the game in the lobby world, where they can apply for the task and wait for other players. By default, the lobby world is an empty place with no objects, but the data collector can d"
2020.lrec-1.876,P17-1162,0,0.0567795,"Missing"
2020.lrec-1.876,L16-1504,0,0.020248,"’s route in the follower’s map only through dialogue. Map Task is an asymmetric task where participants have different roles. In an asymmetric task, a participant with abundant information tells another participant what to do. There have been dialogue corpora collected through asymmetric tasks in the past. The Fruit Cart Corpus (Aist et al., 2012) was collected by a two-person task performed on a PC, where one participant (director) instructs another participant (actor) to place objects on a displayed map and to change their colour. Both participants share the displayed map. The ArtWalk Task (Liu et al., 2016) assigns two par7088 ticipants different roles: a director and a follower, where the follower walks around the real town to find the public art following the director’s instructions via Skype. In these asymmetric tasks, the number of participant’s utterances is often unbalanced due to their different roles. The followers tend to speak less, and their utterances tend to be shorter than the givers (Anderson et al., 1991; Tokunaga et al., 2010). In the symmetric task where the participants are not assigned a specific role a priori, i.e. they are equal partners, the imbalance of the utterance numb"
2020.lrec-1.876,P19-1537,0,0.0284502,"n extend the game system to add functions for data collection. (4) Minecraft is one of the most popular video games in the world, with over 112 million active players. Abundant players make the collection of large amount of data easier. (5) Minecraft is a multi-player game. Players from different locations can play in the same virtual world through the Internet. Minecraft has been gathering interest as a platform for various applications including virtual agents (Johnson et al., 2016; Gray et al., 2019), human behaviour analysis (M¨uller et al., 2015), and dialogue tasks (Dumont et al., 2016; Narayan-Chen et al., 2019). These studies showed the usefulness of Minecraft as a flexible platform. Platform Architecture Overview Our platform provides a virtual world in which a specific task is performed by players through dialogue and functions to collect various information along with dialogue logs. Players access to a Minecraft server run by the data collector via their Minecraft client. All players start the game in the lobby world, where they can apply for the task and wait for other players. By default, the lobby world is an empty place with no objects, but the data collector can decorate it as needed by modi"
2020.lrec-1.876,W19-5941,0,0.0217372,"s by using crowdsourcing needs to cope with several issues. The first issue concerns the worker’s motivation. The workers are told to complete the task through dialogue. Their motivation comes primarily from its reward instead of their intrinsic will to complete the task. The lack of intrinsic motivation might make the collected dialogue unnatural or irrelevant. Related to the motivation, eliminating low-quality workers would be a problem of crowdsourcing in general. Some workers try to obtain a reward at the lowest effort regardless of the relevance of their responses (Vannella et al., 2014; Radlinski et al., 2019). Secondly, Even though crowdsourcing is cost-effective, i.e. being able to collect a large amount of data at low cost, the cost rises as the amount of data increases. Lastly, this is particularly problematic in collecting human-human dialogues, arranging worker’s schedule for making dialogue pairs is a crucial problem. In many tasks that are submitted to the crowdsourcing system, each worker works alone; therefore, they can do the task at any time at any place they want. However, when we collect human-human dialogues, a pair of workers must be online at the same time. Manually scheduling a la"
2020.lrec-1.876,D18-1233,0,0.0402894,"Missing"
2020.lrec-1.876,D08-1027,0,0.342868,"Missing"
2020.lrec-1.876,J00-3003,0,0.0673028,"ator’s motivation and might degrade their annotation performance. Nonetheless, as we described in Section 3.1., gamification might remedy this disadvantage. To validate the effectiveness of self-annotation, we added a self-annotation function to our platform. 5.2. Target annotation: Dialogue Act The target of our annotation is the dialogue act of utterances. The dialogue act is a representation of the speaker’s intention for each utterance in dialogues. It is considered as primary information for dialogue structure and is common as an annotation label for such structure (Core and Allen, 1997; Stolcke et al., 2000). Compared to other shallow information such as dependency and POS, the dialogue act is more dependent on the annotator’s interpretation. Hence, self-annotation should be more suitable for annotating the correct dialogue acts than the shallow information mentioned above. On our platform, we created a simple label set based on the ISO 24617-2 standard (Bunt et al., 2012), which is shown in Table 1. The ISO standard label set is exhaustive and multidimensional. We simplify it to eight labels for introducing annotation into the game. It is well known that human’s short-term memory has a size of a"
2020.lrec-1.876,W10-3206,1,0.6227,"tructs another participant (actor) to place objects on a displayed map and to change their colour. Both participants share the displayed map. The ArtWalk Task (Liu et al., 2016) assigns two par7088 ticipants different roles: a director and a follower, where the follower walks around the real town to find the public art following the director’s instructions via Skype. In these asymmetric tasks, the number of participant’s utterances is often unbalanced due to their different roles. The followers tend to speak less, and their utterances tend to be shorter than the givers (Anderson et al., 1991; Tokunaga et al., 2010). In the symmetric task where the participants are not assigned a specific role a priori, i.e. they are equal partners, the imbalance of the utterance numbers between participants is less likely to occur than in the asymmetric task. For instance, He et al. (2017) conducted the Mutual-Friends task, where two participants are given a different list of “friends” and have to find a common term of the list through dialogue. Though the given lists are different, the participants communicate with each other without any information skew and the difference in their role. players need to communicate wit"
2020.lrec-1.876,P14-1122,0,0.367541,"task-oriented dialogues by using crowdsourcing needs to cope with several issues. The first issue concerns the worker’s motivation. The workers are told to complete the task through dialogue. Their motivation comes primarily from its reward instead of their intrinsic will to complete the task. The lack of intrinsic motivation might make the collected dialogue unnatural or irrelevant. Related to the motivation, eliminating low-quality workers would be a problem of crowdsourcing in general. Some workers try to obtain a reward at the lowest effort regardless of the relevance of their responses (Vannella et al., 2014; Radlinski et al., 2019). Secondly, Even though crowdsourcing is cost-effective, i.e. being able to collect a large amount of data at low cost, the cost rises as the amount of data increases. Lastly, this is particularly problematic in collecting human-human dialogues, arranging worker’s schedule for making dialogue pairs is a crucial problem. In many tasks that are submitted to the crowdsourcing system, each worker works alone; therefore, they can do the task at any time at any place they want. However, when we collect human-human dialogues, a pair of workers must be online at the same time."
2021.argmining-1.2,W19-4505,0,0.0182599,"l structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned it on AM tasks. Lauscher et al. (2018) investigated the MTL setup of argumentative component identification and rhetorical classification tasks. Schulz et al. (2018) performed a cross-genre argumenta2 Past studies used the term domain in a broad context. It has at least five different senses: text genre, text quality, annotation scheme, dataset and topic (or prompt). In the rest of this paper, we use the specific meaning whenever possible. 3 https://github.com/wiragotama/ ArgMin2021 13 tive component identification. They employed a seq"
2021.argmining-1.2,2021.findings-acl.84,0,0.0193075,"t five different senses: text genre, text quality, annotation scheme, dataset and topic (or prompt). In the rest of this paper, we use the specific meaning whenever possible. 3 https://github.com/wiragotama/ ArgMin2021 13 tive component identification. They employed a sequence tagger model with a shared representation but different prediction layers for each genre. els on the cross-topic relation labelling task. Data augmentation can also be applied to mitigate the data sparsity problem. This aims to increase the amount of training data without directly collecting more data (Liu et al., 2020; Feng et al., 2021). A relatively straightforward strategy is to use multiple corpora when training models. For example, Chu et al. (2017) proposed a mixed fine-tuning approach for machine translation; they trained a model on an out-genre corpus and then fine-tune it on a dataset that is a mix of the targetgenre and out-genre corpora. However, the use of multiple corpora of different genres is challenging in AM because argumentation is often modelled differently across genres (Lippi and Torroni, 2016; Lawrence and Reed, 2020). Daxenberger et al. (2017) found that training a claim identification model with mixed-"
2021.argmining-1.2,N16-1165,0,0.0291934,"evych, 2014). A further post-processing step can also be performed to combine the local predictions into an optimised global structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned it on AM tasks. Lauscher et al. (2018) investigated the MTL setup of argumentative component identification and rhetorical classification tasks. Schulz et al. (2018) performed a cross-genre argumenta2 Past studies used the term domain in a broad context. It has at least five different senses: text genre, text quality, annotation scheme, dataset and topic (or prompt). In the rest of this paper, we use the specif"
2021.argmining-1.2,W18-2501,0,0.0285866,"Missing"
2021.argmining-1.2,P17-2061,0,0.0263478,"paper, we use the specific meaning whenever possible. 3 https://github.com/wiragotama/ ArgMin2021 13 tive component identification. They employed a sequence tagger model with a shared representation but different prediction layers for each genre. els on the cross-topic relation labelling task. Data augmentation can also be applied to mitigate the data sparsity problem. This aims to increase the amount of training data without directly collecting more data (Liu et al., 2020; Feng et al., 2021). A relatively straightforward strategy is to use multiple corpora when training models. For example, Chu et al. (2017) proposed a mixed fine-tuning approach for machine translation; they trained a model on an out-genre corpus and then fine-tune it on a dataset that is a mix of the targetgenre and out-genre corpora. However, the use of multiple corpora of different genres is challenging in AM because argumentation is often modelled differently across genres (Lippi and Torroni, 2016; Lawrence and Reed, 2020). Daxenberger et al. (2017) found that training a claim identification model with mixed-genre corpora only perform as good as training on each specific corpus. The use of data augmentation may cause the dist"
2021.argmining-1.2,D18-1370,0,0.0158178,"proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned it on AM tasks. Lauscher et al. (2018) investigated the MTL setup of argumentative component identification and rhetorical classification tasks. Schulz et al. (2018) performed a cross-genre argumenta2 Past studies used the term domain in a broad context. It has at least five different senses: text genre, text quality, annotation scheme, dataset and topic (or prompt). In the rest of this paper, we use the specific meaning whenever possible. 3 https://github.com/wiragotama/ ArgMin2021 13 tive component identification. They employed a sequence tagger model with a shared representation but different prediction layers for each genre. e"
2021.argmining-1.2,D17-1218,0,0.0203554,"ining data without directly collecting more data (Liu et al., 2020; Feng et al., 2021). A relatively straightforward strategy is to use multiple corpora when training models. For example, Chu et al. (2017) proposed a mixed fine-tuning approach for machine translation; they trained a model on an out-genre corpus and then fine-tune it on a dataset that is a mix of the targetgenre and out-genre corpora. However, the use of multiple corpora of different genres is challenging in AM because argumentation is often modelled differently across genres (Lippi and Torroni, 2016; Lawrence and Reed, 2020). Daxenberger et al. (2017) found that training a claim identification model with mixed-genre corpora only perform as good as training on each specific corpus. The use of data augmentation may cause the distributional shift problem as well, where the augmented data alter the target distribution that should be learned by the model (Feng et al., 2021). Our target texts are sourced from the ICNALE-AS2R, a corpus of 434 essays written by Asian college students with intermediate proficiency.4 There are 6,021 sentences in total with 13.9 sentences on average per essay. To the best of our knowledge, this is the only currently"
2021.argmining-1.2,2020.acl-main.298,0,0.019308,"ned a BIO tagging scheme and performed end-to-end parsing at token-level, executing all subtasks (i.e., segmentation, unit type classification, linking and relation labelling) at once. Ye and Teufel (2021) also performed endto-end parsing at the token-level. They proposed a more efficient representation for the dependency structure of arguments, and achieved the state-ofthe-art performance for component and relation identifications on the PEC using a biaffine attention model (Dozat and Manning, 2017). The biaffine attention model was originally designed to parse token-to-token dependency, but Morio et al. (2020) extended it to parse proposition (segment) level dependency. Their model dealt with graph-structured arguments in the Cornell eRulemaking corpus (Park and Cardie, 2018). Using the same architecture, Putra et al. (2021b) parsed tree-structured EFL essays in the ICNALE-AS2R corpus (Ishikawa, 2013, 2018; Putra et al., 2021a,b). In tree-structured argumentation, it is common for groups of sentences about the same sub-topic to operate as a unit, forming a sub-tree (sub-argument). Putra et al. (2021b) found that their linking model has problems in constructing sub-trees, that is, it splits a group"
2021.argmining-1.2,P16-1176,0,0.0124613,"t were annotated using different schemes and of different quality, while ensuring that the model still learns the properties of the target in-domain data well. The choice of EFL texts in this study aims to contribute to a less attended area. In AM, it is common to use well-written texts by proficient authors (e.g., Ashley, 1990; Peldszus and Stede, 2016). However, student texts often suffer from many problems because they are still in the learning phase. Even more, EFL texts are also less coherent and less lexically rich, and exhibit less natural lexical choices and collocations (Silva, 1993; Rabinovich et al., 2016). There are more non-native English speakers (Fujiwara, 2018), and yet, to the best of our knowledge, only one preceding study in AM concerned the EFL genre (Putra et al., 2021b). The codes accompanying this paper are publicly available.3 2 2.1 assuming that the segmentation and AC vs non-AC categorisation have been pre-completed. They experimented on the microtext corpus (Peldszus and Stede, 2016) and the persuasive essay corpus (PEC, Stab and Gurevych (2017)). Eger et al. (2017) formulated argumentative structure parsing in three ways: as relation extraction, as sequence tagging and as depen"
2021.argmining-1.2,D19-1410,0,0.024315,"Missing"
2021.argmining-1.2,L18-1257,0,0.0228185,"abelling) at once. Ye and Teufel (2021) also performed endto-end parsing at the token-level. They proposed a more efficient representation for the dependency structure of arguments, and achieved the state-ofthe-art performance for component and relation identifications on the PEC using a biaffine attention model (Dozat and Manning, 2017). The biaffine attention model was originally designed to parse token-to-token dependency, but Morio et al. (2020) extended it to parse proposition (segment) level dependency. Their model dealt with graph-structured arguments in the Cornell eRulemaking corpus (Park and Cardie, 2018). Using the same architecture, Putra et al. (2021b) parsed tree-structured EFL essays in the ICNALE-AS2R corpus (Ishikawa, 2013, 2018; Putra et al., 2021a,b). In tree-structured argumentation, it is common for groups of sentences about the same sub-topic to operate as a unit, forming a sub-tree (sub-argument). Putra et al. (2021b) found that their linking model has problems in constructing sub-trees, that is, it splits a group of sentences that should belong together into separate sub-arguments (sub-trees) or, conversely, groups together sentences that do not belong together into the same sub-"
2021.argmining-1.2,N18-2006,0,0.101605,"uld know this rule. ... (S18) It is to avoid harming other people. Figure 1: Illustration of linking task, using part of an essay discussing the topic “Smoking should be banned at all restaurants in the country."" The linking task has been identified by earlier works as particularly challenging (Lippi and Torroni, 2016; Cabrio and Villata, 2018; Lawrence and Reed, 2020). There are many possible combinations of links between textual units, and a linking model has to find the most proper structure out of a very large search space. Another typical challenge in AM is the size of annotated corpora (Schulz et al., 2018). Corpus construction is a complex and time-consuming process; it also often requires a team of expert annotators. Existing corpora in AM are relatively “small"" compared with more established fields, such as machine translation or document classification. This hinders training AM models when using a supervised machine learning framework. In this paper, we perform the linking task for essays written by English-as-a-foreign-language (EFL) learners. Given an essay, we identify links between sentences, forming a tree-structured representation of argumentation in the text. Figure 1 illustrates the"
2021.argmining-1.2,D15-1110,0,0.0268365,"ther into separate sub-arguments (sub-trees) or, conversely, groups together sentences that do not belong together into the same sub-trees. Related Work Argumentative Structure Prediction A variety of formulations have been proposed for the linking task. Traditional approaches formulated it as a pairwise classification task, predicting whether an argumentative link exists between a given pair of ACs (Stab and Gurevych, 2014). A further post-processing step can also be performed to combine the local predictions into an optimised global structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned"
2021.argmining-1.2,2020.emnlp-main.225,0,0.166197,"pth (ND) prediction. There are six depth categories employed in this paper: depth 0 to depth 4, and depth 5+. The argumentative structure in ICNALE-AS2R corpus is hierarchical, and there is no relations between nodes (sentences) at the same depth. The ND prediction task should help the model to learn the placement of sentences in the hierarchy and guide where each sentence should point at, that is, sentences at depth X point at sentences at depth X − 1. We also propose to use sentence position (spos) embedding as an input feature because it has been proved to be useful in other studies (e.g., Song et al., 2020). The sentence position encoding is G hN(target) BiLSTM Dense Sentence-BERT Encoder s1 s2 ... Multi-Task Learning with Structural Signal We propose to extend the B IAF model in an MTL setup using two novel structural-modelling-related auxiliary tasks. The first auxiliary task is a quasi argumentative component type (QACT) prediction. ICNALE-AS2R corpus does not assign AC types per se, but we can compile the following four sentence types from the tree typology: hN(source) h2(target) (2) The Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) is applied to create a minimum spanning tree"
2021.argmining-1.2,D14-1006,0,0.0318209,"unit, forming a sub-tree (sub-argument). Putra et al. (2021b) found that their linking model has problems in constructing sub-trees, that is, it splits a group of sentences that should belong together into separate sub-arguments (sub-trees) or, conversely, groups together sentences that do not belong together into the same sub-trees. Related Work Argumentative Structure Prediction A variety of formulations have been proposed for the linking task. Traditional approaches formulated it as a pairwise classification task, predicting whether an argumentative link exists between a given pair of ACs (Stab and Gurevych, 2014). A further post-processing step can also be performed to combine the local predictions into an optimised global structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et"
2021.argmining-1.2,D17-1143,0,0.0195564,"ees. Related Work Argumentative Structure Prediction A variety of formulations have been proposed for the linking task. Traditional approaches formulated it as a pairwise classification task, predicting whether an argumentative link exists between a given pair of ACs (Stab and Gurevych, 2014). A further post-processing step can also be performed to combine the local predictions into an optimised global structure, e.g., using the minimum-spanningtree algorithm (Peldszus and Stede, 2015). Recent studies proposed a more global approach instead, considering the entire input context. For instance, Potash et al. (2017) formulated the linking task as a sequence prediction problem. They jointly performed AC classification and AC linking at once, 2.2 Low-Resource and Cross-Domain Argument Mining Several approaches have been applied to alleviate the data sparsity problem in AM. Al-Khatib et al. (2016) used a distant supervision technique to acquire a huge amount of data without explicit annotation. Accuosto and Saggion (2019) pre-trained a discourse parsing model and then fine-tuned it on AM tasks. Lauscher et al. (2018) investigated the MTL setup of argumentative component identification and rhetorical classif"
2021.argmining-1.2,J17-3005,0,0.156408,"gure 1 illustrates the task. Our contributions are twofold. First, we propose two structural-modelling related Introduction Argument mining (AM) is an emerging area that addresses the automatic analysis of argumentation. Many recent studies commonly try to tackle two major tasks (Lawrence and Reed, 2020). The first of these is argumentative component identification, in which argumentative units (ACs) and nonargumentative components (non-ACs) including their boundaries are determined. ACs can be further classified according to their role in argumentation, e.g., major claim,1 claim and premise (Stab and Gurevych, 2017). The second task is called argumentative structure prediction, which first establishes links from source to target ACs (this is called the linking task) and then labels the relationship between them, for instance using the support 1 The major claim is the statement expressing the writer’s view on the discussion topic; also called main stance or main claim. 12 Proceedings of The 8th Workshop on Argument Mining, pages 12–23 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics auxiliary tasks to train our model in a multi-task learning (MTL) fashi"
2021.argmining-1.2,D18-1402,0,0.0382285,"Missing"
2021.argmining-1.2,2021.bea-1.10,1,0.914761,"s in this study aims to contribute to a less attended area. In AM, it is common to use well-written texts by proficient authors (e.g., Ashley, 1990; Peldszus and Stede, 2016). However, student texts often suffer from many problems because they are still in the learning phase. Even more, EFL texts are also less coherent and less lexically rich, and exhibit less natural lexical choices and collocations (Silva, 1993; Rabinovich et al., 2016). There are more non-native English speakers (Fujiwara, 2018), and yet, to the best of our knowledge, only one preceding study in AM concerned the EFL genre (Putra et al., 2021b). The codes accompanying this paper are publicly available.3 2 2.1 assuming that the segmentation and AC vs non-AC categorisation have been pre-completed. They experimented on the microtext corpus (Peldszus and Stede, 2016) and the persuasive essay corpus (PEC, Stab and Gurevych (2017)). Eger et al. (2017) formulated argumentative structure parsing in three ways: as relation extraction, as sequence tagging and as dependency parsing tasks. They defined a BIO tagging scheme and performed end-to-end parsing at token-level, executing all subtasks (i.e., segmentation, unit type classification, li"
2021.bea-1.10,C16-1324,0,0.0555068,"Missing"
2021.bea-1.10,P12-2041,0,0.0311027,"in AM as a downstream application setting. BERT (Devlin et al., 2019) is a popular transformer-based language model (LM), but as it is designed to be fine-tuned, it can be suboptimal in low-resource settings. SBERT tries to alleviate this problem by producing a more universal sentence embeddings, that can be used as they are in many tasks. The idea of training embeddings on the natural language inference (NLI) task goes back to Conneau et al. (2017), and this is the SBERT variant we use here. The NLI task involves recognising textual entailment (TE), and a TE model has been previously used by Cabrio and Villata (2012) for argumentation. We will quantify how the two encoders perform in our task. All resources of this paper are available on github.1 2 episode in response to the given writing prompt. ACs can be further classified according to their communicative roles, e.g., claim and premise. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, directed relations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we id"
2021.bea-1.10,P18-1058,0,0.0299086,"Missing"
2021.bea-1.10,D17-1070,0,0.0176396,"r and have a quality closer to those of proficient authors. The third contribution of this paper is an evaluation of Sentence-BERT (SBERT, Reimers and Gurevych (2019)) in AM as a downstream application setting. BERT (Devlin et al., 2019) is a popular transformer-based language model (LM), but as it is designed to be fine-tuned, it can be suboptimal in low-resource settings. SBERT tries to alleviate this problem by producing a more universal sentence embeddings, that can be used as they are in many tasks. The idea of training embeddings on the natural language inference (NLI) task goes back to Conneau et al. (2017), and this is the SBERT variant we use here. The NLI task involves recognising textual entailment (TE), and a TE model has been previously used by Cabrio and Villata (2012) for argumentation. We will quantify how the two encoders perform in our task. All resources of this paper are available on github.1 2 episode in response to the given writing prompt. ACs can be further classified according to their communicative roles, e.g., claim and premise. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, dire"
2021.bea-1.10,W19-4510,0,0.0148847,"e biaffine model (“B IAF”) by Dozat and Manning (2017), treating the sentence linking task as sentence-level dependency parsing (Figure 4). The first three layers produce contextual sentence representations in the same manner as in the By averaging subword embeddings. 101 5 We conducted a preliminary fine-tuning experiment on sentence linking task, but the performance did not improve. and finally fed into a prediction layer. As the second option (FF LSTM, Figure 5b), we feed rsource and rtarget to an LSTM layer, and the hidden units of LSTM are concatenated before being sent to a dense layer (Deguchi and Yamaguchi, 2019). T H(source) . h1(source) h1(target) . U H(target) = h2(source) G hN(source) hN(target) h2(target) BiLSTM label label Prediction Prediction Dense Encoder s1 s2 ... sN Concat S EQ T G model. These representations are then fed into two different dense layers, in order to encode the corresponding sentence when   it acts as a source h(source) or target h(target) in a relation. Finally, a biaffine transformation is applied to all source and target representations to produce the final output matrix G ∈ RN ×N , in which each row gi represents where the source sentence si should point to (its highe"
2021.bea-1.10,N19-1423,0,0.00864795,"want to investigate how far the existing labelled corpora for well-written texts can also be useful for training parsers for noisy texts. To this end, we train parsers on both in-domain and out-of-domain texts and evaluate them on the in-domain task. For our out-of-domain texts, we use the improved versions of noisy EFL texts. These improvements were produced by an expert annotator and have a quality closer to those of proficient authors. The third contribution of this paper is an evaluation of Sentence-BERT (SBERT, Reimers and Gurevych (2019)) in AM as a downstream application setting. BERT (Devlin et al., 2019) is a popular transformer-based language model (LM), but as it is designed to be fine-tuned, it can be suboptimal in low-resource settings. SBERT tries to alleviate this problem by producing a more universal sentence embeddings, that can be used as they are in many tasks. The idea of training embeddings on the natural language inference (NLI) task goes back to Conneau et al. (2017), and this is the SBERT variant we use here. The NLI task involves recognising textual entailment (TE), and a TE model has been previously used by Cabrio and Villata (2012) for argumentation. We will quantify how the"
2021.bea-1.10,P17-1002,0,0.0153622,"hed from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing vanilla Bidirectional Long-short-term memory (BiLSTM) network (Hochreiter and Schmidhuber, 1997; Huang et al., 2015), as it can be straightforwardly applied to our task. The dependency parsing formulati"
2021.bea-1.10,W18-2501,0,0.0529767,"Missing"
2021.bea-1.10,P16-2089,0,0.0152679,"se. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, directed relations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing vanilla Bidirectional"
2021.bea-1.10,J86-3001,0,0.518065,"Missing"
2021.bea-1.10,J17-1004,0,0.0165336,"he biaffine model is the current state-of-the-art of syntactic dependency parsing (Dozat and Manning, 2017), and it has been adapted to relation detection and labelling tasks in AM by Morio et al. (2020). In a similar way, we also adapt the biaffine model to our argumentative structure. However, we use sentences instead of spans as ADU, trees instead of graphs. Related Work Most work in AM uses well-written texts in the legal (e.g., Ashley, 1990; Yamada et al., 2019) and news (e.g., Al-Khatib et al., 2016) domains, but there are several AM studies that concentrate on noisy texts. For example, Habernal and Gurevych (2017) focused on the ACI task in web-discourse. Morio and Fujita (2018) investigated how to link arguments in discussion threads. In the education domain, Stab and Gurevych (2017) studied the argumentation in persuasive essays. One of the probArgumentative structure analysis consists of two main steps (Lippi and Torroni, 2016). The first step is argumentative component identification (ACI), which segments a text into argumentative discourse units (ADUs); then differentiates them into argumentative (ACs) and non-argumentative components (non-ACs). ACs function argumentatively while non-ACs do not, e"
2021.bea-1.10,P82-1020,0,0.751772,"Missing"
2021.bea-1.10,iida-tokunaga-2014-building,1,0.802739,"ot an explicitly annotated category. As the last step, annotators rearrange sentences and performed text repair to improve the texts from a discourse perspective. There are four relations between ACs: SUPPORT (sup), ATTACK (att), DETAIL (det) and RESTATE - To improve the texts, annotators were asked to rearrange sentences so that it results in the most logically well-structured texts they can think of. This is the second annotation layer in our corpus. No particular reordering strategy was instructed. Reordering, however, may cause irrelevant or incorrect referring and connective expressions (Iida and Tokunaga, 2014). To correct these expressions, annotators were instructed to minimally repair the text where this is necessary to retain the original meaning of the sentence. For instance, they replaced pronouns with their referents, and removed or replaced inappropriate connectives. Text repair is also necessary to achieve standalone major claims. For example, “I think so” with so referring to the writing prompt (underlined in what follows) can be rephrased as “I think smoking should be banned at all restaurants.” Figure 1 shows an example of our annotation scheme using a real EFL essay. Figure 2 then illus"
2021.bea-1.10,D10-1023,0,0.0225072,"ommunicative roles, e.g., claim and premise. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, directed relations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which ada"
2021.bea-1.10,W15-0501,0,0.0255707,"ntroduces a topic of the discussion in a neutral way by providing general background. From the organisational perspective, the differentiation between DETAIL and SUPPORT is useful. While the source sentence in a SUPPORT relation ideally follows its target, the DETAIL relation has more flexibility. We also use a relation called RE STATEMENT for those situations where high-level parts of an argument are repeated or summarised for the second time, e.g., when the major claim is restated in the conclusion of the essay. D ETAIL and RESTATEMENT links are not common in AM; the first was introduced by Kirschner et al. (2015) and the second by Skeppstedt et al. (2018), but both work on well-written texts. The combination of these four relations is unique in AM. Dataset We use part of the “International Corpus Network of Asian Learners of English” (Ishikawa, 2013, 2018), which we annotated with Argumentative Structure and Sentence Reordering (“ICNALE-AS2R” corpus).2 This corpus contains 434 essays written by college students in various Asian countries. They are written in response to two prompts: (1) about banning smoking and (2) about students’ part-time jobs. Essays are scored in the range of [0, 100]. There are"
2021.bea-1.10,P16-1105,0,0.0274268,"he features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing vanilla Bidirectional Long-short-term memory (BiLSTM) network (Hochreiter and Schmidhuber, 1997; Huang et al., 2015), as it can be straightforwardly applied to our task. The dependency parsing formulation is also a straightforward adaptation as it models tree structures. The biaffine model is the current state-of-the-art of syntactic dependency parsing (Dozat and Manning, 2017), and it has been adapted to relation detection and labelling tasks in AM by Morio et al. (2020). In a similar way, we also adapt the biaffin"
2021.bea-1.10,W18-5202,0,0.015782,"cy parsing (Dozat and Manning, 2017), and it has been adapted to relation detection and labelling tasks in AM by Morio et al. (2020). In a similar way, we also adapt the biaffine model to our argumentative structure. However, we use sentences instead of spans as ADU, trees instead of graphs. Related Work Most work in AM uses well-written texts in the legal (e.g., Ashley, 1990; Yamada et al., 2019) and news (e.g., Al-Khatib et al., 2016) domains, but there are several AM studies that concentrate on noisy texts. For example, Habernal and Gurevych (2017) focused on the ACI task in web-discourse. Morio and Fujita (2018) investigated how to link arguments in discussion threads. In the education domain, Stab and Gurevych (2017) studied the argumentation in persuasive essays. One of the probArgumentative structure analysis consists of two main steps (Lippi and Torroni, 2016). The first step is argumentative component identification (ACI), which segments a text into argumentative discourse units (ADUs); then differentiates them into argumentative (ACs) and non-argumentative components (non-ACs). ACs function argumentatively while non-ACs do not, e.g., describing a personal 1 https://github.com/wiragotama/BEA2021"
2021.bea-1.10,2020.lrec-1.854,1,0.82827,"Missing"
2021.bea-1.10,P16-1176,0,0.289279,"ul to train argument mining system for noisy texts. 1 Introduction Real-world texts are not always well-written, especially in the education area where students are still learning how to write effectively. It has been observed that student texts often require improvements at the discourse-level, e.g., in persuasiveness and content organisation aspects (Bamberg, 1983; Zhang and Litman, 2015; Carlile et al., 2018). Worse still, texts written by non-native speakers are also less coherent, exhibit less lexical richness and more unnatural lexical choices and collocations (Johns, 1986; Silva, 1993; Rabinovich et al., 2016). Our long-term goal is to improve EFL essays from the discourse perspective. One way to do this is by recommending a better arrangement of sentences, 97 Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 97–109 April 20, 2021 ©2021 Association for Computational Linguistics but also perform a structural analysis, giving more insights into the models’ ability to learn different aspects of the argumentative structure. The second contribution is showing the effectiveness of well-written texts as training data for argumentative parsing of noisy t"
2021.bea-1.10,D19-1410,0,0.028206,"o separate domains, and AM systems were trained separately on each domain. We want to investigate how far the existing labelled corpora for well-written texts can also be useful for training parsers for noisy texts. To this end, we train parsers on both in-domain and out-of-domain texts and evaluate them on the in-domain task. For our out-of-domain texts, we use the improved versions of noisy EFL texts. These improvements were produced by an expert annotator and have a quality closer to those of proficient authors. The third contribution of this paper is an evaluation of Sentence-BERT (SBERT, Reimers and Gurevych (2019)) in AM as a downstream application setting. BERT (Devlin et al., 2019) is a popular transformer-based language model (LM), but as it is designed to be fine-tuned, it can be suboptimal in low-resource settings. SBERT tries to alleviate this problem by producing a more universal sentence embeddings, that can be used as they are in many tasks. The idea of training embeddings on the natural language inference (NLI) task goes back to Conneau et al. (2017), and this is the SBERT variant we use here. The NLI task involves recognising textual entailment (TE), and a TE model has been previously used b"
2021.bea-1.10,W18-5218,0,0.0183935,"eutral way by providing general background. From the organisational perspective, the differentiation between DETAIL and SUPPORT is useful. While the source sentence in a SUPPORT relation ideally follows its target, the DETAIL relation has more flexibility. We also use a relation called RE STATEMENT for those situations where high-level parts of an argument are repeated or summarised for the second time, e.g., when the major claim is restated in the conclusion of the essay. D ETAIL and RESTATEMENT links are not common in AM; the first was introduced by Kirschner et al. (2015) and the second by Skeppstedt et al. (2018), but both work on well-written texts. The combination of these four relations is unique in AM. Dataset We use part of the “International Corpus Network of Asian Learners of English” (Ishikawa, 2013, 2018), which we annotated with Argumentative Structure and Sentence Reordering (“ICNALE-AS2R” corpus).2 This corpus contains 434 essays written by college students in various Asian countries. They are written in response to two prompts: (1) about banning smoking and (2) about students’ part-time jobs. Essays are scored in the range of [0, 100]. There are two novelties in this corpus: (1) it uses a"
2021.bea-1.10,W14-2110,0,0.0317128,"g., claim and premise. The second step is argumentative structure prediction, which contains two subtasks: (1) linking and (2) relation labelling. In the linking task, directed relations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing va"
2021.bea-1.10,C14-1142,0,0.0203034,"non-ACs do not, e.g., describing a personal 1 https://github.com/wiragotama/BEA2021 98 lems with the existing corpora is the unclear distinction between native and non-native speakers. Additionally, to investigate and bridge the gap of performance between AM systems on noisy and well-written texts, it is necessary to use a parallel corpus containing both versions of texts. However, none of the above studies did. 3 MENT (res). S UPPORT and ATTACK relations are common in AM. They are used when the source sentence supports or attacks the argument in the target sentence (Peldszus and Stede, 2013; Stab and Gurevych, 2014). We use the DETAIL relation in two cases. First, when the source presents additional details (further explanations, descriptions or elaborations) about the target sentence, and second, when the source introduces a topic of the discussion in a neutral way by providing general background. From the organisational perspective, the differentiation between DETAIL and SUPPORT is useful. While the source sentence in a SUPPORT relation ideally follows its target, the DETAIL relation has more flexibility. We also use a relation called RE STATEMENT for those situations where high-level parts of an argum"
2021.bea-1.10,J17-3005,0,0.0481687,"AM by Morio et al. (2020). In a similar way, we also adapt the biaffine model to our argumentative structure. However, we use sentences instead of spans as ADU, trees instead of graphs. Related Work Most work in AM uses well-written texts in the legal (e.g., Ashley, 1990; Yamada et al., 2019) and news (e.g., Al-Khatib et al., 2016) domains, but there are several AM studies that concentrate on noisy texts. For example, Habernal and Gurevych (2017) focused on the ACI task in web-discourse. Morio and Fujita (2018) investigated how to link arguments in discussion threads. In the education domain, Stab and Gurevych (2017) studied the argumentation in persuasive essays. One of the probArgumentative structure analysis consists of two main steps (Lippi and Torroni, 2016). The first step is argumentative component identification (ACI), which segments a text into argumentative discourse units (ADUs); then differentiates them into argumentative (ACs) and non-argumentative components (non-ACs). ACs function argumentatively while non-ACs do not, e.g., describing a personal 1 https://github.com/wiragotama/BEA2021 98 lems with the existing corpora is the unclear distinction between native and non-native speakers. Additi"
2021.bea-1.10,2020.coling-main.535,0,0.0126975,"ations are established from source to target ACs to form a structured representation of the text, often in the form of a tree. In the relation labelling task, we identify the relations that connect them, e.g., support and attack. In the education domain, argumentative structure interrelates with text quality, and it becomes one of the features that go into automatic essay scoring (AES) systems (Persing et al., 2010; Song et al., 2014; Ghosh et al., 2016; Wachsmuth et al., 2016). End-to-end AES systems also exist, but hybrid models are preferred for both performance and explainability reasons (Uto et al., 2020). Eger et al. (2017) formulated AM in three ways: as relation extraction, as sequence tagging and as dependency parsing. They performed end-toend AM at token-level, executing all subtasks in AM all at once. Eger et al. achieved the highest performance in their experiments with the relation extraction model LSTM-ER (Miwa and Bansal, 2016). We instead use their sequence tagging formulation, which adapts the existing vanilla Bidirectional Long-short-term memory (BiLSTM) network (Hochreiter and Schmidhuber, 1997; Huang et al., 2015), as it can be straightforwardly applied to our task. The dependen"
2021.bea-1.10,C16-1158,0,0.0983403,"sz and Sidner, 1986; Hovy, 1991; Webber and Joshi, 2012). This paper describes the application of Argument Mining (AM) to EFL essays. AM is an emerging area in computational linguistics which aims to explain how argumentative discourse units (e.g., sentences, clauses) function and relate to each other in the discourse, forming an argument as a whole (Lippi and Torroni, 2016). AM has broad applications in various areas, such as in the legal (Ashley, 1990) and news (Al-Khatib et al., 2016) domains. Also in the education domain, AM is beneficial for many downstream tasks such as text assessment (Wachsmuth et al., 2016), text improvement (as described above) and teaching (Putra et al., 2020). It is common in AM to use well-written texts written by proficient authors, as do Peldszus and Stede (2016), Al-Khatib et al. (2016), among others. However, there are more non-native speakers of English than native speakers in the world, and their writings are often noisy as previously described. Yet, EFL is a niche domain in AM. This paper presents three contributions. First, this paper presents an application of AM to EFL essays. We parse the argumentative structure in two steps: (i) a sentence linking step where we i"
2021.bea-1.10,W12-3205,0,0.0606183,"Missing"
2021.bea-1.10,W15-0616,0,0.0584445,"Missing"
2021.wat-1.2,2020.acl-main.34,0,0.0642759,"Missing"
2021.wat-1.2,P11-2093,0,0.0112185,"ring the training phase, constraints are extracted from training data by the proposed methods. During the translation phase, constraints are given by WAT 2021 organizers. lect constraints, we found that the vocabulary list in dev data includes many technical terms and proper nouns. Supposing that the important words such as technical terms and proper nouns tend to be selected as constraints, we extract proper nouns on the basis of the named-entity recognition. ASPEC-JE, with high similarity scores for training the models. We did not use any other resources. 4.2 System Setup We used the KyTea (Neubig et al., 2011) to tokenize Japanese sentences and the Moses toolkit4 to clean and tokenize English sentences. We then used a vocabulary of 48K tokens on the basis of joint byte-pair encoding (BPE) (Sennrich et al., 2016) for the source and target. We used the encoder and decoder of the transformer model (Vaswani et al., 2017), which is a stateof-the-art NMT model. The encoder converts a source sentence into a sequence of continuous representations, and the decoder generates a target sentence. We implemented this system with the Sockeye 2 toolkit (Hieber et al., 2020). All models were trained within at most"
2021.wat-1.2,C18-2019,0,0.0117504,"nstrained words per sentence (Left:Japanese / Right:English) are shown for the dev, devtest, and test data. There are no vocabulary lists for the training data. and consistency scores by WAT 2021 organizers as below: 1. Check whether the translation satisfies the given constraints or not. 2. If the translation does not satisfy the constraint, replace the translation with an empty string. 3. Calculate BLEU with modified translations. Restricted Translation Task Description Furthermore, bilingual human annotators evaluate the top-ranked submitted systems based on source-based direct assessment (Federmann, 2018; Cettolo et al., 2017) and source-based contrastive assessment (Federmann, 2018; Sakaguchi and Van Durme, 2018). 2.1 Official Dataset The main dataset of the restricted translation task is the Japanese-English paper abstract corpus (ASPEC-JE) and the target vocabulary list as constraints. In addition to the main dataset, participants can use any other resources by mentioning their details. The ASPEC-JE dataset consists of training, dev, devtest, and test data. The training data contains 3.0 million bilingual pairs provided with similarity scores automatically calculated by DP matching (Utiyam"
2021.wat-1.2,P02-1040,0,0.111524,"ur lexical-constraint method. 1 Target-vocabulary list: {magnetic features, resonance frequency, feedback circuit, resolution} Requirement for output Output sentence is required to contain all the target words in each target-vocabulary list. Reference This is a feedback circuit shifting resonance frequency by change of input signal phase, which can detect change of magnetic features of an object present at a center of two coils on high sensitivity and resolution. Figure 1: Overview of the restricted translation task (Japanese→English). translation accuracy via bilingual evaluation understudy (Papineni et al., 2002) (BLEU score) and the consistency score of the ratio of sentences satisfying an exact match of given constraints (consistency score). The final ranking is determined by the combined score of both: calculating BLEU with only the exact match sentences2 . In related work (Chen et al., 2020a; Song et al., 2019; Wang et al., 2019; Post and Vilar, 2018; Hokamp and Liu, 2017), since it does not require higher computational complexity than the other methods using the grid beam search (GBS) decoding algorithm (Hokamp and Liu, 2017; Post and Vilar, 2018), we use the lexicalconstraint method of Chen et a"
2021.wat-1.2,N18-1119,0,0.0187053,"detect change of magnetic features of an object present at a center of two coils on high sensitivity and resolution. Figure 1: Overview of the restricted translation task (Japanese→English). translation accuracy via bilingual evaluation understudy (Papineni et al., 2002) (BLEU score) and the consistency score of the ratio of sentences satisfying an exact match of given constraints (consistency score). The final ranking is determined by the combined score of both: calculating BLEU with only the exact match sentences2 . In related work (Chen et al., 2020a; Song et al., 2019; Wang et al., 2019; Post and Vilar, 2018; Hokamp and Liu, 2017), since it does not require higher computational complexity than the other methods using the grid beam search (GBS) decoding algorithm (Hokamp and Liu, 2017; Post and Vilar, 2018), we use the lexicalconstraint method of Chen et al. (2020a). This method concatenates a source sentence and constrained words with a special token to input them into an encoder of the neural machine translation Introduction Our team (NHK) participated in the restricted machine translation task1 using the Japanese-English dataset of the Asian scientific paper excerpt corpus (ASPEC-JE) (Nakazawa"
2021.wat-1.2,P17-1141,0,0.0230517,"etic features of an object present at a center of two coils on high sensitivity and resolution. Figure 1: Overview of the restricted translation task (Japanese→English). translation accuracy via bilingual evaluation understudy (Papineni et al., 2002) (BLEU score) and the consistency score of the ratio of sentences satisfying an exact match of given constraints (consistency score). The final ranking is determined by the combined score of both: calculating BLEU with only the exact match sentences2 . In related work (Chen et al., 2020a; Song et al., 2019; Wang et al., 2019; Post and Vilar, 2018; Hokamp and Liu, 2017), since it does not require higher computational complexity than the other methods using the grid beam search (GBS) decoding algorithm (Hokamp and Liu, 2017; Post and Vilar, 2018), we use the lexicalconstraint method of Chen et al. (2020a). This method concatenates a source sentence and constrained words with a special token to input them into an encoder of the neural machine translation Introduction Our team (NHK) participated in the restricted machine translation task1 using the Japanese-English dataset of the Asian scientific paper excerpt corpus (ASPEC-JE) (Nakazawa et al., 2016) at WAT 20"
2021.wat-1.2,P18-1020,0,0.0222123,"Missing"
2021.wat-1.2,D10-1092,0,0.108062,"Missing"
2021.wat-1.2,P16-1162,0,0.0586985,"he vocabulary list in dev data includes many technical terms and proper nouns. Supposing that the important words such as technical terms and proper nouns tend to be selected as constraints, we extract proper nouns on the basis of the named-entity recognition. ASPEC-JE, with high similarity scores for training the models. We did not use any other resources. 4.2 System Setup We used the KyTea (Neubig et al., 2011) to tokenize Japanese sentences and the Moses toolkit4 to clean and tokenize English sentences. We then used a vocabulary of 48K tokens on the basis of joint byte-pair encoding (BPE) (Sennrich et al., 2016) for the source and target. We used the encoder and decoder of the transformer model (Vaswani et al., 2017), which is a stateof-the-art NMT model. The encoder converts a source sentence into a sequence of continuous representations, and the decoder generates a target sentence. We implemented this system with the Sockeye 2 toolkit (Hieber et al., 2020). All models were trained within at most three days on four Nvidia V100 Tesla GPUs with 16-GB memory in parallel. In training the model, we applied stochastic gradient descent with Adam (Kingma and Ba, 2015) as the optimizer, using a learning rate"
2021.wat-1.2,N19-1044,0,0.0188473,"hange of input signal phase, which can detect change of magnetic features of an object present at a center of two coils on high sensitivity and resolution. Figure 1: Overview of the restricted translation task (Japanese→English). translation accuracy via bilingual evaluation understudy (Papineni et al., 2002) (BLEU score) and the consistency score of the ratio of sentences satisfying an exact match of given constraints (consistency score). The final ranking is determined by the combined score of both: calculating BLEU with only the exact match sentences2 . In related work (Chen et al., 2020a; Song et al., 2019; Wang et al., 2019; Post and Vilar, 2018; Hokamp and Liu, 2017), since it does not require higher computational complexity than the other methods using the grid beam search (GBS) decoding algorithm (Hokamp and Liu, 2017; Post and Vilar, 2018), we use the lexicalconstraint method of Chen et al. (2020a). This method concatenates a source sentence and constrained words with a special token to input them into an encoder of the neural machine translation Introduction Our team (NHK) participated in the restricted machine translation task1 using the Japanese-English dataset of the Asian scientific p"
2021.wat-1.2,L16-1350,0,0.0533879,"Missing"
2021.wat-1.2,2007.mtsummit-papers.63,0,0.328877,"Missing"
2021.wat-1.2,W14-7002,0,0.0203038,"nstraints. Both constraints are made by concatenating the proper-noun and mistranslated-word constraints and removing duplicates. 4 Experiments 4.1 Data In this paper, we used only the first 2.0 million bilingual pairs3 in the official dataset, i.e., mance degraded when using all data in this work. 4 https://github.com/moses-smt/mosesdecoder 5 Sockeye 2 uses a transformer model with 6 encoder and decoder layers, 8 parallel attention heads, model dimensionality of 512, and a feed-forward layer size of 2048 as default. 3 The remaining 1.0 million bilingual pairs were often noisy as described in Neubig (2014). We found the perfor48 Task Japanese→English English→Japanese Method Baseline Random-word Proper-noun Mistranslated-word Prop. & Mistrans. Baseline Random-word Proper-noun Mistranslated-word Prop. & Mistrans. Average of constrained words N/A 4.99 1.25 4.68 5.63 N/A 4.89 1.91 2.74 4.48 Consistency rate (word) 52.3 78.6 78.8 86.1 96.0 61.7 77.8 96.2 85.7 97.4 BLEU 29.3 29.5 36.3 39.2 43.9 45.9 37.4 48.2 48.3 53.2 Table 2: Experimental results for each task. Baseline is trained without any constraint, Random-word is trained with the randomly extracted constraint, Proper-noun is trained with the"
2021.wat-1.2,D19-1085,0,0.0210515,"al phase, which can detect change of magnetic features of an object present at a center of two coils on high sensitivity and resolution. Figure 1: Overview of the restricted translation task (Japanese→English). translation accuracy via bilingual evaluation understudy (Papineni et al., 2002) (BLEU score) and the consistency score of the ratio of sentences satisfying an exact match of given constraints (consistency score). The final ranking is determined by the combined score of both: calculating BLEU with only the exact match sentences2 . In related work (Chen et al., 2020a; Song et al., 2019; Wang et al., 2019; Post and Vilar, 2018; Hokamp and Liu, 2017), since it does not require higher computational complexity than the other methods using the grid beam search (GBS) decoding algorithm (Hokamp and Liu, 2017; Post and Vilar, 2018), we use the lexicalconstraint method of Chen et al. (2020a). This method concatenates a source sentence and constrained words with a special token to input them into an encoder of the neural machine translation Introduction Our team (NHK) participated in the restricted machine translation task1 using the Japanese-English dataset of the Asian scientific paper excerpt corpus"
A94-1027,A92-1018,0,\N,Missing
baldwin-etal-2002-enhanced,J97-4001,0,\N,Missing
baldwin-etal-2002-enhanced,W99-0902,1,\N,Missing
C02-1059,P92-1008,0,0.126265,"ny ways of dealing with selfcorrection have been proposed, these have limitations in both detecting and correcting for this phenomenon. In this paper, we propose a method to overcome these problems in Japanese speech dialog. We evaluate the proposed method using our speech dialog corpus and discuss its limitations and the work that remains to be done. 1 Introduction Self-correction, or speech repair, is a major source of the dis uencies that speech dialog systems have to resolve. Since Hindle (1983), there have been many proposals as to how speech dialog systems can deal with self-correction (Bear et al., 1992; Nakatani and Hirschberg, 1993; Den, 1997; Nakano and Shimazu, 1998; Core and Schubert, 1999). Through self-monitoring, human speakers can instantly correct their mistakes during an utterance (Levelt, 1989). Therefore, we can detect selfcorrection with local models such as the Repair Interval Model (RIM) proposed by Nakatani and Hirschberg (1993). Most work has used the same model or models similar to the RIM. The RIM divides the self-correction into three intervals, reparandum (RPD), dis uency (DF), and repair (RP), and assumes that these three intervals appear in the order of  . . . RPD DF"
C02-1059,P99-1053,0,0.317329,"th detecting and correcting for this phenomenon. In this paper, we propose a method to overcome these problems in Japanese speech dialog. We evaluate the proposed method using our speech dialog corpus and discuss its limitations and the work that remains to be done. 1 Introduction Self-correction, or speech repair, is a major source of the dis uencies that speech dialog systems have to resolve. Since Hindle (1983), there have been many proposals as to how speech dialog systems can deal with self-correction (Bear et al., 1992; Nakatani and Hirschberg, 1993; Den, 1997; Nakano and Shimazu, 1998; Core and Schubert, 1999). Through self-monitoring, human speakers can instantly correct their mistakes during an utterance (Levelt, 1989). Therefore, we can detect selfcorrection with local models such as the Repair Interval Model (RIM) proposed by Nakatani and Hirschberg (1993). Most work has used the same model or models similar to the RIM. The RIM divides the self-correction into three intervals, reparandum (RPD), dis uency (DF), and repair (RP), and assumes that these three intervals appear in the order of  . . . RPD DF RP . . . ."" For example, [I want]RPD [uh]DF [I want]RP a window seat."" However, human speake"
C02-1059,P83-1019,0,0.275278,"human-human dialog. Self-correction (or speech-repair) is a particularly problematic phenomenon. Although many ways of dealing with selfcorrection have been proposed, these have limitations in both detecting and correcting for this phenomenon. In this paper, we propose a method to overcome these problems in Japanese speech dialog. We evaluate the proposed method using our speech dialog corpus and discuss its limitations and the work that remains to be done. 1 Introduction Self-correction, or speech repair, is a major source of the dis uencies that speech dialog systems have to resolve. Since Hindle (1983), there have been many proposals as to how speech dialog systems can deal with self-correction (Bear et al., 1992; Nakatani and Hirschberg, 1993; Den, 1997; Nakano and Shimazu, 1998; Core and Schubert, 1999). Through self-monitoring, human speakers can instantly correct their mistakes during an utterance (Levelt, 1989). Therefore, we can detect selfcorrection with local models such as the Repair Interval Model (RIM) proposed by Nakatani and Hirschberg (1993). Most work has used the same model or models similar to the RIM. The RIM divides the self-correction into three intervals, reparandum (RP"
C04-1096,E91-1028,0,0.0239254,"tify groups of objects that are naturally recognized by humans. We conducted psychological experiments with 42 subjects to collect referring expressions in such situations, and built a generation algorithm based on the results. The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about concrete objects in the world. For that purpose, most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek, 1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) makes use of attributes of an intended object (the target) and binary relations between the target and others (distractors) to distinguish the target from distractors. Therefore, these methods cannot generate proper referring expressions in situations where no significant surface difference exists between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression means a concise and natura"
C04-1096,J95-3003,0,0.163447,"ans. We conducted psychological experiments with 42 subjects to collect referring expressions in such situations, and built a generation algorithm based on the results. The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about concrete objects in the world. For that purpose, most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek, 1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) makes use of attributes of an intended object (the target) and binary relations between the target and others (distractors) to distinguish the target from distractors. Therefore, these methods cannot generate proper referring expressions in situations where no significant surface difference exists between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression means a concise and natural linguistic expression enabling hearers to distinguish the"
C04-1096,P97-1027,0,0.137841,"logical experiments with 42 subjects to collect referring expressions in such situations, and built a generation algorithm based on the results. The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about concrete objects in the world. For that purpose, most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek, 1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) makes use of attributes of an intended object (the target) and binary relations between the target and others (distractors) to distinguish the target from distractors. Therefore, these methods cannot generate proper referring expressions in situations where no significant surface difference exists between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression means a concise and natural linguistic expression enabling hearers to distinguish the target from di"
C04-1096,J03-1003,0,0.174617,"Missing"
C04-1096,J02-1003,0,0.095201,"Missing"
C08-2029,2007.mtsummit-ucnlg.14,0,0.0545314,"advantage of common resources (e.g. TUNA-corpus). A critical question is how to combine the generic human cognitive tendencies and the dependency of attribute selection on a specific distribution of attributes in a specific case. In this research we tackle this question in a corpus-based approach. Specifically, in a given environment, we seek to develop an efficient algorithm for selection of attributes that approximates human selection. 2 The corpus We utilized a simplified version of the TUNAcorpus, which was also the basis for the GREchallenge held as part of the UCNLG+MT workshop in 2007 (Belz and Gatt, 2007). The corpus consists of a collection of paired pictures of objects and human-produced referring expressions annotated with attribute sets. Figure 1 shows an image 115 1 TUNA-corpus: www.csd.abdn.ac.uk/research/tuna Coling 2008: Companion volume – Posters and Demonstrations, pages 115–118 Manchester, August 2008 red blue nitive factors as well as case-dependent factors have to be dealt with. In order to account for the cognitive factor, we define a “selection probability” over a whole domain (i.e. independent from a specific case) and calculate the differences of this selection probability ove"
C08-2029,W06-1410,0,0.0324987,"that in those cases where our algorithm outperformed the IA, our algorithm almost exclusively added solely the type-attribute. In contrast in more complex cases of redundancy in referring expressions, the IA has shown to be superior. Since we achieved overall parity to the IA even though generally performing worse than the IA in cases of more complex redundancy, we can conclude that outside of this phenomenon our algorithm performs better than the IA in terms of human-likeness. In previous research there has been some discussion on “redundancy” vs. “minimality” in referring expressions (e.g. (Viethen and Dale, 2006)). Through our research we have identified the phenomenon of redundancy as a critical topic for further research and for achieving further progress in the generation of human-like referring expressions. Our algorithm includes some strong simplifications, e.g. our treatment of attributes did not take account of the fact that attribute-values are also of different type and did not explore what implications this has for the process of producing referring expressions; binary (hasHair), discrete (hairColour) or graded (x-dim). In future these factors should be integrated into attribute selection al"
C12-2048,P05-1018,0,0.023057,"discourse entity appearing in the current utterance and was realised as most salient in the previous utterance. On the other hand, Miltsakaki and Kukich (2000) focused on investigating the relationship of the coherence of a text and the transition of centers and revealed that the rough-shift transition of centers correlates to incoherence of a text. In these studies, one of the most important work was to represent the relationship of discourse entities and their occurrences in a text based on the transition of discourse entities, which was done in a series of studies (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008). In Barzilay and Lapata (2008), the transition of discourse entities in adjacent discourse units (e.g. sentences) is formalised as an entity grid, which is a matrix of discourse entities and their realised grammatical roles, because a grammatical role of a discourse entity is a good indicator of its salience. For example, a given input text shown in Figure 1, consisting of the three sentences, each discourse entity is represented in the entity grid shown in Table 1. In the entity grid, each column is filled with the corresponding label (e"
C12-2048,J08-1001,0,0.140604,"u Tokunag a1 (1) Tokyo Institute of Technology, W8-73, 2-12-1 Ohokayama Meguro Tokyo, 152-8552 Japan {ryu-i,take} l. s.tite h.a .jp ABSTRACT We propose a simple and effective metric for automatically evaluating discourse coherence of a text using the outputs of a coreference resolution model. According to the idea that a writer tends to appropriately utilise coreference relations when writing a coherent text, we introduce a metric of discourse coherence based on automatically identified coreference relations. We empirically evaluated our metric by comparing it to the entity grid modelling by Barzilay and Lapata (2008) using Japanese newspaper articles as a target data set. The results indicate that our metric better reflects discourse coherence of texts than the existing model. KEYWORDS: discourse coherence, coreference resolution, evaluation metric. Proceedings of COLING 2012: Posters, pages 483–494, COLING 2012, Mumbai, December 2012. 483 1 Introduction The task of automatically evaluating discourse coherence has recently received much attention (Karamanis et al., 2004; Barzilay and Lapata, 2008; Lin et al., 2011, etc.) because it is essential for several NLP applications such as generation (Soricut and"
C12-2048,N04-1015,0,0.037678,"ters, each of which is a discourse entity appearing in the current utterance and was realised as most salient in the previous utterance. On the other hand, Miltsakaki and Kukich (2000) focused on investigating the relationship of the coherence of a text and the transition of centers and revealed that the rough-shift transition of centers correlates to incoherence of a text. In these studies, one of the most important work was to represent the relationship of discourse entities and their occurrences in a text based on the transition of discourse entities, which was done in a series of studies (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008). In Barzilay and Lapata (2008), the transition of discourse entities in adjacent discourse units (e.g. sentences) is formalised as an entity grid, which is a matrix of discourse entities and their realised grammatical roles, because a grammatical role of a discourse entity is a good indicator of its salience. For example, a given input text shown in Figure 1, consisting of the three sentences, each discourse entity is represented in the entity grid shown in Table 1. In the entity grid, each column is filled with"
C12-2048,P06-1049,0,0.0568112,"Missing"
C12-2048,C10-1017,0,0.0155792,"mploying our metric as a feature. 4 Coreference resolution model for a coherence metric The proposed metric introduced in Section 3 is designed for the use of any anaphora (or coreference) resolution model. In this work, we employ an NP coreference resolution model. According to formula (1) in Section 3, calculating our metric needs a reliability score of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines"
C12-2048,W99-0611,0,0.0669354,"of the entity grid model employing our metric as a feature. 4 Coreference resolution model for a coherence metric The proposed metric introduced in Section 3 is designed for the use of any anaphora (or coreference) resolution model. In this work, we employ an NP coreference resolution model. According to formula (1) in Section 3, calculating our metric needs a reliability score of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than si"
C12-2048,N09-1042,0,0.0533691,"Missing"
C12-2048,N07-1030,0,0.018005,"tion). In Section 5.3 we will also demonstrate the results of the entity grid model employing our metric as a feature. 4 Coreference resolution model for a coherence metric The proposed metric introduced in Section 3 is designed for the use of any anaphora (or coreference) resolution model. In this work, we employ an NP coreference resolution model. According to formula (1) in Section 3, calculating our metric needs a reliability score of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Jap"
C12-2048,J95-2003,0,0.939254,"task of automatically evaluating discourse coherence has recently received much attention (Karamanis et al., 2004; Barzilay and Lapata, 2008; Lin et al., 2011, etc.) because it is essential for several NLP applications such as generation (Soricut and Marcu, 2006), summarisation (Lapata, 2003; Okazaki et al., 2004; Bollegala et al., 2006) and automated essay scoring (Miltsakaki and Kukich, 2000; Higgins et al., 2004). Researchers in these areas have mainly been concerned with introducing the linguistic notions of cohesion or coherence addressed in discourse theories, such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Mann and Thompson, 1988), into computational models for each task, ranging from heuristic rule-based to sophisticated machine learning-based approaches. Some of this research has relied on the occurrence of discourse entities (e.g. NPs and pronouns) to capture cohesion of a text for indirectly estimating discourse coherence. Barzilay and Lapata (2008)’s approach, for instance, models the transition of discourse entities appearing in adjacent sentences for capturing local discourse coherence, which is derived from the notion of Centering Theory. In their approa"
C12-2048,N04-1024,0,0.234087,"existing model. KEYWORDS: discourse coherence, coreference resolution, evaluation metric. Proceedings of COLING 2012: Posters, pages 483–494, COLING 2012, Mumbai, December 2012. 483 1 Introduction The task of automatically evaluating discourse coherence has recently received much attention (Karamanis et al., 2004; Barzilay and Lapata, 2008; Lin et al., 2011, etc.) because it is essential for several NLP applications such as generation (Soricut and Marcu, 2006), summarisation (Lapata, 2003; Okazaki et al., 2004; Bollegala et al., 2006) and automated essay scoring (Miltsakaki and Kukich, 2000; Higgins et al., 2004). Researchers in these areas have mainly been concerned with introducing the linguistic notions of cohesion or coherence addressed in discourse theories, such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Mann and Thompson, 1988), into computational models for each task, ranging from heuristic rule-based to sophisticated machine learning-based approaches. Some of this research has relied on the occurrence of discourse entities (e.g. NPs and pronouns) to capture cohesion of a text for indirectly estimating discourse coherence. Barzilay and Lapata (2008)’s approach, f"
C12-2048,P11-1081,1,0.909168,"ed approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines. In spite of the global optimisation by ILP, their formulation can be easily reinterpreted as follows due to the best-first constraint used in their ILP formula, which is for avoiding the redundant choice of more than one candidate antecedent: coref(i, j) = P(cor e f |i, j) + P(anaph |j) 2 (2) where j is a candidate anaphor and i is the most likely candidate antecedent of j. P(cor e f |i, j) is calculated by a simple"
C12-2048,P09-2022,0,0.0138399,"on ranking a pair of coherent and incoherent texts by comparing our metric with the entity grid model. 5.1 Data set For our evaluation, we used the NAIST text corpus, which consists of Japanese newspaper articles containing manually annotated NP coreference relations. Because the corpus has no explicit boundary between training and test sets, articles published from January 1st to January 11th and the editorials from January to August were used for training and articles dated January 14th to 17th and editorials dated October to December are used for testing as done by Taira et al. (2008) and Imamura et al. (2009). Table 2 summarises the statistics of annotated coreference relations in the corpus. Because the data set contains some texts consisting of only a sentence2 , we excluded them for our evaluation of information ordering. In line with the experiments done by Barzilay and Lapata (2008), we created 20 different texts by randomly scrambling the order of the sentences in an original text, each of which is henceforth called an incoherent text, while the original text is called a coherent text. In this evaluation, we followed Barzilay and Lapata (2008)’s experimental setting, that is, the task of pai"
C12-2048,P03-1069,0,0.102416,"Missing"
C12-2048,P11-1100,0,0.22703,"e empirically evaluated our metric by comparing it to the entity grid modelling by Barzilay and Lapata (2008) using Japanese newspaper articles as a target data set. The results indicate that our metric better reflects discourse coherence of texts than the existing model. KEYWORDS: discourse coherence, coreference resolution, evaluation metric. Proceedings of COLING 2012: Posters, pages 483–494, COLING 2012, Mumbai, December 2012. 483 1 Introduction The task of automatically evaluating discourse coherence has recently received much attention (Karamanis et al., 2004; Barzilay and Lapata, 2008; Lin et al., 2011, etc.) because it is essential for several NLP applications such as generation (Soricut and Marcu, 2006), summarisation (Lapata, 2003; Okazaki et al., 2004; Bollegala et al., 2006) and automated essay scoring (Miltsakaki and Kukich, 2000; Higgins et al., 2004). Researchers in these areas have mainly been concerned with introducing the linguistic notions of cohesion or coherence addressed in discourse theories, such as Centering Theory (Grosz et al., 1995) and Rhetorical Structure Theory (Mann and Thompson, 1988), into computational models for each task, ranging from heuristic rule-based to so"
C12-2048,P02-1014,0,0.2621,"ore of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines. In spite of the global optimisation by ILP, their formulation can be easily reinterpreted as follows due to the best-first constraint used in their ILP formula, which is for avoiding the redundant choice of more than one candidate antecedent: coref(i, j) = P(cor e f |i, j) + P(anaph |j) 2 (2) where j is a candidate anaphor and i is the most likel"
C12-2048,C02-1139,0,0.186282,"ore of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines. In spite of the global optimisation by ILP, their formulation can be easily reinterpreted as follows due to the best-first constraint used in their ILP formula, which is for avoiding the redundant choice of more than one candidate antecedent: coref(i, j) = P(cor e f |i, j) + P(anaph |j) 2 (2) where j is a candidate anaphor and i is the most likel"
C12-2048,C04-1108,0,0.0444743,"Missing"
C12-2048,J04-3003,0,0.0368159,"has been an increase in recent work for evaluating discourse (local) coherence of a text (Barzilay and Lapata, 2008; Karamanis et al., 2004; Lin et al., 2011; Miltsakaki and Kukich, 2000; Higgins et al., 2004, etc.), which strongly relates to the cohesion of discourse entities appearing in the text from the theoretical perspective mainly based on Centering Theory (Grosz et al., 1995). For example, Karamanis et al. (2004) and Miltsakaki and Kukich (2000) proposed a metric of coherence directly utilising the transition of centers in a text, as Centering Theory does. According to the analysis by Poesio et al. (2004), Karamanis et al. (2004) define a metric based on the numbers of missing backward-looking centers, each of which is a discourse entity appearing in the current utterance and was realised as most salient in the previous utterance. On the other hand, Miltsakaki and Kukich (2000) focused on investigating the relationship of the coherence of a text and the transition of centers and revealed that the rough-shift transition of centers correlates to incoherence of a text. In these studies, one of the most important work was to represent the relationship of discourse entities and their occurrences in"
C12-2048,J01-4004,0,0.115366,"needs a reliability score of each anaphor and candidate antecedent pair. Recent sophisticated approaches to NP coreference range from considering the transitivity of discourse entities (Denis and Baldridge, 2007) to clustering-based approaches (Cardie and Wagstaf, 1999; Cai and Strube, 2010), but these approaches aim at obtaining globally optimised scores for a set of mentions. Therefore, it is generally difficult using such models to get a reliability score for a pair of two mentions though they typically achieved better performance than simple pairwise coreference resolution models such as Soon et al. (2001) and Ng and Cardie (2002), In the work on Japanese anaphora resolution by Iida and Poesio (2011), they employed an ILP-based approach to optimise final outputs of NP coreference resolution in Japanese and reported it achieved better performance than simple pairwise baselines. In spite of the global optimisation by ILP, their formulation can be easily reinterpreted as follows due to the best-first constraint used in their ILP formula, which is for avoiding the redundant choice of more than one candidate antecedent: coref(i, j) = P(cor e f |i, j) + P(anaph |j) 2 (2) where j is a candidate anapho"
C12-2048,P06-2103,0,0.0485206,"Missing"
C12-2048,D08-1055,0,0.017004,"an empirical evaluation on ranking a pair of coherent and incoherent texts by comparing our metric with the entity grid model. 5.1 Data set For our evaluation, we used the NAIST text corpus, which consists of Japanese newspaper articles containing manually annotated NP coreference relations. Because the corpus has no explicit boundary between training and test sets, articles published from January 1st to January 11th and the editorials from January to August were used for training and articles dated January 14th to 17th and editorials dated October to December are used for testing as done by Taira et al. (2008) and Imamura et al. (2009). Table 2 summarises the statistics of annotated coreference relations in the corpus. Because the data set contains some texts consisting of only a sentence2 , we excluded them for our evaluation of information ordering. In line with the experiments done by Barzilay and Lapata (2008), we created 20 different texts by randomly scrambling the order of the sentences in an original text, each of which is henceforth called an incoherent text, while the original text is called a coherent text. In this evaluation, we followed Barzilay and Lapata (2008)’s experimental setting"
C16-1269,W15-2401,0,0.175071,"Missing"
C16-1269,D13-1095,0,0.0162756,"anaphora resolution have been proposed in the past research (Iida et al., 2007a; Sasano and Kurohashi, 2011; Komachi et al., 2007). The past Japanese PAS analysis methods can be categorised into two classes: one constructs an individual model for predicting each case (ga , wo , and ni ) (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011) and another constructs a single model for predicting all three cases at the same time. The 2862 latter is further divided into two types: (i) identifying all the three cases of one predicate (Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013), and (ii) identifying all the three cases of all predicates in the same sentence (Ouchi et al., 2015; Shibata et al., 2016). Since ga arguments tend to be omitted more than other cases, we focus on identifying the ga case in the present study, thus our proposing method belongs to the former class. Table 1: Features for PAS analysis 3 category ID feature name description predicate 1 2 3 4 5 6 lemma word origin parts of speech conjugation form conjugation type surface form a lemma of the predicate originated in Japanese, Chinese, other language or compound POS of the predicate a conjugation for"
C16-1269,I11-1023,0,0.0158376,"osed in the studies in English (Gildea and Jurafsky, 2002). Unlike English, subject ellipses frequently occur in Japanese. Thus dealing with ellipses, i.e. zero anaphora resolution, is fundamental in processing Japanese texts, and linguistic features for Japanese zero anaphora resolution have been proposed in the past research (Iida et al., 2007a; Sasano and Kurohashi, 2011; Komachi et al., 2007). The past Japanese PAS analysis methods can be categorised into two classes: one constructs an individual model for predicting each case (ga , wo , and ni ) (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011) and another constructs a single model for predicting all three cases at the same time. The 2862 latter is further divided into two types: (i) identifying all the three cases of one predicate (Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013), and (ii) identifying all the three cases of all predicates in the same sentence (Ouchi et al., 2015; Shibata et al., 2016). Since ga arguments tend to be omitted more than other cases, we focus on identifying the ga case in the present study, thus our proposing method belongs to the former class. Table 1: Features for PAS analysis"
C16-1269,W07-1522,0,0.421052,"predicate in most cases. Another complication is the case alternation caused by certain types of auxiliary verbs such as causative and passivisation verbs. In such cases, the original case should be recovered in the PAS analysis. In this respect, the Japanese PAS analysis shares the similarity with semantic role labelling in English (Gildea and Jurafsky, 2002). Furthermore, treating “event nouns” (Komachi et al., 2007) as predicates, we identify their arguments as well as the arguments of verbs and adjectives. Currently, several PAS annotated Japanese corpora such as NAIST Text Corpus (NTC) (Iida et al., 2007b) and BCCWJ-DepParaPAS (Ueda et al., 2015; Maekawa et al., 2014) are available. We use the latter in this study. In order to improve the PAS analysis performance, we propose to utilise the information of annotator behaviour, particularly eye gaze information, during their annotating predicate argument relations in texts. In the past PAS analysis, a model has been constructed by utilising a certain ML technique regarding the human annotated argument as the correct argument for a given predicate, i.e. it is considered the positive example for the predicate and all other argument candidates are"
C16-1269,P09-2022,0,0.141708,"PAS analysis were proposed in the studies in English (Gildea and Jurafsky, 2002). Unlike English, subject ellipses frequently occur in Japanese. Thus dealing with ellipses, i.e. zero anaphora resolution, is fundamental in processing Japanese texts, and linguistic features for Japanese zero anaphora resolution have been proposed in the past research (Iida et al., 2007a; Sasano and Kurohashi, 2011; Komachi et al., 2007). The past Japanese PAS analysis methods can be categorised into two classes: one constructs an individual model for predicting each case (ga , wo , and ni ) (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011) and another constructs a single model for predicting all three cases at the same time. The 2862 latter is further divided into two types: (i) identifying all the three cases of one predicate (Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013), and (ii) identifying all the three cases of all predicates in the same sentence (Ouchi et al., 2015; Shibata et al., 2016). Since ga arguments tend to be omitted more than other cases, we focus on identifying the ga case in the present study, thus our proposing method belongs to the former class. Table 1: F"
C16-1269,W13-4303,1,0.573044,"e way. Our proposal treats ‘tomodati (friend)’ in (1) and ‘watasi (I)’ in (2) as “near miss” candidates, i.e. better negative examples than others in each text, and takes them into account in the training. To salvage the “near miss” candidates that were merely discarded in the existing PAS analysis based on binary classification, we build our PAS analyser with a learning-to-rank framework; we make a ranking of candidates instead of their binary positive-negative distinction. It has been reported that the eye gaze information contributes to detecting annotation disagreement between annotators (Mitsuda et al., 2013), but there has been no study on the Japanese PAS analysis using eye gaze information. We hypothesise that frequent looks at argument candidates imply their plausibility of being the argument of the predicate. In this study, we make a candidate ranking based on the frequency of the annotator gaze at the candidates for training a model with the ranking SVM (Joachims, 2002) This paper is organised as follows. Section 2 overviews the related work, and in Section 3, we define the task setting and propose a method for Japanese PAS analysis. Section 4 discusses the evaluation results and Section 5 c"
C16-1269,P15-1093,0,0.017877,"2011; Komachi et al., 2007). The past Japanese PAS analysis methods can be categorised into two classes: one constructs an individual model for predicting each case (ga , wo , and ni ) (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011) and another constructs a single model for predicting all three cases at the same time. The 2862 latter is further divided into two types: (i) identifying all the three cases of one predicate (Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013), and (ii) identifying all the three cases of all predicates in the same sentence (Ouchi et al., 2015; Shibata et al., 2016). Since ga arguments tend to be omitted more than other cases, we focus on identifying the ga case in the present study, thus our proposing method belongs to the former class. Table 1: Features for PAS analysis 3 category ID feature name description predicate 1 2 3 4 5 6 lemma word origin parts of speech conjugation form conjugation type surface form a lemma of the predicate originated in Japanese, Chinese, other language or compound POS of the predicate a conjugation form of the predicate a conjugation type of the predicate a surface form of the predicate argument 7 8 9"
C16-1269,N07-1036,0,0.0552108,"Missing"
C16-1269,W16-1904,0,0.11077,"Missing"
C16-1269,I11-1085,0,0.0158041,"the related work, and in Section 3, we define the task setting and propose a method for Japanese PAS analysis. Section 4 discusses the evaluation results and Section 5 concludes the paper. 2 Related work Basic features for the PAS analysis were proposed in the studies in English (Gildea and Jurafsky, 2002). Unlike English, subject ellipses frequently occur in Japanese. Thus dealing with ellipses, i.e. zero anaphora resolution, is fundamental in processing Japanese texts, and linguistic features for Japanese zero anaphora resolution have been proposed in the past research (Iida et al., 2007a; Sasano and Kurohashi, 2011; Komachi et al., 2007). The past Japanese PAS analysis methods can be categorised into two classes: one constructs an individual model for predicting each case (ga , wo , and ni ) (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011) and another constructs a single model for predicting all three cases at the same time. The 2862 latter is further divided into two types: (i) identifying all the three cases of one predicate (Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013), and (ii) identifying all the three cases of all predicates in the same sentence (Ouchi"
C16-1269,P16-1117,0,0.0195846,"., 2007). The past Japanese PAS analysis methods can be categorised into two classes: one constructs an individual model for predicting each case (ga , wo , and ni ) (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011) and another constructs a single model for predicting all three cases at the same time. The 2862 latter is further divided into two types: (i) identifying all the three cases of one predicate (Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013), and (ii) identifying all the three cases of all predicates in the same sentence (Ouchi et al., 2015; Shibata et al., 2016). Since ga arguments tend to be omitted more than other cases, we focus on identifying the ga case in the present study, thus our proposing method belongs to the former class. Table 1: Features for PAS analysis 3 category ID feature name description predicate 1 2 3 4 5 6 lemma word origin parts of speech conjugation form conjugation type surface form a lemma of the predicate originated in Japanese, Chinese, other language or compound POS of the predicate a conjugation form of the predicate a conjugation type of the predicate a surface form of the predicate argument 7 8 9 10 11 12 lemma word or"
C16-1269,D08-1055,0,0.424665,"ic features for the PAS analysis were proposed in the studies in English (Gildea and Jurafsky, 2002). Unlike English, subject ellipses frequently occur in Japanese. Thus dealing with ellipses, i.e. zero anaphora resolution, is fundamental in processing Japanese texts, and linguistic features for Japanese zero anaphora resolution have been proposed in the past research (Iida et al., 2007a; Sasano and Kurohashi, 2011; Komachi et al., 2007). The past Japanese PAS analysis methods can be categorised into two classes: one constructs an individual model for predicting each case (ga , wo , and ni ) (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011) and another constructs a single model for predicting all three cases at the same time. The 2862 latter is further divided into two types: (i) identifying all the three cases of one predicate (Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013), and (ii) identifying all the three cases of all predicates in the same sentence (Ouchi et al., 2015; Shibata et al., 2016). Since ga arguments tend to be omitted more than other cases, we focus on identifying the ga case in the present study, thus our proposing method belongs to the fo"
C16-1269,W13-0508,1,0.857426,"various kinds of information, as supervised machine learning (ML) techniques had been a significant device for natural language processing (NLP) (Pustejovsky and Stubbs, 2012). The annotated corpus is used as training data for constructing a task model, considering the annotated information as expected output of the model. In the current framework, however, only annotated information in the corpus is used for training. In this paper, we propose utilising the information of annotator behaviour during the annotation process as well as resulting annotated information for training the task model (Tokunaga et al., 2013). We take the predicate argument structure (PAS) analysis of Japanese texts as the target task in the present work. Predicate argument relations are usually marked by case particles denoting grammatical cases in Japanese, therefore identifying dependencies marked by the major obligatory cases, ga (nominative), wo (accusative) and ni (dative) is the main task. However, since ellipses are ubiquitous in Japanese texts, arguments might be identified beyond the sentence including the target predicates (inter-sentence arguments) as well as within the sentence (intra-sentence arguments). This feature"
C16-1269,P10-1118,0,0.517841,"Missing"
C16-1269,I11-1126,0,0.0184799,"atures for Japanese zero anaphora resolution have been proposed in the past research (Iida et al., 2007a; Sasano and Kurohashi, 2011; Komachi et al., 2007). The past Japanese PAS analysis methods can be categorised into two classes: one constructs an individual model for predicting each case (ga , wo , and ni ) (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011) and another constructs a single model for predicting all three cases at the same time. The 2862 latter is further divided into two types: (i) identifying all the three cases of one predicate (Sasano and Kurohashi, 2011; Yoshikawa et al., 2011; Hangyo et al., 2013), and (ii) identifying all the three cases of all predicates in the same sentence (Ouchi et al., 2015; Shibata et al., 2016). Since ga arguments tend to be omitted more than other cases, we focus on identifying the ga case in the present study, thus our proposing method belongs to the former class. Table 1: Features for PAS analysis 3 category ID feature name description predicate 1 2 3 4 5 6 lemma word origin parts of speech conjugation form conjugation type surface form a lemma of the predicate originated in Japanese, Chinese, other language or compound POS of the predi"
C16-1269,J02-3001,0,\N,Missing
C18-1040,W17-5514,0,0.0163839,"the slot-value pairs From-Location=“New York” and To-Location=“Chicago” are extracted from “I’m going to go to Chicago from New York.”. Regarding this general framework, our task corresponds to the slot-filling task. The slot-filling task can be formalised as a sequential labelling problem where each word in the target utterance is assigned an IOB tag of semantic slots (Ramshaw and Marcus, 1995). Recently, many studies adopt Recurrent Neural Network (RNN)(Mesnil et al., 2013; Yao et al., 2013; Mesnil et al., 2015; Vu et al., 2016; Jaech et al., 2016; Liu and Lane, 2016b; Liu and Lane, 2016a; Bapna et al., 2017) and Long Short-Term Memory (LSTM) (Yao et al., 2014; Hakkani-t et al., 2016) for the sequential labelling to achieve better performance. These methods, however, cannot identify semantic slots from the implicit condition which we are targeting, because they capture only explicitly mentioned semantic slots through the sequential labelling. There are very few studies which address the task of predicting the users implicit condition in the task-oriented dialogue system. Celikyilmaz et al. (2012) proposed the method which predicts the user’s preferred movie genre in the movie search domain. They e"
C18-1040,H94-1010,0,0.558709,"its backend application. When being used as a natural language interface of a database system, the dialogue system should be able to extract pieces of information corresponding to the record fields of the database (DB) for constructing a query. There have been several attempts to extract this type of information from user utterances in database search dialogues. For instance, several studies (Raymond and Riccardi, 2007; Mesnil et al., 2015; Liu and Lane, 2016b) tried to extract values for the database field defined in the ATIS (The Air Travel Information System) corpus (Hemphill et al., 1990; Dahl et al., 1994) from the user utterances. The ATIS corpus includes a set of dialogues between users and an air travel system that were collected through the Wizard-of-Oz method. The tags that correspond to the backend database fields, e.g. departure city, arrival date, are annotated to the expressions in the user utterances. However, the utterances in real dialogues include information that does not always directly correspond to database fields but provides useful information for constructing database queries. It will be natural and more efficient if the dialogue system can utilise this type of information f"
C18-1040,W17-5506,0,0.0128875,"t necessarily mention the movie genre. For instance, they estimate the comedy genre from the utterance “I wanna watch a movie that will make me laugh”. Although the motivation of their work is almost the same as ours, their method estimates only a single attribute, i.e. the movie genre, and does not present the evidence for that estimate. Our method provides the evidence of the estimate at the same time and we deal with multiple attributes simultaneously. In recent years, the end-to-end task-oriented dialogue systems which directly generate system utterances from user utterances are proposed. Eric et al. (2017) proposed an end-to-end dialogue system which accepts queries for the underlying knowledge base and returns the entities related to the users 478 Location available railway lines walking distance to a station nearest station facility zone surrounding facilities land characteristics distance to a specific place Building Facilities building age floor plan floor area room placement in the building room size building structure sunlight building facilities security system number of storeys number of households renovation room facilities air conditioning storage bathroom kitchen TV and Internet Prop"
C18-1040,L18-1434,1,0.847331,"s directly correspond to database fields but provides useful information for constructing database queries. It will be natural and more efficient if the dialogue system can utilise this type of information for retrieving the database. For instance, in real estate search dialogues, which is our target domain, the number of family members provides useful information for deciding the size of a house, but it is rarely a field of the real estate database as the number of family members is an attribute of the customer rather than that of houses. We call this type of information implicit conditions (Fukunaga et al., 2018). To realise a dialogue system that can utilise the implicit conditions for the database search, we need to tackle the following two tasks. (1) extracting pairs of a DB field and its value from user utterances that include the implicit conditions (2) identifying the span in the utterance that represents the evidence for the DB field and value pair extraction The task (1) provides useful information for efficiently constructing database queries from utterances that include the implicit conditions. The identified utterance span by the task (2) is helpful in constructing This work is licenced und"
C18-1040,H90-1021,0,0.384784,"nces highly depends on its backend application. When being used as a natural language interface of a database system, the dialogue system should be able to extract pieces of information corresponding to the record fields of the database (DB) for constructing a query. There have been several attempts to extract this type of information from user utterances in database search dialogues. For instance, several studies (Raymond and Riccardi, 2007; Mesnil et al., 2015; Liu and Lane, 2016b) tried to extract values for the database field defined in the ATIS (The Air Travel Information System) corpus (Hemphill et al., 1990; Dahl et al., 1994) from the user utterances. The ATIS corpus includes a set of dialogues between users and an air travel system that were collected through the Wizard-of-Oz method. The tags that correspond to the backend database fields, e.g. departure city, arrival date, are annotated to the expressions in the user utterances. However, the utterances in real dialogues include information that does not always directly correspond to database fields but provides useful information for constructing database queries. It will be natural and more efficient if the dialogue system can utilise this t"
C18-1040,D16-1011,0,0.0864345,"cting the DB field “number of rooms” and its value is worthwhile for constructing the clarification utterance. We tackle the task (1) in two steps: extracting the DB field and extracting its value. In this paper, we first formalise the DB field extraction as a classification problem of user utterances into the DB fields. As we can derive multiple DB fields from a single utterance, the DB field extraction can be formalised as a multi-label classification. Furthermore, we solve the DB field classification together with the task (2), the evidence identification by adopting the method proposed by Lei et al. (2016). We put the DB field value extraction for the future work, as it requires the information of the database structure and its contents. 2 Related Work The traditional pipeline for task-oriented dialogue systems consists of four modules: Natural Language Understanding (NLU), Dialogue State Tracking, Policy Learning and Natural Language Generation (Chen et al., 2017). The NLU task can be further divided into two tasks: intent detection and slotfilling. The intent detection classifies user utterances into the categories of user intention, e.g. request, question, inform and so on. The slot-filling"
C18-1040,N03-1020,0,0.0609758,"s. One utterance chunk can be assigned with more than one tag. For instance, a content description, “the number of people to live”, which is assigned to an utterance, “I will live alone.”, is mapped onto floor plan and floor area. The evidence span for each assigned DB field tag is also annotated in the chunk. These annotation tasks are done by one of authors. 5.2 Evaluation Measures We use precision, recall and F-measure for the evaluation of the DB field classification. To evaluate the evidence span identification, we calculate F-measure on words, and BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003). In the evaluation of the evidence span identification, we evaluated only cases where the DB field was correctly classified. Let binary vectors z˜ = (z˜1 , z˜2 , · · · , z˜m ) and z = (z1 , z2 , · · · , zm ) be the estimated and true evidence spans respectively, where the vector element one represents that the corresponding word is selected for constituting the evidence. Given z˜ and z, the F-measure on words can be calculated by equation (5). TP = m X z˜t zt , FP = t=1 m X z˜t (1 − zt ), t=1 FN = m X (1 − z˜t )zt (4) t=1 2TP (5) 2TP + FP + FN For calculating BLEU and ROUGE, we define the set"
C18-1040,W16-3603,0,0.0677036,"etter performance than the RCNN-based model. 1 Introduction The information that a dialogue system needs to extract from user utterances highly depends on its backend application. When being used as a natural language interface of a database system, the dialogue system should be able to extract pieces of information corresponding to the record fields of the database (DB) for constructing a query. There have been several attempts to extract this type of information from user utterances in database search dialogues. For instance, several studies (Raymond and Riccardi, 2007; Mesnil et al., 2015; Liu and Lane, 2016b) tried to extract values for the database field defined in the ATIS (The Air Travel Information System) corpus (Hemphill et al., 1990; Dahl et al., 1994) from the user utterances. The ATIS corpus includes a set of dialogues between users and an air travel system that were collected through the Wizard-of-Oz method. The tags that correspond to the backend database fields, e.g. departure city, arrival date, are annotated to the expressions in the user utterances. However, the utterances in real dialogues include information that does not always directly correspond to database fields but provide"
C18-1040,P02-1040,0,0.105956,"tion onto some of the DB field tags. One utterance chunk can be assigned with more than one tag. For instance, a content description, “the number of people to live”, which is assigned to an utterance, “I will live alone.”, is mapped onto floor plan and floor area. The evidence span for each assigned DB field tag is also annotated in the chunk. These annotation tasks are done by one of authors. 5.2 Evaluation Measures We use precision, recall and F-measure for the evaluation of the DB field classification. To evaluate the evidence span identification, we calculate F-measure on words, and BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003). In the evaluation of the evidence span identification, we evaluated only cases where the DB field was correctly classified. Let binary vectors z˜ = (z˜1 , z˜2 , · · · , z˜m ) and z = (z1 , z2 , · · · , zm ) be the estimated and true evidence spans respectively, where the vector element one represents that the corresponding word is selected for constituting the evidence. Given z˜ and z, the F-measure on words can be calculated by equation (5). TP = m X z˜t zt , FP = t=1 m X z˜t (1 − zt ), t=1 FN = m X (1 − z˜t )zt (4) t=1 2TP (5) 2TP + FP + FN For calculating BL"
C18-1040,W95-0107,0,0.381598,"ent detection classifies user utterances into the categories of user intention, e.g. request, question, inform and so on. The slot-filling extracts the semantic contents of user utterances in the form of slot-value pairs, e.g. the slot-value pairs From-Location=“New York” and To-Location=“Chicago” are extracted from “I’m going to go to Chicago from New York.”. Regarding this general framework, our task corresponds to the slot-filling task. The slot-filling task can be formalised as a sequential labelling problem where each word in the target utterance is assigned an IOB tag of semantic slots (Ramshaw and Marcus, 1995). Recently, many studies adopt Recurrent Neural Network (RNN)(Mesnil et al., 2013; Yao et al., 2013; Mesnil et al., 2015; Vu et al., 2016; Jaech et al., 2016; Liu and Lane, 2016b; Liu and Lane, 2016a; Bapna et al., 2017) and Long Short-Term Memory (LSTM) (Yao et al., 2014; Hakkani-t et al., 2016) for the sequential labelling to achieve better performance. These methods, however, cannot identify semantic slots from the implicit condition which we are targeting, because they capture only explicitly mentioned semantic slots through the sequential labelling. There are very few studies which addres"
C88-2136,C86-1001,0,0.0673329,"Missing"
C88-2136,J83-1005,0,0.233852,"Missing"
C88-2136,J81-4003,0,\N,Missing
C94-2139,C90-2067,0,0.0234214,"In such cases, we have several category collocations from a single word collocation, some of which are incorrect. TILe choices arc as follows; (1) use word collocations with all words is assigned a single category. (2) equally distribute frequency of word collcations to all possible category collocations [4] (3) calculate the probability of each category collocation and distribute frequency based on these probabilities; the probability of collocations are calculated by using method (2) [4] (4) determine the correct category collocation by using statistical methods other than word collocations [2, 10, 9, 6] Fortunately, there are few words that are itssigned multiple categories in BGH. Therefore, we use method (1). Word collocations containing words with multiple categories represent about 1/3 of the corpus. If we used other thesauruses, which assign multiple categories to more words, we would need to use method (2), (3), or (4). 2.3 Counting Occurrence gory Collocations of CateAfter assigning the thesaurus categories to words, wc count occurrence frequencies of category collocations as follows: 1. collect word collocations, at this time we collect only patterns of word collocations, but we do n"
C94-2139,C92-2070,0,0.0454175,"Missing"
C94-2139,P91-1036,0,\N,Missing
C94-2139,C94-2125,0,\N,Missing
C94-2139,C92-2099,0,\N,Missing
C94-2139,P91-1030,0,\N,Missing
C94-2139,P90-1034,0,\N,Missing
C96-1012,P92-1032,0,0.411131,"Missing"
C96-1012,C94-1049,0,0.155908,"Missing"
C96-1012,P91-1034,0,0.395741,"Missing"
C96-1012,J94-4003,0,0.176682,"Missing"
C96-1012,P95-1026,0,0.135131,"Missing"
D19-5212,W17-5706,0,0.0194755,"the respective corpora. We used domain adaptations with the names of the styls as domain tags. We used a “<JIJI-style>” tag for the JIJI, Aligned-JIJI, and BT-JIJI corpora and a “<Equivalent-style>” tag for EquivalentJIJI Corpus. In addition, we used a “<YOMIURIstyle>” tag for Aligned-Yomiuri Corpus because it comes from a newspaper other than Jiji Press news. JIJI Corpus, which is extracted from Japanese and English Jiji Press news, is relatively small compared with those used in other Japanese→English or English→Japanese tasks of WAT 2019. To alleviate this low-resource translation problem, Morishita et al. (2017) used other resources for pre-training and fine-tuned with JIJI Corpus. We also used the external resources to improve the translation quality of the newswire tasks. For this purpose, we developed four types of corpora apart from JIJI Corpus. The first one was constructed through content-equivalent manual translation of Japanese Jiji Press news into English and is named Equivalent-JIJI Corpus. The second one was obtained through automatic sentence alignment between Japanese and English Jiji Press news using a sentence similarity score and is named AlignedJIJI. The official JIJI Corpus is const"
D19-5212,N16-1005,0,0.347953,"6–111 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics External Corpus name Construction method data Japanese English Size JIJI Corpus Alignment No Jiji Press news Jiji Press news 0.20 M Equivalent-JIJI Corpus Manual translation Yes Jiji Press news Content-equivalent translation 0.22 M Aligned-JIJI Corpus Alignment Yes Jiji Press news Jiji Press news 0.29 M BT-JIJI Corpus Back translation Yes NMT output Jiji Press news 0.53 M Aligned-Yomiuri Corpus Alignment Yes Yomiuri Shimbun Yomiuri Shimbun 0.61 M Table 1: Dataset. 2 Corpus Description (Chu et al., 2017; Sennrich et al., 2016a) is one of the most effective approaches to leverage out-ofdomain data. Chu et al. (2017) proposed training an NMT system with multi-domain parallel corpora using domain tags such as “<domainname>” attached to the respective corpora. We used domain adaptations with the names of the styls as domain tags. We used a “<JIJI-style>” tag for the JIJI, Aligned-JIJI, and BT-JIJI corpora and a “<Equivalent-style>” tag for EquivalentJIJI Corpus. In addition, we used a “<YOMIURIstyle>” tag for Aligned-Yomiuri Corpus because it comes from a newspaper other than Jiji Press news. JIJI Corpus, which is ext"
D19-5212,P11-2093,0,0.0117932,"pora as shown in Table 1, and evaluated these NMT models with an official test-set, in which the number of data was 2000. Then, we evaluated these NMT models with a further test-set, in which the number of data was 1764, extracted from Equivalent-JIJI Corpus of Equivalent-style in contrast to the official test-set extracted JIJI Corpus in JIJI-style. Finally, we evaluated the effectiveness of the Equivalent-style translation. 4.1 Data Processing and System Setup All of the datasets were preprocessed as follows. We used the Moses toolkit 2 to clean and tokenize the English data and used KyTea (Neubig et al., 2011) to tokenize the Japanese data. Then, we used a vocabulary of 32K units based on a joint source and target byte-pair encoding (BPE) (Sennrich et al., 2016c). For the translation model, Domain Adaptation Techniques In this paper, we used a domain-adaptation technique to train a model adapted to the JIJIand Equivalent-style. The multi-domain method 2 107 https://github.com/moses- smt/ mosesdecoder Training corpus JIJI (Official data) Equivalent-JIJI JIJI, Equivalent-JIJI JIJI, Equivalent-JIJI JIJI, Equivalent-JIJI JIJI, Equivalent-JIJI, Aligned-JIJI, Aligned-Yomiuri JIJI, Equivalent-JIJI, Aligne"
D19-5212,P02-1040,0,0.10581,"eight checkpoints. We set the batch size to 5000 tokens and maximum sentence length to 99 BPE units. For the other hyperparameters of our models, we used the default parameter values of Sockeye. We used early stopping with a patience of 32. Decoding was performed with a beam search with a beam size of 5, and we did not apply an ensemble decoding with multiple models, although this could possibly improve the translation quality, though we used a beam search with a beam size of 30 and an ensemble of ten models when submitting the official results. To evaluate translation quality, we used BLEU (Papineni et al., 2002). BLEU is calculated using multi-bleu.perl 3 . We report case-sensitive scores. 4.2 Results Tables 2 and 3 show the experimental results. The Training corpus column shows the corpora used for training. The Style column shows the tag used for translation, i.e. the JIJI- or Equivalent-style. The JIJI-style test-set is equal to the official testset in the newswire task of WAT 2019. 4.2.1 Trained with Different Combinations of Five Corpora The JIJI-style test-set column of Tables 2 and 3 shows the translation quality of the JIJI-style testsets with the BLEU metric for different combinations of the"
D19-5212,P16-1009,0,0.496165,"6–111 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics External Corpus name Construction method data Japanese English Size JIJI Corpus Alignment No Jiji Press news Jiji Press news 0.20 M Equivalent-JIJI Corpus Manual translation Yes Jiji Press news Content-equivalent translation 0.22 M Aligned-JIJI Corpus Alignment Yes Jiji Press news Jiji Press news 0.29 M BT-JIJI Corpus Back translation Yes NMT output Jiji Press news 0.53 M Aligned-Yomiuri Corpus Alignment Yes Yomiuri Shimbun Yomiuri Shimbun 0.61 M Table 1: Dataset. 2 Corpus Description (Chu et al., 2017; Sennrich et al., 2016a) is one of the most effective approaches to leverage out-ofdomain data. Chu et al. (2017) proposed training an NMT system with multi-domain parallel corpora using domain tags such as “<domainname>” attached to the respective corpora. We used domain adaptations with the names of the styls as domain tags. We used a “<JIJI-style>” tag for the JIJI, Aligned-JIJI, and BT-JIJI corpora and a “<Equivalent-style>” tag for EquivalentJIJI Corpus. In addition, we used a “<YOMIURIstyle>” tag for Aligned-Yomiuri Corpus because it comes from a newspaper other than Jiji Press news. JIJI Corpus, which is ext"
D19-5212,P17-2061,0,0.0177061,"nslation, pages 106–111 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics External Corpus name Construction method data Japanese English Size JIJI Corpus Alignment No Jiji Press news Jiji Press news 0.20 M Equivalent-JIJI Corpus Manual translation Yes Jiji Press news Content-equivalent translation 0.22 M Aligned-JIJI Corpus Alignment Yes Jiji Press news Jiji Press news 0.29 M BT-JIJI Corpus Back translation Yes NMT output Jiji Press news 0.53 M Aligned-Yomiuri Corpus Alignment Yes Yomiuri Shimbun Yomiuri Shimbun 0.61 M Table 1: Dataset. 2 Corpus Description (Chu et al., 2017; Sennrich et al., 2016a) is one of the most effective approaches to leverage out-ofdomain data. Chu et al. (2017) proposed training an NMT system with multi-domain parallel corpora using domain tags such as “<domainname>” attached to the respective corpora. We used domain adaptations with the names of the styls as domain tags. We used a “<JIJI-style>” tag for the JIJI, Aligned-JIJI, and BT-JIJI corpora and a “<Equivalent-style>” tag for EquivalentJIJI Corpus. In addition, we used a “<YOMIURIstyle>” tag for Aligned-Yomiuri Corpus because it comes from a newspaper other than Jiji Press news. JI"
D19-5212,P16-1162,0,0.683911,"6–111 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics External Corpus name Construction method data Japanese English Size JIJI Corpus Alignment No Jiji Press news Jiji Press news 0.20 M Equivalent-JIJI Corpus Manual translation Yes Jiji Press news Content-equivalent translation 0.22 M Aligned-JIJI Corpus Alignment Yes Jiji Press news Jiji Press news 0.29 M BT-JIJI Corpus Back translation Yes NMT output Jiji Press news 0.53 M Aligned-Yomiuri Corpus Alignment Yes Yomiuri Shimbun Yomiuri Shimbun 0.61 M Table 1: Dataset. 2 Corpus Description (Chu et al., 2017; Sennrich et al., 2016a) is one of the most effective approaches to leverage out-ofdomain data. Chu et al. (2017) proposed training an NMT system with multi-domain parallel corpora using domain tags such as “<domainname>” attached to the respective corpora. We used domain adaptations with the names of the styls as domain tags. We used a “<JIJI-style>” tag for the JIJI, Aligned-JIJI, and BT-JIJI corpora and a “<Equivalent-style>” tag for EquivalentJIJI Corpus. In addition, we used a “<YOMIURIstyle>” tag for Aligned-Yomiuri Corpus because it comes from a newspaper other than Jiji Press news. JIJI Corpus, which is ext"
D19-5212,W18-1820,0,0.0215703,"e test-set 15.9 36.67 39.97 36.75 39.93 43.52 43.82 43.68 Table 3: BLEU scores for English→Japanese translation tasks. we used the encoder and decoder of the transformer model (Vaswani et al., 2017), which is a state of the art NMT model. The transformer model uses a multi-headed attention mechanism applied as self-attention and a position-wise fully connected feed-forward network. The encoder converts the received source language sentence into a sequence of continuous representations, and the decoder generates the target language sentence. We implemented our systems with the Sockeye toolkit (Hieber et al., 2018), and trained them on one Nvidia P100 Tesla GPU. While training our models, we used the stochastic gradient descent (SGD) with Adam (Kingma and Ba, 2015) as the optimizer, using a learning rate of 0.0002, multiplied by 0.7 after every eight checkpoints. We set the batch size to 5000 tokens and maximum sentence length to 99 BPE units. For the other hyperparameters of our models, we used the default parameter values of Sockeye. We used early stopping with a patience of 32. Decoding was performed with a beam search with a beam size of 5, and we did not apply an ensemble decoding with multiple mod"
E06-1023,J95-4001,0,0.11516,"Missing"
E06-1023,P03-1070,0,0.219476,"d as a set of propositions as shown in (6). ments when they are presented in parallel with speech. Therefore, this constraint does not apply to them. Grasp(#Agt1, #Obj1), Move(#Agt1, #Obj1, #Dst1), Release(#Agt1, #Obj1) Here, (5.1 – 5.5) are reformulated as in (8.1 – 8.5). “Perform” represents performing the action. (8.1) U: Request(U, V, Put(#Agt1, #Obj1, #Dst1)) (8.2) V: Accept(V, U, α) (8.3) V: Perform(V, U, Grasp(#Agt1, #Obj1)) 180 (8.4) V: Perform(V, U, Move(#Agt1, #Obj1, #Dst1)) sufficient information to identify the target. (9b) and (9c) enable (9a). (8.5) U: Inform(U, V, incorrect(X)) Nakano et al. (2003) experimentally confirmed To understand DRIU (5.5), i.e., (8.5), follower that we observe negative responses as well as posV has to identify repair target X in (8.5) referred itive responses in the process of grounding. Acto as “that” in (5.5). In this case, the repair target cording to their observations, speakers continue of (5.5) X is “the left box”, i.e., #Dst1.5 However, dialogues if negative responses are not found even the pronoun “that” cannot be resolved by anaphora when positive responses are not found. This eviresolution only using textual information. dence supports (9d). We treat"
E99-1013,J90-1003,0,0.100414,"gory as w. 2.3 2.3.1 Corpus-based Thesaurus Co-occurrence-based Thesaurus This method is based on the assumption that a pair of words that frequently occur together in the same document are related to the same subject. Therefore word co-occurrence information can be used to identify semantic relationships between words (Schutze and Pederson, 1997; Schutze and Pederson, 1994). We use mutual information as a 95 tool for computing similarity between words. Mutual information compares the probability of the co-occurence of words a and b with the independent probabilities of occurrence of a and b (Church and Hanks, 1990). P(a, b) I(a, b) = log P(a)P(b) where the probabilities of P(a) and P(b) are estimated by counting the number of occurrences of a and b in documents and normalizing over the size of vocabulary in the documents. The joint probability is estimated by counting the number of times that word a co-occurs with b and is also normalized over the size of the vocabulary. 2.3.2 S y n t a c t i c a l l y - b a s e d T h e s a u r u s In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appear"
E99-1013,P90-1034,0,0.0614803,"he size of vocabulary in the documents. The joint probability is estimated by counting the number of times that word a co-occurs with b and is also normalized over the size of the vocabulary. 2.3.2 S y n t a c t i c a l l y - b a s e d T h e s a u r u s In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appearing in similax grammatical contexts are assumed to be similar, and therefore classified into the same class (Lin, 1998; Grefenstette, 1994; Grefenstette, 1992; Ruge, 1992; Hindle, 1990). First, all the documents are parsed using the Apple Pie Parser. The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University (Sekine and Grishman, 1995). The parser is a bottom-up probabilistic chart parser which finds the parse tree with the best score by way of the best-first search algorithm. Its grammar is a semi-context sensitive grammar with two non-terminals and was automatically extracted from Penn Tree Bank syntactically tagged corpus developed at the University of Pennsylvania. The parser generates a syntactic tree in the manner o"
E99-1013,P98-2127,0,0.267177,"Thesaurus (Chapman, 1977), words are classified according to the ideas they express, and these categories of ideas are numbered in sequence. The terms within a category are further organized by part of speech (nouns, verbs, adjectives, adverbs, prepositions, conjunctions, and interjections). Figure 2 shows a fragment of Roget&apos;s category. In this case, our similarity measure treat all the words in Roger as features. A word w possesses the feature f if f and w belong to the same Roget category. The similarity between two words is then defined as the Dice coefficient of the two feature vectors (Lin, 1998). sim(wl,w2) = 21R(wl) n R(w~)l tn(w,)l + In(w )l where R(w) is the set of words that belong to the same Roget category as w. 2.3 2.3.1 Corpus-based Thesaurus Co-occurrence-based Thesaurus This method is based on the assumption that a pair of words that frequently occur together in the same document are related to the same subject. Therefore word co-occurrence information can be used to identify semantic relationships between words (Schutze and Pederson, 1997; Schutze and Pederson, 1994). We use mutual information as a 95 tool for computing similarity between words. Mutual information compares"
E99-1013,1995.iwpt-1.26,0,0.0154909,".3.2 S y n t a c t i c a l l y - b a s e d T h e s a u r u s In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics. Words appearing in similax grammatical contexts are assumed to be similar, and therefore classified into the same class (Lin, 1998; Grefenstette, 1994; Grefenstette, 1992; Ruge, 1992; Hindle, 1990). First, all the documents are parsed using the Apple Pie Parser. The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University (Sekine and Grishman, 1995). The parser is a bottom-up probabilistic chart parser which finds the parse tree with the best score by way of the best-first search algorithm. Its grammar is a semi-context sensitive grammar with two non-terminals and was automatically extracted from Penn Tree Bank syntactically tagged corpus developed at the University of Pennsylvania. The parser generates a syntactic tree in the manner of a Penn Tree Bank bracketing. Figure 3 shows a parse tree produced by this parser. The main technique used by the parser is the best-first search. Because the grammar is probabilistic, it is enough to find"
E99-1013,C98-2122,0,\N,Missing
fujii-etal-2012-effects,C04-1093,1,\N,Missing
fujii-etal-2012-effects,P09-1024,0,\N,Missing
fujii-etal-2012-effects,P08-1092,0,\N,Missing
fujii-etal-2012-effects,fujii-2008-producing,1,\N,Missing
fujii-etal-2012-effects,fujii-2010-modeling,1,\N,Missing
I05-2019,bird-etal-2002-tabletrans,0,0.019609,"ocessing research for the last decade. Particularly, syntactically annotated corpora (treebanks), such as Penn Treebank (Marcus et al., 1993), Negra Corpus (Skut et al., 1997) and EDR Corpus (Jap, 1994), contribute to improve the performance of morpho-syntactic analysis systems. It is notorious, however, that building a large treebank is labor intensive and time consuming work. In addition, it is quite difficult to keep quality and consistency of a large treebank. To remedy this problem, there have been many attempts to develop software tools for annotating treebanks (Plaehn and Brants, 2000; Bird et al., 2002). 108 • Annotation plug-in module: This module helps to choose a correct syntactic structure from candidate structures. • Retrieval plug-in module: This module retrieves similar sentences to a sentence in question from already annotated sentences in the treebank. These two plug-in modules work cooperatively in the Eclipse framework. For example, information can be transferred easily between these two modules in a copy-and-past manner. Furthermore, since they are implemented as Eclipse plugin modules, these functionalities can also interact with other plug-in modules and Eclipse native features"
I05-2019,J93-2004,0,0.0268136,"helps annotators to choose a correct syntactic structure of a sentence from outputs of a parser, allowing the annotators to retrieve similar sentences in the treebank for referring to their structures. To realize the tight coupling of annotation and retrieval, eBonsai has been implemented as the following two plug-in modules of an universal tool platform: Eclipse (The Eclipse Foundation, 2001). 1 Introduction Statistical approach has been a main stream of natural language processing research for the last decade. Particularly, syntactically annotated corpora (treebanks), such as Penn Treebank (Marcus et al., 1993), Negra Corpus (Skut et al., 1997) and EDR Corpus (Jap, 1994), contribute to improve the performance of morpho-syntactic analysis systems. It is notorious, however, that building a large treebank is labor intensive and time consuming work. In addition, it is quite difficult to keep quality and consistency of a large treebank. To remedy this problem, there have been many attempts to develop software tools for annotating treebanks (Plaehn and Brants, 2000; Bird et al., 2002). 108 • Annotation plug-in module: This module helps to choose a correct syntactic structure from candidate structures. • R"
I05-2019,A97-1014,0,0.112172,"syntactic structure of a sentence from outputs of a parser, allowing the annotators to retrieve similar sentences in the treebank for referring to their structures. To realize the tight coupling of annotation and retrieval, eBonsai has been implemented as the following two plug-in modules of an universal tool platform: Eclipse (The Eclipse Foundation, 2001). 1 Introduction Statistical approach has been a main stream of natural language processing research for the last decade. Particularly, syntactically annotated corpora (treebanks), such as Penn Treebank (Marcus et al., 1993), Negra Corpus (Skut et al., 1997) and EDR Corpus (Jap, 1994), contribute to improve the performance of morpho-syntactic analysis systems. It is notorious, however, that building a large treebank is labor intensive and time consuming work. In addition, it is quite difficult to keep quality and consistency of a large treebank. To remedy this problem, there have been many attempts to develop software tools for annotating treebanks (Plaehn and Brants, 2000; Bird et al., 2002). 108 • Annotation plug-in module: This module helps to choose a correct syntactic structure from candidate structures. • Retrieval plug-in module: This modu"
I05-2019,1993.iwpt-1.10,0,0.00985978,"and Eclipse native features such as CVS. Figure 1: A snapshot of eBonsai                                                                                                            Figure 2: A workflow of annotation using eBonsai 109 2 Annotating treebanks Figure 2 shows a workflow of annotating a treebank using eBonsai. 1. An annotator picks a sentence to annotate from plain-text corpora. 2. The MSLR parser (Tanaka et al., 1993) performs syntactic analysis of the sentence. 3. The annotator chooses a correct syntactic structure from the output of the parser. If necessary, retrieval of structures in the treebank is available in this step. 4. The annotator adds the chosen syntactic structure to the treebank. The coverage of Japanese grammar used in the MSLR parser is fairly wide. The number of grammar rules of the current system is almost 3,000. That means we have a lot of outputs as a result of syntactic analysis in step 2. These structures are represented in terms of a special data structure called packed shared fores"
I05-2019,yoshida-etal-2004-retrieving,1,0.815903,"ually put constraints to narrow down to a correct structure considering the meaning of a sentence. However, there are cases in which it is difficult to pin down a correct one by referring to only that sentence. Annotators can consult the system to retrieve the similar structure of sentences in the treebank. The retrieval plug-in module provides annotators such functionality. The retrieval plugin module receives a syntactic structure as a query and provides a list of sentences which include the given structure. The retrieval plug-in module has been realized with the method proposed by Yoshida (Yoshida et al., 2004). The method is based on Yoshikawa’s method (Yoshikawa et al., 2001) which was originally proposed for handling XML documents effectively by using relational database (RDB) systems. Yoshida adopted Yoshikawa’s method to deal with syntactic structures in the database. Since an XML document can be represented as a tree, Yoshikawa’s method is also applicable to deal with syntactic structures. Figure 4: An input query for retrieval An input query is given as a tree as shown in Figure 4. The structure is then translated into a SQL query and the retrieval is performed. A query involving a large numb"
I05-4002,J93-2004,0,\N,Missing
I05-4002,J94-4001,0,\N,Missing
I05-4002,W02-2016,0,\N,Missing
I05-4002,P03-1054,0,\N,Missing
I05-4002,J05-1003,0,\N,Missing
I05-4002,C04-1056,0,\N,Missing
I05-4002,W04-3224,0,\N,Missing
I08-1052,C00-1014,0,0.7112,"Missing"
I08-1052,C94-1091,1,0.807876,"Missing"
I11-1010,stoia-etal-2008-scare,0,\N,Missing
I11-1010,D10-1046,0,\N,Missing
I11-1010,W98-1119,0,\N,Missing
I11-1010,D08-1069,0,\N,Missing
I11-1010,P10-1128,1,\N,Missing
I11-1010,E09-1032,0,\N,Missing
I11-1010,J95-2003,0,\N,Missing
I11-1010,J94-4002,0,\N,Missing
I11-1010,P87-1022,0,\N,Missing
I11-1010,P02-1014,0,\N,Missing
I11-1010,J01-4004,0,\N,Missing
I11-1010,J86-3001,0,\N,Missing
I11-1010,P96-1036,0,\N,Missing
I11-1010,P03-1023,0,\N,Missing
I17-2049,D14-1179,0,0.044685,"Missing"
I17-2049,D15-1166,0,0.28351,"h and h are concatenated into the hidden states h = (h1 , ..., hM ) ∈ RK×M as − → ← −⊤ ⊤ hi = We [ h ⊤ (3) i ; hi ] , where We ∈ RK×2K is a matrix for the affine transform. Each hidden state, represented as a single vector, can be seen a memory vector that includes not only the lexical information at its source position, but also information about the left and right contexts. Then, the decoder predicts the target sentence y using a conditional probability calculated as bellow: Method 3.1 NMT with Attention p(yj |y1,j−1 , x) = softmax(Wo ej + bo ), Our work is based on an attention-based NMT (Luong et al., 2015), which generates a target sentence y = (y1 , ..., yN ) ∈ RVt ×N from the source sentence x = (x1 , ..., xM ) ∈ RVs ×M . Vs and Vt denote the vocabulary size of the source and target side, respectively. The attention-based model comprises two components, an encoder and a decoder. The encoder embeds the source sentence x into vectors through an embedding matrix and produces the hidden states using a bidirectional RNN, which represents a forward and a backward sequence. Thus, we have − → − → h i = enc1 (Ws xi , h i−1 ), ← − ← − h i = enc2 (Ws xi , h i+1 ). ?"" (4) where Wo ∈ RVt ×K and bo ∈ RVt a"
I17-2049,D16-1147,0,0.416559,"calculated as the inner product of the source hidden-state and the target word hidden-state. Note that the source hiddenstate acts as the key to weight itself. It also acts as the value to predict the target word through the context vector. Daniluk et al. (2017) suppose that the dual use of a single vector makes training the model difficult and propose the use of a key-value paired structure, which is a generalized way of storing content in the vector. In this paper, we propose splitting the matrix of the source hidden-states into two parts, an approach suggested by Daniluk et al. (2017) and Miller et al. (2016). The first part refers to the key used to calculate the attention distribution or weights. The second part refers to the value for the source-side context representation. We empirically demonstrate that the separation of the source-side context vector into the key and value significantly improves the performance of an NMT using three different English-to-Japanese translation tasks. In this paper, we propose a neural machine translation (NMT) with a key-value attention mechanism on the source-side encoder. The key-value attention mechanism separates the source-side content vector into two type"
I17-2049,D15-1044,0,0.0448359,"ory cells with a recurrently self-connected linear unit ∗ This work was performed while the first author was affiliated with National Institute of Information and Communication Technology, Kyoto, Japan. 290 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 290–295, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP have been proposed. Attention-based neural networks with soft or hard attention over an input have shown successful results in a wide range of tasks including machine translation (Bahdanau et al., 2015), sentence summarization (Rush et al., 2015), and image captioning (Xu et al., 2015). These attention-based networks use an encoded memory for both as the key and value as described in the Introduction to calculate the output. In contrast to the dual use of a single memory vector, Miller et al. (2016) have proposed keyvalue memory networks with key- and valuememory vectors to solve question-answering tasks, which use a generalized approach to store content in the memory. The key-memory vectors are used to calculate the attention weights, which address relevant memories with respect to the question, whereas the value-memory vectors are u"
iida-tokunaga-2014-building,W09-2110,0,\N,Missing
iida-tokunaga-2014-building,W07-1522,1,\N,Missing
iida-tokunaga-2014-building,D10-1023,0,\N,Missing
iida-tokunaga-2014-building,P06-1032,0,\N,Missing
iida-tokunaga-2014-building,J95-2003,0,\N,Missing
iida-tokunaga-2014-building,J08-1001,0,\N,Missing
iida-tokunaga-2014-building,P11-1094,0,\N,Missing
iida-tokunaga-2014-building,C12-2048,1,\N,Missing
iida-tokunaga-2014-building,maekawa-etal-2010-design,0,\N,Missing
iida-tokunaga-2014-building,W10-4236,0,\N,Missing
J09-4003,C02-1140,1,0.851582,"Missing"
J09-4003,C02-1059,1,0.871134,"Missing"
J09-4003,1997.iwpt-1.16,1,0.629816,"y of Manchester, UK Hozumi Tanaka—or Tanaka-sensei as he was fondly known to his colleagues and students in Japanese—passed away at the age of 67 in the early morning of 27 July 2009. He is survived by his wife Reiko and two sons. Tanaka-sensei’s primary contributions to natural language processing (NLP) are in parsing and semantic analysis. In parsing, he extended the GLR parsing algorithm to incorporate probabilities, multiple connection tables, and simultaneously carry out morphological and syntactic analysis for non-segmenting languages such as Japanese (Tanaka, Tokunaga, and Aizawa 1993; Inui et al. 1997; Shirai et al. 2000). His research on semantic analysis covered a broad spectrum, encompassing word sense disambiguation (Fujii et al. 1998), spoken language understanding for virtual agent systems (Shinyama, Tokunaga, and Tanaka 2000), lexical semantic approaches to query expansion in information retrieval (Mandala, Tokunaga, and Tanaka 2000), and metaphor processing (Iwayama, Tokunaga, and Tanaka 1990). He also carried out research on machine translation (Tanaka, Isahara, and Yasuhara 1983; Tanaka 1999b; Baldwin and Tanaka 2000), computer-assisted language learning (Bilac, Baldwin, and Tana"
J09-4003,1999.mtsummit-1.1,0,0.0383844,"is for non-segmenting languages such as Japanese (Tanaka, Tokunaga, and Aizawa 1993; Inui et al. 1997; Shirai et al. 2000). His research on semantic analysis covered a broad spectrum, encompassing word sense disambiguation (Fujii et al. 1998), spoken language understanding for virtual agent systems (Shinyama, Tokunaga, and Tanaka 2000), lexical semantic approaches to query expansion in information retrieval (Mandala, Tokunaga, and Tanaka 2000), and metaphor processing (Iwayama, Tokunaga, and Tanaka 1990). He also carried out research on machine translation (Tanaka, Isahara, and Yasuhara 1983; Tanaka 1999b; Baldwin and Tanaka 2000), computer-assisted language learning (Bilac, Baldwin, and Tanaka 2002), speech recognition (Itou, Hayamizu, and Tanaka 1992; Li, Tanaka, and Tokunaga 1995), dialogue systems (Akiba and Tanaka 1994; Funakoshi, Tokunaga, and Tanaka 2002), and automatic music generation (Suzuki, Tokunaga, and Tanaka 1999). He was the author or editor of a number of popular introductory texts on NLP in Japanese (Tanaka 1989, 1999a). Tanaka-sensei was the technical lead on the Japanese government-funded CICC Machine Translation Project (1987–1995) between East and South-East Asian langua"
J09-4003,1993.iwpt-1.10,0,0.382482,"Missing"
J97-4006,H92-1022,0,0.0279469,"tion of which components of an overall system statistical approaches should be introduced into. In Chapter 8, &quot;Recovering from parser failures: A hybrid statistical and symbolic approach,&quot; Ros4 and Waibel propose a method of using parse fragments to recover from a parse error. Their system tries to construct a featurestructure-style intermediate representation from the set of parse fragments, relying on collocational preferences for slot types and fillers. In Chapter 7, &quot;Exploring the nature of transformation-based learning,&quot; Ramshaw and Marcus discuss the advantages of a Brill-like approach (Brill 1992) over pure stochastic (hidden Markov model) and symbolic (decision tree) methods, using both qualitative and quantitative evaluation. They claim that Brill&apos;s approach suffers less from over-training than is the case for HMMs or decision trees because the rules extracted in Brill&apos;s approach are more loosely interdependent. This is due to HMMs being overly monolithic and the structure of decision trees being too rigidly hierarchical. In their evaluation, Ramshaw and Marcus analyzed the dependencies between learned rules and found that more general rules tend to apply at an early stage, but that"
J97-4006,P93-1023,0,0.0341366,"e frequency and mutual information. The results obtained from the different types of statistical filters indicate that, surprisingly, simple frequency returns the best result. Daille attributes this to the use of linguistic filters prior to statistical calculation. Chapter 4, &quot;Do we need linguistics when we have statistics? A comparative analysis of the contributions of linguistic cues to a statistical word grouping system,&quot; discusses a similar acquisition task based on linguistic knowledge and focuses on grouping adjectives with regard to their patterns of usage. Following his previous work (Hatzivassiloglou and McKeown 1993), Hatzivassiloglou thoroughly investigates the effectiveness of various kinds of linguistic knowledge, and also the cost of acquiring linguistic knowledge. The cost of knowledge acquisition represents a unique viewpoint in this type of research and constitutes yet another important facet of the overall balancing act. There is no doubt that the success of statistical techniques in speech recognition has had a considerable influence on the language processing community. In Chapter 6, &quot;Combining linguistics with statistical methods in automatic speech understanding,&quot; Price discusses the evolution"
J98-4002,P91-1034,0,0.0172838,"Missing"
J98-4002,P94-1020,0,0.00915099,"ough experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disa"
J98-4002,J94-4003,0,0.0096378,"d sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such a"
J98-4002,P96-1042,0,0.41794,"s. During this phase, a human expert supervises samples, that is, provides the correct interpretation for the verbs appearing in the samples. Thereafter, samples are simply incorporated into the database without any computational overhead (as would be associated with globally reestimating parameters in statistics-based systems), meaning that the system can be trained on the remaining examples (the ""residue"") for the next iteration. Iterating between these two 1 Note that these problems are associated with corpus-based approaches in general, and have been identified by a number of researchers (Engelson and Dagan 1996; Lewis and Gale 1994; Uramoto 1994a; Yarowsky 1995). 574 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling sampling ~WSD~sD outu t ~ ~ : ( ~ ~ Figure 1 Flow of control of the example sampling system. phases, the system progressively enhances the database. Note that the selective sampiing procedure gives us an optimally informative database of a given size irrespective of the stage at which processing is terminated. Several researchers have proposed this type of approach for NLP applications. Engelson and Dagan (1996) proposed a committee-based sampling method, which is currently applied to"
J98-4002,C96-1012,1,0.808889,"to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is"
J98-4002,C94-2122,0,0.00946794,"tion because the verb in the input is disambiguated by superimposing the sense of the verb appearing in the example of highest similarity.3 The similarity between an input and an example is estimated based on the similarity between case •lers marked with the same case. Furthermore, since the restrictions imposed by the case fillers in choosing the verb sense are not equally selective, Fujii et al. (1996) proposed a weighted case contribution to disambiguation (CCD) of the verb senses. This CCD factor is taken into account 2 Note that unlike the automatic acquisition of word sense definitions (Fukumoto and Tsujii 1994; Pustejovsky and Boguraev 1993; Utsuro 1996; Zernik 1989), the task of the system is to identify the best matched category with a given input, from candidates. 3 In this paper, we use ""example-based systems"" to refer to systems based on nearest neighbor resolution. predefined 576 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling © nominative accusative Figure 3 The semantic ranges of the nominative and accusative for the verb toru. nq-mc~nc Cs~,c~ nc3-rnllc3 v &,;3 -- v ~s3,c2 ~s3,C3- -- V (S3) J database -- (?) G,c2 (s~)l Figure 4 An input and the database. w h e n c o m p u t i n g the s"
J98-4002,P92-1032,0,0.0186836,"Missing"
J98-4002,P91-1019,0,0.0194231,"approaches to word sense disambiguation should be further investigated, this experimental result gives us good motivation to explore example-based verb sense disambiguation approaches, i.e., to introduce the notion of selective sampling into them. 2.4 Enhancement of Verb Sense Disambiguation Let us discuss how further enhancements to our example-based verb sense disambiguation system could be made. First, since inputs are simple sentences, information for word sense disambiguation is inadequate in some cases. External information such as the discourse or domain dependency of each word sense (Guthrie et al. 1991; Nasukawa 1993; Yarowsky 1995) is expected to lead to system improvement. Second, some idiomatic expressions represent highly restricted collocations, and overgeneralizing them semantically through the use of a thesaurus can cause further errors. Possible solutions would include one proposed by Uramoto, in which idiomatic expressions are described separately in the database so that the system can control their overgeneralization (Uramoto 1994b). Third, a number of existing NLP tools such as JUMAN (a morphological analyzer) (Matsumoto et al. 1993) and QJP (a morphological and syntactic analyze"
J98-4002,P90-1034,0,0.0202315,"ion have been proposed, for example, by Yarowsky (1992). 578 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling IIII I I I kare kanojo (he) (she) otoko joshu hisho kane heya kippu uma (man) (assistant) (secretary)(money) (room) (ticket) (horse) Figure 5 A fragment of the Bunruigoihyo thesaurus. statistical factors, although statistical factors are calculated in terms of the predicate argument structure in which each noun appears. Predicate argument structures, which consist of complements (case filler nouns and case markers) and verbs, have also been used in the task of noun classification (Hindle 1990). This can be expressed by Equation (3), where ff is the vector for the noun in question, and items ti represent the statistics for predicate argument structures including n. ff = (h, t2,..., ti . . . . ) (3) In regard to ti, we used the notion of TF. IDF (Salton and McGill 1983). TF (term frequency) gives each context (a case marker/verb pair) importance proportional to the number of times it occurs with a given noun. The rationale behind IDF (inverse document frequency) is that contexts that rarely occur over collections of nouns are valuable, and that therefore the IDF of a context is inver"
J98-4002,C92-2101,0,0.0102778,"Missing"
J98-4002,C96-2104,0,0.0251221,"sukawa 1993; Yarowsky 1995) is expected to lead to system improvement. Second, some idiomatic expressions represent highly restricted collocations, and overgeneralizing them semantically through the use of a thesaurus can cause further errors. Possible solutions would include one proposed by Uramoto, in which idiomatic expressions are described separately in the database so that the system can control their overgeneralization (Uramoto 1994b). Third, a number of existing NLP tools such as JUMAN (a morphological analyzer) (Matsumoto et al. 1993) and QJP (a morphological and syntactic analyzer) (Kameda 1996) could broaden the coverage of our system, as inputs are currently limited to simple, morphologically analyzed sentences. Finally, it should be noted that in Japanese, case markers can be omitted or topicalized (for example, marked with postposition wa), an issue which our framework does not currently consider. 3. Example Sampling Algorithm 3.1 Overview Let us look again at Figure 1 in Section 1. In this figure, ""WSD outputs"" refers to a corpus in which each sentence is assigned an expected verb interpretation during the WSD phase. In the training phase, the system stores supervised samples (w"
J98-4002,H93-1051,0,0.0535285,"Missing"
J98-4002,W96-0208,0,0.0139467,"ly one verb sense remains. W h e n more than one verb sense is selected for any given m e t h o d (or none of them remains, for the rule-based method), the system simply selects the verb sense that appears most frequently in the database, s In the experiment, we conducted sixfold cross-validation, that is, we divided the training/test data into six equal parts, and conducted six trials in which a different 7 A number of experimental results have shown the effectiveness of the Naive-Bayes method for word sense disambiguation (Gale, Church, and Yarowsky 1993; Leacock, Towell, and Voorhees 1993; Mooney 1996; Ng 1997; Pedersen, Bruce, and Wiebe 1997). 8 One may argue that this goes against the basis of the rule-based method, in that, given a proper threshold value for the association degree, the system could improve on accuracy (potentially sacrificing coverage), and that the trade-off between coverage and accuracy is therefore a more appropriate evaluation criterion. However, our trials on the rule-based method with different threshold values did not show significant correlation between the improvement of accuracy and the degeneration of coverage. 581 Computational Linguistics Volume 24, Number"
J98-4002,1993.tmi-1.15,0,0.0285586,"ense disambiguation should be further investigated, this experimental result gives us good motivation to explore example-based verb sense disambiguation approaches, i.e., to introduce the notion of selective sampling into them. 2.4 Enhancement of Verb Sense Disambiguation Let us discuss how further enhancements to our example-based verb sense disambiguation system could be made. First, since inputs are simple sentences, information for word sense disambiguation is inadequate in some cases. External information such as the discourse or domain dependency of each word sense (Guthrie et al. 1991; Nasukawa 1993; Yarowsky 1995) is expected to lead to system improvement. Second, some idiomatic expressions represent highly restricted collocations, and overgeneralizing them semantically through the use of a thesaurus can cause further errors. Possible solutions would include one proposed by Uramoto, in which idiomatic expressions are described separately in the database so that the system can control their overgeneralization (Uramoto 1994b). Third, a number of existing NLP tools such as JUMAN (a morphological analyzer) (Matsumoto et al. 1993) and QJP (a morphological and syntactic analyzer) (Kameda 1996"
J98-4002,W97-0323,0,0.0563667,"ense remains. W h e n more than one verb sense is selected for any given m e t h o d (or none of them remains, for the rule-based method), the system simply selects the verb sense that appears most frequently in the database, s In the experiment, we conducted sixfold cross-validation, that is, we divided the training/test data into six equal parts, and conducted six trials in which a different 7 A number of experimental results have shown the effectiveness of the Naive-Bayes method for word sense disambiguation (Gale, Church, and Yarowsky 1993; Leacock, Towell, and Voorhees 1993; Mooney 1996; Ng 1997; Pedersen, Bruce, and Wiebe 1997). 8 One may argue that this goes against the basis of the rule-based method, in that, given a proper threshold value for the association degree, the system could improve on accuracy (potentially sacrificing coverage), and that the trade-off between coverage and accuracy is therefore a more appropriate evaluation criterion. However, our trials on the rule-based method with different threshold values did not show significant correlation between the improvement of accuracy and the degeneration of coverage. 581 Computational Linguistics Volume 24, Number 4 Table 2"
J98-4002,P96-1006,0,0.0285131,"verhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with sys"
J98-4002,C94-1049,0,0.00878414,"h, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other a"
J98-4002,A97-1056,0,0.0176876,"Missing"
J98-4002,E95-1016,0,0.0189486,"allow only those nouns dominated by the given class in the thesaurus structure as verb complements. In order to identify appropriate thesaurus classes, we used the association measure proposed by Resnik (1993), which computes the information-theoretic association degree between case fillers and thesaurus classes, for each verb sense (Equation (7)). 6 P(rls, c) A(s,c,r) = P(rls, c) • log p(rlc) (7) 6 Note that previous research has applied this technique to tasks other than verb sense disambiguation, such as syntactic disambiguation (Resnik 1993) and disambiguation of case filler noun senses (Ribas 1995). 580 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling Here, A(s, c, r) is the association degree between verb sense s and class r (selectional restriction candidate) with respect to case c. P(rls, c) is the conditional probability that a case filler example associated with case c of sense s is d o m i n a t e d b y class r in the thesaurus. P(rlc ) is the conditional probability that a case filler example for case c (disregarding verb sense) is d o m i n a t e d b y class r. Each probability is estimated based on training data. We used the semantic classes defined in the Bunruigoihyo thes"
J98-4002,C94-2114,0,0.257712,"erformance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other approaches, and motivated * De"
J98-4002,C96-2163,0,0.0117674,"perimposing the sense of the verb appearing in the example of highest similarity.3 The similarity between an input and an example is estimated based on the similarity between case •lers marked with the same case. Furthermore, since the restrictions imposed by the case fillers in choosing the verb sense are not equally selective, Fujii et al. (1996) proposed a weighted case contribution to disambiguation (CCD) of the verb senses. This CCD factor is taken into account 2 Note that unlike the automatic acquisition of word sense definitions (Fukumoto and Tsujii 1994; Pustejovsky and Boguraev 1993; Utsuro 1996; Zernik 1989), the task of the system is to identify the best matched category with a given input, from candidates. 3 In this paper, we use ""example-based systems"" to refer to systems based on nearest neighbor resolution. predefined 576 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling © nominative accusative Figure 3 The semantic ranges of the nominative and accusative for the verb toru. nq-mc~nc Cs~,c~ nc3-rnllc3 v &,;3 -- v ~s3,c2 ~s3,C3- -- V (S3) J database -- (?) G,c2 (s~)l Figure 4 An input and the database. w h e n c o m p u t i n g the score for each sense of the verb in question."
J98-4002,C94-2169,0,0.127069,"E TU(x = s) (13) sEK 3.4 Enhancement of Computation In this section, we discuss how to enhance the computation associated with our example sampling algorithm. First, we note that computation of TU(x = s) in Equation (11) above becomes time consuming because the system is required to search the whole set of unsupervised examples for examples whose interpretation certainty will increase after x is used for training. To avoid this problem, we could apply a method used in efficient database search techniques, by which the system can search for neighbor examples of x with optimal time complexity (Utsuro et al. 1994). However, in this section, we will explain another efficient algorithm to identify neighbors of x, in which neighbors of case fillers are considered to be given directly by the thesaurus structure. 12 The basic idea is the following: the system searches for neighbors of each case filler of x instead of x as a whole, and merges them as a set of neighbors of x. Note that by dividing examples along the lines of each case filler, we can retrieve neighbors based on the structure of the Bunruigoihyo thesaurus (instead of the conceptual semantic space as in Figure 7). Let Nx=s,c be a subset of unsup"
J98-4002,C92-2070,0,0.0363747,"pace model"" (VSM) (Frakes and Baeza-Yates 1992; Leacock, Towell, and Voorhees 1993; Salton and McGill 1983; Sch/itze 1992), which has a long history of application in information retrieval (IR) and text categorization (TC) tasks. In the case of IR/TC, VSM is used to compute the similarity between documents, which is represented by a vector comprising statistical factors of content words in a document. Similarly, in our case, each noun is represented by a vector comprising 4 Different types of application of hand-crafted thesauri to word sense disambiguation have been proposed, for example, by Yarowsky (1992). 578 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling IIII I I I kare kanojo (he) (she) otoko joshu hisho kane heya kippu uma (man) (assistant) (secretary)(money) (room) (ticket) (horse) Figure 5 A fragment of the Bunruigoihyo thesaurus. statistical factors, although statistical factors are calculated in terms of the predicate argument structure in which each noun appears. Predicate argument structures, which consist of complements (case filler nouns and case markers) and verbs, have also been used in the task of noun classification (Hindle 1990). This can be expressed by Equation (3), wh"
J98-4002,P95-1026,0,0.654354,"he system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other approaches, and motivated * Department of Libra"
J98-4002,C92-2107,0,\N,Missing
kaplan-etal-2010-annotation,kingsbury-palmer-2002-treebank,0,\N,Missing
kaplan-etal-2010-annotation,miltsakaki-etal-2004-penn,0,\N,Missing
kaplan-etal-2010-annotation,J93-2004,0,\N,Missing
kaplan-etal-2010-annotation,bird-etal-2000-atlas,0,\N,Missing
kaplan-etal-2010-annotation,W07-1522,1,\N,Missing
kaplan-etal-2010-annotation,W07-1523,0,\N,Missing
kaplan-etal-2010-annotation,W03-2120,0,\N,Missing
L16-1697,P11-3015,0,0.0204331,"possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the possibility of exploring these phenomena “in the wild”, i.e. big data-based analysis. Unfortunately, however, to our knowledge there is no richly annotated resource containing this kind of information. The goal of CiteNe"
L16-1697,W04-3202,0,0.104107,"on on the uses of the tool (Section 6.). 2. k∈K Our goal is to find XL that maximizes the utility U : arg maxXL U (XL ). 3. x∈X where L is a loss function, e.g. squared error loss (f (x) − y)2 . The task of active learning is then to find a set of points While in practice it may be sufficient to optimise a given model, and therefore mould a corpus using AL to fit it, this loses sight of the bigger aim, that of studying phenomena for understanding how something works; further, it has been shown to even be detrimental in some cases to apply data acquired for one model using AL to another model (Baldridge, 2004; Rubens and Sugiyama, 2006; Sugiyama and Rubens, 2008). 2 The tool is available for download at https://github. com/move-tool. Related Work We next introduce works related to our proposed modelfree AL method. 3.1. Method Formulation Let us formulate our proposal in the context of existing AL approaches. The majority of AL methods are designed for supervised learning settings, where given the training data {(x1 , y1 ), . . . , (xn , yn )}, the task is to learn a function f : X 7→ Y that accurately predicts the output values Y of the input X. For AL settings, it is assumed that we are able to s"
L16-1697,W09-1403,0,0.020287,"Missing"
L16-1697,W09-3611,1,0.806146,"d network so that citation-content based summarisation may be possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the possibility of exploring these phenomena “in the wild”, i.e. big data-based analysis. Unfortunately, however, to our knowledge there is no richly annotated resource c"
L16-1697,C08-1087,0,0.0137431,"papers, in an interconnected network so that citation-content based summarisation may be possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the possibility of exploring these phenomena “in the wild”, i.e. big data-based analysis. Unfortunately, however, to our knowledge there is no richl"
L16-1697,J02-4001,0,0.0605987,"with citation function, within research papers, in an interconnected network so that citation-content based summarisation may be possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the possibility of exploring these phenomena “in the wild”, i.e. big data-based analysis. Unfortunat"
L16-1697,P11-1080,0,0.0248296,"ed to models and corpus construction, and introduced our novel method for model-free AL to build phenomena-driven corpora, including the development of a tool, MOVE, and shown its use on a real world scenario of a citation-network based corpus we are developing, CiteNet. The case study introduced in Section 5. is typical of a wide range of phenomena in computational linguistics where entities are linked in some form and can be thus represented as a network. For instance, in the cross-document coreference task, systems must identify noun-phrases which corefer across document boundaries, e.g., (Singh et al., 2011). Our proposed method could be used to find an initial training and test set of such documents, based on obvious named entity (NE) coreferences (e.g., those which are long enough to be guaranteed to be unique if they are found in identical forms across documents). The point is not to find all coreferences in advance or we would not need to make the tool, but to insure enough variance in the documents to produce a subnetwork representing the coreferences adequately. These coreference links represent the equivalent of the citation links from the case study, but in the crossdocument NE task, an a"
L16-1697,W10-4121,0,0.0316975,"s will fail if they are fed poor quality data during training, i.e. “garbage in, garbage out”. On the other end, corpus construction (CC) typically faces the challenge of maximising the usefulness of annotation while keeping the costs within the allotted budget. Corpus annotation is usually undergone in one of three ways: (1) purely automatic, (2) automated annotation followed by manual correction, and (3) purely manual annotation (McEnery et al., 1995). As active learning (AL) for CC identifies the most “informative” data for annotation (Olsson, 2009; Tomanek and Olsson, 2009; Settles, 2009; Song and Yao, 2010), at first glance it seems like a perfect fit when by necessity you cannot annotate all documents. For example, consider Figure 1, which shows random input vs. input selected using AL to consider the network structure. However, this brings us to what we define as the chickenand-egg corpus and model conundrum, which refers to how AL often happens in a closed-loop process, the underlying model or models directly influencing which data is selected for annotation, which improves the model’s accuracy, and so on. For exploratory CC in which creators may be investigating hitherto under/unexplored phe"
L16-1697,W06-1613,1,0.640383,"n our case, we wanted to create an annotated corpus of novelty claims and citation spans, replete with citation function, within research papers, in an interconnected network so that citation-content based summarisation may be possible. Further studies of citation and research paper phenomena may be possible as well. There is a wealth of research from over the decades focusing on research paper-based phenomena, e.g. citations and their analysis (Garfield, 1955; Giles et al., 1998; Kessler, 1963; Small, 1973; White, 2004; Hirsch, 2005), novelty claims and argumentative zoning (Weinstock, 1971; Teufel et al., 2006), paper and domain summarisation (Garfield et al., 1964; Nanba et al., 2000; Radev et al., 2002; Elkiss et al., 2008; Qazvinian and Radev, 2008; Kaplan et al., 2009), sentiment analysis (Nakov et al., 2004; Athar, 2011), and so on. Until recently the study of many of these phenomena has been carried out in an ad-hoc fashion, selecting papers from various domains manually (Spiegel-R¨osing, 1977; Weinstock, 1971; Moravcsik and Murugesa˜n, 1975). The advancement of computers and processing power has 4406 enabled researchers to cull data from an ever increasing sea of information; this opens the p"
L16-1697,W09-1906,0,0.109871,"mproving models), even the best algorithms will fail if they are fed poor quality data during training, i.e. “garbage in, garbage out”. On the other end, corpus construction (CC) typically faces the challenge of maximising the usefulness of annotation while keeping the costs within the allotted budget. Corpus annotation is usually undergone in one of three ways: (1) purely automatic, (2) automated annotation followed by manual correction, and (3) purely manual annotation (McEnery et al., 1995). As active learning (AL) for CC identifies the most “informative” data for annotation (Olsson, 2009; Tomanek and Olsson, 2009; Settles, 2009; Song and Yao, 2010), at first glance it seems like a perfect fit when by necessity you cannot annotate all documents. For example, consider Figure 1, which shows random input vs. input selected using AL to consider the network structure. However, this brings us to what we define as the chickenand-egg corpus and model conundrum, which refers to how AL often happens in a closed-loop process, the underlying model or models directly influencing which data is selected for annotation, which improves the model’s accuracy, and so on. For exploratory CC in which creators may be investi"
L16-1697,D07-1051,0,0.114612,"ning on Networks There is extensive work on utilising network structure for improving AL, e.g., using additional information provided by edges (Bilgic and Getoor, 2009), network topology (Hanneke and Xing, 2009) favouring nodes at centre of clusters (Macskassy, 2009), high connectivity (Shi and Zhao, 2010), and social network metrics (Macskassy, 2009; Kuwadekar, 2010; Ji, 2012). However, these works are model-centred, which as explained in the introduction, is not the case for our method, which is trying to liberate the corpus creator from needing a model. 3.6. (d) +Betweenness Corpus Utility Tomanek and Wermter (2007) state that AL-based corpora should be reusable for training with modified or improved classifiers to have true utility. In part, this is because it can be difficult to predict the best suited algorithm for a task, so swapping learning algorithms during experimentation may be needed (Busser and Morante, 2005). Not knowing which model will be applied to the constructed corpus may seem minor, but it has been shown both empirically (Baldridge, 2004) and methodologically (Rubens and Sugiyama, 2006; Sugiyama and Rubens, 2008) that samples obtained for one model are often detrimental to another, so"
L18-1434,W17-5526,0,0.0281865,"elds of a real estate database. We further analysed the utterances that were not assigned a database field and found that the majority of those utter2. Related Work There have been several attempts to annotate semantic information in database search dialogues. He and Young (2005) annotated the domain-specific lexical classes (e.g. “city name”, “airport name”) and concepts (e.g. ARRIVE, FROMLOC) in the ATIS corpus. The annotated tags were derived from the ATIS database schema. They considered only information corresponding to the database fields. The implicit conditions are out of their scope. Asri et al. (2017) annotated a frame structure to each utterance in dialogues for searching package tours. A frame consists of a dialogue act and a set of slot-value pairs representing the content of the dialogue act. The most slots correspond to the database field, but some do not. These non-database-field slots mainly describe relations to other frames. In this respect, they are meta-level information than the same level information as the filed-slot information; therefore they are different from the implicit condition. Dinarelli et al. (2009) annotated a semantic frame of FrameNet (Baker, 2012) in task-orien"
L18-1434,H94-1010,0,0.795535,"pends its backend application. When being used as a natural language interface of a database system, the dialogue system should be able to extract pieces of information corresponding to the record fields of the database for constructing a query. There have been several attempts to extract this type of information from user utterances in database search dialogues. For instance, several studies (Raymond and Riccardi, 2007; Mesnil et al., 2015; Liu and Lane, 2016) tried to extract values for the database field defined in the ATIS (The Air Travel Information System) corpus (Hemphill et al., 1990; Dahl et al., 1994) from the user utterances. The ATIS corpus includes a set of dialogues between users and an air travel system that were collected through the Wizard-of-Oz method. The tags corresponding to the backend database fields (e.g. departure city, arrival date, etc.) were annotated to the expressions in the user utterances. However, the utterances in real dialogues include information that does not always directly correspond to a database field but provides useful information for constructing database queries. It will be more efficient and natural if the dialogue system can utilise this type of informa"
L18-1434,W09-0505,0,0.0163001,"to the database fields. The implicit conditions are out of their scope. Asri et al. (2017) annotated a frame structure to each utterance in dialogues for searching package tours. A frame consists of a dialogue act and a set of slot-value pairs representing the content of the dialogue act. The most slots correspond to the database field, but some do not. These non-database-field slots mainly describe relations to other frames. In this respect, they are meta-level information than the same level information as the filed-slot information; therefore they are different from the implicit condition. Dinarelli et al. (2009) annotated a semantic frame of FrameNet (Baker, 2012) in task-oriented dialogues. Since the FrameNet frames are designed for general purpose, the annotated frame are not necessarily useful for the database search task. In contrast, we annotated the information that is relevant for constructing database queries regardless whether it corresponds to the database field or not. The above three studies annotated only predefined tags in the dialogues, i.e. they did not explore the information that does not fit into the predefined tags even though it is useful for the task. In contrary, Mitsuda et al."
L18-1434,H90-1021,0,0.385404,"er utterances highly depends its backend application. When being used as a natural language interface of a database system, the dialogue system should be able to extract pieces of information corresponding to the record fields of the database for constructing a query. There have been several attempts to extract this type of information from user utterances in database search dialogues. For instance, several studies (Raymond and Riccardi, 2007; Mesnil et al., 2015; Liu and Lane, 2016) tried to extract values for the database field defined in the ATIS (The Air Travel Information System) corpus (Hemphill et al., 1990; Dahl et al., 1994) from the user utterances. The ATIS corpus includes a set of dialogues between users and an air travel system that were collected through the Wizard-of-Oz method. The tags corresponding to the backend database fields (e.g. departure city, arrival date, etc.) were annotated to the expressions in the user utterances. However, the utterances in real dialogues include information that does not always directly correspond to a database field but provides useful information for constructing database queries. It will be more efficient and natural if the dialogue system can utilise"
L18-1434,D16-1011,0,0.0568581,"Missing"
L18-1434,W16-3603,0,0.226935,"us annotation 1. Introduction ances included the implicit conditions. The information that a dialogue system should extract from user utterances highly depends its backend application. When being used as a natural language interface of a database system, the dialogue system should be able to extract pieces of information corresponding to the record fields of the database for constructing a query. There have been several attempts to extract this type of information from user utterances in database search dialogues. For instance, several studies (Raymond and Riccardi, 2007; Mesnil et al., 2015; Liu and Lane, 2016) tried to extract values for the database field defined in the ATIS (The Air Travel Information System) corpus (Hemphill et al., 1990; Dahl et al., 1994) from the user utterances. The ATIS corpus includes a set of dialogues between users and an air travel system that were collected through the Wizard-of-Oz method. The tags corresponding to the backend database fields (e.g. departure city, arrival date, etc.) were annotated to the expressions in the user utterances. However, the utterances in real dialogues include information that does not always directly correspond to a database field but pro"
N15-1031,N12-1058,0,0.0590805,"Missing"
N15-1031,W12-1633,1,0.512713,"e of RR, ignoring pronouns or deixis. In this paper, we opted to use the model presented in Kennington et al. (2013), the simple incremental update model (SIUM). It has been tested extensively against data from a puzzle-playing human/computer interaction domain (the PENTO data, (Kousidis et al., 2013)); it can incorporate multi-modal information, works in real-time, and can resolve definite, exophoric, and deictic references in a single framework, all of which makes it a potential candidate for working in an interactive, multi-modal dialogue system. The model is similar to the one proposed in Funakoshi et al. (2012), which could resolve descriptions, anaphora, and deixis in a unified manner, but that model does not work incrementally.1 The main contributions of this paper are the more thorough exposition of the model (in Section 3) and its application and evaluation on much less constrained, more interactive (and hence realistic) data than what it has previously been tested on (Section 4). Moreover, the data set used here is also from a typologically very different language (Japanese) than what the model has been previously tested on (German), and so the robustness of the model against these differences"
N15-1031,J95-3003,0,0.412762,"Missing"
N15-1031,P10-1128,1,0.618145,"rence resolution, which has been tested in a simpler setup, on more natural data coming from a corpus of human/human interactions. The model is incremental in that it does not wait until the end of an utterance to process, rather it updates its interpretation at each word increment. The model can also incorporate other modalities, such as gaze or pointing cues (deixis) incrementally. We also model the saliency of the context, and show that the model can easily take such contextual information into account. The model improves over previous work on reference resolution applied to the same data (Iida et al., 2010; Iida et al., 2011). The paper is structured as follows: in the following section we discuss related work on incremental resolution of referring expressions. We explain the model that we use in Section 3 and the data we apply it to in Section 4. We then describe the experiments and the results and provide a discussion. 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015."
N15-1031,I11-1010,1,0.539537,"hich has been tested in a simpler setup, on more natural data coming from a corpus of human/human interactions. The model is incremental in that it does not wait until the end of an utterance to process, rather it updates its interpretation at each word increment. The model can also incorporate other modalities, such as gaze or pointing cues (deixis) incrementally. We also model the saliency of the context, and show that the model can easily take such contextual information into account. The model improves over previous work on reference resolution applied to the same data (Iida et al., 2010; Iida et al., 2011). The paper is structured as follows: in the following section we discuss related work on incremental resolution of referring expressions. We explain the model that we use in Section 3 and the data we apply it to in Section 4. We then describe the experiments and the results and provide a discussion. 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for"
N15-1031,W13-4030,1,0.881205,"(Schlangen et al., 2009), a model that used Markov Logic Networks to resolve objects on a screen (Kennington and Schlangen, 2013), a model of RR and incremental feedback (Traum et al., 2012), and an approach that used a semantic representation to refer to objects (Peldszus et al., 2012; 273 Kennington et al., 2014). However, the approaches reported there did not incorporate multi-modal information, were too slow to work in real-time, were evaluated on constrained data, or only focused on a specific type of RR, ignoring pronouns or deixis. In this paper, we opted to use the model presented in Kennington et al. (2013), the simple incremental update model (SIUM). It has been tested extensively against data from a puzzle-playing human/computer interaction domain (the PENTO data, (Kousidis et al., 2013)); it can incorporate multi-modal information, works in real-time, and can resolve definite, exophoric, and deictic references in a single framework, all of which makes it a potential candidate for working in an interactive, multi-modal dialogue system. The model is similar to the one proposed in Funakoshi et al. (2012), which could resolve descriptions, anaphora, and deixis in a unified manner, but that model"
N15-1031,C14-1170,1,0.724457,"s deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014). Incorporating pointing (deictic) gestures is also potentially useful in situated RR; as for example Matuszek et al. (2014) have shown in work on resolving objects processed by computer vision techniques. Chen and Eugenio (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures. However, these approaches were applied in settings in which communication between the two interlocutors was constrained, or the developed systems did not process incrementally. Kehler (2000) presented approach that focused more on interaction in a map task, though the model was not incremental, nor did grounding occur between language and world, as we do here. Incremental RR has also"
N15-1031,W14-4314,0,0.0217786,"f the corpus we used. 3 The Simple Incremental Update Model Following Kennington et al. (2013) and Kennington et al. (2014), we model the task at hand as one of recovering I, the intention of the speaker making the RE, where I ranges over the possible alternatives (the objects in the domain). This recovery proceeds incrementally (word by word), for RE of arbitrary length. That is, if U denotes the current word, we are interested in P (I|U ), the current hypothesis about 1 It can be argued that any non-incremental model could be made into an incremental one by applying that model at each word (Khouzaimi et al., 2014), but we would argue that more modeling effort is required in order for the model to work in an interactive dialogue system, see (Schlangen and Skantze, 2009; Aist et al., 2007; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991). the intended referent, given the observed word. We assume the presence of an unobserved, latent variable R, which models properties of the candidate objects such as colour or shape; explained further below), and so the computation formally is: P (I|U ) = X P (I, U, R) r∈R P (U ) (1) Which, after making some independence assumptions, can be factored into: P (I"
N15-1031,W13-4048,1,0.882779,"definite descriptions makes up a large part of human communication (Poesio and Vieira, 1997). In task-oriented situations, these references are often to entities that are visible in the shared environment. This kind of reference has attracted attention in recent computational research, but the kinds of interactions studied are often fairly restricted in controlled lab situations (Tanenhaus and Spivey-Knowlton, 1995) or simulated human/computer interactions, (Schlangen David Schlangen Bielefeld University Universit¨atsstraße 25 Bielefeld Germany david.schlangen@ uni-bielefeld.de et al., 2009; Kousidis et al., 2013; Chai et al., 2014). In such task-oriented, co-located settings, interlocutors can make use of extra-linguistic cues such as gaze or pointing gestures. Furthermore, listeners resolve references as they unfold, often identifying the referred entity before the end of the reference (Tanenhaus and Spivey-Knowlton, 1995; Spivey et al., 2002), however research in reference resolution has mostly focused on full, completed referring expressions. In this paper we make a first move towards addressing somewhat more complex domains. We apply a model of reference resolution, which has been tested in a sim"
N15-1031,W13-4010,0,0.019311,"intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014). Incorporating pointing (deictic) gestures is"
N15-1031,W14-4304,0,0.0126023,"ulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car where speakers look at objects in their vicinity (Misu et al., 2014). Incorporating pointing (deictic) gestures is also potentially useful in situated RR; as for example Matuszek et al. (2014) have shown in work on resolving objects processed by computer vision techniques. Chen and Eugenio (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures. However, these approaches were applied in settings in which communication between the two interlocutors was constrained, or the developed systems did not process incrementally. Kehler (2000) presented approach that focused more on interaction in a map task, thou"
N15-1031,E12-1052,1,0.853341,"n in a map task, though the model was not incremental, nor did grounding occur between language and world, as we do here. Incremental RR has also been studied in a number of papers, including a framework for fast incremental interpretation (Schuler et al., 2009), a Bayesian filtering model approach that was sensitive to disfluencies (Schlangen et al., 2009), a model that used Markov Logic Networks to resolve objects on a screen (Kennington and Schlangen, 2013), a model of RR and incremental feedback (Traum et al., 2012), and an approach that used a semantic representation to refer to objects (Peldszus et al., 2012; 273 Kennington et al., 2014). However, the approaches reported there did not incorporate multi-modal information, were too slow to work in real-time, were evaluated on constrained data, or only focused on a specific type of RR, ignoring pronouns or deixis. In this paper, we opted to use the model presented in Kennington et al. (2013), the simple incremental update model (SIUM). It has been tested extensively against data from a puzzle-playing human/computer interaction domain (the PENTO data, (Kousidis et al., 2013)); it can incorporate multi-modal information, works in real-time, and can re"
N15-1031,D10-1046,0,0.029976,"vide a discussion. 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this"
N15-1031,E09-1081,1,0.837559,"one of recovering I, the intention of the speaker making the RE, where I ranges over the possible alternatives (the objects in the domain). This recovery proceeds incrementally (word by word), for RE of arbitrary length. That is, if U denotes the current word, we are interested in P (I|U ), the current hypothesis about 1 It can be argued that any non-incremental model could be made into an incremental one by applying that model at each word (Khouzaimi et al., 2014), but we would argue that more modeling effort is required in order for the model to work in an interactive dialogue system, see (Schlangen and Skantze, 2009; Aist et al., 2007; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991). the intended referent, given the observed word. We assume the presence of an unobserved, latent variable R, which models properties of the candidate objects such as colour or shape; explained further below), and so the computation formally is: P (I|U ) = X P (I, U, R) r∈R P (U ) (1) Which, after making some independence assumptions, can be factored into: P (I|U ) = X 1 P (I) P (U |R)P (R|I) P (U ) (2) r∈R This is an update model in the usual sense that the posterior P (I|U ) at one step becomes the prior P (I) at"
N15-1031,W09-3905,1,0.960556,"on (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen"
N15-1031,J09-3001,0,0.0280009,"io (2012) looked into reference in multi-modal settings, with focus on co-referential pronouns and pointing gestures. However, these approaches were applied in settings in which communication between the two interlocutors was constrained, or the developed systems did not process incrementally. Kehler (2000) presented approach that focused more on interaction in a map task, though the model was not incremental, nor did grounding occur between language and world, as we do here. Incremental RR has also been studied in a number of papers, including a framework for fast incremental interpretation (Schuler et al., 2009), a Bayesian filtering model approach that was sensitive to disfluencies (Schlangen et al., 2009), a model that used Markov Logic Networks to resolve objects on a screen (Kennington and Schlangen, 2013), a model of RR and incremental feedback (Traum et al., 2012), and an approach that used a semantic representation to refer to objects (Peldszus et al., 2012; 273 Kennington et al., 2014). However, the approaches reported there did not incorporate multi-modal information, were too slow to work in real-time, were evaluated on constrained data, or only focused on a specific type of RR, ignoring pr"
N15-1031,W08-0113,1,0.80871,"lated Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are 272 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static sc"
N15-1031,E09-1085,1,0.886272,"er making the RE, where I ranges over the possible alternatives (the objects in the domain). This recovery proceeds incrementally (word by word), for RE of arbitrary length. That is, if U denotes the current word, we are interested in P (I|U ), the current hypothesis about 1 It can be argued that any non-incremental model could be made into an incremental one by applying that model at each word (Khouzaimi et al., 2014), but we would argue that more modeling effort is required in order for the model to work in an interactive dialogue system, see (Schlangen and Skantze, 2009; Aist et al., 2007; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991). the intended referent, given the observed word. We assume the presence of an unobserved, latent variable R, which models properties of the candidate objects such as colour or shape; explained further below), and so the computation formally is: P (I|U ) = X P (I, U, R) r∈R P (U ) (1) Which, after making some independence assumptions, can be factored into: P (I|U ) = X 1 P (I) P (U |R)P (R|I) P (U ) (2) r∈R This is an update model in the usual sense that the posterior P (I|U ) at one step becomes the prior P (I) at the next. P (R|I) provides the link between the"
N15-1031,tokunaga-etal-2012-rex,1,0.782204,"ter of the ACL, pages 272–282, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics intended to refer to, has been well-studied in various fields such as psychology (Isaacs and Clark, 1987; Tanenhaus and Spivey-Knowlton, 1995), linguistics (Pineda and Garza, 2000), as well as human/human (Iida et al., 2010) and human/machine interaction (Prasov and Chai, 2010; Siebert and Schlangen, 2008; Schlangen et al., 2009). In recent years, multi-modal corpora have emerged which provide RR with important contextual information: collecting dialogue between two humans (Tokunaga et al., 2012; Spanger et al., 2012), between a human and a (simulated) dialogue system (Kousidis et al., 2013; Liu et al., 2013), with gaze, information about the shared environment, and in some cases deixis. It has been shown that incorporating gaze improves RR in a situated setting because speakers need to look at and distinguish from distractors the objects they are describing: this has been shown in a static scene on a computer screen (Prasov and Chai, 2008), in human-human interactive puzzle tasks (Iida et al., 2010; Iida et al., 2011), in web browsing (Hakkani-t¨ur et al., 2014), and in a moving car"
N15-1031,J98-2001,0,\N,Missing
N15-1031,J00-2002,0,\N,Missing
noguchi-etal-2006-new,J93-2004,0,\N,Missing
noguchi-etal-2006-new,A97-1014,0,\N,Missing
noguchi-etal-2006-new,I05-2019,1,\N,Missing
noguchi-etal-2006-new,I05-4002,1,\N,Missing
P06-2052,I05-4002,1,0.925251,"hms using Tree Kernel. We use the following simple algorithm. First we calculate the similarity KC (T1 , T2 ) between a query tree and every tree in the corpus and rank them in descending order of KC . Tree Kernel exploits all subtrees shared by trees. Therefore, it requires considerable amount of time in retrieval because similarity calculation must be performed for every pair of trees. To improve retrieval time, an index table can be used in general. However, indexing by all subtrees is difficult because a tree often includes millions of subtrees. For example, one sentence in Titech Corpus (Noro et al., 2005) with 22 words and 87 nodes includes 8,213,574,246 subtrees. The number of subtrees in a tree with N nodes is bounded above by 2N . 3 Tree Overlapping 3.1 Definition of similarity KC (T1 , T2 ) = max n1 ∈N1 , n2 ∈N2 C(n1 , n2 ) (1) where C(n1 , n2 ) is the number of shared subtrees by two trees rooted at nodes n1 and n2 . 2.2 Algorithm to calculate similarity Collins et al. (Collins, 2001a; Collins, 2001b) proposed an efficient method to calculate Tree Kernel by using C(n1 , n2 ) as follows. • If the productions at n1 and n2 are different C(n1 , n2 ) = 0 • If the productions at n1 and n2 are t"
P06-2052,yoshida-etal-2004-retrieving,1,\N,Missing
P06-2106,francopoulo-etal-2006-lexical,1,0.861995,"Missing"
P06-2106,W03-1905,1,0.81081,"ess ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and allowing for encoding individual lexical entries as instances of the model (Ide et al., 2003; Bertagna et al., 2004b). In the framework of our project, by situating our work in the context of W3C standards and relying on standardized technologies underlying this community, the original RDF schema for ISLE lexical entries has been made compliant to OWL. The whole data model has been formalized in OWL by using Prot´eg´e 3.2 beta and has been extended to cover the morphological component as well (see Figure 2). Prot´eg´e 3.2 beta has been also used as a tool to instantiate the lexical entries of our sample monolingual lexicons, thus ensuring adherence to the model, encoding coherence an"
P06-2106,bel-etal-2000-simple,1,0.877753,"s: to increase the competitive edge of Asian countries, to bring Asian countries to closer to their western counterparts, and to bring more cohesion among Asian countries. To achieve this goal, we have launched a two year project to create a common standard for Asian language resources. The project is comprised of the following four research items. There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES1 , PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003) and LIRICS2 . These continuous efforts has been crystallized as activities in ISO-TC37/SC4 which aims to make an international standard for language resources. 2 (4) Evaluation through application classification Figure 1: Relations among research items 1 Introduction 1 (2) Sample lexicons (1) building a description framework of lexical entries (2) building sample lexicons (3) building an upper-layer ontology (4) evaluating the proposed framework through an application Figure 1 illustrates the relations among these research items. Our main aim is the researc"
P06-2106,C94-1091,1,0.485541,"ical operations, which are special lexical entities allowing the user to define multilin3 MILE is based on the experience derived from existing computational lexicons (e.g. LE-PAROLE, SIMPLE, EuroWordNet, etc.). 828 “CL” stands for a classifier. They always follow cardinal numbers in Japanese. Note that different classifiers are used for different nouns. In the above examples, classifier “hiki” is used to count noun “inu (dog)”, while “satsu” for “hon (book)”. The classifier is determined based on the semantic type of the noun. In the Thai language, classifiers are used in various situations (Sornlertlamvanich et al., 1994). The classifier plays an important role in construction with noun to express ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and al"
P06-2106,zhang-etal-2004-distributional,1,0.766907,"the set of postpositions as values of FunctionType instead of conventional function types such as “subj” and “obj”. It might be an user defined data category or language dependent data category. Furthermore, it is preferable to prepare the mapping between Japanese postpositions and conventional function types. This is interesting because it seems more a terminological difference, but the model can be applied also to Japanese. 4 Building sample lexicons 4.1 Swadesh list and basic lexicon The issue involved in defining a basic lexicon for a given language is more complicated than one may think (Zhang et al., 2004). The naive approach of simply taking the most frequent words in a language is flawed in many ways. First, all frequency counts are corpus-based and hence inherit the bias of corpus sampling. For instance, since it is easier to sample written formal texts, words used predominantly in informal contexts are usually underrepresented. Second, frequency of content words is topic-dependent and may vary from corpus to corpus. Last, and most crucially, frequency of a word does not correlate to its conceptual necessity, 4.2 Aligning multilingual lexical entries Since our goal is to build a multilingual"
P06-2106,bertagna-etal-2004-content,1,0.887142,"e morphological, syntactic and semantic layers. Moreover, an intermediate module allows to define mechanisms of linkage and mapping between the syntactic and semantic layers. Within each layer, a basic linguistic information unit is identified; basic units are separated but still interlinked each other across the different layers. Within each of the MLM layers, different types of lexical object are distinguished : fits with as many Asian languages as possible, and contributing to the ISO-TC37/SC4 activities. As a starting point, we employ an existing description framework, the MILE framework (Bertagna et al., 2004a), to describe several lexical entries of several Asian languages. Through building sample lexicons (research item (2)), we will find problems of the existing framework, and extend it so as to fit with Asian languages. In this extension, we need to be careful in keeping consistency with the existing framework. We start with Chinese, Japanese and Thai as target Asian languages and plan to expand the coverage of languages. The research items (2) and (3) also comprise the similar feedback loop. Through building sample lexicons, we refine an upper-layer ontology. An application built in the resea"
P06-2106,Y06-1043,1,\N,Missing
P10-1128,W98-1119,0,0.100768,"duction The task of identifying reference relations including anaphora and coreferences within texts has received a great deal of attention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it"
P10-1128,J95-2003,0,0.240397,"pieces illustrated on the computer display. As a basis for our reference resolution model, we adopt an existing model for reference resolution. Recently, machine learning-based approaches to reference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) have been developed, particularly focussing on identifying anaphoric relations in texts, and have achieved better performance than hand-crafted rule-based approaches. These models for reference resolution take into account linguistic factors, such as relative salience of candidate antecedents, which have been modeled in Centering Theory (Grosz et al., 1995) by ranking candidate antecedents appearing in the preceding discourse (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). In order to take advantage of existing models, we adopt the rankingbased approach as a basis for our reference resolution model. In conventional ranking-based models, Yang et al. (2003) and Iida et al. (2003) decompose the ranking process into a set of pairwise comparisons of two candidate antecedents. However, recent work by Denis and Baldridge (2008) reports that appropriately constructing a model for ranking all candidates yields improved performance over"
P10-1128,W03-2604,1,0.710746,"we adopt an existing model for reference resolution. Recently, machine learning-based approaches to reference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) have been developed, particularly focussing on identifying anaphoric relations in texts, and have achieved better performance than hand-crafted rule-based approaches. These models for reference resolution take into account linguistic factors, such as relative salience of candidate antecedents, which have been modeled in Centering Theory (Grosz et al., 1995) by ranking candidate antecedents appearing in the preceding discourse (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). In order to take advantage of existing models, we adopt the rankingbased approach as a basis for our reference resolution model. In conventional ranking-based models, Yang et al. (2003) and Iida et al. (2003) decompose the ranking process into a set of pairwise comparisons of two candidate antecedents. However, recent work by Denis and Baldridge (2008) reports that appropriately constructing a model for ranking all candidates yields improved performance over those utilising pairwise ranking. Similarly we adopt a ranking-based model, in which all"
P10-1128,E06-1007,0,0.0355446,"Missing"
P10-1128,P07-1103,0,0.072211,"Missing"
P10-1128,P02-1014,0,0.472713,"referent of the current referring expression. 3.2 Use of extra-linguistic information 3.1 Ranking model to identify referents To investigate the impact of extra-linguistic information on reference resolution, we conduct an em4 pirical evaluation in which a reference resolution model chooses a referent (i.e. a piece) for a given referring expression from the set of pieces illustrated on the computer display. As a basis for our reference resolution model, we adopt an existing model for reference resolution. Recently, machine learning-based approaches to reference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) have been developed, particularly focussing on identifying anaphoric relations in texts, and have achieved better performance than hand-crafted rule-based approaches. These models for reference resolution take into account linguistic factors, such as relative salience of candidate antecedents, which have been modeled in Centering Theory (Grosz et al., 1995) by ranking candidate antecedents appearing in the preceding discourse (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). In order to take advantage of existing models, we adopt the rankingbased approach as a basis fo"
P10-1128,D08-1068,0,0.0189595,"ention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it makes the coreference resolution task more information extraction-oriented. In other words, the coreference task as deﬁned by MUC and ACE is"
P10-1128,J01-4004,0,0.61212,"f seven pieces as a referent of the current referring expression. 3.2 Use of extra-linguistic information 3.1 Ranking model to identify referents To investigate the impact of extra-linguistic information on reference resolution, we conduct an em4 pirical evaluation in which a reference resolution model chooses a referent (i.e. a piece) for a given referring expression from the set of pieces illustrated on the computer display. As a basis for our reference resolution model, we adopt an existing model for reference resolution. Recently, machine learning-based approaches to reference resolution (Soon et al., 2001; Ng and Cardie, 2002, etc.) have been developed, particularly focussing on identifying anaphoric relations in texts, and have achieved better performance than hand-crafted rule-based approaches. These models for reference resolution take into account linguistic factors, such as relative salience of candidate antecedents, which have been modeled in Centering Theory (Grosz et al., 1995) by ranking candidate antecedents appearing in the preceding discourse (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). In order to take advantage of existing models, we adopt the rankingbased a"
P10-1128,P03-1022,0,0.0604902,"Missing"
P10-1128,P03-1023,0,0.314027,"ding anaphora and coreferences within texts has received a great deal of attention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it makes the coreference resolution task more information ext"
P10-1128,P05-1021,0,0.0182173,"texts has received a great deal of attention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it makes the coreference resolution task more information extraction-oriented. In other words, the"
P10-1128,P08-1096,0,0.0196794,"a great deal of attention in natural language processing, from both theoretical and empirical perspectives. Recently, research trends for reference resolution have drastically shifted from handcrafted rule-based approaches to corpus-based approaches, due predominately to the growing success of machine learning algorithms (such as Support Vector Machines (Vapnik, 1998)); many researchers have examined ways for introducing various linguistic clues into machine learning-based models (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Iida et al., 2005; Yang et al., 2005; Yang et al., 2008; Poon and Domingos, 2008, etc.). Research has continued to progress each year, focusing on tackling the problem as it is represented in the annotated data sets provided by the Message Understanding Conference (MUC)1 and the Automatic Content Extraction (ACE)2 . In these data sets, coreference relations are deﬁned as a limited version of a typical coreference; this generally means that only the relations where expressions refer to the same named entities are addressed, because it makes the coreference resolution task more information extraction-oriented. In other words, the coreference task as"
P10-1128,D08-1069,0,\N,Missing
S01-1013,P01-1004,1,0.766483,"in IN;,.., and le~ (IN;,..) is the character bigram length of IN;,... 2 B1gram frequency is weighted according to character type: a bigram made up entirely of hiragana charact~rs (gener_ally used in functional words/ particles) is given a weight of 0.2 and all other bigrams a weight of 1. Note that Dice&apos;s Coefficient ignores segment order, and that each string is thus treated as a ""bag of character bigrams"". Our choice of the combination of Dice&apos;s Coefficient, character-based indexing and character higrams (rather than any other n-gram order or mixed n-gram model) is based on the findings of Baldwin (2001b; 2001a), who compared character- and wordbased indexing in combination with both segment order-sensitive and bag-of-words similarity measures and with various n-gram models. As a result of extensive evaluation, Baldwin found the combination of character bigram-based indexing and a bagof-words method (in the form of either the vector space model or Dice&apos;s Coefficient) to be optimal. Our choice of Dice&apos;s Coefficient over the vector space m?del is due to the vector space model tending to bhthely prefer shorter strings in cases of low-level character overlap, and the ability of Dice&apos;s Coefficien"
S01-1013,H92-1045,0,0.0170499,"by the analysis of punctuation. These clause-level instances served as the inputs for the str·uctur·al method. We 2 freqTR;(e) 56 and len(TRi) are defined similarly. inputs with translation records is undesirable as high levels of spurious matches can be expected outside the scope of the original translation record expression. Inter-comparison of full inputs, on the other hand, provides a primitive model of domain similarity. Assuming that high similarity correlates with a high level of domain correspondence, we can apply a cross-lingual corollary of the ""one sense per discourse"" observation (Gale et al., 1992) in stipulating that a given word will be translated consistently within a given domain. By ascertaining that a given input closely resembles a second input, we can use the combined translation retrieval results for the two inputs to hone in on the optimal translation for the two. We term this procedure domain-based similarity consolidation. The overall retrieval process thus involves: (1) carrying out standard translation retrieval based on the abbreviated input, (2) using the original test set to determine the full input string most similar to the current input, and (3) performing translatio"
shirai-etal-2000-semi,W96-0112,0,\N,Missing
shirai-etal-2000-semi,W98-1510,1,\N,Missing
shirai-etal-2000-semi,J93-2004,0,\N,Missing
shirai-etal-2000-semi,E99-1026,0,\N,Missing
shirai-etal-2000-semi,P96-1025,0,\N,Missing
shirai-etal-2000-semi,W98-1511,0,\N,Missing
tokunaga-etal-2008-adapting,bel-etal-2000-simple,1,\N,Missing
tokunaga-etal-2008-adapting,P06-2106,1,\N,Missing
tokunaga-etal-2008-adapting,I08-1052,1,\N,Missing
tokunaga-etal-2008-adapting,francopoulo-etal-2006-lexical,1,\N,Missing
tokunaga-etal-2012-rex,stoia-etal-2008-scare,0,\N,Missing
tokunaga-etal-2012-rex,W10-3206,1,\N,Missing
tokunaga-etal-2012-rex,W10-4214,1,\N,Missing
tokunaga-etal-2012-rex,W09-0618,1,\N,Missing
tokunaga-etal-2012-rex,P10-1128,1,\N,Missing
tokunaga-etal-2012-rex,J96-2004,0,\N,Missing
tokunaga-etal-2012-rex,byron-fosler-lussier-2006-osu,0,\N,Missing
tokunaga-etal-2017-eye,W14-1609,0,\N,Missing
tokunaga-etal-2017-eye,P13-1153,0,\N,Missing
tokunaga-etal-2017-eye,P10-1118,0,\N,Missing
tokunaga-etal-2017-eye,J95-2003,0,\N,Missing
tokunaga-etal-2017-eye,N13-1088,0,\N,Missing
tokunaga-etal-2017-eye,W13-4303,1,\N,Missing
tokunaga-etal-2017-eye,R11-1024,1,\N,Missing
tokunaga-etal-2017-eye,W15-2401,0,\N,Missing
tokunaga-etal-2017-eye,W16-1904,0,\N,Missing
tokunaga-etal-2017-eye,C16-1269,1,\N,Missing
tokunaga-etal-2017-eye,Q16-1026,0,\N,Missing
tokunaga-etal-2017-eye,W12-4906,0,\N,Missing
W03-1107,W00-1016,0,\N,Missing
W03-1107,W02-1904,0,\N,Missing
W03-1107,P01-1026,0,\N,Missing
W03-1611,P01-1008,0,0.0245794,"ntact” means, although it is the core of the definition. On what basis could we consider different linguistic expressions denoting the same meaning? This becomes a crucial question when finding paraphrases automatically. In past research, various types of clues have been used to find paraphrases. For example, Shinyama et al. tried to find paraphrases assuming that two sentences sharing many Named Entities and a similar structure are likely to be paraphrases of each other (Shinyama et al., 2002). Barzilay and McKeown assume that two translations from the same original text contain paraphrases (Barzilay and McKeown, 2001). Torisawa used subcategorization information of verbs to paraphrase Japanese noun phrase construction “NP1 no NP2 ” into a noun phrase with a relative clause (Torisawa, 2001). Most of previous work on paraphrasing took corpus-based approach with notable exceptions of Jacquemin (Jacquemin et al., 1997; Jacquemin, 1999) and Katz (Katz, 1997). In particular, text alignment technique is generally used to find sentence level paraphrases (Shimohata and Sumita, 2002; Barzilay and Lee, 2002). In this paper, we follow the corpus-based approach and propose a method to find paraphrases of a Japanese nou"
W03-1611,shimohata-sumita-2002-automatic,0,0.0462492,"to, 1999). The query expansion works well for single-word index terms, but more sophisticated techniques are necessary for larger index units, such as phrases. The effectiveness of phrasal indexing has recently drawn researchers’ attention (Lewis, 1992; Mitra et al., 1997; Tokunaga et al., 2002). However, query expansion of phrasal index terms has not been fully investigated yet (Jacquemin et al., 1997). To deal with variations of linguistic expressions, paraphrasing has recently been studied for various applications of natural language processing, such as machine translation (Mitamura, 2001; Shimohata and Sumita, 2002), dialog systems (Ebert et al., 2001), QA systems (Katz, 1997) and information extraction (Shinyama et al., 2002). Paraphrasing is defined as a process of transforming an expression into another while keeping its meaning intact. However, it is difficult to define what “keeping its meaning intact” means, although it is the core of the definition. On what basis could we consider different linguistic expressions denoting the same meaning? This becomes a crucial question when finding paraphrases automatically. In past research, various types of clues have been used to find paraphrases. For example"
W03-1611,W01-1608,0,0.0307897,"single-word index terms, but more sophisticated techniques are necessary for larger index units, such as phrases. The effectiveness of phrasal indexing has recently drawn researchers’ attention (Lewis, 1992; Mitra et al., 1997; Tokunaga et al., 2002). However, query expansion of phrasal index terms has not been fully investigated yet (Jacquemin et al., 1997). To deal with variations of linguistic expressions, paraphrasing has recently been studied for various applications of natural language processing, such as machine translation (Mitamura, 2001; Shimohata and Sumita, 2002), dialog systems (Ebert et al., 2001), QA systems (Katz, 1997) and information extraction (Shinyama et al., 2002). Paraphrasing is defined as a process of transforming an expression into another while keeping its meaning intact. However, it is difficult to define what “keeping its meaning intact” means, although it is the core of the definition. On what basis could we consider different linguistic expressions denoting the same meaning? This becomes a crucial question when finding paraphrases automatically. In past research, various types of clues have been used to find paraphrases. For example, Shinyama et al. tried to find parap"
W03-1611,P97-1004,0,0.460149,"the basis of surface string matching. To remedy this problem, the current information retrieval system adopts query expansion techniques which replace a query term with a set of its synonyms (BaezaYates and Riberto-Neto, 1999). The query expansion works well for single-word index terms, but more sophisticated techniques are necessary for larger index units, such as phrases. The effectiveness of phrasal indexing has recently drawn researchers’ attention (Lewis, 1992; Mitra et al., 1997; Tokunaga et al., 2002). However, query expansion of phrasal index terms has not been fully investigated yet (Jacquemin et al., 1997). To deal with variations of linguistic expressions, paraphrasing has recently been studied for various applications of natural language processing, such as machine translation (Mitamura, 2001; Shimohata and Sumita, 2002), dialog systems (Ebert et al., 2001), QA systems (Katz, 1997) and information extraction (Shinyama et al., 2002). Paraphrasing is defined as a process of transforming an expression into another while keeping its meaning intact. However, it is difficult to define what “keeping its meaning intact” means, although it is the core of the definition. On what basis could we consider"
W03-1611,P95-1026,0,0.00447346,"not sufficient for evaluating the quality of the paraphrases. However, it reflects relatedness between the input noun phrase 4.3 Contextual information We assume that phrases sharing the same Kanzi characters likely represent the same meaning. Therefore they could be paraphrases of each other. However, even though a Kanzi denotes a certain meaning, its meaning is often ambiguous. This problem is similar to word sense ambiguities, which have been studied for many years. To solve this problem, we adopt an idea one sense per collocation which was introduced in word sense disambiguation research (Yarowsky, 1995). Considering a newspaper article in which the retrieved passage and the input noun phrase is included as the context, the context similarity is taken into account for ranking paraphrase candidates. More concretely, context similarity is calculated by following procedure. 1. For each paraphrase candidate, a context vector is constructed from the newspaper article containing the passage from which the candidate is derived. The article is morphologically analyzed and content words are extracted to make the context vector. The tf · idf metric is used for term weighting. 2. Since the input is give"
W03-1611,W02-1022,0,\N,Missing
W03-1611,P99-1044,0,\N,Missing
W06-1411,J95-3003,0,0.274215,"tion, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world. In order to refer to an intended object (the target) among others (distractors), most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek, 1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) utilized attributes of the target and binary relations between the target and distractors. Therefore, these methods cannot generate proper referring expressions in situations where there is no significant surface difference between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression ∗ † a c Table b P Figure 1: An example of problematic situations In the situation of Figure 1, the speaker can indicate object b to person P with a simple expre"
W06-1411,P97-1027,0,0.020935,"s a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world. In order to refer to an intended object (the target) among others (distractors), most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek, 1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) utilized attributes of the target and binary relations between the target and distractors. Therefore, these methods cannot generate proper referring expressions in situations where there is no significant surface difference between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression ∗ † a c Table b P Figure 1: An example of problematic situations In the situation of Figure 1, the speaker can indicate object b to person P with a simple expression “the fron"
W06-1411,J03-1003,0,0.304934,"Missing"
W06-1411,W00-1416,0,0.0321357,"on P does not share these labels with the speaker. Because object b is not distinguishable from objects a or c by means of their appearance, one would try to use a binary relation between object b and the table, i.e., “a ball to the right of the table”. However, “to the right of” is not a discriminatory relation, for objects a and c are also located to the right of the table. Using a and c as a reference object instead of the table does not make sense, since a and c cannot be uniquely identified because of the same reason that b cannot be identified. Such situations have drawn less attention (Stone, 2000), but can frequently occur in some domains such as object arrangement (Tanaka et al., 2004). Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects in order to distinguish the target object from others. However, such an approach does not work well when there is no distinctive attribute among objects. To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively gen"
W06-1411,J02-1003,0,0.170841,"Missing"
W06-1411,E91-1028,0,0.0976104,"istinctive attribute among objects. To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them. The evaluation using 18 subjects showed that the proposed method could effectively generate proper referring expressions. 1 Introduction In the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about objects in the world. In order to refer to an intended object (the target) among others (distractors), most past work (Appelt, 1985; Dale and Haddock, 1991; Dale, 1992; Dale and Reiter, 1995; Heeman and Hirst, 1995; Horacek, 1997; Krahmer and Theune, 2002; van Deemter, 2002; Krahmer et al., 2003) utilized attributes of the target and binary relations between the target and distractors. Therefore, these methods cannot generate proper referring expressions in situations where there is no significant surface difference between the target and distractors, and no binary relation is useful to distinguish the target. Here, a proper referring expression ∗ † a c Table b P Figure 1: An example of problematic situations In the situation of Figure 1, the sp"
W09-0618,stoia-etal-2008-scare,0,0.363745,"Missing"
W09-0618,byron-fosler-lussier-2006-osu,0,\N,Missing
W09-0634,2007.mtsummit-ucnlg.14,0,0.0311081,"Missing"
W09-0634,W05-1606,0,0.119751,"been paid much attention to. One major aim of our research is to relax this constraint on uniqueness because of the reason explained below. The fundamental goal of our research is to deal with multipartite objects, which have constituents Figure 1: An example scene Proceedings of the 12th European Workshop on Natural Language Generation, pages 191–194, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 191 2 Related work A referring expression e is represented as a set of n pairs of an attribute value expression eaj and a part expression epj modiﬁed by eaj as Horacek (2005) proposes to introduce probabilities to overcome uncertainties due to discrepancies in knowledge and cognition between subjects. While our model shares the same awareness of issues with Horacek’s work, our focus is on rather different issues (i.e., handling multipartite objects and relaxing the constraint on uniqueness). In addition, Horacek’s work is concerned only with generation while our model is available both for generation and understanding. Roy (2002) also proposes a probabilistic model for generation but presupposes uniform objects. Horacek (2006) deals with references for structured"
W09-0634,C08-2029,1,0.868086,"ain modeling as a subset. 1 Introduction Generation of referring expressions has been studied for the last two decades. The basic orientation of this research was pursuing an algorithm that generates a minimal description which uniquely identiﬁes a target object from distractors. Thus the research was oriented and limited by two constraints: minimality and uniqueness. The constraint on minimality has, however, been relaxed due to the computational complexity of generation, the perceived naturalness of redundant expressions, and the easiness of understanding them (e.g., (Dale and Reiter, 1995; Spanger et al., 2008)). On the other hand, the other constraint of uniqueness has not been paid much attention to. One major aim of our research is to relax this constraint on uniqueness because of the reason explained below. The fundamental goal of our research is to deal with multipartite objects, which have constituents Figure 1: An example scene Proceedings of the 12th European Workshop on Natural Language Generation, pages 191–194, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 191 2 Related work A referring expression e is represented as a set of n pairs of an attribute"
W09-0634,W06-1408,0,\N,Missing
W09-3421,francopoulo-etal-2006-lexical,1,0.857972,"Missing"
W09-3421,bel-etal-2000-simple,1,0.763962,"the advantages of corpusbased approaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian langua"
W09-3421,W03-1905,1,0.826021,"pproaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian languages, we elucidated the shortco"
W09-3421,P06-2106,1,0.822025,"Missing"
W09-3421,tokunaga-etal-2008-adapting,1,0.730656,"Missing"
W09-3421,W06-1001,1,\N,Missing
W09-3611,N04-1037,0,0.0150312,"Missing"
W09-3611,J00-3005,0,0.0982094,"Missing"
W09-3611,P02-1014,0,0.278408,"6.27 80.49 50.00 48.35 88.00 62.41 46.70 86.73 60.71 salient for increased performance. We also extended this list by adding a cosine-similarity metric between two noun phrases; it uses bag-ofwords to create a vector for each noun phrase (where each word is a term in the vector) to compute their similarity. The intuition behind this is that noun phrases with more similar surface forms should be more likely to corefer. We further optimized string recognition and plurality detection for handling citation-strings. See Table 3 for the full list of our features. While both (Soon et al., 2001) and (Ng and Cardie, 2002) induced decision trees (C5 and C4.5, respectively) we opted for using an SVM-based approach instead (Vapnik, 1998; Joachims, 1999). SVMs are known for being reliable and having good performance. Training the Coreference Resolver To create and train our coreference resolver, we used a combination of techniques as outlined originally by (Soon et al., 2001) and subsequently extended by (Ng and Cardie, 2002). Mimicking their approaches, we used the corpora provided for the MUC-7 coreference resolution task (LDC2001T02, 2001), which includes sets of newspaper articles, annotated with coreference r"
W09-3611,C08-1087,0,0.354515,"Missing"
W09-3611,W06-0804,0,0.15775,"Missing"
W09-3611,J01-4004,0,0.510177,"1 Sentence Eval. R P F 36.27 80.49 50.00 48.35 88.00 62.41 46.70 86.73 60.71 salient for increased performance. We also extended this list by adding a cosine-similarity metric between two noun phrases; it uses bag-ofwords to create a vector for each noun phrase (where each word is a term in the vector) to compute their similarity. The intuition behind this is that noun phrases with more similar surface forms should be more likely to corefer. We further optimized string recognition and plurality detection for handling citation-strings. See Table 3 for the full list of our features. While both (Soon et al., 2001) and (Ng and Cardie, 2002) induced decision trees (C5 and C4.5, respectively) we opted for using an SVM-based approach instead (Vapnik, 1998; Joachims, 1999). SVMs are known for being reliable and having good performance. Training the Coreference Resolver To create and train our coreference resolver, we used a combination of techniques as outlined originally by (Soon et al., 2001) and subsequently extended by (Ng and Cardie, 2002). Mimicking their approaches, we used the corpora provided for the MUC-7 coreference resolution task (LDC2001T02, 2001), which includes sets of newspaper articles, an"
W09-3611,W06-1613,0,0.407422,"m of information overload, a form of knowledge reduction may be necessary. Past research (Garfield et al., 1964; Small, 1973) has shown that citations contain a plethora of latent information available and that much can be gained by exploiting it. Indeed, there is a wealth of literature on topic-clustering, e.g. bibliographic coupling (Kessler, 1963), or cocitation analysis (Small, 1973). Subsequent research demonstrated that citations could be clustered on their quality, using keywords that appeared in the running-text of the citation (Weinstock, 1971; Nanba et al., 2000; Nanba et al., 2004; Teufel et al., 2006). Similarly, other work has shown the utility in the IR domain of ranking the relevance of cited papers by using supplementary index terms extracted from the content of citations in citing papers, including methods that search through a fixed character-length window (O’Connor, 1982; Bradshaw, 2003), or that focus solely on the sentence containing the citation (Ritchie et al., 2008) for It is clear that merit exists behind extraction of citations in running text. This paper proposes a new method for performing this task based on coreference-chains. To evaluate our method we created a corpus of"
W10-3206,gargett-etal-2010-give,0,0.0165687,"o participants, who worked together on a simple 2-D design task, buying and arranging furniture for two rooms. The COCONUT corpus is limited in annotations which describe symbolic object information such as object intrinsic attributes and location in discrete co-ordinates. As an initial work of constructing a corpus for collaborative tasks, the COCONUT corpus can be characterised as having a rather simple domain as well As for a multilingual aspect, all the above corpora are English. There have been several recent attempts at collecting multilingual corpora in situated domains. For instance, (Gargett et al., 2010) collected German and English corpora in the same setting. Their domain is similar to the QUAKE corpus. Van der Sluis et al. (2009) aim at a comparative study of referring expressions between English and Japanese. Their domain is still static at the moment. Our corpora aim at dealing with the dynamic nature of situated dialogues between very different languages, English and Japanese. 6 We called such expressions as action-mentioning expressions (AME) in our previous work. 44 eration, and evaluation in different tasks (puzzles) as well. We are planning to distribute the REX-J corpus family thro"
W10-3206,W09-0618,1,0.828626,"notations which describe symbolic object information such as object intrinsic attributes and location in discrete co-ordinates. As an initial work of constructing a corpus for collaborative tasks, the COCONUT corpus can be characterised as having a rather simple domain as well As for a multilingual aspect, all the above corpora are English. There have been several recent attempts at collecting multilingual corpora in situated domains. For instance, (Gargett et al., 2010) collected German and English corpora in the same setting. Their domain is similar to the QUAKE corpus. Van der Sluis et al. (2009) aim at a comparative study of referring expressions between English and Japanese. Their domain is still static at the moment. Our corpora aim at dealing with the dynamic nature of situated dialogues between very different languages, English and Japanese. 6 We called such expressions as action-mentioning expressions (AME) in our previous work. 44 eration, and evaluation in different tasks (puzzles) as well. We are planning to distribute the REX-J corpus family through GSK (Language Resources Association in Japan)8 , and the REX-E corpus from both University of Brighton and GSK. Table 7: The RE"
W10-3206,J95-3003,0,0.0876286,"he QUAKE and SCARE corpora, we allowed a comparatively large flexibility in the actions necessary for achieving the goal shape (i.e. flipping, turning and moving of puzzle pieces at different degrees), relative to the complexity of the domain. Providing this relatively larger freedom of actions to the participants together with the recording of detailed information allows for research into new aspects of referring expressions. Related work Over the last decade, with a growing recognition that referring expressions frequently appear in collaborative task dialogues (Clark and WilkesGibbs, 1986; Heeman and Hirst, 1995), a number of corpora have been constructed to study the nature of their use. This tendency also reflects the recognition that this area yields both challenging research topics as well as promising applications such as human-robot interaction (Foster et al., 2008; Kruijff et al., 2010). The COCONUT corpus (Di Eugenio et al., 2000) was collected from keyboard-dialogs between two participants, who worked together on a simple 2-D design task, buying and arranging furniture for two rooms. The COCONUT corpus is limited in annotations which describe symbolic object information such as object intrins"
W10-3206,W10-4214,1,0.485114,"Missing"
W10-3206,P10-1128,1,0.781753,"Missing"
W10-3206,stoia-etal-2008-scare,0,0.0932985,"erest (e.g. used in describing where the object is located in space). REs have attracted a great deal of attention in both language analysis and language generation research. In language analysis research, 1 2 http://www.nlpir.nist.gov/related projects/muc/ http://www.itl.nist.gov/iad/tests/ace/ 38 Proceedings of the 8th Workshop on Asian Language Resources, pages 38–46, c Beijing, China, 21-22 August 2010. 2010 Asian Federation for Natural Language Processing have been developed (Di Eugenio et al., 2000; Byron, 2005; van Deemter et al., 2006; Foster and Oberlander, 2007; Foster et al., 2008; Stoia et al., 2008; Spanger et al., 2009a; Belz et al., 2010). Unlike the corpora of MUC and ACE, many are collected from situated dialogues, and therefore include multimodal information (e.g. gestures and eye-gaze) other than just transcribed text (Martin et al., 2007). Foster and Oberlander (2007) emphasised that any corpus for language generation should include all possible contextual information at the appropriate granularity. Since constructing a dialogue corpus generally requires experiments for data collection, this kind of corpus tends to be small-scale compared with corpora for reference resolution. Ag"
W10-3206,W06-1420,0,0.0651162,"Missing"
W10-3206,P93-1007,0,0.0262941,"y the sequence of possible referents as its referent, if any are present. • All expressions appearing in muttering to oneself are excluded. Table 2 shows a list of attributes of referring expressions used in annotating the corpus. The rest of the 20 Japanese dialogues were annotated by two of the authors and discrepancies were resolved by discussion. Four English dialogues have been annotated so far by one of the authors. • The minimum span of a noun phrase including necessary information to identify a referent is annotated. The span might include repairs with their reparandum and disfluency (Nakatani and Hirschberg, 1993) if needed. 4 Preliminary corpus analysis We have already completed the Japanese corpus, which is named REX-J (2008-08), but only 4 out of 24 dialogues have been annotated for the English counterpart (REX-E (2010-03)). Table 3 shows a summary of the trials. The horizontal • Demonstrative adjectives are included in expressions. 42 lines divide the trials by pairs, “o” in the “success” column denotes that the trial was successfully completed in the time limit (15 minutes), and the “OP-REX” and “SV-REX” columns show the number of referring expressions used by the operator and the solver respectiv"
W10-3206,byron-fosler-lussier-2006-osu,0,\N,Missing
W10-4214,P08-2050,0,0.114127,"been a number of criticisms against this type of evaluation. (Reiter and Sripada, 2002) argue, for example, that generated text might be very different from a corpus but still achieve the specific communicative goal. An additional problem is that corpus-similarity metrics measure how well a system reproduces what speakers (or writers) do, while for most NLG systems ultimately the most important consideration is its effect on the human user (i.e. listener or reader). Thus (Khan et al., 2009) argues that “measuring human-likeness disregards effectiveness of these expressions”. Furthermore, as (Belz and Gatt, 2008) state “there are no significant correlations between intrinsic and extrinsic evaluation measures”, concluding that “similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance”. From early on in the NLG community, taskbased extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of a system (Reiter and Belz, 2009). Task performance evaluation is recognized as the “only known way to measure the effectiveness of NLG systems with r"
W10-4214,W09-0628,0,0.0117979,"generation of referring expressions has moved to dynamic domains such as situated dialog, e.g. (Jordan and Walker, 2005) and (Stoia et al., 2006). However, both of them carried out an intrinsic evaluation measuring corpus-similarity or asking evaluators to compare system output to expressions used by human (the right bottom quadrant in Figure 1). The construction of effective generation systems in the dynamic domain requires the implementation of an extrinsic task performance evaluation. There has been work on extrinsic evaluation of instructions in the dynamic domain on the GIVE-2 challenge (Byron et al., 2009), which is a task to generate instructions in a virtual world. It is based on the GIVE-corpus (Gargett et al., 2010), which is collected through keyboard interaction. The evaluation measures used are e.g. the number of successfully completed trials, completion time as well as the numbers of instructions the system sent to the user. As part of the JAST project, a Joint Construction Task (JCT) puzzle construction corpus (Foster et al., 2008) was created which is similar in some ways in its set-up to the REXJ corpus which we use in the current research. There has been some work on evaluating gene"
W10-4214,P06-1130,0,0.0309125,"Missing"
W10-4214,W08-1113,0,0.0328025,"Missing"
W10-4214,gargett-etal-2010-give,0,0.0630249,"2005) and (Stoia et al., 2006). However, both of them carried out an intrinsic evaluation measuring corpus-similarity or asking evaluators to compare system output to expressions used by human (the right bottom quadrant in Figure 1). The construction of effective generation systems in the dynamic domain requires the implementation of an extrinsic task performance evaluation. There has been work on extrinsic evaluation of instructions in the dynamic domain on the GIVE-2 challenge (Byron et al., 2009), which is a task to generate instructions in a virtual world. It is based on the GIVE-corpus (Gargett et al., 2010), which is collected through keyboard interaction. The evaluation measures used are e.g. the number of successfully completed trials, completion time as well as the numbers of instructions the system sent to the user. As part of the JAST project, a Joint Construction Task (JCT) puzzle construction corpus (Foster et al., 2008) was created which is similar in some ways in its set-up to the REXJ corpus which we use in the current research. There has been some work on evaluating generation strategies of instructions for a collaborative construction task on this corpus, both considering intrinsic a"
W10-4214,W09-0629,0,0.0313814,"Missing"
W10-4214,W09-0615,0,0.0520131,"Missing"
W10-4214,P09-2076,0,0.0136793,"luation measures”, concluding that “similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance”. From early on in the NLG community, taskbased extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of a system (Reiter and Belz, 2009). Task performance evaluation is recognized as the “only known way to measure the effectiveness of NLG systems with real users” (Reiter et al., 2003). Following this direction, the GIVE-Challenges (Koller et al., 2009) at INLG 2010 (instruction generation) also include a taskperformance evaluation. In contrast to the vertical axis of Figure 1, there is the horizontal axis of the domain in which referring expressions are used. Referring expressions can thus be distinguished according to whether they are used in a static or a dynamic domain, corresponding to the left and right of the horizontal axis of Figure 1. A static domain is one such as the TUNA corpus (van Deemter, 2007), which collects referring expressions based on a motionless image. In contrast, a dynamic domain comprises a constantly changing situ"
W10-4214,P02-1040,0,0.0850087,"verview of recent work on evaluation of referring expressions Figure 1 shows a schematic overview of recent work on evaluation of referring expressions along the two axes of evaluation method and domain in which referring expressions are used. There are two different evaluation methods corresponding to the bottom and the top of the vertical axis in Figure 1: intrinsic and extrinsic evaluations (Sparck Jones and Galliers, 1996). Intrinsic methods often measure similarity between the system output and the gold standard corpora using metrics such as tree similarity, string-editdistance and BLEU (Papineni et al., 2002). Intrinsic methods have recently become popular in the NLG community. In contrast, extrinsic methods evaluate generated expressions based on an external metric, such as its impact on human task performance. While intrinsic evaluations have been widely used in NLG, e.g. (Reiter et al., 2005), (Cahill and van Genabith, 2006) and the competitive 2009 TUNA-Challenge, there have been a number of criticisms against this type of evaluation. (Reiter and Sripada, 2002) argue, for example, that generated text might be very different from a corpus but still achieve the specific communicative goal. An ad"
W10-4214,W06-1409,0,0.0635106,"Missing"
W10-4214,J09-4008,0,0.0146143,"n et al., 2009) argues that “measuring human-likeness disregards effectiveness of these expressions”. Furthermore, as (Belz and Gatt, 2008) state “there are no significant correlations between intrinsic and extrinsic evaluation measures”, concluding that “similarity to human-produced reference texts is not necessarily indicative of quality as measured by human task performance”. From early on in the NLG community, taskbased extrinsic evaluations have been considered as the most meaningful evaluation, especially when having to convince people in other communities of the usefulness of a system (Reiter and Belz, 2009). Task performance evaluation is recognized as the “only known way to measure the effectiveness of NLG systems with real users” (Reiter et al., 2003). Following this direction, the GIVE-Challenges (Koller et al., 2009) at INLG 2010 (instruction generation) also include a taskperformance evaluation. In contrast to the vertical axis of Figure 1, there is the horizontal axis of the domain in which referring expressions are used. Referring expressions can thus be distinguished according to whether they are used in a static or a dynamic domain, corresponding to the left and right of the horizontal"
W10-4214,W02-2113,0,0.014679,"e similarity between the system output and the gold standard corpora using metrics such as tree similarity, string-editdistance and BLEU (Papineni et al., 2002). Intrinsic methods have recently become popular in the NLG community. In contrast, extrinsic methods evaluate generated expressions based on an external metric, such as its impact on human task performance. While intrinsic evaluations have been widely used in NLG, e.g. (Reiter et al., 2005), (Cahill and van Genabith, 2006) and the competitive 2009 TUNA-Challenge, there have been a number of criticisms against this type of evaluation. (Reiter and Sripada, 2002) argue, for example, that generated text might be very different from a corpus but still achieve the specific communicative goal. An additional problem is that corpus-similarity metrics measure how well a system reproduces what speakers (or writers) do, while for most NLG systems ultimately the most important consideration is its effect on the human user (i.e. listener or reader). Thus (Khan et al., 2009) argues that “measuring human-likeness disregards effectiveness of these expressions”. Furthermore, as (Belz and Gatt, 2008) state “there are no significant correlations between intrinsic and"
W10-4214,W09-0618,1,0.262804,"next referring expression (go to (1)). Figure 3 shows a screenshot of the interface prepared for this experiment. The test data consists of three types of referring expressions: DPs (demonstrative pronouns), AMEs (action-mentioning expressions), and OTHERs (any other expression that is neither a DP nor AME, e.g intrinsic attributes and spatial relations). DPs are the most frequent type of referring expression in the corpus. AMEs are expressions that utilize an action on the referent such as “the triangle you put away to the top right” (see Table 1)1 . As we pointed out in our previous paper (Spanger et al., 2009a), they are also a fundamental type of referring expression in this domain. The basic question in investigating a suitable context is what information to consider about the preceding interaction; i.e. over what parameters to vary the context. In previous work on the generation of demonstrative pronouns in a situated domain (Spanger et al., 2009b), we investigated the role of linguistic and extra-linguistic information, and found that time distance from the last action (LA) on the referent as well as the last mention (LM) to the referent had a significant influence on the usage of referring ex"
W10-4214,W06-1412,0,0.0301277,"ntrinsic evaluation is (van der Sluis et al., 2007), who employed the Dice-coefficient measuring corpus-similarity. There have been a number of extrinsic evaluations as well, such as (Paraboni et al., 2006) and (Khan et al., 2009), respectively measuring the effect of overspecification on task performance and the impact of generated text on accuracy as well as processing speed. They belong thus in the top-left quadrant of Figure 1. Over a recent period, research in the generation of referring expressions has moved to dynamic domains such as situated dialog, e.g. (Jordan and Walker, 2005) and (Stoia et al., 2006). However, both of them carried out an intrinsic evaluation measuring corpus-similarity or asking evaluators to compare system output to expressions used by human (the right bottom quadrant in Figure 1). The construction of effective generation systems in the dynamic domain requires the implementation of an extrinsic task performance evaluation. There has been work on extrinsic evaluation of instructions in the dynamic domain on the GIVE-2 challenge (Byron et al., 2009), which is a task to generate instructions in a virtual world. It is based on the GIVE-corpus (Gargett et al., 2010), which is"
W12-1633,P02-1011,0,0.100083,"Missing"
W12-1633,W09-0609,0,0.059174,"he REX-J corpus shows promising results. 1 Introduction Referring expressions (REs) are expressions intended by speakers to identify entities to hearers. REs can be classified into three categories: descriptions, anaphora, and deixis; and, in most cases, have been studied within each category and with a narrowly focused interest. Descriptive expressions (such as “the blue glass on the table”) exploit attributes of entities and relations between them to distinguish an entity from the rest. They are well studied in natural language generation, e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Dale and Viethen, 2009). Anaphoric expressions (such as “it”) refer to entities or concepts introduced in the preceding discourse and are studied mostly on textual monologues, e.g., (Kamp and Reyle, 1993; Mitkov, 2002; Ng, 2010). Deictic (exophoric) expressions (such as “this one”) refer to entities outside the preceding discourse. They are often studied focusing on pronouns accompanied with pointing gestures in physical spaces, e.g., (Gieselmann, 2004). Dialogue systems (DSs) as natural humanmachine (HM) interfaces are expected to handle all the three categories of referring expressions (Salmon-Alt and Romary, 2001"
W12-1633,W10-4203,0,0.263673,"ndent attributes. In addition, by treating a reference domain as a referent, REs referring to sets of entities are handled, too. As far as the authors know, this work is the first that takes a probabilistic approach to reference domains. 237 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 237–246, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics 1.1 Reference domains First, we explain reference domains concretely. Reference domains (RDs) (Salmon-Alt and Romary, 2000; Salmon-Alt and Romary, 2001; Denis, 2010) are theoretical constructs, which are basically sets of entities presupposed at each use of REs. RDs in the original literature are not mere sets of entities but mental objects equipped with properties such as type, focus, or saliency and internally structured with partitions. In this paper, while we do not explicitly handle partitions, reference domains can be nested as an approximation of partitioning, that is, an entity included in a RD is either an individual entity or another RD. Each RD d has its focus and degree of saliency (a non-negative real number). Hereafter, two of them are denot"
W12-1633,P10-1128,1,0.172335,"We employ a Bayesian network (BN) to model a RE. Dealing with continuous information and vague situations is critical to handle real world problems. Probabilistic approaches enable this for reference resolvers. Each BN is dynamically constructed based on the structural analysis result of a RE and contextual information available at that moment. The BN is used to estimate the probability with which the corresponding RE refers to an entity. One of the two major contributions of this paper is our probabilistic formulation that handles the above three kinds of REs in a unified manner. Previously Iida et al. (2010) proposed a quantitative approach that handles anaphoric and deictic expressions in a unified manner. However it lacks handling of descriptive expressions. Our formulation subsumes and extends it to handle descriptive REs. So far, no previously proposed method for reference resolution handles all three types of REs. The other contribution is bringing reference domains into that formulation. Reference domains (Salmon-Alt and Romary, 2000) are sets of referents implicitly presupposed at each use of REs. By considering them, our approach can appropriately interpret context-dependent attributes. I"
W12-1633,J03-1003,0,0.11197,"Missing"
W12-1633,P07-1103,0,0.0586855,"Missing"
W12-1633,P10-1142,0,0.0213797,"ra, and deixis; and, in most cases, have been studied within each category and with a narrowly focused interest. Descriptive expressions (such as “the blue glass on the table”) exploit attributes of entities and relations between them to distinguish an entity from the rest. They are well studied in natural language generation, e.g., (Dale and Reiter, 1995; Krahmer et al., 2003; Dale and Viethen, 2009). Anaphoric expressions (such as “it”) refer to entities or concepts introduced in the preceding discourse and are studied mostly on textual monologues, e.g., (Kamp and Reyle, 1993; Mitkov, 2002; Ng, 2010). Deictic (exophoric) expressions (such as “this one”) refer to entities outside the preceding discourse. They are often studied focusing on pronouns accompanied with pointing gestures in physical spaces, e.g., (Gieselmann, 2004). Dialogue systems (DSs) as natural humanmachine (HM) interfaces are expected to handle all the three categories of referring expressions (Salmon-Alt and Romary, 2001). In fact, the three categories are not mutually exclusive. To be concrete, a descriptive expression in conversation is either deictic or anaphoric. It is, however, not easy to tell whether a RE is deicti"
W13-0508,P10-1118,0,0.116178,"dates of predicates and arguments were marked beforehand in the annotation tool, the annotators were instructed to add links between correct predicateargument pairs by using the keyboard and mouse. We distinguished three types of links based on the case marker of arguments, i.e. ga (nominative), o (accusative) and ni (dative). For elliptical arguments 81 Number of instances 50 fin and Bock, 2000; Richardson et al., 2007). Compared to the studies on language and eye-gaze, the role of gaze in general problem solving settings has been less studied (Bednarik and Tukiainen, 2008; Rosengrant, 2010; Tomanek et al., 2010). Since our current interest, corpus annotation, can be considered as a problem solving as well as language comprehension, various existing metrics derived from eye-tracking data would be useful. Rosengrant (2010) proposed an analysis method named gaze scribing where eye-tracking data is combined with subjects thought process derived by the TAP, underlining the importance of applying gaze scribing to various problem solving. Tomanek et al. (2010) utilised eye-tracking data to evaluate difficulties of named entities for selecting training instances for active learning techniques. Our analysis i"
W13-0508,J96-2004,0,0.36999,"as 1, 280 × 1, 024 pixels and the distance between the display and the annotator’s eye was maintained at about 50 cm. In order to minimise the head movement, we used a chin rest. The following are potential uses of the collected data. Finding useful information for NLP Given a certain task, the collected data would give some hints to understand human decision processes. Therefore, the information can be useful for replicating human decisions by using ML-based approaches. Evaluating annotation quality The quality of corpora is often evaluated based on the agreement ratio and the κ coefficient (Carletta, 1996) between multiple annotators. Analysing the collected data would help to estimate the realiability of each annotation instance. For instance, the long annotation time for an instance is an indication of its difficulty, therefore the annotation on such an instance might be less reliable. Evaluating and training annotators Unlike the quality of corpora, the quality of annotators is rarely discussed. In addition to the extent to which annotators can replicate the gold standard annotation (result-based metric), the collected data can be used for evaluating annotators by comparing their behaviours"
W13-0508,N10-1061,0,0.0149646,"collect the annotator’s rationales behind the annotation process. 79 From an engineering viewpoint, a machine does not need to perform a task in the same manner as a human does. The currently used clues might be sufficient for doing the job even though a human uses different information. For instance, POS tagging and parsing are successful instances of the CCML approach. However, this approach does not always work well on other tasks such as semantic and discourse processing. For instance, the performance of the state-of-the-art coreference resolution model still stays around 0.7 in F-score (Haghighi and Klein, 2010). Furthermore, the performance of zero anaphora resolution in Japanese is much worse, around 0.4 in F-score (Iida and Poesio, 2011). Such relatively low performance suggests that some crucial information should have been utilised for ML techniques. Against this background, we propose annotating each annotation with the annotator’s rationale behind her/his decision. Since the rationale explains the validity of the annotation instance, it can be considered as a kind of meta-level annotation against the object-level annotation rather than a mere attribute of the annotation. We expect that analysi"
W13-0508,P11-1081,1,0.79637,"m a task in the same manner as a human does. The currently used clues might be sufficient for doing the job even though a human uses different information. For instance, POS tagging and parsing are successful instances of the CCML approach. However, this approach does not always work well on other tasks such as semantic and discourse processing. For instance, the performance of the state-of-the-art coreference resolution model still stays around 0.7 in F-score (Haghighi and Klein, 2010). Furthermore, the performance of zero anaphora resolution in Japanese is much worse, around 0.4 in F-score (Iida and Poesio, 2011). Such relatively low performance suggests that some crucial information should have been utilised for ML techniques. Against this background, we propose annotating each annotation with the annotator’s rationale behind her/his decision. Since the rationale explains the validity of the annotation instance, it can be considered as a kind of meta-level annotation against the object-level annotation rather than a mere attribute of the annotation. We expect that analysing these rationales behind human decisions reveals more effective information for a given task that has never been used by existing"
W13-0508,J10-2007,0,0.013271,"ing the annotation and discusses their potential uses. Finally a preliminary experiment for data collection is described with the data analysis. 1 The primacy of the CC-ML approach over the rule-based approach has been shown in fundamental NLP tasks (e.g. POS tagging, syntactic parsing and word sense disambiguation) as well as in various applications (e.g. information extraction, machine translation and summarisation) through prevalent competition-type conferences. However, too much dominance of the revived empiricism has recently worried a number of researchers (Reiter, 2007; Steedman, 2008; Krahmer, 2010; Church, 2011). For instance, Church (2011), who is one of the initiators of the revived empiricism, warned us that we should follow the CC-ML approach with an awareness of the limitations of the underlying ML techniques. Introduction The last two decades witnessed a great success of revived empiricism in NLP research. Namely, the corpus construction and machine learning (CC-ML) approach has been the main stream of NLP research, where corpora are annotated for a specific task and then they are used as training data for machine learning (ML) techniques to build a system for the task. The CC-ML"
W13-0508,maekawa-etal-2010-design,0,\N,Missing
W13-0508,J07-2013,0,\N,Missing
W13-2118,W11-2826,0,0.0220204,"k, passivisation is performed by taking into account both linguistic and extra-linguistic information. The linguistic information explains passivisation in an incremental generation process; realising the most salient discourse entity in short term memory as a subject eventually leads to passivisation. In contrast, extra-linguistic information is used to move a less salient entity to a subject position when an explicit agent is missing in the text. Although these two kinds of information seem adequate for explaining passivisation, their applicability was not examined in empirical evaluations. Sheikha and Inkpen (2011) focused attention on voice selection in the generation task distinguishing formal and informal sentences. In their work, passivisation is considered as a rhetorical technique for conveying formal intentions. However, they did not discuss passivisation in terms of discourse coherence. 3 where vi is a verb in question1 , f reqall (vi ) is the frequency of vi appearing in corpora, and f reqpas (vi ) is the frequency of vi with the passive marker, (ra)reru. The logarithm of f reqall (vi ) is multiplied due to avoiding the overestimation of the score for less frequent instances. In the evaluation,"
W13-2118,E93-1002,0,0.495819,"se Japanese journalists tend to write their opinions objectively by omitting the agent role. To take into account this preference of verb passivisation, we define a preference score by the following formula: f reqpas (vi ) scorepas (vi ) = ·log f reqall (vi ) (1) f reqall (vi ) Related work The task of automatic voice selection has been mainly developed in the NLG community. However, it has attracted less attention compared with other major NLG problems, such as generating referring expressions. There is less work focusing singly on voice selection, but not entirely without exception, such as Abb et al. (1993). In their work, passivisation is performed by taking into account both linguistic and extra-linguistic information. The linguistic information explains passivisation in an incremental generation process; realising the most salient discourse entity in short term memory as a subject eventually leads to passivisation. In contrast, extra-linguistic information is used to move a less salient entity to a subject position when an explicit agent is missing in the text. Although these two kinds of information seem adequate for explaining passivisation, their applicability was not examined in empirical"
W13-2118,J95-2003,0,0.0289711,") in the preceding context. P stands for the predicate in question. The four feature types (PRED, SYN, ARG and COREF) correspond to each information described in Section 3. Table 1: Feature set for voice selection named entity labels provided by CaboCha, such as Person and Organisation, and the ontological information defined in a Japanese ontology, nihongo goi taikei (Ikehara et al., 1997)) as features. (in this case, the agent filler) of a predicate is exophoric, the passive voice is selected. Coreference and anaphora of arguments As discussed in discourse theories such as Centering Theory (Grosz et al., 1995), arguments which have been already most salient in the preceding context tend to be placed at the beginning of a sentence for reducing the cognitive cost of reading, as argued in Functional Grammar (Halliday and Matthiessen, 2004). In order to consider the characteristic, we employ an extension of Centering Theory (Grosz et al., 1995), proposed by Nariyama (2002) for implementing the COREF type features in Table 1. She proposed a generalised version of the forward looking-center list, called the Salient Reference List (SRL), which stores all salient discourse entities (e.g. NP) in the precedi"
W13-2118,W07-1522,1,0.751044,"take into account the following four information as features The details of the feature set are shown in Table 1. research tackles the problem of voice selection considering a wide range of linguistic information that is assumed to be already decided in the preceding stages of a generation process. The paper is organised as follows. We first overview the related work in Section 2, and then propose a voice selection model based on the four kinds of information that impact voice selection in Section 3. Section 4 then demonstrates the results of empirical evaluation using the NAIST Text Corpus (Iida et al., 2007) as training and evaluation data sets. Finally, Section 5 concludes and discusses our future directions. 2 Passivisation preference of each verb An important factor of voice selection is the preference for how frequently a verb is used in passive sentences. This means each verb has a potential tendency of being used in passive sentences in a domain. For example, the verb ‘yosou-suru (to expect)’ tends to be realised in the passive in the newspaper domain because Japanese journalists tend to write their opinions objectively by omitting the agent role. To take into account this preference of ver"
W13-2118,2002.tmi-papers.15,0,0.0378611,"(Ikehara et al., 1997)) as features. (in this case, the agent filler) of a predicate is exophoric, the passive voice is selected. Coreference and anaphora of arguments As discussed in discourse theories such as Centering Theory (Grosz et al., 1995), arguments which have been already most salient in the preceding context tend to be placed at the beginning of a sentence for reducing the cognitive cost of reading, as argued in Functional Grammar (Halliday and Matthiessen, 2004). In order to consider the characteristic, we employ an extension of Centering Theory (Grosz et al., 1995), proposed by Nariyama (2002) for implementing the COREF type features in Table 1. She proposed a generalised version of the forward looking-center list, called the Salient Reference List (SRL), which stores all salient discourse entities (e.g. NP) in the preceding contexts in the order of their saliency. A highly ranked argument’s entity in the SRL tends to be placed in the subject position, resulting in a passive sentence if that salient entity has a THEME role in the predicateargument structure. To capture this characteristic, the order and rank of discourse entities in the SRL are used as features5 . In addition, as d"
W13-2118,J98-3005,0,0.036923,"paper, we propose an automatic voice selection model based on various linguistic information, ranging from lexical to discourse information. Our empirical evaluation using a manually annotated corpus in Japanese demonstrates that the proposed model achieved 0.758 in F-score, outperforming the two baseline models. 1 The realisation from a semantic representation (e.g. predicate argument structures) to an actual text has been mainly developed in the area of natural language generation (Reiter and Dale, 2000), and has been applied to various NLP applications such as multi-document summarisation (Radev and McKeown, 1998) and tutoring systems (Di Eugenio et al., 2005). During the course of a text generation process, various kinds of decisions should be made, including decisions on textual content, clustering the content of each clause, discourse structure of the clauses, lexical choices, types of referring expressions and syntactic structures. Since these different kinds of decisions are interrelated to each other, it is not a trivial problem to find an optimal order among these decisions. This issue has been much discussed in terms of architecture of generation systems. Although a variety of architectures has"
W13-2118,W94-0319,0,0.0239267,"choices, types of referring expressions and syntactic structures. Since these different kinds of decisions are interrelated to each other, it is not a trivial problem to find an optimal order among these decisions. This issue has been much discussed in terms of architecture of generation systems. Although a variety of architectures has been proposed in the past, e.g. an integrated architecture (Appelt, 1985) and a revision-based architecture (Inui et al., 1994; Robin, 1994), a pipeline architecture is considered as a consensus architecture in which decisions are made in a predetermined order (Reiter, 1994). Voice selection is a syntactic decision that tends to be made in a later stage of the pipeline architecture, even though it influences various decisions, such as discourse structure and lexical choice. Unlike referring expression generation, voice selection has received less attention and been less discussed in the past. Against this background, this Introduction Generating a readable text is the primary goal in natural language generation (NLG). To realise such text, we need to arrange discourse entities (e.g. NPs) in appropriate positions in a sentence according to their discourse salience"
W13-2326,P09-1075,0,0.0200399,"contrast, in semantic and discourse processing, such as coreference resolution and discourse structure analysis, it is not trivial to employ as features deeper linguistic knowledge and human linguistic intuition that are indispensable for these tasks. In order to improve system performance, past attempts have integrated deeper linguistic knowledge through manually constructed linguistic resources such as WordNet (Miller, 1995) and linguistic theories such as Centering Theory (Grosz et al., 1995). They partially succeed in improving performance, but there is still room for further improvement (duVerle and Prendinger, 2009; Ng, 2010; Lin et al., 2010; Pradhan et al., 2012). This paper presents an analysis of an annotator’s behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotator’s behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal use"
W13-2326,P10-1142,0,0.02903,"ourse processing, such as coreference resolution and discourse structure analysis, it is not trivial to employ as features deeper linguistic knowledge and human linguistic intuition that are indispensable for these tasks. In order to improve system performance, past attempts have integrated deeper linguistic knowledge through manually constructed linguistic resources such as WordNet (Miller, 1995) and linguistic theories such as Centering Theory (Grosz et al., 1995). They partially succeed in improving performance, but there is still room for further improvement (duVerle and Prendinger, 2009; Ng, 2010; Lin et al., 2010; Pradhan et al., 2012). This paper presents an analysis of an annotator’s behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotator’s behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal useful inform"
W13-2326,W12-4501,0,0.0170225,"reference resolution and discourse structure analysis, it is not trivial to employ as features deeper linguistic knowledge and human linguistic intuition that are indispensable for these tasks. In order to improve system performance, past attempts have integrated deeper linguistic knowledge through manually constructed linguistic resources such as WordNet (Miller, 1995) and linguistic theories such as Centering Theory (Grosz et al., 1995). They partially succeed in improving performance, but there is still room for further improvement (duVerle and Prendinger, 2009; Ng, 2010; Lin et al., 2010; Pradhan et al., 2012). This paper presents an analysis of an annotator’s behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotator’s behaviour during annotation can be seen as reflecting her/his cognitive process during her/his attempt to understand the text for annotation, analysing the process of text annotation has potential to reveal useful information for NLP tasks, in particular semant"
W13-2326,J95-2003,0,0.158349,"e information like words and their POS within a window of a certain size can be easily employed as useful features. In contrast, in semantic and discourse processing, such as coreference resolution and discourse structure analysis, it is not trivial to employ as features deeper linguistic knowledge and human linguistic intuition that are indispensable for these tasks. In order to improve system performance, past attempts have integrated deeper linguistic knowledge through manually constructed linguistic resources such as WordNet (Miller, 1995) and linguistic theories such as Centering Theory (Grosz et al., 1995). They partially succeed in improving performance, but there is still room for further improvement (duVerle and Prendinger, 2009; Ng, 2010; Lin et al., 2010; Pradhan et al., 2012). This paper presents an analysis of an annotator’s behaviour during her/his annotation process for eliciting useful information for natural language processing (NLP) tasks. Text annotation is essential for machine learning-based NLP where annotated texts are used for both training and evaluating supervised systems. Since an annotator’s behaviour during annotation can be seen as reflecting her/his cognitive process du"
W13-2326,W07-1522,1,0.830245,"Missing"
W13-2326,N09-1059,0,0.0271494,"of arguments and dispersal of eye gaze Existing Japanese corpora annotated with predicate-argument relations (Iida et al., 2007; Kawahara et al., 2002) have had syntactic heads (nouns) of their projected NPs related their predicates. Since Japanese is a head-final language, a head noun is always placed in the last position of an NP. This scheme has the advantage that predicate-argument relations can be annotated without identifying the starting boundary of the argument NP under consideration. The scheme is also reflected in the structure of automatically constructed Japanese case frames, e.g. Sasano et al. (2009), which consist of triplets in the form of hNoun, Case, Verbi. Noun is a head noun extracted from its projected NP in the original text. We followed this scheme in our annotation experiments. However, a head noun of an argument does not always have enough information. A nominaliser which often appears in the head position in an NP does not have any semantic meaning by itself. For instance, in the NP “benkyˆo suru koto (to study/studying)”, the head noun “koto” has no specific semantic meaning, corresponding to an English morpheme “to” or “-ing”. In such cases, inspecting a whole NP including i"
W13-2326,kawahara-etal-2002-construction,0,0.111046,"Missing"
W13-2326,W13-0508,1,0.832708,"ntity annotation task, our annotation task, annotating predicate-argument relations, is more complex. In addition, our experimental setting is more natural, meaning that all possible relations in a text were annotated in a single session, while each session targeted a single named entity (NE) in a limited context in the setting of Tomanek et al. (2010). Finally, our fixation target is more precise, i.e. words, rather than a coarse area around the target NE. We have also discussed evaluating annotation difficulty for predicate-argument relations by using the same data introduced in this paper (Tokunaga et al., 2013). Through manual analysis of the collected data, we suggested that an annotation time necessary for annotating a single predicateargument relation was correlated with the agreement ratio among multiple human annotators. tions in Japanese texts. The collected data were analysed from three aspects: (i) the relationship of predicate-argument distances and argument’s cases, (ii) the effect of already-existing links and (iii) specificity of arguments and dispersal of eye gaze. The analysis on these aspects suggested that obtained insight into human annotation behaviour could be useful for exploring"
W13-2326,P10-1118,0,0.391111,"tic knowledge. Particularly we focus on annotator eye gaze during annotation. Because of recent developments in eye-tracking technology, eye gaze data has been widely used in various research fields, including psycholinguistics and problem solving (Duchowski, 2002). There have been a number of studies on the relations between eye gaze and language comprehension/production (Griffin and Bock, 2000; Richardson et al., 2007). Compared to the studies on language and eye gaze, the role of gaze in general problem solving settings has been less studied (Bednarik and Tukiainen, 2008; Rosengrant, 2010; Tomanek et al., 2010). Since our current interest, text annotation, can be considered a problem solving as well as language comprehension task, we refer to them when defining our probIntroduction Text annotation is essential for machine learning (ML)-based natural language processing (NLP) where annotated texts are used for both training and evaluating supervised systems. This annotation-then-learning approach has been broadly applied to various NLP tasks, ranging from shallow processing tasks, such as POS tagging and NP chunking, to tasks requiring deeper linguistic information, such as coreference resolution and"
W13-2326,maekawa-etal-2010-design,0,\N,Missing
W13-4303,J08-4004,0,0.114752,"nt using Eye Gaze Information Koh Mitsuda Ryu Iida Takenobu Tokunaga Department of Computer Science, Tokyo Institute of Technology {mitsudak,ryu-i,take}@cl.cs.titech.ac.jp Abstract ing better annotation tools (Kaplan et al., 2012; Lenzi et al., 2012; Marci´nczuk et al., 2012). The assessment of annotation quality is also an important issue in corpus building. The annotation quality is often evaluated with the agreement ratio among annotation results by multiple independent annotators. Various metrics for measuring reliability of annotation have been proposed (Carletta, 1996; Passonneau, 2006; Artstein and Poesio, 2008; Fort et al., 2012), which are based on inter-annotator agreement. Unlike these past studies, we look at annotation processes rather than annotation results, and aim at eliciting useful information for NLP through the analysis of annotation processes. This is in line with Behaviour mining (Chen, 2006) instead of data mining. There is few work looking at the annotation process for assessing annotation quality with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze during her annotation process. They concluded tha"
W13-4303,J96-2004,0,0.0311936,"ting Missing Annotation Disagreement using Eye Gaze Information Koh Mitsuda Ryu Iida Takenobu Tokunaga Department of Computer Science, Tokyo Institute of Technology {mitsudak,ryu-i,take}@cl.cs.titech.ac.jp Abstract ing better annotation tools (Kaplan et al., 2012; Lenzi et al., 2012; Marci´nczuk et al., 2012). The assessment of annotation quality is also an important issue in corpus building. The annotation quality is often evaluated with the agreement ratio among annotation results by multiple independent annotators. Various metrics for measuring reliability of annotation have been proposed (Carletta, 1996; Passonneau, 2006; Artstein and Poesio, 2008; Fort et al., 2012), which are based on inter-annotator agreement. Unlike these past studies, we look at annotation processes rather than annotation results, and aim at eliciting useful information for NLP through the analysis of annotation processes. This is in line with Behaviour mining (Chen, 2006) instead of data mining. There is few work looking at the annotation process for assessing annotation quality with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze duri"
W13-4303,W06-0602,0,0.0269414,"tor for detecting the MADs. 1 Introduction Over the last two decades, with the development of supervised machine learning techniques, annotating texts has become an essential task in natural language processing (NLP) (Stede and Huang, 2012). Since the annotation quality directly impacts on performance of ML-based NLP systems, many researchers have been concerned with building high-quality annotated corpora at a lower cost. Several different approaches have been taken for this purpose, such as semi-automating annotation by combining human annotation and existing NLP tools (Marcus et al., 1993; Chou et al., 2006; Rehbein et al., 2012; Voutilainen, 2012), implement19 International Joint Conference on Natural Language Processing, pages 19–26, Nagoya, Japan, 14-18 October 2013. while her counterpart correctly annotates it. We call this type of disagreement missing annotation disagreement (MAD). MADs were excluded from our previous analysis. Estimating MADs from the behaviour of a single annotator would be useful in a situation where only a single annotator is available. Against this background, we tackle a problem of detecting MADs based on both linguistic information of annotation targets and annotator"
W13-4303,fort-etal-2012-analyzing,0,0.0231814,"Missing"
W13-4303,bartalesi-lenzi-etal-2012-cat,0,0.0522053,"Missing"
W13-4303,voutilainen-2012-improving,0,0.0291368,"n Over the last two decades, with the development of supervised machine learning techniques, annotating texts has become an essential task in natural language processing (NLP) (Stede and Huang, 2012). Since the annotation quality directly impacts on performance of ML-based NLP systems, many researchers have been concerned with building high-quality annotated corpora at a lower cost. Several different approaches have been taken for this purpose, such as semi-automating annotation by combining human annotation and existing NLP tools (Marcus et al., 1993; Chou et al., 2006; Rehbein et al., 2012; Voutilainen, 2012), implement19 International Joint Conference on Natural Language Processing, pages 19–26, Nagoya, Japan, 14-18 October 2013. while her counterpart correctly annotates it. We call this type of disagreement missing annotation disagreement (MAD). MADs were excluded from our previous analysis. Estimating MADs from the behaviour of a single annotator would be useful in a situation where only a single annotator is available. Against this background, we tackle a problem of detecting MADs based on both linguistic information of annotation targets and annotator eye gaze. In our approach, the eye gaze d"
W13-4303,marcinczuk-etal-2012-inforex,0,0.0429848,"Missing"
W13-4303,J93-2004,0,0.0416852,"ould be a good indicator for detecting the MADs. 1 Introduction Over the last two decades, with the development of supervised machine learning techniques, annotating texts has become an essential task in natural language processing (NLP) (Stede and Huang, 2012). Since the annotation quality directly impacts on performance of ML-based NLP systems, many researchers have been concerned with building high-quality annotated corpora at a lower cost. Several different approaches have been taken for this purpose, such as semi-automating annotation by combining human annotation and existing NLP tools (Marcus et al., 1993; Chou et al., 2006; Rehbein et al., 2012; Voutilainen, 2012), implement19 International Joint Conference on Natural Language Processing, pages 19–26, Nagoya, Japan, 14-18 October 2013. while her counterpart correctly annotates it. We call this type of disagreement missing annotation disagreement (MAD). MADs were excluded from our previous analysis. Estimating MADs from the behaviour of a single annotator would be useful in a situation where only a single annotator is available. Against this background, we tackle a problem of detecting MADs based on both linguistic information of annotation ta"
W13-4303,passonneau-2006-measuring,0,0.0324465,"otation Disagreement using Eye Gaze Information Koh Mitsuda Ryu Iida Takenobu Tokunaga Department of Computer Science, Tokyo Institute of Technology {mitsudak,ryu-i,take}@cl.cs.titech.ac.jp Abstract ing better annotation tools (Kaplan et al., 2012; Lenzi et al., 2012; Marci´nczuk et al., 2012). The assessment of annotation quality is also an important issue in corpus building. The annotation quality is often evaluated with the agreement ratio among annotation results by multiple independent annotators. Various metrics for measuring reliability of annotation have been proposed (Carletta, 1996; Passonneau, 2006; Artstein and Poesio, 2008; Fort et al., 2012), which are based on inter-annotator agreement. Unlike these past studies, we look at annotation processes rather than annotation results, and aim at eliciting useful information for NLP through the analysis of annotation processes. This is in line with Behaviour mining (Chen, 2006) instead of data mining. There is few work looking at the annotation process for assessing annotation quality with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze during her annotation"
W13-4303,W13-0508,1,0.748029,"with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze during her annotation process. They concluded that the annotation difficulty depended on the semantic and syntactic complexity of the annotation targets, and the estimated difficulty would be useful for selecting training data for active learning techniques. We also reported an analysis of relations between a necessary time for annotating a single predicate-argument relation in Japanese text and the agreement ratio of the annotation among three annotators (Tokunaga et al., 2013). The annotation time was defined based on annotator actions and eye gaze. The analysis revealed that a longer annotation time suggested difficult annotation. Thus, we could estimate annotation quality based on the eye gaze and actions of a single annotator instead of the annotation results of multiple annotators. Following up our previous work (Tokunaga et al., 2013), this paper particularly focuses on a certain type of disagreement in which an annotator misses annotating a predicate-argument relation This paper discusses the detection of missing annotation disagreements (MADs), in which an a"
W13-4303,P10-1118,0,0.281943,"independent annotators. Various metrics for measuring reliability of annotation have been proposed (Carletta, 1996; Passonneau, 2006; Artstein and Poesio, 2008; Fort et al., 2012), which are based on inter-annotator agreement. Unlike these past studies, we look at annotation processes rather than annotation results, and aim at eliciting useful information for NLP through the analysis of annotation processes. This is in line with Behaviour mining (Chen, 2006) instead of data mining. There is few work looking at the annotation process for assessing annotation quality with a few exceptions like Tomanek et al. (2010), which estimated difficulty of annotating named entities by analysing annotator eye gaze during her annotation process. They concluded that the annotation difficulty depended on the semantic and syntactic complexity of the annotation targets, and the estimated difficulty would be useful for selecting training data for active learning techniques. We also reported an analysis of relations between a necessary time for annotating a single predicate-argument relation in Japanese text and the agreement ratio of the annotation among three annotators (Tokunaga et al., 2013). The annotation time was d"
W13-4303,maekawa-etal-2010-design,0,\N,Missing
W16-5401,P10-1128,1,0.825071,"In this paper, we assume that there are multiple objects on the 2 2D plane, and the worker is asked to place the objects at specified locations according to instructions by the instructor. Therefore, the worker needs to infer the location to place the object, and it also needs to infer the direction that the object faces to. The spatial placement problem can be broken down into the following three steps. 1. Identifying the object The worker needs to identify the object to be manipulated in the instructional utterance. This task is regarded as the reference resolution in a multimodal setting (Iida et al., 2010; Prasov and Chai, 2010). 2. Deciding the specified location The worker needs to decide the location where the target object should be placed. This is also considered as resolving referring expression, but the referent is a location instead of an object. The spatial referring expressions include expressions such as “next to a triangle”, “about one meter right of the bed”, and “the centre of the room”. Those expressions often specify the location in terms of the spatial relations between the target object and the other objects, often called the reference object or landmark (Coventry and Garrod,"
W16-5401,mani-etal-2008-spatialml,0,0.665277,"rget object to move, the location at which the target object should be placed and often the direction of the target object itself. We call this kind of task the spatial placement problem, namely the task placing an specified object at a specified location with a specified direction according to a natural language instruction. As a necessary first step to realising a computer agent that is capable of dealing with the spatial placement problem, the present paper proposes an annotation scheme to represent spatial relations by extending an existing scheme. In order to represent spatial relations, Mani et al. (2008) proposed an annotation scheme that annotates spatial objects and relations between them. However, their scheme does not handle object direction. In the above example, the Mani’s scheme annotates the spatial relation “next to” between the two objects “the refrigerator” and “the sink”, but does not annotate the direction of the refrigerator specified by “to this side”. As long as using their annotation scheme, the annotated corpus lacks the information of the object direction. When taking a machine learning approach with the annotated corpus to deal with the spatial placement problem, annotatin"
W16-5401,D10-1046,0,0.0225085,"assume that there are multiple objects on the 2 2D plane, and the worker is asked to place the objects at specified locations according to instructions by the instructor. Therefore, the worker needs to infer the location to place the object, and it also needs to infer the direction that the object faces to. The spatial placement problem can be broken down into the following three steps. 1. Identifying the object The worker needs to identify the object to be manipulated in the instructional utterance. This task is regarded as the reference resolution in a multimodal setting (Iida et al., 2010; Prasov and Chai, 2010). 2. Deciding the specified location The worker needs to decide the location where the target object should be placed. This is also considered as resolving referring expression, but the referent is a location instead of an object. The spatial referring expressions include expressions such as “next to a triangle”, “about one meter right of the bed”, and “the centre of the room”. Those expressions often specify the location in terms of the spatial relations between the target object and the other objects, often called the reference object or landmark (Coventry and Garrod, 2004). 3. Deciding the"
W16-5401,E99-1023,0,0.209975,"al Signal Direction Signal Total 850 * Our proposal elements are underlined. number 270 1,420 126 101 81 62 2,060 Table 2: Distribution of annotated tags 1. identifying the spans to be assigned the entity tags in Table 1, and the Spatial Signal and Direction Signal tags, and 2. identifying the rest of the relations in Table 1 by linking the spans identified in step 1. In the following subsections, each of the steps is described in more detail. 5.1 Identifying spans for entity tags Considering the span identification for a certain tag as a sequential labelling problem, we adopt the IOB2 model (Tjong et al., 1999) to identify the tagged span. We employed the CRF++2 implementation to conduct sequential labelling. We prepared the labelling program for each tag and ran them in parallel. Thus each tag has its own I-O-B labels. Table 3 shows correct labelling for the instruction “Rotate it so that the right angle comes down.”. tag Spatial Entity Location Motion Part Spatial Signal Direction Signal Rotate O O B O O O it B O O O O O so O O O O O B that O O O O O I the O O O B O I right O O O I O I angle O O O I O I comes O O O O O I down O O O O O I Table 3: Example of entity tag labelling Given an instructio"
W16-5401,tokunaga-etal-2012-rex,1,0.937894,"notated in the sentence. ISO-Space, however, does not have a tag for representing the direction of an object itself neither. Since ISO-Space has an advantage over SpatialML that it can represent events and changes in spatial relations, we extend the ISO-Space scheme by introducing tags that describe object intrinsic direction, namely the direction that the object faces to. This kind of tags play an important role in the spatial placement problem as we saw in the previous section. There have been several attempts of constructing corpora related to the spatial placement problem. The REX corpus (Tokunaga et al., 2012) and the PentoRef corpus (Zarrieß et al., 2016) are the examples of this sort. Both corpora were collected through situated dialogues in which dialogue participants jointly solved geometric puzzles such as Tangram and Pentomino. The main goal of the dialogues is placing puzzle pieces in the right places, thus, these tasks are the typical spatial placement problem. These corpora come with the visual information that is updated during the course of dialogues, thus they include the spatial information of the objects. However, the transcribed utterances were not annotated with spatial information"
W16-5401,H89-1033,0,0.141973,"ormation that is updated during the course of dialogues, thus they include the spatial information of the objects. However, the transcribed utterances were not annotated with spatial information corresponding to the object direction. To our knowledge, there is no corpus that is linguistically annotated with spatial information including both object location and direction. Our attempt compensates for these missing information in the corpora for the spatial placement problem. Winograd’s SHRDLU is the first and seminal working system that is capable of dealing with the spatial placement problem (Winograd, 1972). SHRDLU could understand natural language questions and instructions on a virtual block world, and could manipulate various kinds of blocks to change the state of the block world. More recently, Tellex et al. (2011) realised a SHRDLU-like system in the real environment. They proposed Generalised Grounding Graphs to infer corresponding plans to linguistic instructions. They collected possible expressions of the instruction through crowdsourcing to construct a corpus which is used to train the inference model. However, SHRDLU nor the Tellex’s system do not care about the direction of manipulate"
W16-5401,L16-1019,0,0.0164757,"s not have a tag for representing the direction of an object itself neither. Since ISO-Space has an advantage over SpatialML that it can represent events and changes in spatial relations, we extend the ISO-Space scheme by introducing tags that describe object intrinsic direction, namely the direction that the object faces to. This kind of tags play an important role in the spatial placement problem as we saw in the previous section. There have been several attempts of constructing corpora related to the spatial placement problem. The REX corpus (Tokunaga et al., 2012) and the PentoRef corpus (Zarrieß et al., 2016) are the examples of this sort. Both corpora were collected through situated dialogues in which dialogue participants jointly solved geometric puzzles such as Tangram and Pentomino. The main goal of the dialogues is placing puzzle pieces in the right places, thus, these tasks are the typical spatial placement problem. These corpora come with the visual information that is updated during the course of dialogues, thus they include the spatial information of the objects. However, the transcribed utterances were not annotated with spatial information corresponding to the object direction. To our k"
W17-2410,J08-1001,0,0.142016,"elopment of a text, while lexical and semantic continuity is also indispensable for coherent text (Feng et al., 2014). Fifth, it is easier to read a coherent text than its less coherent counterpart (Garing, 2014). Thus when writing a text, it is not enough to only revise the text with careful editing and proofreading from the lexical, or grammatical aspect. Coherence aspect also should be taken into account in revising the text (Bamberg, 1983; Garing, 2014). There are studies on computational modelling of text coherence based on the supervised learning approach, such as the Entity Grid model (Barzilay and Lapata, 2008). The Entity Grid model has been further extended into the Role Matrix model (Lin et al., 2011; Feng et al., 2014). However, these models have a few drawbacks. First, Coherence is a crucial feature of text because it is indispensable for conveying its communication purpose and meaning to its readers. In this paper, we propose an unsupervised text coherence scoring based on graph construction in which edges are established between semantically similar sentences represented by vertices. The sentence similarity is calculated based on the cosine similarity of semantic vectors representing sentence"
W17-2410,P11-2022,0,0.0604154,"Missing"
W17-2410,D14-1162,0,0.0836486,"ional cost for training which often arise in the supervised approach. We encode a text into a graph G(V, E), where V is a set of vertices and E is a set of edges in the graph. The vertex vi ∈ V represents the i-th sentence si in the text, and the weighted directed edge ei,j ∈ E represents a semantic relation from the i-th to the j-th sentences. In what follows, the term “edge” refers to the weighted directed edge. As stated by Halliday and Hasan (1976), cohesion is a matter of lexicosemantics. Our method projects a sentence into a vector representation using pre-trained GloVe word vectors1 by Pennington et al. (2014). A sentence consists of multiple words {w1 , w2 , · · · , wM } where each of them is mapped into a vector space, i.e. ~1 , w ~ 2, · · · , w ~ M }. A sentence s can be encoded {w as a vector ~s by taking the average of consisting word vectors. Formally, a sentence vector ~s is described as M 1 X ~s = ~ k, w M Figure 4: Graph construction algorithm with similarity of PAV on information provided in the preceding part. When they do not understand a particular part, people look backwards for what they have missed. We mimic this reading process into graph construction that is reflected in the algor"
W17-2410,J86-3001,0,0.590765,".w.aa@m.titech.ac.jp Abstract First, a text is coherent if it can convey its communication purpose and meaning to its readers (Wolf and Gibson, 2005; Somasundaran et al., 2014; Feng et al., 2014). Second, a text needs to be integrated as a whole, rather than a series of independent sentences (Bamberg, 1983; Garing, 2014). It means that sentences in the text are centralised around a certain theme or topic, and are arranged in a particular order in terms of logical, spatial, and temporal relations. Third, every sentence in a coherent text has relation(s) to each other (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005). It suggests that a text exhibits discourse/rhetorical relation and cohesion. Fourth, text coherence is greatly influenced by the presence of a certain organisation in the text (Persing et al., 2010; Somasundaran et al., 2014). The organisation helps readers to anticipate the upcoming textual information. Although a wellorganised text is highly probable to be coherent, only the organisation does not constitute coherence. Textual organisation concerns the structural formation and logical development of a text, while lexical and semantic continui"
W17-2410,J95-2003,0,0.762885,"Missing"
W17-2410,D10-1023,0,0.0520484,"Missing"
W17-2410,P13-1010,0,0.351596,": Entity Grid example Entity Grid using co-reference resolution has a bias towards the original ordering of text when comparing a text with its permutated counterparts. The co-reference resolution module is trained on well-formed texts; thus it does not perform very well for ill-organised texts. The methods utilising a discourse parser for modelling text coherence (Lin et al., 2011; Feng et al., 2014) have the same problem. Second, the supervised model often suffers from data sparsity, domain dependence, and computational cost for training. To alleviate these problems in the supervised model, Guinaudeau and Strube (2013) proposed an unsupervised coherence model known as the Entity Graph model. The Entity Grid, Role Matrix, and Entity Graph model assumed coherence was achieved by local cohesion, i.e. repeated mentions of the same entities constitute cohesion. However, they did not capture the contribution of related-yet-notidentical entities (Petersen et al., 2015). To our best knowledge, the closest study addressing this problem was done by Li and Hovy (2014). The key idea of Li and Hovy (2014) is to learn a distributed sentence representation which captures the underlying semantic relations between consecuti"
W17-2410,D14-1218,0,0.0296737,"Missing"
W17-2410,C14-1090,0,0.017833,"ndependent sentences (Bamberg, 1983; Garing, 2014). It means that sentences in the text are centralised around a certain theme or topic, and are arranged in a particular order in terms of logical, spatial, and temporal relations. Third, every sentence in a coherent text has relation(s) to each other (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Mann and Thompson, 1988; Wolf and Gibson, 2005). It suggests that a text exhibits discourse/rhetorical relation and cohesion. Fourth, text coherence is greatly influenced by the presence of a certain organisation in the text (Persing et al., 2010; Somasundaran et al., 2014). The organisation helps readers to anticipate the upcoming textual information. Although a wellorganised text is highly probable to be coherent, only the organisation does not constitute coherence. Textual organisation concerns the structural formation and logical development of a text, while lexical and semantic continuity is also indispensable for coherent text (Feng et al., 2014). Fifth, it is easier to read a coherent text than its less coherent counterpart (Garing, 2014). Thus when writing a text, it is not enough to only revise the text with careful editing and proofreading from the lex"
W17-2410,P11-1100,0,0.391393,"et al., 2014). Fifth, it is easier to read a coherent text than its less coherent counterpart (Garing, 2014). Thus when writing a text, it is not enough to only revise the text with careful editing and proofreading from the lexical, or grammatical aspect. Coherence aspect also should be taken into account in revising the text (Bamberg, 1983; Garing, 2014). There are studies on computational modelling of text coherence based on the supervised learning approach, such as the Entity Grid model (Barzilay and Lapata, 2008). The Entity Grid model has been further extended into the Role Matrix model (Lin et al., 2011; Feng et al., 2014). However, these models have a few drawbacks. First, Coherence is a crucial feature of text because it is indispensable for conveying its communication purpose and meaning to its readers. In this paper, we propose an unsupervised text coherence scoring based on graph construction in which edges are established between semantically similar sentences represented by vertices. The sentence similarity is calculated based on the cosine similarity of semantic vectors representing sentences. We provide three graph construction methods establishing an edge from a given vertex to a p"
W17-2410,C16-1094,0,0.0362076,"Missing"
W17-2410,J05-2005,0,0.122459,"Missing"
W17-2410,P14-5010,0,0.00865015,"Missing"
W17-2410,N09-3014,0,0.0459832,"Missing"
W17-2410,C14-1089,0,\N,Missing
W17-5008,C16-1107,0,0.0296216,"nts. Twentythree students were instructed to write essays and then to assess the trigger questions if these questions could improve their essays. Because the machine-generated trigger questions were created based on the collected student essays, their analysis showed that they were effective only for the collected student essays while the human-made trigger questions were effective for general essays as well as the collected essays. Zhang and VanLehn (2016) employed students to rate machine-generated questions and humanmade questions based on relevance, fluency, ambiguity, pedagogy and depth. Araki et al. (2016) evaluated their question generation system by judging the questions on three metrics: grammatical correctness, answer existence and inference steps. This study provides a detailed analysis of evaluation of English pronoun reference questions which are created automatically by machine. Pronoun reference questions are multiple choice questions that ask test takers to choose an antecedent of a target pronoun in a reading passage from four options. The evaluation was performed from two perspectives: the perspective of English teachers and that of English learners. Item analysis suggests that mach"
W17-5008,J95-2003,0,0.169766,"by the relative pronoun in the original sentence. For instance, the text (2) is derived from the text (1) by extracting the nonrestrictive relative clause (underlined part) and replacing the relative pronoun “which” with a pronoun “it”. The antecedent of “it” in the third sentence looks to be “legend”, a subject in the previous sentence. But it should be “knowledge” in the previous sentence when we look at the original sentence where “which”, the counterpart of “it” in (2), obviously refers to “knowledge”. To exclude such spurious anaphora, we apply the Centering theory (Brennan et al., 1987; Grosz et al., 1995) to see the introduced pronoun refers to the same antecedent as in the original sentence. In this particular example, the Centering theory tells us that “legend” in the second sentence of (2) has a higher status than “knowledge” because the former is a 1 subject and the latter is an element in the prepositional phrase. Thus “legend” is a more probable antecedent of “it”, which contradicts the original sentence of (1). (1) The church of S. Croce has seen another strange death of a Pope, that of Sylvester II. (999-1003), a Frenchman, Gerbert by name. A legend, related first by cardinal Benno in"
W17-5008,P16-1056,0,0.0625472,"Missing"
W17-5008,P87-1022,0,\N,Missing
W17-5103,C16-1324,0,0.052201,"Missing"
W17-5103,W15-0501,0,0.102812,"ing was measured at K=0.69 (N =9879; n=4, k=2) and source agreement is given in Table 5. The baseline results are given in Table 6. The Near29 non-agreed links still share linking structure. We observe that often the same set of source spans are linked to some destination span, although the destination itself is different across annotators. Our agreement metrics are thus underestimating the degree of shared linkage structure. 6 of a text. There has also been some recent work on agreement metrics for argument relations. As far as agreement on detection of argumentative components is concerned, Kirschner et al. (2015) point out that measures such as kappa and F1 score may cause some inappropriate penalty for slight differences of annotation between annotators, and proposed a graph-based metric based on pair-wise comparison of predefined argument components. This particular metric, while addressing some of the problems of kappa and F1, is not directly applicable to our annotation where annotators can freely chose the beginnings and ends of spans. Duthie et al. (2007) introduce CASS, a further, very recent adaptation of the metric by Kirschner et al. that can deal with disagreement in segmentation. However,"
W17-5103,C14-1142,0,0.0602475,"tation detection, achieving interannotator agreement of K=0.75 in Cohen’s Kappa (ECHR). On a genre other than legal text, Faulkner (2014) annotated student essays using three tags (“for”, “against” and “neither”), reaching interannotator agreement of K=0.70 (Cohen). As far as the rhetorical status classification part of our scheme is concerned, the closest approach to ours is Al Khatib et al. (2016), but they do not employ any explicit links, and they work on a different genre (news editorials). A task related to our linking steps is the determination of relations between argument components. Stab and Gurevych (2014) annotated argumentative relations (support and attack) in essays; they reported inter-annotator agreement of K=0.81 (Fleiss) for both support and attack. Hua and Wang (2017) proposed an annotation scheme for labeling sentence-level supporting arguments relations with four types (STUDY, FACTUAL, OPINION, REASONING). Their results for argument type classification are as follows: K=0.61 for STUDY, K=0.75 for FACTUAL, K=0.71 for OPINION, and K=0.29 for REASONING. However, these two relation-based studies discover only one aspect of argument structure, whereas our combination of linking tasks and"
W17-5103,J02-4002,1,0.549369,"egligent or not”. While full Issue Topic and FRAMING linking annotation would allow very sophisticated summaries, our fall-back strategy is to create simpler summaries using only the final conclusion and the main supporting text such as judicial decisions. For these simple summaries, performing only automatic rhetorical status classification is sufficient. 2.3 Rhetorical structure To exploit the idea that argument structure is a crucial aspect for legal summarization, we take a rhetorical status based approach. This method was originally defined for the summarization of scientific articles by Teufel and Moens (2002), but later studies such as Hachey and Grover (2006) applied the rhetorical status approach to the legal text domain for the context of English law. Hachey and Grover defined the following seven labels: In English law, the judgment first states the facts and events, corresponding to category “FACT”. “PROCEEDINGS” labels sentences which restate the details of previous judgments in lower courts. “BACKGROUND” is the label for quotations or citations of law materials which Law Lords use to discuss precedents and legislation. “FRAMING” is a rhetorical role that captures all aspects of the Law Lord’"
W17-5103,P17-2032,0,0.0197756,"(“for”, “against” and “neither”), reaching interannotator agreement of K=0.70 (Cohen). As far as the rhetorical status classification part of our scheme is concerned, the closest approach to ours is Al Khatib et al. (2016), but they do not employ any explicit links, and they work on a different genre (news editorials). A task related to our linking steps is the determination of relations between argument components. Stab and Gurevych (2014) annotated argumentative relations (support and attack) in essays; they reported inter-annotator agreement of K=0.81 (Fleiss) for both support and attack. Hua and Wang (2017) proposed an annotation scheme for labeling sentence-level supporting arguments relations with four types (STUDY, FACTUAL, OPINION, REASONING). Their results for argument type classification are as follows: K=0.61 for STUDY, K=0.75 for FACTUAL, K=0.71 for OPINION, and K=0.29 for REASONING. However, these two relation-based studies discover only one aspect of argument structure, whereas our combination of linking tasks and a rhetorical status classification task means that we address the global hierarchical argument structure 7 Discussion and future work It is hard to evaluate a newly defined,"
W17-5103,W16-2805,0,\N,Missing
W19-4436,L18-1511,0,0.0192917,"could be a problem in analyzing student summaries, particularly those written by second language learners, due to grammatical errors and punctuation. In Applied Linguistics, IUs have been employed for in-depth analyses of the content of student summaries in the second language learning and assessment literature (Johns and Mayes, 1990). Accordingly, adopting the IU enables us to interpret our study results in reference to such previous investigations of summary content. the summary. Automatic summary evaluation tools based on the Pyramid approach, such as PEAK (Yang et al., 2016) and PyrEval (Gao et al., 2018), are not suitable in educational environments as we cannot expect a number of reliable summaries large enough to certify a proper weighting. In addition, the quality of summaries is not guaranteed due to insufficient student proficiency in comprehension or composition. Their summaries might overlook obscure yet paramount information. These facts lead to imprecise SCU weighting. Lastly, writing a Gold Standard summary is a time-consuming task; therefore we are forced to compare the summaries against the source text directly. FRESA (Torres-Moreno et al., 2010) is a framework for the evaluation"
W19-4436,D14-1162,0,0.0821516,"sentences would be more versatile for making a correspondence between the summaries and source text. Foster et al. (2000) analyzed several segmentation units from the viewpoint of intonation, syntax and semantics. For our purpose, we consider three kinds of syntactic units: IU, T-Unit (Hunt, 1965, Ranking Method To link two corresponding IUs across the summary and source text, we calculate the similarity between the units based on word embedding. A vector representing an IU is constructed by averaging the vector representation of the words appearing in the unit. We use the GloVe word vectors (Pennington et al., 2014) that have been pretrained with the Wikipedia + Gigaword data. We ignored the words that are not included in the word vector model when constructing the IU vector. We call an IU in a summary “Summary IU” and one in the source text “Source IU” hereafter. Given a Summary IU, its cosine similarity to every Source IU is calculated to create a ranking list of Source IUs that are arranged in descending order of similarity. We called this list “Prediction Ranking”. As a baseline, we use ROUGE-1, ROUGE2 and ROUGE-L-based rankings. We selected ROUGE as it has proven to be effective in evaluating short"
W19-4436,W04-1013,0,0.295777,"phrased expressions than the conventional ROUGEbased baselines. Also, the proposed method outperformed the baselines in recall. We implemented the proposed method in a GUI tool “Segment Matcher” that aids teachers to establish a link between corresponding IUs across the summary and source text. 1 take@c.titech.ac.jp 2 Related Work Evaluation is one of the important aspects of the automated text summarization research (Lin and Hovy, 2003). BLEU (Papineni et al., 2002) delivers a similarity score by analyzing n-grams that appear both in the source and summary texts in terms of precision. ROUGE (Lin, 2004) expands on BLEU by providing recall-oriented statistics with n-grams and Longest Common Subsequence. As these measures are based on string matching of n-grams, they fail in making a correspondence between rephrased expressions. The Pyramid approach (Passonneau, 2009) divides the texts into text fragments named Summary Content Units (SCU). Assuming a set of summaries for a source text, SCUs are weighted based on their frequency over the summary set. The rationale is that frequent SCUs contain important ideas. The score of a summary is calculated by summing up the weight of every SCU in Introdu"
W19-4436,N03-1020,0,0.473024,"Missing"
W96-0105,P91-1034,0,0.0225877,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,J94-4003,0,0.0277881,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,1993.mtsummit-1.10,0,0.0328193,"is small in both figure l l - a and ll-b. However, in the situation as in figure ll-b, since (a) the task of distinction between the verb senses 1 and 2 is easier, and (b) instances where the sense ambiguity of case fillers corresponds to distinct verb senses will be rare, training using either ""xl"" or ""x2"" will be less effective than as in figure ll-a. It should also be noted that since Bunruigoihyo is a relatively small-sized thesaurus and does not enumerate many word senses, this problem is not critical in our case. However, given other existing thesauri like the EDR electronic dictionary [4] or WordNet [15], these two situations should be strictly differentiated. 6 Conclusion In this paper we proposed an example sampling method for example-based verb sense disambiguation. We also reported on the system&apos;s performance by way of experiments. The experiments showed that our method, which is based on the notion of training utility, has reduced the overhead for the training of the system, as well as the size of the database. As pointed out in section 1, the generalization of examples [8, 19] is another method for reducing the size of the database. Whether coupling these two methods wou"
W96-0105,C96-1012,1,0.79323,"x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for each x, the system simply needs to compare it with the newly computed score between x and the newly stored"
W96-0105,P92-1032,0,0.0179473,"cles. Each of the sentences in the training/test data used 64 in our experiment contained one or several complement(s) followed by one of the ten verbs enumerated in table 2. In table 2, the column of ""English gloss"" describes typical English translations of the Japanese verbs. The column of "" # of sentences"" denotes the number of sentences in the corpus, "" # of senses"" denotes the number of verb senses based on IPAL, and ""lower bound"" denotes the precision gained by using a naive method, where the system systematically chooses the most frequently appearing interpretation in the training data [6]. Table 2: The corpus used for the experiments verb II English gloss ataeru kakeru kuwaeru noru osameru tsukuru torn umu wakaru yameru total [I # of sentences # of senses lower bound give hang add ride govern make take bear offspring understand stop 136 160 167 126 108 126 84 90 60 54 4 29 5 10 8 15 29 2 5 2 66.9 25.6 53.9 45.2 25.0 19.8 26.2 81.1 48.3 59.3 -- 1111 -- 43.7 We at first estimated the system&apos;s performance by its precision, that is the ratio of the number of correct outputs, compared to the number of inputs. In this experiment, we set = 0.5 in equation (7), and k = 1 in equation ("
W96-0105,C92-2101,0,0.216335,"with respect to the correctness of the answer. Dagan et al. proposed a committee-based sampling method, which is currently applied to HMM training for part-of-speech tagging [2]. This method selects samples based on the training utility factor of the examples, i.e. the informativity of the data with respect to future training. However, as all these methods are implemented for statistics-based models, there is a need to explore how to formalize and map these concepts into the examplebased approach. With respect to problem 3, a possible solution would be the generalization of redundant examples [8, 19]. However, such an approach implies a significant overhead for the manual training of each example prior to the generalization. This shortcoming is precisely what our approach allows to avoid: reducing both the overhead as well as the size of the database. Section 2 briefly describes our method for a verb sense disambiguation system. The next Section 3 elaborates on the example sampling method, while section 4 reports on the results of our experiment. Before concluding in section 6, discussion is added in section 5. 57 2 E x a m p l e - b a s e d verb sense disambiguation s y s t e m suri kano"
W96-0105,C94-1049,0,0.0253196,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,P95-1026,0,0.0470033,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,C92-2107,0,\N,Missing
W97-0803,A92-1013,0,0.0584445,"Missing"
W97-0803,P93-1024,0,0.251123,"Missing"
W97-0803,P93-1023,0,0.0593941,"Missing"
W97-0803,P90-1034,0,0.136806,"Missing"
W97-0803,A94-1027,1,0.887039,"Missing"
W97-0803,C96-2161,0,0.149207,"Missing"
W97-0803,C96-2212,0,0.0605532,"Missing"
W97-0803,C92-2070,0,0.10255,"Missing"
W97-0803,P92-1053,0,\N,Missing
W97-0807,P92-1032,0,0.0422726,"Missing"
W97-0807,C94-2119,0,0.0313115,"Missing"
W97-0807,P90-1034,0,0.0911779,"Missing"
W97-0807,C96-2104,0,0.0249835,"Missing"
W97-0807,P94-1038,0,0.0591022,"Missing"
W97-0807,P93-1024,0,0.124804,"Missing"
W97-0807,J98-4002,1,0.85571,"Missing"
W97-0807,C96-1012,1,\N,Missing
W98-0704,W97-0809,0,0.0240824,"results in a significant improvement of information retrieval performance. 1 Introduction Development of WordNet began in 1985 at Princeton University (Miller, 1990). A team lead by Prof. George Miller aimed to create a source of lexical knowledge whose organization would reflect some of the recent findings of psycholinguistic research into the human lexicon. WordNet has been used in numerous natural language processing, such as part of speech tagging (Segond et al., 97), word sense disambiguation (Resnik, 1995), text categorization (Gomez-Hidalgo and Rodriguez, 1997), information extraction (Chai and Biermann, 1997), and so on with considerable success. However the usefulness of WordNet in information retrieval applications has been debatable. Information retrieval is concerned with locating documents relevant to a user&apos;s information needs from a collection of documents. The user describes his/her information needs with a query which consists of a number of words. The information retrieval system compares the query with documents in the collection and returns the documents that are likely to satisfy the user&apos;s information requirements. A fundamental weakness of current information retrieval methods is th"
W98-0704,W97-0806,0,0.0137169,"information retrieval test collections show that our method results in a significant improvement of information retrieval performance. 1 Introduction Development of WordNet began in 1985 at Princeton University (Miller, 1990). A team lead by Prof. George Miller aimed to create a source of lexical knowledge whose organization would reflect some of the recent findings of psycholinguistic research into the human lexicon. WordNet has been used in numerous natural language processing, such as part of speech tagging (Segond et al., 97), word sense disambiguation (Resnik, 1995), text categorization (Gomez-Hidalgo and Rodriguez, 1997), information extraction (Chai and Biermann, 1997), and so on with considerable success. However the usefulness of WordNet in information retrieval applications has been debatable. Information retrieval is concerned with locating documents relevant to a user&apos;s information needs from a collection of documents. The user describes his/her information needs with a query which consists of a number of words. The information retrieval system compares the query with documents in the collection and returns the documents that are likely to satisfy the user&apos;s information requirements. A fundamental weakn"
W98-0704,P90-1034,0,0.0399144,"Voorhees (Voorhees, 1994) have proposed an expansion method using WordNet. Our method differs from theirs in that we enrich the coverage of WordNet using two methods of automatic thesatmm construction, and we weight the expausion term appropriately so that it can accommodate the polysemous word problem. Although Stairmand (Stairmand, 1997) and Richardson (Richardson and Smeaton, 1995) have proposed the use of WordNet in information retrieval, they did not used WordNet in the query expansion framework. Our predicate-argument structure-based thesatmis is based on the method proposed by Hindie (Hindle, 1990), although Hindle did not apply it to information retrieval. Instead, he used mutual information statistics as a Similarity coefficient, wheras we used the Dice coefficient for normalization purposes. Hindle only extracted the subject-verb and the object-verb predicatearguments, while we also extract adjective-noun predicate-arguments. Our weighting method follows the Qiu 36 method (Qiu and Frei, 1993), except that Qiu used it to expand terms only from a single automatically constructed thesarus and did not consider the use of more than one thesaurus. 7 Conclusions This paper analyzed why the"
W98-0704,J91-1002,0,0.0241408,"Missing"
W98-0704,W95-0105,0,0.0420743,"Missing"
W98-0704,1995.iwpt-1.26,0,0.02587,"Missing"
W98-0704,W97-0811,0,\N,Missing
W98-1510,1997.iwpt-1.16,1,0.671327,"that we don&apos;t cousider any lexical association, VC estimate• the probability of its derivation as follows. Fignre 1: A parse derivation for an input string &quot;11Ji!c/J&quot;;&apos; 1 ii: ]t~t.: (She ate a pie)&quot; 2.1 P(taR) &quot;&apos;P(ta!Av.1&apos;) P(t.abeR, ta)&quot;&apos; P(tabeV) ing a wide range of existing syntactic-based language modeling frameworks, from simple PCFG models to more context-sensitive models including those proposed in [2, 13, 19]. Am.olg these, we, at present, use probabilistic GLil (PGLH.) language modeling, which is given by incorpo~ rating probabilistic distributions into the GLR parsing framework [10, 21]. The advantages of PGLR modeling are (a) PGLR. models are mildly context~sensitive, compared with PCFG models, and (b) PGLR. models inherently capture both structural preferences and POS bigram statistics, which meets our integration requirement. For further discussion, see [10]. &apos;l&apos;(gaR, tabe, ta) &quot;&apos; P(gal&apos;r[h(tabe,[Pr,Pz])]) P(oR, ga, tabe, ta) &quot;&apos; P(oPz[h(tabe, [l&apos;r :ga, Pz])]) The lexical model P(WIR) is the product of the probability of each lexical derivation li ·-7 Wi, where 11 E L (L C R) is the POS tag of w; E W: =II l&apos;(w;R,w 1 , ... ,w 1_1) (5) (G) where h(h, [s1, ... , sn]) is"
W98-1510,E91-1004,0,0.0223692,"ile maintaining the probabilistically wcll-foundedness of the overall model. ta (PAST) -; tabe (eat) -+ ga (NO III) (ACC) ·-+ kanojo (she) -; pai (pie) --t o First, for each lcxieal item that we don&apos;t cousider any lexical association, VC estimate• the probability of its derivation as follows. Fignre 1: A parse derivation for an input string &quot;11Ji!c/J&quot;;&apos; 1 ii: ]t~t.: (She ate a pie)&quot; 2.1 P(taR) &quot;&apos;P(ta!Av.1&apos;) P(t.abeR, ta)&quot;&apos; P(tabeV) ing a wide range of existing syntactic-based language modeling frameworks, from simple PCFG models to more context-sensitive models including those proposed in [2, 13, 19]. Am.olg these, we, at present, use probabilistic GLil (PGLH.) language modeling, which is given by incorpo~ rating probabilistic distributions into the GLR parsing framework [10, 21]. The advantages of PGLR modeling are (a) PGLR. models are mildly context~sensitive, compared with PCFG models, and (b) PGLR. models inherently capture both structural preferences and POS bigram statistics, which meets our integration requirement. For further discussion, see [10]. &apos;l&apos;(gaR, tabe, ta) &quot;&apos; P(gal&apos;r[h(tabe,[Pr,Pz])]) P(oR, ga, tabe, ta) &quot;&apos; P(oPz[h(tabe, [l&apos;r :ga, Pz])]) The lexical model P(WIR) is t"
W98-1510,J96-1002,0,0.0088305,"Missing"
W98-1510,P95-1037,0,0.159717,"Missing"
W98-1510,P93-1005,0,0.0208939,"ile maintaining the probabilistically wcll-foundedness of the overall model. ta (PAST) -; tabe (eat) -+ ga (NO III) (ACC) ·-+ kanojo (she) -; pai (pie) --t o First, for each lcxieal item that we don&apos;t cousider any lexical association, VC estimate• the probability of its derivation as follows. Fignre 1: A parse derivation for an input string &quot;11Ji!c/J&quot;;&apos; 1 ii: ]t~t.: (She ate a pie)&quot; 2.1 P(taR) &quot;&apos;P(ta!Av.1&apos;) P(t.abeR, ta)&quot;&apos; P(tabeV) ing a wide range of existing syntactic-based language modeling frameworks, from simple PCFG models to more context-sensitive models including those proposed in [2, 13, 19]. Am.olg these, we, at present, use probabilistic GLil (PGLH.) language modeling, which is given by incorpo~ rating probabilistic distributions into the GLR parsing framework [10, 21]. The advantages of PGLR modeling are (a) PGLR. models are mildly context~sensitive, compared with PCFG models, and (b) PGLR. models inherently capture both structural preferences and POS bigram statistics, which meets our integration requirement. For further discussion, see [10]. &apos;l&apos;(gaR, tabe, ta) &quot;&apos; P(gal&apos;r[h(tabe,[Pr,Pz])]) P(oR, ga, tabe, ta) &quot;&apos; P(oPz[h(tabe, [l&apos;r :ga, Pz])]) The lexical model P(WIR) is t"
W98-1510,H94-1048,0,0.0696817,"Missing"
W98-1510,P96-1025,0,0.155593,"Missing"
W98-1510,C92-2065,0,0.0419749,"Missing"
W98-1510,P97-1003,0,0.058909,"ng A) we rank its pars&lt;~ dr:rlYations according to the joint distribution J&apos;(H, lr). where H1 is a word sequence candidate for A, and R is a parse derivation candidate for H-- whos(&apos; terminal symbols constitute a POS tag scquc-;nce L (see Figure 1 2 ). We first. decompose 1&apos;( fl. lr) • Pmbabilistically well-fottnded semantics: The language model used in a statistical parser should have probabilistically well-founclecl semantics) whieh vould a.lso facilitate the anal:,&apos;·· sis of the model&apos;s behavior. , 1 For further discussion, see [8]. This is also tlH&apos; case with recent works such as [3] and [5] due to t.hc lad&lt; of modularity of statistical types. &apos;-Although syntactic structure. R is represented af&apos; a dependency structure in this figure, our framework 80 into two submodels, the syntactic model l&apos;(R) and the lexical model P(WR): P(R, W) = P(R) · P(WR) depends only on a certain small part of its whole context. We first assume that syntactic structure R in P(wiiR,w 1 , .. . ,wi_- 1 ) can always be reduced to l; ( E R), which allows us to deal with the lexical model separately from the syntactic model. The question then is which subset C of { ·uJI, ... , Wi-l} has the strongest influen"
W98-1510,H92-1027,0,0.075636,"Missing"
W98-1510,1995.iwpt-1.26,0,0.109798,"Missing"
W98-1510,W96-0112,0,\N,Missing
W98-1510,C92-2066,0,\N,Missing
W98-1510,1991.iwpt-1.22,0,\N,Missing
Y18-1066,W09-1201,0,0.0758025,"grained classes, namely, the exophoric text writer (exo1), reader (exo2) and the other entity (exoX). Example (2) depicts the necessity of the subcategorization. (2) sandoitti taberu. sandwich eat I eat sandwich. / Do you eat sandwich? Both the exophoric speaker (exo1) and hearer (exo2) can be the nominative argument of the verb “eat” and accordingly the sentence meaning is different. To distinguish these two meanings, the subcategorization of the exophora is necessary. Secondly, we introduce domain-adaptation techniques into the Japanese PAS analysis. Surdeanu et al. (2008) and Hajiˇc et al. (2009) reported that the SRL performance degraded when the domains were different between the training and testing data. Yang et al. (2015) tackled this problem by introducing the domain adaptation into a deep learning method. As most of the past studies of the Japanese PAS analysis targeted a mono-type of texts, i.e. newspaper articles, the domain adaptation did not matter, except for Imamura et al. (2014). They trained the PAS analyzer for dialogues by using newspaper articles. However, pairs of other media types have not been investigated yet. In contrast, we target various types of Japanese text"
Y18-1066,P09-2022,0,0.0302338,"tive argument; thus the accusative case of intransitive verbs should be filled with none. Table 2 shows that the inter-sentential arguments were tackled by the fewer studies than the intra578 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 endophora intra-sentential inter-sentential non-zero work  label present work Matsubayashi and Inui (2017) Ouchi et al. (2017) Shibata et al. (2016) Imamura et al. (2014) Hangyo et al. (2013) Yoshikawa et al. (2013) Hayashibe et al. (2011) Sasano and Kurohashi (2011) Imamura et al. (2009) zero intra(dep) intra(zero) ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ exophora inter ◦ ◦ ◦ ◦ ◦ exo1 exo2 exoX none ◦ ◦ 4 ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ ◦ 4 4 ◦ ◦ ◦ ◦ ◦ 4 4 Table 2: Target argument types of the past studies sentential arguments. Identifying the inter-sentential arguments requires searching in larger space compared with the intra-sentential arguments, and thus the problem becomes more difficult. Unlike the inter-sentential arguments, identifying the exophoric arguments, in particular, the exo1 and exo2 arguments do not drastically increase the search space. They are easy to be introduced into the P"
Y18-1066,C16-1038,0,0.171317,"ferent over the source media; thus consideration of the difference in the source media is necessary. We start with a recurrent neural network (RNN)based base model and extend it by introducing the 1 http://pj.ninjal.ac.jp/corpus_center/ bccwj/en/ following five kinds of domain adaptation. (1) The fine-tuning method trains the model with the entire training data and uses the learnt parameters as the initial parameter values for the second stage learning with the target-domain training data. (2) The feature augmentation method trains a shared network and domain-specific networks simultaneously (Kim et al., 2016). (3) The class probability shift method skews the output probability of the network based on the prior probability distribution of the argument types across the domains. (4) The voting method determines the output by the majority of the above three methods. (5) The mixture method combines the fine-tuning method, the feature augmentation method and the class probability shift method into a single method. We describe the details of each method in section 4. 2 2.1 Problem setting Argument type The past Japanese PAS analysis targeted various combinations of argument types. Table 2 summarizes the"
Y18-1066,Q15-1020,0,0.07042,"the necessity of the subcategorization. (2) sandoitti taberu. sandwich eat I eat sandwich. / Do you eat sandwich? Both the exophoric speaker (exo1) and hearer (exo2) can be the nominative argument of the verb “eat” and accordingly the sentence meaning is different. To distinguish these two meanings, the subcategorization of the exophora is necessary. Secondly, we introduce domain-adaptation techniques into the Japanese PAS analysis. Surdeanu et al. (2008) and Hajiˇc et al. (2009) reported that the SRL performance degraded when the domains were different between the training and testing data. Yang et al. (2015) tackled this problem by introducing the domain adaptation into a deep learning method. As most of the past studies of the Japanese PAS analysis targeted a mono-type of texts, i.e. newspaper articles, the domain adaptation did not matter, except for Imamura et al. (2014). They trained the PAS analyzer for dialogues by using newspaper articles. However, pairs of other media types have not been investigated yet. In contrast, we target various types of Japanese texts; we use Balanced Corpus of Contemporary Written Japanese (BCCWJ)1 (Maekawa et al., 2014) for evaluation. BCCWJ contains 100 million"
Y18-1089,P16-1074,0,0.24744,"the dative argument is “the nearby shopping district” which appears in the previous sentence (intersentential zero anaphora). On the other hand, the nominative argument of v1 (see) is the writer of this text who is not explicitly mentioned in the text (exophora). This study focuses on zero anaphora resolution of Japanese texts, but we observe such pronoundropping phenomenon in other languages as well, e.g. Chinese, Italian, Turkish and so on. There have been many studies on the task similar to the Japanese zero anaphora resolution in other languages (Iida and Poesio, 2011; Rello et al., 2012; Chen and Ng, 2016; Yin et al., 2017). The zero anaphora resolution is one of the active research areas in the Japanese language processing as it is crucial for improving the performance of various natural language processing applications such as automatic text summarisation (Yamada et al., 2017), information extraction (Sudo et al., 2001) and machine translation (Kudo et al., 769 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2014). Therefore it has been extensively studied as an urgent problem to be solved (Sasano and"
Y18-1089,Y12-1058,0,0.0531944,"both intra- and inter-sentential Japanese zero anaphora for three cases: nominative, accusative and dative in a single model, and to evaluate it by using a balanced corpus, BCCWJ. 2 Related Work 2.1 Japanese Zero Anaphora Resolution Table 3 summarises related work regarding task types, text genres, corpus size, and methods. Hangyo et al. (2013) proposed a method based on ranking SVM for resolving intra- and intersentential anaphora and exophora in a Web corpus which they created for their study. The corpus consists of 1,000 text fragments extracted from the first three sentences of Web pages (Hangyo et al., 2012). Shibata et al. (2016) used a feedforward neural network (FNN) for the analysis of directly dependent arguments and intra-sentential zero anaphora in the Web corpus created by Hangyo et al. (2012). Matsubayashi and Inui (2017) used a combination of an FNN and a recurrent neural network (RNN) to analyse directly dependent arguments and intra-sentential zero anaphora in NTC to show it outperformed the state-of-the-art model for directly dependent arguments and intra-sentential zero anaphora. Sasano and Kurohashi (2011) used a log-linear model to analyse intra and inter-sentential zero anaphora"
Y18-1089,D13-1095,0,0.500224,"7). The zero anaphora resolution is one of the active research areas in the Japanese language processing as it is crucial for improving the performance of various natural language processing applications such as automatic text summarisation (Yamada et al., 2017), information extraction (Sudo et al., 2001) and machine translation (Kudo et al., 769 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2014). Therefore it has been extensively studied as an urgent problem to be solved (Sasano and Kurohashi, 2011; Hangyo et al., 2013; Ouchi et al., 2017; Matsubayashi and Inui, 2017). Our contribution in this study is twofold: proposing a method for both intra- and inter-sentential zero anaphora of the Japanese language and evaluating the method with a large-scale balanced corpus. The past research evaluated their system with NAIST Text Corpus (NTC) (Iida et al., 2007) that consists of newspaper articles; therefore the evaluation is skewed regarding text genres. When considering real applications, we need a zero-anaphora resolution method that is robust against the difference in text genres. We use Balanced Corpus of Conte"
Y18-1089,W07-1522,0,0.015643,"., 769 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2014). Therefore it has been extensively studied as an urgent problem to be solved (Sasano and Kurohashi, 2011; Hangyo et al., 2013; Ouchi et al., 2017; Matsubayashi and Inui, 2017). Our contribution in this study is twofold: proposing a method for both intra- and inter-sentential zero anaphora of the Japanese language and evaluating the method with a large-scale balanced corpus. The past research evaluated their system with NAIST Text Corpus (NTC) (Iida et al., 2007) that consists of newspaper articles; therefore the evaluation is skewed regarding text genres. When considering real applications, we need a zero-anaphora resolution method that is robust against the difference in text genres. We use Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa et al., 2014) for evaluation. BCCWJ consists of about 100 million words that were systematically sampled from several sources such as newspaper articles, novels, magazines, white papers, QA texts on the internet and blog texts. We use the core data set of BCCWJ consisting of about two million words"
Y18-1089,P11-1081,0,0.567652,"tence (intra-sentential zero anaphora) and the dative argument is “the nearby shopping district” which appears in the previous sentence (intersentential zero anaphora). On the other hand, the nominative argument of v1 (see) is the writer of this text who is not explicitly mentioned in the text (exophora). This study focuses on zero anaphora resolution of Japanese texts, but we observe such pronoundropping phenomenon in other languages as well, e.g. Chinese, Italian, Turkish and so on. There have been many studies on the task similar to the Japanese zero anaphora resolution in other languages (Iida and Poesio, 2011; Rello et al., 2012; Chen and Ng, 2016; Yin et al., 2017). The zero anaphora resolution is one of the active research areas in the Japanese language processing as it is crucial for improving the performance of various natural language processing applications such as automatic text summarisation (Yamada et al., 2017), information extraction (Sudo et al., 2001) and machine translation (Kudo et al., 769 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2014). Therefore it has been extensively studied as an u"
Y18-1089,D16-1132,0,0.27827,"t against the difference in text genres. We use Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa et al., 2014) for evaluation. BCCWJ consists of about 100 million words that were systematically sampled from several sources such as newspaper articles, novels, magazines, white papers, QA texts on the internet and blog texts. We use the core data set of BCCWJ consisting of about two million words that are annotated with co-reference relations and predicate-argument relations for nominative, dative and accusative cases. Most past research on the Japanese zero anaphora resolution (Iida et al., 2016; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) has targeted only intra-sentential anaphora. As a reason to focus on intra-sentential zero anaphora, Ouchi et al. (2017) pointed out a search space problem. The system needs to consider antecedent candidates in the entire text for the inter-sentential anaphora. It makes the search space larger than that for the intra-sentential anaphora. Matsubayashi and Inui (2017) introduced a recurrent neural network (RNN) for intra-sentential zero anaphora resolution which takes an entire sentence as an input. However, if we apply the"
Y18-1089,P09-2022,0,0.208415,"Missing"
Y18-1089,D15-1166,0,0.0424931,"a machine learning approach, we have a far larger number of negative instances than that of positive instances. The ratio can be one against 1,000 in our case with BCCWJ. Such skewed training data unnecessarily increases computing time and hinder the system generalisation ability. To reduce the unnecessary negative instances, we filter out antecedent candidates by using the case frame information of the target predicate. We achieved 1/1,000 in reduction rate of candidate numbers by the proposed filtering method. Also, we incorporated the RNN into our model with the local attention mechanism (Luong et al., 2015) so that the system can selectively utilise the useful preceding sentences. This study is the first attempt to deal with both intra- and inter-sentential Japanese zero anaphora for three cases: nominative, accusative and dative in a single model, and to evaluate it by using a balanced corpus, BCCWJ. 2 Related Work 2.1 Japanese Zero Anaphora Resolution Table 3 summarises related work regarding task types, text genres, corpus size, and methods. Hangyo et al. (2013) proposed a method based on ranking SVM for resolving intra- and intersentential anaphora and exophora in a Web corpus which they cre"
Y18-1089,I17-2022,0,0.444198,"f the active research areas in the Japanese language processing as it is crucial for improving the performance of various natural language processing applications such as automatic text summarisation (Yamada et al., 2017), information extraction (Sudo et al., 2001) and machine translation (Kudo et al., 769 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2014). Therefore it has been extensively studied as an urgent problem to be solved (Sasano and Kurohashi, 2011; Hangyo et al., 2013; Ouchi et al., 2017; Matsubayashi and Inui, 2017). Our contribution in this study is twofold: proposing a method for both intra- and inter-sentential zero anaphora of the Japanese language and evaluating the method with a large-scale balanced corpus. The past research evaluated their system with NAIST Text Corpus (NTC) (Iida et al., 2007) that consists of newspaper articles; therefore the evaluation is skewed regarding text genres. When considering real applications, we need a zero-anaphora resolution method that is robust against the difference in text genres. We use Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa et al.,"
Y18-1089,P15-1093,0,0.0135243,"dates of the predicateargument structure for each predicate in BCCWJ. 4.1 Mean Vector for Predicate We propose an effective candidate reduction method using two kinds of mean vectors: MVC introduced in Section 3.2 and mean vector for predicate (MVP) ϕp(c) which is a weighted mean vector of MVC ϕcf p (c) over the case frames of the predicate p for l each case c. The weight is calculated based on the frequency of each case frame in KUCF. Our candidate reduction method reduces the number of combination of case frame candidates and argument candidates by using the hill climbing method proposed by Ouchi et al. (2015). The purpose of this candidate reduction is not only for efficiency but also for alleviating the imbalance between the number of positive and negative examples in the training data. In our case, a single positive example has 20,000 negative counterparts. We assume that most of the negative examples in the training data do not make much contribution to training. 773 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 Algorithm 1 Candidate reduction algorithm Input: a predicate p to be analyzed, a set of case"
Y18-1089,P17-1146,0,0.499069,"Missing"
Y18-1089,E12-1072,0,0.0147261,"zero anaphora) and the dative argument is “the nearby shopping district” which appears in the previous sentence (intersentential zero anaphora). On the other hand, the nominative argument of v1 (see) is the writer of this text who is not explicitly mentioned in the text (exophora). This study focuses on zero anaphora resolution of Japanese texts, but we observe such pronoundropping phenomenon in other languages as well, e.g. Chinese, Italian, Turkish and so on. There have been many studies on the task similar to the Japanese zero anaphora resolution in other languages (Iida and Poesio, 2011; Rello et al., 2012; Chen and Ng, 2016; Yin et al., 2017). The zero anaphora resolution is one of the active research areas in the Japanese language processing as it is crucial for improving the performance of various natural language processing applications such as automatic text summarisation (Yamada et al., 2017), information extraction (Sudo et al., 2001) and machine translation (Kudo et al., 769 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2014). Therefore it has been extensively studied as an urgent problem to be"
Y18-1089,N06-1023,0,0.101226,"Missing"
Y18-1089,C08-1097,0,0.564832,"g of 979 sentences and showed it outperformed the stateof-the-art model for intra and inter-sentential zero anaphora. Unlike these past studies, we adopt ranking SVM2 (Joachims, 2006) and a combination of FNN and RNN to analyse intra- and inter-sentential zero anaphora in BCCWJ. 2.2 Large-scale Case Frames A case frame represents co-occurrence information of a predicate and its possible arguments organised in case patterns of the predicate and its cases. Organising the case frame based on the case pattern as shown in Table 4 enables us to utilise its lexical preference for resolving anaphora (Sasano et al., 2008; Sasano and Kurohashi, 2011; Hangyo et al., 2013). We adopt Kyoto University Case Frames (KUCF)3 which were compiled from a large-scale Web corpus by Kawahara and Kurohashi (2006). 2.3 Candidate Reduction The past studies for inter-sentential zero anaphora resolution adopted criteria for candidate reduction. Sasano and Kurohashi (2011) and Hangyo et al. (2013) collected antecedent candidates from the sentence containing the target predicate and its preceding three sentences. Although antecedents could appear in the sentence beyond the preceding three sentences, Hangyo et al. (2013) reported t"
Y18-1089,P14-2091,0,0.0340557,"Missing"
Y18-1089,I11-1085,0,0.62757,"nd Ng, 2016; Yin et al., 2017). The zero anaphora resolution is one of the active research areas in the Japanese language processing as it is crucial for improving the performance of various natural language processing applications such as automatic text summarisation (Yamada et al., 2017), information extraction (Sudo et al., 2001) and machine translation (Kudo et al., 769 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2014). Therefore it has been extensively studied as an urgent problem to be solved (Sasano and Kurohashi, 2011; Hangyo et al., 2013; Ouchi et al., 2017; Matsubayashi and Inui, 2017). Our contribution in this study is twofold: proposing a method for both intra- and inter-sentential zero anaphora of the Japanese language and evaluating the method with a large-scale balanced corpus. The past research evaluated their system with NAIST Text Corpus (NTC) (Iida et al., 2007) that consists of newspaper articles; therefore the evaluation is skewed regarding text genres. When considering real applications, we need a zero-anaphora resolution method that is robust against the difference in text genres. We use Bal"
Y18-1089,P14-2050,0,0.0448643,"eduction algorithm works well. Effect of word embedding and MVC Introducing word embeddings of the arguments and the predicate (F1) into the baseline model (F0) degrades the total accuracy. However, replacing the predicate embedding with the MVC (F2) increases the accuracy in comparison with F0. This indicates that using the case frame information (F2) instead of the predicate information (F1) is more effective. As described in 3.2, we used word embeddings learnt from the Wikipedia articles, but the word embeddings calculated from corpora of more diverse genre texts and syntactic information (Levy and Goldberg, 2014) might further improve the performance. Effect of context embedding Introducing the context information using the RNN model with the local attention mechanism (F3) shows some improvement over F2. This suggests that the model succeeded to learn effective preceding context information. Although the overall accuracy of F3 is still lower than that of S0, the FNN models with various features show higher accuracy for the intersentential cases. Additional Results Appendix A. describes a further detailed analysis focusing on the interaction between the result of the dependency analysis and the accurac"
Y18-1089,P16-1117,0,0.202672,"rence in text genres. We use Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa et al., 2014) for evaluation. BCCWJ consists of about 100 million words that were systematically sampled from several sources such as newspaper articles, novels, magazines, white papers, QA texts on the internet and blog texts. We use the core data set of BCCWJ consisting of about two million words that are annotated with co-reference relations and predicate-argument relations for nominative, dative and accusative cases. Most past research on the Japanese zero anaphora resolution (Iida et al., 2016; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2017) has targeted only intra-sentential anaphora. As a reason to focus on intra-sentential zero anaphora, Ouchi et al. (2017) pointed out a search space problem. The system needs to consider antecedent candidates in the entire text for the inter-sentential anaphora. It makes the search space larger than that for the intra-sentential anaphora. Matsubayashi and Inui (2017) introduced a recurrent neural network (RNN) for intra-sentential zero anaphora resolution which takes an entire sentence as an input. However, if we apply the same method to inter-"
Y18-1089,Y18-1000,0,0.265628,"Missing"
Y18-1089,D17-1135,0,0.444225,"is “the nearby shopping district” which appears in the previous sentence (intersentential zero anaphora). On the other hand, the nominative argument of v1 (see) is the writer of this text who is not explicitly mentioned in the text (exophora). This study focuses on zero anaphora resolution of Japanese texts, but we observe such pronoundropping phenomenon in other languages as well, e.g. Chinese, Italian, Turkish and so on. There have been many studies on the task similar to the Japanese zero anaphora resolution in other languages (Iida and Poesio, 2011; Rello et al., 2012; Chen and Ng, 2016; Yin et al., 2017). The zero anaphora resolution is one of the active research areas in the Japanese language processing as it is crucial for improving the performance of various natural language processing applications such as automatic text summarisation (Yamada et al., 2017), information extraction (Sudo et al., 2001) and machine translation (Kudo et al., 769 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 2014). Therefore it has been extensively studied as an urgent problem to be solved (Sasano and Kurohashi, 2011; Ha"
yoshida-etal-2004-retrieving,J93-2004,0,\N,Missing
