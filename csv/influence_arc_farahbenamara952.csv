2009.jeptalnrecital-court.5,W04-2322,0,0.0343493,"Missing"
2009.jeptalnrecital-court.5,J05-2005,0,0.110676,"Missing"
2020.acl-main.373,C18-1139,0,0.0226767,". BERTR features . We relied on state of the art features that have shown to be useful for the task of hate speech detection: Surface features (tweet length in words, the presence of personal 10 In case a particular web page is not available anymore, the URL is removed from the tweet. 11 We relied on a manually built emoji lexicon that contains 1,644 emojis along with their polarity and detailed description. 12 We experimented with different configurations by incorporating different French pre-trained embeddings available: Glove (Pennington et al., 2014), FastText (Grave et al., 2018), Flair (Akbik et al., 2018) and CamemBERT (Martin et al., 2019) but none of the configurations were able to achieve results better than BERTbase . 13 https://github.com/google/ sentencepiece 4060 pronoun and third-person pronoun, punctuation marks, URLs, images, hashtags, @userMentions and the number of words written in capital), Emoji features11 (number of positive and negative emojis), Opinion features (number of positive, negative and neutral words in each tweet relying on opinion (Benamara et al., 2014), emotion (Piolat and Bannour, 2009) and slang French lexicons. We also account for hedges (negation and modality),"
2020.acl-main.373,W14-6305,1,0.805459,"Missing"
2020.acl-main.373,W19-3621,0,0.0111274,"h-Tweets 4056 istics, reference to private life, etc. From a sociological perspective, studies focus on social media contents (tweets) or SMS in order to analyze public opinion on gender-based violence (Purohit et al., 2016) or violence and sexist behaviours (Barak, 2005; Megarry, 2014). Gender bias in word embeddings. Bolukbasi et al. (2016) have shown that word embeddings trained on news articles exhibit female/male gender stereotypes. Several algorithms have then been proposed to attenuate this bias (Dev and Phillips, 2019) or to make embeddings gender-neutral (Zhao et al., 2018), although Gonen and Goldberg (2019) consider that bias removal techniques are insufficient. Debiased embeddings were used by Park et al. (2018) observing a decrease in sexism detection performance compared to the non-debiased model. To overcome this limitation, Badjatiya et al. (2019) propose neural methods for stereotypical bias removal for hate speech detection (i.e., hateful vs. non-hateful). They first identify a set of bias sensitive words, then mitigate their impact by replacing them with their POS, NER tags, K-nearest neighbours and hypernyms obtained via WordNet. Automatic sexism detection. To our knowledge, the automat"
2020.acl-main.373,N19-1423,0,0.0821389,"Missing"
2020.acl-main.373,L16-1218,0,0.0217864,"Missing"
2020.acl-main.373,S19-2009,0,0.101898,"tweets and then identifying the type of sexist behaviour according to a taxonomy defined by (Anzovino et al., 2018): discredit, stereotype, objectification, sexual harassment, threat of violence, dominance and derailing. Most participants used SVM models and ensemble of classifiers for both tasks with features such as n-grams and opinions (Fersini et al., 2018b). These datasets have also been used in the Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter shared task at SemEval 2019. Best results were obtained with an SVM model using sentence embeddings as features (Indurthi et al., 2019). There are also a few notable neural network techniques. Jha and Mamidi (2017) employ an LSTM model to classify messages as: benevolent, hostile and non-sexist. Zhang and Luo (2018) implement two deep neural network models (CNN + Gated Recurrent Unit layer and CNN + modified CNN layers for feature extraction) in order to classify social media texts as racist, sexist, or non-hateful. Karlekar and Bansal (2018) use a single-label CNN-LSTM model with character-level embeddings to classify three forms of sexual harassment: commenting, ogling/staring, and touching/groping. Sharifirad et al. (2018)"
2020.acl-main.373,W17-2902,0,0.626604,"not be moderated. As far as we are aware, the distinction between reports/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types of reports/denunciations of sexism experiences in French tweets, based on their impact on the target. Our contributions are: (1) A novel characterization of sexist contentforce relation inspired by speech acts theory (Au"
2020.acl-main.373,D18-1303,0,0.520173,"orts/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types of reports/denunciations of sexism experiences in French tweets, based on their impact on the target. Our contributions are: (1) A novel characterization of sexist contentforce relation inspired by speech acts theory (Austin, 1962) and discourse studies in gender (Lazar, 2007; Mills, 2008). We"
2020.acl-main.373,2020.acl-main.645,0,0.174441,"Missing"
2020.acl-main.373,D19-1474,0,0.0110437,"ton, 2012; Bianchi, 2014) or concentrate on the analytical level at which the derogatory content is interpreted, whether it provides meaning at the level of the presupposition (or more largely non at-issue content (Potts, 2005)) or of the assertion (Cepollaro, 2015). We have chosen to distinguish cases where the 4057 addressee is directly addressed from those in which she is not, as done in hate speech analysis. For example, Waseem et al. (2017) and ElSherief et al. (2018) consider that directed hate speech is explicitly directed at a person while generalized hate speech targets a group. For (Ousidhoum et al., 2019), a hateful tweet is direct when the target is explicitly named, or indirect when ”less easily discernible”. Unlike these approaches and the definitions of target used in (Basile et al., 2019; Fersini et al., 2018a), we do not consider the number of targets of a sexist message (it can indifferently be a woman, a group of women or all women) but rather distinguish the target from the addressee. Our use of the notions of directness and indirectness are also transverse to the ones used in (Lazar, 2007; Chew and Kelley-Chew, 2007) or (Mills, 2008), who resort to the label indirectness for subtle f"
2020.acl-main.373,D19-1174,0,0.461026,"m experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types of reports/denunciations of sexism experiences in French tweets, based on their impact on the target. Our contributions are: (1) A novel characterization of sexist contentforce relation inspired by speech acts theory (Austin, 1962) and discourse studies in gender (Lazar, 2007; Mills, 2008). We distinguish different"
2020.acl-main.373,D18-1302,0,0.0370247,"a contents (tweets) or SMS in order to analyze public opinion on gender-based violence (Purohit et al., 2016) or violence and sexist behaviours (Barak, 2005; Megarry, 2014). Gender bias in word embeddings. Bolukbasi et al. (2016) have shown that word embeddings trained on news articles exhibit female/male gender stereotypes. Several algorithms have then been proposed to attenuate this bias (Dev and Phillips, 2019) or to make embeddings gender-neutral (Zhao et al., 2018), although Gonen and Goldberg (2019) consider that bias removal techniques are insufficient. Debiased embeddings were used by Park et al. (2018) observing a decrease in sexism detection performance compared to the non-debiased model. To overcome this limitation, Badjatiya et al. (2019) propose neural methods for stereotypical bias removal for hate speech detection (i.e., hateful vs. non-hateful). They first identify a set of bias sensitive words, then mitigate their impact by replacing them with their POS, NER tags, K-nearest neighbours and hypernyms obtained via WordNet. Automatic sexism detection. To our knowledge, the automatic detection of sexist messages currently deals only with English, Italian and Spanish. For example in the A"
2020.acl-main.373,D14-1162,0,0.0903832,"Missing"
2020.acl-main.373,D15-1309,0,0.0293218,"e said “who’s gonna take care of your children when you are at ACL?”). Indeed, whereas messages could be reported and moderated in the first case as recommended by European laws, messages relating sexism experiences should not be moderated. As far as we are aware, the distinction between reports/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types o"
2020.acl-main.373,W18-5114,0,0.084013,"far as we are aware, the distinction between reports/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et al., 2019) shared tasks). This paper proposes the first approach to detect different types of reports/denunciations of sexism experiences in French tweets, based on their impact on the target. Our contributions are: (1) A novel characterization of sexist contentforce relation inspired by speech acts theory (Austin, 1962) and discourse"
2020.acl-main.373,N19-1214,0,0.0217527,"Missing"
2020.acl-main.373,W17-3012,0,0.0266952,"ated in a variety of manners. Most accounts however either focus on the type of act (assault-like, propaganda, authoritative, etc.) that derogatory language performs (Langton, 2012; Bianchi, 2014) or concentrate on the analytical level at which the derogatory content is interpreted, whether it provides meaning at the level of the presupposition (or more largely non at-issue content (Potts, 2005)) or of the assertion (Cepollaro, 2015). We have chosen to distinguish cases where the 4057 addressee is directly addressed from those in which she is not, as done in hate speech analysis. For example, Waseem et al. (2017) and ElSherief et al. (2018) consider that directed hate speech is explicitly directed at a person while generalized hate speech targets a group. For (Ousidhoum et al., 2019), a hateful tweet is direct when the target is explicitly named, or indirect when ”less easily discernible”. Unlike these approaches and the definitions of target used in (Basile et al., 2019; Fersini et al., 2018a), we do not consider the number of targets of a sexist message (it can indifferently be a woman, a group of women or all women) but rather distinguish the target from the addressee. Our use of the notions of dir"
2020.acl-main.373,N16-2013,0,0.113851,"ping this pregnant woman shooting), and messages which relate sexism experiences (e.g., He said “who’s gonna take care of your children when you are at ACL?”). Indeed, whereas messages could be reported and moderated in the first case as recommended by European laws, messages relating sexism experiences should not be moderated. As far as we are aware, the distinction between reports/denunciations of sexism experience and real sexist messages has not been addressed. Previous work considers sexism either as a type of hate speech, along with racism, homophobia, or hate speech against immigrants (Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Basile et al., 2019; Schrading et al., 2015) or study it as such. In this latter case, detection is casted as a binary classification problem (sexist vs. nonsexist) or a multi-label classification by identifying the type of sexist behaviours (Jha and Mamidi, 2017; Sharifirad et al., 2018; Fersini et al., 2018b; Karlekar and Bansal, 2018; Parikh et al., 2019). English is dominant, although Italian and Spanish have already been studied (see the IberEval 2018 (Fersini et al., 2018b), EvalIta 2018 (Fersini et al., 2018a) and HateEval 2019 (Basile et a"
2020.acl-main.373,D18-1521,0,0.0181732,"-for-Sexism-Detectionin-French-Tweets 4056 istics, reference to private life, etc. From a sociological perspective, studies focus on social media contents (tweets) or SMS in order to analyze public opinion on gender-based violence (Purohit et al., 2016) or violence and sexist behaviours (Barak, 2005; Megarry, 2014). Gender bias in word embeddings. Bolukbasi et al. (2016) have shown that word embeddings trained on news articles exhibit female/male gender stereotypes. Several algorithms have then been proposed to attenuate this bias (Dev and Phillips, 2019) or to make embeddings gender-neutral (Zhao et al., 2018), although Gonen and Goldberg (2019) consider that bias removal techniques are insufficient. Debiased embeddings were used by Park et al. (2018) observing a decrease in sexism detection performance compared to the non-debiased model. To overcome this limitation, Badjatiya et al. (2019) propose neural methods for stereotypical bias removal for hate speech detection (i.e., hateful vs. non-hateful). They first identify a set of bias sensitive words, then mitigate their impact by replacing them with their POS, NER tags, K-nearest neighbours and hypernyms obtained via WordNet. Automatic sexism dete"
2020.coling-main.116,W19-7723,1,0.837527,"el, it’s just the law of the strongest Italian Nuovo governo monti: dal bunga bunga al banca banca... New Monti’s Governmet: from bunga bunga to bank bank... REUTERS ANALISI Crisi, di corsa verso il governo Monti REUTERS ANALYSIS Crisis, running towards Monti’s Government not ironic not ironic not ironic not Table 2: Examples from the datasets. Provided that the availability of morphological and syntactic knowledge is crucial for performing the experiments described in the rest of the paper, we needed to obtain a representation of all the datasets in UD format. With the exception of TWITTIRÒ (Cignarella et al., 2019), a subset of the IronITA dataset already available as a UD treebank (see the current release in the UD official repository3 ), we obtained the dependency-based annotation for the other corpora by applying the UDPipe4 pipeline (for tokenization, PoS-tagging and parsing). Figure 1 provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained"
2020.coling-main.116,S19-2197,1,0.824824,"and their interaction in several NLP tasks, showing their effectiveness. For example, Sidorov et al. (2012) exploited syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information specifically for irony detection. Among them we cite Cignarella and Bosco (2019), who employed a SVC combined with shallow features based on morphology and dependency syntax outperforming strong baselines in the IroSvA 2019 irony detection shared task in Spanish variants (Ortega-Bueno et al., 2019). This paper aims to go one step further focusing for the first time on the development of syntax-aware irony d"
2020.coling-main.116,S15-2080,1,0.750861,"e directions for future work. 2 Related work The identification of irony and the description of pragmatic and linguistic devices that trigger it has always been a controversial topic (Grice, 1975; Sperber and Wilson, 1981; Utsumi, 1996), a challenge for both humans and automatic tools. This motivates the interest for irony of the NLP community, the organization of tasks regarding its processing and the subsequent creation of benchmarks. In the last decade several shared tasks have been organized in order to advance with the techniques regarding the automatic processing of figurative language (Ghosh et al., 2015), and more recently irony and sarcasm detection have been gaining greater attention. Being irony a pragmatic device inherently related to culture and language, the task captured the attention of various researchers, dealing with different languages and mostly focusing on social media texts. The presence of irony for Italian has been investigated -among other things- in the SENTIPOLC task of the EVALITA 2014 (Basile et al., 2014) and 2016 (Barbieri et al., 2016) and at IronITA (Cignarella et al., 2018). It was addressed for French within the context of DEFT 2017 (Benamara et al., 2017), for Eng"
2020.coling-main.116,P15-2124,0,0.0315116,"of the phenomenon itself, that may assume a large variety of merged forms and facets. Several semantic and syntactic devices can be used to express irony (e.g., analogy, oxymoron, paradox, euphemism and rhetorical questions), and a variety of different linguistic elements can cause the incongruity, determine the clash and play the role of irony triggers within a text. Most computational research in irony detection is applied on social media texts and focuses primarily on content-based processing of the linguistic information using semantic or pragmatic devices, neglecting syntax. For example, Joshi et al. (2015) propose a method for sarcasm detection in tweets as the task of detecting incongruity between an observed and an expected word, while Karoui et al. (2015) rely on external common knowledge to the utterance shared by author and reader. However, in addition to these pragmatic devices, some hints about the role of syntax can be found in linguistic literature where the deviation from syntactic norms has been reported as a possible trigger of the phenomenon. For instance, Michaelis and Feng (2015) explore the usage of split interrogatives and of topic-comment utterances in English, arguing they bo"
2020.coling-main.116,2020.lrec-1.500,0,0.0286436,"ns occurring in texts has been moreover confirmed by Mahler et al. (2017), where artificial syntactic manipulations are applied, such as those based on negations and adverbs, with the purpose of fooling sentiment analysis systems. Some research already explored different kinds of syntactic features and their interaction in several NLP tasks, showing their effectiveness. For example, Sidorov et al. (2012) exploited syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information specifically for irony detection. Among them we cite Cignarella and Bosco (2019), who employed a SVC combined with sh"
2020.coling-main.116,P15-2106,0,0.0190749,"(e.g., analogy, oxymoron, paradox, euphemism and rhetorical questions), and a variety of different linguistic elements can cause the incongruity, determine the clash and play the role of irony triggers within a text. Most computational research in irony detection is applied on social media texts and focuses primarily on content-based processing of the linguistic information using semantic or pragmatic devices, neglecting syntax. For example, Joshi et al. (2015) propose a method for sarcasm detection in tweets as the task of detecting incongruity between an observed and an expected word, while Karoui et al. (2015) rely on external common knowledge to the utterance shared by author and reader. However, in addition to these pragmatic devices, some hints about the role of syntax can be found in linguistic literature where the deviation from syntactic norms has been reported as a possible trigger of the phenomenon. For instance, Michaelis and Feng (2015) explore the usage of split interrogatives and of topic-comment utterances in English, arguing they both have a double function (they express two distinct communicative acts) and that one of those is precisely that of conveying an ironic meaning (e.g. “Who"
2020.coling-main.116,P14-2050,0,0.356056,"forward binary classification task on irony detection, that is, the task for which literature offers baselines and fair-sized annotated datasets for a variety of languages. For addressing the task, we performed a set of experiments where several models were implemented exploiting classical machine learning algorithms, deep learning architectures and state-of-the-art language models implemented with the Python libraries scikit-learn6 and keras7 . We tested different sets of pre-trained word embeddings to initialize the neural models, namely fastText8 and a dependency-based word2vec proposed by Levy and Goldberg (2014) (word2vecf ). The latter was trained on the concatenation of all the treebanks available in the UD repository for each considered language. In order to combine these methods with syntactic features inspired by Sidorov et al. (2014), we used data where not only a binary annotation for irony is applied, but also a morphological and syntactic analysis is available (see § 4). 4.1 Pre-processing and Features We stripped all the URLs and we normalized all characters to lowercase letters, as it is often done before the application of sentiment analysis tools. We investigated the use of novel feature"
2020.coling-main.116,C18-1159,0,0.0498333,"syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information specifically for irony detection. Among them we cite Cignarella and Bosco (2019), who employed a SVC combined with shallow features based on morphology and dependency syntax outperforming strong baselines in the IroSvA 2019 irony detection shared task in Spanish variants (Ortega-Bueno et al., 2019). This paper aims to go one step further focusing for the first time on the development of syntax-aware irony detection systems in a multilingual perspective (English, Spanish, French and Italian), providing an in-depth inve"
2020.coling-main.116,N18-1088,0,0.0576339,"syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information specifically for irony detection. Among them we cite Cignarella and Bosco (2019), who employed a SVC combined with shallow features based on morphology and dependency syntax outperforming strong baselines in the IroSvA 2019 irony detection shared task in Spanish variants (Ortega-Bueno et al., 2019). This paper aims to go one step further focusing for the first time on the development of syntax-aware irony detection systems in a multilingual perspective (English, Spanish, French and Italian), providing an in-depth inve"
2020.coling-main.116,N18-4006,0,0.0195055,"quality of the morpho-syntactic annotation might provide. The Italian dataset is, in fact, the only one amongst the four ones that has been manually checked and corrected and whose quality could be considered superior compared to the resources used for the other languages. Furthermore, Italian is the only language configuration in which the dependency-based word embeddings have been trained on UD-based text from the same textual genre (tweets). To the best of our knowledge, dependency-based word embeddings were not used before in the detection of irony; on the other hand, in a previous work, MacAvaney and Zeldes (2018) expand the findings of Levy and Goldberg (2014) exploring the effectiveness of various dependency-based word embeddings on domain similarity, word similarity, and two other downstream tasks in English (question-type classification and named entity recognition.). They found that embeddings trained with UD contexts excel only in some tasks but not always improve the performance. 1353 It is our belief that fine-grained syntactic information such as the features we implemented in § 4.1, when added to an already robust architecture, could capture important structures of language and therefore boos"
2020.coling-main.116,W17-5405,0,0.0208141,"Missing"
2020.coling-main.116,P13-2017,0,0.0368726,"t release in the UD official repository3 ), we obtained the dependency-based annotation for the other corpora by applying the UDPipe4 pipeline (for tokenization, PoS-tagging and parsing). Figure 1 provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained on the EWT treebank (Silveira et al., 2014), that for Spanish on both GSD-Spanish (McDonald et al., 2013) and ANCORA corpora (Taulé et al., 2008). The model for Italian – for the remaining part of IronITA5 – was trained on POSTWITA (Sanguinetti et al., 2018) and ISDT treebanks (Simi et al., 2014), while that for French on the GSDFrench corpus (McDonald et al., 2013). We are aware that there actually exists a Twitter treebank for English, i.e. TWEEBANK V2 (Liu et al., 2018b), but it is not fully compliant with the UD format 3 https://github.com/UniversalDependencies/UD_Italian-TWITTIRO. http://ufal.mff.cuni.cz/udpipe. 5 Approximately 1,400 out 4,849 tweets from IronITA are also part of the TWITTIR"
2020.coling-main.116,2020.lrec-1.497,0,0.0484969,"Missing"
2020.coling-main.116,W19-7811,0,0.011322,"dge extracted from treebanks can be usefully exploited for addressing the irony detection task. In particular, they pave the way for a further investigation where the combination of a dependency-based syntactic approach and state-of-the-art neural models can be explored. In the future, in order to validate our findings, we will propose a novel and wider experimental setting, where more languages are included, e.g. testing our approach on other languages for which both ironyannotated datasets and UD resources are available, such as Arabic (Ghanem et al., 2020; Seddah et al., 2020) or German21 (Rehbein et al., 2019). Acknowledgments The work of C. Bosco and V. Basile was partially funded by Progetto di Ateneo/CSP 2016 Immigrants, Hate and Prejudice in Social Media (S1618_L2_BOSC_01). The work of M. Sanguinetti is funded by PRIN 2017 (2019-2022) project HOPE - High quality Open data Publishing and Enrichment. The work of P. Rosso was partially funded by the Spanish MICINN under the research project MISMIS-FAKEnHATE on Misinformation and Miscommunication in social media: FAKE news and HATE speech(PGC2018-096212-B-C31) and by the Generalitat Valenciana under the project DeepPattern (PROMETEO/2019/121). Than"
2020.coling-main.116,L18-1279,1,0.84297,"kenization, PoS-tagging and parsing). Figure 1 provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained on the EWT treebank (Silveira et al., 2014), that for Spanish on both GSD-Spanish (McDonald et al., 2013) and ANCORA corpora (Taulé et al., 2008). The model for Italian – for the remaining part of IronITA5 – was trained on POSTWITA (Sanguinetti et al., 2018) and ISDT treebanks (Simi et al., 2014), while that for French on the GSDFrench corpus (McDonald et al., 2013). We are aware that there actually exists a Twitter treebank for English, i.e. TWEEBANK V2 (Liu et al., 2018b), but it is not fully compliant with the UD format 3 https://github.com/UniversalDependencies/UD_Italian-TWITTIRO. http://ufal.mff.cuni.cz/udpipe. 5 Approximately 1,400 out 4,849 tweets from IronITA are also part of the TWITTIRÒ corpus, already available in UD. 4 1349 root conj punct ccomp ccomp cop punct cc obj cop Spero sia colite . Ma ho paura sia amore . I hope it’s colitis"
2020.coling-main.116,2020.acl-main.107,0,0.0189529,"hesis that morpho-syntactic knowledge extracted from treebanks can be usefully exploited for addressing the irony detection task. In particular, they pave the way for a further investigation where the combination of a dependency-based syntactic approach and state-of-the-art neural models can be explored. In the future, in order to validate our findings, we will propose a novel and wider experimental setting, where more languages are included, e.g. testing our approach on other languages for which both ironyannotated datasets and UD resources are available, such as Arabic (Ghanem et al., 2020; Seddah et al., 2020) or German21 (Rehbein et al., 2019). Acknowledgments The work of C. Bosco and V. Basile was partially funded by Progetto di Ateneo/CSP 2016 Immigrants, Hate and Prejudice in Social Media (S1618_L2_BOSC_01). The work of M. Sanguinetti is funded by PRIN 2017 (2019-2022) project HOPE - High quality Open data Publishing and Enrichment. The work of P. Rosso was partially funded by the Spanish MICINN under the research project MISMIS-FAKEnHATE on Misinformation and Miscommunication in social media: FAKE news and HATE speech(PGC2018-096212-B-C31) and by the Generalitat Valenciana under the project De"
2020.coling-main.116,silveira-etal-2014-gold,0,0.0569576,"Missing"
2020.coling-main.116,simi-etal-2014-less,1,0.831725,"provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained on the EWT treebank (Silveira et al., 2014), that for Spanish on both GSD-Spanish (McDonald et al., 2013) and ANCORA corpora (Taulé et al., 2008). The model for Italian – for the remaining part of IronITA5 – was trained on POSTWITA (Sanguinetti et al., 2018) and ISDT treebanks (Simi et al., 2014), while that for French on the GSDFrench corpus (McDonald et al., 2013). We are aware that there actually exists a Twitter treebank for English, i.e. TWEEBANK V2 (Liu et al., 2018b), but it is not fully compliant with the UD format 3 https://github.com/UniversalDependencies/UD_Italian-TWITTIRO. http://ufal.mff.cuni.cz/udpipe. 5 Approximately 1,400 out 4,849 tweets from IronITA are also part of the TWITTIRÒ corpus, already available in UD. 4 1349 root conj punct ccomp ccomp cop punct cc obj cop Spero sia colite . Ma ho paura sia amore . I hope it’s colitis . But I fear it’s love . VERB AUX NOUN"
2020.coling-main.116,D13-1170,0,0.00799628,"t. The fact that the expression of opinion or the production of irony are sensitive to syntactic variations occurring in texts has been moreover confirmed by Mahler et al. (2017), where artificial syntactic manipulations are applied, such as those based on negations and adverbs, with the purpose of fooling sentiment analysis systems. Some research already explored different kinds of syntactic features and their interaction in several NLP tasks, showing their effectiveness. For example, Sidorov et al. (2012) exploited syntactic dependency-based n-grams for general-purpose classification tasks, Socher et al. (2013) investigated sentiment and syntax with the development of a sentiment treebank, and Kanayama and Iwamoto (2020) showed a pipeline method that makes the most of syntactic structures based on Universal Dependencies, achieving high precision in sentiment detection for 17 languages. Morphology and syntax have also been proved useful in a number of other tasks, such as rumor detection (Ghanem et al., 2019), authorship attribution (Posadas-Duran et al., 2014; Sidorov et al., 2014) and humor detection (Liu et al., 2018a). To the best of our knowledge, very few studies use syntactic information speci"
2020.coling-main.116,taule-etal-2008-ancora,0,0.0304766,"we obtained the dependency-based annotation for the other corpora by applying the UDPipe4 pipeline (for tokenization, PoS-tagging and parsing). Figure 1 provides an example drawn from TWITTIRÒ. Considering that all the datasets used in this work consist of Twitter data, whenever possible, we used resources where this genre, or at least user-generated content of some kind, was included as training data for parsing. More precisely, the model for English has been trained on the EWT treebank (Silveira et al., 2014), that for Spanish on both GSD-Spanish (McDonald et al., 2013) and ANCORA corpora (Taulé et al., 2008). The model for Italian – for the remaining part of IronITA5 – was trained on POSTWITA (Sanguinetti et al., 2018) and ISDT treebanks (Simi et al., 2014), while that for French on the GSDFrench corpus (McDonald et al., 2013). We are aware that there actually exists a Twitter treebank for English, i.e. TWEEBANK V2 (Liu et al., 2018b), but it is not fully compliant with the UD format 3 https://github.com/UniversalDependencies/UD_Italian-TWITTIRO. http://ufal.mff.cuni.cz/udpipe. 5 Approximately 1,400 out 4,849 tweets from IronITA are also part of the TWITTIRÒ corpus, already available in UD. 4 134"
2020.coling-main.116,C96-2162,0,0.260223,"is surveyed, while Section 3 describes the data and experimental settings we followed. In Section 4, we provide a description of the experiments we performed, and, in the last section, we discuss the experiments and provide some final remarks through an error analysis. We conclude the paper with some insights about the impact of morpho-syntax information and suggest possible directions for future work. 2 Related work The identification of irony and the description of pragmatic and linguistic devices that trigger it has always been a controversial topic (Grice, 1975; Sperber and Wilson, 1981; Utsumi, 1996), a challenge for both humans and automatic tools. This motivates the interest for irony of the NLP community, the organization of tasks regarding its processing and the subsequent creation of benchmarks. In the last decade several shared tasks have been organized in order to advance with the techniques regarding the automatic processing of figurative language (Ghosh et al., 2015), and more recently irony and sarcasm detection have been gaining greater attention. Being irony a pragmatic device inherently related to culture and language, the task captured the attention of various researchers, d"
2020.coling-main.116,S18-1005,0,0.0382354,"Missing"
2020.jeptalnrecital-taln.3,P19-1279,0,0.0298723,"Missing"
2020.jeptalnrecital-taln.3,D19-5719,0,0.038578,"Missing"
2020.jeptalnrecital-taln.3,P04-1054,0,0.256619,"Missing"
2020.jeptalnrecital-taln.3,N19-1423,0,0.00982144,"Missing"
2020.jeptalnrecital-taln.3,P15-1061,0,0.0585566,"Missing"
2020.jeptalnrecital-taln.3,W09-2421,0,0.0782934,"Missing"
2020.jeptalnrecital-taln.3,C92-2082,0,0.639688,"Missing"
2020.jeptalnrecital-taln.3,S10-1006,0,0.109141,"Missing"
2020.jeptalnrecital-taln.3,E17-2068,0,0.125571,"Missing"
2020.jeptalnrecital-taln.3,S17-2171,0,0.0407835,"Missing"
2020.jeptalnrecital-taln.3,N07-2032,0,0.13143,"Missing"
2020.jeptalnrecital-taln.3,W15-1506,0,0.0571958,"Missing"
2020.jeptalnrecital-taln.3,C16-1238,0,0.0439949,"Missing"
2020.jeptalnrecital-taln.3,P16-1123,0,0.0489825,"Missing"
2020.jeptalnrecital-taln.3,C16-1119,0,0.0419077,"Missing"
2020.jeptalnrecital-taln.3,C14-1220,0,0.0757326,"Missing"
2020.jeptalnrecital-taln.3,D17-1004,0,0.0402514,"Missing"
2020.jeptalnrecital-taln.3,P05-1053,0,0.337764,"Missing"
2020.jeptalnrecital-taln.3,P16-2034,0,0.0783687,"Missing"
2020.lrec-1.151,abdul-mageed-diab-2012-awatif,0,0.030907,"cities of the ALGD. The annotation platform is described in Section 4 and experiments in Section 5. We finally conclude providing some perspectives for future work. 2. Related Work Over the years OEA has been widely used in a variety of applications such as marketing and politics, etc. These have inspired several methods ranging from lexicon-based approaches (Al-Moslmi et al., 2018) to corpus-based (AbdulMageed and Diab, 2012) to recently Deep learning (AlSmadi et al., 2018). As mentioned in the introduction, numerous studies on Arabic sentiment analysis have been carried out in recent years (Abdul-Mageed and Diab, 2012; Nabil et al., 2015; Badaro et al., 2018). The Arabic dialects are a variety of MSA which includes languages with less normalisation and standardisation (Saadane and Habash, 2015). They differ from 1202 MSA on all levels of linguistic representation, from phonology and morphology to lexicon and syntax. It is worth mentioning that the highest proportion of available resources and research publications in Arabic OEA are devoted to MSA. Regarding Arabic dialects, the MiddleEastern and Egyptian dialects received the largest share of all research effort and funding. On the other hand, very little"
2020.lrec-1.151,habash-etal-2012-conventional,0,0.0652311,"Missing"
2020.lrec-1.151,W14-3603,0,0.0432194,"Missing"
2020.lrec-1.151,W17-1307,0,0.124056,"Missing"
2020.lrec-1.151,S18-1001,0,0.0623725,"Missing"
2020.lrec-1.151,W19-4604,0,0.0228736,"Missing"
2020.lrec-1.151,D15-1299,0,0.194401,"ation platform is described in Section 4 and experiments in Section 5. We finally conclude providing some perspectives for future work. 2. Related Work Over the years OEA has been widely used in a variety of applications such as marketing and politics, etc. These have inspired several methods ranging from lexicon-based approaches (Al-Moslmi et al., 2018) to corpus-based (AbdulMageed and Diab, 2012) to recently Deep learning (AlSmadi et al., 2018). As mentioned in the introduction, numerous studies on Arabic sentiment analysis have been carried out in recent years (Abdul-Mageed and Diab, 2012; Nabil et al., 2015; Badaro et al., 2018). The Arabic dialects are a variety of MSA which includes languages with less normalisation and standardisation (Saadane and Habash, 2015). They differ from 1202 MSA on all levels of linguistic representation, from phonology and morphology to lexicon and syntax. It is worth mentioning that the highest proportion of available resources and research publications in Arabic OEA are devoted to MSA. Regarding Arabic dialects, the MiddleEastern and Egyptian dialects received the largest share of all research effort and funding. On the other hand, very little work has been conduc"
2020.lrec-1.151,pak-paroubek-2010-twitter,0,0.209456,"Missing"
2020.lrec-1.151,W19-5606,0,0.506626,"n Arabic have been widely studied (Baly et al., 2017; Al-Smadi et al., 2018; Abo et al., 2018). Most related work focus on Modern Standard Arabic (MSA), although a few investigated Arab dialects, such as Jordanian (Atoum and Nouman, 2019; Duwairi, 2015), Egyptian (Shoukry and Rafea, 2012), Iraqi (Alnawas and Arici, 2019), Levantine (Baly 1 https://wearesocial.com/blog/2018/01/global-digital-report2018 (visited on 23rd, November 2019) 2 https://www.slideshare.net/wearesocial/digital-in-2018-innorthern-africa-86865355 3 http://gs.statcounter.com/social-media-stats/all/algeria/2019 et al., 2019; Qwaider et al., 2019) and Tunisian (Medhaffar et al., 2017). North African dialects, including Algerian dialects (ALGD) are less normalised compared to MSA. They have been enriched by many languages over the years, which resulted in a complex linguistic situation. Also, we found a significant lack of resources for most of these dialects such as lexicons, dictionaries, and annotated corpora. In this paper, we address the lack of resources for opinion and emotion analysis related to North African dialects, targeting Algerian dialect. We present TWIFIL (TWItter proFILing) a collaborative annotation platform for crowd"
2020.lrec-1.151,W15-3208,0,0.206557,"ver the years OEA has been widely used in a variety of applications such as marketing and politics, etc. These have inspired several methods ranging from lexicon-based approaches (Al-Moslmi et al., 2018) to corpus-based (AbdulMageed and Diab, 2012) to recently Deep learning (AlSmadi et al., 2018). As mentioned in the introduction, numerous studies on Arabic sentiment analysis have been carried out in recent years (Abdul-Mageed and Diab, 2012; Nabil et al., 2015; Badaro et al., 2018). The Arabic dialects are a variety of MSA which includes languages with less normalisation and standardisation (Saadane and Habash, 2015). They differ from 1202 MSA on all levels of linguistic representation, from phonology and morphology to lexicon and syntax. It is worth mentioning that the highest proportion of available resources and research publications in Arabic OEA are devoted to MSA. Regarding Arabic dialects, the MiddleEastern and Egyptian dialects received the largest share of all research effort and funding. On the other hand, very little work has been conducted for the OEA of the Maghrebian dialects (Medhaffar et al., 2017). In addition, research into ALGD is rare which resulted in a lack of resources. The proposed"
2020.lrec-1.151,2012.amta-caas14.7,0,0.0891123,"Missing"
2020.lrec-1.151,J11-2001,0,0.0950831,"Missing"
2020.lrec-1.151,zribi-etal-2014-conventional,0,0.060373,"Missing"
2020.lrec-1.175,2019.jeptalnrecital-court.21,1,0.827557,"sexism detection in French. Furthermore, in a context of offensive content moderation on social media (see the recommendations of the European commission6 ), we think that it is important not 6 only to be able to automatically detect messages with a sexist content but also to distinguish between real sexist messages and reports/denunciations of sexism experiences. Indeed, whereas messages could be reported and moderated in the first case, messages reporting sexism experiences should not be moderated. 3. 3.1. Data and Annotation Data Collection Our corpus is new and extends the one we used in (Chiril et al., 2019). It contains French tweets collected between October 2017 and May 2018. In order to collect sexist and non sexist tweets, we followed Anzovino et al. (2018) approach using: • a set of representative keywords: femme, fille (woman, girl), enceinte (pregnant), some activities (cuisine (cooking), football, ...), insults, etc., • the names of women/men potentially victims or guilty of sexism (mainly politicians): Ségolène Royal, Nadine Morano, Theresa May, Hillary Clinton, Dominique Strauss-Kahn, Nicolas Hulot, etc., • specific hashtags to collect stories of sexism experiences: #balancetonporc, #s"
2020.lrec-1.175,N19-1423,0,0.065004,"Missing"
2020.lrec-1.175,L18-1550,0,0.0299867,"Missing"
2020.lrec-1.175,S19-2009,0,0.0612504,"d ensemble of classifiers for both tasks with features such as n-grams and opinions (Fersini et al., 2018). Very few participants used deep learning approaches with word embeddings and best results were obtained with SVM models. These datasets have also been used in the Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter shared task at SemEval 2019. The tasks were the same as those of AMI except that it concerned not only sexism against women but also hate speech against immigrants. The best results were obtained with a SVM model using sentence embeddings as feature (Indurthi et al., 2019). As far as we know, no work have addressed sexism detection in French. Furthermore, in a context of offensive content moderation on social media (see the recommendations of the European commission6 ), we think that it is important not 6 only to be able to automatically detect messages with a sexist content but also to distinguish between real sexist messages and reports/denunciations of sexism experiences. Indeed, whereas messages could be reported and moderated in the first case, messages reporting sexism experiences should not be moderated. 3. 3.1. Data and Annotation Data Collection Our co"
2020.lrec-1.175,W17-2902,0,0.664991,"7), (Waseem and Hovy, 2016). Hate speech detection covers mainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Sharifirad et al., 2018), except in (Parikh et al., 2019) who consider messages of sexism experienced by women in the &quot;Everyday Sexism Project&quot; web site and whose categories are not mutually exclusive. To our knowledge, the automatic detection of sexist messages currently deals only with English, Italian and Spanish. For example in the Automatic Misogyny Identification (AMI) shared task at IberEval and EvalIta 2018, the tasks consisted in detecting sexist tweets and then identifying the type of sexist behaviour according to a taxonomy defined by (Anzovino et al., 2018): discredit, stereotype"
2020.lrec-1.175,P15-2106,1,0.88968,"Missing"
2020.lrec-1.175,D19-1474,0,0.0124371,"ther it provides meaning at the level of the presupposition (or more largely non at-issue content (Potts, 2005)) or of the assertion (Cepollaro, 2015). Our study pursues a different line of analysis, whereby speech acts bearing on derogatory content are ranked according to their perlocutionary force and assertions are classified as more or less direct. Specifically, in order to make emerge different degrees of downgrading tones, we have chosen to distinguish cases where the addressee is directly addressed from those in which she is not, as done in hate speech analysis (ElSherief et al., 2018; Ousidhoum et al., 2019). ElSherief et al. (2018) consider that directed hate speech is explicitly directed at a https://eur-lex.europa.eu/legal-content/ EN/TXT/HTML/?uri=CELEX:52017DC0555 1398 person while generalized hate speech targets a group. For (Ousidhoum et al., 2019), a hateful tweet is direct when the target is explicitly named, or indirect when &quot;less easily discernible&quot;. Unlike these approaches, we newly consider three different stages in the scale of ‘directedness’ of an assertion: assertions directed to the addressee, descriptive assertions and reported assertions. Sexist content in directed assertions i"
2020.lrec-1.175,D19-1174,0,0.206688,"ainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Sharifirad et al., 2018), except in (Parikh et al., 2019) who consider messages of sexism experienced by women in the &quot;Everyday Sexism Project&quot; web site and whose categories are not mutually exclusive. To our knowledge, the automatic detection of sexist messages currently deals only with English, Italian and Spanish. For example in the Automatic Misogyny Identification (AMI) shared task at IberEval and EvalIta 2018, the tasks consisted in detecting sexist tweets and then identifying the type of sexist behaviour according to a taxonomy defined by (Anzovino et al., 2018): discredit, stereotype, objectification, sexual harassment, threat of violence, d"
2020.lrec-1.175,W17-1101,0,0.0274121,"on 4. presents the experiments we carried out on our data. We conclude providing some perspectives for future work. 2. Related Work In corpus construction, sexism is often considered as “hate speech” (Golbeck et al., 2017) and has been widely studied as such in a purpose of automatic detection (Badjatiya et al., 2017), (Waseem and Hovy, 2016). Hate speech detection covers mainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Sharifirad et al., 2018), except in (Parikh et al., 2019) who consider messages of sexism experienced by women in the &quot;Everyday Sexism Project&quot; web site and whose categories are not mutually exclusive. To our knowledge, the automatic detection of sexist messages currently deals only wit"
2020.lrec-1.175,W18-5114,0,0.0121795,"2016). Hate speech detection covers mainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Sharifirad et al., 2018), except in (Parikh et al., 2019) who consider messages of sexism experienced by women in the &quot;Everyday Sexism Project&quot; web site and whose categories are not mutually exclusive. To our knowledge, the automatic detection of sexist messages currently deals only with English, Italian and Spanish. For example in the Automatic Misogyny Identification (AMI) shared task at IberEval and EvalIta 2018, the tasks consisted in detecting sexist tweets and then identifying the type of sexist behaviour according to a taxonomy defined by (Anzovino et al., 2018): discredit, stereotype, objectification, sexual"
2020.lrec-1.175,N19-1214,0,0.041673,"Missing"
2020.lrec-1.175,N16-2013,0,0.0780955,"ts are encouraging and constitute the novel state of the art on sexism detection in French. The paper is organized as follows. Section 2. presents state of the art. Section 3. describes our data, the characterization of sexism content we propose and the annotation scheme. Section 4. presents the experiments we carried out on our data. We conclude providing some perspectives for future work. 2. Related Work In corpus construction, sexism is often considered as “hate speech” (Golbeck et al., 2017) and has been widely studied as such in a purpose of automatic detection (Badjatiya et al., 2017), (Waseem and Hovy, 2016). Hate speech detection covers mainly the detection of explicit racist, abusive or offensive textual content using supervised machine learning approaches either with bag-of-words, or dedicated lexicons, word embeddings, clustering, or author profile (see (Schmidt and Wiegand, 2017) for a survey). Current work consider sexism detection (sexist vs. non sexist) or sexism classification (identifying the type of sexist behaviours). In the last case, categories are most often mutually exclusive (e.g., harassment, threat, physical violence, body shaming, benevolent, etc.) (Jha and Mamidi, 2017; Shari"
2021.findings-emnlp.242,2020.lrec-1.175,1,0.865363,"Missing"
2021.findings-emnlp.242,S19-2007,0,0.0226553,"gnize myself in the &quot;Just Fab&quot; ad with a media is mainly supported by dedicated shared screaming hysterical bitch?. tasks that developed their own datasets, for ex• Activities are activities, jobs, hobbies that are ample the AMI corpus mentioned above. These stereotypically assigned to women as in Never datasets (in English, Spanish and Italian) have also marry a woman who cannot cook which imbeen used in the Multilingual Detection of Hate plies that a woman’s place is in the kitchen, or Speech Against Immigrants and Women in Twitter no woman understands football. shared task at SemEval 2019 (Basile et al., 2019). Compared to existing datasets annotated for GS, Best results were obtained with an SVM model ours offers a finer characterization (e.g., 2 cateusing sentence embeddings as features (Indurthi gories in (Parikh et al., 2019) and only 1 in AMI), et al., 2019). Lazzardi et al. (2021) conducted a while capturing major stereotypes dimensions, as study on this corpus to understand why participants proposed in gender and communication science obtained low scores on the identification of the studies (Ellemers, 2018; Crawford et al., 2002). particular type of misogynous behaviour against 3 women (amon"
2021.findings-emnlp.242,2020.acl-main.373,1,0.764726,"s of stereotypes. Note that when a stereotype is present, it can be expressed explicitly, implicitly (i.e., one can infer a content such as ‘(all) women are...’) or it can be a denunciation/criticism of a GS.4 Waseem and Hovy (2016) provide the first corpus of tweets annotated with racism and sexism and use a logistic regression classifier with n-grams features for hate speech detection. There are also a few notable neural network techniques: LSTM • Physical characteristics are related to physi(Jha and Mamidi, 2017) or CNN+GRU (Zhang cal strength or aspect. For example, the mesand Luo, 2018). Chiril et al. (2020b) use a BERT sage Short hair for a girl it’s a bad idea conmodel trained on word embeddings, linguistic feaveys the stereotype &quot;Girls must have long tures and generalization strategies to distinguish hair&quot;. reports/denunciations of sexism from real sexist • Behavioural characteristics are related to incontent that are directly addressed to a target. telligence, emotions, sensibility or behaviour Overall, as for stereotype detection, the work on as in the denouncing tweet Am I supposed to automatic detection of sexist messages on social recognize myself in the &quot;Just Fab&quot; ad with a media is mai"
2021.findings-emnlp.242,W19-2306,0,0.0136897,"/bit.ly/FrenchSexism The corpus being quite small, especially the stereotype class, we decided to augment the training data to counter class imbalance. There are several strategies for data augmentation among which (see (Padurariu and Breaban, 2019) for an overview): oversampling (adding instances to the minority class with replacement (bootstrapping)), weighting the data during classification, adapting the loss function of the classification model, collecting more data or generating new instances similar to the ones belonging to the minority class. To generate new data, Ray et al. (2018) and Cho et al. (2019) use paraphrase generation in the domain of Spoken Language Understanding. Chawla et al. (2002) use the Synthetic Minority Oversampling Technique (SMOTE) which finds an instance similar to the one being oversampled and creates an instance that is a randomly weighted average of the original and the neighboring instance. Wei and Zou (2019) propose to extend data with simple operations: synonym replacement, random insertion, random swap, and random deletion. Hemker and Schuller (2018) use Natural Language Generation models for auto-generating new semantically similar instances based on the traini"
2021.findings-emnlp.242,N19-1423,0,0.0769604,"Missing"
2021.findings-emnlp.242,D19-1635,0,0.0489517,"Missing"
2021.findings-emnlp.242,L18-1590,0,0.0329763,". 5 http://bit.ly/FrenchSexism The corpus being quite small, especially the stereotype class, we decided to augment the training data to counter class imbalance. There are several strategies for data augmentation among which (see (Padurariu and Breaban, 2019) for an overview): oversampling (adding instances to the minority class with replacement (bootstrapping)), weighting the data during classification, adapting the loss function of the classification model, collecting more data or generating new instances similar to the ones belonging to the minority class. To generate new data, Ray et al. (2018) and Cho et al. (2019) use paraphrase generation in the domain of Spoken Language Understanding. Chawla et al. (2002) use the Synthetic Minority Oversampling Technique (SMOTE) which finds an instance similar to the one being oversampled and creates an instance that is a randomly weighted average of the original and the neighboring instance. Wei and Zou (2019) propose to extend data with simple operations: synonym replacement, random insertion, random swap, and random deletion. Hemker and Schuller (2018) use Natural Language Generation models for auto-generating new semantically similar instanc"
2021.findings-emnlp.242,S19-2009,0,0.0448745,"Missing"
2021.findings-emnlp.242,W17-2902,0,0.0407662,"occupational status). These both definitions lead us to the definition of the following 3 categories of stereotypes. Note that when a stereotype is present, it can be expressed explicitly, implicitly (i.e., one can infer a content such as ‘(all) women are...’) or it can be a denunciation/criticism of a GS.4 Waseem and Hovy (2016) provide the first corpus of tweets annotated with racism and sexism and use a logistic regression classifier with n-grams features for hate speech detection. There are also a few notable neural network techniques: LSTM • Physical characteristics are related to physi(Jha and Mamidi, 2017) or CNN+GRU (Zhang cal strength or aspect. For example, the mesand Luo, 2018). Chiril et al. (2020b) use a BERT sage Short hair for a girl it’s a bad idea conmodel trained on word embeddings, linguistic feaveys the stereotype &quot;Girls must have long tures and generalization strategies to distinguish hair&quot;. reports/denunciations of sexism from real sexist • Behavioural characteristics are related to incontent that are directly addressed to a target. telligence, emotions, sensibility or behaviour Overall, as for stereotype detection, the work on as in the denouncing tweet Am I supposed to automati"
2021.findings-emnlp.242,D18-1303,0,0.0602845,"Missing"
2021.findings-emnlp.242,2020.lrec-1.302,0,0.0884107,"Missing"
2021.findings-emnlp.242,2020.acl-main.45,0,0.0606229,"Missing"
2021.findings-emnlp.242,P19-1441,0,0.0298015,"ate speech and stereotypes. However, in our case, since we rely on two different datasets (one for each task), we used the stereotype predictions of the best performing stereotype model (i.e., BERTConceptNet ) to automatically label the sexism dataset with stereotype information. AngryBERT (Awal et al., 2021). This model was specifically designed to address the problem of imbalanced datasets by jointly learning hate speech detection with emotion classification and target identification as secondary tasks. It has been shown to outperform many strong existing multitask models, including MT-DNN (Liu et al., 2019). In our case, the primary task of AngryBERT is sexism detection while the second being the detection of stereotypes. In addition to this initial configuration (AngryBERTbase ), four models are newly proposed, depending on both (i) the number of labels to predict in the auxiliary task, and (ii) the dataset on which the generalization with hypernyms is performed. Chiril et al. (2020b) showed that on their sexism dataset the generalization strategy performs well. In addition, we observed that a similar generalization can be employed for our task with good results. Based on these observations we"
2021.findings-emnlp.242,2020.acl-main.645,0,0.0895932,"Missing"
2021.findings-emnlp.242,D19-1174,0,0.247066,"al. (2020) consider the interaction between hate speech and stereotype detection by employing a multitask learning approach achieving the best scores in the competition. The presence of stereotypes against immigrants has also been annotated in Italian (Sanguinetti et al., 2018) and Spanish political debates (Sánchez-Junquera et al., 2021), the latter being annotated according to a finegrained taxonomy to capture the positive (threats) and negative dimensions (victims) of stereotypes. Concerning GS, there are some datasets dedicated to sexist hate speech annotated with stereotype. Among them, Parikh et al. (2019) propose a dataset which contains 13,023 accounts of sexism extracted from the Everyday Sexism Project 2 Related Work website manually annotated with 23 labels. The annotation scheme includes two categories for GS: 2.1 Stereotypes in Social Sciences role stereotyping (i.e., false generalizations about Stereotypes can be useful for making quick assercertain roles being more appropriate for women) tions, but the reader should keep in mind that by and attribute stereotyping (i.e., linking women to categorizing people only based on their gender, relisome physical, psychological, or behavioural qua"
2021.findings-emnlp.242,D18-1302,0,0.0391436,"Missing"
2021.findings-emnlp.242,D19-1410,0,0.0288869,"nstances based on the training data. However, the new instances with these methods may contain the same or similar words as the original instance but in a different order, which may result in generating instances that do not make sense to humans. In addition, these methods do not guarantee that the new generated instances belong to the same class as the original ones. To avoid this, we propose a new approach for data augmentation based on sentence similarity. We use SentenceBERT, a modification of BERT that derives semantically sentence embeddings that can be compared using cosine-similarity (Reimers and Gurevych, 2019), to extend our training dataset with the most similar sentences from two sources: (S1) New tweets in French collected with a small set of keywords usually used in stereotypes about women: moche (ugly), fesses (butt), jupe (skirt), bavarde (gossipy), dépensière (spendthrift), dévouée (devoted), infirmière (nurse), poupée (doll). These keywords are different from those used for the initial data collection; and (S2) New tweets from existing multilingual datasets annotated for stereotypes. Since there is no other available resource in French, we tried to extend our initial training corpus in two"
2021.findings-emnlp.242,P18-1216,0,0.0430395,"Missing"
2021.findings-emnlp.242,N16-2013,0,0.236135,"tribute supposedly “natural” and “normal” characteristics (psychological traits, behaviours, social roles or activities) to women and men. Deaux and Lewis (1984) define GS as having different and independent components (i.e., trait descriptors, physical characteristics, role behaviours and occupational status). These both definitions lead us to the definition of the following 3 categories of stereotypes. Note that when a stereotype is present, it can be expressed explicitly, implicitly (i.e., one can infer a content such as ‘(all) women are...’) or it can be a denunciation/criticism of a GS.4 Waseem and Hovy (2016) provide the first corpus of tweets annotated with racism and sexism and use a logistic regression classifier with n-grams features for hate speech detection. There are also a few notable neural network techniques: LSTM • Physical characteristics are related to physi(Jha and Mamidi, 2017) or CNN+GRU (Zhang cal strength or aspect. For example, the mesand Luo, 2018). Chiril et al. (2020b) use a BERT sage Short hair for a girl it’s a bad idea conmodel trained on word embeddings, linguistic feaveys the stereotype &quot;Girls must have long tures and generalization strategies to distinguish hair&quot;. repor"
2021.findings-emnlp.242,D19-1670,0,0.0286437,"(bootstrapping)), weighting the data during classification, adapting the loss function of the classification model, collecting more data or generating new instances similar to the ones belonging to the minority class. To generate new data, Ray et al. (2018) and Cho et al. (2019) use paraphrase generation in the domain of Spoken Language Understanding. Chawla et al. (2002) use the Synthetic Minority Oversampling Technique (SMOTE) which finds an instance similar to the one being oversampled and creates an instance that is a randomly weighted average of the original and the neighboring instance. Wei and Zou (2019) propose to extend data with simple operations: synonym replacement, random insertion, random swap, and random deletion. Hemker and Schuller (2018) use Natural Language Generation models for auto-generating new semantically similar instances based on the training data. However, the new instances with these methods may contain the same or similar words as the original instance but in a different order, which may result in generating instances that do not make sense to humans. In addition, these methods do not guarantee that the new generated instances belong to the same class as the original on"
2021.findings-emnlp.242,N18-2003,0,0.0508564,"Missing"
2021.findings-emnlp.242,L18-1443,0,0.0225369,"peeDe 2 shared task contains annotated tweets and newspaper headlines, with the main goal of identifying contents that convey hate or prejudice against a given target (immigrants, Muslims and Roma people) with an auxiliary task of determining the presence or absence of a stereotype towards that given target. Among participants, only Lavergne et al. (2020) consider the interaction between hate speech and stereotype detection by employing a multitask learning approach achieving the best scores in the competition. The presence of stereotypes against immigrants has also been annotated in Italian (Sanguinetti et al., 2018) and Spanish political debates (Sánchez-Junquera et al., 2021), the latter being annotated according to a finegrained taxonomy to capture the positive (threats) and negative dimensions (victims) of stereotypes. Concerning GS, there are some datasets dedicated to sexist hate speech annotated with stereotype. Among them, Parikh et al. (2019) propose a dataset which contains 13,023 accounts of sexism extracted from the Everyday Sexism Project 2 Related Work website manually annotated with 23 labels. The annotation scheme includes two categories for GS: 2.1 Stereotypes in Social Sciences role ster"
afantenos-etal-2012-empirical,C10-1001,1,\N,Missing
afantenos-etal-2012-empirical,J00-3005,0,\N,Missing
afantenos-etal-2012-empirical,J96-1002,0,\N,Missing
afantenos-etal-2012-empirical,J95-2003,0,\N,Missing
afantenos-etal-2012-empirical,J05-2005,0,\N,Missing
afantenos-etal-2012-empirical,prasad-etal-2008-penn,0,\N,Missing
afantenos-etal-2012-empirical,J86-3001,0,\N,Missing
afantenos-etal-2012-empirical,W04-2322,0,\N,Missing
afantenos-etal-2012-empirical,2009.jeptalnrecital-court.23,0,\N,Missing
C04-1170,W04-0202,1,0.815793,"nstraint on the focus): close-by villages where bungalows meet the 15 person constraint. The response involves evaluating village proximity and sorting responses, e.g. by increasing distance from Meg`eve. The first part of the response There are no bungalows in Meg`eve is the direct response, which corrects the user false presupposition, while the remainder of the response reflects the cooperative know-how of the responder. Cooperative know-how involves several forms of responses that include relaxations, intensional calculus, expression of restrictions, of warnings and conditional responses (Benamara et al. 2004). This simple example shows that, if a direct response cannot be found, several forms of knowledge and reasoning schemas need to be used and that the NL form of the response requires adequate and subtle lexicalisations, often directed by the reasoning schemas. Lexicalisation is the operation that associates a word or an expression to a concept. It is a major parameter in response production (Reiter et Dale, 1997), (a good synthesis can be found in (Cahill, 1999)). Lexicalisation is often decomposed into two different stages: lexical choice, which occurs during content determination, where the"
C04-1170,C82-1066,0,\N,Missing
C08-2002,W07-1515,0,\N,Missing
candito-etal-2014-developing,mouton-etal-2010-framenet,1,\N,Missing
candito-etal-2014-developing,burchardt-etal-2006-salto,0,\N,Missing
candito-etal-2014-developing,burchardt-pennacchiotti-2008-fate,0,\N,Missing
candito-etal-2014-developing,sagot-etal-2010-lexicon,1,\N,Missing
candito-etal-2014-developing,N10-1138,0,\N,Missing
candito-etal-2014-developing,burchardt-etal-2006-salsa,0,\N,Missing
candito-etal-2014-developing,P98-1013,0,\N,Missing
candito-etal-2014-developing,C98-1013,0,\N,Missing
candito-etal-2014-developing,W13-4917,1,\N,Missing
candito-etal-2014-developing,P11-2023,1,\N,Missing
candito-etal-2014-developing,J02-3001,0,\N,Missing
candito-etal-2014-developing,J05-1004,0,\N,Missing
candito-etal-2014-developing,heppin-gronostaj-2012-rocky,0,\N,Missing
candito-etal-2014-developing,I11-1132,1,\N,Missing
candito-etal-2014-developing,abeille-barrier-2004-enriching,0,\N,Missing
D13-1035,Q13-1005,0,0.0263927,"adapt their algorithm from coherence relations to unary dialogue acts. Further, while they assume that preferences are given, here we apply versions of the NLP techniques from Cadilhac et al (2012) to estimate the preferences of EDUs automatically. And we go further than any of these works by using the elicited preferences to infer the domain-level actions that result from information exchanged in the conversation. In this respect, our work relates to models for grounding language, where semantic parsing techniques are used to automatically map linguistic instructions to domain-level actions (Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013). Our domain of application is more challenging, however: to our knowledge, this is the first attempt to map non-cooperative dialogues into predictions about domain-level actions. We can tackle these strategic scenarios because we exploit a logic of preferences as part of our model, yielding inferences about rational action even when agents’ preferences conflict. Compared to previous work, our task is new. Our aim is not to predict what dialogue act to perform next, but what non verbal action should be performed, mapping dialogue acts to non verbal actions. The differenc"
D13-1035,I11-1132,1,0.757664,"position of the speaker’s first turn in the dialogue. We have 15 unigram and bigram features (at levels 0 and -1), as well as templates that combine feature values for the two levels. These include 14 boolean features that indicate if the EDU contains: bargaining verbs (e.g. trade, offer), references to another player (e.g. you), resource tokens as encoded in a task dedicated lexicon (e.g. wheat, clay), quantifiers (e.g. one, none), anaphoric pronouns, occurrences of “for” prepositional phrases (e.g. wheat for clay), acceptance words (e.g. OK), negation words, emoticons, opinion words (from (Benamara et al., 2011)), words of politeness, exclamation marks, questions, and finally whether the EDU’s speaker has talked previously in the dialogue. The last feature gives the EDU speaker lemma. In addition, 3 unigram and bigram booleans indicate whether the current EDU contains the most frequent tokens, couple of tokens and syntactic patterns in our corpus. Finally, we use 2 composed bigram features that encode whether the EDU contains an acceptance or refusal word, given that the previous EDU is a question. To assign sequential tags of dialogue acts within a negotiation dialogue, we use the CRF++ tool (crfpp."
D13-1035,W11-2023,1,0.906158,"s of resources in the EDUs (Givable, etc.) to identify the preference that a speaker conveys in the EDU, and we use the dialogue acts (Offer, Accept, etc.) to update a model of the preferences expressed so far in the dialogue with this new preference (see Section 5.2). Our model of preferences consists of a set of partial CP-nets, one for each player (see Section 5.1 for details). The resulting CP-nets are then used to infer the executed trading action (if any) automatically, via wellunderstood principles from game theory for identifying rational behavior (Bonzon, 2007). 5.1 CP-Nets Following Cadilhac et al. (2011), we use CP-nets (Boutilier et al., 2004) to model preferences and their dependencies. CP-nets are compatible with the kind of partial information about preferences that utterances reveal, and inference with CP-nets is com361 putationally efficient. Just as Bayesian nets are a graphical model that exploits probabilistic conditional independence to provide a compact representation of a joint probability distribution (Pearl, 1988), CP-nets are a graphical model that exploits conditional preferential independence to provide a compact representation of the preference order over all outcomes. The C"
D13-1035,2005.sigdial-1.9,0,0.0601039,"Missing"
D13-1035,W02-0213,0,0.0928596,"Missing"
D13-1035,P13-1022,0,0.0245899,"herence relations to unary dialogue acts. Further, while they assume that preferences are given, here we apply versions of the NLP techniques from Cadilhac et al (2012) to estimate the preferences of EDUs automatically. And we go further than any of these works by using the elicited preferences to infer the domain-level actions that result from information exchanged in the conversation. In this respect, our work relates to models for grounding language, where semantic parsing techniques are used to automatically map linguistic instructions to domain-level actions (Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013). Our domain of application is more challenging, however: to our knowledge, this is the first attempt to map non-cooperative dialogues into predictions about domain-level actions. We can tackle these strategic scenarios because we exploit a logic of preferences as part of our model, yielding inferences about rational action even when agents’ preferences conflict. Compared to previous work, our task is new. Our aim is not to predict what dialogue act to perform next, but what non verbal action should be performed, mapping dialogue acts to non verbal actions. The difference between our work and"
D13-1035,D10-1084,0,0.022097,"FP FN TN WP Accuracy 10 43 154 15 73.4 Table 5: Results for the end to end trade prediction. 365 6 6.1 Related Work Dialogue act modeling Most work on dialogue act modeling focuses on spoken dialogue (Stolcke et al., 2000; Fern´andez et al., 2005; Keizer et al., 2002). But live chats introduce specific complications (Kim et al., 2012): ill-formed data, abbreviations and acronyms, emotional indicators and entanglement (especially for multi-party chat). Among related work in this emerging field, Joty et al. (2011) use unsupervised learning to model dialogue acts in Twitter, Ivanovic (2008) and Kim et al. (2010) analyze one-to-one online chat in a customer service domain, and Wu et al. (2002) and Kim et al. (2012) predict dialogue acts in a multi-party setting. We used a similar classifier to predict dialogue acts as the one reported in (Kim et al., 2012) and evaluation yields similar results. This paper proposes an approach to dialogue act identification in online chat that aims to predict strategic actions like bargaining. Compared to (Sidner, 1994) and DAMSL (Core and Allen, 1997), our domain level annotation is much more detailed: we not only predict moves like Accept but also features like the G"
D13-1035,Y12-1050,0,0.0151934,"asks: automatically identifying each EDU’s dialogue act; detecting the EDU’s resources; and specifying the attributes of those resources (i.e., Givable, Receivable, etc.). 359 4.1 Identifying dialogue acts As is well established, one EDU’s dialogue act depends on previous dialogue acts (Stolcke et al., 2000). In our corpus, Accept or Reject frequently follow Offer and Counteroffer. Since labeling is sequential, we use Conditional Random Fields (CRFs) to learn dialogue acts. CRFs have been shown to yield better results in dialogue act classification on online chat than HMM-SVN and Naive Bayes (Kim et al., 2012). We use three types of features: lexical, syntactic and semantic. And we exploit them as unigrams and bigrams: unigrams associate the value of the feature with the current output class (level 0); bigrams take account of the value of the feature associated with a combination of the current output class and previous output class (level -1). 6 features were used exclusively as unigrams: the EDU’s position in the dialogue, its first and last words, its subject lemma, a boolean feature to indicate if the current speaker is the one that initiates the dialogue and the position of the speaker’s first"
D13-1035,P03-1054,0,0.009038,"Missing"
D13-1035,J00-3003,0,0.154358,"unknown values: the annotation tool inserts a ? in these cases. We also insist that the annotators resolve anaphoric dependencies when specifying values to attributes, as shown in EDU (4) in Table 1. 4 Dialogue act and resource prediction Predicting the executed trades from the dialogues starts with three sub-tasks: automatically identifying each EDU’s dialogue act; detecting the EDU’s resources; and specifying the attributes of those resources (i.e., Givable, Receivable, etc.). 359 4.1 Identifying dialogue acts As is well established, one EDU’s dialogue act depends on previous dialogue acts (Stolcke et al., 2000). In our corpus, Accept or Reject frequently follow Offer and Counteroffer. Since labeling is sequential, we use Conditional Random Fields (CRFs) to learn dialogue acts. CRFs have been shown to yield better results in dialogue act classification on online chat than HMM-SVN and Naive Bayes (Kim et al., 2012). We use three types of features: lexical, syntactic and semantic. And we exploit them as unigrams and bigrams: unigrams associate the value of the feature with the current output class (level 0); bigrams take account of the value of the feature associated with a combination of the current o"
E03-1060,W03-2301,1,0.851268,"Missing"
E17-1025,barbieri-saggion-2014-modelling-irony,0,0.0169511,"Related work Most state of the art approaches rely on automatically built social media data collections to detect irony using a variety of features gleaned from the utterance-internal context going from ngram models, stylistic, to dictionary-based features (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Joshi et al., 2015; Hern´andez Far´ıas et al., 2015). In addition to the above more lexical features, many authors point out the contribution of pragmatic features, such as the use of common vs. rare words or synonyms (Barbieri and Saggion, 2014). Recent work explores other kinds of contextual information like author profiles, conversational threads, or querying external sources of information (Bamman and Smith, 2015; Wallace et al., 2015; Karoui et al., 5 For Italian, only values for markers automatically identified reliably, without need of manual correction, are reported (e.g. emoticons, negations). Values for other markers are currently missing since they require a manual check, for instance the case of capital letters, because of the presence in the Italian corpus where all the letters are capital. 6 For both settings, frequencie"
E17-1025,W13-1614,0,0.0120924,"Missing"
E17-1025,L16-1256,1,0.860899,"sentiment polarity and irony levels. E.g. Van Hee et al. (2016) distinguish between ironic, possibly ironic, and non-ironic tweets in English and Dutch. For ironic statements, polarity change that causes irony was annotated to specify whether the change comes from an opposition explicitly marked by a contrast between a positive situation and a negative one, an hyperbole, or an understatement. Stranisci et al. (2016) recently extend the Italian Senti-TUT schema (cf. Section 2) to mark the aspects of the topic being discussed in the tweet, as well as the sentiment expressed towards each aspect. Bosco et al. (2016) propose a second extension with the annotation of French tweets using three labels: positive irony, negative irony, and metaphorical expression. Current state of the art corpus-based studies are mainly oriented to a sentiment analysis perspective on irony, focusing almost exclusively on cap8 Exploiting the annotated corpus for automatic irony detection The French and Italian parts of the annotated corpus have been respectively exploited as datasets for the first irony detection shared tasks DEFT@TALN20177 and for the SENTIPOLC@Evalita shared task on irony detection8 in both 2014 and 2016 edit"
E17-1025,P09-2041,0,0.00722378,"arkers are correlated to irony categories, the more discriminant markers are: intensifiers, punctuation, false assertion and opinion words for French (large Cramer’s V); negations, discourse connectors and personal pronouns for English (medium Cramer’s V); and punctuation, interjections and named entities for Italian (medium Cramer’s V). 7 Related work Most state of the art approaches rely on automatically built social media data collections to detect irony using a variety of features gleaned from the utterance-internal context going from ngram models, stylistic, to dictionary-based features (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Joshi et al., 2015; Hern´andez Far´ıas et al., 2015). In addition to the above more lexical features, many authors point out the contribution of pragmatic features, such as the use of common vs. rare words or synonyms (Barbieri and Saggion, 2014). Recent work explores other kinds of contextual information like author profiles, conversational threads, or querying external sources of information (Bamman and Smith, 2015; Wallace et al., 2015; Karoui et al., 5 For Italian, only values for markers autom"
E17-1025,S15-2117,1,0.0614441,"Missing"
E17-1025,W14-2608,0,0.0178236,"ges have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis of the phenomenon of irony in order to represent how Twitter’s users exploit irony devices within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts local to the tweet. Informed by linguistic theories, we propose for the first tim"
E17-1025,P15-2124,0,0.0531044,"nd opinion words for French (large Cramer’s V); negations, discourse connectors and personal pronouns for English (medium Cramer’s V); and punctuation, interjections and named entities for Italian (medium Cramer’s V). 7 Related work Most state of the art approaches rely on automatically built social media data collections to detect irony using a variety of features gleaned from the utterance-internal context going from ngram models, stylistic, to dictionary-based features (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Joshi et al., 2015; Hern´andez Far´ıas et al., 2015). In addition to the above more lexical features, many authors point out the contribution of pragmatic features, such as the use of common vs. rare words or synonyms (Barbieri and Saggion, 2014). Recent work explores other kinds of contextual information like author profiles, conversational threads, or querying external sources of information (Bamman and Smith, 2015; Wallace et al., 2015; Karoui et al., 5 For Italian, only values for markers automatically identified reliably, without need of manual correction, are reported (e.g. emoticons, negations). Values f"
E17-1025,P15-2106,1,0.432443,"d here http://github.com/ IronyAndTweets/. addressed languages in order to tackle their features. In English and French, users employ specific hashtags (#irony, #sarcasm, #sarcastic) to mark their intention to be ironic. These hashtags have been often used as gold labels to detect irony in a supervised learning setting. Although this approach cannot be generalized well since not all ironic tweets contain hashtags, it has however shown to be quite reliable as good inter-annotator agreements (kappa around 0.75) between annotators’ irony label and the reference irony hashtags have been reported (Karoui et al., 2015). Nevertheless, irony corpus construction through hashtag filtering is not always possible for all languages. For instance, both in Czech and Italian, Twitter users generally do not use the sarcasm (i.e. ‘#sarkasmus’, in Czech; ‘#sarcasmo’ in Italian) or irony (‘#ironie’ in Czech or ‘#ironia’ in Italian) hashtag variants to mark their intention to be ironic, thus in such cases relying on simple self-tagging for collecting ironic samples is not an option (Pt´acˇ ek et al., 2014; Bosco et al., 2013). Similar considerations hold for Chinese (Tang and Chen, 2014). For what concerns Italian, we obs"
E17-1025,W10-2914,0,0.355093,"between the categories and markers; and finally (4) see if different languages have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis of the phenomenon of irony in order to represent how Twitter’s users exploit irony devices within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts lo"
E17-1025,S15-2080,0,0.0470233,"is thus four folds: (1) analyse if these categories are also valid in social media contents, focusing on tweets which are short messages (140 characters) where the context may not be explicitly represented; (2) examine whether these categories are linguistically marked; (3) test if there is a correlation between the categories and markers; and finally (4) see if different languages have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis of the phenomenon of"
E17-1025,W13-1605,0,0.024899,"Missing"
E17-1025,maynard-greenwood-2014-cares,0,0.0325935,"media. The goal of the paper is thus four folds: (1) analyse if these categories are also valid in social media contents, focusing on tweets which are short messages (140 characters) where the context may not be explicitly represented; (2) examine whether these categories are linguistically marked; (3) test if there is a correlation between the categories and markers; and finally (4) see if different languages have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis"
E17-1025,P11-2102,0,0.340944,"s and markers; and finally (4) see if different languages have a preference for different categories. This analysis can be exploited in a purpose of automatic irony detection, which is progressively gaining relevance within sentiment analysis (Maynard and Greenwood, 2014; Ghosh et al., 2015). In particular, it will bring out the most discriminant pragmatic features that need to be taken into account for an accurate irony detection, therefore helping systems improve beyond standard approaches that still heavily rely on features gleaned from the utterance-internal context (Davidov et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Buschmeier et al., 2014; Hern´andez Far´ıas et al., 2016). To this end, informed by well-established linguistic theories of irony, we propose for the first time: This paper provides a linguistic and pragmatic analysis of the phenomenon of irony in order to represent how Twitter’s users exploit irony devices within their communication strategies for generating textual contents. We aim to measure the impact of a wide-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts local to the tweet. Informed by"
E17-1025,L16-1283,0,0.152755,"Missing"
E17-1025,P15-1100,0,0.00952849,"ngram models, stylistic, to dictionary-based features (Burfoot and Baldwin, 2009; Davidov et al., 2010; Tsur et al., 2010; Gonzalez-Ibanez et al., 2011; Liebrecht et al., 2013; Joshi et al., 2015; Hern´andez Far´ıas et al., 2015). In addition to the above more lexical features, many authors point out the contribution of pragmatic features, such as the use of common vs. rare words or synonyms (Barbieri and Saggion, 2014). Recent work explores other kinds of contextual information like author profiles, conversational threads, or querying external sources of information (Bamman and Smith, 2015; Wallace et al., 2015; Karoui et al., 5 For Italian, only values for markers automatically identified reliably, without need of manual correction, are reported (e.g. emoticons, negations). Values for other markers are currently missing since they require a manual check, for instance the case of capital letters, because of the presence in the Italian corpus where all the letters are capital. 6 For both settings, frequencies &lt; 5 were removed. 268 Emoticon F E I Negation F E I Discourse F E I Humour #* F E I Intensifier F E I Ex Im NI 7 2 1 6 4 7 5 10 0 Opposition F E I 37 34 58 58 15 61 9 75 9 Capital F E I 6 41 29"
E17-1025,L16-1462,1,0.889268,"Missing"
E17-1025,C14-1120,0,0.0591015,"irony hashtags have been reported (Karoui et al., 2015). Nevertheless, irony corpus construction through hashtag filtering is not always possible for all languages. For instance, both in Czech and Italian, Twitter users generally do not use the sarcasm (i.e. ‘#sarkasmus’, in Czech; ‘#sarcasmo’ in Italian) or irony (‘#ironie’ in Czech or ‘#ironia’ in Italian) hashtag variants to mark their intention to be ironic, thus in such cases relying on simple self-tagging for collecting ironic samples is not an option (Pt´acˇ ek et al., 2014; Bosco et al., 2013). Similar considerations hold for Chinese (Tang and Chen, 2014). For what concerns Italian, we observe that even if occasionally Italian tweeters do use creative hashtags to explicitly mark the presence of irony, no generic shared hashtags have been used for longtime which can be considered as firmly established indicators of irony like those used for English. • A qualitative and quantitative study, focusing in particular on the interactions between irony activation types and markers, irony categories and markers, and the impact of external knowledge on irony detection. Our results demonstrate that implicit activation of irony is a major challenge for fut"
E17-1025,C96-2162,0,0.349092,"-range of pragmatic phenomena in the interpretation of irony, and to investigate how these phenomena interact with contexts local to the tweet. Informed by linguistic theories, we propose for the first time a multi-layered annotation schema for irony and its application to a corpus of French, English and Italian tweets.We detail each layer, explore their interactions, and discuss our results according to a qualitative and quantitative perspective. 1 Introduction Irony is a complex linguistic phenomenon widely studied in philosophy and linguistics (Grice et al., 1975; Sperber and Wilson, 1981; Utsumi, 1996). Glossing over differences across approaches, irony can be defined as an incongruity between the literal meaning of an utterance and its intended meaning. For many researchers, irony overlaps with a variety of other figurative devices such as satire, parody, and sarcasm (Clark and Gerrig, 1984; Gibbs, 2000). In this paper, we use irony as an umbrella term that includes sarcasm, although some researchers make a distinction between them, considering that sarcasm tends to be more aggressive (Lee and Katz, 1998; Clift, 1999). Different categories of irony have been studied in the linguistic liter"
E17-1025,C14-1022,0,\N,Missing
F12-2026,W05-0613,0,0.0548971,"Missing"
F12-2026,W11-2023,1,0.840081,"Missing"
F12-2026,S12-1018,1,0.821137,"Missing"
I11-1132,afantenos-etal-2010-learning,0,0.0762784,"clauses, but also to other syntactic units describing eventualities, adjuncts (like appositions or frame adverbials), nonrestrictive relatives and appositions (for embedded EDUs). In case of S EDUs, we observe that several opinion expressions (often conjoined NPs or APs clauses) can be related by discourse relations.We resegment such EDUs into separate clauses – for instance [the film is beautiful and powerful] is taken to express two segments: [the film is beautiful][and powerful]. For segment annotation, we rely on an already existing annotation guide elaborated during the ANNODIS project (Afantenos et al., 2010) that shows that segmentation is a relatively easy task even for naives. In order to avoid errors in determining the basic units, segmentation relies on annotation consensus. For segment classification, we elaborated a specific annotation guide where we ask the judges to annotate each EDU into S, OO, O and SN according to the definitions given in Section 2.2. First, the judges were trained to the task and discussed while annotating the same documents (10 reviews that were subsequently discarded from the gold). Then, they separately doubly-annotated each review. This yielded an average Cohens k"
I11-1132,D09-1020,0,0.03294,"odependent, since subjectivity analysis filters out statements that contain no opinion. A common approach in these tasks is to rely on the prior polarity of words and expressions as encoded in external lexical resources. However, as (Polanyi and Zaenen, 2006) stated, identifying prior polarity alone may not suffice to improve sentiment analysis at a finer grain, we need both local and global context. Context provided locally can help in two ways. First, it can be used in subjectivity word sense disambiguation (SWSD) in order to determine if a given word has a subjective or an objective sense (Akkaya et al., 2009). It can also be used to identify valence shifters (viz. negations, modalities and intensifiers) that strengthen, weaken or reverse the prior polarity of a word or an expression (Kennedy and Inkpen, 2006; Wilson et al., 2009). Global context on the other hand can be used to identify implicit opinions and to improve the recognition of the overall stance. Few research efforts have been undertaken on using discourse as features for sentence / clausebased opinion analysis. Among them, (Pang and Lee, 2004) assume that subjective and objective sentences are more likely to appear together, (Asher et"
I11-1132,C08-2002,1,0.927299,"l., 2009). It can also be used to identify valence shifters (viz. negations, modalities and intensifiers) that strengthen, weaken or reverse the prior polarity of a word or an expression (Kennedy and Inkpen, 2006; Wilson et al., 2009). Global context on the other hand can be used to identify implicit opinions and to improve the recognition of the overall stance. Few research efforts have been undertaken on using discourse as features for sentence / clausebased opinion analysis. Among them, (Pang and Lee, 2004) assume that subjective and objective sentences are more likely to appear together, (Asher et al., 2008) have developed an annotation schema for a fine-grained contextual opinion analysis using discourse relations, (Taboada et al., 2008) have used a Rhetorical Structure Theory discourse parser in order to calculate semantic orientation by weighting the nuclei more heavily, and finally, (Somasundaran, 2010) has proposed a discourse-level treatment to improve sentence-based polarity classification and to recognize the overall stance. More recently, (Zhou et al., 2011) proposed an unsupervised method to recognize RST-based discourse relations for eliminating intra-sentence polarity ambiguities. How"
I11-1132,J96-1002,0,0.0435266,"Missing"
I11-1132,N09-1057,0,0.0116966,"ressions (OO). Secondly, our classes can be used to enhance polarity classification, since they allow for the removal of the O and SN segments, which do not convey any positive, negative or neutral opinion. Finally, our classes can also be used to enhance the overall opinion strength assessment. SN segments, especially in news articles, can play an important role since they convey the degree of veracity of the information and the degree of the commitment of the author and of the writer. Recently, some efforts have been done on the automatic identification of implicit sentiments. For example, (Greene and Resnik, 2009) used lexical semantics and syntax. (Mus¸at and Tr˘aus¸anMatu, 2010) investigated the influence of valence shifters on the identification of implicit sentiment in economic texts. However, to our knowledge, as yet no work proposed to automatically distinguish between evaluative and non-evaluative segments on the one hand, and between implicit and explicit opinions on the other hand, by using contextual features. 2.3 Rhetorical relations and SA Using SDRT as a formal framework, we have the following discourse relations: Contrast(a,b) in (1) marked by although, Continuation(a,b) in 1 This definit"
I11-1132,2009.jeptalnrecital-court.22,0,0.0279743,"Missing"
I11-1132,P04-1035,0,0.0276099,"hieu LLF-CNRS Paris Vladimir Popescu IRIT-CNRS Toulouse benamara@irit.fr baptiste.chardon yannick.mathieu@ popescu@irit.fr @synapse-fr.com linguist.jussieu.fr Abstract We propose a new subjectivity classification at the segment level that is more appropriate for discourse-based sentiment analysis. Our approach automatically distinguish between subjective nonevaluative and objective segments and between implicit and explicit opinions, by using local and global context features. 1 Introduction Subjectivity and polarity classification is one of the most studied research area in opinion analysis (Pang and Lee, 2004; Wiebe and Riloff, 2005; Wilson et al., 2009). The first task generally distinguish between objective and subjective statements. Polarity classification is then performed in order to extract positive, negative and possibly neutral statements. These two tasks are codependent, since subjectivity analysis filters out statements that contain no opinion. A common approach in these tasks is to rely on the prior polarity of words and expressions as encoded in external lexical resources. However, as (Polanyi and Zaenen, 2006) stated, identifying prior polarity alone may not suffice to improve sentime"
I11-1132,2007.sigdial-1.5,0,0.00972057,"essing, pages 1180–1188, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP clauses that can be connected by rhetorical relations. Moving to the clause level is also not appropriate, since several opinion expressions can be discursively related as in The movie is great but too long where we have a Contrast relation or as in Mr. Dupont, a rich business man, has been savagely killed where we have an Elaboration because the appositive gives further information about the eventuality introduced in the main clause. Therefore, we need to move to a finergrained analysis, at the segment level. (Somasundaran et al., 2007) have used a similar level to detect the presence of sentiment and arguing in dialogues. However, segment annotations were provided by their corpus, whereas in our case, segments are defined according to the Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003) and are automatically detected. 2.2 Beyond Binary Classification SA can not be simply reduced to binary subjective vs. objective classification. The following examples extracted from our corpus of French movie reviews illustrate this (they are translated in English and discourse segments are between []): (1) [The"
I11-1132,P10-1059,0,0.0366757,"y-annotated each review. This yielded an average Cohens kappa of 0.7 for S, 0.72 for O, 0.61 for SN and 0.54 for OO. The latter two are moderate agreements and figure, we believe, an artifact of the length of the texts. Indeed, the longer a text is, the higher difficulty for human subjects is in detecting discourse context in longer texts. However, the study of this hypothesis falls out of the scope of this paper and is therefore left for future work. Nonetheless, these figures are well in the range of state-of-the-art research reports in distinguishing between explicit and implicit opinions (Toprak et al., 2010). For our experiments, the conflicting cases were resolved through discussion between annotators. 4 Subjective Lexicon Our lexicon is composed of 270 verbs, 632 adjectives, 296 nouns, 594 adverbs, 51 interjections, 178 opinion expressions, with 95 modalities among all these. Since there is no existing free subjective lexicon for French, we have manually built our own lexicon from the study of a wide variety of corpora.Following the opinion categorization described in (Asher et al., 2008), each entry (except for adverbs) is associated to four high-level semantic categories (namely reporting, ju"
I11-1132,J09-3003,0,0.0439425,"RS Toulouse benamara@irit.fr baptiste.chardon yannick.mathieu@ popescu@irit.fr @synapse-fr.com linguist.jussieu.fr Abstract We propose a new subjectivity classification at the segment level that is more appropriate for discourse-based sentiment analysis. Our approach automatically distinguish between subjective nonevaluative and objective segments and between implicit and explicit opinions, by using local and global context features. 1 Introduction Subjectivity and polarity classification is one of the most studied research area in opinion analysis (Pang and Lee, 2004; Wiebe and Riloff, 2005; Wilson et al., 2009). The first task generally distinguish between objective and subjective statements. Polarity classification is then performed in order to extract positive, negative and possibly neutral statements. These two tasks are codependent, since subjectivity analysis filters out statements that contain no opinion. A common approach in these tasks is to rely on the prior polarity of words and expressions as encoded in external lexical resources. However, as (Polanyi and Zaenen, 2006) stated, identifying prior polarity alone may not suffice to improve sentiment analysis at a finer grain, we need both loc"
I11-1132,D11-1015,0,0.0177867,"ion analysis. Among them, (Pang and Lee, 2004) assume that subjective and objective sentences are more likely to appear together, (Asher et al., 2008) have developed an annotation schema for a fine-grained contextual opinion analysis using discourse relations, (Taboada et al., 2008) have used a Rhetorical Structure Theory discourse parser in order to calculate semantic orientation by weighting the nuclei more heavily, and finally, (Somasundaran, 2010) has proposed a discourse-level treatment to improve sentence-based polarity classification and to recognize the overall stance. More recently, (Zhou et al., 2011) proposed an unsupervised method to recognize RST-based discourse relations for eliminating intra-sentence polarity ambiguities. However, no work has investigated so far how discourse structure can be used to enhance subjectivity analysis (SA). Using discourse for SA raises new issues: Is sentence/clause subjectivity-based analysis appropriate? Is binary subjective vs. objective classification enough for capturing how opinions are expressed within discourse? and finally, how can rhetorical relations help to correctly identify subjective orientation at a finer-grained level? In this paper, we a"
J17-1006,E09-1004,0,0.0216021,"e (Balahur et al. 2011), which associates polarity and affective information with affective situations such as accomplishing a goal, failing an exam, or celebrating a special occasion. Modeling such situations is particularly important for recognizing implicit emotions (Balahur et al. 5 See Liu (2015) Chapter 7 for an overview of existing techniques. 6 http://liwc.wpengine.com/. 220 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words 2012). Besides the obvious usefulness of affect lexicons in emotion detection, their use in sentiment analysis tasks has shown to be helpful (Agarwal et al. 2009), especially in figurative language detection (Reyes and Rosso 2012) (see Section 6.3). The choice of the right resources to use generally depends on the task and the corpus genre. Musto et al. (2014) compare the effectiveness of SentiWordNet, WordNetAffect, MPQA, and SenticNet for sentiment classification of Twitter posts. Their results show that MPQA and SentiWordNet performed the best. This is interesting because although MPQA is a lexicon with small coverage, its results are comparable to a generalpurpose lexicon like SentiWordNet. To overcome coverage limitation, one can consider combinin"
J17-1006,W11-0705,0,0.0291036,"., Wikipedia) to identify explicit aspects (Hu and Liu 2004a; Popescu and Etzioni 2005; Zhao and Li 2009; Wu et al. 2009).1 3.2.2 Sentiment Determination. Task (3), sentiment determination, is probably the most studied. It consists of identifying polarity or orientation. In its simplest form, it corresponds to the binary orientation (positive or negative) of a subjective span regardless of external context within or outside a sentence or document. Some researchers argue instead for a ternary classification, with a neutral category to indicate the absence of evaluation (Koppel and Schler 2006; Agarwal et al. 2011). The intensity of a subjective span is often combined with its prior binary polarity to form an evaluation score that tells us about the degree of the evaluation, that is, how positive or negative the word is. Several types of scales have been used in sentiment analysis research, going from continuous scales (Benamara et al. 2007) to discrete ones (Taboada et al. 2011). For example, if we have a three-point scale to encode strength, we can propose score(good) = +1 and score(brilliant) = +3. Generally, there is no consensus on how many points are needed on the scale, but the chosen length of t"
J17-1006,agerri-garcia-serrano-2010-q,0,0.0602408,"Missing"
J17-1006,D09-1020,0,0.0215632,"summary of each review. At the sentence level, the task is to determine the subjective orientation and then the opinion orientation of sequences of words in the sentence that are determined to be subjective or express an opinion (Riloff and Wiebe 2003; Yu and Vasileios 2003; Wiebe and Riloff 2005; Taboada et al. 2011), with the assumption that each sentence usually contains a single opinion. To better compute the contextual polarity of opinion expressions, some researchers have used subjectivity word sense disambiguation to identify whether a given word has a subjective or an objective sense (Akkaya et al. 2009). Other approaches identify valence shifters (negation, modality, and intensifiers) that strengthen, weaken, or reverse the prior polarity of a word or an expression (Polanyi and Zaenen 2006; Moilanen and Pulman 2007; Shaikh et al. 2007; Choi and Cardie 2008). The contextual polarity of individual expressions is then used for sentence as well as document classification (Kennedy and Inkpen 2006; Li et al. 2010). 1 For a survey on topic detection for aspect-based sentiment analysis, see Liu (2015), Chapter 6. 212 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words At the docu"
J17-1006,baccianella-etal-2010-sentiwordnet,0,0.0500004,"ance the binary categorization of evaluative expressions. In the remainder of this section, we focus on these studies, as we believe they can be beneficial to subjective lexicon creation. Studies can be roughly divided into how they approach categorization of lexical expressions: by tackling intensity, emotion, and either syntactic or semantic principles for classification (including Appraisal categories). 5.1.1 Intensity-Based Categorizations. One way to improve upon simple polarity determination is to associate with each subjective entry an evaluation score or intensity level. SentiWordnet (Baccianella et al. 2010) is an extension of WordNet (Fellbaum 1998) that assigns to each synset three sentiment scores: positive (P), negative (N), and objective (O). Hence different senses of the same term may have different sentiment scores. For example, the adjective happy has four senses: The first two expressing joy or pleasure are highly positive (P = 0.875 and P = 0.75, respectively); the third corresponding to eagerly disposed to act or to be of service (as in happy to help) is ambiguous (P = 0.5 and O = 0.5); and the last sense (a happy turn of phrase) is likely to be objective (P = 0.125 and O = 0.875). Alt"
J17-1006,P98-1013,0,0.0550591,"an: The experience or the causation of a rather unpleasant feeling, pleasant feeling, and neither pleasant nor unpleasant. Semantic classes are linked by meaning, intensity, and antonymy relationships. She associates a set of linguistic properties with words and classes and builds semantic representations, described by means of feature structures. Mathieu and Fellbaum (2010) extended this classification to English verbs of emotion. SentiFrameNet (Ruppenhofer and Rehbein 2012) is probably the best example of the syntactico-semantic categorization of evaluative expressions. It extends FrameNet (Baker et al. 1998) to connect opinion source and target to semantic roles, and to add semantic features such as polarity, intensity, and affectedness (changes of state that leave an event participant in a changed state). Evaluative language per se is not described in FrameNet, but several frames are relevant for sentiment analysis like J UDGMENT, O PINION, E MOTION D IRECTED, and semantic roles such as J UDGE or E XPERIENCER. Each frame is associated with a set of lexical units composed of words that evoke that frame. Example (8) illustrates the frame elements of the lexical unit splendid (from the frame D ESIR"
J17-1006,barbieri-saggion-2014-modelling-irony,0,0.0601431,"Missing"
J17-1006,P10-2061,0,0.023718,"02). Other genres studied include law texts (Palau and Moens 2009), biomedical articles (Agarwal and Yu 2009), and movie reviews (Bieler et al. 2007). Top–down approaches in sentiment analysis assume that in a subjective document only some parts are relevant to the overall sentiment. Irrelevant parts thus have to be filtered out or de-emphasized, and the remaining parts are used to infer an overall evaluation at the document level. For example, a recent psycholinguistic and psychological study shows that polarity classification should concentrate on messages in the final position of the text (Becker and Aharonson 2010). Pang et al. (2002) were the first to empirically investigate positional features. Specifically, depending on the position at which a token appears (first quarter, last quarter, or middle half of the document), the same unigram is treated as different features. However, the outcomes did not result in a significant improvement. An error analysis showed that this low improvement is due to the positional features that fail to adequately handle the “thwarted expectation” phenomenon, very common in such text genre (cf. Section 4). Pang et al. (2002) argued that a more sophisticated form of discour"
J17-1006,S15-1016,1,0.802396,"vantages. First of all, for classification tasks such as discourse parsing, access to larger amounts of data is likely to yield better results. Secondly, and from a more theoretical point of view, we think that differences across approaches are minimal, and a unified set of relations is possible. Third, a unified set of discourse relations would allow us to compile a list of discourse markers and other signals for those relations, which would also benefit discourse annotation. Recent efforts to merge existing discourse relation taxonomies and annotations should help improve discourse parsing (Benamara and Taboada 2015; Rehbein et al. 2015). Notable also is work being carried out within the COST Action TextLink, a pan-European initiative to unify definitions of relations and their signals across languages (http://www.textlink.ii.metu.edu.tr/). Once powerful discourse parsers are developed, the argumentative structure of evaluative text can be fully exploited. Processing arguments for sentiment analysis is still at an early stage and we feel that recent progress in argument mining will likely spur new research in this direction (Bex et al. 2013; Stab and Gurevych 2014; Peldszus and Stede 2015). We see sentim"
J17-1006,I11-1132,1,0.879153,"Missing"
J17-1006,W12-3802,1,0.931947,"ssity, permission, obligation, or desire, and it is grammatically expressed via adverbial phrases (perhaps, maybe, certainly), conditional verb mood, some modals (must, can, may), and intensional verbs (think, believe). Adjectives and nouns 224 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words can also express modality (a probable cause; It remains a possibility). A general consensus in sentiment analysis is that nonveridicality and irrealis result in the unreliability of any expression of sentiment in the sentences containing it (Wilson et al. 2009; Taboada et al. 2011; Benamara et al. 2012; Morante and Sporleder 2012b; Denis et al. 2014). Consider the effect of the intensional verb thought and the modal would in Example (9) and the modal plus question in Example (10), which completely discount any positive evaluation that may be present in good and more suitable. In some cases, however, evaluative expressions under the scope of modality do not have to be ignored (Benamara et al. 2012). This is true for the cumulative modality in Example (11) and the use of negation in Example (12), where the deontic modal should strengthens the negative recommendation. (9) I thought this movie"
J17-1006,D15-1263,0,0.0210575,"Missing"
J17-1006,2007.sigdial-1.16,0,0.0180853,"ting discourse into different parts that serve different functions. This segmentation is achieved through a conventionalized set of building blocks that contribute to the overall text function. These building blocks are called content, functional, or discourse zones. Discourse zones are specific to particular genres, such as the communicative roles played by the introduction, background, and conclusion sections in a scientific paper (Swales 1990; Teufel and Moens 2002). Other genres studied include law texts (Palau and Moens 2009), biomedical articles (Agarwal and Yu 2009), and movie reviews (Bieler et al. 2007). Top–down approaches in sentiment analysis assume that in a subjective document only some parts are relevant to the overall sentiment. Irrelevant parts thus have to be filtered out or de-emphasized, and the remaining parts are used to infer an overall evaluation at the document level. For example, a recent psycholinguistic and psychological study shows that polarity classification should concentrate on messages in the final position of the text (Becker and Aharonson 2010). Pang et al. (2002) were the first to empirically investigate positional features. Specifically, depending on the position"
J17-1006,P07-1056,0,0.23253,"7; Shaikh et al. 2007; Choi and Cardie 2008). The contextual polarity of individual expressions is then used for sentence as well as document classification (Kennedy and Inkpen 2006; Li et al. 2010). 1 For a survey on topic detection for aspect-based sentiment analysis, see Liu (2015), Chapter 6. 212 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words At the document level, the standard task is either a classification problem, categorizing documents globally as being positive, negative, or neutral towards a given topic (Pang et al. 2002; Turney 2002; Mullen and Nigel 2004; Blitzer et al. 2007), or regression, assigning a multi-scale rating to the document (Pang and Lee 2005; Goldberg and Zhu 2006; Snyder and Barzilay 2007; Lu et al. 2009; Lizhen et al. 2010; Leung et al. 2011; Moghaddam and Ester 2011; Ganu et al. 2013). The classification/regression approach to determining document sentiment typically uses bag-of-words representations (BOW), which model each text as a vector of the number of occurrences, or the frequency with which each word/construction appears. Bag-of-words features also include n-grams, parts of speech, and features that account for the presence/absence of subj"
J17-1006,N07-1039,0,0.0206754,"gment, for instance, was not part of the classification or the information extracted from the text. The authors do point out potential benefits of Appraisal analysis, including identifying the Appraiser and Appraised (what elsewhere have been termed source and target). Similar work by Read and Carroll uses a corpus annotated for Appraisal (not only Attitude, but also Graduation and Engagement) to train a classifier that detects whether unseen words and phrases are instances of Appraisal and, when they are, their polarity (Read and Carroll 2012b, 2012a). Work by Argamon, Bloom, and colleagues (Bloom et al. 2007; Argamon et al. 2009; Bloom and Argamon 2010) also focused on using Appraisal to build lexicons. Of note is the effort in Bloom et al. (2007) to extract adjectives according to Appraisal categories (the three top-level Attitude types) and present them as output (rather than just use them as input to build a lexicon). Furthermore, their system assigns a target to each adjective, specifying whether, for example, it refers to an actor, a character, or the plot or special effects of a movie. This linking of Appraisal expression and target is achieved via dependency parsing. Their method seems to"
J17-1006,P11-1014,0,0.0114236,"ntiment composition process that captures how opinion expressions interact with each other and with specific linguistic operators such as intensifiers, negation, or modality (Polanyi and Zaenen 2006; Moilanen and Pulman 2007; Wilson et al. 2009). For instance, in This restaurant is not good enough, the prior positive orientation of the word good has to be combined with the negation not and the modifier enough. Apart from local linguistic operators, prior polarity may also vary according to the context outside of the utterance, including domain factors (Aue and Gamon 2005; Blitzer et al. 2007; Bollegala et al. 2011). A given span may be subjective in one context and objective in another. Haas and Versley (2015) observe that seemingly neutral adjectives can become polar when combined with aspects of a movie (elaborate continuation, expanded vision), as can words that are intensified (simply intrusive was considered negative, but intrusive was neutral). Even if there is any ambiguity on the subjectivity status of a word, orientation can be highly context-dependent: A horrible movie may be positive if it is a thriller, but negative in a romantic comedy. Additionally, out of context, some subjective expressi"
J17-1006,D10-1005,0,0.00921772,"inary positive/negative axis. Domain adaptation has been extensively studied in the literature (cf. Section 3.2 for a discussion). Language adaptation often consists of transferring knowledge from a resource-rich language such as English to a language with fewer resources, using parallel corpora or standard machine translation techniques (Mihalcea et al. 2007; Abbasi et al. 2008; Balahur et al. 2012; Balahur and Perea-Ortega 2015; Gao et al. 2015). Other approaches make few assumptions about available resources by using a holistic statistical model that discovers connections across languages (Boyd-Graber and Resnik 2010). Under the assumption that similar terms have similar emotional or subjective orientation (Hatzivassiloglou and McKeown 1997), lexicon expansion techniques grow an initial set of subjective seed words by diverse semantic similarity metrics. These techniques may also exploit word relationships such as synonyms, antonyms, and hypernyms 219 Computational Linguistics Volume 43, Number 1 within general linguistic resources such as WordNet, or syntactic properties such as dependency relations.5 Compared with lexicon expansion and domain and language adaptation, few studies propose to enhance the bi"
J17-1006,N10-1122,0,0.0379154,"rs to an actor, a character, or the plot or special effects of a movie. This linking of Appraisal expression and target is achieved via dependency parsing. Their method seems to perform well, as shown by a manual evaluation of the extracted expressions, and by its potential usefulness in creating rules for opinion patterns. In a follow-up paper, Bloom and Argamon (2010) present an automatic method for linking Appraisal expressions and patterns. Such linking of Appraisal expressions and targets precedes later work in featurebased sentiment extraction or opinion mining (Titov and McDonald 2008; Brody and Elhadad 2010; Liu 2012), which seems to have developed independently, and without taking into account the possibility of bootstrapping the identification of features with Appraisal expressions. Information on whether an Appraisal expression is likely to be Judgement rather than Appreciation will help determine whether its target is human or not, and vice versa. A notable exception in the practical application of Appraisal is the Attitude Analysis Model of Neviarouskaya and colleagues (Neviarouskaya 2010; Neviarouskaya et al. 2010b, 2010a; Neviarouskaya and Aono 2013). They propose a method of assigning wh"
J17-1006,R09-1010,1,0.70421,"pproaches may suffer from negative transfer where instead of improving performance, the transfer from other domains degrades the performance on the target domain. One possible solution is active learning, which relies on a small amount of good labeled data in the target domain to quickly reduce the difference between the two domains (Rai et al. 2010; Li et al. 2013b). Compared with machine learning, lexicon-based methods make use of the linguistic information contained in the text that makes them more robust across different domains (Kennedy and Inkpen 2006; Taboada et al. 2011). Furthermore, Brooke et al. (2009) showed that porting dictionaries to a new language or a new domain is not an onerous task, probably less onerous than labeling data in a new domain for a classifier. Combining lexicon-based learning and corpus-based learning could be a good solution to incorporate both domain-specific and domain-independent knowledge. This has been investigated in several studies relying either on a complete lexicon or fully labeled corpus (Andreevskaia and Bergler 2008; Qiu et al. 2009), or a partially labeled corpus as training examples (Yang et al. 2015). 4. Towards a Dynamic Model of Evaluative Language T"
J17-1006,P09-2041,0,0.0173401,"Missing"
J17-1006,D13-1035,1,0.853977,"Missing"
J17-1006,W15-2903,0,0.0262439,"ve lexicons contain words that are intrinsically positive or negative. To overcome this limitation, Feng et al. (2013) propose a set of induction algorithms to automatically build the first broad-coverage connotation lexicon.8 This lexicon performs better than denotation lexicons in binary sentiment classification on SemEval and Tweet corpora. Later, Kang et al. (2014) extended this lexicon to deal with polysemous words and introduced ConnotationWordNet, a connotation lexicon over words in conjunction with senses. Connotation is also being explored in other NLP tasks like machine translation (Carpuat 2015). The third way in which implicit evaluation can arise is when one expresses an evaluation towards an implicit aspect of an entity. This has been more frequently observed in aspect-based sentiment analysis (Liu 2012). For example, the adjective heavy in The cell phone is heavy implicitly provides a negative opinion about the aspect weight. Similarly, the verb last in My new phone lasted three days suggests that the aspect durability is assigned a negative opinion. Some of these implicit evaluations arise out of connotations, some of them because of polysemy (the problem to be solved in word se"
J17-1006,W13-0105,1,0.937113,"e that the score of the adjective excellent is +3, then the opinion score in This student is not excellent cannot be −3. The sentence probably means that the student is not good enough. It is thus difficult to negate a strongly positive word without implying that a less positive one is to some extent possible (not excellent, but not horrible either). A possible solution is to use shift negation, in which the effect of a negator is to shift the negated term in the scale by a certain amount, but without making it the polar opposite of the original term (Liu and Seneff 2009; Taboada et al. 2011; Chardon et al. 2013a). 5.2.2 Sentiment Composition. Sentiment composition aims at computing the sentiment orientation of an expression or a sentence (in terms of polarity and/or strength) on the basis of the sentiment orientation of its constituents. This process, based on the principle of compositionality (Dowty et al. 1981), captures how opinion expressions interact with each other and with specific linguistic operators such as intensifiers, negations, or modalities. For instance, the sentiment expressed in the sentence This restaurant is good but expensive is a combination of the prior sentiment orientation o"
J17-1006,N13-1124,0,0.0145436,"ms. For example, evaluation involving comparatives expresses an ordering towards targets based on some of their shared aspects, for example, the picture quality of camera X is better than that of Y (Jindal and Liu 2006a, 2006b). There are also evaluations that concern relative judgment towards actions or intentions, preferring them or not over others (e.g., I prefer the first season over the second one, or Shall we see Game of Thrones next week?). In this last case, reasoning about preferences determines an order over outcomes that predicts how a rational agent will act (Cadilhac et al. 2012; Chen et al. 2013). Other forms of evaluative language involve finding a consensus or conflict over participants by identifying agreement/disagreement on a given topic in a debate or discussion (Somasundaran and Wiebe 2009; Mukherjee and Bhattacharyya 2012). We address preferences and intentions in Section 6.5. 4.2 A New Dynamic Model of Evaluative Language We hope to have established by now that evaluative language is context-dependent at different discourse organization levels: r The sentence: Interactions with linguistic operators like negation, modality, and intensifiers; or syntactic constraints such as al"
J17-1006,D08-1083,0,0.286962,"nd Vasileios 2003; Wiebe and Riloff 2005; Taboada et al. 2011), with the assumption that each sentence usually contains a single opinion. To better compute the contextual polarity of opinion expressions, some researchers have used subjectivity word sense disambiguation to identify whether a given word has a subjective or an objective sense (Akkaya et al. 2009). Other approaches identify valence shifters (negation, modality, and intensifiers) that strengthen, weaken, or reverse the prior polarity of a word or an expression (Polanyi and Zaenen 2006; Moilanen and Pulman 2007; Shaikh et al. 2007; Choi and Cardie 2008). The contextual polarity of individual expressions is then used for sentence as well as document classification (Kennedy and Inkpen 2006; Li et al. 2010). 1 For a survey on topic detection for aspect-based sentiment analysis, see Liu (2015), Chapter 6. 212 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words At the document level, the standard task is either a classification problem, categorizing documents globally as being positive, negative, or neutral towards a given topic (Pang et al. 2002; Turney 2002; Mullen and Nigel 2004; Blitzer et al. 2007), or regression, assigni"
J17-1006,W06-1651,0,0.0142484,"movie; My mother said that the movie is great) (Wiebe and Riloff 2005). The holder evaluates a topic or target that is the entity or a part or attribute of the entity that the sentiment is predicated upon (Liu 2015). A topic is thus a global entity e (e.g., a product, service, person, event, or issue) organized hierarchically into a set of attributes or aspects a (e.g., engine, tires are part of the entity car), as can be done in a thesaurus or domain ontology. For holder recognition, it has been shown that semantic role labeling a` la PropBank or FrameNet is beneficial (Bethard et al. 2004; Choi et al. 2006; Kim and Hovy 2006; Gangemi et al. 2014), although Ruppenhofer et al. (2008) argue that evaluation that is connected to its source indirectly via attribution poses challenges that go beyond the capabilities of automatic semantic role labeling, and that discourse structure has to be considered. Topic and aspect recognition, on the other hand, are seen as information 211 Computational Linguistics Volume 43, Number 1 extraction tasks that generally exploit noun or noun phrases, dependency relations, some syntactic patterns at the sentence level, knowledge representation paradigms (like hierarchi"
J17-1006,W10-3110,0,0.0608916,"tics Volume 43, Number 1 In the remainder of this survey, we focus on automatic detection of polarized evaluation in text, excluding other forms of evaluative language. For the sake of readability, the terms evaluation, evaluative language, and polarized evaluation are used interchangeably. We leave outside of the scope of this survey research on emotion detection and classification, which is surveyed by Khurshid (2013) and Mohammad (2016). Related, but also somewhat beyond our scope, is work on detecting negation and speculation, in particular in biomedical text (Saur´ı and Pustejovsky 2009; Councill et al. 2010; Morante and Sporleder 2012a; Cruz et al. 2016). A frequently used definition of evaluative language as a structured model has been proposed by Liu (2012), drawing from Kim and Hovy (2004) and Hu and Liu (2004a). This model is a quintuple (e, a, s, h, t) where e is the entity that is the topic or target of the opinion (restaurant in Example (1)), a the specific aspect or feature of that entity (food), s the sentiment or evaluation towards a (incredibly delicious), h the opinion holder or source (the author in our example), and t the posting time of s. Liu (2012) further represents s by a trip"
J17-1006,J12-2003,0,0.0307116,"Missing"
J17-1006,Q13-1023,0,0.0417474,"Missing"
J17-1006,C14-1009,0,0.0610327,"lations from the French CASOAR corpus (Benamara et al. 2016); Example (20) comes from Zhang and Liu (2011). (18) The movie is not bad, although some persons left the auditorium. (19) This movie is poignant, and the actors excellent. It will remain in your DVD closet. (20) Within a month, a valley formed in the middle of the mattress. Situations that affect the evaluation of entities can be automatically identified relying either on co-occurrence assumptions, a set of rules, or patterns enlarged via bootstrapping (Goyal et al. 2010; Benamara et al. 2011; Zhang and Liu 2011; Riloff et al. 2013; Deng et al. 2014b; Wiebe and Deng 2014). For example, Riloff et al. (2013) learn from tweets patterns of the form [VP+ ].[Situation− ] that correspond to a contrast between positive sentiment in a verb phrase and a negative situation. Tweets following this pattern are more likely to be sarcastic (see Section 6.3 on sarcasm and figurative language). Zhang and Liu (2011) exploit context to identify nouns and noun phrases that imply sentiment. They hypothesize that such phrases often have a single polarity (either positive or negative, but not both) and tend to co-occur in an explicit negative (or positive) cont"
J17-1006,S14-2106,0,0.0213393,"grammatically expressed via adverbial phrases (perhaps, maybe, certainly), conditional verb mood, some modals (must, can, may), and intensional verbs (think, believe). Adjectives and nouns 224 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words can also express modality (a probable cause; It remains a possibility). A general consensus in sentiment analysis is that nonveridicality and irrealis result in the unreliability of any expression of sentiment in the sentences containing it (Wilson et al. 2009; Taboada et al. 2011; Benamara et al. 2012; Morante and Sporleder 2012b; Denis et al. 2014). Consider the effect of the intensional verb thought and the modal would in Example (9) and the modal plus question in Example (10), which completely discount any positive evaluation that may be present in good and more suitable. In some cases, however, evaluative expressions under the scope of modality do not have to be ignored (Benamara et al. 2012). This is true for the cumulative modality in Example (11) and the use of negation in Example (12), where the deontic modal should strengthens the negative recommendation. (9) I thought this movie would be as good as the Grinch. (10) Couldn’t you"
J17-1006,P12-1105,0,0.0542853,"Missing"
J17-1006,C08-1031,0,0.0173619,"Missing"
J17-1006,J15-1002,0,0.0229645,"resources. Most of them share four main characteristics: They are domain-and language-specific, of limited coverage, and they group evaluative expressions along a binary positive/negative axis. Domain adaptation has been extensively studied in the literature (cf. Section 3.2 for a discussion). Language adaptation often consists of transferring knowledge from a resource-rich language such as English to a language with fewer resources, using parallel corpora or standard machine translation techniques (Mihalcea et al. 2007; Abbasi et al. 2008; Balahur et al. 2012; Balahur and Perea-Ortega 2015; Gao et al. 2015). Other approaches make few assumptions about available resources by using a holistic statistical model that discovers connections across languages (Boyd-Graber and Resnik 2010). Under the assumption that similar terms have similar emotional or subjective orientation (Hatzivassiloglou and McKeown 1997), lexicon expansion techniques grow an initial set of subjective seed words by diverse semantic similarity metrics. These techniques may also exploit word relationships such as synonyms, antonyms, and hypernyms 219 Computational Linguistics Volume 43, Number 1 within general linguistic resources"
J17-1006,D14-1168,0,0.0439931,"Missing"
J17-1006,S15-2080,0,0.0524834,"Missing"
J17-1006,P11-2102,0,0.0400725,"Missing"
J17-1006,P14-1022,0,0.0235791,"es being language-and context-dependent, another alternative approach is to represent each node in a parse tree with a vector, and then learn how to compose leaf vectors in a bottom–up fashion. The composition process is modeled as a function learned from the training data, which can be standard data sets that only have document-level sentiment annotation (e.g., star ratings [Yessenalina and Cardie 2011; Socher et al. 2011, 2012] or sentiment treebanks with fine-grained annotations for every single node of the top parse tree (Johansson and Moschitti 2013; Socher et al. 2013; Dong et al. 2014; Hall et al. 2014; Zhu et al. 2015). The Stanford Sentiment Treebank is probably the best known example (Socher et al. 2013). It is composed of 215,154 phrases annotated by three human judges according to six sentiment values, ranging from very negative to very positive, with neutral in the middle. Those phrase annotations are then combined with a dependency parse to propagate sentiment up through the nodes of the tree. Various composition functions have been proposed in the literature. For example, Yessenalina and Cardie (2011) represent each word as a matrix and combine words using iterated matrix multiplica"
J17-1006,P94-1002,0,0.0606175,"s on building either a topic structure or a functional structure (see Purver 2011 and Stede 2011 for comprehensive surveys). Organizing discourse by topicality consists of splitting discourse into a linear sequence of segments, each of which focuses on a distinct subtopic occurring in the context of one or more main topics. Topic segmentation is generally guided by local discourse continuity or the lexical cohesion assumption (Halliday and Hasan 1976) that stipulates that topic and lexical usage (such as word repetition, discourse connectives, and paradigmatic relations) are strongly related (Hearst 1994). Functional structure, on the other hand, analyzes discourse from the point of view of the communicative roles or intentions of discourse units in genre-specific texts or from the speaker’s (or writer’s) communicative intention perspective (Grosz and Sidner 1986; Moore and Paris 1993; Lochbaum 1998). Genre-induced text structure aims at segmenting discourse into different parts that serve different functions. This segmentation is achieved through a conventionalized set of building blocks that contribute to the overall text function. These building blocks are called content, functional, or dis"
J17-1006,S15-2117,0,0.020656,"Missing"
J17-1006,C04-1200,0,0.170216,"Missing"
J17-1006,W06-0301,0,0.18233,"said that the movie is great) (Wiebe and Riloff 2005). The holder evaluates a topic or target that is the entity or a part or attribute of the entity that the sentiment is predicated upon (Liu 2015). A topic is thus a global entity e (e.g., a product, service, person, event, or issue) organized hierarchically into a set of attributes or aspects a (e.g., engine, tires are part of the entity car), as can be done in a thesaurus or domain ontology. For holder recognition, it has been shown that semantic role labeling a` la PropBank or FrameNet is beneficial (Bethard et al. 2004; Choi et al. 2006; Kim and Hovy 2006; Gangemi et al. 2014), although Ruppenhofer et al. (2008) argue that evaluation that is connected to its source indirectly via attribution poses challenges that go beyond the capabilities of automatic semantic role labeling, and that discourse structure has to be considered. Topic and aspect recognition, on the other hand, are seen as information 211 Computational Linguistics Volume 43, Number 1 extraction tasks that generally exploit noun or noun phrases, dependency relations, some syntactic patterns at the sentence level, knowledge representation paradigms (like hierarchies or domain ontolo"
J17-1006,W15-0516,0,0.0191594,"ve a role in building arguments. Hunston and Thompson (2000a) suggest that evaluation helps organize the discourse, in addition to its role of strictly conveying an opinion (see Section 2.5). Argumentation is a process by which arguments are constructed by the writer to clarify or defend their opinions. An argument is generally defined as a set of premises that provide the evidence or the reasons for or against a conclusion, also known as a claim (Walton 2009). When using arguments, holders are able to tell not just what views are being expressed, but also why those particular views are held (Lawrence and Reed 2015). Premises can be introduced in texts by specific markers or cue phrases such as for example, but, or because (Knott and Dale 1994). The claim is a proposition stating the general feeling or recommendation of the writer. It can be supported or attacked through various statements making a holder reveal preferences and priorities. For instance, in The movie is good because the characters were great, the second clause is evidence that supports the conclusion in the first clause, whereas in The movie is good but the script was bad, the same conclusion is attacked. Tracking arguments in text consis"
J17-1006,P13-1160,0,0.0124839,"ambiguities at the sentence level. Zirn et al. (2011) grouped RST relations into Contrast vs. Non-Contrast and integrated them as features in a Markov Logic Network to encode information between neighboring segments. Somasundaran et al. (2009) proposed the notion of opinion frames as a representation of documents at the discourse level in order to improve sentence-based polarity classification and to recognize the overall stance. Two sets of relations were used: relations between targets (S AME and A LTERNATIVE) and relations between opinion expressions (R EINFORCING and N ON - REINFORCING). Lazaridou et al. (2013) also use a specific scheme of discourse relations. However, rather than relying on gold discourse annotations, they jointly predict sentiment, aspect, and discourse relations and show that the model improves accuracy of both aspect and sentiment polarity at the sub-sentential level. Leveraging overall discourse structure. Coping with discourse relations at the local level has three main disadvantages: (1) the local approach captures explicitly marked relations—indeed, most approaches do not handle cases where a signal can trigger different relations or does not have a discourse use; (2) it ac"
J17-1006,C10-1072,0,0.0132563,"contextual polarity of opinion expressions, some researchers have used subjectivity word sense disambiguation to identify whether a given word has a subjective or an objective sense (Akkaya et al. 2009). Other approaches identify valence shifters (negation, modality, and intensifiers) that strengthen, weaken, or reverse the prior polarity of a word or an expression (Polanyi and Zaenen 2006; Moilanen and Pulman 2007; Shaikh et al. 2007; Choi and Cardie 2008). The contextual polarity of individual expressions is then used for sentence as well as document classification (Kennedy and Inkpen 2006; Li et al. 2010). 1 For a survey on topic detection for aspect-based sentiment analysis, see Liu (2015), Chapter 6. 212 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words At the document level, the standard task is either a classification problem, categorizing documents globally as being positive, negative, or neutral towards a given topic (Pang et al. 2002; Turney 2002; Mullen and Nigel 2004; Blitzer et al. 2007), or regression, assigning a multi-scale rating to the document (Pang and Lee 2005; Goldberg and Zhu 2006; Snyder and Barzilay 2007; Lu et al. 2009; Lizhen et al. 2010; Leung et"
J17-1006,W13-1605,0,0.0248581,"Missing"
J17-1006,W13-4306,0,0.0109712,"ered by the condition that readers have to be able to change their expectations about the author’s typical style and previous books. Narayanan et al. (2009) also focused on conditionals marked by connectives such as if, unless, and even if, and proposed a supervised learning algorithm to determine if sentiment expressed on different topics in a conditional sentence is positive, negative, or neutral. Instead of extracting specific connectives, some researchers use a compiled list of connectives and incorporate it as features in a bag-of-words model to improve sentiment classification accuracy (Mittal et al. 2013; Trivedi and Eisenstein 2013). Others identify discourse connectives automatically, relying on a discourse tagger trained on the Penn Discourse Treebank (Yang and Cardie 2014). Relations that have been used in sentiment analysis are either relations proposed under various theories of discourse (e.g., RST, SDRT), or a set of relations built specifically to be used in sentiment analysis. Asher et al. (2008) considered five types of SDRTlike rhetorical relations, both explicit and implicit (C ONTRAST, C ORRECTION , R ESULT, C ONTINUATION, and S UPPORT), and conducted a manual study in which they"
J17-1006,J93-4004,0,0.0678763,"rring in the context of one or more main topics. Topic segmentation is generally guided by local discourse continuity or the lexical cohesion assumption (Halliday and Hasan 1976) that stipulates that topic and lexical usage (such as word repetition, discourse connectives, and paradigmatic relations) are strongly related (Hearst 1994). Functional structure, on the other hand, analyzes discourse from the point of view of the communicative roles or intentions of discourse units in genre-specific texts or from the speaker’s (or writer’s) communicative intention perspective (Grosz and Sidner 1986; Moore and Paris 1993; Lochbaum 1998). Genre-induced text structure aims at segmenting discourse into different parts that serve different functions. This segmentation is achieved through a conventionalized set of building blocks that contribute to the overall text function. These building blocks are called content, functional, or discourse zones. Discourse zones are specific to particular genres, such as the communicative roles played by the introduction, background, and conclusion sections in a scientific paper (Swales 1990; Teufel and Moens 2002). Other genres studied include law texts (Palau and Moens 2009), b"
J17-1006,J12-2001,0,0.445973,"1 In the remainder of this survey, we focus on automatic detection of polarized evaluation in text, excluding other forms of evaluative language. For the sake of readability, the terms evaluation, evaluative language, and polarized evaluation are used interchangeably. We leave outside of the scope of this survey research on emotion detection and classification, which is surveyed by Khurshid (2013) and Mohammad (2016). Related, but also somewhat beyond our scope, is work on detecting negation and speculation, in particular in biomedical text (Saur´ı and Pustejovsky 2009; Councill et al. 2010; Morante and Sporleder 2012a; Cruz et al. 2016). A frequently used definition of evaluative language as a structured model has been proposed by Liu (2012), drawing from Kim and Hovy (2004) and Hu and Liu (2004a). This model is a quintuple (e, a, s, h, t) where e is the entity that is the topic or target of the opinion (restaurant in Example (1)), a the specific aspect or feature of that entity (food), s the sentiment or evaluation towards a (incredibly delicious), h the opinion holder or source (the author in our example), and t the posting time of s. Liu (2012) further represents s by a triple (y, o, i) in order to cap"
J17-1006,C12-1113,0,0.139287,"06b). There are also evaluations that concern relative judgment towards actions or intentions, preferring them or not over others (e.g., I prefer the first season over the second one, or Shall we see Game of Thrones next week?). In this last case, reasoning about preferences determines an order over outcomes that predicts how a rational agent will act (Cadilhac et al. 2012; Chen et al. 2013). Other forms of evaluative language involve finding a consensus or conflict over participants by identifying agreement/disagreement on a given topic in a debate or discussion (Somasundaran and Wiebe 2009; Mukherjee and Bhattacharyya 2012). We address preferences and intentions in Section 6.5. 4.2 A New Dynamic Model of Evaluative Language We hope to have established by now that evaluative language is context-dependent at different discourse organization levels: r The sentence: Interactions with linguistic operators like negation, modality, and intensifiers; or syntactic constraints such as altering the order of constituents in a clause or sentence. r The document: Discourse connectives, discourse structure, rhetorical relations, topicality. r Beyond the document:2 Effects of various pragmatic phenomena such as common-sense kno"
J17-1006,W04-3253,0,0.189203,"Missing"
J17-1006,C12-1115,0,0.0218978,"Missing"
J17-1006,N10-1120,0,0.0224072,"mine the scope of valence shifters and then use syntax to do compositional sentiment analysis. Jia et al. (2009) propose a set of complex heuristic rules to determine the scope of negation and then study the impact of different scope models for sentence and document polarity analysis. Their results show that incorporating linguistic insights into negation modeling is meaningful. Another way to model composition is to rely on a sentiment lexicon and predefined set of heuristics that predicts how shifters affect the sentiment of a phrase/sentence (Moilanen and Pulman 2007; Choi and Cardie 2008; Nakagawa et al. 2010; Taboada et al. 2011; Benamara et al. 2012). For example, Moilanen and Pulman (2007) propose three types of rules to deal with negation and intensifiers: sentiment propagation (the polarity of a neutral constituent is overridden by that of the evaluative constituent), polarity conflict resolution (a non-neutral polarity value is changed to another non-neutral polarity value), and polarity reversal. Lexicon-based sentiment composition has been shown to outperform bag-of-words learning classification of sentiment at the sentence level (Choi and Cardie 2008). 226 Benamara, Taboada, and Mathieu E"
J17-1006,D09-1019,0,0.0173688,"Boris is brilliant at math, he is a horrible teacher shows that the positivity of brilliant is neutralized, downtoned at best. A C ONDITION relation will also limit the extent of a positive evaluation, as observed by Trnavac and Taboada (2012) in a corpus study of Appraisal in movie and book reviews. For instance, in It is an interesting book if you can look at it without expecting the Grisham “law and order” style, the positive evaluation in interesting is tempered by the condition that readers have to be able to change their expectations about the author’s typical style and previous books. Narayanan et al. (2009) also focused on conditionals marked by connectives such as if, unless, and even if, and proposed a supervised learning algorithm to determine if sentiment expressed on different topics in a conditional sentence is positive, negative, or neutral. Instead of extracting specific connectives, some researchers use a compiled list of connectives and incorporate it as features in a bag-of-words model to improve sentiment classification accuracy (Mittal et al. 2013; Trivedi and Eisenstein 2013). Others identify discourse connectives automatically, relying on a discourse tagger trained on the Penn Dis"
J17-1006,L16-1008,0,0.0418002,"Missing"
J17-1006,D15-1258,0,0.076975,"Missing"
J17-1006,W10-0210,0,0.0134553,"n featurebased sentiment extraction or opinion mining (Titov and McDonald 2008; Brody and Elhadad 2010; Liu 2012), which seems to have developed independently, and without taking into account the possibility of bootstrapping the identification of features with Appraisal expressions. Information on whether an Appraisal expression is likely to be Judgement rather than Appreciation will help determine whether its target is human or not, and vice versa. A notable exception in the practical application of Appraisal is the Attitude Analysis Model of Neviarouskaya and colleagues (Neviarouskaya 2010; Neviarouskaya et al. 2010b, 2010a; Neviarouskaya and Aono 2013). They propose a method of assigning what are essentially Attitude values to adjectives. In their system, a classifier is trained to determine whether a particular word or phrase expresses the three basic types of Attitude, with the determination being done in context, that is, taking into account the sentence in which the word appears. In summary, we see Appraisal analysis as a richer, more detailed analysis that goes beyond simple polarity labels, and that can help characterize texts across several categories. Once more data and resources become availabl"
J17-1006,C10-1091,0,0.0149432,"Missing"
J17-1006,P06-2079,0,0.0348634,"Missing"
J17-1006,P15-1131,0,0.0187415,"Missing"
J17-1006,R13-1072,0,0.0552441,"Missing"
J17-1006,P04-1035,0,0.225549,"Missing"
J17-1006,P05-1015,0,0.0236846,"Missing"
J17-1006,W02-1011,0,0.0430644,"sion (Polanyi and Zaenen 2006; Moilanen and Pulman 2007; Shaikh et al. 2007; Choi and Cardie 2008). The contextual polarity of individual expressions is then used for sentence as well as document classification (Kennedy and Inkpen 2006; Li et al. 2010). 1 For a survey on topic detection for aspect-based sentiment analysis, see Liu (2015), Chapter 6. 212 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words At the document level, the standard task is either a classification problem, categorizing documents globally as being positive, negative, or neutral towards a given topic (Pang et al. 2002; Turney 2002; Mullen and Nigel 2004; Blitzer et al. 2007), or regression, assigning a multi-scale rating to the document (Pang and Lee 2005; Goldberg and Zhu 2006; Snyder and Barzilay 2007; Lu et al. 2009; Lizhen et al. 2010; Leung et al. 2011; Moghaddam and Ester 2011; Ganu et al. 2013). The classification/regression approach to determining document sentiment typically uses bag-of-words representations (BOW), which model each text as a vector of the number of occurrences, or the frequency with which each word/construction appears. Bag-of-words features also include n-grams, parts of speech,"
J17-1006,D15-1110,0,0.0281195,"Missing"
J17-1006,N16-1013,0,0.0181405,"Missing"
J17-1006,D14-1119,0,0.265058,"Missing"
J17-1006,P13-1041,0,0.0231431,"O). Hence different senses of the same term may have different sentiment scores. For example, the adjective happy has four senses: The first two expressing joy or pleasure are highly positive (P = 0.875 and P = 0.75, respectively); the third corresponding to eagerly disposed to act or to be of service (as in happy to help) is ambiguous (P = 0.5 and O = 0.5); and the last sense (a happy turn of phrase) is likely to be objective (P = 0.125 and O = 0.875). Although SentiWordNet has been successfully deployed to derive document-level sentiment orientation (Denecke 2009; Martin-Wanton et al. 2010; Popat et al. 2013; Manoussos et al. 2014), word-sense disambiguation algorithms are often needed to find the right sense of a given term. Other intensity-based lexicons in English include Q-WordNet (Agerri and Garc´ıa-Serrano 2010), the MPQA Subjectivity Lexicon (Wiebe et al. 2005), and SO-CAL (Taboada et al. 2011). Methods for automatic ordering of polar adjectives according to their intensity include pattern-based approaches, assuming one single intensity-scale for all adjectives (de Melo and Bansal 2013), or corpus-driven techniques, providing intensity levels to adjectives that bear the same semantic prope"
J17-1006,H05-1043,0,0.64293,"e overview the standard approaches to evaluative text on the three tasks just mentioned. The overview is necessarily brief, because our aim is not to provide an exhaustive survey of the field of sentiment analysis, but to focus on how more linguistically informed representations can contribute to the analysis and extraction of evaluation. For an excellent benchmark comparison of twenty-four different sentiment analysis systems, see Ribeiro et al. 2016. 3.2.1 Topic/Aspect and Holder Detection. Tasks (1) and (2) are important sub-tasks in sentiment analysis (Hu and Liu 2004a; Kim and Hovy 2005; Popescu and Etzioni 2005; Stoyanov and Cardie 2008; Wiegand and Klakow 2010). The holder can be the author, expressing their own evaluation (The movie is great), or the author stating or reporting someone else’s evaluation (My mother loves the movie; My mother said that the movie is great) (Wiebe and Riloff 2005). The holder evaluates a topic or target that is the entity or a part or attribute of the entity that the sentiment is predicated upon (Liu 2015). A topic is thus a global entity e (e.g., a product, service, person, event, or issue) organized hierarchically into a set of attributes or aspects a (e.g., engine,"
J17-1006,prasad-etal-2008-penn,0,0.0126136,"xical knowledge and probably domain knowledge as well (that beaches are usually avoided when it is raining). The study of discourse relations in language can be broadly characterized as falling under two main approaches: the lexically grounded approach and an approach that aims at complete discourse coverage. Perhaps the best example of the first approach is 7 For an introduction to rhetorical, coherence, or discourse relations, see Asher and Lascarides (2003) and Taboada and Mann (2006b). 230 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words the Penn Discourse Treebank (Prasad et al. 2008). The annotation starts that specific lexical items that signal the relation explicitly, most of them conjunctions, and includes two arguments for each conjunction. This leads to partial discourse coverage: There is no guarantee that the entire text is annotated, because parts of the text not related through a conjunction would be excluded. Complete discourse coverage requires annotation of the entire text, with most of the propositions in the text integrated in a structure. It includes work from two theoretical perspectives, either intentionally driven, such as Rhetorical Structure Theory (RS"
J17-1006,D15-1019,0,0.0608447,"Missing"
J17-1006,W10-0104,0,0.0194937,"Missing"
J17-1006,D15-1300,0,0.104013,"ds that evoke that frame. Example (8) illustrates the frame elements of the lexical unit splendid (from the frame D ESIRABILITY). (8) [On clear days,]Circumstance [the view]Evaluee was [absolutely]Degree splendid. Ruppenhofer and Rehbein (2012) argue that a frame-based representation of evaluative language is suitable for capturing multi-word evaluative expressions and idioms such as give away the store and sentiment composition. However, apart from using semantic frames for identifying the topics (or targets) of sentiment (Kim and Hovy 2006) and deriving an intensity-based sentiment lexicon (Raksha et al. 2015), little work has been 221 Computational Linguistics Volume 43, Number 1 done to show the real effectiveness of this deep representation in practical sentiment analysis systems. Purely semantic categorizations include the categories defined within the MPQA project (Wiebe et al. 2005), and the Blogoscopy lexicon (Daille et al. 2011), which classifies opinions according to four main categories following Charaudeau (1992), and the lexicon model for subjectivity description of Dutch verbs proposed by Maks and Vossen (2012). Drawing from Levin (1993), Mathieu (2005), and Wierzbicka (1987), Asher et"
J17-1006,W10-0207,0,0.0218379,"urce domain. These templates are then used to predict wishes in two target domains: product reviews and political discussions. The source domain is a subset of the WISH corpus composed of about 100,000 multilingual wish sentences collected over a period of 10 days in December 2007, when Web users sent in their wishes for the new year. Peace on earth, To be financially stable, and I wish for health and happiness for my family, are typical sentences. Extraction suggestions for products using templates has also been explored for tweets (Dong et al. 2013). Using a small set of hand-crafted rules, Ramanand et al. (2010) focus on two specific kinds of wishes characteristic of product reviews: sentences 241 Computational Linguistics Volume 43, Number 1 that make suggestions about existing products, and sentences that indicate the writer is interested in purchasing a product. The same approach has been used in Brun and Hag`ege (2013) to improve feature-based sentiment analysis of product reviews. It is, however, limited, since the system only detects those wishes that match previously defined rules. Preference extraction. Preference extraction from text has been investigated with the study of comparative opinio"
J17-1006,W03-1014,0,0.367476,"ements. Wiebe and colleagues have devoted considerable effort to finding indicators of subjectivity in sentences (Wiebe et al. 2004; Wiebe and Riloff 2005; Wilson et al. 2006). They propose a set of clues to subjectivity, some of them lexical and some syntactic. Among the lexical clues are psychological verbs and verbs of judgment (dread, love, commend, reprove); verbs and adjectives that usually involve an experiencer (fuss, worry, please, upset, embarrass, dislike); and adjectives that have been previously annotated for polarity. The syntactic clues are learned from manually annotated data (Riloff and Wiebe 2003; Wiebe et al. 2003). 2.5 Evaluation and Pattern Grammar Under the label “evaluation” we will examine a fruitful area of research that has studied evaluation and opinion within a functional framework. The best example of such endeavor is the edited volume by Hunston and Thompson (2000b). Hunston and Thompson (2000a), in their Introduction, propose that there are two aspects to evaluative language: modality and something else, which is variously called evaluation, appraisal, or stance. Modality tends to express opinions about propositions, such as their likelihood (It may rain). It also tends t"
J17-1006,D13-1066,0,0.0678118,"Missing"
J17-1006,W12-3716,0,0.0217792,"points out that syntactic structure influences the interpretation of evaluative expressions and distinguishes between three classes of verbs that mean: The experience or the causation of a rather unpleasant feeling, pleasant feeling, and neither pleasant nor unpleasant. Semantic classes are linked by meaning, intensity, and antonymy relationships. She associates a set of linguistic properties with words and classes and builds semantic representations, described by means of feature structures. Mathieu and Fellbaum (2010) extended this classification to English verbs of emotion. SentiFrameNet (Ruppenhofer and Rehbein 2012) is probably the best example of the syntactico-semantic categorization of evaluative expressions. It extends FrameNet (Baker et al. 1998) to connect opinion source and target to semantic roles, and to add semantic features such as polarity, intensity, and affectedness (changes of state that leave an event participant in a changed state). Evaluative language per se is not described in FrameNet, but several frames are relevant for sentiment analysis like J UDGMENT, O PINION, E MOTION D IRECTED, and semantic roles such as J UDGE or E XPERIENCER. Each frame is associated with a set of lexical uni"
J17-1006,ruppenhofer-etal-2008-finding,0,0.0166808,"05). The holder evaluates a topic or target that is the entity or a part or attribute of the entity that the sentiment is predicated upon (Liu 2015). A topic is thus a global entity e (e.g., a product, service, person, event, or issue) organized hierarchically into a set of attributes or aspects a (e.g., engine, tires are part of the entity car), as can be done in a thesaurus or domain ontology. For holder recognition, it has been shown that semantic role labeling a` la PropBank or FrameNet is beneficial (Bethard et al. 2004; Choi et al. 2006; Kim and Hovy 2006; Gangemi et al. 2014), although Ruppenhofer et al. (2008) argue that evaluation that is connected to its source indirectly via attribution poses challenges that go beyond the capabilities of automatic semantic role labeling, and that discourse structure has to be considered. Topic and aspect recognition, on the other hand, are seen as information 211 Computational Linguistics Volume 43, Number 1 extraction tasks that generally exploit noun or noun phrases, dependency relations, some syntactic patterns at the sentence level, knowledge representation paradigms (like hierarchies or domain ontologies), and external sources (e.g., Wikipedia) to identify"
J17-1006,E14-4023,0,0.0156027,"oussos et al. 2014), word-sense disambiguation algorithms are often needed to find the right sense of a given term. Other intensity-based lexicons in English include Q-WordNet (Agerri and Garc´ıa-Serrano 2010), the MPQA Subjectivity Lexicon (Wiebe et al. 2005), and SO-CAL (Taboada et al. 2011). Methods for automatic ordering of polar adjectives according to their intensity include pattern-based approaches, assuming one single intensity-scale for all adjectives (de Melo and Bansal 2013), or corpus-driven techniques, providing intensity levels to adjectives that bear the same semantic property (Ruppenhofer et al. 2014; Sharma et al. 2015). 5.1.2 Emotion and Affect Categorizations. A second way to enhance polarity consists of encoding, in addition to polarity and/or intensity, information about the semantic category of the evaluative expression. Categories can be defined according to psychologically based classifications of emotions and affect of various sorts that attempt to group evaluation into a set of basic emotions such as anger, fear, surprise, or love (Osgood et al. 1957; Izard 1971; Russell 1983; Ekman 1984; Ortony et al. 1988). Well-known Affect resources in English include the General Inquier (St"
J17-1006,shutova-teufel-2010-metaphor,0,0.0183466,"Missing"
J17-1006,J13-2003,0,0.00811326,"Missing"
J17-1006,J15-4002,0,0.064021,"Missing"
J17-1006,N07-1038,0,0.0193166,"Missing"
J17-1006,D11-1014,0,0.0385287,"Missing"
J17-1006,D12-1110,0,0.0561387,"Missing"
J17-1006,D13-1170,0,0.00901532,"tive Language Beyond Bags of Words Rules being language-and context-dependent, another alternative approach is to represent each node in a parse tree with a vector, and then learn how to compose leaf vectors in a bottom–up fashion. The composition process is modeled as a function learned from the training data, which can be standard data sets that only have document-level sentiment annotation (e.g., star ratings [Yessenalina and Cardie 2011; Socher et al. 2011, 2012] or sentiment treebanks with fine-grained annotations for every single node of the top parse tree (Johansson and Moschitti 2013; Socher et al. 2013; Dong et al. 2014; Hall et al. 2014; Zhu et al. 2015). The Stanford Sentiment Treebank is probably the best known example (Socher et al. 2013). It is composed of 215,154 phrases annotated by three human judges according to six sentiment values, ranging from very negative to very positive, with neutral in the middle. Those phrase annotations are then combined with a dependency parse to propagate sentiment up through the nodes of the tree. Various composition functions have been proposed in the literature. For example, Yessenalina and Cardie (2011) represent each word as a matrix and combine wo"
J17-1006,P09-1026,0,0.03014,"ly subjective genre, then finding high levels of subjective and evaluative words is not surprising. On the other hand, in what Biber and Finegan call Faceless texts, a high proportion of evaluative expressions is more significant, and is probably indicative of an overtly subjective text (i.e., one with a higher “volume,” from the point of view of evaluative language). The term stance, in Biber and Finegan’s work, is quite close to our use of evaluative language or opinion. There is another meaning of stance, with regard to position in a debate, namely, for or against, or ideological position (Somasundaran and Wiebe 2009). We will not have much to say about recognizing positions, although it is an active and related area of research (Thomas et al. 2006; Hasan and Ng 2013). The SemEval 2016 competition included both sentiment analysis and stance detection tasks, where stance is defined as “automatically determining from text whether the author is in favor of, against, or neutral towards a proposition or target (Mohammad et al. in press). Targets mentioned in the SemEval task include legalization of abortion, atheism, climate change, or Hillary Clinton. The SemEval task organizers further specify that sentiment"
J17-1006,D09-1018,0,0.00958932,"relations on both subjectivity and polarity analysis in movie reviews in French and in English, as well as letters to the editor in French. Zhou et al. (2011) focused on five RST relations (C ONTRAST, C ONDITION , C ONTINUATION , C AUSE, and P URPOSE). Instead of relying on cue phrases, they proposed an unsupervised method for discovering these relations and eliminating polarity ambiguities at the sentence level. Zirn et al. (2011) grouped RST relations into Contrast vs. Non-Contrast and integrated them as features in a Markov Logic Network to encode information between neighboring segments. Somasundaran et al. (2009) proposed the notion of opinion frames as a representation of documents at the discourse level in order to improve sentence-based polarity classification and to recognize the overall stance. Two sets of relations were used: relations between targets (S AME and A LTERNATIVE) and relations between opinion expressions (R EINFORCING and N ON - REINFORCING). Lazaridou et al. (2013) also use a specific scheme of discourse relations. However, rather than relying on gold discourse annotations, they jointly predict sentiment, aspect, and discourse relations and show that the model improves accuracy of"
J17-1006,N03-1030,0,0.128973,"Missing"
J17-1006,D14-1006,0,0.0236959,"Missing"
J17-1006,C08-1103,0,0.00946047,"proaches to evaluative text on the three tasks just mentioned. The overview is necessarily brief, because our aim is not to provide an exhaustive survey of the field of sentiment analysis, but to focus on how more linguistically informed representations can contribute to the analysis and extraction of evaluation. For an excellent benchmark comparison of twenty-four different sentiment analysis systems, see Ribeiro et al. 2016. 3.2.1 Topic/Aspect and Holder Detection. Tasks (1) and (2) are important sub-tasks in sentiment analysis (Hu and Liu 2004a; Kim and Hovy 2005; Popescu and Etzioni 2005; Stoyanov and Cardie 2008; Wiegand and Klakow 2010). The holder can be the author, expressing their own evaluation (The movie is great), or the author stating or reporting someone else’s evaluation (My mother loves the movie; My mother said that the movie is great) (Wiebe and Riloff 2005). The holder evaluates a topic or target that is the entity or a part or attribute of the entity that the sentiment is predicated upon (Liu 2015). A topic is thus a global entity e (e.g., a product, service, person, event, or issue) organized hierarchically into a set of attributes or aspects a (e.g., engine, tires are part of the ent"
J17-1006,strapparava-valitutti-2004-wordnet,0,0.127348,"s can be defined according to psychologically based classifications of emotions and affect of various sorts that attempt to group evaluation into a set of basic emotions such as anger, fear, surprise, or love (Osgood et al. 1957; Izard 1971; Russell 1983; Ekman 1984; Ortony et al. 1988). Well-known Affect resources in English include the General Inquier (Stone et al. 1962), the Affective Norms for English Words (ANEW) (Bradley and Lang 1999), the LIWC Dictionary,6 and the LEW list (Francisco et al. 2010). WordNet-Affect is also a resource for the lexical representation of affective knowledge (Strapparava and Valitutti 2004). It associates with each affective synset from WordNet an emotion class, following the classes defined within Ortony et al.’s (1988) model of emotions. Other interesting affective resources are SenticNet (Poria et al. 2013) and the EmotiNet knowledge base (Balahur et al. 2011), which associates polarity and affective information with affective situations such as accomplishing a goal, failing an exam, or celebrating a special occasion. Modeling such situations is particularly important for recognizing implicit emotions (Balahur et al. 5 See Liu (2015) Chapter 7 for an overview of existing tech"
J17-1006,C12-3005,0,0.0605719,"Missing"
J17-1006,W15-2916,0,0.0239058,"n, on the other hand, is not necessarily perceived as a nuisance, and the expression can connote sympathy and be used to appeal to charity. Taboada et al. (2011) noticed that some nouns and verbs often have both neutral and non-neutral connotations. For instance, inspire has a very positive meaning (The teacher inspired her students to pursue their dreams), as well as a rather neutral meaning (This movie was inspired by 235 Computational Linguistics Volume 43, Number 1 real events). Some instances of different connotations can be addressed through wordsense disambiguation (Akkaya et al. 2011; Sumath and Inkpen 2015). In other cases, the problem is framed as domain dependency. What is considered positive in one domain may be negative in another. Example (22) was seen on the Toulouse transit system. The word volume changes its connotation, or polarity, with different domains of application: Volume is good for hair; (loud) volume is bad for public transit. (22) Le volume c’est bien dans les cheveux. . . moins dans les transports. ‘Volume is good in hair. . . less so in transportation.’ Although connotation has a strong impact on sentiment analysis, most current subjective lexicons contain words that are int"
J17-1006,N15-3001,0,0.0227026,"Missing"
J17-1006,W09-3909,1,0.863176,"Missing"
J17-1006,J11-2001,1,0.0815037,"e span regardless of external context within or outside a sentence or document. Some researchers argue instead for a ternary classification, with a neutral category to indicate the absence of evaluation (Koppel and Schler 2006; Agarwal et al. 2011). The intensity of a subjective span is often combined with its prior binary polarity to form an evaluation score that tells us about the degree of the evaluation, that is, how positive or negative the word is. Several types of scales have been used in sentiment analysis research, going from continuous scales (Benamara et al. 2007) to discrete ones (Taboada et al. 2011). For example, if we have a three-point scale to encode strength, we can propose score(good) = +1 and score(brilliant) = +3. Generally, there is no consensus on how many points are needed on the scale, but the chosen length of the scale has to ensure a trade-off between a fine-grained categorization of subjective words and the reliability of this categorization with respect to human judgments. Two main approaches have been proposed, corpus-based (mostly using machine learning techniques) and lexicon-based systems, which typically perform, in their most simplistic form, a lexical lookup and com"
J17-1006,P11-2100,0,0.04135,"Missing"
J17-1006,J02-4002,0,0.0273946,"iter’s) communicative intention perspective (Grosz and Sidner 1986; Moore and Paris 1993; Lochbaum 1998). Genre-induced text structure aims at segmenting discourse into different parts that serve different functions. This segmentation is achieved through a conventionalized set of building blocks that contribute to the overall text function. These building blocks are called content, functional, or discourse zones. Discourse zones are specific to particular genres, such as the communicative roles played by the introduction, background, and conclusion sections in a scientific paper (Swales 1990; Teufel and Moens 2002). Other genres studied include law texts (Palau and Moens 2009), biomedical articles (Agarwal and Yu 2009), and movie reviews (Bieler et al. 2007). Top–down approaches in sentiment analysis assume that in a subjective document only some parts are relevant to the overall sentiment. Irrelevant parts thus have to be filtered out or de-emphasized, and the remaining parts are used to infer an overall evaluation at the document level. For example, a recent psycholinguistic and psychological study shows that polarity classification should concentrate on messages in the final position of the text (Bec"
J17-1006,W06-1639,0,0.0325824,"Missing"
J17-1006,P08-1036,0,0.0446461,"Missing"
J17-1006,P10-1059,0,0.0366618,"Missing"
J17-1006,N13-1100,0,0.0205653,"n that readers have to be able to change their expectations about the author’s typical style and previous books. Narayanan et al. (2009) also focused on conditionals marked by connectives such as if, unless, and even if, and proposed a supervised learning algorithm to determine if sentiment expressed on different topics in a conditional sentence is positive, negative, or neutral. Instead of extracting specific connectives, some researchers use a compiled list of connectives and incorporate it as features in a bag-of-words model to improve sentiment classification accuracy (Mittal et al. 2013; Trivedi and Eisenstein 2013). Others identify discourse connectives automatically, relying on a discourse tagger trained on the Penn Discourse Treebank (Yang and Cardie 2014). Relations that have been used in sentiment analysis are either relations proposed under various theories of discourse (e.g., RST, SDRT), or a set of relations built specifically to be used in sentiment analysis. Asher et al. (2008) considered five types of SDRTlike rhetorical relations, both explicit and implicit (C ONTRAST, C ORRECTION , R ESULT, C ONTINUATION, and S UPPORT), and conducted a manual study in which they represented opinions in text"
J17-1006,P02-1053,0,0.0598221,"Zaenen 2006; Moilanen and Pulman 2007; Shaikh et al. 2007; Choi and Cardie 2008). The contextual polarity of individual expressions is then used for sentence as well as document classification (Kennedy and Inkpen 2006; Li et al. 2010). 1 For a survey on topic detection for aspect-based sentiment analysis, see Liu (2015), Chapter 6. 212 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words At the document level, the standard task is either a classification problem, categorizing documents globally as being positive, negative, or neutral towards a given topic (Pang et al. 2002; Turney 2002; Mullen and Nigel 2004; Blitzer et al. 2007), or regression, assigning a multi-scale rating to the document (Pang and Lee 2005; Goldberg and Zhu 2006; Snyder and Barzilay 2007; Lu et al. 2009; Lizhen et al. 2010; Leung et al. 2011; Moghaddam and Ester 2011; Ganu et al. 2013). The classification/regression approach to determining document sentiment typically uses bag-of-words representations (BOW), which model each text as a vector of the number of occurrences, or the frequency with which each word/construction appears. Bag-of-words features also include n-grams, parts of speech, and features"
J17-1006,C96-2162,0,0.245567,"Missing"
J17-1006,C14-1221,0,0.0249773,"Missing"
J17-1006,J12-2005,0,0.0117103,"ntensification, downtoning, presuppositions, discourse, and irony. This section focuses on valence shifters that may impact evaluative expressions at the sentence or the sub-sentential level. Dealing with valence shifters roughly involves three sub-tasks: identifying these expressions and their scope, analyzing their effect on evaluation, and computing sentiment composition, leveraging this effect to update the prior polarity of opinion expressions. The first task makes use of full sentence parsing or dependency parsing to identify the scope of cues (Councill et al. 2010; Wiegand et al. 2010; Velldall et al. 2012). We detail in the following sections the latter two tasks. 5.2.1 Effect of Valence Shifters on Sentiment Analysis Intensification and downtoning. Whatever parts of speech are identified as conveying sentiment, they can be intensified and downtoned by being modified. The general term intensifier is used for devices that change the intensity of an individual word, whether by bringing it up or down. Many devices intensify; for instance, adjectives may intensify or downtone the noun they accompany (a definite success). Periphrastic expressions and hedges also change the intensity of other words,"
J17-1006,W08-0606,0,0.0410925,"Missing"
J17-1006,P14-1018,0,0.0646258,"Missing"
J17-1006,C14-1053,0,0.020648,"umentative scheme that takes the form of a number of premises working together to support or attack a conclusion. Finally, the last approach makes use of topic changes as indicators of a change in the argumentation line. For instance, if the topic of a given proposition is similar to the one discussed in the preceding propositions, then one can assume that these propositions are connected and are following the same line of reasoning. In sentiment analysis, the role of argumentation has been investigated for document polarity classification (Hogenboom et al. 2010; Vincent and Winterstein 2014; Wachsmuth et al. 2014). For example, Vincent and Winterstein (2014) noticed in their corpus of movie reviews that a persuasive argumentation in positive reviews often consists of several independent arguments in favor of its conclusion. In negative reviews, however, a single negative argument appears to be enough. To date, most existing studies use a predefined list of markers to extract arguments and build the document’s argumentative structure. Although the approach is rather simple (only a few arguments are marked), it has shown to improve document classification. 6.1.2 Bottom–Up Approaches. Bottom–up parsing de"
J17-1006,P15-1100,0,0.016396,"of words in a review and its star rating. 238 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words Most of these pragmatic features still rely on linguistic aspects of the tweet by using only the text of the tweet. Recent work explores other ways to go further by capturing the context outside of the utterance that is needed to infer irony. Bamman and Smith (2015) explore properties of the author (like profile information and historical salient terms), the audience (such as author/addressee interactional topics), and the immediate communicative environment (previous tweets). Wallace et al. (2015) exploit signals extracted from the conversational threads to which comments belong. Finally, Karoui et al. (2015) propose a model that detects irony in tweets containing an asserted fact of the form Not(P). They hypothesize that such tweets are ironic if and only if one can prove the validity of P in reliable external sources, such as Wikipedia or online newspapers. 6.4 Extra-Linguistic Information In the previous section, we saw how irony classification can gain in accuracy when extra-linguistic or extra-textual features are taken into account. In this section, we further discuss how sentime"
J17-1006,P09-1027,0,0.0543674,"Missing"
J17-1006,D14-1126,0,0.0208461,"determination. These tasks are either performed independently from each other or simultaneously. When sub-tasks (1), (2), and (3) are treated independently, the dependencies between sentiment and topics are ignored. To account for these dependencies, two main approaches have been explored: sequential learning (Jakob and Gurevych 2010; Yang and Cardie 2012; Mitchell et al. 2013; Vo and Zhang 2015) and probabilistic joint sentiment-topic models, which are capable of detecting sentiment and topic at the same time in an unsupervised fashion (Hu and Liu 2004a; Zhuang et al. 2006; Lin and He 2009; Wang and Ester 2014; Nguyen and Shirai 2015), even though some approaches still require a seed of sentiment-bearing words and/or aspect seeds (Qiu et al. 2009; Hai et al. 2012). In the following sections we overview the standard approaches to evaluative text on the three tasks just mentioned. The overview is necessarily brief, because our aim is not to provide an exhaustive survey of the field of sentiment analysis, but to focus on how more linguistically informed representations can contribute to the analysis and extraction of evaluation. For an excellent benchmark comparison of twenty-four different sentiment"
J17-1006,Y13-1035,0,0.114647,"Missing"
J17-1006,Q14-1024,0,0.0949871,"Missing"
J17-1006,J04-3002,0,0.0729659,"ir polarity downtoned or otherwise hedged (in the presence of modal verbs or intensional verbs), or may interact with nonveridical operators in complex ways. The presence of conditionals as nonveridical operators has led to some work on the nature of coherence relations and their role in evaluative language (Asher et al. 2009; Trnavac and Taboada 2012). We discuss computational treatment of nonveridicality in Section 5. 2.4 Subjectivity The term subjectivity has different senses and has been adopted in computational linguistics to encompass automatic extraction of both sentiment and polarity (Wiebe et al. 2004). Subjectivity in linguistics is a much more general and complex phenomenon, often relating to point of view. Researchers working with this definition are interested in deixis, locative expressions, and use of modal verbs, among other phenomena (Langacker 1990). The connections to sentiment analysis are obvious, because epistemic modals convey subjective meaning and are related to evidentiality. White (2004) has also written about subjectivity as point of view, and some of his work is framed within the Appraisal framework (see Section 2.6). White discusses the opposition between objective and"
J17-1006,J94-2004,0,0.258176,"Aspect-based models of sentiment have been most popular in the domain of consumer reviews, where movies, books, restaurants, hotels, or other consumer products are evaluated in a decompositional manner, with each aspect (e.g., ambiance, food, service, or price for a restaurant) evaluated separately. In Section 4.2, we will introduce a new definition, because we believe that contextual phenomena need to be accounted for in a definition of evaluative language. 3.2 Standard Approaches The study of how to automatically extract evaluation from natural language data began in the 1990s (Hearst 1992; Wiebe 1994; Spertus 1997; Hatzivassiloglou and McKeown 1997; Bruce and Wiebe 1999). These initial efforts proceeded in a rather similar way by making an in-depth inspection of linguistic properties of evaluative language a prior step to any automatic treatment. The detection of expressions of evaluation relied not only on individual words taken in isolation but also on surrounding material or contextual information that were considered essential for a better understanding of evaluative language in text both at the expression and document level. For example, Hearst (1992) proposed a model inspired in cog"
J17-1006,N10-1121,0,0.0184236,"t on the three tasks just mentioned. The overview is necessarily brief, because our aim is not to provide an exhaustive survey of the field of sentiment analysis, but to focus on how more linguistically informed representations can contribute to the analysis and extraction of evaluation. For an excellent benchmark comparison of twenty-four different sentiment analysis systems, see Ribeiro et al. 2016. 3.2.1 Topic/Aspect and Holder Detection. Tasks (1) and (2) are important sub-tasks in sentiment analysis (Hu and Liu 2004a; Kim and Hovy 2005; Popescu and Etzioni 2005; Stoyanov and Cardie 2008; Wiegand and Klakow 2010). The holder can be the author, expressing their own evaluation (The movie is great), or the author stating or reporting someone else’s evaluation (My mother loves the movie; My mother said that the movie is great) (Wiebe and Riloff 2005). The holder evaluates a topic or target that is the entity or a part or attribute of the entity that the sentiment is predicated upon (Liu 2015). A topic is thus a global entity e (e.g., a product, service, person, event, or issue) organized hierarchically into a set of attributes or aspects a (e.g., engine, tires are part of the entity car), as can be done i"
J17-1006,W10-3111,0,0.0183769,"aspects of meaning, intensification, downtoning, presuppositions, discourse, and irony. This section focuses on valence shifters that may impact evaluative expressions at the sentence or the sub-sentential level. Dealing with valence shifters roughly involves three sub-tasks: identifying these expressions and their scope, analyzing their effect on evaluation, and computing sentiment composition, leveraging this effect to update the prior polarity of opinion expressions. The first task makes use of full sentence parsing or dependency parsing to identify the scope of cues (Councill et al. 2010; Wiegand et al. 2010; Velldall et al. 2012). We detail in the following sections the latter two tasks. 5.2.1 Effect of Valence Shifters on Sentiment Analysis Intensification and downtoning. Whatever parts of speech are identified as conveying sentiment, they can be intensified and downtoned by being modified. The general term intensifier is used for devices that change the intensity of an individual word, whether by bringing it up or down. Many devices intensify; for instance, adjectives may intensify or downtone the noun they accompany (a definite success). Periphrastic expressions and hedges also change the int"
J17-1006,W05-0308,0,0.0580658,"y result in the speaker conveying, in addition to the literal meaning of the utterance, an additional meaning that does not contribute to the truth-conditional content of the utterance, which leads to conversational implicature. Implicatures are thus inferences that can defeat literal and compositional meaning. Example (16) is a typical example of relevance violation: B conveys to A that he will not be accepting A’s invitation for dinner although he has not literally said so. (16) A. Let’s have dinner tonight. B. I have to finish my homework. Borrowing from Grice’s conversational implicature, Wilson and Wiebe (2005) view implicit evaluation as opinion implicatures, which are “the default inferences that may not go through in context.” Hence, subjectivity is part of what is said while private-state inferences is part of what is implied. Example (17), taken from the MPQA corpus (Wiebe et al. 2005), illustrates the inter-dependencies among explicit and implicit sentiment. The explicit sentiment happy clearly indicates a positive sentiment but, at the same time, a negative sentiment toward Chavez himself may be inferred (somebody’s fall is a negative thing; being happy about it implies that they deserved it,"
J17-1006,H05-1044,0,0.0463397,"tematic way. Research in sentiment analysis has found that accurately identifying negative sentiment is more difficult, perhaps because we use fewer negative terms and because negative evaluation is couched in positive terms (Pang and Lee 2008, Chapter 3). One way to solve this problem is to, in a sense, follow the Negativity Bias (Rozin and Royzman 2001; Jing-Schmidt 2007): If a negative word appears, then it has more impact. This has been achieved by weighing negative words more heavily than positives in aggregation (Taboada et al. 2011). Most approaches treat negation as polarity reversal (Wilson et al. 2005; Polanyi and Zaenen 2006; Moilanen and Pulman 2007; Choi and Cardie 2008). However, negation cannot be reduced to reversing polarity. For example, if we assume that the score of the adjective excellent is +3, then the opinion score in This student is not excellent cannot be −3. The sentence probably means that the student is not good enough. It is thus difficult to negate a strongly positive word without implying that a less positive one is to some extent possible (not excellent, but not horrible either). A possible solution is to use shift negation, in which the effect of a negator is to shi"
J17-1006,J09-3003,0,0.0890485,"(o) and Intensity (i). Sentiment s is further defined by Liu (2012) as a triple of y = type of sentiment; o = orientation or polarity; and i = intensity of the opinion (cf. Section 3.1). With respect to polarity, the prior polarity of a word, that is, its polarity in the dictionary sense, may be different from contextual polarity, which is determined on the basis of a sentiment composition process that captures how opinion expressions interact with each other and with specific linguistic operators such as intensifiers, negation, or modality (Polanyi and Zaenen 2006; Moilanen and Pulman 2007; Wilson et al. 2009). For instance, in This restaurant is not good enough, the prior positive orientation of the word good has to be combined with the negation not and the modifier enough. Apart from local linguistic operators, prior polarity may also vary according to the context outside of the utterance, including domain factors (Aue and Gamon 2005; Blitzer et al. 2007; Bollegala et al. 2011). A given span may be subjective in one context and objective in another. Haas and Versley (2015) observe that seemingly neutral adjectives can become polar when combined with aspects of a movie (elaborate continuation, exp"
J17-1006,wilson-2008-annotating,0,0.0210625,"d with it, will lead us to interpret many of the meanings in the review as negative, even if negative opinion is not always explicitly stated. Bednarek (2006) also discusses this phenomenon in news discourse, characterizing it as evaluative prosody. Discourse prosody is also related to the sentence proximity assumption of Wiebe’s (1994), whereby subjective or objective sentences are assumed to cluster together (see Section 3.2). In general terms, there are three ways to make an evaluation implicit or invoked. The first one is to describe desirable or undesirable situations (states or events). Wilson (2008) refers to these as polar facts, that is, they are facts (as opposed to opinions), but they convey polarity because of the way such states or events are conventionally 234 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words associated with a positive or negative evaluation. Van de Kauter et al. (2015) state that the polarity of such facts is inferred using common sense, world knowledge, or context. Situations can be conveyed through verb phrases like those in italics in Examples (18) and (19), or noun phrases like the word valley in Example (20). The first two examples are"
J17-1006,D09-1159,0,0.0116032,"n poses challenges that go beyond the capabilities of automatic semantic role labeling, and that discourse structure has to be considered. Topic and aspect recognition, on the other hand, are seen as information 211 Computational Linguistics Volume 43, Number 1 extraction tasks that generally exploit noun or noun phrases, dependency relations, some syntactic patterns at the sentence level, knowledge representation paradigms (like hierarchies or domain ontologies), and external sources (e.g., Wikipedia) to identify explicit aspects (Hu and Liu 2004a; Popescu and Etzioni 2005; Zhao and Li 2009; Wu et al. 2009).1 3.2.2 Sentiment Determination. Task (3), sentiment determination, is probably the most studied. It consists of identifying polarity or orientation. In its simplest form, it corresponds to the binary orientation (positive or negative) of a subjective span regardless of external context within or outside a sentence or document. Some researchers argue instead for a ternary classification, with a neutral category to indicate the absence of evaluation (Koppel and Schler 2006; Agarwal et al. 2011). The intensity of a subjective span is often combined with its prior binary polarity to form an eval"
J17-1006,D11-1123,0,0.0289187,"mposed of 215,154 phrases annotated by three human judges according to six sentiment values, ranging from very negative to very positive, with neutral in the middle. Those phrase annotations are then combined with a dependency parse to propagate sentiment up through the nodes of the tree. Various composition functions have been proposed in the literature. For example, Yessenalina and Cardie (2011) represent each word as a matrix and combine words using iterated matrix multiplication, which allows for modeling both additive (for negation) and multiplicative (for intensifiers) semantic effects. Wu et al. (2011) propose a graph-based method for computing a sentence-level sentiment representation. The vertices of the graph are the opinion targets, opinion expressions, and modifiers of opinion; the edges represent relations among them (mainly, opinion restriction and opinion expansion). However, using recursive neural tensor networks trained over the Stanford Sentiment Treebank yields significant improvements over approaches based on token-level features. Haas and Versley (2015) observe that this gain in accuracy comes at the cost of the huge effort needed to built such treebanks, which are necessarily"
J17-1006,C10-2153,0,0.0145068,"Missing"
J17-1006,D12-1122,0,0.0198715,"ndard” approaches) focus on the automatic extraction of one or several elements of the quadruple (e, a, s, h), making sentiment analysis a field that involves roughly three main sub-tasks: (1) topic/aspect extraction, (2) holder identification, and (3) sentiment determination. These tasks are either performed independently from each other or simultaneously. When sub-tasks (1), (2), and (3) are treated independently, the dependencies between sentiment and topics are ignored. To account for these dependencies, two main approaches have been explored: sequential learning (Jakob and Gurevych 2010; Yang and Cardie 2012; Mitchell et al. 2013; Vo and Zhang 2015) and probabilistic joint sentiment-topic models, which are capable of detecting sentiment and topic at the same time in an unsupervised fashion (Hu and Liu 2004a; Zhuang et al. 2006; Lin and He 2009; Wang and Ester 2014; Nguyen and Shirai 2015), even though some approaches still require a seed of sentiment-bearing words and/or aspect seeds (Qiu et al. 2009; Hai et al. 2012). In the following sections we overview the standard approaches to evaluative text on the three tasks just mentioned. The overview is necessarily brief, because our aim is not to pro"
J17-1006,P14-1031,0,0.0105284,"conditionals marked by connectives such as if, unless, and even if, and proposed a supervised learning algorithm to determine if sentiment expressed on different topics in a conditional sentence is positive, negative, or neutral. Instead of extracting specific connectives, some researchers use a compiled list of connectives and incorporate it as features in a bag-of-words model to improve sentiment classification accuracy (Mittal et al. 2013; Trivedi and Eisenstein 2013). Others identify discourse connectives automatically, relying on a discourse tagger trained on the Penn Discourse Treebank (Yang and Cardie 2014). Relations that have been used in sentiment analysis are either relations proposed under various theories of discourse (e.g., RST, SDRT), or a set of relations built specifically to be used in sentiment analysis. Asher et al. (2008) considered five types of SDRTlike rhetorical relations, both explicit and implicit (C ONTRAST, C ORRECTION , R ESULT, C ONTINUATION, and S UPPORT), and conducted a manual study in which they represented opinions in text as shallow semantic feature structures. These are combined into 231 Computational Linguistics Volume 43, Number 1 an overall opinion using hand-wr"
J17-1006,P11-1164,0,0.0218217,"Missing"
J17-1006,N15-1057,0,0.0171548,"d Inkpen 2006; Taboada et al. 2011). Furthermore, Brooke et al. (2009) showed that porting dictionaries to a new language or a new domain is not an onerous task, probably less onerous than labeling data in a new domain for a classifier. Combining lexicon-based learning and corpus-based learning could be a good solution to incorporate both domain-specific and domain-independent knowledge. This has been investigated in several studies relying either on a complete lexicon or fully labeled corpus (Andreevskaia and Bergler 2008; Qiu et al. 2009), or a partially labeled corpus as training examples (Yang et al. 2015). 4. Towards a Dynamic Model of Evaluative Language The model presented in Section 3, where evaluation = (e, a, s, h, t), is an operationalized representation that views sentiment analysis as an information extraction task. The model builds a structured representation from any unstructured evaluative text that captures the core elements relative to the evaluations expressed in the text. Liu (2012) points out that not all applications need all five elements of the quintuple. In some cases, it is hard to distinguish between entity and aspect, or there is no need to deal with aspect. In other cas"
J17-1006,D11-1016,0,0.0106657,"ment composition has been shown to outperform bag-of-words learning classification of sentiment at the sentence level (Choi and Cardie 2008). 226 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words Rules being language-and context-dependent, another alternative approach is to represent each node in a parse tree with a vector, and then learn how to compose leaf vectors in a bottom–up fashion. The composition process is modeled as a function learned from the training data, which can be standard data sets that only have document-level sentiment annotation (e.g., star ratings [Yessenalina and Cardie 2011; Socher et al. 2011, 2012] or sentiment treebanks with fine-grained annotations for every single node of the top parse tree (Johansson and Moschitti 2013; Socher et al. 2013; Dong et al. 2014; Hall et al. 2014; Zhu et al. 2015). The Stanford Sentiment Treebank is probably the best known example (Socher et al. 2013). It is composed of 215,154 phrases annotated by three human judges according to six sentiment values, ranging from very negative to very positive, with neutral in the middle. Those phrase annotations are then combined with a dependency parse to propagate sentiment up through the no"
J17-1006,D10-1102,0,0.0170333,"Missing"
J17-1006,W03-1017,0,0.121386,"step, only words that appear near a holder and/or topic are considered. The sentiment scores of these words are then combined with various aggregation functions (average, geometric mean, etc.). Hu and Liu (2004a) propose a similar approach, taking into account in addition opposition words (like but, however). They also generate a feature-based summary of each review. At the sentence level, the task is to determine the subjective orientation and then the opinion orientation of sequences of words in the sentence that are determined to be subjective or express an opinion (Riloff and Wiebe 2003; Yu and Vasileios 2003; Wiebe and Riloff 2005; Taboada et al. 2011), with the assumption that each sentence usually contains a single opinion. To better compute the contextual polarity of opinion expressions, some researchers have used subjectivity word sense disambiguation to identify whether a given word has a subjective or an objective sense (Akkaya et al. 2009). Other approaches identify valence shifters (negation, modality, and intensifiers) that strengthen, weaken, or reverse the prior polarity of a word or an expression (Polanyi and Zaenen 2006; Moilanen and Pulman 2007; Shaikh et al. 2007; Choi and Cardie 2"
J17-1006,P11-2101,0,0.0102737,"convey polarity because of the way such states or events are conventionally 234 Benamara, Taboada, and Mathieu Evaluative Language Beyond Bags of Words associated with a positive or negative evaluation. Van de Kauter et al. (2015) state that the polarity of such facts is inferred using common sense, world knowledge, or context. Situations can be conveyed through verb phrases like those in italics in Examples (18) and (19), or noun phrases like the word valley in Example (20). The first two examples are translations from the French CASOAR corpus (Benamara et al. 2016); Example (20) comes from Zhang and Liu (2011). (18) The movie is not bad, although some persons left the auditorium. (19) This movie is poignant, and the actors excellent. It will remain in your DVD closet. (20) Within a month, a valley formed in the middle of the mattress. Situations that affect the evaluation of entities can be automatically identified relying either on co-occurrence assumptions, a set of rules, or patterns enlarged via bootstrapping (Goyal et al. 2010; Benamara et al. 2011; Zhang and Liu 2011; Riloff et al. 2013; Deng et al. 2014b; Wiebe and Deng 2014). For example, Riloff et al. (2013) learn from tweets patterns of t"
J17-1006,D10-1006,0,0.0123786,"witches to other related topics, or reverts back to an older topic. This is known as discourse popping, where a topic switch is signaled by the fact that the new information does not attach to the prior clause, but rather to an earlier one that dominates it (Asher and Lascarides 2003). Investigating evaluations toward a given topic and how related topics influence the holder’s evaluation on this main topic is an interesting and challenging research problem (He et al. 2013). Multi-topic sentiment analysis is generally seen as a special case of multi-aspect sentiment analysis (Hu and Liu 2004b; Zhao et al. 2010). With the rise of social media, tracking follow-up evaluations and how they change towards a topic over time has become very popular (Wang et al. 2012; Farzindar and Inkpen 2015). See also Section 6.4 on the contribution of social network structure to sentiment analysis. Sentiment (s). Polarized evaluative expressions may be explicit or implicit. The former are triggered by specific subjective words or symbols (adjectives, adverbs, verbs, nouns, interjections, emoticons, etc.), whereas the latter, also known as fact-implied opinions (Liu 2012), are triggered by situations that describe a desi"
J17-1006,D11-1015,0,0.0117872,"ical relations, both explicit and implicit (C ONTRAST, C ORRECTION , R ESULT, C ONTINUATION, and S UPPORT), and conducted a manual study in which they represented opinions in text as shallow semantic feature structures. These are combined into 231 Computational Linguistics Volume 43, Number 1 an overall opinion using hand-written rules based on manually annotated discourse relations. Benamara et al. (2016) extended this study by assessing the impact of 17 relations on both subjectivity and polarity analysis in movie reviews in French and in English, as well as letters to the editor in French. Zhou et al. (2011) focused on five RST relations (C ONTRAST, C ONDITION , C ONTINUATION , C AUSE, and P URPOSE). Instead of relying on cue phrases, they proposed an unsupervised method for discovering these relations and eliminating polarity ambiguities at the sentence level. Zirn et al. (2011) grouped RST relations into Contrast vs. Non-Contrast and integrated them as features in a Markov Logic Network to encode information between neighboring segments. Somasundaran et al. (2009) proposed the notion of opinion frames as a representation of documents at the discourse level in order to improve sentence-based pol"
J17-1006,S15-1001,0,0.024432,"and context-dependent, another alternative approach is to represent each node in a parse tree with a vector, and then learn how to compose leaf vectors in a bottom–up fashion. The composition process is modeled as a function learned from the training data, which can be standard data sets that only have document-level sentiment annotation (e.g., star ratings [Yessenalina and Cardie 2011; Socher et al. 2011, 2012] or sentiment treebanks with fine-grained annotations for every single node of the top parse tree (Johansson and Moschitti 2013; Socher et al. 2013; Dong et al. 2014; Hall et al. 2014; Zhu et al. 2015). The Stanford Sentiment Treebank is probably the best known example (Socher et al. 2013). It is composed of 215,154 phrases annotated by three human judges according to six sentiment values, ranging from very negative to very positive, with neutral in the middle. Those phrase annotations are then combined with a dependency parse to propagate sentiment up through the nodes of the tree. Various composition functions have been proposed in the literature. For example, Yessenalina and Cardie (2011) represent each word as a matrix and combine words using iterated matrix multiplication, which allows"
J17-1006,I11-1038,0,0.0158758,"ume 43, Number 1 an overall opinion using hand-written rules based on manually annotated discourse relations. Benamara et al. (2016) extended this study by assessing the impact of 17 relations on both subjectivity and polarity analysis in movie reviews in French and in English, as well as letters to the editor in French. Zhou et al. (2011) focused on five RST relations (C ONTRAST, C ONDITION , C ONTINUATION , C AUSE, and P URPOSE). Instead of relying on cue phrases, they proposed an unsupervised method for discovering these relations and eliminating polarity ambiguities at the sentence level. Zirn et al. (2011) grouped RST relations into Contrast vs. Non-Contrast and integrated them as features in a Markov Logic Network to encode information between neighboring segments. Somasundaran et al. (2009) proposed the notion of opinion frames as a representation of documents at the discourse level in order to improve sentence-based polarity classification and to recognize the overall stance. Two sets of relations were used: relations between targets (S AME and A LTERNATIVE) and relations between opinion expressions (R EINFORCING and N ON - REINFORCING). Lazaridou et al. (2013) also use a specific scheme of"
J17-1006,C08-2002,1,\N,Missing
J17-1006,C10-1103,0,\N,Missing
J17-1006,D10-1101,0,\N,Missing
J17-1006,J98-4001,0,\N,Missing
J17-1006,D10-1008,0,\N,Missing
J17-1006,N10-1042,0,\N,Missing
J17-1006,J15-3002,0,\N,Missing
J17-1006,W07-1515,0,\N,Missing
J17-1006,N09-1030,0,\N,Missing
J17-1006,J13-3002,0,\N,Missing
J17-1006,H05-2017,0,\N,Missing
J17-1006,P07-1034,0,\N,Missing
J17-1006,E14-3007,0,\N,Missing
J17-1006,C98-1013,0,\N,Missing
J17-1006,J95-2003,0,\N,Missing
J17-1006,P09-2079,0,\N,Missing
J17-1006,P07-1062,0,\N,Missing
J17-1006,P07-1123,0,\N,Missing
J17-1006,D14-1083,0,\N,Missing
J17-1006,D09-1017,0,\N,Missing
J17-1006,W11-0311,0,\N,Missing
J17-1006,W14-2608,0,\N,Missing
J17-1006,E14-1040,0,\N,Missing
J17-1006,P08-1034,0,\N,Missing
J17-1006,J86-3001,0,\N,Missing
J17-1006,P14-1145,0,\N,Missing
J17-1006,P14-1002,0,\N,Missing
J17-1006,P15-2106,0,\N,Missing
J17-1006,P13-1174,0,\N,Missing
J17-1006,D13-1171,0,\N,Missing
J17-1006,maynard-greenwood-2014-cares,0,\N,Missing
J17-1006,C12-2031,0,\N,Missing
J17-1006,P14-1048,0,\N,Missing
J17-1006,N15-1071,0,\N,Missing
J17-1006,L16-1165,0,\N,Missing
J17-1006,D11-1120,0,\N,Missing
J17-1006,W06-3808,0,\N,Missing
J17-1006,C04-1121,0,\N,Missing
J17-1006,P07-1055,0,\N,Missing
J17-1006,P13-2142,0,\N,Missing
J17-1006,W14-6305,1,\N,Missing
J18-4006,D14-1124,0,0.0735886,"Missing"
J18-4006,P06-2042,0,0.015628,"Missing"
J18-4006,J08-1001,0,0.0623289,"Missing"
J18-4006,J17-1006,1,0.895954,"Missing"
J18-4006,D11-1120,0,0.0169394,"Missing"
J18-4006,J15-3002,0,0.0485541,"Missing"
J18-4006,P16-1165,0,0.0400509,"Missing"
J18-4006,P18-1052,0,0.038273,"Missing"
J18-4006,P15-2106,0,0.0226009,"uses only textual information. For example, user profiles like age, gender, and location can be used to enhance subjectivity detection (including sentiment and emotion) (Volkova, Coppersmith, and Van Durme 2014; Volkova and Bachrach 2016), vote predictions (Persing and Ng 2014), or language identification (Saloot et al. 2016). Also, information from the conversational thread structure (e.g., links between previous posts) or valuable external sources can serve as contextual constraints to better capture the sentiment or the figurative reading of an utterance (Mukherjee and Bhattacharyya 2012; Karoui et al. 2015; Wallace, Choe, and Charniak 2015)2 . Finally, the social network, like social relationships, can enable grouping users according to specific communities regarding the topics or the sentiments they share (Deitrick and Hu 2013; West et al. 2014). Besides social media processing, the interaction of contextual information derived from sentences, discourse, and other forms of linguistic and extra-linguistic information have shown their effectiveness in language technology in general (Taboada and Mann 2006; Webber, Egg, and Kordoni 2012). This shows that computational linguistics is 1 See Farzinda"
J18-4006,D09-1036,0,0.106432,"Missing"
J18-4006,C16-1214,0,0.031032,"Missing"
J18-4006,C12-1113,0,0.033088,"s performance over a baseline that uses only textual information. For example, user profiles like age, gender, and location can be used to enhance subjectivity detection (including sentiment and emotion) (Volkova, Coppersmith, and Van Durme 2014; Volkova and Bachrach 2016), vote predictions (Persing and Ng 2014), or language identification (Saloot et al. 2016). Also, information from the conversational thread structure (e.g., links between previous posts) or valuable external sources can serve as contextual constraints to better capture the sentiment or the figurative reading of an utterance (Mukherjee and Bhattacharyya 2012; Karoui et al. 2015; Wallace, Choe, and Charniak 2015)2 . Finally, the social network, like social relationships, can enable grouping users according to specific communities regarding the topics or the sentiments they share (Deitrick and Hu 2013; West et al. 2014). Besides social media processing, the interaction of contextual information derived from sentences, discourse, and other forms of linguistic and extra-linguistic information have shown their effectiveness in language technology in general (Taboada and Mann 2006; Webber, Egg, and Kordoni 2012). This shows that computational linguisti"
J18-4006,N16-1013,0,0.0526016,"Missing"
J18-4006,D14-1119,0,0.0151401,"ocation detection) (Aiello et al. 2013; Ghosh et al. 2015; Inkpen et al. 2015; Londhe, Srihari, and Gopalakrishnan 2016).1 Other research explores the interactions between content and extra-linguistic or extra-textual features, showing that combining linguistic data with network and/or user context improves performance over a baseline that uses only textual information. For example, user profiles like age, gender, and location can be used to enhance subjectivity detection (including sentiment and emotion) (Volkova, Coppersmith, and Van Durme 2014; Volkova and Bachrach 2016), vote predictions (Persing and Ng 2014), or language identification (Saloot et al. 2016). Also, information from the conversational thread structure (e.g., links between previous posts) or valuable external sources can serve as contextual constraints to better capture the sentiment or the figurative reading of an utterance (Mukherjee and Bhattacharyya 2012; Karoui et al. 2015; Wallace, Choe, and Charniak 2015)2 . Finally, the social network, like social relationships, can enable grouping users according to specific communities regarding the topics or the sentiments they share (Deitrick and Hu 2013; West et al. 2014). Besides social"
J18-4006,J14-4007,0,0.0527101,"Missing"
J18-4006,D13-1170,0,0.00265532,"llows the syntactic tree up to the main clause by combining pairs of sister nodes by means of a set of sentiment composition rules. In Example (1), sentiment calculation has first to deal with the composition good enough that softens the positivity of the evaluation, which in turn has to be composed with the negation (not) that makes the overall opinion negative. In Example (2), the sentence’s syntactic structure indicates that the atmosphere and the cuisine have both a positive evaluation. For more discussions on sentiment composition, the reader can refer to the Stanford Sentiment Treebank (Socher et al. 2013). The composition process assumes that the interpretation of a given word within a sentence is fixed or disambiguated before being combined, which makes it restrictive in that it “precludes nonlinguistic information to go into the computation of meaning” (Bunt 2001).3 Indeed, the meaning of a sentence is closely tied to the pragmatics of how language is used, and thus to the meaning of the words themselves, which can be assigned different possible readings in different situations (Pustejovsky 1995; Lenci 2006). Consider the problem of lexical ambiguity. For example, A sad movie expresses a sen"
J18-4006,P17-2103,0,0.0541065,"Missing"
J18-4006,C96-2162,0,0.242394,"Missing"
J18-4006,C14-1221,0,0.0673781,"Missing"
J18-4006,P16-1148,0,0.0254568,"Missing"
J18-4006,P14-1018,0,0.0499274,"Missing"
J18-4006,P15-1100,0,0.0555802,"Missing"
J18-4006,E17-2108,0,0.0560014,"Missing"
J18-4006,Q14-1024,0,0.0209316,"predictions (Persing and Ng 2014), or language identification (Saloot et al. 2016). Also, information from the conversational thread structure (e.g., links between previous posts) or valuable external sources can serve as contextual constraints to better capture the sentiment or the figurative reading of an utterance (Mukherjee and Bhattacharyya 2012; Karoui et al. 2015; Wallace, Choe, and Charniak 2015)2 . Finally, the social network, like social relationships, can enable grouping users according to specific communities regarding the topics or the sentiments they share (Deitrick and Hu 2013; West et al. 2014). Besides social media processing, the interaction of contextual information derived from sentences, discourse, and other forms of linguistic and extra-linguistic information have shown their effectiveness in language technology in general (Taboada and Mann 2006; Webber, Egg, and Kordoni 2012). This shows that computational linguistics is 1 See Farzindar and Inkpen (2017) for an overview of the main NLP approaches for social media. 2 See Benamara, Taboada, and Mathieu (2017) for a recent overview of context-based approaches to evaluative language processing. 664 Benamara, Inkpen, and Taboada S"
J18-4006,W15-4614,0,0.0273634,"Missing"
J18-4006,C16-1230,0,0.0559677,"Missing"
J18-4006,S15-2080,0,\N,Missing
J18-4006,J95-2003,0,\N,Missing
J18-4006,J86-3001,0,\N,Missing
J18-4006,W15-1527,1,\N,Missing
J18-4006,P14-1048,0,\N,Missing
J18-4006,W16-2801,0,\N,Missing
J18-4006,D17-1245,0,\N,Missing
keskes-etal-2012-clause,P07-1062,0,\N,Missing
S12-1018,W11-2023,1,0.505772,".co.uk/worldservice/ learningenglish). It contains 21 randomly selected dialogues, in which one agent (the customer) calls a service to book a room, a flight, a taxi, etc. Here is a typical fragment: π1 A: Northwind Airways, good morning. May I help you? π2 B: Yes, do you have any flights to Sydney next Tuesday? π3 A: Yes, there’s a flight at 16:45 and one at 18:00. π4 A: Economy, business class or first class ticket? π5 B: Economy, please. Our approach to preference acquisition exploits discourse structure and aims to study the impact of discourse for extracting and reasoning on preferences. Cadilhac et al. (2011) show how to compute automatically preference representations for a whole stretch of dialogue from the preference representations for elementary discourse units. Our annotation here concentrates on the commitments to pref107 erences expressed in elementary discourse units or EDU s. We analyze how the outcomes and the dependencies between them are linguistically expressed by performing, on each corpus, a two-level annotation. First, we perform a segmentation of the dialogue into EDUs. Second, we annotate preferences expressed by the EDUs. The examples above show the effects of segmentation. Eac"
S12-1018,C08-1031,0,0.0130598,"bsolute judgments towards objects or persons (positive, negative or neutral), while preferences concern relative judgments towards actions (preferring them or not over others). The following examples illustrate this: (a) The movie is not bad. (b) The scenario of the first season is better than the second one. (c) I would like to go to the cinema. Let’s go and see Madagascar 2. (a) expresses a direct positive opinion towards the movie but we do not know if this movie is the most preferred. (b) expresses a comparative opinion between two movies with respect to their shared features (scenarios) (Ganapathibhotla and Liu, 2008). If actions involving these movies (e.g. seeing them) are clear in the context, such a comparative opinion will imply a preference, ordering the first season scenario over the second. Finally, (c) expresses two preferences, one depending on the other. The first is that the speaker prefers to go to the cinema over other alternative actions; the second is, given that preference, that he wants to see Madagascar 2 over other possible movies. Reasoning about preferences is also distinct from reasoning about opinions. An agent’s preferences determine an order over outcomes that predicts how the age"
S15-1016,chiarcos-2014-towards,0,0.144573,"rs across languages, and how they differ in translation (Degand, 2009; Zufferey and Degand, 2014). We would like to contribute to that area of study by unifying and integrating the types of relations that markers can signal. Merging different discourse relation taxonomies involves, in our view, different steps, having to do with: (1) segmentation, (2) unifying the set of relations, (3) proposing possible signals, (4) unifying discourse structures, and (5) providing a language for merging annotations. We focus here on step (2). For proposals for steps (4) and (5), see Venant et al. (2013), and Chiarcos (2014), respectively. 2 Methodology Our first focus are the two theories that we are most familiar with, RST and SDRT. We next plan to find correspondences between our unified RST-SDRT hierarchy and the PDTB taxonomy. 148 The first step consists of grouping relations in toplevel classes. Our goal is to minimize the number of top-level classes and, at the same time, reduce the number at the fine-grained level, avoiding the proliferation of relations seen in the RST Discourse TreeBank (Carlson et al., 2003). Two main criteria were used in creating the hierarchy. First of all, the proposed hierarchy sh"
S15-1016,W11-0401,0,0.341748,"Missing"
S15-1016,P14-1048,0,0.0479369,"urces exist in Basque, Dutch, German, English, Portuguese and Spanish. Captured in a graph-based representation, with longdistance attachments, SDRT proposes relations between abstract objects using a relatively small set of relations. Corpora following SDRT exist in Arabic, French and English. Manually annotated resources have contributed to a number of applications, most notably discourse segmentation into elementary discourse units, identification of explicit and implicit relations for the purpose of discourse parsing, and development of end-to-end discourse parsers (Hernault et al., 2010; Feng and Hirst, 2014; Joty et al., 2015). These parsers have been successfully deployed in NLP applications including machine translation, sentiment analysis and automatic summarization (Thione et al., 2004; Heerschop et al., 2011; Hardmeier, 2013). Each approach has its own hierarchy of discourse relations, but relations tend to overlap or be re147 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 147–152, Denver, Colorado, June 4–5, 2015. lated in a few specific ways. We suggest there are four general ways of mapping relations across approaches: (1) Specializat"
S15-1016,J15-3002,0,0.050524,"Dutch, German, English, Portuguese and Spanish. Captured in a graph-based representation, with longdistance attachments, SDRT proposes relations between abstract objects using a relatively small set of relations. Corpora following SDRT exist in Arabic, French and English. Manually annotated resources have contributed to a number of applications, most notably discourse segmentation into elementary discourse units, identification of explicit and implicit relations for the purpose of discourse parsing, and development of end-to-end discourse parsers (Hernault et al., 2010; Feng and Hirst, 2014; Joty et al., 2015). These parsers have been successfully deployed in NLP applications including machine translation, sentiment analysis and automatic summarization (Thione et al., 2004; Heerschop et al., 2011; Hardmeier, 2013). Each approach has its own hierarchy of discourse relations, but relations tend to overlap or be re147 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 147–152, Denver, Colorado, June 4–5, 2015. lated in a few specific ways. We suggest there are four general ways of mapping relations across approaches: (1) Specialization, where a relatio"
S15-1016,prasad-etal-2008-penn,0,0.294481,"that works across languages is possible. This paper details a new taxonomy of relations organized in four top level-classes with a total of 26 relations. We propose a mapping between existing annotations and show that our taxonomy is robust across theories, and can be applied to multiple languages. 1 Motivation The annotation of discourse relations in language can be broadly characterized as falling under two main approaches: the lexically grounded approach and an approach that aims at complete discourse coverage. Perhaps the best example of the first approach is the Penn Discourse Treebank (Prasad et al., 2008). The annotation starts with specific lexical items, most of them conjunctions, and includes two arguments for each conjunction. This leads to partial discourse coverage, as there is no guarantee that the entire text is annotated, since parts of the text not related through a conjunction are excluded. On the positive side, such annotations tend to be reliable. PDTB-style annotations have been carried out in a variety of languages (Arabic, Chinese, Czech, Danish, Dutch, French, Hindi and Turkish), and in some cases the taxonomy of relations had to be modified, by adding or merging relations (Pr"
S15-1016,J14-4007,0,0.0233708,"8). The annotation starts with specific lexical items, most of them conjunctions, and includes two arguments for each conjunction. This leads to partial discourse coverage, as there is no guarantee that the entire text is annotated, since parts of the text not related through a conjunction are excluded. On the positive side, such annotations tend to be reliable. PDTB-style annotations have been carried out in a variety of languages (Arabic, Chinese, Czech, Danish, Dutch, French, Hindi and Turkish), and in some cases the taxonomy of relations had to be modified, by adding or merging relations (Prasad et al., 2014). Complete discourse coverage requires annotation of the entire text, with most of the propositions in the text integrated in a structure. It includes work from two theoretical perspectives, either intentionally driven, such as RST (Mann and Thompson, 1988) or semantically driven, such as SDRT (Asher and Lascarides, 2003). RST proposes a tree-based representation, with relations between adjacent segments, and emphasizes a differential status for discourse components (the nucleus vs. satellite distinction). Annotated resources exist in Basque, Dutch, German, English, Portuguese and Spanish. Cap"
S15-1016,W04-1009,0,0.331425,"Missing"
S15-1016,W13-4002,0,0.0185152,"the use of discourse markers across languages, and how they differ in translation (Degand, 2009; Zufferey and Degand, 2014). We would like to contribute to that area of study by unifying and integrating the types of relations that markers can signal. Merging different discourse relation taxonomies involves, in our view, different steps, having to do with: (1) segmentation, (2) unifying the set of relations, (3) proposing possible signals, (4) unifying discourse structures, and (5) providing a language for merging annotations. We focus here on step (2). For proposals for steps (4) and (5), see Venant et al. (2013), and Chiarcos (2014), respectively. 2 Methodology Our first focus are the two theories that we are most familiar with, RST and SDRT. We next plan to find correspondences between our unified RST-SDRT hierarchy and the PDTB taxonomy. 148 The first step consists of grouping relations in toplevel classes. Our goal is to minimize the number of top-level classes and, at the same time, reduce the number at the fine-grained level, avoiding the proliferation of relations seen in the RST Discourse TreeBank (Carlson et al., 2003). Two main criteria were used in creating the hierarchy. First of all, the"
S15-1016,W01-1605,0,\N,Missing
S15-1016,afantenos-etal-2012-empirical,1,\N,Missing
S15-1016,L12-1000,0,\N,Missing
W04-0202,C04-1170,1,0.886212,"Missing"
W04-0202,miltsakaki-etal-2004-penn,0,0.0738972,"Missing"
W04-0202,P02-1006,0,0.0237204,"yms, sisters, subtypes. 3.4.3 Linguistic marks In this section, for space reasons, we explore only three typical CR: justifications (AJ), restrictions (AR) and warnings (AA). These MUs are characterized by markers which are general terms, domain independent for most of them. The study of these marks for French reveals that there is little marker overlap between units. Markers have been defined in a first stage from corpus analysis and then generalized to similar terms in order to have a larger basis for evaluation. We also used, to a limited extend, a bootstrapping technique to get more data (Ravinchandran and Hovy 2002), a method that starts by an unambiguous set of anchors (often arguments of a relational term) for a target sense. Searching text fragments on the Web based on these anchors then produces a number of ways of relating these anchors. Let us now characterize linguistic markers for each of these categories: Restrictions (AR) are an important unit in cooperative discourse. There is a quite large literature in linguistics about the expression of restrictions. In cooperative discourse, the expression of restrictions is realized quite straightforwardly by a small number of classes of terms: (a) restri"
W04-0202,C82-1066,0,\N,Missing
W04-0202,W03-2120,0,\N,Missing
W04-0202,baumann-etal-2004-muli,0,\N,Missing
W04-0202,W03-2301,1,\N,Missing
W04-0506,W01-0906,0,0.0184691,"ect to more cases of misunderstandings than others, depending, e.g. on the complexity of their associated knowledge and on the type of services expected by users. Similarly, the inference procedures used in WEBCOOP have been designed with a certain level of genericity. They should be portable provided that the knowledge resources of the new domain can be implemented using WEBCOOP format, which is quite generic. But, besides QA annotations, which is a very useful perspective, the adequacy of inferences can only be evaluated a posteriori. In a future stage, we plan to use what Barr and Klavans (Barr and Klavans, 2001) call component performance evaluation which consists of assessing the performance of system components and determining their impact on the overall system performance. 4.2 Evaluating Response intelligibility Finally, since WEBCOOP produces responses in NL, some of which on a template basis (different from TREC which simply reproduces text extracts), it is important to evaluate the portability of those templates. We propose a method based on experimental psychology, that aims at evaluating the cooperative responses generated in the know-how component of WEBCOOP. Our methodology involves the fol"
W04-0506,N03-1022,0,0.0575547,"in order to provide, for example, better answer ranking, answer justiﬁcation, responses to unanticipated questions or to resolve situations in which no answer is found in the data sources. Cooperative answering systems are typically designed to deal with such situations, by providing non-misleading, and useful answers to a query. (Grice, 1975) maxims of conversation namely the quality, quantity, relation and style maxims are frequently used as a basis for designing cooperative answering systems. An overview of cooperative answering techniques is given in (Gaasterland et al., 1994). In COGEX (Moldovan et al., 2003), a recent QA system, authors used automated reasoning for QA and showed that it is feasible, eﬀective and scalable. This logical prover aims at checking and extracting all kinds of lexical relationships between the question and its candidate answers using world knowledge axioms, supplied by WordNet glosses, as well as rewriting rules representing equivalent classes of linguistic patterns. Such inference techniques (e.g. lexical equivalence, uniﬁcation on logical representations of texts) are not suﬃcient for providing intelligent or cooperative responses. Indeed, advanced strategies for QA re"
W10-3309,afantenos-etal-2010-learning,0,0.0217875,"Missing"
W10-3309,W06-0301,0,0.0339826,"Missing"
W10-3309,W02-1011,0,0.0154685,"s (which are not exclusive): • Development of linguistic and cognitive models of opinion/sentiment where already existing psycholinguistic theories of emotions are used to analyse how opinions are lexically expressed in texts (Wiebe et al, 2005; Read et al, 2007; Asher et al, 2009) • Elaboration of linguistic resources where corpus based and dictionary based approaches are used to automatically or semiautomatically extract opinion bearing terms/expressions as well as their sentiment orientation (Strapparava et al., 2004; Turney and Littman, 2002) • Opinion extraction/analysis at the document (Pang et al., 2002; Turney, 2002), at the sentence or at the clause level (Kim et al., 2006; Choi et al., 2005) where local Introduction Opinion mining is a growing research area both in natural language processing and information retrieval communities. Companies, politicians, as well as customers need powerful tools to track opinions, sentiments, judgments and beliefs that people may express in blogs, reviews, audios and videos data regarding a product/service/person/organisation/etc. The importance of emotion-oriented computing in the 77 Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ont"
W10-3309,H05-1043,0,0.440792,"g and information retrieval communities. Companies, politicians, as well as customers need powerful tools to track opinions, sentiments, judgments and beliefs that people may express in blogs, reviews, audios and videos data regarding a product/service/person/organisation/etc. The importance of emotion-oriented computing in the 77 Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 77–86, Beijing, August 2010 opinions are aggregated in order to compute the overall orientation of a document/sentence/clause. • Feature based opinion mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Carenini et al., 2005; Cheng and Xu, 2008) where opinions expressed towards the features of an object or a product are exacted and summarized. The work described in this paper feats into the last category. The aim is not to compute the general orientation of a document or a sentence, since a positive sentiment towards an object does not imply a positive sentiment towards all the aspects of this object, as in: I like this restaurant even if the service is slow. In feature based opinion mining, a holder (the person who posts the review) expresses a positive/negative or neutral opinions towards"
W10-3309,strapparava-valitutti-2004-wordnet,0,0.121948,"Missing"
W10-3309,P02-1053,0,0.00265104,"clusive): • Development of linguistic and cognitive models of opinion/sentiment where already existing psycholinguistic theories of emotions are used to analyse how opinions are lexically expressed in texts (Wiebe et al, 2005; Read et al, 2007; Asher et al, 2009) • Elaboration of linguistic resources where corpus based and dictionary based approaches are used to automatically or semiautomatically extract opinion bearing terms/expressions as well as their sentiment orientation (Strapparava et al., 2004; Turney and Littman, 2002) • Opinion extraction/analysis at the document (Pang et al., 2002; Turney, 2002), at the sentence or at the clause level (Kim et al., 2006; Choi et al., 2005) where local Introduction Opinion mining is a growing research area both in natural language processing and information retrieval communities. Companies, politicians, as well as customers need powerful tools to track opinions, sentiments, judgments and beliefs that people may express in blogs, reviews, audios and videos data regarding a product/service/person/organisation/etc. The importance of emotion-oriented computing in the 77 Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pag"
W11-2023,W05-0613,1,0.901401,"Missing"
W11-2023,J98-4001,0,0.11267,"ed purely on Gricean cooperative principles (Grice, 1975). On a purely Gricean approach, conversation is cooperative in at least two ways: a basic level concerning the conventions that govern linguistic meaning (basic cooperativity); and a level concerning shared attitudes towards what is said, including shared intentions (content cooperativity). While basic cooperation is needed for communication to work at all, content cooperativity involves strongly cooperative axioms like Cooperativity (interlocutors normally adopt the speaker’s intentions) (Allen and Litman, 1987, Grosz and Sidner, 1990, Lochbaum, 1998). Our approach allows for divergent preferences and divergent intentions, i.e. conversations that aren’t based on content cooperativity. This will allow us to exploit information about conflicting agents’ preferences and game-theoretic techniques that are inherent in the logics of CP-nets for computing optimal moves (Bonzon, 2007). And in contrast to Franke et al. (2009), who analyse conversations where content cooperativity doesn’t hold using a game-theoretic framework, our approach allows for partial and qualitative representations of preferences rather than demanding complete and quantitati"
W12-3619,S12-1018,1,0.697462,"and Foo, 2004). Modeling preferences divides into three subtasks (Brafman and Domshlak, 2009): preference acquisition, which extracts preferences from users, preference modeling where a model of users’ preferences is built using a preference representation language and preference reasoning which aims at computing the set of optimal outcomes. We focus in this paper on a particular instantiation of the first task, extracting preferences from chat turns of actual conversation; and we propose an annotation scheme that is general enough to cover several domains. We extend the annotation scheme of (Cadilhac et al., 2012), which investigates preferences within negotiation dialogues with a common goal like fixing a meeting time (Verbmobil (CV )) or making a hotel or plane reservation (Booking (CB )) to a more complex domain provided by a corpus of on line chats concerning the game Settlers of Catan. In Settlers, players with opposing strategic Preferences in game theory A preference is traditionally a complete ordering by an agent over outcomes. In traditional game theory (Osborne and Rubinstein, 1994), preferences or utilities over outcomes drive rational, strategic decision. They are the terminal states of th"
W12-3802,abeille-godard-2010-grande,0,0.114657,"Missing"
W12-3802,I11-1132,1,0.901974,"Missing"
W12-3802,D08-1083,0,0.635539,"g with their direct translation in English. Note however that there are substantial semantic differences between the two languages. 2 Related Work 2.1 Negation in Sentiment Analysis Research efforts using negation in sentiment analysis can be grouped according to three main criteria: the effect of negation on opinion expressions, the types of negation used and the method employed 11 to update the prior polarity of opinion expressions. According to the first criterion, most approaches treat negation as polarity reversal (Polanyi and Zaenen, 2006; Wilson et al., 2005; Moilanen and Pulman, 2007; Choi and Cardie, 2008). However, negation cannot be reduced to reversing polarity. For example, if we assume that the score of the adjective “excellent” is +3, then the opinion score in “this student is not excellent” cannot be -3. It rather means that the student is not good enough. Hence, dealing with negation requires to go beyond polarity reversal. Liu and Seneff (2009) propose a linear additive model that treats negations as modifying adverbs. In the same way, in (Taboada et al., 2011), the negation of an opinion expression shifts the value of its score to the opposite polarity by a fixed amount. Thus a +2 adj"
W12-3802,P09-2044,0,0.0126464,"ions. In NLP, modality is less addressed than other linguistic operators, such as negations. Most of the computational studies involving modality are focused on: (i) building annotated resources in terms of factuality information and (ii) uncertainty modeling and hedge detection in texts. Among annotated resources, we cite the FactBank corpus (Saur´ı and Pustejovsky, 2009) and the BioScope corpus (Vincze et al., 2008). In the second research strand, the efforts go from detecting uncertainty in texts (Rubin, 2010), to finding hedges and their scopes in specialized corpora (Vincze et al., 2008; Ganter and Strube, 2009; Zhao et al., 2010). However, there is only partial overlapping between hedges and modal constructions. Hedges are linguistic means whereby the authors show that they cannot back their opinions with facts. Thus, hedges include certain modal constructions (especially epistemic), along with other markers such as indirect speech, e.g., “According to certain researchers,...”. On the 12 other hand, there are modal constructions which are not hedges, e.g. when expressing a factual possibility, without uncertainty on behalf of the speaker, e.g. may in “These insects may play a part in the reproducti"
W12-3802,N09-1057,0,0.0317149,"r, 2009). We thus distinguish three types of negation: negative operators, negative quantifiers and lexical negations and three types of modality: buletic, epistemic and deontic. We show that each type has a specific effect on the opinion expression in its scope: both on the polarity and the strength for negation, and on the strength and/or the degree of certainty for modality. These effects are structured as a set of hypotheses that we empirically validated via several linguistic experiments informed by native speakers. This evaluation methodology has already been used in sentiment analysis. Greene and Resnik (2009) chose psycholinguistic methods for assessing the connection between sentence structure and implicit sentiment. Taboada et al. (2011) used Mechanical Turk to check subjective dictionaries for consistency. The empirical results reported in this paper provide a basis for future opinion analysis systems that have to compute the sentiment orientation at the sentence or at the clause level. The methodology we used for deriving this basis was applied for French but it can be easily instantiated for other languages like English. In this paper, all examples are in French along with their direct transl"
W12-3802,D09-1017,0,0.405131,"sed and the method employed 11 to update the prior polarity of opinion expressions. According to the first criterion, most approaches treat negation as polarity reversal (Polanyi and Zaenen, 2006; Wilson et al., 2005; Moilanen and Pulman, 2007; Choi and Cardie, 2008). However, negation cannot be reduced to reversing polarity. For example, if we assume that the score of the adjective “excellent” is +3, then the opinion score in “this student is not excellent” cannot be -3. It rather means that the student is not good enough. Hence, dealing with negation requires to go beyond polarity reversal. Liu and Seneff (2009) propose a linear additive model that treats negations as modifying adverbs. In the same way, in (Taboada et al., 2011), the negation of an opinion expression shifts the value of its score to the opposite polarity by a fixed amount. Thus a +2 adjective is negated to a -2, but the negation of a very negative adjective is only slightly positive. Based on (Taboada et al., 2011)’s shift model, Yessenalina and Cardie (2011) propose to represent each word as a matrix and combine words using iterated matrix multiplication, which allows for modeling both additive (for negations) and multiplicative (fo"
W12-3802,W06-3907,0,0.0882226,"Missing"
W12-3802,J11-2001,0,0.732935,"modality: buletic, epistemic and deontic. We show that each type has a specific effect on the opinion expression in its scope: both on the polarity and the strength for negation, and on the strength and/or the degree of certainty for modality. These effects are structured as a set of hypotheses that we empirically validated via several linguistic experiments informed by native speakers. This evaluation methodology has already been used in sentiment analysis. Greene and Resnik (2009) chose psycholinguistic methods for assessing the connection between sentence structure and implicit sentiment. Taboada et al. (2011) used Mechanical Turk to check subjective dictionaries for consistency. The empirical results reported in this paper provide a basis for future opinion analysis systems that have to compute the sentiment orientation at the sentence or at the clause level. The methodology we used for deriving this basis was applied for French but it can be easily instantiated for other languages like English. In this paper, all examples are in French along with their direct translation in English. Note however that there are substantial semantic differences between the two languages. 2 Related Work 2.1 Negation"
W12-3802,W10-3111,0,0.567847,"can also express modality (e.g. ”a probable cause”). Negation and modality can aggregate in a variety of ways: (1) multiple negatives, e.g, “This restaurant never fails to disappoint on flavor”. In some languages, double negatives cancel the effect of negation, while in negative-concord languages like French, double negations usually intensify the effect of negation. (2) cumulative modalities, as in “You definitely must see this movie” and (3) both negation and modality, as in “you should not go see this movie”. Several reports have shown that negations and modalities are sentiment-relevant (Wiegand et al., 2010). Kennedy and Inkpen (2006) point out that 10 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 10–18, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics negations are more sentiment-relevant than diminishers. Wilson et al. (2009) show that modalities as well as negations are good cues for opinion identification. Given that the sentiment-relevance of negations and modalities is an established fact, this paper aims to go further by exploring how this relevance is distilled accordi"
W12-3802,H05-1044,0,0.0697067,". In this paper, all examples are in French along with their direct translation in English. Note however that there are substantial semantic differences between the two languages. 2 Related Work 2.1 Negation in Sentiment Analysis Research efforts using negation in sentiment analysis can be grouped according to three main criteria: the effect of negation on opinion expressions, the types of negation used and the method employed 11 to update the prior polarity of opinion expressions. According to the first criterion, most approaches treat negation as polarity reversal (Polanyi and Zaenen, 2006; Wilson et al., 2005; Moilanen and Pulman, 2007; Choi and Cardie, 2008). However, negation cannot be reduced to reversing polarity. For example, if we assume that the score of the adjective “excellent” is +3, then the opinion score in “this student is not excellent” cannot be -3. It rather means that the student is not good enough. Hence, dealing with negation requires to go beyond polarity reversal. Liu and Seneff (2009) propose a linear additive model that treats negations as modifying adverbs. In the same way, in (Taboada et al., 2011), the negation of an opinion expression shifts the value of its score to the"
W12-3802,J09-3003,0,0.471032,"the effect of negation. (2) cumulative modalities, as in “You definitely must see this movie” and (3) both negation and modality, as in “you should not go see this movie”. Several reports have shown that negations and modalities are sentiment-relevant (Wiegand et al., 2010). Kennedy and Inkpen (2006) point out that 10 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 10–18, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics negations are more sentiment-relevant than diminishers. Wilson et al. (2009) show that modalities as well as negations are good cues for opinion identification. Given that the sentiment-relevance of negations and modalities is an established fact, this paper aims to go further by exploring how this relevance is distilled according to the semantics of each operator. To this end, we first study several taxonomies along with their associated categories of both modality and negation given by the linguistic literature. Among these categories, we decide to choose the categories of (Godard, to appear) for negations. For modalities, we rely on the categories of (Larreya, 2004"
W12-3802,D11-1016,0,0.0534985,"opinion score in “this student is not excellent” cannot be -3. It rather means that the student is not good enough. Hence, dealing with negation requires to go beyond polarity reversal. Liu and Seneff (2009) propose a linear additive model that treats negations as modifying adverbs. In the same way, in (Taboada et al., 2011), the negation of an opinion expression shifts the value of its score to the opposite polarity by a fixed amount. Thus a +2 adjective is negated to a -2, but the negation of a very negative adjective is only slightly positive. Based on (Taboada et al., 2011)’s shift model, Yessenalina and Cardie (2011) propose to represent each word as a matrix and combine words using iterated matrix multiplication, which allows for modeling both additive (for negations) and multiplicative (for intensifiers) semantic effects. In our framework, we assume, as in (Liu and Seneff, 2009) and (Taboada et al., 2011), that negation affects both the polarity and the strength of an opinion expression. However, unlike other studies, we distill that effect depending on the type of the negation. Two main types of negation were studied in the literature: negators such as “not” and content word negators such as “eliminate"
W12-3802,W10-3014,0,0.0121327,"less addressed than other linguistic operators, such as negations. Most of the computational studies involving modality are focused on: (i) building annotated resources in terms of factuality information and (ii) uncertainty modeling and hedge detection in texts. Among annotated resources, we cite the FactBank corpus (Saur´ı and Pustejovsky, 2009) and the BioScope corpus (Vincze et al., 2008). In the second research strand, the efforts go from detecting uncertainty in texts (Rubin, 2010), to finding hedges and their scopes in specialized corpora (Vincze et al., 2008; Ganter and Strube, 2009; Zhao et al., 2010). However, there is only partial overlapping between hedges and modal constructions. Hedges are linguistic means whereby the authors show that they cannot back their opinions with facts. Thus, hedges include certain modal constructions (especially epistemic), along with other markers such as indirect speech, e.g., “According to certain researchers,...”. On the 12 other hand, there are modal constructions which are not hedges, e.g. when expressing a factual possibility, without uncertainty on behalf of the speaker, e.g. may in “These insects may play a part in the reproduction of plants as well"
W12-3802,W08-0606,0,\N,Missing
W13-0105,abeille-godard-2010-grande,0,0.0434285,"Missing"
W13-0105,W12-3802,1,0.93671,"The vertices of the graph are the opinion targets, opinion expressions and modifiers of opinion and the edges represent relations among them (mainly, opinion restriction and opinion expansion). Finally (Socher et al., 2012) propose a matrix-vector representations with a recursive neural network. The model is build on a parse tree where the nodes are associated to a vector. The matrix captures how each constituent modifies its neighbour. The model was applied to predict fine-grained sentiment distributions of adverb-adjective pairs. Based on linguistic experiments informed by native speakers (Benamara et al., 2012), we propose a sentiment composition model based on a parabolic representation where an opinion expression is represented as a point on a parabola. Our model is designed to handle the interactions between opinion expressions and specific linguistic operators at the sub-sentential level. This paper focus particularly on modality and negation but our model can be used to treat intensifier as well. Within the model, negation are modelled as functions over this parabola whereas modality through a family of parabolas of different slopes; each slope corresponds to a different certainty degree. The m"
W13-0105,D08-1083,0,0.184212,"are also available. Among them, we can cite the BioScope corpus (Vincze et al., 2008) and FactBank (Saur´ı and Pustejovsky, 2009). In sentiment analysis, the presence of modalities is generally used as a feature in a supervised learning setting for sentence-level opinion classification (Kobayakawa et al., 2009). However, to our knowledge, no work has investigated how modality impacts on opinions. There are two ways of treating negation when computing the contextual polarity an opinion expression at the sentense-level: (a) polarity reversal (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Choi and Cardie, 2008) that flips the prior polarity of the expression to its opposite value. For instance, if the score of the adjective “excellent” is +3, then the opinion in “this student is not excellent” is -3 ; (b) polarity shift (Taboada et al., 2011) that assumes that negation affects both the polarity and the strength. For instance, the opinion in “this student is not excellent” cannot be -3 ; it rather means that the student is not good enough. Two main types of negation were taken into account in these models: negators such as “not” and / or content word negators (Choi and Cardie, 2008) that can be posit"
W13-0105,P08-1118,0,0.0591452,"Missing"
W13-0105,D12-1110,0,0.073364,"e model is learned in order to assign ordinal sentiment scores to sentiment-bearing phrases. (Socher et al., 2011) model sentences in a vectorial representation and propose an approach based on semi-supervised recursive autoencoders in order to predict sentence-level sentiment distributions. (Wu et al., 2011) propose a graph-based method for computing a sentence-level sentiment representation. The vertices of the graph are the opinion targets, opinion expressions and modifiers of opinion and the edges represent relations among them (mainly, opinion restriction and opinion expansion). Finally (Socher et al., 2012) propose a matrix-vector representations with a recursive neural network. The model is build on a parse tree where the nodes are associated to a vector. The matrix captures how each constituent modifies its neighbour. The model was applied to predict fine-grained sentiment distributions of adverb-adjective pairs. Based on linguistic experiments informed by native speakers (Benamara et al., 2012), we propose a sentiment composition model based on a parabolic representation where an opinion expression is represented as a point on a parabola. Our model is designed to handle the interactions betwe"
W13-0105,D11-1014,0,0.142223,"dels: sentiment propagation, polarity conflict resolution and polarity reversal. (Shaikh et al., 2007) use verb frames representation for sentence-level classification and show that their compositional model outperfoms a non-compositional rule-based system. (Yessenalina and Cardie, 2011) represent each word as a matrix and combine words using iterated matrix multiplication, which allows for modelling both additive (for negations) and multiplicative (for intensifiers) semantic effects. This matrix-space model is learned in order to assign ordinal sentiment scores to sentiment-bearing phrases. (Socher et al., 2011) model sentences in a vectorial representation and propose an approach based on semi-supervised recursive autoencoders in order to predict sentence-level sentiment distributions. (Wu et al., 2011) propose a graph-based method for computing a sentence-level sentiment representation. The vertices of the graph are the opinion targets, opinion expressions and modifiers of opinion and the edges represent relations among them (mainly, opinion restriction and opinion expansion). Finally (Socher et al., 2012) propose a matrix-vector representations with a recursive neural network. The model is build o"
W13-0105,P08-1033,0,0.0120206,"egation and modality. We then give in section 3 the linguistic motivations behind our approach. The parabolic model and its evaluation are respectively described in section 4 and section 5. 2 Related Works The computational treatment of negation and modality has recently become an emerging research area. These complex linguistic phenomena have been shown to be relevant in several NLP applications such as sentiment analysis (Wiegand et al., 2010), information retrieval (Jia and Meng, 2009), recognizing contrasts and contradictions (de Marneffe and Manning, 2008) and biomedical text processing (Szarvas, 2008). Due to the emergence of this field, several workshops and conferences have been organized such as the Negation and Speculation in Natural Language Processing (NeSp-NLP 2010) workshop, the Extra-Propositional Aspects of Meaning in Computational Linguistics (ExPRom 2012) workshop, and the publication of a special issue of the journal Computational Linguistics. A number of resources annotated with factuality information are also available. Among them, we can cite the BioScope corpus (Vincze et al., 2008) and FactBank (Saur´ı and Pustejovsky, 2009). In sentiment analysis, the presence of modalit"
W13-0105,J11-2001,0,0.740384,"setting for sentence-level opinion classification (Kobayakawa et al., 2009). However, to our knowledge, no work has investigated how modality impacts on opinions. There are two ways of treating negation when computing the contextual polarity an opinion expression at the sentense-level: (a) polarity reversal (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Choi and Cardie, 2008) that flips the prior polarity of the expression to its opposite value. For instance, if the score of the adjective “excellent” is +3, then the opinion in “this student is not excellent” is -3 ; (b) polarity shift (Taboada et al., 2011) that assumes that negation affects both the polarity and the strength. For instance, the opinion in “this student is not excellent” cannot be -3 ; it rather means that the student is not good enough. Two main types of negation were taken into account in these models: negators such as “not” and / or content word negators (Choi and Cardie, 2008) that can be positive polarity shifters (like abate) or negative polarity shifters (like lack). Few studies take into account other types of negation. (Taboada et al., 2011) treat negative polarity items (NPIs) (as well as modalities) as “irrealis blocke"
W13-0105,W10-3111,0,0.0389371,"representation of extra-propositional aspects of meaning. The paper is organized as follow. We first give an overview of how existing sentiment analysis systems deal with negation and modality. We then give in section 3 the linguistic motivations behind our approach. The parabolic model and its evaluation are respectively described in section 4 and section 5. 2 Related Works The computational treatment of negation and modality has recently become an emerging research area. These complex linguistic phenomena have been shown to be relevant in several NLP applications such as sentiment analysis (Wiegand et al., 2010), information retrieval (Jia and Meng, 2009), recognizing contrasts and contradictions (de Marneffe and Manning, 2008) and biomedical text processing (Szarvas, 2008). Due to the emergence of this field, several workshops and conferences have been organized such as the Negation and Speculation in Natural Language Processing (NeSp-NLP 2010) workshop, the Extra-Propositional Aspects of Meaning in Computational Linguistics (ExPRom 2012) workshop, and the publication of a special issue of the journal Computational Linguistics. A number of resources annotated with factuality information are also ava"
W13-0105,D11-1123,0,0.0682253,"l model outperfoms a non-compositional rule-based system. (Yessenalina and Cardie, 2011) represent each word as a matrix and combine words using iterated matrix multiplication, which allows for modelling both additive (for negations) and multiplicative (for intensifiers) semantic effects. This matrix-space model is learned in order to assign ordinal sentiment scores to sentiment-bearing phrases. (Socher et al., 2011) model sentences in a vectorial representation and propose an approach based on semi-supervised recursive autoencoders in order to predict sentence-level sentiment distributions. (Wu et al., 2011) propose a graph-based method for computing a sentence-level sentiment representation. The vertices of the graph are the opinion targets, opinion expressions and modifiers of opinion and the edges represent relations among them (mainly, opinion restriction and opinion expansion). Finally (Socher et al., 2012) propose a matrix-vector representations with a recursive neural network. The model is build on a parse tree where the nodes are associated to a vector. The matrix captures how each constituent modifies its neighbour. The model was applied to predict fine-grained sentiment distributions of"
W13-0105,D11-1016,0,0.045755,"he verb confirm, the adjective good and the adverbs not and enough. Several computational models were proposed to account for sentiment composition. (Moilanen and Pulman, 2007) use a syntactic tree representation where nodes are associated to a set of specific handmade composition rules that treat both negation and intensifier via three models: sentiment propagation, polarity conflict resolution and polarity reversal. (Shaikh et al., 2007) use verb frames representation for sentence-level classification and show that their compositional model outperfoms a non-compositional rule-based system. (Yessenalina and Cardie, 2011) represent each word as a matrix and combine words using iterated matrix multiplication, which allows for modelling both additive (for negations) and multiplicative (for intensifiers) semantic effects. This matrix-space model is learned in order to assign ordinal sentiment scores to sentiment-bearing phrases. (Socher et al., 2011) model sentences in a vectorial representation and propose an approach based on semi-supervised recursive autoencoders in order to predict sentence-level sentiment distributions. (Wu et al., 2011) propose a graph-based method for computing a sentence-level sentiment r"
W13-0105,W08-0606,0,\N,Missing
