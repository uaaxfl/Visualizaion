2020.trac-1.1,2020.trac-1.14,0,0.231789,"Missing"
2020.trac-1.1,2020.trac-1.15,0,0.0507202,"Missing"
2020.trac-1.1,2020.trac-1.12,0,0.0357812,"Missing"
2020.trac-1.1,S19-2007,0,0.414215,"l., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et al., 2019) and aggression (Kumar et al., 2018). These have motivated the creation of for various languages such as English, German, Hindi, Italian, Spanish, and others. In this paper, we discuss the results of the second iteration of the TRAC shared task, organized as part of the Workshop on Trolling, Aggression and Cyberbullying at LREC 2020. The task consisted of two sub-tasks - aggression identification and gendered aggression identification on YouTube comments in three languages: Bengali, Hindi and English. To the best of our knowledge, TRAC-2 is the first shared task to include YouTube comments as"
2020.trac-1.1,2020.trac-1.17,0,0.0557343,"Missing"
2020.trac-1.1,W18-4401,1,0.88136,"provided in each language for each sub-task. A total of 70 teams registered to participate in the task and 19 teams submitted their test runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages. Keywords: Aggression, Gendered Aggression, English, Hindi, Bengali, TRAC 1. Introduction 2. In recent years, there have been several studies exploring the computational modelling and automatic detection of abusive content in social media focusing on toxic comments1 , aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et"
2020.trac-1.1,2020.trac-1.18,0,0.295322,"Missing"
2020.trac-1.1,2020.trac-1.10,0,0.0770099,"Missing"
2020.trac-1.1,malmasi-zampieri-2017-detecting,1,0.860794,"(Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004), misogyny ((Menczer et al., 2015; Frenda et al., 2019; Hewitt et al., 2016; Fersini et al., 2018; Anzovino et al., 2018; Sharifirad and Matwin, 2019)), online aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018), and offensive language (Wiegand et al., 2018; Zampieri et al., 2019b). The terms used in the literature have overlapping properties as discussed in Waseem et al. (2017) and Zampieri et al. (2019a). The most important differences concern their target (e.g. hate speech is typically targeted at groups whereas cyberbulling targets individuals), which is represented in TRAC-2 Task B, and types (e.g. veiled or direct abuse), represented in TRAC-2 Task A. Most related studies focus on English, but significant amount of work has been carried out for other languages too."
2020.trac-1.1,K15-1032,0,0.029561,"s. Section 2. discusses related studies and shared tasks to TRAC2. Section 3. presents the setup and schedule of TRAC-2 and Section 4. presents the dataset used in the competition. Section 5. presents the approaches used by participants of the competition and Section 6. presents and analyzes the results they obtained. Finally, 7. concludes this paper and presents avenues for future work. 1 Related Work Automatically identifying the various forms of abusive language online has been studied from different angles. Examples include trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004), misogyny ((Menczer et al., 2015; Frenda et al., 2019; Hewitt et al., 2016; Fersini et al., 2018; Anzovino et al., 2018; Sharifirad and Matwin, 2019)), online aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018), and offensive language"
2020.trac-1.1,2020.trac-1.19,0,0.0543347,"Missing"
2020.trac-1.1,2020.trac-1.11,0,0.0381996,"Missing"
2020.trac-1.1,2020.lrec-1.629,1,0.848565,"). The terms used in the literature have overlapping properties as discussed in Waseem et al. (2017) and Zampieri et al. (2019a). The most important differences concern their target (e.g. hate speech is typically targeted at groups whereas cyberbulling targets individuals), which is represented in TRAC-2 Task B, and types (e.g. veiled or direct abuse), represented in TRAC-2 Task A. Most related studies focus on English, but significant amount of work has been carried out for other languages too. This includes languages such as Arabic (Mubarak et al., 2020), German (Struß et al., 2019), Greek (Pitenis et al., 2020), Hindi (Mandl et al., 2019), and Spanish (Basile et al., 2019). TRAC - 2 is the second iteration of the TRAC shared task on Aggression Identification (Kumar et al., 2018) hosted at the TRAC workshop at COLING 2018. The first edition of TRAC included English and Hindi data from Facebook and Twitter. It consisted of a three-way classification task with posts labelled as overtly aggressive, covertly aggressive, and non-aggressive. TRAC received 30 submissions and the results obtained by participants suggested that neural network-based systems and machine learning classifiers http://bit.ly/2FhLMV"
2020.trac-1.1,2020.trac-1.9,0,0.502008,"Missing"
2020.trac-1.1,2020.trac-1.20,0,0.330248,"Missing"
2020.trac-1.1,2020.trac-1.16,0,0.0574059,"Missing"
2020.trac-1.1,W17-3012,0,0.0377661,"sm (Greevy and Smeaton, 2004; Greevy, 2004), misogyny ((Menczer et al., 2015; Frenda et al., 2019; Hewitt et al., 2016; Fersini et al., 2018; Anzovino et al., 2018; Sharifirad and Matwin, 2019)), online aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018), and offensive language (Wiegand et al., 2018; Zampieri et al., 2019b). The terms used in the literature have overlapping properties as discussed in Waseem et al. (2017) and Zampieri et al. (2019a). The most important differences concern their target (e.g. hate speech is typically targeted at groups whereas cyberbulling targets individuals), which is represented in TRAC-2 Task B, and types (e.g. veiled or direct abuse), represented in TRAC-2 Task A. Most related studies focus on English, but significant amount of work has been carried out for other languages too. This includes languages such as Arabic (Mubarak et al., 2020), German (Struß et al., 2019), Greek (Pitenis et al., 2020), Hindi (Mandl et al., 2019), and Spanish (Basile et al., 2019). TRAC - 2 is th"
2020.trac-1.1,N12-1084,0,0.623973,"ub-task. A total of 70 teams registered to participate in the task and 19 teams submitted their test runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages. Keywords: Aggression, Gendered Aggression, English, Hindi, Bengali, TRAC 1. Introduction 2. In recent years, there have been several studies exploring the computational modelling and automatic detection of abusive content in social media focusing on toxic comments1 , aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et al., 2019) and aggression (Kuma"
2020.trac-1.1,N19-1144,1,0.919061,"t runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages. Keywords: Aggression, Gendered Aggression, English, Hindi, Bengali, TRAC 1. Introduction 2. In recent years, there have been several studies exploring the computational modelling and automatic detection of abusive content in social media focusing on toxic comments1 , aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et al., 2019) and aggression (Kumar et al., 2018). These have motivated the creation of for various languages such as English, German, Hind"
2020.trac-1.1,S19-2010,1,0.879571,"t runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages. Keywords: Aggression, Gendered Aggression, English, Hindi, Bengali, TRAC 1. Introduction 2. In recent years, there have been several studies exploring the computational modelling and automatic detection of abusive content in social media focusing on toxic comments1 , aggression (Kumar et al., 2018), cyberbullying (Xu et al., 2012; Dadvar et al., 2013), hate speech (Davidson et al., 2017), and offensive content (Zampieri et al., 2019a) to name a few. Prior studies have tackled abusive language identification in content from different platforms such as Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments1 , and Facebook (Kumar et al., 2018). A number of shared tasks been organized focusing on the automatic detection of offensive language (Struß et al., 2019; Zampieri et al., 2019b; Mandl et al., 2019), hate speech (Basile et al., 2019) and aggression (Kumar et al., 2018). These have motivated the creation of for various languages such as English, German, Hind"
2021.naacl-main.118,P19-1524,0,0.113926,"ntial for overcoming these issues, and critical in the absence of casing. Even a human may not correctly parse “what is [[life is beautiful]]?” without knowing that a movie is being referenced. However, most models start with no knowledge of real world entities, learning them from the training data. Continuous data annotation can add new entities, but is expensive and often not feasible. Consequently, methods for integrating external knowledge, e.g., Knowledge Bases (KBs) or gazetteers, into neural architectures have gained renewed attention. However, such studies have reported limited gains (Liu et al., 2019; Rijhwani et al., 2020). The mixed success of gazetteers stems from three main limitations in current work: gazetteer feature representation, their integration with contextual models, and a lack of data. For the representation, one-hot binary encoding is often used to represent gazetteer features (Song et al., 2020). However, this does not capture contextual info or span boundaries. Alternatively, independent span taggers trained on gazetteers have been proposed to extract potential entities Liu et al. (2019), but such models can be difficult to train and ∗ This research was done during an in"
2021.naacl-main.118,2020.repl4nlp-1.1,0,0.0155078,"on gazetteer updates. Recent work has considered directly integrating knowledge into transformers, e.g., KnowBert adds knowledge to BERT layers (Peters et al., 2019), and LUKE is pretrained to predict masked entities (Yamada et al., 2020). The drawbacks of such methods are that they are specific to Transformers, and the model’s knowledge cannot be updated without retraining. We aim to overcome the limitations of previous work by designing a model-agnostic gazetteer representation that can be fused into any word-level model. for sentiment analysis, and POS tagging (Guo et al., 2018). For NER, Liu et al. (2020) proposed a Mixture of Entity Experts (MoEE) approach where they train an expert layer for each entity type, and then combine them using an MoE approach. Their approach does not include external gazetteers, and the experts provide an independent representation that is not combined with the word representation. In our work we treat word and external gazetteer representations as independent experts, applying MoE to learn a dynamically fused representation. 3 Datasets We experiment using three standard benchmarks: CoNLL03, OntoNotes, and WNUT17. However, these corpora do not capture the issues fr"
2021.naacl-main.118,P16-1101,0,0.0485865,", and allows the MoE gating network to better learn how to balance the two experts. 5 Experiments Data: All experiments are uncased, using standard benchmarks (CoNLL03, OntoNotes, WNUT17) and the new datasets we create (see §3). Models: We integrate G EM N ET with both BERT and BiLSTM word encoders.For BERT, we use the pretrained BERTBASE model. The last output layer is used, and for each word, we use the first wordpiece representation as its representation. The BiLSTM model has 3 inputs: GloVe embeddings (Pennington et al., 2014), ELMo embeddings (Peters et al., 2018) and CharCNN embeddings (Ma and Hovy, 2016). Evaluation: We evaluate MD and NER, and report entity-level precision, recall and F1 scores. 5.1 MD Baselines Our first experiment aims to measure the difficulty of our datasets (§3) relative to existing benchmarks. We train a BERT model on CoNLL03 and use it to measure MD performance on our data. Measuring NER performance is not possible as we use a different tag set (WNUT17 vs CoNLL03). Dataset CoNLL03 L OWNER M SQ -N ER O RCAS -N ER P 96.9 67.5 38.9 56.8 R 95.7 74.5 38.7 51.6 F1 96.3 70.9 38.8 54.1 Table 4: Mention detection (MD) results for a BERT model trained on CoNLL03, tested on our"
2021.naacl-main.118,D19-1005,0,0.0259382,"g minor improvements on CoNLL03. This concatenation approach has been shown to cause feature “under-training” (Yang et al., 2016), as discussed in §1. An alternative approach uses gazetteers to train a subtagger model to recognize entity spans. Liu et al. (2019) propose a hybrid semi-Markov CRF subtagger, reporting minor improvements. While a subtagger may learn regularities in entity names, a key limitation is that it needs retraining and evaluation on gazetteer updates. Recent work has considered directly integrating knowledge into transformers, e.g., KnowBert adds knowledge to BERT layers (Peters et al., 2019), and LUKE is pretrained to predict masked entities (Yamada et al., 2020). The drawbacks of such methods are that they are specific to Transformers, and the model’s knowledge cannot be updated without retraining. We aim to overcome the limitations of previous work by designing a model-agnostic gazetteer representation that can be fused into any word-level model. for sentiment analysis, and POS tagging (Guo et al., 2018). For NER, Liu et al. (2020) proposed a Mixture of Entity Experts (MoEE) approach where they train an expert layer for each entity type, and then combine them using an MoE appro"
2021.naacl-main.118,2020.acl-main.722,0,0.0199265,"ng these issues, and critical in the absence of casing. Even a human may not correctly parse “what is [[life is beautiful]]?” without knowing that a movie is being referenced. However, most models start with no knowledge of real world entities, learning them from the training data. Continuous data annotation can add new entities, but is expensive and often not feasible. Consequently, methods for integrating external knowledge, e.g., Knowledge Bases (KBs) or gazetteers, into neural architectures have gained renewed attention. However, such studies have reported limited gains (Liu et al., 2019; Rijhwani et al., 2020). The mixed success of gazetteers stems from three main limitations in current work: gazetteer feature representation, their integration with contextual models, and a lack of data. For the representation, one-hot binary encoding is often used to represent gazetteer features (Song et al., 2020). However, this does not capture contextual info or span boundaries. Alternatively, independent span taggers trained on gazetteers have been proposed to extract potential entities Liu et al. (2019), but such models can be difficult to train and ∗ This research was done during an internship at Amazon. may"
2021.naacl-main.118,2020.emnlp-main.523,0,0.0310682,"wn to cause feature “under-training” (Yang et al., 2016), as discussed in §1. An alternative approach uses gazetteers to train a subtagger model to recognize entity spans. Liu et al. (2019) propose a hybrid semi-Markov CRF subtagger, reporting minor improvements. While a subtagger may learn regularities in entity names, a key limitation is that it needs retraining and evaluation on gazetteer updates. Recent work has considered directly integrating knowledge into transformers, e.g., KnowBert adds knowledge to BERT layers (Peters et al., 2019), and LUKE is pretrained to predict masked entities (Yamada et al., 2020). The drawbacks of such methods are that they are specific to Transformers, and the model’s knowledge cannot be updated without retraining. We aim to overcome the limitations of previous work by designing a model-agnostic gazetteer representation that can be fused into any word-level model. for sentiment analysis, and POS tagging (Guo et al., 2018). For NER, Liu et al. (2020) proposed a Mixture of Entity Experts (MoEE) approach where they train an expert layer for each entity type, and then combine them using an MoE approach. Their approach does not include external gazetteers, and the experts"
D14-1144,de-marneffe-etal-2006-generating,0,0.00289728,"Missing"
D14-1144,W14-3625,1,0.761165,"work also has a backwards link in this regard by providing qualitative evidence about the underpinning linguistic theories that make NLI work. 1388 The work presented here has a number of applications; chief among them is the development of tools for SLA researchers. This would enable them to not just provide new evidence for previous findings, but to also perform semi-automated data-driven generation of new and viable hypotheses. This, in turn, can help reduce expert effort and involvement in the process, particularly as such studies expand to more corpora and emerging language like Chinese (Malmasi and Dras, 2014b) and Arabic (Malmasi and Dras, 2014a). The brief analysis included here represents only a tiny portion of what can be achieved with this methodology. We included but a few of the thousands of features revealed by this method; practical SLA tools based on this would have a great impact on current research. In addition to language transfer hypotheses, such systems could also be applied to aid development of pedagogical material within a needsbased and data-driven approach. Once language use patterns are uncovered, they can be assessed for teachability and used to create tailored, L1specific ex"
D14-1144,E14-4019,1,0.747522,"work also has a backwards link in this regard by providing qualitative evidence about the underpinning linguistic theories that make NLI work. 1388 The work presented here has a number of applications; chief among them is the development of tools for SLA researchers. This would enable them to not just provide new evidence for previous findings, but to also perform semi-automated data-driven generation of new and viable hypotheses. This, in turn, can help reduce expert effort and involvement in the process, particularly as such studies expand to more corpora and emerging language like Chinese (Malmasi and Dras, 2014b) and Arabic (Malmasi and Dras, 2014a). The brief analysis included here represents only a tiny portion of what can be achieved with this methodology. We included but a few of the thousands of features revealed by this method; practical SLA tools based on this would have a great impact on current research. In addition to language transfer hypotheses, such systems could also be applied to aid development of pedagogical material within a needsbased and data-driven approach. Once language use patterns are uncovered, they can be assessed for teachability and used to create tailored, L1specific ex"
D14-1144,W13-1716,1,0.667772,"irst and second. One possibility is that this relates to argumentation styles that are possibly influenced by cultural norms. More broadly, this effect could also be teaching rather than transfer related. For example, it may be case that a widely-used text book for learning English in Korea happens to overuse this construction. Some recent findings from the 2013 NLI Shared Task found that L1 Hindi and Telugu learners of English had similar transfer effects and their writings were difficult to distinguish. It has been posited that this is likely due to shared culture and teaching environments (Malmasi et al., 2013). Despite some clearcut instances of overuse,9 more research is required to determine the causal factors. We hope to expand on this in future work using more data. 9 More than half of the Korean scripts contained a sentence-initial however. 5 Discussion and Conclusion Using the proposed methodology, we generated lists of linguistic features overused and underused by English learners of various L1 backgrounds. Through an analysis of the top items in these ranked lists, we demonstrated the high applicability of the output by formulating plausible language transfer hypotheses supported by current"
D14-1144,E14-4033,0,0.369465,"e and positive examples.2 More discriminative features have higher scores. Another alternative method is Information Gain (Yang and Pedersen, 1997). As defined in equation (2), it measures the entropy gain associated with feature t in assigning the class label c. G(t) = − + + Pm i=1 Pr (t) Pr (t¯) Pr (ci ) log Pr (ci ) Pm i=1 Pr (ci |t) log Pr (ci |t) i=1 Pr (ci |t¯) log Pr (ci |t¯) Pm (2) However, these methods are limited: they do not provide ranked lists per-L1 class, and more importantly, they do not explicitly capture underuse. Among the efflorescence of NLI work, a new trend explored by Swanson and Charniak (2014) aims to extract lists of candidate language transfer features by comparing L2 data against the writer’s L1 to find features where the L1 use is mirrored in L2 use. This allows the detection of obvious effects, but Jarvis and Crossley (2012) note (p. 183) that many transfer effects are “too complex” to observe in this manner. Moreover, this method is unable to detect underuse, is only suitable for syntactic features, and has only been applied to very small data (4,000 sentences) over three L1s. Addressing these issues is the focus of the present work. 3 Experimental Setup 3.1 Corpus Features A"
D14-1144,P10-1117,0,0.0103612,"r to discover arbitrary length n-gram collocations. We explore both the pure part-of-speech (POS) n-grams as 2 3 Sentence → Docj Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w j ∈ 1, . . . , m j ∈ 1, . . . , m i ∈ 1, . . . , t; j ∈ 1, . . . , m i ∈ 1, . . . , t w ∈ Vpos ; w ∈ Vpos+f w Vpos contains 119 distinct POS tags based on the Brown tagset and Vpos+f w is extended with 398 function words. The number of topics t is set to 50. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available by Johnson (2010).4 Stanford dependencies We use Stanford dependencies as a syntactic feature: for each text we extract all the basic dependencies returned by the Stanford Parser (de Marneffe et al., 2006). We then generate all the variations for each of the dependencies (grammatical relations) by substituting each lemma with its corresponding POS tag. For instance, a grammatical relation of det(knowledge, the) yields the following variations: det(NN, the), det(knowledge, DT), and det(NN, DT). Lexical features Content and function words are also considered as two feature types related to learner’s vocabulary a"
D14-1144,C12-1158,0,0.251498,"overuse, the extensive use of some linguistic structures, and underuse, the underutilization of particular structures, also known as avoidance (Gass and Selinker, 2008). While there have been some attempts in SLA to use computational approaches on small-scale data,1 these still use fairly elementary techniques and have several shortcomings, including in the manual approaches to annotation and the computational artefacts derived from these. Conversely, NLI work has focused on automatic learner L1 classification using Machine Learning with large-scale data and sophisticated linguistic features (Tetreault et al., 2012). Here, feature ranking could be performed with relevancy methods such as the F-score: 1 E.g. Chen (2013), Lozan´o and Mendikoetxea (2010) and Di´ez-Bedmar and Papp (2008). 1385 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1385–1390, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2  (−) ¯j − x ¯j + x   n+  n−  P P (+) (+) 2 (−) (−) 2 ¯j ¯j xi,j − x xi,j − x + n−1−1  F (j) ≡ 1 n+ −1 (+) ¯j x ¯j −x 2 i=1 (1) i=1 The F-score (Fisher score) measures the ratio between the intraclass and interclass v"
D14-1144,W13-1706,0,0.387579,"Missing"
D14-1144,W07-0602,0,0.345237,"007, for example) that the English spoken in India still retains characteristics of the English that was spoken during the time of the Raj and the East India Company that have disappeared from other English varieties, so it sounds more formal to other speakers, or retains traces of an archaic business correspondence style; the features noted fit that pattern. The second list includes content words overused by Arabic L1 learners. Analysis of content words here, and for other L1s in our data, reveals very frequent misspellings which are believed to be due to orthographic or phonetic influences (Tsur and Rappoport, 2007; Odlin, 1989). Since Arabic does not share orthography with English, we believe most of these are due to phonetics. Looking at items 1, 3 and 5 we can see a common pattern: the English letter u which has various phonetic realizations is being replaced by a vowel that more often represents that sound. Items 2 and 5 are also phonetically similar to the intended words. For Spanish L1 authors we provide both underuse and overuse lists of syntactic dependencies. The top 3 overuse rules show the word that is very often used as the subject of verbs. This is almost 6 certainly a consequence of the pr"
D14-1144,D12-1064,1,0.856292,"ate language transfer features by comparing L2 data against the writer’s L1 to find features where the L1 use is mirrored in L2 use. This allows the detection of obvious effects, but Jarvis and Crossley (2012) note (p. 183) that many transfer effects are “too complex” to observe in this manner. Moreover, this method is unable to detect underuse, is only suitable for syntactic features, and has only been applied to very small data (4,000 sentences) over three L1s. Addressing these issues is the focus of the present work. 3 Experimental Setup 3.1 Corpus Features Adaptor grammar collocations Per Wong et al. (2012), we utilize an adaptor grammar to discover arbitrary length n-gram collocations. We explore both the pure part-of-speech (POS) n-grams as 2 3 Sentence → Docj Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w j ∈ 1, . . . , m j ∈ 1, . . . , m i ∈ 1, . . . , t; j ∈ 1, . . . , m i ∈ 1, . . . , t w ∈ Vpos ; w ∈ Vpos+f w Vpos contains 119 distinct POS tags based on the Brown tagset and Vpos+f w is extended with 398 function words. The number of topics t is set to 50. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carl"
E14-4019,W13-1706,0,0.378307,"Missing"
E14-4019,C12-1025,0,0.340093,"Missing"
E14-4019,W07-0602,0,0.284166,"Missing"
E14-4019,C12-1027,0,0.150967,"age Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researchers have obtained good results under this paradigm. While a detailed exposition of NLI has been omi"
E14-4019,U09-1008,1,0.432497,"ue. This relates to Cross-Linguistic Influence (CLI), a key topic in the field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researche"
E14-4019,D11-1148,1,0.849433,"ross-Linguistic Influence (CLI), a key topic in the field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researchers have obtained good"
E14-4019,W13-1716,1,0.422025,"till relatively young and many fundamental questions have yet to be answered. All of the tested models are effective, and they appear to be complementary as combining them improves overall accuracy. We also note the difference in the efficacy of the feature representations and see a clear preference for frequency-based feature values. Others have found that binary features are the most effective for English NLI (Brooke and Hirst, 2012), but our results indicate frequency information is more informative in this task. The combination of both feature types has also been reported to be effective (Malmasi et al., 2013). To see how these models perform across languages, we also compare the results against the TOEFL11 corpus used in the NLI2013 shared task. We perform the same experiments on that dataset using the English CoreNLP models, Penn Treebank PoS tagset and a set of 400 English function words. Figure 2 shows the results side by side. Remarkably, we see that the model results closely mirror each other across corpora. This is a highly interesting finding from our study that merits further investigation. There is a systematic pattern occurring across data from learners of completely different L1-L2 pair"
E14-4019,P12-2038,0,0.0919377,"ence (CLI), a key topic in the field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researchers have obtained good results under this paradigm"
E14-4019,C12-1158,0,0.301215,"he field of Second Language Acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages (Ortega, 2009). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. Rising numbers of language learners have led to an increasing need 2 Related Work NLI is a fairly recent, but rapidly growing area of research. While some research was conducted in the early 2000s, the most significant work has only appeared in the last few years (Wong and Dras, 2009; Wong and Dras, 2011; Swanson and Charniak, 2012; Tetreault et al., 2012; Bykh and Meurers, 2012). Most studies approach NLI as a multi-class supervised classification task. In this experimental design, the L1 metadata are used as class labels 95 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95–99, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and the individual writings are used as training and testing data. Using lexical and syntactic features of increasing sophistication, researchers have obtained good results under this paradigm. While a detailed expos"
J18-3003,W17-5025,0,0.0414871,"Missing"
J18-3003,P06-4020,0,0.0185625,"Missing"
J18-3003,brooke-hirst-2012-measuring,0,0.346229,"can be applied for NLI. We aim to examine several different ensemble and meta-classification architectures, each of which can utilize different configurations and algorithms. Furthermore, previous ensemble methods have not been tested on different data sets, making the ability of these models to generalize for NLI unclear. Ideally, the same method should be tested across multiple corpora to assess its validity. To this end, we also apply our methods to multiple data sets, in both English and other languages, to evaluate their generalization capacity. In addition, following the observation of Brooke and Hirst (2012b) that patterns utilized by machine learners can be corpus-specific and influenced by topic, even with apparently topic-neutral features—something that could also be true in our multi-level classifier context—we carry out cross-corpus experiments along the lines of Brooke and Hirst (2012b), Malmasi and Dras (2015), and Ionescu, Popescu, and Cahill (2016). To summarize, the principal aims of the present study are the following: 1. Apply several advanced ensemble combination methods to NLI and evaluate their performance against previously used ensemble methods. 1 Traditional text classification"
J18-3003,C12-1025,0,0.0420896,"Missing"
J18-3003,C14-1185,0,0.0367403,"Missing"
J18-3003,W13-1726,0,0.0502666,"Missing"
J18-3003,W17-5023,0,0.0367823,"Missing"
J18-3003,W17-5049,0,0.221304,"Missing"
J18-3003,W13-1727,0,0.0506005,"Missing"
J18-3003,W17-5041,0,0.0299776,"Missing"
J18-3003,W13-1712,0,0.0558704,"Missing"
J18-3003,W13-1729,0,0.049149,"Missing"
J18-3003,W13-1713,0,0.050284,"Missing"
J18-3003,W13-1730,0,0.0605592,"Missing"
J18-3003,W17-5024,0,0.184364,"Missing"
J18-3003,D14-1142,0,0.12137,"Missing"
J18-3003,J16-3005,0,0.0296618,"Missing"
J18-3003,W17-5021,0,0.461653,"Missing"
J18-3003,W13-1714,0,0.0445402,"Missing"
J18-3003,W17-5043,0,0.0399461,"Missing"
J18-3003,W17-5044,0,0.0385807,"Missing"
J18-3003,W15-0606,1,0.788838,"rmed well on a range of classification tasks and comment that “whatever exotic tools are the rage of the day, we should always have available these two simple tools.” 5. Features As its focus is on classifier architecture, this study just utilizes a standard set of NLI features used in previous work, as noted in, for example, the overview of the systems in the NLI shared task of 2013 (Tetreault, Blanchard, and Cahill 2013). We combine these various feature types because it has been shown that both lexical and syntactic features each capture diverse types of information that are complementary (Malmasi and Cahill 2015). Different feature types are extracted for each of our data sets, as shown in Table 4. Table 4 An overview of the features used for each data set. Feature Word/Lemma n-grams Character n-grams Function word unigrams Function word bigrams POS n-grams Dependencies CFG Rules Adaptor Grammars TSG Fragments T OEFL11 X X X X X X X X X E F C AM D AT Chinese Norwegian X X X X X X X X X X X X 419 Computational Linguistics Volume 44, Number 3 The feature types for each data set were chosen based on properties of the data set and the availability of NLP resources for the L2. For stylistic classification"
J18-3003,W14-3625,1,0.859836,"cal relations between other words; consequently, they have been widely used in stylistic classification tasks. In this work, the English word list was obtained from the Onix Text Retrieval Toolkit.18 For Norwegian we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.19 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis [whose], ikke [not], jeg [I], s˚a [so], and hj˚a [at]. We also make this list available on our Web site.20 For Chinese, we utilize the function word list described in Malmasi and Dras (2014b). 17 It is true that other features can still detect topic bias: Brooke and Hirst (2011) showed that the ICLE corpus suffered from this, as, for example, the fact that French texts were more about philosophy and religion, whereas Japanese texts were more about personal experiences, such as language learning and travel, had related register differences that could be detected by non-content features. 18 http://www.lextek.com/manuals/onix/stopwords1.html. 19 https://github.com/apache/lucene-solr. 20 http://web.science.mq.edu.au/~smalmasi/data/norwegian-funcwords.txt. 420 Malmasi and Dras Native"
J18-3003,E14-4019,1,0.846416,"cal relations between other words; consequently, they have been widely used in stylistic classification tasks. In this work, the English word list was obtained from the Onix Text Retrieval Toolkit.18 For Norwegian we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.19 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis [whose], ikke [not], jeg [I], s˚a [so], and hj˚a [at]. We also make this list available on our Web site.20 For Chinese, we utilize the function word list described in Malmasi and Dras (2014b). 17 It is true that other features can still detect topic bias: Brooke and Hirst (2011) showed that the ICLE corpus suffered from this, as, for example, the fact that French texts were more about philosophy and religion, whereas Japanese texts were more about personal experiences, such as language learning and travel, had related register differences that could be detected by non-content features. 18 http://www.lextek.com/manuals/onix/stopwords1.html. 19 https://github.com/apache/lucene-solr. 20 http://web.science.mq.edu.au/~smalmasi/data/norwegian-funcwords.txt. 420 Malmasi and Dras Native"
J18-3003,U14-1020,1,0.85752,"cal relations between other words; consequently, they have been widely used in stylistic classification tasks. In this work, the English word list was obtained from the Onix Text Retrieval Toolkit.18 For Norwegian we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.19 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis [whose], ikke [not], jeg [I], s˚a [so], and hj˚a [at]. We also make this list available on our Web site.20 For Chinese, we utilize the function word list described in Malmasi and Dras (2014b). 17 It is true that other features can still detect topic bias: Brooke and Hirst (2011) showed that the ICLE corpus suffered from this, as, for example, the fact that French texts were more about philosophy and religion, whereas Japanese texts were more about personal experiences, such as language learning and travel, had related register differences that could be detected by non-content features. 18 http://www.lextek.com/manuals/onix/stopwords1.html. 19 https://github.com/apache/lucene-solr. 20 http://web.science.mq.edu.au/~smalmasi/data/norwegian-funcwords.txt. 420 Malmasi and Dras Native"
J18-3003,N15-1160,1,0.882698,"Missing"
J18-3003,W15-0620,1,0.882544,"Missing"
J18-3003,W13-1716,1,0.9035,"Missing"
J18-3003,W16-4801,1,0.877455,"Missing"
J18-3003,P14-5010,0,0.005809,"Missing"
J18-3003,W17-5042,0,0.0222264,"Missing"
J18-3003,de-marneffe-etal-2006-generating,0,0.0128688,"Missing"
J18-3003,W17-5047,0,0.0450588,"Missing"
J18-3003,W13-1735,0,0.0534509,"Missing"
J18-3003,W17-5027,0,0.0454599,"Missing"
J18-3003,P12-2038,0,0.0326668,"tanford parser (de Marneffe, Maccartney, and Manning 2006). We then generate all the variations for each of the dependencies (grammatical relations) by substituting each lemma with its corresponding POS tag. For instance, a grammatical relation of det(knowledge, the) yields the following variations: det(NN, the), det(knowledge, DT), and det(NN, DT). CFG Rules. For English and Chinese, we obtain a constituent-based parse using the Stanford parser, and extract the production rules that constitute those trees to use as features. Tree Substitution Grammar Fragments. TSG fragments were proposed by Swanson and Charniak (2012) as another type of syntactic feature for NLI that captures a broader syntactic context than the single-level fragments of phrase-structure trees that constitute the CFG rules. We only extract TSG fragments for the T OEFL11 data as they include lexical terminal nodes. 21 http://nlp.stanford.edu/software/corenlp.shtml. 22 http://users.sussex.ac.uk/~johnca/rasp/. 421 Computational Linguistics Volume 44, Number 3 6. Classification Models We conduct a set of three experiments, each based on different ensemble structures that we describe in this section. The first model is based on a traditional pa"
J18-3003,tenfjord-etal-2006-ask,0,0.188231,"Missing"
J18-3003,W13-1706,0,0.0869832,"Missing"
J18-3003,C12-1158,0,0.219699,"corpora (Malmasi and Dras 2014a,c). Ensemble Methods: The 2013 Shared Task. As mentioned earlier, some of the most successful approaches to NLI have used ensemble learning methods, and investigating ensemble configurations is a key goal of this article. We now therefore present an overview of work in NLI that has used ensembles, focusing on the way in which ensembles were defined and what benefit they brought. Most of this work leads into, is part of, or is positioned with respect to the first NLI shared task of 2013. We discuss briefly after this some observations from the 2017 shared task. Tetreault et al. (2012) were the first to propose the use of classifier ensembles for NLI, and they performed a comprehensive evaluation of the feature types used until that point. In their study they used an ensemble of logistic regression learners, using a wide range of features that included character and word n-grams, function words, parts of speech, spelling errors, and writing quality markers. With regard to syntactic features, they also investigated the use of Tree Substitution Grammars and dependency features extracted using the Stanford parser. Furthermore, they also proposed using language models for this"
J18-3003,N01-1031,0,0.0900484,"d to various classification tasks with good results. Not surprisingly, researchers have attempted to use them for improving the performance of NLI, as we discuss in the next section. 2.2 Native Language Identification General Background. Determining the identity of, or properties of, an author of a text is a problem of longstanding interest, and attempts to solve it are similarly longstanding (Koppel, Schler, and Argamon 2009). NLI is the subtype of this task focusing on identifying the L1 of the writer. To our knowledge, the first work tackling this task in a computational manner was that of Tomokiyo and Jones (2001), who worked with speech data recorded for this task. Their main aim was to detect non-native speech, using part-of-speech and lexical features in a na¨ıve Bayes classifier, and to also determine the native language of the non-native speakers. Another early work was that of Jarvis, Castaneda-Jim´enez, and Nielsen (2004), who presented an early empirical approach to investigating L1 transfer using lexical style and word choice, which they termed wordprints. Working with a written corpus compiled for the task and focusing exclusively on lexical transfer, they collected the 30 most frequent words"
J18-3003,W15-0614,1,0.905428,"Missing"
J18-3003,U09-1008,1,0.84359,"Missing"
J18-3003,D12-1064,1,0.884234,"Missing"
J18-3003,W17-5028,0,\N,Missing
J18-3003,J15-4006,0,\N,Missing
L16-1284,W15-5412,0,0.0342684,"Missing"
L16-1284,W15-5410,0,0.477289,"e were many cases of republication (e.g. British texts republished by an American newspaper and tagged as American by the original sources) that made the task for this language group unfeasible.(Zampieri et al., 2014) Closed Open MAC MMS NRC SUKI BOBICEV BRUNIBP PRHLT INRIA NLEL OSEVAL 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 - 95.65 91.84 76.17 MAC SUKI NRC MMS BOBICEV PRHLT NLEL OSEVAL 94.01 93.02 93.01 92.78 92.22 90.80 62.78 - 93.41 89.56 75.30 System Description Test Set A (Malmasi and Dras, 2015b) (Zampieri et al., 2015a) (Goutte and L´eger, 2015) (Jauhiainen et al., 2015) (Bobicev, 2015) ´ et al., 2015) (Acs (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) Test Set B (Malmasi and Dras, 2015b) (Jauhiainen et al., 2015) (Goutte and L´eger, 2015) (Zampieri et al., 2015a) (Bobicev, 2015) (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) - Table 2: DSL Shared Task 2015 - Accuracy results for open and closed submissions using test sets A and B. Teams ranked by their results in the closed submission. We observe that for all systems, performance dropped from test set A (complete texts) to test set B (name entities removed). This was the expected outcome. However, p"
L16-1284,D14-1069,0,0.0549297,"Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a),"
L16-1284,L16-1522,0,0.0379652,"014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between simila"
L16-1284,W15-5409,0,0.249914,"y an American newspaper and tagged as American by the original sources) that made the task for this language group unfeasible.(Zampieri et al., 2014) Closed Open MAC MMS NRC SUKI BOBICEV BRUNIBP PRHLT INRIA NLEL OSEVAL 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 - 95.65 91.84 76.17 MAC SUKI NRC MMS BOBICEV PRHLT NLEL OSEVAL 94.01 93.02 93.01 92.78 92.22 90.80 62.78 - 93.41 89.56 75.30 System Description Test Set A (Malmasi and Dras, 2015b) (Zampieri et al., 2015a) (Goutte and L´eger, 2015) (Jauhiainen et al., 2015) (Bobicev, 2015) ´ et al., 2015) (Acs (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) Test Set B (Malmasi and Dras, 2015b) (Jauhiainen et al., 2015) (Goutte and L´eger, 2015) (Zampieri et al., 2015a) (Bobicev, 2015) (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) - Table 2: DSL Shared Task 2015 - Accuracy results for open and closed submissions using test sets A and B. Teams ranked by their results in the closed submission. We observe that for all systems, performance dropped from test set A (complete texts) to test set B (name entities removed). This was the expected outcome. However, performance is, in most cases, only 1 or 2 percentage points lower which is not a"
L16-1284,W15-5403,0,0.727991,".g. British texts republished by an American newspaper and tagged as American by the original sources) that made the task for this language group unfeasible.(Zampieri et al., 2014) Closed Open MAC MMS NRC SUKI BOBICEV BRUNIBP PRHLT INRIA NLEL OSEVAL 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 - 95.65 91.84 76.17 MAC SUKI NRC MMS BOBICEV PRHLT NLEL OSEVAL 94.01 93.02 93.01 92.78 92.22 90.80 62.78 - 93.41 89.56 75.30 System Description Test Set A (Malmasi and Dras, 2015b) (Zampieri et al., 2015a) (Goutte and L´eger, 2015) (Jauhiainen et al., 2015) (Bobicev, 2015) ´ et al., 2015) (Acs (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) Test Set B (Malmasi and Dras, 2015b) (Jauhiainen et al., 2015) (Goutte and L´eger, 2015) (Zampieri et al., 2015a) (Bobicev, 2015) (Franco-Salvador et al., 2015) (Fabra-Boluda et al., 2015) - Table 2: DSL Shared Task 2015 - Accuracy results for open and closed submissions using test sets A and B. Teams ranked by their results in the closed submission. We observe that for all systems, performance dropped from test set A (complete texts) to test set B (name entities removed). This was the expected outcome. However, performance is, in most cases, only 1 or 2 percentage"
L16-1284,W13-1728,1,0.85986,"A and 4 In the 2015 edition, organizers did not use language group names as in the 2014 edition. We use them for both editions in this paper for the sake of clarity and consistency. 1801 B was MAC (Malmasi and Dras, 2015b) which proposed an ensemble of SVM classifiers for this task. Two other SVMbased approaches were tied in 2nd for test set A, one by the NRC team (Goutte and L´eger, 2015) and MMS (Zampieri et al., 2015a), which experimented with three different approaches and obtained the best results combining TF-IDF and an SVM classifier previously used for native language identification (Gebre et al., 2013). The NRC team included members of NRC-CNRC, winners of the DSL closed submission track in 2014. Both in 2014 and in 2015 they used a two-stage classification approach to predict first the language group and then the language within the predicted group. Two other teams used two-stage classification approaches: NLEL (Fabra-Boluda et al., 2015) and BRUniBP ´ et al., 2015). (Acs A number of computational techniques have been explored in the DSL 2015 including token-based backoff by SUKI team (Jauhiainen et al., 2015), prediction by partial matching (PPM) by BOBICEV (Bobicev, 2015), and word and s"
L16-1284,W15-5413,1,0.877349,"Missing"
L16-1284,W14-5316,1,0.633192,"Missing"
L16-1284,Y08-1042,0,0.049537,"en similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpu"
L16-1284,W15-5408,0,0.33486,"Missing"
L16-1284,W14-5317,0,0.112771,"Missing"
L16-1284,U13-1003,0,0.421039,"more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switche"
L16-1284,Q14-1003,0,0.14435,"ieri et al., 2014; Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and"
L16-1284,W14-5315,0,0.117061,"ieri et al., 2014; Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Language identification in written texts is a wellestablished research topic in computational linguistics. Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim˜oes et al., 2014). The interest in the discrimination of similar languages, language varieties, and dialects is more recent but it has been growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and"
L16-1284,W14-4204,0,0.229199,"Missing"
L16-1284,D14-1144,1,0.843128,"al., 2014). The dataset is entitled DSL Corpus Collection, or DSLCC, and it includes short excerpts from journalistic texts from previously released corpora and repository.2 Texts in the DSLCC v. 1.0 were written in thirteen languages or language varieties and divided into the following six groups: Group A (Bosnian, Croatian, Serbian), Group B (Indonesian, Malay), Group C (Czech, Slovak), Group D (Brazilian Portuguese, European Portuguese), Group E (Peninsu1 This task focuses on identifying the mother tongue of a learner writer based on stylistic cues; all the texts are in the same language (Malmasi and Dras, 2014; Malmasi and Dras, 2015c). 2 See Tan et al. (2014) for a complete list of sources. 1800 lar Spanish, Argentine Spanish), and Group F3 (American English, British English). In the 2014 edition, eight teams participated and submitted results to the DSL shared task (eight teams in the closed and two teams in the open submission). Five of these teams wrote system description papers. The complete shared task report is available in Zampieri et al. (2014). We summarize the results in Table 1 in terms of accuracy (best performing entries displayed in bold). Team NRC-CNRC RAE UMich UniMelb-NLP QMUL LIR"
L16-1284,W15-5407,1,0.445859,"anguage identification systems (Tiedemann and Ljubeˇsi´c, 2012). Closely-related languages such as Indonesian and Malay or Croatian and Serbian are very similar both at their spoken and at their written forms making it difficult for systems to discriminate between them. Varieties of the same language, e.g. Spanish from South America or Spain, are even more difficult to detect than similar languages. Nevertheless, in both cases, recent work has shown that it is possible to train algorithms to discriminate between similar languages and language varieties with high accuracy (Goutte et al., 2014; Malmasi and Dras, 2015b). This study looks in more detail into the features that help algorithms discriminating between similar languages, taking into account the results of two recent editions of the Discriminating between Similar Languages (DSL) shared task (Zampieri et al., 2014; Zampieri et al., 2015b). The analysis and results of this paper complement the information presented in the shared task reports and provide novel and important information for researchers and developers interested in language identification and particularly in the problem of discriminating between similar languages. 2. Related Work Lang"
L16-1284,W15-0620,1,0.852128,"w years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task. To our knowledge, however, no comprehensive analysis of the kind we are proposing in this paper has been carried ou"
L16-1284,W14-5314,0,0.212335,"Missing"
L16-1284,W14-3907,0,0.0354693,"n and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task. To our knowledge, however, no comprehensive analysis of the kind we are proposing in this paper has been carried out on the results obtained in a language identification shared task and our work fills this gap. The most similar analysis was applied to Native Language Identification (NLI)1 using the 2013 NLI shared task dataset (Malmasi et al., 2015b). In the next sections we present the systems that participated in the two editions of the DSL shared task. 2.1. DSL Shared Task 2014 The first edition of the"
L16-1284,C12-1160,0,0.332039,"Missing"
L16-1284,W14-5313,0,0.149574,"growing in the past few years. Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malanc¸on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljubeˇsi´c et al., 2007; Ljubeˇsi´c and Kranjˇci´c, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G´omez-Rodrıguez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a) A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task. To our knowledge, however, no comprehensive analysis of the kind we are proposing in this pap"
L16-1284,W14-5307,1,0.851072,"Missing"
L16-1284,W15-5411,1,0.885723,"Missing"
L16-1284,W15-5401,1,0.771913,"Missing"
L16-1647,P12-1011,0,0.0220187,"estion of representing time intervals for temporal text classification. Related Work Modeling temporal information in text is a relevant task to a number of NLP applications. Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a). Time expressions (e.g. after 2010), can help algorithms to identify the approximate publication date of texts (Chambers, 2012), but there are a number of cases in which they are not present in text and one alternative is to use features related to language change as we propose in this paper. As will be evidenced in this section, even though there were a number of attempts to approach temporal text classification, to our knowledge this task was not substantially explored as other text classification tasks. The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans. The method"
L16-1647,R13-1018,0,0.0129348,"investigate the most informative features for this task and how they are linked to language change. Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization 1. Introduction 2. It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century"
L16-1647,W13-2714,0,0.0164411,"investigate the most informative features for this task and how they are linked to language change. Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization 1. Introduction 2. It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century"
L16-1647,W06-0903,0,0.343169,"e a number of attempts to approach temporal text classification, to our knowledge this task was not substantially explored as other text classification tasks. The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans. The method was tested on a collection of Dutch journalistic texts published from January 1999 to February 2005. Other methods, such as Kumar et al. (2011), make use of information gain to estimate the best features in classification. In Dalli and Wilks (2006) researchers train a classifier to predict the publication date of texts within a time span of nine years. The method uses words as features and it is aided by words which increase their frequency at some point of time, particularly named entities. Another study that works under a similar assumption is the one published by Abe and Tsumoto (2010). The authors proposed the use of similarity metrics to categorize texts based on keywords calculated using tf-idf (term frequency - inverse document frequency). Garcia-Fernandez et al. (2011) presents a method to predict the publication dates of excerp"
L16-1647,Q16-1003,0,0.0689138,"Missing"
L16-1647,E14-4019,1,0.882439,"tion of historical corpora. Although Colonia is to our knowledge the biggest Portuguese corpus of its kind, it does not contain many texts from each period. The number of documents varies between 13 from the 16th century and 38 from the 19th century. For text classification, however, the number of documents available (especially at the training stage) is very important to provide enough information to achieve high classification performance. To circumvent this limitation, in this paper we propose the use of composite documents made of sentences from various texts. Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing. This methodology ensures that the texts for each class are a mix of different authorship styles and topics. It also means that all documents are similar and comparable in length making the task more challenging. Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015). In our experiments we model two dimensions of language"
L16-1647,D14-1144,1,0.86872,"tion of historical corpora. Although Colonia is to our knowledge the biggest Portuguese corpus of its kind, it does not contain many texts from each period. The number of documents varies between 13 from the 16th century and 38 from the 19th century. For text classification, however, the number of documents available (especially at the training stage) is very important to provide enough information to achieve high classification performance. To circumvent this limitation, in this paper we propose the use of composite documents made of sentences from various texts. Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing. This methodology ensures that the texts for each class are a mix of different authorship styles and topics. It also means that all documents are similar and comparable in length making the task more challenging. Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015). In our experiments we model two dimensions of language"
L16-1647,P12-2051,0,0.075944,"Missing"
L16-1647,E14-4004,1,0.951879,"The authors concluded that the use of lexical features is the best source of information for this task. An important issue to take into account when working on temporal text classification is how to represent time. Most studies, including our own, model the task as supervised classification in which algorithms are trained to assign texts to an n number of classes. Each of these n classes represent an arbitrarily defined time interval, for example: a month, a year, or a decade. However, there have been a few attempts to approach this task without relying on predefined time spans. The study by Niculae et al. (2014) approached the task using ranking and pairwise comparisons to predict for each pair of documents which one is older and finally to produce a rank of all documents in a collection from older to newer. Another recent study to tackle the issue of time intervals is Efremova et al. (2015). In this study authors apply clustering methods to automatically obtain optimal time partitions in a dataset of historical Dutch notary acts. We return to this question in Section 6.1. of this paper. The style of texts also changes over time and it can be a good indicator to predict the publication date of a docˇ"
L16-1647,I13-1040,0,0.041697,"Missing"
L16-1647,S15-2147,0,0.336739,"formative features for this task and how they are linked to language change. Keywords: Language Change, Temporal Text Classification, Support Vector Machines, Text Categorization 1. Introduction 2. It is well-known that language changes over time both in spoken and in written forms. Changes in written language can be manifested in many ways such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015). This task is called temporal text classification and to our knowledge it has not been substantially explored in the literature as other text classification tasks. This paper contributes in this direction. In this paper we investigate the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information. The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century. The approach we propose here is"
L16-1647,S15-2142,0,0.0219181,"Missing"
L16-1647,S15-2148,0,0.327017,"Missing"
L16-1647,S15-2144,1,0.861705,"Missing"
malmasi-zampieri-2017-detecting,N12-1084,0,\N,Missing
malmasi-zampieri-2017-detecting,N15-1160,1,\N,Missing
malmasi-zampieri-2017-detecting,W16-0314,1,\N,Missing
malmasi-zampieri-2017-detecting,W17-3003,0,\N,Missing
malmasi-zampieri-2017-detecting,W17-1101,0,\N,Missing
malmasi-zampieri-2017-detecting,W17-3008,0,\N,Missing
N15-1160,brooke-hirst-2012-measuring,0,0.140452,"y first NLI Shared Task that was held in 2013 (Tetreault et al., 2013). Most English NLI work has been done using two corpora. The International Corpus of Learner En1 An ideal NLI corpus should have multiple L1s, be balanced by topic, proficiency, texts per L1 and be large in size. 1403 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using feat"
N15-1160,C12-1025,0,0.0479453,"y first NLI Shared Task that was held in 2013 (Tetreault et al., 2013). Most English NLI work has been done using two corpora. The International Corpus of Learner En1 An ideal NLI corpus should have multiple L1s, be balanced by topic, proficiency, texts per L1 and be large in size. 1403 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using feat"
N15-1160,W13-1727,0,0.144938,"Missing"
N15-1160,W00-1322,0,0.0193141,"Missing"
N15-1160,W01-0521,0,0.113463,"similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extract lists of overused and underused linguistic features for each L1 group. Cross-corpus studies have been conducted for various data-driven NLP tasks, including parsing (Gildea, 2001), WSD (Escudero et al., 2000) and NER (Nothman et al., 2009). While most such experiments show a drop in performance, the effect varies widely across tasks, making it hard to predict the expected drop for NLI. We aim to address this question using large training and testing data. 3 EFCamDat: A new corpus for NLI The EF Cambridge Open Language Database (E F C AM DAT) is an English L2 corpus that was released recently (Geertzen et al., 2013). It is composed of texts submitted to Englishtown, an online school used by thousands of learners daily. This corpus is notable for its size, containing som"
N15-1160,W15-0606,1,0.335813,"Missing"
N15-1160,W14-3625,1,0.731842,"hnologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extrac"
N15-1160,E14-4019,1,0.718241,"hnologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extrac"
N15-1160,U14-1020,1,0.701847,"hnologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extrac"
N15-1160,D14-1144,1,0.667583,"hnologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introduced the Chinese Learner Corpus for NLI and their results indicate that feature performance may be similar across corpora and even L1L2 pairs. This is a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extrac"
N15-1160,W13-1716,1,0.210371,"baseline used by Malmasi et al. (2015). The oracle correctly classifies a text if any ensemble member correctly predicts its label and defines an upper-bound for classification accuracy. We avoid using lexical features as E F C AM DAT is not topic balanced. We extract the following topicindependent feature types: Function words are topic-independent grammatical words such as prepositions which indicate the relations between other words. They are known to be useful for NLI. Frequencies of 400 English function words3 are extracted as features. We also apply function word bigrams as described in Malmasi et al. (2013). Context-free Grammar Production Rules are extracted after parsing each sentence. Each rule is a classification feature (Wong and Dras, 2011) and captures global syntactic patterns. 3 Like previous work, this also includes stop words, which we sourced from the Onix Text Retrieval Toolkit: http://www.lextek.com/manuals/onix/stopwords1.html Confusion Matrix ARA PR CLC 480 ITA 400 JPN 320 KOR 240 SPA Within-Corpus Evaluation Our first experiment applies 10-fold cross-validation within the corpus to assess feature efficacy. The results are shown in the first column of Table 2. All features perfor"
N15-1160,W15-0620,1,0.806188,"e table also indicates the 9 classes common to both corpora. This subset of common classes enables us to perform large-scale cross-corpus validation experiments that have not been possible until now. 4 Methodology We use the standard NLI classification approach. A linear Support Vector Machine is used for classification and feature vectors are created using relative frequency values. We also combine features with a mean probability ensemble classifier (Polikar, 2006, §4.2) using the probabilities assigned to each class. We compare results with a random baseline and the oracle baseline used by Malmasi et al. (2015). The oracle correctly classifies a text if any ensemble member correctly predicts its label and defines an upper-bound for classification accuracy. We avoid using lexical features as E F C AM DAT is not topic balanced. We extract the following topicindependent feature types: Function words are topic-independent grammatical words such as prepositions which indicate the relations between other words. They are known to be useful for NLI. Frequencies of 400 English function words3 are extracted as features. We also apply function word bigrams as described in Malmasi et al. (2013). Context-free Gr"
N15-1160,E09-1070,0,0.0336972,"a claim that we will test here. NLI is now also moving towards using features to generate SLA hypotheses. Swanson and Charniak (2014) approach this by using both L1 and L2 data to identify features exhibiting non-uniform usage in both datasets, creating lists of candidate transfer features. Malmasi and Dras (2014d) propose a different method, using linear SVM weights to extract lists of overused and underused linguistic features for each L1 group. Cross-corpus studies have been conducted for various data-driven NLP tasks, including parsing (Gildea, 2001), WSD (Escudero et al., 2000) and NER (Nothman et al., 2009). While most such experiments show a drop in performance, the effect varies widely across tasks, making it hard to predict the expected drop for NLI. We aim to address this question using large training and testing data. 3 EFCamDat: A new corpus for NLI The EF Cambridge Open Language Database (E F C AM DAT) is an English L2 corpus that was released recently (Geertzen et al., 2013). It is composed of texts submitted to Englishtown, an online school used by thousands of learners daily. This corpus is notable for its size, containing some 550k texts from numerous nationalities, making it an ideal"
N15-1160,E14-4033,0,0.198827,"author based on a second language (L2) text, has received much attention recently. Much of this is motivated by Second Language Acquisition (SLA) as NLI, often accomplished via machine learning methods, can be used to study language transfer effects. Most NLI research hitherto has focused on identifying linguistic phenomena that can capture transfer effects, with little effort towards interpreting discriminant features. Some researchers have now shifted their focus to developing data-driven methods for the automatic extraction and ranking of linguistic features that distinguish specific L1s (Swanson and Charniak, 2014). Such methods could be used not only to confirm existing SLA hypotheses, but also to create new ones. This hypothesis formulation is an inherently Mark Dras Centre for Language Technology Macquarie University Sydney, NSW, Australia mark.dras@mq.edu.au difficult problem requiring copious amounts of data. Contrary to this requirement, researchers have long noted the paucity of suitable corpora1 for this task (Brooke and Hirst, 2011). This is one of the research issues addressed by this work. Furthermore, deriving SLA hypotheses from a single corpus may not be entirely useful for SLA research. M"
N15-1160,C12-1158,0,0.105171,"Missing"
N15-1160,W13-1706,0,0.0749477,"NLI, (2) perform within-corpus evaluation with a comparative analysis against equivalent corpora, (3) perform cross-corpus evaluation to determine the efficiency of corpus independent features and (4) analyze the features’ utility for SLA & ESL research. 2 Background and Motivation NLI work has been growing in recent years, using a wide range of syntactic and more recently, lexical features to distinguish the L1. A detailed review of NLI methods is omitted here for reasons of space, but a thorough exposition is presented in the report from the very first NLI Shared Task that was held in 2013 (Tetreault et al., 2013). Most English NLI work has been done using two corpora. The International Corpus of Learner En1 An ideal NLI corpus should have multiple L1s, be balanced by topic, proficiency, texts per L1 and be large in size. 1403 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1403–1409, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics glish (Granger et al., 2009) was widely used until recently, despite its shortcomings2 being widely noted (Brooke and Hirst, 2012a). More recently, T OEFL11, the first corpus de"
N15-1160,D11-1148,1,0.637281,"Missing"
N19-1144,N12-1084,0,0.430083,"are the performance of different machine learning models on OLID. 1 Introduction Offensive content has become pervasive in social media and thus a serious concern for government organizations, online communities, and social media platforms. One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Rec"
N19-1144,W18-3901,1,0.892109,"Missing"
N19-1144,S19-2010,1,0.442727,"Missing"
N19-1144,D14-1181,0,0.011836,"i) an input embedding layer, (ii) a bidirectional LSTM layer, and (iii) an average pooling layer of input features. The concatenation of the LSTM layer and the average pooling layer is further passed through a dense layer, whose output is ultimately passed through a softmax to produce the final prediction. We set two input channels for the input embedding layers: pre-trained FastText embeddings (Bojanowski et al., 2017), as well as updatable embeddings learned by the model during training. CNN Finally, we experiment with a Convolutional Neural Network (CNN) model based on the architecture of (Kim, 2014), and using the same multi-channel inputs as the above BiLSTM. 4.1 Offensive Language Detection The performance on discriminating between offensive (OFF) and non-offensive (NOT) posts is reported in Table 4. We can see that all models perform significantly better than chance, with the neural models performing substantially better than the SVM. The CNN outperforms the RNN model, achieving a macro-F1 score of 0.80. 4.2 Categorization of Offensive Language In this set of experiments, the models were trained to discriminate between targeted insults and threats (TIN) and untargeted (UNT) offenses,"
N19-1144,W18-4401,1,0.535969,"edia and thus a serious concern for government organizations, online communities, and social media platforms. One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was"
N19-1144,malmasi-zampieri-2017-detecting,1,0.80182,"h this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalized group, and whether the abusive content is explicit or implicit. Wiegand et al. (2018) further applied this idea to German tweets. They experimented with a task on detecting offensive vs. nonoffensive tweets, and also with a second task on further"
N19-1144,W17-3008,0,0.10368,"he problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalize"
N19-1144,W17-3012,0,0.142505,"p and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalized group, and whether the abusive content is explicit or implicit. Wiegand et al. (2018) further applied this idea to German tweets. They experimented with a task on detecting offensive vs. nonoffensive tweets, and also with a second task on further sub-classifying the offensive tweets as profanity, insult, or abuse. However, to the b"
N19-1144,Q17-1010,0,\N,Missing
P17-1134,P15-2082,0,0.030673,"e as an example the case that in Hebrew there are seven synonyms for the word fear, and that different authors may choose consistently from among them. Then, having constructed their own synsets using available biblical resources and annotations, they represent texts by vectors of synonyms and apply a modified cosine similarity measure to compare and cluster these vectors. While the general task is relevant to this paper, the particular notion of synonymy here means the approach is specific to this problem, although their approach is extended to other kinds of text in Akiva and Koppel (2013). Aldebei et al. (2015) proposed a new approach motivated by this work, similarly clustering sentences, then using a Naive Bayes classifier with modified prior probabilities to classify sentences. Poetry Voice Detection Brooke et al. (2012) perform stylistic segmentation of a well-known poem, The Waste Land by T.S. Eliot. This poem is renowned for the great number of voices that appear throughout the text and has been the subject of much literary analysis (Bedient and Eliot, 1986; Cooper, 1987). These distinct voices, conceived of as representing different characters, have differing tones, lexis and grammatical styl"
P17-1134,N10-1083,0,0.0984604,"Missing"
P17-1134,W12-2504,0,0.33007,"our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation. 1 Introduction Most work on automatically segmenting text has been on the basis of topic: segment boundaries correspond to topic changes (Hearst, 1997). There are various contexts, however, in which it is of interest to identify changes in other characteristics; for example, there has been work on identifying changes in authorship (Koppel et al., 2011) and poetic voice (Brooke et al., 2012). In this paper we investigate text segmentation on the basis of change in the native language of the writer. Two illustrative contexts where this task might be of interest are patchwriting detection and literary analysis. Patchwriting is the heavy use of text from a different source with some modification and insertion of additional words and sentences to form a new text. Pecorari (2003) notes that this is a kind of textual plagiarism, but is a strategy for learning to write in an appropriate language and style, rather than for deception. Keck (2006), Gilmore et al. (2010) and Vieyra et al. ("
P17-1134,W13-1406,0,0.0505659,"Missing"
P17-1134,C14-1185,0,0.247824,"Missing"
P17-1134,N13-1019,1,0.877831,"lexical cohesion — expressed in this context as topic segments having compact and consistent lexical distributions — and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment. Much other subsequent work either uses this as a baseline, or extends it in some way: Jeong and Titov (2010), for example, who propose a model for joint discourse segmentation and alignment for documents with parallel structures, such as a text with commentaries or presenting alternative views on the same topic; or Du et al. (2013), who use hierarchical topic structure to improve the linear segmentation. Bible Authorship Koppel et al. (2011) consider the task of decomposing a document into its authorial components based on their stylistic properties and propose an unsupervised method for doing so. The authors use as their data two biblical books, Jeremiah and Ezekiel, that are generally believed to be single-authored: their task was to segment a single artificial text constructed by interleaving chapters of the two books. Their most successful method used work in biblical scholarship on lexical choice: they give as an e"
P17-1134,N09-1040,0,0.176558,"hat incorporating knowledge about which features are associated with which L1 could potentially help improve the results. One approach to do this is the use of asymmetric priors. We note that features associated with an L1 often dominate in a segment. Accordingly, priors can represent evidence external to the data that some some aspect should be weighted more strongly: for us, this is evidence from the NLI classification task. The segmentation models discussed so far only make use of a symmetric prior but later work mentions that it would be possible to modify this to use an asymmetric prior (Eisenstein, 2009). Given that priors are effective for incorporating external information, recent work has highlighted the importance of optimizing over such priors, and in particular, the use of asymmetric priors. Key work on this is by Wallach et al. (2009) on LDA, who report that “an asymmetric Dirichlet prior over the document-topic distributions has substantial advantages over a symmetric prior”, 1462 with prior values being determined through hyperparameter optimization. Such methods have since been applied in other tasks such as sentiment analysis (Lin and He, 2009; Lin et al., 2012) to achieve substant"
P17-1134,D08-1035,0,0.195164,"ns, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm (Hearst, 1994, 1997) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts. There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised Bayesian technique BAYES S EG of Eisenstein and Barzilay (2008), based on a generative model that assumes that each segment has its own language model. Under this assumption the task can be framed as predicting boundaries at points which maximize the probability of a text being generated by a given language model. Their method is based on lexical cohesion — expressed in this context as topic segments having compact and consistent lexical distributions — and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment. Much other subsequent work either uses t"
P17-1134,W13-1729,0,0.0166827,"ate the importance of inducing a compact distribution, which we did here by reducing the vocabulary size by stripping non-informative features. 5.4 Applying Two Asymmetric Priors Our final model, L1S EG -A SYM P, assesses whether setting different priors for each L1 can improve performance. Our grid search over two priors gives 900 possible prior combinations. These combinations also include cases where θa and θb are symmetric, which is equivalent to the L1S EG -C OMP model. We observe (Table 1) that 8 In §2 we noted the comparison of PTB and CLAWS2 tagsets in Malmasi and Cahill (2015); also, Gyawali et al. (2013) compared Penn Treebank and Universal POS tagsets and found that the more fine-grained PTB ones did better. 1464 L1 A native L1s. There isn’t yet a topic-balanced corpus like T OEFL11 which includes native speaker writing for evaluation, although we expect (given recent results on distinguishing native from nonnative text in Malmasi and Dras (2015)) that the techniques should carry over. For the literary analysis, as well, to bridge the gap between work like Morzinski (1994) and a computational application, it remains to be seen how precise an annotation is possible for this task. Additionally"
P17-1134,P94-1002,0,0.847302,"esion (Halliday and Hasan, 1976) is an important concept here: the principle that text is not formed by a random set of words and sentences but rather logically ordered sets of related words that together form a topic. In addition to the semantic relation between words, other methods such as back-references and conjunctions also help achieve cohesion. Based on this, Morris and Hirst (1991) proposed the use of lexical chains, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm (Hearst, 1994, 1997) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts. There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised Bayesian technique BAYES S EG of Eisenstein and Barzilay (2008), based on a generative model that assumes that each segment has its own language model. Under this assumption the task can be framed as predicting boundarie"
P17-1134,J97-1003,0,0.895698,"er, stylistically expressed characteristics such as change of authorship or native language. We propose a Bayesian unsupervised text segmentation approach to the latter. While baseline models achieve essentially random segmentation on our task, indicating its difficulty, a Bayesian model that incorporates appropriately compact language models and alternating asymmetric priors can achieve scores on the standard metrics around halfway to perfect segmentation. 1 Introduction Most work on automatically segmenting text has been on the basis of topic: segment boundaries correspond to topic changes (Hearst, 1997). There are various contexts, however, in which it is of interest to identify changes in other characteristics; for example, there has been work on identifying changes in authorship (Koppel et al., 2011) and poetic voice (Brooke et al., 2012). In this paper we investigate text segmentation on the basis of change in the native language of the writer. Two illustrative contexts where this task might be of interest are patchwriting detection and literary analysis. Patchwriting is the heavy use of text from a different source with some modification and insertion of additional words and sentences to"
P17-1134,D14-1142,0,0.143057,"Missing"
P17-1134,P10-2028,0,0.0212724,"t each segment has its own language model. Under this assumption the task can be framed as predicting boundaries at points which maximize the probability of a text being generated by a given language model. Their method is based on lexical cohesion — expressed in this context as topic segments having compact and consistent lexical distributions — and implements this within a probabilistic framework by modelling words within each segment as draws from a multinomial language model associated with that segment. Much other subsequent work either uses this as a baseline, or extends it in some way: Jeong and Titov (2010), for example, who propose a model for joint discourse segmentation and alignment for documents with parallel structures, such as a text with commentaries or presenting alternative views on the same topic; or Du et al. (2013), who use hierarchical topic structure to improve the linear segmentation. Bible Authorship Koppel et al. (2011) consider the task of decomposing a document into its authorial components based on their stylistic properties and propose an unsupervised method for doing so. The authors use as their data two biblical books, Jeremiah and Ezekiel, that are generally believed to"
P17-1134,P06-1004,0,0.539462,"j} |θ0 ) = Segmentation Models For all of our segmentation we use as a starting point the unsupervised Bayesian method of Eisenstein and Barzilay (2008); see §2.3 We recap the important technical definitions here. In Equation 1 of their work they define the observation likelihood as, p(X |z, Θ) = T Y t p(xt |θzt ), (1) where X is the set of all T sentences, z is the vector of segment assignments for each sentence, xt is the bag of words drawn from the language model and Θ is the set of all K language models Θ1 . . . ΘK . As is standard in segmentation work, K is assumed to be fixed and known (Malioutov and Barzilay, 2006); it is set to the actual number of segments. The authors also impose an additional constraint, that zt must be equal to either zt−1 (the previous sentence’s segment) or zt−1 + 1 (the next segment), in order to ensure a linear segmentation. This segmentation model has two parameters: the set of language models Θ and the segment assignment indexes z. The authors note that since this task is only concerned with the segment assignments, searching in the space of language models is not desirable. They offer two alternatives to overcome this: (1) taking point estimates of the language models, which"
P17-1134,W15-0606,1,0.938516,"of 5 segments each for the single L1 pair, and from all language pairs (L1S EG -PTB-A LL D EV, L1S EG -PTB-A LL -T EST). We would expect that these datasets should not be segmentable by topic, as all the segments are on the same topic; the segments should however, differ in stylistic characteristics related to the L1. L1S EG -CLAWS2 This dataset is generated using the same methodology as L1S EG -PTB, with the exception that the essays are tagged using the RASP tagger which uses the more fine-grained CLAWS2 tagset, noting that the CLAWS2 tagset performed better in the NLI classification task (Malmasi and Cahill, 2015). 3.3 Evaluation We use the standard Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) metrics, which (broadly speaking) select sentences using a moving window of size k and determines whether these sentences correctly or incorrectly fall into the same or different reference segmentations. Pk and WD scores range between 0 and 1, with a lower score indicating better performance, and 0 a perfect segmentation. It has been noted that some “degenerate” algorithms — such as placing boundaries randomly or at every possible position — can score 0.5 (Pevzner and Hearst, 2002)."
P17-1134,D14-1144,1,0.910428,"Missing"
P17-1134,J91-1002,0,0.270624,"ation based on authorial characteristics, applied to native language. 2 Related Work Topic Segmentation The most widelyresearched text segmentation task has as its goal to divide a text into topically coherent segments. Lexical cohesion (Halliday and Hasan, 1976) is an important concept here: the principle that text is not formed by a random set of words and sentences but rather logically ordered sets of related words that together form a topic. In addition to the semantic relation between words, other methods such as back-references and conjunctions also help achieve cohesion. Based on this, Morris and Hirst (1991) proposed the use of lexical chains, sequences of related words (defined via thesaurus), to break up a text into topical segments: breaks in lexical chains indicate breaks in topic. The TextTiling algorithm (Hearst, 1994, 1997) took a related approach, defining a function over lexical frequency and distribution information to determine topic boundaries, and assuming that each topic has its own vocabulary and that large shifts in this vocabulary usage correspond to topic shifts. There have been many approaches since that time. A key one, which is the basis for our own work, is the unsupervised"
P17-1134,J02-1002,0,0.450824,"L -T EST). We would expect that these datasets should not be segmentable by topic, as all the segments are on the same topic; the segments should however, differ in stylistic characteristics related to the L1. L1S EG -CLAWS2 This dataset is generated using the same methodology as L1S EG -PTB, with the exception that the essays are tagged using the RASP tagger which uses the more fine-grained CLAWS2 tagset, noting that the CLAWS2 tagset performed better in the NLI classification task (Malmasi and Cahill, 2015). 3.3 Evaluation We use the standard Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) metrics, which (broadly speaking) select sentences using a moving window of size k and determines whether these sentences correctly or incorrectly fall into the same or different reference segmentations. Pk and WD scores range between 0 and 1, with a lower score indicating better performance, and 0 a perfect segmentation. It has been noted that some “degenerate” algorithms — such as placing boundaries randomly or at every possible position — can score 0.5 (Pevzner and Hearst, 2002). WD scores are typically similar to Pk , correcting for differential penalties between false positive boundaries"
P17-1134,N13-1009,0,0.041223,"Missing"
P17-1134,W13-1706,0,0.105765,"Missing"
P17-1134,C12-1158,0,0.107886,"Missing"
P17-1134,D12-1064,1,0.884455,"Missing"
P17-2063,hughes-etal-2006-reconsidering,0,0.211293,"LID and holds great promise for future work in this area. 1 Mark Dras Macquarie University Sydney, Australia mark.dras@mq.edu.au Introduction Language Identification (LID) is the task of determining the language of a text, at the document, sub-document or even sentence level. LID is a fundamental preprocessing task in NLP and is also used in lexicography, machine translation and information retrieval. It is widely used for filtering to select documents in a specific language; e.g. LID can filter webpages or tweets by language. Although LID has been widely studied, several open issues remain (Hughes et al., 2006). Current goals include developing models that can identify thousands of languages; extending the task to more fine-grained dialect identification; and making LID functionality more readily available to users/developers. A common challenge among these goals is dealing with high dimensional feature spaces. LID differs from traditional text categorization tasks in some important aspects. Standard tasks, such as topic classification, are usually 399 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 399–403 c Vancouver, Canada, July 30 -"
P17-2063,W15-5407,1,0.871205,"Missing"
P17-2063,W16-4801,1,0.852423,"Missing"
P17-2063,D14-1069,0,0.0621414,"Missing"
R15-1053,brooke-hirst-2012-measuring,0,0.0527535,"Missing"
R15-1053,guthrie-etal-2006-closer,0,0.0312481,"choices. In this work we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.4 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis (whose), ikke (not), jeg (I), s˚a (so) and hj˚a (at). We also make this list available on our website.5 In addition to single function words, we also extract function word bigrams, as described by Malmasi et al. (2013). Function word bigrams are a type of word n-gram where content words are skipped: they are thus a specific subtype of skipgram discussed by Guthrie et al. (2006). For example, the sentence We should all start taking the bus would be reduced to we should all the, from which we would extract the n-grams. Given its many morphosyntactic markers and detailed categories, the ASK dataset has a rich tagset with over 300 unique tags. 4 Experimental Methodology In this study we employ a supervised multi-class classification approach. The learner texts are organized into classes according to the author’s L1 and these documents are used for training and testing in our experiments. A diagram conceptualizing our NLI system is shown in Figure 2. 4.1 Classifier We us"
R15-1053,W13-1716,1,0.846813,"English function words have been found to be useful in studies of authorship attribution and NLI. Unlike POS tags, this model analyzes the author’s specific word choices. In this work we used a list of 176 function words obtained from the distribution of the Apache Lucene search engine software.4 This list includes stop words for the Bokm˚al variant of the language and contains entries such as hvis (whose), ikke (not), jeg (I), s˚a (so) and hj˚a (at). We also make this list available on our website.5 In addition to single function words, we also extract function word bigrams, as described by Malmasi et al. (2013). Function word bigrams are a type of word n-gram where content words are skipped: they are thus a specific subtype of skipgram discussed by Guthrie et al. (2006). For example, the sentence We should all start taking the bus would be reduced to we should all the, from which we would extract the n-grams. Given its many morphosyntactic markers and detailed categories, the ASK dataset has a rich tagset with over 300 unique tags. 4 Experimental Methodology In this study we employ a supervised multi-class classification approach. The learner texts are organized into classes according to the author’"
R15-1053,W15-0620,1,0.899129,"Missing"
R15-1053,W15-0606,1,0.871575,"Missing"
R15-1053,E14-4033,0,0.0367768,"Missing"
R15-1053,W14-3625,1,0.846376,"point to consider here is that this additional complexity also increases the possibility and number of potential learner errors. A more in-depth exposition of Norwegian syntax and morphology can be found in Haugen (2009). 3 150 Mean = 310.55 Std. Dev. = 15.291 … Frequency 100 50 0 275.00 295.00 315.00 335.00 355.00 Document Length (tokens) Figure 1: A histogram of the number of tokens per document in the dataset that we generated. Data In this work we extracted 750k tokens of text from the ASK corpus in the form of individual sentences. Following the methodology of Brooke and Hirst (2011) and Malmasi and Dras (2014b), we randomly select and combine the sentences from the same L1 to generate texts of approximately 300 tokens on average, creating a set of documents suitable for NLI. This methodology ensures that the texts for each L1 are a mix of different authorship styles, topics and proficiencies. It also means that all documents are similar and comparable in length. The 10 native languages and the number of texts generated per class are listed in Table 1. In addition to these we also generate 250 control texts written by natives. A histogram of the number of tokens per document is shown in Figure 1. T"
R15-1053,E14-4019,1,0.864397,"point to consider here is that this additional complexity also increases the possibility and number of potential learner errors. A more in-depth exposition of Norwegian syntax and morphology can be found in Haugen (2009). 3 150 Mean = 310.55 Std. Dev. = 15.291 … Frequency 100 50 0 275.00 295.00 315.00 335.00 355.00 Document Length (tokens) Figure 1: A histogram of the number of tokens per document in the dataset that we generated. Data In this work we extracted 750k tokens of text from the ASK corpus in the form of individual sentences. Following the methodology of Brooke and Hirst (2011) and Malmasi and Dras (2014b), we randomly select and combine the sentences from the same L1 to generate texts of approximately 300 tokens on average, creating a set of documents suitable for NLI. This methodology ensures that the texts for each L1 are a mix of different authorship styles, topics and proficiencies. It also means that all documents are similar and comparable in length. The 10 native languages and the number of texts generated per class are listed in Table 1. In addition to these we also generate 250 control texts written by natives. A histogram of the number of tokens per document is shown in Figure 1. T"
R15-1053,tenfjord-etal-2006-ask,0,0.59785,"Missing"
R15-1053,U14-1020,1,0.77517,"point to consider here is that this additional complexity also increases the possibility and number of potential learner errors. A more in-depth exposition of Norwegian syntax and morphology can be found in Haugen (2009). 3 150 Mean = 310.55 Std. Dev. = 15.291 … Frequency 100 50 0 275.00 295.00 315.00 335.00 355.00 Document Length (tokens) Figure 1: A histogram of the number of tokens per document in the dataset that we generated. Data In this work we extracted 750k tokens of text from the ASK corpus in the form of individual sentences. Following the methodology of Brooke and Hirst (2011) and Malmasi and Dras (2014b), we randomly select and combine the sentences from the same L1 to generate texts of approximately 300 tokens on average, creating a set of documents suitable for NLI. This methodology ensures that the texts for each L1 are a mix of different authorship styles, topics and proficiencies. It also means that all documents are similar and comparable in length. The 10 native languages and the number of texts generated per class are listed in Table 1. In addition to these we also generate 250 control texts written by natives. A histogram of the number of tokens per document is shown in Figure 1. T"
R15-1053,W15-5407,1,0.869865,"Missing"
R15-1053,C12-1158,0,0.0771052,"Missing"
R15-1053,N15-1160,1,0.663033,"Missing"
R15-1053,W13-1706,0,0.273777,"in distinguishing the different syntactic patterns used by different L1 groups. The availability of post-corrected POS tags in our data, as described in §3, can provide some insight into how much this issue affects NLI by comparing its performance with previously reported results. NLI work has been growing in recent years, using a wide range of syntactic and more recently, lexical features to distinguish the L1. A detailed review of NLI methods is omitted here for reasons of space, but a thorough exposition is presented in the report from the very first NLI Shared Task that was held in 2013 (Tetreault et al., 2013). Most English NLI work has been done using two corpora. The International Corpus of Learner English (Granger et al., 2009) was widely used until recently, despite its shortcomings1 being widely noted (Brooke and Hirst, 2012). More recently, T OEFL11, the first corpus designed for NLI was released (Blanchard et al., 2013). While it is the largest NLI dataset available, it only contains argumentative essays, limiting analyses to this genre. Research has also expanded to use non-English learner corpora (Malmasi and Dras, 2014a; Malmasi and Dras, 2014c). Recently, Malmasi and Dras (2014b) introdu"
R15-1053,N03-1033,0,0.0671314,"Missing"
R15-1053,W15-0614,1,0.875239,"Missing"
R15-1053,D11-1148,1,0.802039,"Missing"
R15-1053,D12-1064,1,0.874612,"Missing"
R15-1053,D14-1144,1,\N,Missing
S16-1153,E14-4019,1,0.903466,"Missing"
S16-1153,W15-5407,1,0.733845,"re. We describe this process below. 4.1 Ensemble Construction Our ensemble was created using a set of decision stump classifiers. A decision stump is a decision tree trained using only a single feature (Iba and Langley, 1992); it is usually considered a weak learner. We used the features listed in Section 3 to create an ensemble of 9 classifiers. Each classifier predicts every input and assigns a probability output to each of the two possible labels. Classifiers ensembles have proved to an efficient 993 and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a) and grammatical error detection (Xiang et al., 2015). This motivated us to try this approach in the CWI SemEval task. 4.2 Meta-classifier For our meta-classifier, we adopted a decision tree with bootstrap aggregating (bagging). The inputs to each decision tree are the two probability outputs from each decision stump in our ensemble, along with the original gold label. 200 bagged decision trees were created using this input. The final label was selected through a plurality voting process over the entire set of bagged decision trees. 4.3 Systems Using the methods described so far, we created"
S16-1153,P13-3015,0,0.178415,"short phrases for simpler ones to improve text readability and comprehension aimed at a given target population (e.g. children, language learners, people with reading impairment, etc.). Lexical simplification is considered to be the sub-task of text simplification that deals with the lexicon while other sub-tasks address, for example, complex syntactic structures (Siddharthan, 2014). To perform lexical simplification efficiently, computational methods should be first applied to identify which words in a text pose more difficulty to readers and they therefore good candidates for substitution (Shardlow, 2013). This task is called complex word In the example presented above, the CWI systems should label extraordinary, bipedal and locomotion as complex words.1 To accomplish this task, the MAZA team applied a decision stump meta-classifier and a wide set of features that we will describe here. 2 Data Organizers of the SemEval CWI task provided a training and test set comprising English sentences with each word annotated with a complex or simple label. According to the CWI task website2 : ‘400 annotators were presented with several sentences and asked to select which words they did not understand 1 No"
S16-1153,S12-1046,0,0.124254,"Missing"
S16-1153,W13-1706,0,0.0291774,"f 200 sentences. A word is considered to be complex if at least one of the 20 annotators assigned them as complex. Subsequently a test set with the same format was released containing 88,221 sentences. According to the organizers, the test set contains by judgments made over 9,000 sentences by a single annotator. The proportion of training vs. test instances of 1:40 should also be noted as it represents and additional challenge to participants. This data split is different from other similar text classification shared tasks which provide much more training than test instances (at least 10:1) (Tetreault et al., 2013; Zampieri et al., 2015). Given the amount of training data, participating teams should employ efficient algorithms able to perform generalizations on a much larger test set.3 3 Features We experimented with two types of features in our submissions. Each of these two classes, as described below, contains several features which we combine using a meta-classifier. 3.1 Frequency and Length Features These are features based on the occurrence of the target word in a given reference corpus and its length. The idea is inspired by the Zipfian frequency distribution of words that indicate that the most"
S16-1153,W15-4415,0,0.0299871,"ion Our ensemble was created using a set of decision stump classifiers. A decision stump is a decision tree trained using only a single feature (Iba and Langley, 1992); it is usually considered a weak learner. We used the features listed in Section 3 to create an ensemble of 9 classifiers. Each classifier predicts every input and assigns a probability output to each of the two possible labels. Classifiers ensembles have proved to an efficient 993 and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a) and grammatical error detection (Xiang et al., 2015). This motivated us to try this approach in the CWI SemEval task. 4.2 Meta-classifier For our meta-classifier, we adopted a decision tree with bootstrap aggregating (bagging). The inputs to each decision tree are the two probability outputs from each decision stump in our ensemble, along with the original gold label. 200 bagged decision trees were created using this input. The final label was selected through a plurality voting process over the entire set of bagged decision trees. 4.3 Systems Using the methods described so far, we created two different systems for the shared task. They are sum"
S16-1153,W15-5401,1,0.898649,"Missing"
S16-1154,E14-4019,1,0.88592,"Missing"
S16-1154,P13-3015,0,0.106496,"ormed by human annotators required to label the data.1 We present the description of the LTG entry in the SemEval-2016 Complex Word Identification (CWI) task, which aimed to develop systems for identifying complex words in English sentences. Our entry focused on the use of contextual language model features and the application of ensemble classification methods. Both of our systems achieved good performance, ranking in 2nd and 3rd place overall in terms of F-Score. 1 2 Introduction Complex Word Identification (CWI) is the task of identifying complex words in texts using computational methods (Shardlow, 2013). The task is usually carried out as part of lexical and text simplification systems. Shardlow (2014) considers CWI as the first processing step in lexical simplification pipelines. Complex or difficult words should first be identified so they can be later substituted by simpler ones to improve text readability. CWI has gained more importance in the last decade as lexical and text simplification systems have been developed or tailored for a number of purposes. They have been applied to make texts more accessible to language learners (Petersen and Ostendorf, 2007); other researchers have explor"
S19-2010,S19-2121,0,0.0487207,"Missing"
S19-2010,S19-2100,0,0.0543071,"Missing"
S19-2010,S19-2120,0,0.040833,"Missing"
S19-2010,S19-2110,0,0.0445018,"Missing"
S19-2010,S19-2129,0,0.043829,"Missing"
S19-2010,S19-2144,0,0.044782,"Missing"
S19-2010,W18-4411,0,0.0794205,"Aggression identification: The TRAC shared task on Aggression Identification (Kumar et al., 2018) provided participants with a dataset containing 15,000 annotated Facebook posts and comments in English and Hindi for training and validation. For testing, two different sets, one from Facebook and one from Twitter, were used. The goal was to discriminate between three classes: non-aggressive, covertly aggressive, and overtly aggressive. The best-performing systems in this competition used deep learning approaches based on convolutional neural networks (CNN), recurrent neural networks, and LSTM (Aroyehun and Gelbukh, 2018; Majumder et al., 2018). While each of the above tasks tackles a particular type of abuse or offense, there are many commonalities. For example, an insult targeted at an individual is commonly known as cyberbulling and insults targeted at a group are known as hate speech. The hierarchical annotation model proposed in OLID (Zampieri et al., 2019) and used in OffensEval aims to capture this. We hope that the OLID’s dataset would become a useful resource for various offensive language identification tasks. 3 The training and testing material for OffensEval is the aforementioned Offensive Languag"
S19-2010,W18-4416,0,0.0526243,"ings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 75–86 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 2 Related Work Toxic comments: The Toxic Comment Classification Challenge5 was an open competition at Kaggle, which provided participants with comments from Wikipedia organized in six classes: toxic, severe toxic, obscene, threat, insult, identity hate. The dataset was also used outside of the competition (Georgakopoulos et al., 2018), including as additional training material for the aforementioned TRAC shared (Fortuna et al., 2018). Different abusive and offense language identification problems have been explored in the literature ranging from aggression to cyber bullying, hate speech, toxic comments, and offensive language. Below we discuss each of them briefly. Aggression identification: The TRAC shared task on Aggression Identification (Kumar et al., 2018) provided participants with a dataset containing 15,000 annotated Facebook posts and comments in English and Hindi for training and validation. For testing, two different sets, one from Facebook and one from Twitter, were used. The goal was to discriminate between t"
S19-2010,S19-2111,0,0.0516978,"Missing"
S19-2010,S19-2131,0,0.076679,"Missing"
S19-2010,S19-2107,0,0.0613142,"Missing"
S19-2010,P11-2008,0,0.060614,"Missing"
S19-2010,S19-2098,1,0.882958,"Missing"
S19-2010,W15-4322,0,0.0652014,"Missing"
S19-2010,W18-4401,1,0.628306,"Facebook and Twitter. As manual filtering is very time consuming, and as it can cause post-traumatic stress disorder-like symptoms to human annotators, there have been many research efforts aiming at automating the process. The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content. Examples of offensive content studied in previous work include hate speech (Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), cyberbulling (Dinakar et al., 2011), and aggression (Kumar et al., 2018). Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018). Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval. Section 3 presents the shared task description and the subtasks included in OffensEval. Section 4 includes a brief description of OLID based on (Zampieri et al., 2019)."
S19-2010,S19-2139,0,0.0394818,"Missing"
S19-2010,S19-2011,0,0.137461,"Missing"
S19-2010,S19-2115,0,0.0421739,"Missing"
S19-2010,W18-4423,0,0.178724,"Missing"
S19-2010,S19-2116,0,0.0624146,"Missing"
S19-2010,malmasi-zampieri-2017-detecting,1,0.577817,"ears have seen the proliferation of offensive language in social media platforms such as Facebook and Twitter. As manual filtering is very time consuming, and as it can cause post-traumatic stress disorder-like symptoms to human annotators, there have been many research efforts aiming at automating the process. The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content. Examples of offensive content studied in previous work include hate speech (Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), cyberbulling (Dinakar et al., 2011), and aggression (Kumar et al., 2018). Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018). Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval. Section 3 presents the shared task description and the subtasks included in OffensEval."
S19-2010,S19-2109,0,0.0646835,"Missing"
S19-2010,S19-2134,0,0.0461997,"Missing"
S19-2010,P14-5010,0,0.00482749,"Missing"
S19-2010,S19-2105,0,0.0325559,"Missing"
S19-2010,D14-1162,0,0.0847332,"Missing"
S19-2010,S19-2127,0,0.0671146,"Missing"
S19-2010,S19-2103,0,0.0392423,"Missing"
S19-2010,N18-1202,0,0.0384382,"tators on the platform and by using test questions to discard annotators who did not achieve a certain threshold. All the tweets were annotated by two people. In case of disagreement, a third annotation was requested, and ultimately we used a majority vote. Examples of tweets from the dataset with their annotation labels are shown in Table 1. 5 Results The models used in the task submissions ranged from traditional machine learning, e.g., SVM and logistic regression, to deep learning, e.g., CNN, RNN, BiLSTM, including attention mechanism, to state-of-the-art deep learning models such as ELMo (Peters et al., 2018) and BERT (Devlin et al.). Figure 2 shows a pie chart indicating the breakdown by model type for all participating systems in sub-task A. Deep learning was clearly the most popular approach, as were also ensemble models. Similar trends were observed for subtasks B and C. Table 2: The teams that participated in OffensEval and submitted system description papers. 7 78 https://www.figure-eight.com/ The results for each of the sub-tasks are shown in Table 4. Due to the large number of submissions, we only show the F1-score for the top-10 teams, followed by result ranges for the rest of the teams."
S19-2010,S19-2118,0,0.0364634,"Missing"
S19-2010,S19-2104,0,0.0457917,"Missing"
S19-2010,S19-2123,0,0.1488,"Missing"
S19-2010,S19-2136,0,0.0650507,"Missing"
S19-2010,S19-2112,0,0.0343921,"Missing"
S19-2010,S19-2141,0,0.0614093,"Missing"
S19-2010,S19-2140,0,0.0351825,"Missing"
S19-2010,S19-2119,0,0.0413788,"Missing"
S19-2010,S19-2113,0,0.0437079,"Missing"
S19-2010,S19-2066,0,0.0526442,"Missing"
S19-2010,S19-2102,0,0.0607101,"Missing"
S19-2010,S19-2125,0,0.0321167,"Missing"
S19-2010,S19-2106,0,0.0611208,"Missing"
S19-2010,S19-2132,0,0.0476906,"Missing"
S19-2010,S19-2108,0,0.131809,"Missing"
S19-2010,W17-3012,0,0.152463,"ing the process. The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content. Examples of offensive content studied in previous work include hate speech (Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), cyberbulling (Dinakar et al., 2011), and aggression (Kumar et al., 2018). Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018). Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval. Section 3 presents the shared task description and the subtasks included in OffensEval. Section 4 includes a brief description of OLID based on (Zampieri et al., 2019). Section 5 discusses the participating systems and their results in the shared task. Finally, Section 6 concludes and suggests directions for future work. 1 http://competitions.codalab.org/ competitions/20011"
S19-2010,S19-2126,0,0.0759606,"Missing"
S19-2010,S19-2137,0,0.0528763,"Missing"
S19-2010,S19-2135,0,0.0463278,"Missing"
S19-2010,S19-2128,0,0.0505463,"Missing"
S19-2010,S19-2097,0,0.0448575,"Missing"
S19-2010,N12-1084,0,0.417325,"nsive language identification tasks. 3 The training and testing material for OffensEval is the aforementioned Offensive Language Identification Dataset (OLID) dataset, which was built specifically for this task. OLID was annotated using a hierarchical three-level annotation model introduced in Zampieri et al. (2019). Four examples of annotated instances from the dataset are presented in Table 1. We use the annotation of each of the three layers in OLID for a sub-task in OffensEval as described below. Bullying detection: There have been several studies on cyber bullying detection. For example, Xu et al. (2012) used sentiment analysis and topic models to identify relevant topics, and Dadvar et al. (2013) used user-related features such as the frequency of profanity in previous messages. Hate speech identification: This is the most studied abusive language detection task (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015). More recently, Davidson et al. (2017) presented the hate speech detection dataset with over 24,000 English tweets labeled as non offensive, hate speech, and profanity. 3.1 Sub-task A: Offensive language identification In this sub-task, the goal is to discriminate"
S19-2010,S19-2124,0,0.0512483,"Missing"
S19-2010,S19-2101,0,0.0376161,"Missing"
S19-2010,N19-1144,1,0.39554,"orizing Offensive Language in Social Media (OffensEval) Marcos Zampieri,1 Shervin Malmasi,2 Preslav Nakov,3 Sara Rosenthal,4 Noura Farra,5 Ritesh Kumar6 1 University of Wolverhampton, UK, 2 Amazon Research, USA 3 Qatar Computing Research Institute, HBKU, Qatar 4 IBM Research, USA, 5 Columbia University, USA, 6 Bhim Rao Ambedkar University, India m.zampieri@wlv.ac.uk Abstract Interestingly, none of this previous work has studied both the type and the target of the offensive language, which is our approach here. Our task, OffensEval1 , uses the Offensive Language Identification Dataset (OLID)2 (Zampieri et al., 2019), which we created specifically for this task. OLID is annotated following a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account. Thus, it can relate to phenomena captured by previous datasets such as the one by Davidson et al. (2017). Hate speech, for example, is commonly understood as an insult targeted at a group, whereas cyberbulling is typically targeted at an individual. We defined three sub-tasks, corresponding to the three levels in our annotation schema:3 We present the results and the main findings of SemEval-2019 Task"
S19-2010,S19-2130,0,0.0564906,"Missing"
S19-2010,S19-2142,0,0.0349466,"Missing"
S19-2010,S19-2117,0,0.0415284,"Missing"
S19-2010,S19-2138,0,0.094085,"Missing"
S19-2010,S19-2143,0,0.05351,"Missing"
S19-2093,S19-2007,0,0.0280948,"ns, then a second set of layers produces a final representation of the sentence. Finally, this representation is passed through a softmax dense layer that produces a final classification label. For each language, we created two variants of UTFPR: one trained exclusively over the training data provided by the organizers (UTFPR/O), and another that uses a pre-trained set of character-toword RNN layers extracted from the models introduced by Paetzold (2018) (UTFPR/W). The pretrained model was trained for the English multiclass classification Emotion Analysis shared task Task Description HatEval (Basile et al., 2019) provides participants with annotated datasets to create systems capable of properly identifying hate speech in tweets writ1 https://competitions.codalab.org/ competitions/20011 520 Figure 1: Architecture of the UTFPR models. missions for English, and 31st out of 35 valid submissions for Spanish. of WASSA 2018, which featured a training set of 153, 383 instances composed of a tweet and an emotion label. This pre-trained model for English was used for the UTFPR/W variant of both languages, since we wanted to test the hypothesis that pre-training a character-to-word RNN on a large dataset for En"
S19-2093,W18-6224,1,0.837176,"ectly predicting all three of the aforementioned labels. In this paper, we focus on Task A exclusively, for both English and Spanish. We participated in the competition using the team name UTFPR. 4 The UTFPR Models The UTFPR models are minimalistic Recurrent Neural Networks (RNNs) that learn compositional numerical representations of words based on the sequence of characters that compose them, then use them to learn a final representation for the sentence being analyzed. These models, of which the architecture is illustrated in Figure 1, are somewhat similar to those of Ling et al. (2015) and Paetzold (2018), who use RNNs to create compositional neural models for different tasks. As illustrated, the UTFPR models take as input a sentence, split it into words, then split the words into a sequence of characters in order to pass them through a character embedding layer. The character embeddings are passed onto a set of bidirectional RNN layers that produces word representations, then a second set of layers produces a final representation of the sentence. Finally, this representation is passed through a softmax dense layer that produces a final classification label. For each language, we created two v"
S19-2093,W17-1101,0,0.0327898,"vesting in ways to cope with such content using human moderation of posts, triage of content, deletion of offensive posts, and banning abusive users. One of the most common and effective strategies to tackle the problem of offensive language online is to train systems capable of recognizing such content. Several studies have been published in the last few years on identifying abusive language (Nobata et al., 2016), cyber aggression (Kumar et al., 2018), cyber bullying (Dadvar et al., 2013), and hate speech (Burnap and Williams, 2015; Davidson et al., 2017). As evidenced in two recent surveys (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018) and in a number of other 2 Related Work As evidenced in the introduction of this paper, there have been a number of studies on automatic hate speech identification published in the last few years. One of the most influential recent papers on hate speech identification is the one by Davidson et al. (2017). In this paper, the authors presented the Hate Speech Detection dataset which contains posts retrieved from social media labeled with three categories: OK (posts not containing profanity or hate speech), Offensive (posts containing swear words and general profanity),"
S19-2093,W17-3012,0,0.0138337,"ation for Computational Linguistics gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity (Malmasi and Zampieri, 2018). Along with the recent studies published, there have been a few related shared tasks organized on the topic. These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval1 (Zampieri et al., 2019b) for English. The latter is also organized within the scope of SemEval-2019. OffensEval considers offensive language in general whereas HatEval focuses on hate speech. Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit. Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important role in the identification and the definition of hate speech when compared to other forms of abusive content. The two SemEval-2019 shared tasks, HatEval and OffensEval, both include a sub-task"
S19-2093,N19-1144,1,0.884481,"in swear words. It has been ar519 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 519–523 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity (Malmasi and Zampieri, 2018). Along with the recent studies published, there have been a few related shared tasks organized on the topic. These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval1 (Zampieri et al., 2019b) for English. The latter is also organized within the scope of SemEval-2019. OffensEval considers offensive language in general whereas HatEval focuses on hate speech. Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit. Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important ro"
S19-2093,W17-3013,0,0.0670864,"Missing"
S19-2093,S19-2010,1,0.83211,"in swear words. It has been ar519 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 519–523 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics gued that annotating texts with respect to the presence of hate speech has an intrinsic degree of subjectivity (Malmasi and Zampieri, 2018). Along with the recent studies published, there have been a few related shared tasks organized on the topic. These include GermEval (Wiegand et al., 2018) for German, TRAC (Kumar et al., 2018) for English and Hindi, and OffensEval1 (Zampieri et al., 2019b) for English. The latter is also organized within the scope of SemEval-2019. OffensEval considers offensive language in general whereas HatEval focuses on hate speech. Waseem et al. (2017) proposes a typology of abusive language detection sub-tasks taking two factors into account: the target of the message and whether the content is explicit or implicit. Considering that hate speech is commonly understood as speech attacking a group based on ethnicity, religion, etc, and that cyber bulling, for example, is understood as an attack towards an individual, the target factor plays an important ro"
S19-2093,W18-4401,1,0.891356,"Missing"
S19-2093,D15-1176,0,0.0398909,"ful. • Sub-task B: Correctly predicting all three of the aforementioned labels. In this paper, we focus on Task A exclusively, for both English and Spanish. We participated in the competition using the team name UTFPR. 4 The UTFPR Models The UTFPR models are minimalistic Recurrent Neural Networks (RNNs) that learn compositional numerical representations of words based on the sequence of characters that compose them, then use them to learn a final representation for the sentence being analyzed. These models, of which the architecture is illustrated in Figure 1, are somewhat similar to those of Ling et al. (2015) and Paetzold (2018), who use RNNs to create compositional neural models for different tasks. As illustrated, the UTFPR models take as input a sentence, split it into words, then split the words into a sequence of characters in order to pass them through a character embedding layer. The character embeddings are passed onto a set of bidirectional RNN layers that produces word representations, then a second set of layers produces a final representation of the sentence. Finally, this representation is passed through a softmax dense layer that produces a final classification label. For each langua"
S19-2093,malmasi-zampieri-2017-detecting,1,\N,Missing
U14-1020,C12-1025,0,0.0932506,"ed for topic, we do not consider the use of purely lexical features such as word n-grams in this study. Topic bias can occur as a result of the subject matters or topics of the texts to be classified not not evenly distributed across the classes. For example, if in our training data all the texts written by English L1 speakers are on topic A, while all the French L1 authors write about topic B, then we have implicitly trained our classifier on the topics as well. In this case the classifier learns to distinguish our target variable through another confounding variable. Others researchers like Brooke and Hirst (2012), however, argue that lexical features cannot be simply ignored. Given the small size of our data and 4 http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 141 Finnish Function Words Part-of-Speech tag n-grams In this model POS n-grams of size 1–3 were extracted. These n-grams capture small and very local syntactic patterns of language production and were used as classification features. Previous work and our experiments showed that sequences of size 4 or greater achieve lower accuracy, possibly due to data sparsity, so we do not include them. 5.3 Character n-grams This is a sub-lexical feature that"
U14-1020,W14-3625,1,0.431244,"ast few years. This surge of interest, coupled with the inaugural shared task in 2013 have resulted in NLI becoming a well-established NLP task. The NLI Shared Task in 2013 was attended by 29 teams from the NLP and SLA areas. An overview of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field has successfully presented the first applications of NLI to a large non-English datasets (Malmasi and Dras, 2014b; Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese and L2 Arabic texts. Finnish poses a particular challenge. In terms of morphological complexity, it is among the world’s most extreme: its number of cases, for example, places it in the highest category in the comparative World Atlas of Language Structures (Iggesen, 2013). Comrie (1989) proposed two scales for characterising morphology, the index of synthesis (based on the number of categories expressed per morpheme) and the index of fusion (based on the number of categories expressed per"
U14-1020,E14-4019,1,0.894263,"ast few years. This surge of interest, coupled with the inaugural shared task in 2013 have resulted in NLI becoming a well-established NLP task. The NLI Shared Task in 2013 was attended by 29 teams from the NLP and SLA areas. An overview of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field has successfully presented the first applications of NLI to a large non-English datasets (Malmasi and Dras, 2014b; Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese and L2 Arabic texts. Finnish poses a particular challenge. In terms of morphological complexity, it is among the world’s most extreme: its number of cases, for example, places it in the highest category in the comparative World Atlas of Language Structures (Iggesen, 2013). Comrie (1989) proposed two scales for characterising morphology, the index of synthesis (based on the number of categories expressed per morpheme) and the index of fusion (based on the number of categories expressed per"
U14-1020,W14-3708,1,0.792786,"ast few years. This surge of interest, coupled with the inaugural shared task in 2013 have resulted in NLI becoming a well-established NLP task. The NLI Shared Task in 2013 was attended by 29 teams from the NLP and SLA areas. An overview of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field has successfully presented the first applications of NLI to a large non-English datasets (Malmasi and Dras, 2014b; Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese and L2 Arabic texts. Finnish poses a particular challenge. In terms of morphological complexity, it is among the world’s most extreme: its number of cases, for example, places it in the highest category in the comparative World Atlas of Language Structures (Iggesen, 2013). Comrie (1989) proposed two scales for characterising morphology, the index of synthesis (based on the number of categories expressed per morpheme) and the index of fusion (based on the number of categories expressed per"
U14-1020,W13-1716,1,0.704144,"sh function words5 were extracted from the learner texts and used as features in this model. 5.2 4.2 Evaluation Methodology Consistent with most previous NLI studies and the NLI 2013 shared task, we report results as classification accuracy under k-fold cross-validation, with k = 10. In recent years this has become a de facto standard for reporting NLI results. 5 Experiments We experiment using three different feature types described in this section. Previous NLI research on English data has utilized a range of features types varying from surface features to more sophisticated syntactic ones (Malmasi et al., 2013). However, in most such studies the use of such deeper features is predicated on the availability of NLP tools and models for extracting those features. This, unfortunately, is not the case for Finnish and it was decided to make use of a simpler feature set in this preliminary study. As our data is not balanced for topic, we do not consider the use of purely lexical features such as word n-grams in this study. Topic bias can occur as a result of the subject matters or topics of the texts to be classified not not evenly distributed across the classes. For example, if in our training data all th"
U14-1020,E14-4033,0,0.126596,"Missing"
U14-1020,C12-1158,0,0.0549664,"ning improvements (Laufer and Girsai, 2008). There are a number of avenues for future work. A key limitation of this study, although beyond our control, is the limited amount of data used. We hope to evaluate our system on larger data as it becomes available. The application of more linguistically sophisticated features also merits further investigation, but this is limited by the availability of Finnish NLP tools and resources. Another possible improvement is the use of classifier ensembles to improve classification accuracy. This has previously been applied to English NLI with good results (Tetreault et al., 2012). We would also like to point to the failure to distinguish between the L2 and any other acquired languages as a more general criticism of the NLI literature to date. The current body of NLI literature fails to distinguish whether the learner language is in fact the writer’s second language, or whether it is possibly a third language (L3). It has been noted in the SLA literature that when acquiring an L3, there may be instances of both L1- and L2-based transfer effects on L3 production (Ringbom, 2001). Studies of such second language transfer effects during third language acquisition have been"
U14-1020,W13-1706,0,0.554553,"ive computational models in NLP (Jarvis and Crossley, 2012). Such analyses have traditionally been conducted manually by researchers, and the issues that arise when they are attempted on large corpora are well known (Ellis, 2008). Recently, researchers have noted that NLP has the tools to use large amounts of data to automate this analysis, using complex feature types. This has motivated studies in Native Language Identification (NLI), a subtype of text classification where the goal is to determine the native language (L1) of an author using texts they have written in a second language or L2 (Tetreault et al., 2013). Most work in SLA, NLI and NLP for that matter has dealt with English. This is largely due to the fact that since World War II, the world has witnessed the ascendancy of English as its lingua franca. While English is the native language of over 400 million people in the U.S., U.K. and the Commonwealth, there are also over a billion people who speak English as their second or foreign language (Guo and Beckett, 2007). This has created a global environment where learning multiple languages is not exceptional and this has fueled the growing research into language acquisition. However, while Engli"
U14-1020,D14-1144,1,\N,Missing
U14-1021,D11-1120,0,0.0331707,"d to language technology, given that names are found in social networks, news articles and many other document types. Inference of demographic details from social media and online content is useful for marketing, personalization, and forensic purposes and gender prediction has received much attention (Peersman et al., 2011; Argamon et al., 2007). Shervin Malmasi. 2014. A Data-driven Approach to Studying Given Names and their Gender and Ethnicity Associations. In Proceedings of Australasian Language Technology Association Workshop, pages 145−149. In a study on discriminating gender on Twitter, Burger et al. (2011) used names and screen names as features in their classification system, finding over 400k distinct values for each feature. They found the features to be highly discriminative and informative for this task. Similarly, Tang et al. (2011) take a name-centric approach to gender classification for Facebook, reporting that first names are highly informative for the task. Name-gender info is also used in the NLP task of co-reference resolution and the state-of-theart Stanford Deterministic Coreference Resolution System (Lee et al., 2011) uses a list of male and female names to resolve anaphora. How"
U14-1021,W10-1737,0,0.0302594,"Missing"
U14-1021,W11-1902,0,0.0142529,"s 145−149. In a study on discriminating gender on Twitter, Burger et al. (2011) used names and screen names as features in their classification system, finding over 400k distinct values for each feature. They found the features to be highly discriminative and informative for this task. Similarly, Tang et al. (2011) take a name-centric approach to gender classification for Facebook, reporting that first names are highly informative for the task. Name-gender info is also used in the NLP task of co-reference resolution and the state-of-theart Stanford Deterministic Coreference Resolution System (Lee et al., 2011) uses a list of male and female names to resolve anaphora. However, more generic approaches that use probabilistic models of name features have also been recently applied for this task (Le Nagard and Koehn, 2010). Name information has also been used in text mining (Patman and Thompson, 2003). One example is in the field of Onomastics where publicly available name information can be used to to infer diversity and gender statistics in various areas. In computer vision, name information from associated text or captions has been used to aid image-based gender classifiers (Gallagher and Chen, 2008)"
U15-1008,J92-4003,0,0.622882,"ructure and the sequence of sentences in the text. Domain-specific 3 Word Representations Word representations are mathematical objects associated with words. This representation is often, but not always, a vector where each dimension is a word feature (Turian et al., 2010). Various methods for inducing word representations have been proposed. These include distributional represen67 3.2 tations, such as LSA, LSI and LDA, as well as distributed representations, also known as word embeddings. Yet another type of representation is based on inducing a clustering over words, with Brown clustering (Brown et al., 1992) being the most well known method. This is the approach that we take in the present study. Recent work has demonstrated that unsupervised word representations induced from large unlabelled data can be used to improve supervised tasks, a type of semi-supervised learning. Examples of tasks where this has been applied include dependency parsing (Koo et al., 2008), Named Entity Recognition (NER) (Miller et al., 2004), sentiment analysis (Maas et al., 2011) and chunking (Turian et al., 2010). Such an approach could also be applied to the clinical information extraction task where although we only h"
U15-1008,U15-1006,1,0.883866,"Missing"
U15-1008,U11-1012,0,0.323456,"Missing"
U15-1008,P09-1056,0,0.0605632,"Missing"
U15-1008,E09-1070,0,0.0400976,"Missing"
U15-1008,N13-1039,0,0.0619871,"Missing"
U15-1008,P08-1068,0,0.0446688,"ributional represen67 3.2 tations, such as LSA, LSI and LDA, as well as distributed representations, also known as word embeddings. Yet another type of representation is based on inducing a clustering over words, with Brown clustering (Brown et al., 1992) being the most well known method. This is the approach that we take in the present study. Recent work has demonstrated that unsupervised word representations induced from large unlabelled data can be used to improve supervised tasks, a type of semi-supervised learning. Examples of tasks where this has been applied include dependency parsing (Koo et al., 2008), Named Entity Recognition (NER) (Miller et al., 2004), sentiment analysis (Maas et al., 2011) and chunking (Turian et al., 2010). Such an approach could also be applied to the clinical information extraction task where although we only have a very limited amount of labelled data, large-scale unlabelled data — hundreds of millions of tokens — is readily available to us. Researchers have noted a number of advantages to using word representations in supervised learning tasks. They produce substantially more compact models compared to fully lexicalized approaches where feature vectors have the sa"
U15-1008,P11-1015,0,0.0399372,"ations, also known as word embeddings. Yet another type of representation is based on inducing a clustering over words, with Brown clustering (Brown et al., 1992) being the most well known method. This is the approach that we take in the present study. Recent work has demonstrated that unsupervised word representations induced from large unlabelled data can be used to improve supervised tasks, a type of semi-supervised learning. Examples of tasks where this has been applied include dependency parsing (Koo et al., 2008), Named Entity Recognition (NER) (Miller et al., 2004), sentiment analysis (Maas et al., 2011) and chunking (Turian et al., 2010). Such an approach could also be applied to the clinical information extraction task where although we only have a very limited amount of labelled data, large-scale unlabelled data — hundreds of millions of tokens — is readily available to us. Researchers have noted a number of advantages to using word representations in supervised learning tasks. They produce substantially more compact models compared to fully lexicalized approaches where feature vectors have the same length as the entire vocabulary and suffer from sparsity. They better estimate the values f"
U15-1008,E14-4019,1,0.770194,"Missing"
U15-1008,P10-1040,0,0.580479,"pt has been made to leverage large-scale unlabelled data. The main aim of this work is to evaluate the feasibility of such an approach. been devised to automatically recognise these scientific artefacts in publications (Hassanzadeh et al., 2014a). The most common approach, as discussed in §2, is the use of supervised learning to classify sentences into the various categories. Separately, another recent trend in Natural Language Processing (NLP) has been the use of word representations to integrate large amounts of unlabelled data into such supervised tasks, a form of semi-supervised learning (Turian et al., 2010). This is something that has not been applied to scientific artefacts extraction. Accordingly, the primary aim of the present work is to draw together the two areas, evaluating the utility of word representations for this task and comparing them against the most commonly used features to see if they can enhance accuracy. A secondary goal is to inspect the induced word representations and the resulting discriminative models to gain further insights about this approach. The paper is structured as follows. We present related work on biomedical information extraction in §2. Word representations ar"
U15-1008,N15-1160,1,0.860043,"Missing"
U15-1008,D12-1053,0,0.0547617,"Missing"
U15-1008,W15-0620,1,0.785459,"s, they did not provide any improvement over the standard Brown features. 4.4 Evaluation We report our results as classification accuracy under k-fold cross-validation, with k = 10. These results are compared against a majority baseline and an oracle. The oracle considers the predictions by all the classifiers in Table 3 and will assign the correct class label for an instance if at least one of the the classifiers produces the correct label for that data point. This approach can help us quantify the potential upper limit of a classification system’s performance on the given data and features (Malmasi et al., 2015). In sum, these results show that Brown clusters, using far fewer features, can outperform the widely used word features. 7 e.g. Our own analysis showed that O UTCOME sentences contained substantially more past tense verbs, comparative adverbs and comparative adjectives. 70 Class Clusters of words BACKGROUND [have has had] — [describes presents examines discusses summarizes addresses] [objectives goal] — [emerged evolved attracted fallen arisen risen proliferated] I NTERVENTION [received underwent undergoing taking] — [gel cream spray ointment] [orally intravenously subcutaneously intramuscula"
U15-1008,N04-1043,0,0.321998,"and LDA, as well as distributed representations, also known as word embeddings. Yet another type of representation is based on inducing a clustering over words, with Brown clustering (Brown et al., 1992) being the most well known method. This is the approach that we take in the present study. Recent work has demonstrated that unsupervised word representations induced from large unlabelled data can be used to improve supervised tasks, a type of semi-supervised learning. Examples of tasks where this has been applied include dependency parsing (Koo et al., 2008), Named Entity Recognition (NER) (Miller et al., 2004), sentiment analysis (Maas et al., 2011) and chunking (Turian et al., 2010). Such an approach could also be applied to the clinical information extraction task where although we only have a very limited amount of labelled data, large-scale unlabelled data — hundreds of millions of tokens — is readily available to us. Researchers have noted a number of advantages to using word representations in supervised learning tasks. They produce substantially more compact models compared to fully lexicalized approaches where feature vectors have the same length as the entire vocabulary and suffer from spa"
U15-1008,W01-0521,0,\N,Missing
U15-1018,N01-1014,0,0.13991,"Missing"
U15-1018,E14-4019,1,0.891074,"Missing"
U15-1018,N15-1160,1,0.857019,"Missing"
U15-1018,W13-1718,0,0.0675242,"Missing"
U15-1018,U14-1003,0,0.0368465,"Missing"
U18-1012,W16-0314,1,0.929087,"thods to categorize patent applications according to standardized taxonomies such as the International Patent Classification (IPC)2 as discussed in the studies by Benzineb and Guyot (2011); Fall et al. (2003). In this paper, we present a system to automatically categorize patent applications from Australia according to the top sections of the IPC taxonomy using a dataset provided by the organizers of the ALTA 2018 shared task on Classifying Patent Applications (Molla and Seneviratne, 2018).3 The dataset and the taxonomy are presented in more detail in Section 3. Building on our previous work (Malmasi et al., 2016a; Malmasi and Zampieri, 2017), our system is based on SVM ensembles and it achieved the highest performance of the competition. 2 Related Work There have been a number of studies applying NLP and Information Retrieval (IR) methods to patent applications specifically, and to legal texts in general, published in the last few years. Applications of NLP and IR to legal texts include the use of text summarization methods (Farzindar and Lapalme, 2004) to summarize legal documents and most recently, court ruling prediction. A few papers have been published on this topic, such as the one by Katz et a"
U18-1012,U18-1011,0,0.0202833,"NLP methods in patent application processing systems as evidenced in Section 2. One such example is the use of text classification methods to categorize patent applications according to standardized taxonomies such as the International Patent Classification (IPC)2 as discussed in the studies by Benzineb and Guyot (2011); Fall et al. (2003). In this paper, we present a system to automatically categorize patent applications from Australia according to the top sections of the IPC taxonomy using a dataset provided by the organizers of the ALTA 2018 shared task on Classifying Patent Applications (Molla and Seneviratne, 2018).3 The dataset and the taxonomy are presented in more detail in Section 3. Building on our previous work (Malmasi et al., 2016a; Malmasi and Zampieri, 2017), our system is based on SVM ensembles and it achieved the highest performance of the competition. 2 Related Work There have been a number of studies applying NLP and Information Retrieval (IR) methods to patent applications specifically, and to legal texts in general, published in the last few years. Applications of NLP and IR to legal texts include the use of text summarization methods (Farzindar and Lapalme, 2004) to summarize legal docu"
U18-1012,sulea-etal-2017-predicting,1,0.874672,"Missing"
U18-1012,W04-1006,0,0.0432954,"Missing"
U18-1012,W13-1728,1,0.82671,"zers of the ALTA 2018 shared task consists of a collection of Australian patent applications. The dataset contains 5,000 documents released for training and 1,000 documents for testing. The classes relevant for the task consisted of eight different main branches of the WIPO class ontology as follows: 4.2 Features For feature extraction we used and extended the methods reported in Malmasi and Zampieri (2017). Term Frequency (TF) of n-grams with n ranging from 3 to 6 for characters and 1-2 for words have been used. Along with term frequency we calculated the inverse document frequency (TF-IDF) (Gebre et al., 2013) which resulted in the best single feature set for prediction. • A: Human necessities; • B: Performing operations, transporting; • C: Chemistry, metallurgy; 4.3 • D: Textiles, paper; Classifier We used an ensemble-based classifier for this task. Our base classifiers are linear Support Vector Machines (SVM). SVMs have proven to deliver very good performance in a number of text classification problems. It was previously used for complex word identification (Malmasi et al., 2016a), triage of forum posts (Malmasi et al., 2016b), dialect identification (Malmasi and Zampieri, 2017), hate speech dete"
U18-1012,W18-3901,1,0.894099,"Missing"
U18-1012,W18-3929,0,0.127619,"Missing"
U18-1012,S16-1154,1,0.901714,"thods to categorize patent applications according to standardized taxonomies such as the International Patent Classification (IPC)2 as discussed in the studies by Benzineb and Guyot (2011); Fall et al. (2003). In this paper, we present a system to automatically categorize patent applications from Australia according to the top sections of the IPC taxonomy using a dataset provided by the organizers of the ALTA 2018 shared task on Classifying Patent Applications (Molla and Seneviratne, 2018).3 The dataset and the taxonomy are presented in more detail in Section 3. Building on our previous work (Malmasi et al., 2016a; Malmasi and Zampieri, 2017), our system is based on SVM ensembles and it achieved the highest performance of the competition. 2 Related Work There have been a number of studies applying NLP and Information Retrieval (IR) methods to patent applications specifically, and to legal texts in general, published in the last few years. Applications of NLP and IR to legal texts include the use of text summarization methods (Farzindar and Lapalme, 2004) to summarize legal documents and most recently, court ruling prediction. A few papers have been published on this topic, such as the one by Katz et a"
U18-1012,W17-1220,1,0.848826,"This was done be replacing all consecutive non-alphanumeric characters with a single space. Next, we converted the text to lowercase and removed any tokens representing numbers. Data The dataset released by the organizers of the ALTA 2018 shared task consists of a collection of Australian patent applications. The dataset contains 5,000 documents released for training and 1,000 documents for testing. The classes relevant for the task consisted of eight different main branches of the WIPO class ontology as follows: 4.2 Features For feature extraction we used and extended the methods reported in Malmasi and Zampieri (2017). Term Frequency (TF) of n-grams with n ranging from 3 to 6 for characters and 1-2 for words have been used. Along with term frequency we calculated the inverse document frequency (TF-IDF) (Gebre et al., 2013) which resulted in the best single feature set for prediction. • A: Human necessities; • B: Performing operations, transporting; • C: Chemistry, metallurgy; 4.3 • D: Textiles, paper; Classifier We used an ensemble-based classifier for this task. Our base classifiers are linear Support Vector Machines (SVM). SVMs have proven to deliver very good performance in a number of text classificati"
W13-1716,P06-4020,0,0.0157839,"Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w POS n-grams Most studies have found that POS tag n-grams are a very useful feature for NLI (Koppel et al., 2005; Bykh and Meurers, 2012, for example). The tagset provided by the Penn TreeBank is the most widely used in these experiments, with tagging performed by the Stanford Tagger (Toutanova et al., 2003). We investigate the effect of tagset granularity on classification accuracy by comparing the classification accuracy of texts tagged with the PTB tagset against those annotated by the RASP Tagger (Briscoe et al., 2006). The PTB POS tagset contains 36 unique tags, while the RASP system uses a subset of the CLAWS2 tagset, consisting of 150 tags. This is a significant size difference and we hypothesize that a larger tagset could provide richer levels of syntactically meaningful info which is more fine-grained in distinction between syntactic categories and contains more morpho-syntactic information such as gender, number, person, case and tense. For example, while the PTB tagset has four tags for pronouns (PRP, PRP$, WP, WP$), the CLAWS tagset provides over 20 pronoun tags (PPHO1, PPIS1, PPX2, PPY, etc.) disti"
W13-1716,C12-1025,0,0.317902,"e Identification (NLI) noted by the shared task organisers, there are two trends in recent work in particular that we considered in building our submission. The first is the proposal and use of new features that might have relevance to NLI: for example, Wong and Dras (2011), motivated by the Contrastive Analysis Hypothesis (Lado, 1957) from the field of Second Language Acquisition, introduced A second trend, most apparent in 2012, was the examination of other corpora besides the International Corpus of Learner English used in earlier work, and in particular the use of cross-corpus evaluation (Brooke and Hirst, 2012; Tetreault et al., 2012) to avoid topic bias in determining native language. Possible topic bias had been a reason for avoiding a full range of n-grams, in particular those containing content words (Koppel et al., 2009); the development of new corpora and the analysis of the effect of topic bias mitigated this. The consequent use of a full range of n-grams further reinforced the view that novel features were unlikely to be a major source of interesting results. We therefore concentrated on two areas: the use of classifier ensembles, and the choice of part-ofspeech tags. With classifier ensemb"
W13-1716,C12-1027,0,0.0298664,"e n-grams. Character n-grams Tsur and Rappoport (2007) demonstrated that character n-grams are a useful feature for NLI. These n-grams can be considered as a sub-word feature and their effectiveness is hypothesized to be a result of phoneme transfer from the writer’s L1. They can also capture orthographic conventions of a language. Accordingly, we limit our n-grams to a maximum size of 3 as longer sequences would correspond to short words and not phonemes or syllables. Word n-grams There has been a shift towards the use of word-based features in several recent studies (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Tetreault et al., 2012), with new corpora come into use for NLI and researchers exploring and addressing the issues relating to topic bias that previously prevented their use. Lexical choice is considered to be a prime feature for studying language transfer effects, and researchers have found word n-grams to be one of the strongest features for NLI. Tetreault et al. (2012) expanded on this by integrating 5-gram language models into their system. While we did not replicate this, we made use of word trigrams. 3.2 mars where each is associated with a different set of vocabulary: either pure POS"
W13-1716,P07-2009,0,0.0171866,"Missing"
W13-1716,de-marneffe-etal-2006-generating,0,0.00600023,"Missing"
W13-1716,E03-1068,0,0.0975297,"Missing"
W13-1716,guthrie-etal-2006-closer,0,0.113095,"auxiliary verbs. They have been widely used in studies of authorship attribution as well as NLI and established to be informative for these tasks. We use the list of 398 common English function words from Wong and Dras (2011). We also tested smaller sets, but observed that the larger sets achieve higher accuracy. Function Word n-grams We devised and tested a new feature that attempts to capture patterns of function word use at the sentence level. We define function word n-grams as a type of word n-gram where content words are skipped: they are thus a specific subtype of skip-gram discussed by Guthrie et al. (2006). For example, the sentence We should all start taking the bus would be reduced to we should all the, from which we would extract the n-grams. Character n-grams Tsur and Rappoport (2007) demonstrated that character n-grams are a useful feature for NLI. These n-grams can be considered as a sub-word feature and their effectiveness is hypothesized to be a result of phoneme transfer from the writer’s L1. They can also capture orthographic conventions of a language. Accordingly, we limit our n-grams to a maximum size of 3 as longer sequences would correspond to short words and not phonemes or sylla"
W13-1716,P10-1117,0,0.022804,"archers exploring and addressing the issues relating to topic bias that previously prevented their use. Lexical choice is considered to be a prime feature for studying language transfer effects, and researchers have found word n-grams to be one of the strongest features for NLI. Tetreault et al. (2012) expanded on this by integrating 5-gram language models into their system. While we did not replicate this, we made use of word trigrams. 3.2 mars where each is associated with a different set of vocabulary: either pure POS or the mixture of POS and function words. We use the grammar proposed by Johnson (2010) for capturing topical collocations as presented below: Sentence → Docj Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w POS n-grams Most studies have found that POS tag n-grams are a very useful feature for NLI (Koppel et al., 2005; Bykh and Meurers, 2012, for example). The tagset provided by the Penn TreeBank is the most widely used in these experiments, with tagging performed by the Stanford Tagger (Toutanova et al., 2003). We investigate the effect of tagset granularity on classification accuracy by comparing the classification accuracy of texts"
W13-1716,P03-1054,0,0.00307486,"n Monte Carlo technique made available by Johnson (2010).3 Tree Subtitution Grammar fragments In relation to the context-free grammar (CFG) rules explored in the previous NLI work of Wong and Dras (2011), Tree Substitution Grammar (TSG) fragments have been proposed by Swanson and Charniak (2012) as another form of syntactic features for NLI classification tasks. Here, as an approximation to deploying the Bayesian approach to induce a TSG (Post and Gildea, 2009; Swanson and Charniak, 2012), we first parse each of the essays in the TOEFL training corpus with the Stanford Parser (version 2.0.4) (Klein and Manning, 2003) to obtain the parse trees. We then extract the TSG fragments from the parse trees using the TSG system made available by Post and Gildea (2009).4 Stanford dependencies In Tetreault et al. (2012), Stanford dependencies were investigated as yet another form of syntactic features. We follow a similar approach: for each essay in the training corpus, we extract all the basic (rather than 3 http://web.science.mq.edu.au/˜mjohnson/ Software.htm 4 https://github.com/mjpost/dptsg the collapsed) dependencies returned by the Stanford Parser (de Marneffe et al., 2006). Similarly, we generate all the varia"
W13-1716,P09-2012,0,0.0101138,"Wong et al. (2012)) given that the TOEFL corpus is larger than the ICLE corpus. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available by Johnson (2010).3 Tree Subtitution Grammar fragments In relation to the context-free grammar (CFG) rules explored in the previous NLI work of Wong and Dras (2011), Tree Substitution Grammar (TSG) fragments have been proposed by Swanson and Charniak (2012) as another form of syntactic features for NLI classification tasks. Here, as an approximation to deploying the Bayesian approach to induce a TSG (Post and Gildea, 2009; Swanson and Charniak, 2012), we first parse each of the essays in the TOEFL training corpus with the Stanford Parser (version 2.0.4) (Klein and Manning, 2003) to obtain the parse trees. We then extract the TSG fragments from the parse trees using the TSG system made available by Post and Gildea (2009).4 Stanford dependencies In Tetreault et al. (2012), Stanford dependencies were investigated as yet another form of syntactic features. We follow a similar approach: for each essay in the training corpus, we extract all the basic (rather than 3 http://web.science.mq.edu.au/˜mjohnson/ Software.ht"
W13-1716,P12-2038,0,0.0508456,"t POS tags based on the Brown tagset and Vpos+f w is extended with 398 function words used in Wong and Dras (2011). The number of topics t is set to 50 (instead of 25 as per Wong et al. (2012)) given that the TOEFL corpus is larger than the ICLE corpus. The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available by Johnson (2010).3 Tree Subtitution Grammar fragments In relation to the context-free grammar (CFG) rules explored in the previous NLI work of Wong and Dras (2011), Tree Substitution Grammar (TSG) fragments have been proposed by Swanson and Charniak (2012) as another form of syntactic features for NLI classification tasks. Here, as an approximation to deploying the Bayesian approach to induce a TSG (Post and Gildea, 2009; Swanson and Charniak, 2012), we first parse each of the essays in the TOEFL training corpus with the Stanford Parser (version 2.0.4) (Klein and Manning, 2003) to obtain the parse trees. We then extract the TSG fragments from the parse trees using the TSG system made available by Post and Gildea (2009).4 Stanford dependencies In Tetreault et al. (2012), Stanford dependencies were investigated as yet another form of syntactic fe"
W13-1716,C12-1158,0,0.354586,"guage Technology Macquarie University Sydney, Australia {shervin.malmasi,sze.wong,mark.dras}@mq.edu.au Abstract syntactic structure as a feature; Swanson and Charniak (2012) introduced more complex Tree Substitution (TSG) structures, learned by Bayesian inference; and Bykh and Meurers (2012) used recurring n-grams, inspired by the variation n-gram approach to corpus error annotation detection (Dickinson and Meurers, 2003). Starting from the features introduced in these papers and others, then, other recent papers have compiled a comprehensive collection of features based on the earlier work — Tetreault et al. (2012) is an example, combining and analysing most of the features used in previous work. Given the timeframe of the shared task, there seemed to be not much mileage in trying new features that were likely to be more peripheral to the task. Our submission for this NLI shared task used for the most part standard features found in recent work. Our focus was instead on two other aspects of our system: at a high level, on possible ways of constructing ensembles of multiple classifiers; and at a low level, on the granularity of part-of-speech tags used as features. We found that the choice of ensemble co"
W13-1716,N03-1033,0,0.0244956,"associated with a different set of vocabulary: either pure POS or the mixture of POS and function words. We use the grammar proposed by Johnson (2010) for capturing topical collocations as presented below: Sentence → Docj Docj → j Docj → Docj T opici T opici → W ords W ords → W ord W ords → W ords W ord W ord → w POS n-grams Most studies have found that POS tag n-grams are a very useful feature for NLI (Koppel et al., 2005; Bykh and Meurers, 2012, for example). The tagset provided by the Penn TreeBank is the most widely used in these experiments, with tagging performed by the Stanford Tagger (Toutanova et al., 2003). We investigate the effect of tagset granularity on classification accuracy by comparing the classification accuracy of texts tagged with the PTB tagset against those annotated by the RASP Tagger (Briscoe et al., 2006). The PTB POS tagset contains 36 unique tags, while the RASP system uses a subset of the CLAWS2 tagset, consisting of 150 tags. This is a significant size difference and we hypothesize that a larger tagset could provide richer levels of syntactically meaningful info which is more fine-grained in distinction between syntactic categories and contains more morpho-syntactic informat"
W13-1716,W07-0602,0,0.0518239,"sh function words from Wong and Dras (2011). We also tested smaller sets, but observed that the larger sets achieve higher accuracy. Function Word n-grams We devised and tested a new feature that attempts to capture patterns of function word use at the sentence level. We define function word n-grams as a type of word n-gram where content words are skipped: they are thus a specific subtype of skip-gram discussed by Guthrie et al. (2006). For example, the sentence We should all start taking the bus would be reduced to we should all the, from which we would extract the n-grams. Character n-grams Tsur and Rappoport (2007) demonstrated that character n-grams are a useful feature for NLI. These n-grams can be considered as a sub-word feature and their effectiveness is hypothesized to be a result of phoneme transfer from the writer’s L1. They can also capture orthographic conventions of a language. Accordingly, we limit our n-grams to a maximum size of 3 as longer sequences would correspond to short words and not phonemes or syllables. Word n-grams There has been a shift towards the use of word-based features in several recent studies (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Tetreault et al., 2012), with"
W13-1716,D11-1148,1,0.789005,"tion approach, broadly speaking points are assigned to ranks, and these tallied for the overall weight. With the exception of the plurality vote, all of these can be weighted. In our ensembles we also experiment with weighting the output of each classifier using its individual accuracy on the training data as an indication of our degree of confidence in it. 2.4 Feature Representation Most NLI studies have used two types of feature representations: binary (presence or absence of a feature in a text) and normalized frequencies. Although binary feature values have been used in some studies (e.g. Wong and Dras (2011)), most have used frequency-based values. In the course of our experiments we have observed that the effect of the feature representation varies with the feature type, size of the feature space and the learning algorithm itself. In our current system, then, we generate two classifiers for each feature type, one trained with frequency-based values (raw counts scaled using the L2-norm) and the other with binary. Our experiments assess both their individual and joint performance. 2.5 Proficiency-level Based Classification To utilise the proficiency level information provided in the TOEFL11 corpus"
W13-1716,D12-1064,1,0.459335,"vide richer levels of syntactically meaningful info which is more fine-grained in distinction between syntactic categories and contains more morpho-syntactic information such as gender, number, person, case and tense. For example, while the PTB tagset has four tags for pronouns (PRP, PRP$, WP, WP$), the CLAWS tagset provides over 20 pronoun tags (PPHO1, PPIS1, PPX2, PPY, etc.) distinguishing between person, number and grammatical role. Consequently, these tags could help better capture error patterns to be used for classification. 3.3 Syntactic Features Adaptor grammar collocations Drawing on Wong et al. (2012), we also utilise an adaptor grammar to discover arbitrary lengths of n-gram collocations for the TOEFL11 corpus. We explore both the pure part-of-speech (POS) n-grams as well as the more promising mixtures of POS and function words. Following a similar experimental setup as per Wong et al. (2012), we derive two adaptor gram127 j ∈ 1, . . . , m j ∈ 1, . . . , m i ∈ 1, . . . , t; j ∈ 1, . . . , m i ∈ 1, . . . , t w ∈ Vpos ; w ∈ Vpos+f w As per Wong et al. (2012), Vpos contains 119 distinct POS tags based on the Brown tagset and Vpos+f w is extended with 398 function words used in Wong and Dras"
W14-3625,abuhakema-etal-2008-annotating,0,0.100196,"we employ a supervised multi-class classification approach. The learner texts are organized into classes according on the author’s L1 and these documents are used for training and testing in our experiments. A diagram conceptualizing our NLI system is shown in Figure 1. Data Although the majority of currently available learner corpora are based on English L2 (Granger, 2012), data from learners of other languages such as Chinese have also attracted attention in the past several years. No Arabic learner corpora were available for a long time. This paucity of data has been noted by researchers (Abuhakema et al., 2008; Zaghouani et al., 2014) and is thought to be due to issues such as difficulties with non-Latin script and a lack of linguistic and NLP software to work with the data. More recently, the first version of the Arabic Learner Corpus1 (ALC) was released by Alfaifi and Atwell (2013). The corpus includes texts by Arabic learners studying in Saudi Arabia, mostly timed essays written in class. In total, 66 different L1 backgrounds are represented. While texts by native Arabic speakers studying to improve their writing are also included, we do not utilize these. We use the more recent second version o"
W14-3625,W13-1716,1,0.412699,"Missing"
W14-3625,E14-4033,0,0.083742,"Missing"
W14-3625,C12-1158,0,0.182398,"Missing"
W14-3625,W13-1706,0,0.0776445,"180–186, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2 Native Language Texts Chinese 76 Urdu 64 Malay 46 French 44 Fulani 36 English 35 Yoruba 28 Total 329 Background NLI has drawn the attention of many researchers in recent years. With the influx of new researchers, the most substantive work in this field has come in the last few years, leading to the organization of the inaugural NLI Shared Task in 2013 which was attended by 29 teams from the NLP and SLA areas. A detailed exposition of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field successfully presented the first application of NLI to a large non-English dataset (Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese texts. 3 Table 1: The L1 classes included in this experiment and the number of texts within each class. texts are provided with the corpus. Here we use text versions and strip the metadata information from the files, leaving only the auth"
W14-3625,E14-4019,1,0.799275,"x of new researchers, the most substantive work in this field has come in the last few years, leading to the organization of the inaugural NLI Shared Task in 2013 which was attended by 29 teams from the NLP and SLA areas. A detailed exposition of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field successfully presented the first application of NLI to a large non-English dataset (Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese texts. 3 Table 1: The L1 classes included in this experiment and the number of texts within each class. texts are provided with the corpus. Here we use text versions and strip the metadata information from the files, leaving only the author’s writings. 4 In this study we employ a supervised multi-class classification approach. The learner texts are organized into classes according on the author’s L1 and these documents are used for training and testing in our experiments. A diagram conceptualizing our NLI system i"
W14-3625,W14-3708,1,0.904561,"x of new researchers, the most substantive work in this field has come in the last few years, leading to the organization of the inaugural NLI Shared Task in 2013 which was attended by 29 teams from the NLP and SLA areas. A detailed exposition of the shared task results and a review of prior NLI work can be found in Tetreault et al. (2013). While there exists a large body of literature produced in the last decade, almost all of this work has focused exclusively on L2 English. The most recent work in this field successfully presented the first application of NLI to a large non-English dataset (Malmasi and Dras, 2014a), evidencing the usefulness of syntactic features in distinguishing L2 Chinese texts. 3 Table 1: The L1 classes included in this experiment and the number of texts within each class. texts are provided with the corpus. Here we use text versions and strip the metadata information from the files, leaving only the author’s writings. 4 In this study we employ a supervised multi-class classification approach. The learner texts are organized into classes according on the author’s L1 and these documents are used for training and testing in our experiments. A diagram conceptualizing our NLI system i"
W14-3625,U09-1008,1,0.825041,"Missing"
W14-3625,D14-1144,1,\N,Missing
W14-3625,zaghouani-etal-2014-large,0,\N,Missing
W14-3708,P06-4020,0,0.00970702,"ms (n = 1, 2, 3) as features in this work. Note that we use the terminology of Yannakoudakis et al. (2012) here: what had their origin as features in the essay classification task are still referred to as features in the visualisation tool, although the task carried out there is not a classification one. Similarly, we refer to our PoS n-grams as features, although we are not classifying errors using these features and so are not carrying out feature selection for the typical purpose of optimising classification performance. For this, as did Yannakoudakis et al. (2012), we use the RASP parser (Briscoe et al., 2006) for tagging; the tags are consequently from the CLAWS2 tagset,4 which are more fine-grained in terms of linguistic analysis than the more frequently used Penn Treebank tags. For our task, we then used the subset of the FCE corpus where the languages overlapped with the TOEFL11 corpus: we refer to this as FCE SUB. This gives 799 scripts over 8 languages, distributed as in Table 1; a positive byproduct is that the L1s are more similar in size than the full FCE corpus. 3.2 Association Measure We noted in §1 that SLA studies such as Di´ez-Bedmar and Papp (2008) use standard hypothesis testing tec"
W14-3708,E14-4033,0,0.371275,"; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in Yannakoudakis et al. (2012), the goal of which is to develop visualisation tools for SLA researchers. They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of Di´ez-Bedmar and Papp (2008) above. In this paper, we look at approa"
W14-3708,W13-1706,0,0.581249,"arried out on relatively small datasets, and use fairly elementary tools. Sources such as Ellis (2008) and Ortega (2009) give good overviews of such studies and of SLA research in general. A goal of this paper is to investigate a particular way in which Natural Language Processing (NLP) can usefully contribute to SLA. In terms of existing work, the subfield of Native Language Identification (NLI) has been quite active recently, which looks at predicting the L1 of writers writing in a common L2 within a classification task framework; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in"
W14-3708,D11-1148,1,0.83211,"nce to other tasks has varied. Below we discuss works which directly look at how features might be related to language-learning tasks or SLA research. The seminal work of Koppel et al. (2005) that presented NLI as a classification task included, in addition to standard lexical and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classifi"
W14-3708,D12-1064,1,0.839857,"ied using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classification accuracy. Swanson and Charniak (2012) similarly explore using syntax, where they propose a richer representation 13 We assume that the multiple examples come from the larger CLC corpus. 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and redundancy of features through a"
W14-3708,W14-3625,1,0.847854,"and redundancy of features through a number of metrics (including the χ2 -statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical dependency. Coming from a linguistic perspective, the works in Jarvis and Crossley (2012) use Linear Discriminant Analysis for classification of texts by L1, a"
W14-3708,P11-1019,0,0.0125675,"ing Hypotheses: A Visualisation Tool The context of the Yannakoudakis et al. (2012) work is automated grading of English as a Second or Other Language (ESOL) exam scripts, as described in Briscoe et al. (2010). The automated grading takes a classification approach, using a binary discriminative learner, with useful features including lexical and part-of-speech (PoS) n-grams. The publicly available dataset on which the work was carried out consists of texts from the First Certificate in English (FCE) exam, aimed at upper-intermediate students of English across various L1s, and was presented in Yannakoudakis et al. (2011). This FCE corpus2 consists of a subset of 1244 texts of the Cambridge Learner Corpus,3 and is manually annotated with errors and their corrections, as well as a classification according to an error typology, as in Figure 1. Yannakoudakis et al. (2012) present their English Profile (EP) visualiser as a way to “visually analyse as well as perform a linguistic interpretation of discriminative features that characterise learner English”, using the features of this essay classification task. They define a measure of co-occurrence of features, among themselves and with errors, as a core part of the"
W14-3708,E14-4019,1,0.812532,"and redundancy of features through a number of metrics (including the χ2 -statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical dependency. Coming from a linguistic perspective, the works in Jarvis and Crossley (2012) use Linear Discriminant Analysis for classification of texts by L1, a"
W14-3708,W12-0206,0,0.0613498,"n the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in Yannakoudakis et al. (2012), the goal of which is to develop visualisation tools for SLA researchers. They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of Di´ez-Bedmar and Papp (2008) above. In this paper, we look at approaches to finding such hypothesis candidates automatically in the context of L1–L2 interaction by analysing the graphs used in the visualisations One research goal in Second Language Acquisition (SLA) is to formulate and test hypotheses about er"
W14-3708,D14-1144,1,0.814348,"and redundancy of features through a number of metrics (including the χ2 -statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical dependency. Coming from a linguistic perspective, the works in Jarvis and Crossley (2012) use Linear Discriminant Analysis for classification of texts by L1, a"
W14-3708,W13-1716,1,0.779168,"ly find error context such that they behave differently with respect to the L1s according to the ANOVA F-statistic, but produces false positives. Overall, a recurring issue illustrated for all models by the examples is the proposal of error context far away from any likely relevance to SLA. 5 Related Work While Native Language Identification (NLI) as a subfield of NLP has seen much new work in the last few years — the papers from the shared task (Tetreault et al., 2013) provide a recent sample — the emphasis on optimising classification task results, for example by using classifier ensembles (Malmasi et al., 2013), versus analysing features for relevance to other tasks has varied. Below we discuss works which directly look at how features might be related to language-learning tasks or SLA research. The seminal work of Koppel et al. (2005) that presented NLI as a classification task included, in addition to standard lexical and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features ("
W14-3708,P12-2038,0,0.0156654,"eatures on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classification accuracy. Swanson and Charniak (2012) similarly explore using syntax, where they propose a richer representation 13 We assume that the multiple examples come from the larger CLC corpus. 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and redundancy of features through a number of metrics (including the χ2 -statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could ben"
W14-3708,N13-1009,0,0.155119,"a classification task framework; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in Yannakoudakis et al. (2012), the goal of which is to develop visualisation tools for SLA researchers. They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of Di´ez-Bedmar and Papp (2008) above."
W15-0606,C12-1025,0,0.535925,"Missing"
W15-0606,C14-1185,0,0.431649,"Missing"
W15-0606,de-melo-2014-etymological,0,0.0331883,"Missing"
W15-0606,guthrie-etal-2006-closer,0,0.0332885,"L1 and word usage. Using the Etymological WordNet10 database (de Melo, 2014), we extracted two lists of English words with either Old English (508 words) or Latin origins (1,310 words). These words were used as unigram features to train two classifiers. The F1-scores for classification on T OEFL11 are shown in Figure 5. The Old English words, with their West Germanic roots, yield the best results for classifying German data. Conversely, the Latinate features achieve the 9 Hladka et al. (2013) and Henderson et al. (2013) previously used a skip-gram variant that did not include 0 skips as per (Guthrie et al., 2006) and did not improve accuracy. 10 http://www1.icsi.berkeley.edu/%7edemelo/etymwn/ ROOT → S^<ROOT&gt; S^<ROOT&gt; → NP^<S&gt; VP^<S&gt; . NP^<S&gt; → DT JJ JJ NN 0.4 0.35 0.3 0.25 Figure 7: Parent-annotated CFG rules from Fig. 4. 0.2 0.15 0.1 0.05 0 ARA CHI FRE GER HIN ITA Old English Word Features JPN KOR SPA TEL TUR Latin Word Features Figure 5: F1-scores for classifying L1 using English words with Old English or Latin origins. ROOT S JJ . VP NP DT JJ NN Production Rules Extracted from Tree: ROOT Ȳ S S Ȳ NP VP . NP Ȳ DT JJ JJ NN PP Ȳ IN NP VP Ȳ VBD PP NP Ȳ DT JJ NN . PP VBD The quick brown fox jumped NP IN"
W15-0606,W13-1713,0,0.0655331,"tes and word form similarities, but also semantics and meanings. We also examine the link between L1 and word usage. Using the Etymological WordNet10 database (de Melo, 2014), we extracted two lists of English words with either Old English (508 words) or Latin origins (1,310 words). These words were used as unigram features to train two classifiers. The F1-scores for classification on T OEFL11 are shown in Figure 5. The Old English words, with their West Germanic roots, yield the best results for classifying German data. Conversely, the Latinate features achieve the 9 Hladka et al. (2013) and Henderson et al. (2013) previously used a skip-gram variant that did not include 0 skips as per (Guthrie et al., 2006) and did not improve accuracy. 10 http://www1.icsi.berkeley.edu/%7edemelo/etymwn/ ROOT → S^<ROOT&gt; S^<ROOT&gt; → NP^<S&gt; VP^<S&gt; . NP^<S&gt; → DT JJ JJ NN 0.4 0.35 0.3 0.25 Figure 7: Parent-annotated CFG rules from Fig. 4. 0.2 0.15 0.1 0.05 0 ARA CHI FRE GER HIN ITA Old English Word Features JPN KOR SPA TEL TUR Latin Word Features Figure 5: F1-scores for classifying L1 using English words with Old English or Latin origins. ROOT S JJ . VP NP DT JJ NN Production Rules Extracted from Tree: ROOT Ȳ S S Ȳ NP VP . N"
W15-0606,W13-1730,0,0.123388,"Missing"
W15-0606,D14-1142,1,0.760542,"Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work has also shown improvements from longer sequences (Jarvis et al., 2013; Ionescu et al., 2014). 3 Obtained by training on the T OEFL11 train and development sets and evaluating on the test set. 4 Listed in alphabetical order. 49 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 49–55, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Classification Accuracy By Feature fier outputs for each data point where the output values represent correct (1) or incorrect (0) predictions made by that learner. Each classifier Ci produces a result vector yi = [yi,1 , . . . , yi,N ] for a set of N documents where"
W15-0606,W13-1714,0,0.429447,"ong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work has also shown improvements from longer sequences (Jarvis et al., 2013; Ionescu et al., 2014). 3 Obtained by training on the T OEFL11 train and development sets and evaluating on the test set. 4 Listed in alphabetical order. 49 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 49–55, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Classification Accuracy By Feature fier outputs for each data point where the output values represent correct (1) or incorrect (0) predictions made by that learner. Each classifier Ci produces a result vector yi = [yi,1 , . . . , yi,N ] for a s"
W15-0606,J98-4004,0,0.0134303,"Missing"
W15-0606,E14-4019,1,0.929774,"ich aims to infer the native language (L1) of an author based on texts written in a second language (L2). Machine Learning methods are usually used to identify language use patterns common to speakers of the same L1 (Tetreault et al., 2012). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. In this context, by identifying L1-specific language usage and error patterns, NLI can be used to better understand SLA and develop teaching methods, instructions and learner feedback that is tailored to their mother tongue (Malmasi and Dras, 2014b). Although researchers have employed tens of feature types, no effort has been made to measure the overlap of information they capture. Results from previous studies show that while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classi"
W15-0606,D14-1144,1,0.906635,"ich aims to infer the native language (L1) of an author based on texts written in a second language (L2). Machine Learning methods are usually used to identify language use patterns common to speakers of the same L1 (Tetreault et al., 2012). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. In this context, by identifying L1-specific language usage and error patterns, NLI can be used to better understand SLA and develop teaching methods, instructions and learner feedback that is tailored to their mother tongue (Malmasi and Dras, 2014b). Although researchers have employed tens of feature types, no effort has been made to measure the overlap of information they capture. Results from previous studies show that while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classi"
W15-0606,N15-1160,1,0.333488,"Missing"
W15-0606,W13-1716,1,0.897502,"tly, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work ha"
W15-0606,W15-0620,1,0.830362,"Missing"
W15-0606,P14-2138,0,0.0110556,"ed with other languages (Malmasi and Dras, 2014a), this feature can be a good approximation of the dependencies feature for low-resourced languages without an accurate parser. However, results may vary by language and possibly genre (Liu, 2008). We also note that the skip-gram feature space grows prodigiously as k increases. Another related issue is whether sub-lexical character n-grams are independent of word features. Previously, Tsur and Rappoport (2007) hypothesized that these n-grams are discriminative due to writer choices “strongly influenced by the phonology of their native language”. Nicolai and Kondrak (2014) also investigate the source of L1 differences in the relative frequencies of character bigrams. They propose an algorithm to identify the most discriminative words and subsequently, the bigrams corresponding to these words. They found that removing a small set of highly discriminative words greatly degrades the accuracy of a bigrambased classifier. Based on this they conclude that bigrams capture differences in word usage and lexical transfer rather than L1 phonology. Evidence from our analysis also points to a similar pattern with the predictions of character bigrams and trigrams being stron"
W15-0606,P12-2038,0,0.0320334,"ar Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work has also shown improvements from longer sequences (Jarvis et al., 2013; Ionescu et al., 2014). 3 Obtained by training on the T OEFL11 train and development sets and evaluating on the test set. 4 Listed in alphabetical order. 49 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Ap"
W15-0606,C12-1158,1,0.847225,"Service 660 Rosedale Rd Princeton, NJ 08541, USA acahill@ets.org 2 Introduction Researchers in Second Language Acquisition (SLA) investigate the multiplex of factors that influence our ability to acquire new languages and chief among these is the role of the learner’s mother tongue. This core factor has recently been studied in the task of Native Language Identification (NLI), which aims to infer the native language (L1) of an author based on texts written in a second language (L2). Machine Learning methods are usually used to identify language use patterns common to speakers of the same L1 (Tetreault et al., 2012). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. In this context, by identifying L1-specific language usage and error patterns, NLI can be used to better understand SLA and develop teaching methods, instructions and learner feedback that is tailored to their mother tongue (Malmasi and Dras, 2014b). Although researchers have employed tens of feature types, no effort has been made to measure the overlap of information they capture. Results from previous studies show that while some feature types yield similar a"
W15-0606,W13-1706,1,0.652589,"d error patterns, NLI can be used to better understand SLA and develop teaching methods, instructions and learner feedback that is tailored to their mother tongue (Malmasi and Dras, 2014b). Although researchers have employed tens of feature types, no effort has been made to measure the overlap of information they capture. Results from previous studies show that while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Depe"
W15-0606,W07-0602,0,0.417303,"hat while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work"
W15-0606,D11-1148,0,0.441435,"nd Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work has also shown improvements from longer sequences (Jarvis et al., 2013"
W15-0606,D12-1064,0,0.159994,"e. Results from previous studies show that while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat char"
W15-0614,abuhakema-etal-2008-annotating,0,0.0317443,"s. Practical applications of the data and future directions are discussed. 1 While such corpus-based studies have become an accepted standard in SLA research and relevant NLP tasks, there remains a paucity of large-scale L2 corpora. For L2 English, the two main datasets are the ICLE (Granger, 2003) and TOEFL11 (Blanchard et al., 2013) corpora, with the latter being the largest publicly available corpus of non-native English writing.1 However, this data scarcity is far more acute for L2 other than English and this has not gone unnoticed by the research community (Lozano and Mendikoetxea, 2013; Abuhakema et al., 2008). Introduction Despite the rapid growth of learner corpus research in recent years (D´ıaz-Negrillo et al., 2013), no largescale corpus of second language (L2) Chinese has been made readily available to the research community. Learner corpora are often used to investigate learner language production in an exploratory manner in order to generate hypotheses about learner language. Recently, learner corpora have also been utilized in various educational NLP tasks including error detection and correction (Gamon et al., The present work attempts to address this gap by making available the Jinan Chin"
W15-0614,C14-1028,0,0.0144669,"grading techniques to Chinese, something which has not been done to date. Error Detection and Correction There is growing research in building error detection and correction systems trained on learner corpus data (Dahlmeier and Ng, 2011; Han et al., 2010). This was also the focus of a recent shared tasks including Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013). A recent shared task also focused on Chinese error correction (Yu et al., 2014). This research was also recently extended to Chinese word ordering error detection and correction (Cheng et al., 2014), also using learner texts. The large JCLC can be used in such tasks through the addition of error annotations. 121 Transfer Hypothesis Extraction Researchers have recently investigated using data-driven techniques combined with machine learning and NLP to extract language transfer hypotheses from learner corpora (Swanson and Charniak, 2014). Second Language Acquisition researchers are interested in contrasting the productions of natives and non-natives (Housen, 2002). This is made possible with the JCLC data and the presence of multiple L1s allows for contrastive interlanguage analysis betwee"
W15-0614,P11-1092,0,0.0264282,"To this end, this corpus can be used in various areas, as outlined here. Automatic Essay Scoring is an active area of research that relies on examining the differences between proficiency levels using large learner data and NLP methods (Yannakoudakis et al., 2011). Given the inclusion of proficiency data, the JCLC could also be used to investigate the extension of current automatic grading techniques to Chinese, something which has not been done to date. Error Detection and Correction There is growing research in building error detection and correction systems trained on learner corpus data (Dahlmeier and Ng, 2011; Han et al., 2010). This was also the focus of a recent shared tasks including Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013). A recent shared task also focused on Chinese error correction (Yu et al., 2014). This research was also recently extended to Chinese word ordering error detection and correction (Cheng et al., 2014), also using learner texts. The large JCLC can be used in such tasks through the addition of error annotations. 121 Transfer Hypothesis Extraction Researchers have recently investigated using data-driven techniques co"
W15-0614,W11-2838,0,0.0262619,"research that relies on examining the differences between proficiency levels using large learner data and NLP methods (Yannakoudakis et al., 2011). Given the inclusion of proficiency data, the JCLC could also be used to investigate the extension of current automatic grading techniques to Chinese, something which has not been done to date. Error Detection and Correction There is growing research in building error detection and correction systems trained on learner corpus data (Dahlmeier and Ng, 2011; Han et al., 2010). This was also the focus of a recent shared tasks including Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013). A recent shared task also focused on Chinese error correction (Yu et al., 2014). This research was also recently extended to Chinese word ordering error detection and correction (Cheng et al., 2014), also using learner texts. The large JCLC can be used in such tasks through the addition of error annotations. 121 Transfer Hypothesis Extraction Researchers have recently investigated using data-driven techniques combined with machine learning and NLP to extract language transfer hypotheses from learner corpora (Swanson and Charniak, 2"
W15-0614,W12-2006,0,0.0141907,"mining the differences between proficiency levels using large learner data and NLP methods (Yannakoudakis et al., 2011). Given the inclusion of proficiency data, the JCLC could also be used to investigate the extension of current automatic grading techniques to Chinese, something which has not been done to date. Error Detection and Correction There is growing research in building error detection and correction systems trained on learner corpus data (Dahlmeier and Ng, 2011; Han et al., 2010). This was also the focus of a recent shared tasks including Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013). A recent shared task also focused on Chinese error correction (Yu et al., 2014). This research was also recently extended to Chinese word ordering error detection and correction (Cheng et al., 2014), also using learner texts. The large JCLC can be used in such tasks through the addition of error annotations. 121 Transfer Hypothesis Extraction Researchers have recently investigated using data-driven techniques combined with machine learning and NLP to extract language transfer hypotheses from learner corpora (Swanson and Charniak, 2014). Second Languag"
W15-0614,han-etal-2010-using,0,0.0281111,"s can be used in various areas, as outlined here. Automatic Essay Scoring is an active area of research that relies on examining the differences between proficiency levels using large learner data and NLP methods (Yannakoudakis et al., 2011). Given the inclusion of proficiency data, the JCLC could also be used to investigate the extension of current automatic grading techniques to Chinese, something which has not been done to date. Error Detection and Correction There is growing research in building error detection and correction systems trained on learner corpus data (Dahlmeier and Ng, 2011; Han et al., 2010). This was also the focus of a recent shared tasks including Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013). A recent shared task also focused on Chinese error correction (Yu et al., 2014). This research was also recently extended to Chinese word ordering error detection and correction (Cheng et al., 2014), also using learner texts. The large JCLC can be used in such tasks through the addition of error annotations. 121 Transfer Hypothesis Extraction Researchers have recently investigated using data-driven techniques combined with machine"
W15-0614,W05-0203,0,0.0345303,"y and used to create tailored, native language-specific exercises and teaching material. Automatic Assessment Generation Combined with the above-mentioned error detection and language transfer extraction methods, this data can be used to automatically generate testing material (e.g. Cloze tests). Following such an approach, recent work by Sakaguchi et al. (2013) made use of largescale English learner data to generate fill-in-theblank quiz items for language learners. Previous research in this space had also considered the automatic generation of multiple-choice questions for language testing (Hoshino and Nakagawa, 2005), but without learner data. The use of learner corpora containing naturally produced errors provides a much more promising synergy, enabling the assessment of more complex linguistic errors beyond articles, prepositions and synonyms. With further annotations of the present errors, the JCLC could be used for such tasks. 6 Conclusion and Future Work The JCLC, a sizeable project that has been ongoing for the last 8 years, has yielded a large-scale language resource for researchers – the first of its kind. As the only such corpus of this size, the JCLC is a valuable resource to support research in"
W15-0614,W14-3625,1,0.811569,"interlanguage. A criticism of SLA has been that its empirical foundation is weak (Granger, 2002), casting doubts on the generalizability of results. However, this is beginning to change with the shift towards using large learner corpora. The creation of such corpora has led to an efflorescence of empirical research into language acquisition (Granger, 2002). The use of NLP and machine learning methods has also extended to SLA, with a new focus on a combined multidisciplinary approach to developing methods for extracting ranked lists of language transfer candidates (Swanson and Charniak, 2014; Malmasi and Dras, 2014c). 119 Data Collection and Design The JCLC project, started in 2006, aims to create a corpus of non-native Chinese texts, similar to the ICLE. The majority of the data has been collected from foreign students learning Chinese at various universities in China, with some data coming from universities outside China. This data includes both exams and assignments. The texts are manually transcribed with all errors being maintained. Error annotations are not available at this stage. In order to be representative, the corpus includes student data from a wide range of countries and proficiency levels"
W15-0614,U14-1020,1,0.838274,"interlanguage. A criticism of SLA has been that its empirical foundation is weak (Granger, 2002), casting doubts on the generalizability of results. However, this is beginning to change with the shift towards using large learner corpora. The creation of such corpora has led to an efflorescence of empirical research into language acquisition (Granger, 2002). The use of NLP and machine learning methods has also extended to SLA, with a new focus on a combined multidisciplinary approach to developing methods for extracting ranked lists of language transfer candidates (Swanson and Charniak, 2014; Malmasi and Dras, 2014c). 119 Data Collection and Design The JCLC project, started in 2006, aims to create a corpus of non-native Chinese texts, similar to the ICLE. The majority of the data has been collected from foreign students learning Chinese at various universities in China, with some data coming from universities outside China. This data includes both exams and assignments. The texts are manually transcribed with all errors being maintained. Error annotations are not available at this stage. In order to be representative, the corpus includes student data from a wide range of countries and proficiency levels"
W15-0614,D14-1144,1,0.871934,"interlanguage. A criticism of SLA has been that its empirical foundation is weak (Granger, 2002), casting doubts on the generalizability of results. However, this is beginning to change with the shift towards using large learner corpora. The creation of such corpora has led to an efflorescence of empirical research into language acquisition (Granger, 2002). The use of NLP and machine learning methods has also extended to SLA, with a new focus on a combined multidisciplinary approach to developing methods for extracting ranked lists of language transfer candidates (Swanson and Charniak, 2014; Malmasi and Dras, 2014c). 119 Data Collection and Design The JCLC project, started in 2006, aims to create a corpus of non-native Chinese texts, similar to the ICLE. The majority of the data has been collected from foreign students learning Chinese at various universities in China, with some data coming from universities outside China. This data includes both exams and assignments. The texts are manually transcribed with all errors being maintained. Error annotations are not available at this stage. In order to be representative, the corpus includes student data from a wide range of countries and proficiency levels"
W15-0614,N15-1160,1,0.864143,"Missing"
W15-0614,P13-2043,0,0.0464311,"ide the design process. Combined with language transfer analysis, learner data can be used to aid development of pedagogical material within a needs-based and data-driven approach. Once language use patterns are uncovered, they can be assessed for teachability and used to create tailored, native language-specific exercises and teaching material. Automatic Assessment Generation Combined with the above-mentioned error detection and language transfer extraction methods, this data can be used to automatically generate testing material (e.g. Cloze tests). Following such an approach, recent work by Sakaguchi et al. (2013) made use of largescale English learner data to generate fill-in-theblank quiz items for language learners. Previous research in this space had also considered the automatic generation of multiple-choice questions for language testing (Hoshino and Nakagawa, 2005), but without learner data. The use of learner corpora containing naturally produced errors provides a much more promising synergy, enabling the assessment of more complex linguistic errors beyond articles, prepositions and synonyms. With further annotations of the present errors, the JCLC could be used for such tasks. 6 Conclusion and"
W15-0614,E14-4033,0,0.196672,"o identify deficits in their interlanguage. A criticism of SLA has been that its empirical foundation is weak (Granger, 2002), casting doubts on the generalizability of results. However, this is beginning to change with the shift towards using large learner corpora. The creation of such corpora has led to an efflorescence of empirical research into language acquisition (Granger, 2002). The use of NLP and machine learning methods has also extended to SLA, with a new focus on a combined multidisciplinary approach to developing methods for extracting ranked lists of language transfer candidates (Swanson and Charniak, 2014; Malmasi and Dras, 2014c). 119 Data Collection and Design The JCLC project, started in 2006, aims to create a corpus of non-native Chinese texts, similar to the ICLE. The majority of the data has been collected from foreign students learning Chinese at various universities in China, with some data coming from universities outside China. This data includes both exams and assignments. The texts are manually transcribed with all errors being maintained. Error annotations are not available at this stage. In order to be representative, the corpus includes student data from a wide range of countrie"
W15-0614,W13-1706,0,0.112908,"Missing"
W15-0614,P11-1019,0,0.076779,"Learner Corpus (left) and the TOEFL11 corpus (right). In sum, we see that the JCLC is a large corpus and represents various native language and proficiency groups. These characteristics make it suitable for a wide range of research tasks, as described in the next section. 5 Applications Educational studies in linguistics and NLP have been increasing recently. To this end, this corpus can be used in various areas, as outlined here. Automatic Essay Scoring is an active area of research that relies on examining the differences between proficiency levels using large learner data and NLP methods (Yannakoudakis et al., 2011). Given the inclusion of proficiency data, the JCLC could also be used to investigate the extension of current automatic grading techniques to Chinese, something which has not been done to date. Error Detection and Correction There is growing research in building error detection and correction systems trained on learner corpus data (Dahlmeier and Ng, 2011; Han et al., 2010). This was also the focus of a recent shared tasks including Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL shared tasks (Ng et al., 2013). A recent shared task also focused on Chinese error correct"
W15-0620,C14-1185,0,0.408647,"Missing"
W15-0620,W13-1726,0,0.0970702,"Missing"
W15-0620,P05-1022,0,0.225282,"Missing"
W15-0620,W13-1727,0,0.253176,"Missing"
W15-0620,W13-1712,0,0.0938667,"Missing"
W15-0620,D14-1142,0,0.246652,"Missing"
W15-0620,W13-1714,0,0.336048,"Missing"
W15-0620,W15-0606,1,0.830691,"Missing"
W15-0620,D14-1144,1,0.807671,"Missing"
W15-0620,W13-1716,1,0.797299,"Missing"
W15-0620,D08-1027,0,0.0773695,"Missing"
W15-0620,C12-1158,1,0.820422,"Missing"
W15-0620,W13-1706,1,0.864163,"Missing"
W15-5407,W15-0606,1,0.885187,"Missing"
W15-5407,N15-1160,1,0.920145,"ning (1994). Automatic LID methods have since been widely used in NLP. Although LID can be extremely accurate in distinguishing languages that use distinct character sets (e.g. Chinese or Japanese) or are very dissimilar (e.g. Spanish and Swedish), performance is degraded when it is used for discriminating similar languages or dialects. This has led to researchers turning their attention to the sub-problem of discriminating between closely-related languages and varieties. This issue has been researched in the context of confusable languages, including MalayIndonesian (Bali, 2006), Farsi-Dari (Malmasi and Dras, 2015a), Croatian-Slovene-Serbian (Ljubesic et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013), and Chinese varieties (Huang and Lee, 2008). The task of Arabic Dialect Identification has also drawn attention in the Arabic NLP community (Malmasi et al., 2015a). This issue was also the focus of the first “Discriminating Similar Language” (DSL) shared task1 in 2014. The shared task used data from 13 different languages and varieties divided into 6 sub-groups and teams needed to build systems for distinguishing these classes. They were provided wit"
W15-5407,Y08-1042,0,0.370058,"Missing"
W15-5407,W13-1716,1,0.836051,",000 18,000 18,000 252,000 Dev 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 2,000 28,000 Test 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 14,000 Table 1: The languages included in the corpus and the number of sentences in each set. 3 In particular, we use the LIBLINEAR2 package (Fan et al., 2008) which has been shown to be efficient for text classification problems such as this. For example, it has been demonstrated to be a very effective classifier for the task of Native Language Identification (Malmasi and Dras, 2015b; Malmasi et al., 2013) which also relies on text classification methods. Data The data for the shared task comes from the DSL Corpus Collection (Tan et al., 2014). The task is performed at the sentence-level and the corpus consists of 294,000 sentences distributed evenly between 14 language classes. The corpus is subdivided into training, development and test sets. The languages and the number of sentences in each set are listed in Table 1. An interesting addition to this year’s data is the inclusion of an “other” class which contains data from various additional languages. The motivation here is to emulate a reali"
W15-5407,W15-0620,1,0.928553,"imilar languages or dialects. This has led to researchers turning their attention to the sub-problem of discriminating between closely-related languages and varieties. This issue has been researched in the context of confusable languages, including MalayIndonesian (Bali, 2006), Farsi-Dari (Malmasi and Dras, 2015a), Croatian-Slovene-Serbian (Ljubesic et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013), and Chinese varieties (Huang and Lee, 2008). The task of Arabic Dialect Identification has also drawn attention in the Arabic NLP community (Malmasi et al., 2015a). This issue was also the focus of the first “Discriminating Similar Language” (DSL) shared task1 in 2014. The shared task used data from 13 different languages and varieties divided into 6 sub-groups and teams needed to build systems for distinguishing these classes. They were provided with a training and development dataset comprised of 20,000 sentences from each language and an unlabelled test set of 1,000 sentences per language was used for evaluation. Most entries used surface features and many applied hierarchical classifiers, taking advantage of the structure provided by the language"
W15-5407,C12-1160,0,0.121099,"Missing"
W15-5407,W15-5401,0,0.0774697,"Missing"
W16-0314,W14-3207,0,0.0144592,"native language (Gebre et al., 2013; Malmasi and Dras, 2015a), age and gender (Nguyen et al., 2013), and even economic conditions such as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classi"
W16-0314,W13-1728,1,0.766386,"individual classifiers as well as an ensemble classifier. This meta-classifier was then extended to a Random Forest of meta-classifiers, yielding further improvements in classification accuracy. We achieved competitive results, ranking first among a total of 60 submitted entries in the competition. 1 Introduction Computational methods have been widely used to extract and/or predict a number of phenomena in text documents. It has been shown that algorithms are able to learn a wide range of information about the authors of texts as well. This includes, for example, the author’s native language (Gebre et al., 2013; Malmasi and Dras, 2015a), age and gender (Nguyen et al., 2013), and even economic conditions such as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples"
W16-0314,guthrie-etal-2006-closer,0,0.0183918,"prioritize addressing this post. • Amber: a moderator needs to look at this and assess if there are enough responses and support from others or if they should reply. • Red: a moderator needs to look at this as soon as possible and take action. • Crisis: the author (or someone they know) might hurt themselves or others (a red instance that is of urgent importance). Participating systems should be trained to predict these labels, with evaluation on the test set. • Word skip-grams: To capture the longer distance dependencies not covered by word ngrams we also used word skip-grams as described in Guthrie et al. (2006). We extract 1, 2 and 3-skip word bigrams. • Lemma n-grams: we used a lemmatized version of the texts and extract lemma n-grams of order 1–3. • Word Representations: To increase the generalizability of our models we used word representation features based on Brown clustering as a form of semi-supervised learning. This was done using the method described by Malmasi et al. (2015a). We used the clusters generated by Owoputi et al. (2013). They collected From 56 million English tweets (837 million tokens) and used it to generate 1,000 hierarchical clusters over 217 thousand words. 3.3 Feature Extr"
W16-0314,E14-4019,1,0.894839,"Missing"
W16-0314,W15-5407,1,0.746304,"rs as well as an ensemble classifier. This meta-classifier was then extended to a Random Forest of meta-classifiers, yielding further improvements in classification accuracy. We achieved competitive results, ranking first among a total of 60 submitted entries in the competition. 1 Introduction Computational methods have been widely used to extract and/or predict a number of phenomena in text documents. It has been shown that algorithms are able to learn a wide range of information about the authors of texts as well. This includes, for example, the author’s native language (Gebre et al., 2013; Malmasi and Dras, 2015a), age and gender (Nguyen et al., 2013), and even economic conditions such as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psycholog"
W16-0314,U15-1008,1,0.837443,"systems should be trained to predict these labels, with evaluation on the test set. • Word skip-grams: To capture the longer distance dependencies not covered by word ngrams we also used word skip-grams as described in Guthrie et al. (2006). We extract 1, 2 and 3-skip word bigrams. • Lemma n-grams: we used a lemmatized version of the texts and extract lemma n-grams of order 1–3. • Word Representations: To increase the generalizability of our models we used word representation features based on Brown clustering as a form of semi-supervised learning. This was done using the method described by Malmasi et al. (2015a). We used the clusters generated by Owoputi et al. (2013). They collected From 56 million English tweets (837 million tokens) and used it to generate 1,000 hierarchical clusters over 217 thousand words. 3.3 Feature Extraction We used three categories of features: lexical, syntactic, and metadata features. These features and our preprocessing method are outlined here. 3.1 Lexical Features • Character n-grams: we extracted n-grams of order 2–8. Table 1: CLPsych Corpus Divided by Data Set 3 3.2 Preprocessing The following preprocessing was performed on the texts: HTML removal was performed, wit"
W16-0314,S16-1154,1,0.836465,"asi and Dras, 2015b). Run Run 1 Run 2 Run 3 Run 4 Run 5 Official Accuracy F-score Accuracy Rank Score (NG vs. G) (NG vs. G) 0.37 0.80 0.83 0.89 11th 0.38 0.80 0.83 0.89 9th 0.42 0.83 0.87 0.91 1st 0.42 0.84 0.87 0.91 1st 0.40 0.82 0.85 0.90 6th Table 2: Official CLPsych scores. Best results in bold. Rankings are out of the 60 systems submitted. Classifiers ensembles have proven to be an efficient and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a), grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi et al., 2016). 4.2 Meta-classifier For our meta-classifier, We experimented with three algorithms: Random Forests of decision trees, a linear SVM just like our base classifiers and a Radial Basis Function (RBF) kernel SVM. The inputs to the meta-classifier are the continuous outputs from each base SVM classifier in our ensemble, along with the original gold label. For the Random Forest classifiers, the final label is selected through a plurality voting process across all decision trees in the forest. All were found to perform well, but the linear SVM was was outperformed by its RBF-kernel counterpart. This"
W16-0314,W15-1202,0,0.0122244,"task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classification task serves to provide automatic triage of user posts in order to help moderators of forums and related online communities to respond to urgent posts. Our approach competed in the CLPsych 2016 shared task and achieved the highest accuracy among submitte"
W16-0314,N13-1039,0,0.0716559,"Missing"
W16-0314,W15-1214,0,0.0518374,"Missing"
W16-0314,W14-3214,0,0.158613,"Missing"
W16-0314,W14-3211,0,0.0226416,"considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classification task serves to provide automatic triage of user posts in order to help moderators of forums and related online communities to respond to urgent posts. Our approach competed in the CLPs"
W16-0314,W14-3201,0,0.146759,"uch as income (Preot¸iuc-Pietro et al., 2015). These tasks are often considered to be a part of a broader natural language processing task known as authorship profiling (Rangel et al., 2013). More recently, such approaches have been applied to investigating psychological factors associated with the author of a text. For practical purposes most of the applications that deal with clinical psychology use social media data such as Twitter, Facebook, and online forums (Coppersmith et al., 2014). Examples of health and psychological conditions studied using texts and social media are: suicide risk (Thompson et al., 2014), depression (Schwartz et al., 2014), autism (Tanaka et al., 2014; Rouhizadeh et al., 2015), and schizophrenia (Mitchell et al., 2015). In this paper we propose an approach to predict the severity of posts in a mental health online forum. Posts were classified into for levels of severity (or urgency) represented by the labels green, amber, red, and crisis according to indication of risky or harmful behavior by users (e.g. self-harm, suicide, etc.). This kind of classification task serves to provide automatic triage of user posts in order to help moderators of forums and related online communit"
W16-0314,W15-4415,0,0.0452785,"s (Malmasi and Dras, 2014; Malmasi et al., 2015b; Malmasi and Dras, 2015b). Run Run 1 Run 2 Run 3 Run 4 Run 5 Official Accuracy F-score Accuracy Rank Score (NG vs. G) (NG vs. G) 0.37 0.80 0.83 0.89 11th 0.38 0.80 0.83 0.89 9th 0.42 0.83 0.87 0.91 1st 0.42 0.84 0.87 0.91 1st 0.40 0.82 0.85 0.90 6th Table 2: Official CLPsych scores. Best results in bold. Rankings are out of the 60 systems submitted. Classifiers ensembles have proven to be an efficient and robust alternative in other text classification tasks such as language identification (Malmasi and Dras, 2015a), grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi et al., 2016). 4.2 Meta-classifier For our meta-classifier, We experimented with three algorithms: Random Forests of decision trees, a linear SVM just like our base classifiers and a Radial Basis Function (RBF) kernel SVM. The inputs to the meta-classifier are the continuous outputs from each base SVM classifier in our ensemble, along with the original gold label. For the Random Forest classifiers, the final label is selected through a plurality voting process across all decision trees in the forest. All were found to perform well, but the linear SVM"
W16-4801,W16-4821,0,0.0339271,"Missing"
W16-4801,W16-4826,0,0.0502832,"Missing"
W16-4801,W16-4827,0,0.11257,"Missing"
W16-4801,W16-4819,0,0.0808398,"Missing"
W16-4801,W16-4816,0,0.0637155,"Missing"
W16-4801,W15-5410,0,0.238809,"Missing"
W16-4801,W16-4802,0,0.114498,"Missing"
W16-4801,W16-4831,0,0.0913939,"Missing"
W16-4801,W16-4830,0,0.0438028,"Missing"
W16-4801,D14-1154,0,0.0630846,"otivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification"
W16-4801,W16-4828,0,0.0387334,"Missing"
W16-4801,W15-5409,0,0.149587,"Missing"
W16-4801,W16-4829,0,0.0511602,"Missing"
W16-4801,W15-5403,0,0.27462,"Missing"
W16-4801,W16-4822,0,0.0759257,"Missing"
W16-4801,W15-5413,0,0.562658,"Missing"
W16-4801,W16-4823,0,0.0418563,"Missing"
W16-4801,W14-5316,0,0.385305,"Missing"
W16-4801,L16-1284,1,0.849869,"Missing"
W16-4801,W16-4824,0,0.0430542,"Missing"
W16-4801,W16-4817,0,0.0342647,"Missing"
W16-4801,W16-4815,0,0.0384942,"Missing"
W16-4801,Y08-1042,0,0.0574334,"ticipants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic"
W16-4801,W16-4818,0,0.128184,"Missing"
W16-4801,J16-3005,0,0.112984,"Missing"
W16-4801,W15-5408,0,0.217783,"Missing"
W16-4801,W16-4820,0,0.38437,"Missing"
W16-4801,W14-5317,0,0.0630949,"Missing"
W16-4801,U13-1003,0,0.256503,"size and scope featuring two subtasks and attracting a record number of participants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natur"
W16-4801,W14-5315,0,0.0614151,"Missing"
W16-4801,W15-5407,1,0.493189,"detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods incl"
W16-4801,W16-4814,1,0.85912,"Missing"
W16-4801,W16-4825,0,0.0362109,"Missing"
W16-4801,W14-5314,0,0.0924571,"Missing"
W16-4801,W15-3205,0,0.0361398,"ortuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with met"
W16-4801,W14-5313,0,0.115089,"of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification. Below, we discuss some related shared tasks including the first two editions of the DSL challenge. 2.1 Related Shared Tasks Several shared tasks related to the DSL task have been organized in recent years. Two examples are the ALTW language identification shared task (Baldwin and Lui, 2010) on general-purpose language identification,"
W16-4801,P11-1122,0,0.0270219,"Missing"
W16-4801,W14-5307,1,0.744553,"Missing"
W16-4801,W15-5411,1,0.882098,"Missing"
W16-4812,P14-3013,0,0.0144338,"n Retrieval. Another example is the application of the output from these LID methods to adapt NLP tools that require annotated data, such as part-of-speech taggers, for resource-poor languages. This will be further discussed in §2.2. The primary aim of this work is to apply classification methods to regional variants of Central Kurdish, also known as Sorani. Kurdish is a low-resourced but important language. It is classified within a group of “non-Western European languages critical to U.S. national security”.1 In recent years there has been increasing research interest in processing Kurdish (Aliabadi, 2014; Esmaili et al., 2014). As we will outline in §3, Kurdish is spoken in a number of countries and has several dialects. Sorani is one of these dialects and is spoken in several regions. The main objective of this study is to determine whether subdialectal variations in Sorani can be identified in texts produced from different regions. More specifically, we consider the two main areas where Sorani is spoken, Iran and Iraq. As the first such study, we identify the relevant data sources and attempt to establish the performance of currently used classification methods on this dialect. We also make"
W16-4812,W12-2108,0,0.032024,"(Zampieri et al., 2014).3 Although LID has been investigated using data from many languages, to our knowledge, the present study is the first treatment of Sorani within this context. 2.2 Applications of LID Further to determining the language of documents, LID has applications in statistical machine translation, lexicography (e.g. inducing dialect-to-dialect lexicons) and authorship profiling in the forensic linguistics domain. In an Information Retrieval context it can help filter documents (e.g. news articles or search results) by language and even dialect; one such example is presented by (Bergsma et al., 2012) where LID is used for creating language-specific Twitter collections. LID serves as an important preprocessing method for other NLP tasks. This includes selecting appropriate models for machine translation, sentiment analysis or other types of text analysis, e.g. Native Language Identification (Malmasi et al., 2013; Malmasi and Dras, 2015b). LID can also be used in the adaptation of NLP tools, such as part-of-speech taggers for low-resourced languages (Feldman et al., 2006). If Sorani subdialects are too different to directly share the same resources and statistical models, the distinguishing"
W16-4812,brooke-hirst-2012-measuring,0,0.0309209,"assification. Each unique word may be used as a feature (i.e. unigrams), but the use of bigram distributions is also common. In this scenario, the n-grams are extracted along with their frequency distributions. For this study we evaluate unigram features. 5.2 Named Entity Masking Our dataset is not controlled for topic and it is possible implicitly capture topical cues and are thus susceptible to topic bias. Topic bias can occur as a result of the themes or topics of the texts to be classified not being evenly distributed across the classes, leading to correlations between classes and topics (Brooke and Hirst, 2012; Malmasi and Dras, 2014a; Malmasi and Dras, 2015c). More specifically for the current work, the topics can refer to regional toponyms and location names. One way to counter this issue is to create a balanced or parallel corpus (Malmasi and Dras, ). This is a non-trivial task that requires time and resources, and so was not considered for this preliminary study. Another approach is based on named entity masking, which aims to identify and remove named entities such as location names to minimize their influence on the classification models. This approach requires the identification of such toke"
W16-4812,P13-2054,0,0.454407,"slation, sentiment analysis or other types of text analysis, e.g. Native Language Identification (Malmasi et al., 2013; Malmasi and Dras, 2015b). LID can also be used in the adaptation of NLP tools, such as part-of-speech taggers for low-resourced languages (Feldman et al., 2006). If Sorani subdialects are too different to directly share the same resources and statistical models, the distinguishing features identified through LID can assist in adapting existing resources for one subdialect to another. 3 Kurdish Language Overview Spoken by twenty to thirty million Kurds (Haig and Matras, 2002; Esmaili and Salavati, 2013; Salih, 2014; Blau, 2016; Kurdish Academy of Language, 2016), “Kurdish” as a language is nonetheless not easily defined, producing both difficulty and debate for many scholars and researchers (Haig and Matras, 2002; Hassani and Medjedovic, 2016; Paul, 2008). Kurdish, spoken in “Kurdistan” (a region split primarily among Turkey, Iran, Iraq and Syria (Esmaili and Salavati, 2013; Haig and Matras, 2002)), has been embroiled in conflict, so the question of Kurdish linguistic boundaries is complicated by political, cultural and historical factors (Paul, 2008). One reason for disagreement about the"
W16-4812,feldman-etal-2006-cross,0,0.0409741,"filter documents (e.g. news articles or search results) by language and even dialect; one such example is presented by (Bergsma et al., 2012) where LID is used for creating language-specific Twitter collections. LID serves as an important preprocessing method for other NLP tasks. This includes selecting appropriate models for machine translation, sentiment analysis or other types of text analysis, e.g. Native Language Identification (Malmasi et al., 2013; Malmasi and Dras, 2015b). LID can also be used in the adaptation of NLP tools, such as part-of-speech taggers for low-resourced languages (Feldman et al., 2006). If Sorani subdialects are too different to directly share the same resources and statistical models, the distinguishing features identified through LID can assist in adapting existing resources for one subdialect to another. 3 Kurdish Language Overview Spoken by twenty to thirty million Kurds (Haig and Matras, 2002; Esmaili and Salavati, 2013; Salih, 2014; Blau, 2016; Kurdish Academy of Language, 2016), “Kurdish” as a language is nonetheless not easily defined, producing both difficulty and debate for many scholars and researchers (Haig and Matras, 2002; Hassani and Medjedovic, 2016; Paul, 2"
W16-4812,Y08-1042,0,0.030482,".gov/content/critical-languages 89 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 89–96, Osaka, Japan, December 12 2016. This has led to researchers turning their attention to the sub-problem of discriminating between closely-related languages and varieties. This issue has been investigated in the context of confusable languages/dialects, including Malay-Indonesian (Bali, 2006), Croatian-Slovene-Serbian (Ljubeˇsi´c et al., 2007), Bosnian-Croatian-Serbian (Tiedemann and Ljubeˇsi´c, 2012), Farsi-Dari (Malmasi and Dras, 2015a) and Chinese varieties (Huang and Lee, 2008). This issue was also the focus of the recent “Discriminating Similar Language” (DSL) shared task.2 The shared task used data from 13 different languages and varieties divided into 6 sub-groups and teams needed to build systems for distinguishing these classes. They were provided with a training and development dataset comprised of 20,000 sentences from each language and an unlabelled test set of 1,000 sentences per language was used for evaluation. Most entries used surface features and many applied hierarchical classifiers, taking advantage of the structure provided by the language family me"
W16-4812,W14-3625,1,0.844987,"e word may be used as a feature (i.e. unigrams), but the use of bigram distributions is also common. In this scenario, the n-grams are extracted along with their frequency distributions. For this study we evaluate unigram features. 5.2 Named Entity Masking Our dataset is not controlled for topic and it is possible implicitly capture topical cues and are thus susceptible to topic bias. Topic bias can occur as a result of the themes or topics of the texts to be classified not being evenly distributed across the classes, leading to correlations between classes and topics (Brooke and Hirst, 2012; Malmasi and Dras, 2014a; Malmasi and Dras, 2015c). More specifically for the current work, the topics can refer to regional toponyms and location names. One way to counter this issue is to create a balanced or parallel corpus (Malmasi and Dras, ). This is a non-trivial task that requires time and resources, and so was not considered for this preliminary study. Another approach is based on named entity masking, which aims to identify and remove named entities such as location names to minimize their influence on the classification models. This approach requires the identification of such tokens through Named Entity"
W16-4812,E14-4019,1,0.84407,"e word may be used as a feature (i.e. unigrams), but the use of bigram distributions is also common. In this scenario, the n-grams are extracted along with their frequency distributions. For this study we evaluate unigram features. 5.2 Named Entity Masking Our dataset is not controlled for topic and it is possible implicitly capture topical cues and are thus susceptible to topic bias. Topic bias can occur as a result of the themes or topics of the texts to be classified not being evenly distributed across the classes, leading to correlations between classes and topics (Brooke and Hirst, 2012; Malmasi and Dras, 2014a; Malmasi and Dras, 2015c). More specifically for the current work, the topics can refer to regional toponyms and location names. One way to counter this issue is to create a balanced or parallel corpus (Malmasi and Dras, ). This is a non-trivial task that requires time and resources, and so was not considered for this preliminary study. Another approach is based on named entity masking, which aims to identify and remove named entities such as location names to minimize their influence on the classification models. This approach requires the identification of such tokens through Named Entity"
W16-4812,D14-1144,1,0.847377,"e word may be used as a feature (i.e. unigrams), but the use of bigram distributions is also common. In this scenario, the n-grams are extracted along with their frequency distributions. For this study we evaluate unigram features. 5.2 Named Entity Masking Our dataset is not controlled for topic and it is possible implicitly capture topical cues and are thus susceptible to topic bias. Topic bias can occur as a result of the themes or topics of the texts to be classified not being evenly distributed across the classes, leading to correlations between classes and topics (Brooke and Hirst, 2012; Malmasi and Dras, 2014a; Malmasi and Dras, 2015c). More specifically for the current work, the topics can refer to regional toponyms and location names. One way to counter this issue is to create a balanced or parallel corpus (Malmasi and Dras, ). This is a non-trivial task that requires time and resources, and so was not considered for this preliminary study. Another approach is based on named entity masking, which aims to identify and remove named entities such as location names to minimize their influence on the classification models. This approach requires the identification of such tokens through Named Entity"
W16-4812,N15-1160,1,0.907231,"ecommons.org/licenses/by/4.0/ 1 https://www.nsep.gov/content/critical-languages 89 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 89–96, Osaka, Japan, December 12 2016. This has led to researchers turning their attention to the sub-problem of discriminating between closely-related languages and varieties. This issue has been investigated in the context of confusable languages/dialects, including Malay-Indonesian (Bali, 2006), Croatian-Slovene-Serbian (Ljubeˇsi´c et al., 2007), Bosnian-Croatian-Serbian (Tiedemann and Ljubeˇsi´c, 2012), Farsi-Dari (Malmasi and Dras, 2015a) and Chinese varieties (Huang and Lee, 2008). This issue was also the focus of the recent “Discriminating Similar Language” (DSL) shared task.2 The shared task used data from 13 different languages and varieties divided into 6 sub-groups and teams needed to build systems for distinguishing these classes. They were provided with a training and development dataset comprised of 20,000 sentences from each language and an unlabelled test set of 1,000 sentences per language was used for evaluation. Most entries used surface features and many applied hierarchical classifiers, taking advantage of th"
W16-4812,W13-1716,1,0.838522,"cography (e.g. inducing dialect-to-dialect lexicons) and authorship profiling in the forensic linguistics domain. In an Information Retrieval context it can help filter documents (e.g. news articles or search results) by language and even dialect; one such example is presented by (Bergsma et al., 2012) where LID is used for creating language-specific Twitter collections. LID serves as an important preprocessing method for other NLP tasks. This includes selecting appropriate models for machine translation, sentiment analysis or other types of text analysis, e.g. Native Language Identification (Malmasi et al., 2013; Malmasi and Dras, 2015b). LID can also be used in the adaptation of NLP tools, such as part-of-speech taggers for low-resourced languages (Feldman et al., 2006). If Sorani subdialects are too different to directly share the same resources and statistical models, the distinguishing features identified through LID can assist in adapting existing resources for one subdialect to another. 3 Kurdish Language Overview Spoken by twenty to thirty million Kurds (Haig and Matras, 2002; Esmaili and Salavati, 2013; Salih, 2014; Blau, 2016; Kurdish Academy of Language, 2016), “Kurdish” as a language is no"
W16-4812,C12-1160,0,0.0647496,"Missing"
W16-4812,W14-5307,0,0.0449425,"Missing"
W16-4812,W15-5401,0,0.0262995,"Missing"
W16-4814,al-sabbagh-girju-2012-yadac,0,0.193839,"We experimented with three different ensemble fusion strategies, with the mean probability approach providing the best performance. 1 Introduction The interest in processing Arabic texts and speech data has grown substantially in the last decade.1 Due to its intrinsic variation, research has been carried out not only on Modern Standard Arabic (MSA), but also on the various Arabic dialects spoken in North Africa and in the Middle East. Research in NLP and Arabic dialects includes, most notably, machine translation of Arabic dialects (Zbib et al., 2012), corpus compilation for Arabic dialects (Al-Sabbagh and Girju, 2012; Cotterell and CallisonBurch, 2014), parsing (Chiang et al., 2006), and Arabic dialect identification (Zaidan and Callison-Burch, 2014). The latter has become a vibrant research topic with several papers published in the last few years (Sadat et al., 2014; Malmasi et al., 2015). In this paper we revisit the task of Arabic dialect identification proposing an ensemble method applied to a corpus of broadcast speeches transcribed from MSA and four Arabic dialects: Egyptian, Gulf, Levantine, and North African (Ali et al., 2016). The system competed in the Arabic dialect identification sub-task of"
W16-4814,bouamor-etal-2014-multidialectal,0,0.0955956,"Missing"
W16-4814,E06-1047,0,0.0283987,"e mean probability approach providing the best performance. 1 Introduction The interest in processing Arabic texts and speech data has grown substantially in the last decade.1 Due to its intrinsic variation, research has been carried out not only on Modern Standard Arabic (MSA), but also on the various Arabic dialects spoken in North Africa and in the Middle East. Research in NLP and Arabic dialects includes, most notably, machine translation of Arabic dialects (Zbib et al., 2012), corpus compilation for Arabic dialects (Al-Sabbagh and Girju, 2012; Cotterell and CallisonBurch, 2014), parsing (Chiang et al., 2006), and Arabic dialect identification (Zaidan and Callison-Burch, 2014). The latter has become a vibrant research topic with several papers published in the last few years (Sadat et al., 2014; Malmasi et al., 2015). In this paper we revisit the task of Arabic dialect identification proposing an ensemble method applied to a corpus of broadcast speeches transcribed from MSA and four Arabic dialects: Egyptian, Gulf, Levantine, and North African (Ali et al., 2016). The system competed in the Arabic dialect identification sub-task of the 2016 edition of the DSL shared task (Malmasi et al., 2016b)2 un"
W16-4814,L16-1522,0,0.0911986,"Missing"
W16-4814,cotterell-callison-burch-2014-multi,0,0.297758,"Missing"
W16-4814,D14-1154,0,0.433541,"Missing"
W16-4814,P13-2081,0,0.265769,"Missing"
W16-4814,W14-5316,0,0.222489,"Missing"
W16-4814,L16-1284,1,0.858644,"Missing"
W16-4814,U13-1003,0,0.342387,"Missing"
W16-4814,W15-5407,1,0.70075,"released one month later for the official evaluation. A breakdown of the number of training sentences for each of these classes is listed in Table 1. Dialect Egyptian Gulf Levantine Modern Standard North African Total Class EGY GLF LAV MSA NOR Sentences 1,578 1,672 1,758 999 1,612 7,619 Table 1: The breakdown of the dialectal training data provided (Ali et al., 2016). 3.2 Approach There have been various methods proposed for dialect identification in recent years. Given its success in previous work, we decided to use an ensemble classifier for our entry. We follow the methodology described by Malmasi and Dras (2015b): we extract a number of different feature types and train a single linear model using each feature type. We extract the following feature types, each of them used to train a single classification model: • Character n-grams (n = 1–6): these substrings, depending on the order, can implicitly capture various sub-lexical features including single letters, phonemes, syllables, morphemes and suffixes. They could capture interesting inter-dialectal differences that generalize better than word n-grams. • Word unigrams: entire words can capture lexical differences between dialects. 107 We did not pe"
W16-4814,W16-4801,1,0.864908,"Missing"
W16-4814,W14-5904,0,0.151233,"insic variation, research has been carried out not only on Modern Standard Arabic (MSA), but also on the various Arabic dialects spoken in North Africa and in the Middle East. Research in NLP and Arabic dialects includes, most notably, machine translation of Arabic dialects (Zbib et al., 2012), corpus compilation for Arabic dialects (Al-Sabbagh and Girju, 2012; Cotterell and CallisonBurch, 2014), parsing (Chiang et al., 2006), and Arabic dialect identification (Zaidan and Callison-Burch, 2014). The latter has become a vibrant research topic with several papers published in the last few years (Sadat et al., 2014; Malmasi et al., 2015). In this paper we revisit the task of Arabic dialect identification proposing an ensemble method applied to a corpus of broadcast speeches transcribed from MSA and four Arabic dialects: Egyptian, Gulf, Levantine, and North African (Ali et al., 2016). The system competed in the Arabic dialect identification sub-task of the 2016 edition of the DSL shared task (Malmasi et al., 2016b)2 under the team name MAZA. The system achieved very good performance and was ranked first among the 18 teams that participated in the closed submission track. 2 Related Work There have been se"
W16-4814,W15-3205,0,0.138174,"Missing"
W16-4814,W14-5313,0,0.134582,"Missing"
W16-4814,P11-2007,0,0.232637,"Missing"
W16-4814,W14-5307,1,0.890445,"Missing"
W16-4814,N12-1006,0,0.0257736,"er ensemble with a set of linear models as base classifiers. We experimented with three different ensemble fusion strategies, with the mean probability approach providing the best performance. 1 Introduction The interest in processing Arabic texts and speech data has grown substantially in the last decade.1 Due to its intrinsic variation, research has been carried out not only on Modern Standard Arabic (MSA), but also on the various Arabic dialects spoken in North Africa and in the Middle East. Research in NLP and Arabic dialects includes, most notably, machine translation of Arabic dialects (Zbib et al., 2012), corpus compilation for Arabic dialects (Al-Sabbagh and Girju, 2012; Cotterell and CallisonBurch, 2014), parsing (Chiang et al., 2006), and Arabic dialect identification (Zaidan and Callison-Burch, 2014). The latter has become a vibrant research topic with several papers published in the last few years (Sadat et al., 2014; Malmasi et al., 2015). In this paper we revisit the task of Arabic dialect identification proposing an ensemble method applied to a corpus of broadcast speeches transcribed from MSA and four Arabic dialects: Egyptian, Gulf, Levantine, and North African (Ali et al., 2016). T"
W17-1201,W16-4802,0,0.127115,"character ngrams and a Na”ive Bayes classifier. The system followed the work of the system submitted to the DSL 2016 by Barbaresi (2016). • CECL: The system uses a two-step approach as in (Goutte et al., 2014). The first step identifies the language group using an SVM classifier with a linear kernel trained on character n-grams (1-4) that occur at least 100 times in the dataset weighted by Okapi BM25 (Robertson et al., 1995). The second step discriminates between each language within the group using a set of SVM classifiers trained • tubasfs: Following the success of tubasfs at DSL 2016 (C¸o¨ ltekin and Rama, 2016), which was ranked first in the closed training track, this year’s tubasfs submission used a linear SVM classifier. The system used both characters and words as features, and carefully optimized hyperparameters: n-gram size and margin/regularization parameter for SVM. 5 In 2016 ADI and DSL were organized under the name DSL shared task, and ADI was run as a sub-task. 4 • gauge: This team submitted a total of three runs. Run 1 used an SVM classifier with character n-grams (2–6), run 2 (their best run) used logistic regression trained using character n-grams (1–6), and run 3 used hard voting of t"
W17-1201,W17-1221,0,0.532877,"of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent pu"
W17-1201,W17-1215,0,0.0474398,"Missing"
W17-1201,W17-1213,0,0.0702486,"pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar langua"
W17-1201,W13-1728,1,0.0339111,"rovided lexical features. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL"
W17-1201,W17-1217,0,0.0300428,"Missing"
W17-1201,W15-5413,0,0.101469,"Missing"
W17-1201,W13-1712,0,0.199423,"Missing"
W17-1201,W14-5316,0,0.160565,"Missing"
W17-1201,U13-1003,0,0.160089,"est set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we looked at how systems perform on discriminating between similar languages and language varieties across different domains, an aspect highlighted by Lui and Cook (2013) and Lui (2014). For this purpose, we provided an out-of-domain test set containing manually annotated microblog posts written in Bosnian, Croatian, Serbian, Brazilian and European Portuguese. 2.1 2.2 Dataset The DSLCC v4.04 contains 22,000 short excerpts of news texts for each language or language variety divided into 20,000 texts for training (18,000 texts) and development (2,000 texts), and 2,000 texts for testing. It contains a total of 8.6 million tokens for training and over half a million tokens for testing. The fourteen languages included in the v4.0 grouped by similarity are Bosnian,"
W17-1201,L16-1284,1,0.900242,"Missing"
W17-1201,W16-3928,0,0.0176574,"ranging from 3 for CLP to 11 for DSL. Below we describe the individual tasks. 2 Discriminating between Similar Languages (DSL) Discriminating between similar languages is one of the main challenges faced by language identification systems. Since 2014 the DSL shared task has been organized every year providing scholars and developers with an opportunity to evaluate language identification methods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the"
W17-1201,W15-5407,1,0.88226,"ods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we l"
W17-1201,W17-1222,1,0.800029,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W17-1211,0,0.333885,"Danish, and Norwegian (TL) – Swedish (SL). Note that the latter two pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the r"
W17-1201,W17-1220,1,0.878577,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W16-4801,1,0.679876,"Missing"
W17-1201,W17-1225,0,0.428199,"us Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to the DSL challenge grow from 8 in 2014 to 10 in 2015 and then to 17 in 2016.2 The 2015 and the 2016 editions of the DSL"
W17-1201,W13-1714,0,0.148384,"ures. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL task. 2.5 Arabic Dial"
W17-1201,W17-1219,0,0.132166,"cia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to th"
W17-1201,W15-5408,0,0.243101,"Missing"
W17-1201,W16-4820,0,0.425079,"Missing"
W17-1201,W17-1212,0,0.183999,"Missing"
W17-1201,W17-1226,0,0.0868204,"Missing"
W17-1201,L16-1641,1,0.367123,"Missing"
W17-1201,N15-1010,0,0.0146702,"Missing"
W17-1201,W15-3040,0,0.0163982,"ams (1–6), and run 3 used hard voting of three systems: SVM, Logistic Regression, and Na”ive Bayes and character ngrams (2–6) as features. • bayesline: This team participated with a Multinomial Na¨ıve Bayes (MNB) classifier similar to that of Tan et al. (2014), with no special parameter tuning, as this system was initially intended to serve as an intelligent baseline for the task (but now it has matured into a competitive system). In their bestperforming run 1, they relied primarily on character 4-grams as features. The feature sets they used were selected by a search strategy as proposed in (Scarton et al., 2015). • cic ualg: This team submitted three runs. Runs 1 and 2 first predict the language group, and then discriminate between the languages within that group. The first step uses an SVM classifier with a combination of character 3– 5-grams, typed character 3-grams, applying the character n-gram categories introduced by Sapkota et al. (2015), and word unigrams using TF-weighting. The second step uses the same features and different classifiers: SVMs + Multinominal Na¨ıve Bayes (MNB) in run 1, and MNB in run 2 (which works best). Run 3 uses a single MNB classifier to discriminate between all fourte"
W17-1201,D10-1112,1,0.910362,"CN i-vector (as in Run 2) with (ii) an SVM model trained on count bag of characters 2–4-grams, which yielded an F1 of 0.612. This year, we introduced a new dialectal area, which focused on German dialects of Switzerland. Indeed, the German-speaking part of Switzerland is characterized by the widespread use of dialects in everyday communication, and by a large number of different dialects and dialectal areas. There have been two major approaches to Swiss German dialect identification in the literature. The corpus-based approach predicts the dialect of any text fragment extracted from a corpus (Scherrer and Rambow, 2010; Hollenstein and Aepli, 2015). The dialectological approach tries to identify a small set of distinguishing dialectal features, which are then elicited interactively from the user in order to identify his or her dialect (Leemann et al., 2016). In this task, we adopt a corpus-based approach, and we develop a new dataset for this. • deepCybErNet: This team submitted two runs. Run 1 adopted a Bi-LSTM architecture using the lexical features, and achieved an F1 score of 0.208, while run 2 used the i-vector features and achieved an F1 of 0.574. 3.3 Results Table 5 shows the evaluation results for t"
W17-1201,W14-5307,1,0.307051,"Missing"
W17-1201,W15-5411,1,0.900323,"Missing"
W17-1201,L16-1680,0,0.0188911,"Missing"
W17-1201,N12-1052,0,0.0102303,"Missing"
W17-1201,W14-1614,1,0.904663,"Missing"
W17-1201,tiedemann-2012-parallel,1,0.0255189,"LP task: parallel training data. Participants were asked not to use the development data with their gold standard annotation of dependency relations for any training purposes. The purpose of the development datasets is entirely for testing model performance during system development. All the knowledge used for parsing should origin in the provided source language data. Other sources (except for target language sources) could also be used in unconstrained submissions, but none of the participants chose that option. For the constrained setup, we also provided parallel datasets coming from OPUS (Tiedemann, 2012) that could be used for training cross-lingual parsers in any way. The datasets included translated movie subtitles and contained quite a bit of noise in terms of alignment, encoding, and translation quality. They were also from a very different domain, which made the setup quite realistic considering that one would used whatever could be found for the task. The sizes of the parallel datasets are given in Table 8. In the setup of the shared task, we also provided simple baselines and an “upper bound” of a model trained on annotated target language data. The cross-lingual baselines included del"
W17-1201,C14-1175,1,0.927054,"nd without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in the literature in particular in connection with dependency parsing (Hwa et al., 2005; McDonald et al., 2013; T¨ackstr¨om et al., 2012; Tiedemann, 2014). The motivation for cross-lingual models is the attempt to bootstrap tools for languages that do not have annotated resources, which are typically necessary for supervised data-driven techniques, using data and resources from other languages. This is especially successful for closely related languages with similar syntactic structures and strong lexical overlap (Agi´c et al., 2012). With this background, it is a natural extension for our shared task to consider cross-lingual parsing as well. We do so by simulating the resource-poor situation by selecting language pairs from the Universal Depe"
W17-1201,W15-2137,1,0.514813,"ad of around 0.7. 4.5 Summary This first edition of the GDI task was a success, given the short time between the 2016 and 2017 editions. In the future, we would like to better control transcriber effects, either by a more thorough selection of training and test data, or by adding transcriber-independent features such as acoustic features, as has been done in the ADI task this year. Further dialectal areas could also be added. 10 5 Cross-lingual Dependency Parsing (CLP) Avoiding gold labels is important here in order to avoid exaggerated results that blur the picture of a more realistic setup (Tiedemann, 2015). The tagger models are trained on the original target language treebanks using UDpipe (Straka et al., 2016) with standard settings and without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in th"
W17-1201,W17-1216,1,0.921592,"shop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and G"
W17-1220,L16-1522,0,0.0672129,"been the focus of a few recent studies. Methods for German dialect identification have proved to be particularly important for the validation of methods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), English varieties (Lui and Cook, 2013), Romanian dialects (Ciobanu and Dinu, 2016), Chinese varieties (Xu et al., 2016), and past editions of the DSL shared task (Zampieri et al., 2014; Zampieri et al., 2015; Malmasi et al., 2016c). Introduction German is well-known for its intrinsic dialectal variation. Standard national varieties spoken in Germany, Austria, and Switzerland co-exist with a number of dialects spoken in everyday communication. The case of Switzerland is particular representative of this situation because of the multitude and importance of dialects which are widely spoken throughout the country. The German Dialect Identification (GDI) task, part of the VarDia"
W17-1220,L16-1641,0,0.266934,"Missing"
W17-1220,W14-5310,0,0.131388,"I shared task. 1 2 Processing dialectal data is a challenge for NLP applications. When dealing with nonstandard language, systems are trained to recognize spelling and syntactic variation for further processing in applications such as Machine Translation. In the case of German, a number of studies have been published on developing NLP tools and resources for processing non-standard language (Dipper et al., 2013), dealing with spelling variation on dialectal data and carrying out spelling normalization (Samardˇzi´c et al., 2015), and improving the performance of POS taggers for dialectal data (Hollenstein and Aepli, 2014). The identification of Swiss German dialects, the topic of the GDI shared task, has been the focus of a few recent studies. Methods for German dialect identification have proved to be particularly important for the validation of methods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Za"
W17-1220,D10-1112,0,0.109538,"n developing NLP tools and resources for processing non-standard language (Dipper et al., 2013), dealing with spelling variation on dialectal data and carrying out spelling normalization (Samardˇzi´c et al., 2015), and improving the performance of POS taggers for dialectal data (Hollenstein and Aepli, 2014). The identification of Swiss German dialects, the topic of the GDI shared task, has been the focus of a few recent studies. Methods for German dialect identification have proved to be particularly important for the validation of methods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), English varieties (Lui and Cook, 2013), Romanian dialects (Ciobanu and Dinu, 2016), Chinese varieties (Xu et al., 2016), and past editions of the DSL shared task (Zampieri et al., 2014; Zampieri et al., 2015; Malmasi et al., 2016c). Introduction German is well-known for its intrinsic dialect"
W17-1220,W15-4415,0,0.0480206,"achieved first place in both the 2015 (Malmasi and Dras, 2015a) and 2014 (Goutte et al., 2014) editions of the DSL shared task.4 3.4 Ensemble Classifiers The best performing system in the 2015 edition of the DSL challenge (Malmasi and Dras, 2015a) used SVM ensembles evidencing the adequacy of this approach for the task of discriminating between similar languages and language varieties. In light of this, we decided to test two ensemble methods. Classifier ensembles have also proven to be an efficient and robust alternative in other text classification tasks such as grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi et al., 2016a). We follow the methodology described by Malmasi and Dras (2015a): we extract a number of different feature types and train a single linear model using each feature type. Our ensemble was created using linear Support Vector Machine classifiers. We used the seven feature types listed in Section 3.2 to create our ensemble of classifiers. Each classifier predicts every input and also assigns a continuous output to each of the possible labels. Using this information, we created the following two ensembles. Features We employ two lexical surf"
W17-1220,U13-1003,0,0.157345,"s, the topic of the GDI shared task, has been the focus of a few recent studies. Methods for German dialect identification have proved to be particularly important for the validation of methods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), English varieties (Lui and Cook, 2013), Romanian dialects (Ciobanu and Dinu, 2016), Chinese varieties (Xu et al., 2016), and past editions of the DSL shared task (Zampieri et al., 2014; Zampieri et al., 2015; Malmasi et al., 2016c). Introduction German is well-known for its intrinsic dialectal variation. Standard national varieties spoken in Germany, Austria, and Switzerland co-exist with a number of dialects spoken in everyday communication. The case of Switzerland is particular representative of this situation because of the multitude and importance of dialects which are widely spoken throughout the country. The German Dialect I"
W17-1220,W14-5307,1,0.899768,"Missing"
W17-1220,W15-5407,1,0.768111,"sified by the systems. The training set contains a total of around 14,000 instances (114,000 tokens) and the test set contains a total of 3,638 instances (29,500 tokens). We approach the text using ensemble classifiers and a meta-classifier. In the next sections we describe the features and algorithms used in the MAZA submissions in detail. 3.2 For our base classifier we use a linear Support Vector Machine (SVM). SVMs have proven to deliver very good performance in discriminating between language varieties and in other text classification problems,3 SVMs achieved first place in both the 2015 (Malmasi and Dras, 2015a) and 2014 (Goutte et al., 2014) editions of the DSL shared task.4 3.4 Ensemble Classifiers The best performing system in the 2015 edition of the DSL challenge (Malmasi and Dras, 2015a) used SVM ensembles evidencing the adequacy of this approach for the task of discriminating between similar languages and language varieties. In light of this, we decided to test two ensemble methods. Classifier ensembles have also proven to be an efficient and robust alternative in other text classification tasks such as grammatical error detection (Xiang et al., 2015), and complex word identification (Malmasi"
W17-1220,W16-4814,1,0.849057,"e used a Random Forest as our meta-classification algorithm. We submitted this system as run 3. 4 Results In this section we present results in two steps. First we comment on the performance obtained using each feature type and the results obtained by cross-validation on the training set. Secondly, we present the official results obtained by our system on the test set and we discuss the performance of our best method in identifying each dialect. Meta-classifier System In addition to classifier ensembles, meta-classifier systems have proven to be very competitive for text classification tasks (Malmasi and Zampieri, 2016) and we decided to include a meta-classifier in our entry. Also referred to as classifier stacking. A meta-classifier architecture is generally composed of an ensemble of base classifiers that each make predictions for all of the input data. Their individual predictions, along with the gold labels are used to train a second-level meta-classifier that learns to predict the label for an input, given the decisions of the individual classifiers. This setup is illustrated in Figure 1. This meta-classifier attempts to learn from the collective knowledge rep4.1 Cross-validation Results We first repor"
W17-1220,W16-0314,1,0.945278,"ods applied to the compilation of German dialect corpora (Scherrer and Rambow, 2010a; Scherrer and Rambow, 2010b; Hollenstein and Aepli, 2015). The work presented here also relates to studies on the discrimination between groups of similar languages, language varieties, and dialects such as South Slavic languages (Ljubeˇsi´c et al., 2007), Portuguese varieties (Zampieri and Gebre, 2012), English varieties (Lui and Cook, 2013), Romanian dialects (Ciobanu and Dinu, 2016), Chinese varieties (Xu et al., 2016), and past editions of the DSL shared task (Zampieri et al., 2014; Zampieri et al., 2015; Malmasi et al., 2016c). Introduction German is well-known for its intrinsic dialectal variation. Standard national varieties spoken in Germany, Austria, and Switzerland co-exist with a number of dialects spoken in everyday communication. The case of Switzerland is particular representative of this situation because of the multitude and importance of dialects which are widely spoken throughout the country. The German Dialect Identification (GDI) task, part of the VarDial Evaluation Campaign 2017 (Zampieri et al., 2017), addressed the problem of German dialectal variation by providing a dataset of transcripts from"
W17-1220,W16-4801,1,0.834739,"Missing"
W17-1222,al-sabbagh-girju-2012-yadac,0,0.0987336,"Missing"
W17-1222,W09-0807,0,0.161607,"Missing"
W17-1222,E06-1047,0,0.0769645,"Missing"
W17-1222,cotterell-callison-burch-2014-multi,0,0.0718728,"Missing"
W17-1222,W15-3205,0,0.0141688,"s grown substantially in the last decades. This is evidenced by several publications on the topic and the dedicated series of workshops (WANLP) co-located with major international computational linguistics conferences.1 Several Arabic dialects are spoken in North Africa and in the Middle East co-existing with Modern Standard Arabic (MSA) in a diglossic situation. Arabic dialects are used in both spoken and written forms (e.g. user-generated content) and pose a number of challenges for NLP applications. Several studies on dialectal variation of Arabic have been published including corpus 2 See Shoufan and Al-Ameri (2015) for a survey on NLP methods for processing Arabic dialects including a section on Arabic dialect identification. 1 https://sites.google.com/a/nyu.edu/ wanlp2017/ 178 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 178–183, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics 3 Methods and Data • Word n-grams: The surface forms of words can be used as a feature for classification. Each unique word may be used as a feature (i.e. unigrams), but the use of bigram distributions is also common. In this scenario, the n-grams"
W17-1222,W14-5316,0,0.0484881,"Missing"
W17-1222,L16-1284,1,0.901992,"Missing"
W17-1222,W14-5313,0,0.115926,"Missing"
W17-1222,W15-4415,0,0.112675,"Missing"
W17-1222,W14-5307,1,0.910842,"Missing"
W17-1222,E14-4019,1,0.882308,"Missing"
W17-1222,W15-5407,1,0.899557,"used as a feature for classification. Each unique word may be used as a feature (i.e. unigrams), but the use of bigram distributions is also common. In this scenario, the n-grams are extracted along with their frequency distributions. For this study we evaluate unigram features. We approach this task as a multi-class classification problem. For our base classifier we utilize a linear Support Vector Machine (SVM). SVMs have proven to deliver very good performance in discriminating between language varieties and in other text classification problems, SVMs achieved first place in both the 2015 (Malmasi and Dras, 2015a) and 2014 (Goutte et al., 2014) editions of the DSL shared task.3 3.1 • iVector Audio Features: Identity vectors or iVectors are a probabilistic compression process for dimensionality reduction. They have been used in speech processing for dialect and accent identification (Bahari et al., 2014), as well as for language identification systems (Dehak et al., 2011). Data The data comes from the aforementioned Arabic dialect dataset by Ali et al. (2016) used in the 2016 edition of the ADI shared task. It contains audio and ASR transcripts of broadcast, debate, and discussion programs from videos"
W17-1222,W16-4814,1,0.537212,"is taken in to account, even when it is not the predicted label (e.g. it could have the second highest probability). This method has been shown to work well on a wide range of problems and, in general, it is considered to be simple, intuitive, stable (Kuncheva, 2014, p. 155) and resilient to estimation errors (Kittler et al., 1998) making it one of the most robust combiners discussed in the literature. We submitted this system as run 2. 3.6 Meta-classifier (System 3) In addition to classifier ensembles, meta-classifier systems have proven to be very competitive for text classification tasks (Malmasi and Zampieri, 2016) and we decided to include a meta-classifier in our entry. Also referred to as classifier stacking, a meta-classifier architecture is generally composed of an ensemble of base classifiers that each make predictions for all of the input data. Their individual predictions, along with the gold labels are used to train a second-level meta-classifier that learns to predict the label for an input, given the decisions of the individual classifiers. This setup is illustrated in Figure 2. This meta-classifier attempts to learn from the collective knowledge represented by the ensemble of local classifie"
W17-1222,N12-1006,0,0.0388755,"Missing"
W17-1222,W16-0314,1,0.87207,"iadsy and Hirschberg (2009), and Bahari et al. (2014). Identifying Arabic dialects in text also became a popular research topic in recent years with several studies published about it (Zaidan and Callison-Burch, 2014; Sadat et al., 2014; Tillmann et al., 2014; Malmasi et al., 2015). To our knowledge, however, the 2017 ADI is the first shared task to provide participants with the opportunity to carry out Arabic dialect identification using a dataset containing both audio and text (transcriptions). The first edition of the ADI shared task, organized in 2016 as a sub-task of the DSL shared task (Malmasi et al., 2016c), used a similar dataset to the ADI 2017 dataset, but included only transcriptions. Introduction The interest in Arabic natural language processing (NLP) has grown substantially in the last decades. This is evidenced by several publications on the topic and the dedicated series of workshops (WANLP) co-located with major international computational linguistics conferences.1 Several Arabic dialects are spoken in North Africa and in the Middle East co-existing with Modern Standard Arabic (MSA) in a diglossic situation. Arabic dialects are used in both spoken and written forms (e.g. user-generat"
W17-1222,W16-4801,1,0.877372,"Missing"
W17-1222,W14-5904,0,0.0644087,"Missing"
W17-5007,C14-1185,0,0.251184,"Missing"
W17-5007,W17-5023,0,0.0297696,"iple kernel learning. WLZ (Li and Zou, 2017) build an ensemble of single-feature SVMs fed into a multi-layer perceptron (MLP), which is a meta-classifier trained on the outputs of the base SVM classifiers. The single features are based on lexical and syntactic information and the best submission includes character, word, stem, and function word n-grams as well as syntactic dependencies. Essay-only Track The best essay-only submission for each team, along with rankings and other details, are listed in Table 4.1. Each team’s best system is briefly described below, ordered by rankings. Uvic-NLP (Chan et al., 2017) trained a single SVM model on word n-grams (1–3) and character n-grams (4-5). They also conducted several postevaluation experiments, improving their results to 0.8730 using an LDA meta-classifier trained on individual SVM classifiers. ItaliaNLP Lab (Cimino and Dell’Orletta, 2017) utilize a novel classifier stacking approach based on a sentence-level classifier whose predictions are used by a second document-level classifier. The sentence classifier is based on a Logistic Regression model trained on standard lexical, stylistic, and syntactic NLI features. The documentclassifier is an SVM, tra"
W17-5007,W17-5049,0,0.19025,"Missing"
W17-5007,W17-5041,0,0.0444426,"Missing"
W17-5007,W17-5024,0,0.0496381,"trained on word bigrams and character 7-grams. They tried a variety of n-gram combinations and found this to work best on the development data. A total of 19 teams participated in the task, 17 of which submitted system description papers. Participation across the three tracks varied, with 17 participants in the essay-only track, 9 in the speech-only track, and 10 in the fusion track. The results for each track are described in the following sections. For every track we briefly outline each team’s best system. Interested readers can refer to the team’s paper for more details. 4.1 UnibucKernel (Ionescu and Popescu, 2017) use different types of character-level string kernels which are combined with multiple kernel learning. WLZ (Li and Zou, 2017) build an ensemble of single-feature SVMs fed into a multi-layer perceptron (MLP), which is a meta-classifier trained on the outputs of the base SVM classifiers. The single features are based on lexical and syntactic information and the best submission includes character, word, stem, and function word n-grams as well as syntactic dependencies. Essay-only Track The best essay-only submission for each team, along with rankings and other details, are listed in Table 4.1."
W17-5007,W17-5025,0,0.0281986,"Missing"
W17-5007,D14-1142,1,0.743654,"Missing"
W17-5007,W17-5021,0,0.0649744,"Missing"
W17-5007,W13-1714,0,0.641856,"Missing"
W17-5007,P17-1134,1,0.867612,"NLI works by identifying language use patterns that are common to certain groups of speakers that share the same native language. This process is underpinned by the presupposition that an author’s linguistic background will dispose them towards particular language production patterns in their learned languages, as influenced by their mother tongue. Predicting the native language of a writer has applications in different fields. It can be used for authorship identification (Estival et al., 2007), forensic analysis (Gibbons, 2003), tracing linguistic influence in potentially multi-author texts (Malmasi et al., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and"
W17-5007,W15-0620,1,0.926713,"Missing"
W17-5007,W17-5048,0,0.0366318,"aseline: essay/transcript/i-vector 0.7901 0.7909 SVM trained on word unigrams (essay/transcript) + i-vectors Baseline: Essay + Transcript 0.7786 0.7791 Linear SVM trained on word unigrams (essays + transcripts) 4 ut.dsp 0.7748 0.7764 n-gram language models over chars/words (essay+transcript) 5 ltl 0.7346 0.7345 No paper submitted. 0.0910 0.0910 Randomly select an L1 Random Baseline Table 3: Official results in the fusion track. The official metric is the macro-averaged F1-score. Accuracy (Acc.) is also reported. Team rankings are determined by statistical significance testing (see §3.1). L2F (Kepler et al., 2017) designed a system that combined three types of text-based classifiers (an RNN with a bidirectional GRU layer, a Naive Bayes classifier with byte n-grams, and a Naive Bayes classifier with n-grams based on representations of the words using Byte Pair Encoding) with versions of the i-vector features that were postprocessed using centering and whitening in an attempt to reduce channel variability. These classifiers were combined together in a Neural Network fusion approach and the authors demonstrated that the i-vector features were the main driver of performance. ETRI-SLP (Oh et al., 2017) subm"
W17-5007,W16-4801,1,0.85993,"Missing"
W17-5007,W17-5043,0,0.116863,"Missing"
W17-5007,W17-5042,0,0.105,"labels. Their experiments indicate that inclusion of the sentence prediction features provides a small increase in performance. ETRI-SLP (Oh et al., 2017) designed a system that was based on word n-gram features (with n ranging from 1 to 3) and character n-gram features (with n ranging from 4 to 6). The normalized count vectors based on these features were used to extract LSA features, which were then reduced using LDA. The count and LSA-LDA features were used to train SVM and DNN classifiers whose outputs were subsequently combined via late fusion in a DNN-based ensemble classifier. CIC-FBK (Markov et al., 2017) build an SVM with multiple lexical and syntactic features. They introduce two new feature types – typed character n-grams and syntactic n-grams – and combine them with word, lemma, and POS n-grams, function words, and spelling error character n-grams. Features are weighted using log-entropy. CEMI (Ircing et al., 2017) use a Logistic Regression meta-classifier to achieve their best essay-only results. The meta-classifier is trained on the outputs of several base classifiers, which are trained on TF-IDF weighted word unigrams, word bigrams, character n-grams and POS n-grams. Groningen (Kulmizev"
W17-5007,W17-5044,0,0.0282337,"t data. A total of 19 teams participated in the task, 17 of which submitted system description papers. Participation across the three tracks varied, with 17 participants in the essay-only track, 9 in the speech-only track, and 10 in the fusion track. The results for each track are described in the following sections. For every track we briefly outline each team’s best system. Interested readers can refer to the team’s paper for more details. 4.1 UnibucKernel (Ionescu and Popescu, 2017) use different types of character-level string kernels which are combined with multiple kernel learning. WLZ (Li and Zou, 2017) build an ensemble of single-feature SVMs fed into a multi-layer perceptron (MLP), which is a meta-classifier trained on the outputs of the base SVM classifiers. The single features are based on lexical and syntactic information and the best submission includes character, word, stem, and function word n-grams as well as syntactic dependencies. Essay-only Track The best essay-only submission for each team, along with rankings and other details, are listed in Table 4.1. Each team’s best system is briefly described below, ordered by rankings. Uvic-NLP (Chan et al., 2017) trained a single SVM mode"
W17-5007,W17-5022,0,0.0430648,"). GadjahMada (Sari et al., 2017) apply a character embedding model with a feed-forward neural network classifier in the essay track. This is based on the relatively high performance of character ngrams in previous research. An embedding size of 25 was used with n-grams of length 2–5. NLI-ISU (Vajjala and Banerjee, 2017) explored the use of n-grams and embeddings in their submissions. Their best run was a Logistic Regression model trained on word 1-3 grams. They also report that spell checking features, as well as word and document embeddings did not work well on the development data. ut.dsp (Mohammadi et al., 2017) utilize n-gram language models over words and characters. For each L1, a language model over character 3- and 4-grams as well as word unigrams and bigrams is calculated and smoothing is applied. For each text in the test set, the probably of the whole text for all language models in each class is calculated and the class with the maximum probability is chosen as the predicted label. This approach does not involve any supervised learning. ¨ IUCL (Smiley and Kubler, 2017) investigated the use of phonetic features for the essay classification task based on the hypothesis that speakers from diffe"
W17-5007,W15-0606,1,0.926679,"vidence from various participants suggests that high-order character n-grams (as high as n = 10) are extremely useful for this task. This is likely because when extracted across word boundaries, these features capture not only sub-word (e.g. morphological) information, but also dependencies between words. However, it should also be noted that the top systems in all tracks made use of syntactic features which can give them a slight performance boost. This is not surprising as it has been shown that lexical and syntactic features each capture diverse types of information that are complementary (Malmasi and Cahill, 2015). Average performance is much higher than 2013. Although much of the training data remains the same, the submissions were much more competitive than the first NLI shared tasks. This is likely due to NLI being a much more established task, as well as the aforementioned prevalence of more sophisticated models such as metaclassifiers. A number of open questions remain. For example, it is not clear if any one approach is dominant across all tracks as most of the top-ranked teams in the essay track did not participate in the other tracks. It is hard to say how well their systems would have done in"
W17-5007,W17-5047,0,0.0363074,"ed on individual SVM classifiers. ItaliaNLP Lab (Cimino and Dell’Orletta, 2017) utilize a novel classifier stacking approach based on a sentence-level classifier whose predictions are used by a second document-level classifier. The sentence classifier is based on a Logistic Regression model trained on standard lexical, stylistic, and syntactic NLI features. The documentclassifier is an SVM, trained using the same features, as well as the sentence prediction labels. Their experiments indicate that inclusion of the sentence prediction features provides a small increase in performance. ETRI-SLP (Oh et al., 2017) designed a system that was based on word n-gram features (with n ranging from 1 to 3) and character n-gram features (with n ranging from 4 to 6). The normalized count vectors based on these features were used to extract LSA features, which were then reduced using LDA. The count and LSA-LDA features were used to train SVM and DNN classifiers whose outputs were subsequently combined via late fusion in a DNN-based ensemble classifier. CIC-FBK (Markov et al., 2017) build an SVM with multiple lexical and syntactic features. They introduce two new feature types – typed character n-grams and syntact"
W17-5007,D14-1144,1,0.925479,"Missing"
W17-5007,N01-1031,0,0.692118,"lly or via Automatic Speech Recognition) and audio features for dialect identification (Malmasi et al., 2016), a task that involves identifying specific dialects of pluricentric languages, such as Spanish or Arabic.1 The combination of transcripts and acoustic features has also provided good results for dialect identification (Zampieri et al., 2017b), demonstrating that it is possible to improve performance by combining this information. While there has been growing interest in using such features, the use of speech transcripts for NLI is not entirely new. In fact, the very first NLI study by Tomokiyo and Jones (2001) was based on applying a Naive Bayes classifier to transcriptions of speech from native and non-native speakers, albeit using limited data. However, this strand of NLI research has not received much attention, most likely due to the costly and laborious nature of collecting and transcribing non-native speech. Following this trend, the 2016 Computational Paralinguistics Challenge (Schuller et al., 2016) also included an NLI task based on the spoken response using the raw audio. The NLI Shared Task 2017 attempts to combine these approaches by including a written response (essay) and a spoken res"
W17-5007,W17-5028,0,0.0402701,"Missing"
W17-5007,W07-0602,0,0.514904,"author texts (Malmasi et al., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on"
W17-5007,W17-5026,0,0.110995,"N classifier 0.7104 0.7109 Linear SVM trained on word unigrams 0.0910 0.0910 Randomly select an L1 Table 1: Official results in the essay-only track. The official metric is the macro-averaged F1-score. Accuracy (Acc.) is also reported. Rankings are determined by statistical significance testing (see §3.1). GadjahMada (Sari et al., 2017) apply a character embedding model with a feed-forward neural network classifier in the essay track. This is based on the relatively high performance of character ngrams in previous research. An embedding size of 25 was used with n-grams of length 2–5. NLI-ISU (Vajjala and Banerjee, 2017) explored the use of n-grams and embeddings in their submissions. Their best run was a Logistic Regression model trained on word 1-3 grams. They also report that spell checking features, as well as word and document embeddings did not work well on the development data. ut.dsp (Mohammadi et al., 2017) utilize n-gram language models over words and characters. For each L1, a language model over character 3- and 4-grams as well as word unigrams and bigrams is calculated and smoothing is applied. For each text in the test set, the probably of the whole text for all language models in each class is"
W17-5007,P11-1093,0,0.0862778,"s, as influenced by their mother tongue. Predicting the native language of a writer has applications in different fields. It can be used for authorship identification (Estival et al., 2007), forensic analysis (Gibbons, 2003), tracing linguistic influence in potentially multi-author texts (Malmasi et al., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Iden"
W17-5007,U09-1008,0,0.367747,"l., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on NLI have been organiz"
W17-5007,W17-5027,0,0.03595,"els over characters (3-4) and words (1-2) Word Unigram Baseline Random Baseline 0.8318 0.8264 0.8264 0.8110 Ensemble of resnets, LSTM and document embeddings Logistic Regression model with word n-grams (1-3) Phonetic features combined in an SVM Char embeddings w/ a feed-forward NN classifier 0.7104 0.7109 Linear SVM trained on word unigrams 0.0910 0.0910 Randomly select an L1 Table 1: Official results in the essay-only track. The official metric is the macro-averaged F1-score. Accuracy (Acc.) is also reported. Rankings are determined by statistical significance testing (see §3.1). GadjahMada (Sari et al., 2017) apply a character embedding model with a feed-forward neural network classifier in the essay track. This is based on the relatively high performance of character ngrams in previous research. An embedding size of 25 was used with n-grams of length 2–5. NLI-ISU (Vajjala and Banerjee, 2017) explored the use of n-grams and embeddings in their submissions. Their best run was a Logistic Regression model trained on word 1-3 grams. They also report that spell checking features, as well as word and document embeddings did not work well on the development data. ut.dsp (Mohammadi et al., 2017) utilize n"
W17-5007,W17-5045,0,0.0520433,"nd the transcriptions, and a second SVM combining the unigrams from the essays and the transcriptions with the i-vectors. The test period for each track lasted 3 days, and teams could submit up to 12 systems per track. The essay-only and speech-only test phases ran concurrently. The IDs for the essay data and transcription data were generated by separate random processes for this test period. For the fusion test period, an updated package providing linked IDs between the essay and spoken transcription data was released. 2 3 http://kaldi-asr.org 65 For more details see §7.3 of Malmasi and Dras (2017) 4 Results tubasfs (Rama and C ¸ o¨ ltekin, 2017) used a single SVM classifier trained on word bigrams and character 7-grams. They tried a variety of n-gram combinations and found this to work best on the development data. A total of 19 teams participated in the task, 17 of which submitted system description papers. Participation across the three tracks varied, with 17 participants in the essay-only track, 9 in the speech-only track, and 10 in the fusion track. The results for each track are described in the following sections. For every track we briefly outline each team’s best system. Intere"
W17-5007,W17-1201,1,0.784529,"Missing"
W17-5007,W17-5046,0,0.0229943,"Missing"
W17-5007,W13-1706,1,0.61284,"cal error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on NLI have been organized where the aim was to identify the L1 of learners of English based on essays (2013) and spoken responses (2016) they provided during a standardized assessment of academic Engl"
W17-5007,C12-1158,1,0.924147,"Missing"
W17-5910,S16-1150,0,0.136082,"Missing"
W17-5910,S16-1161,0,0.157498,"Missing"
W17-5910,L16-1284,1,0.850305,"Missing"
W17-5910,S16-1164,0,0.117013,"Missing"
W17-5910,S16-1157,0,0.169796,"Missing"
W17-5910,P13-3015,0,0.19796,"Goutte et al. (2016) for language variety identification. Introduction Lexical complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI has been addressed as a stand-alone task (Shardlow, 2013) and as part of studies in lexical and text simplification (Paetzold, 2016). The recent SemEval 2016 Task 11 on Complex Word Identification – henceforth SemEval CWI – addressed this challenge by providing participants with a manually annotated dataset for this purpose (Paetzold and Specia, 2016a). In the SemEval CWI dataset, words in context were tagged as complex or non-complex, that is, difficult to be understood by non-native English speakers, or not. Participating teams used this dataset to train classi2 Methods and Experiments In this section we present the data, the methods, and an overv"
W17-5910,S16-1162,0,0.0588997,"Missing"
W17-5910,S16-1146,0,0.0839136,"Missing"
W17-5910,yimam-etal-2017-multilingual,0,0.0467335,"otators over a set of 200 sentences, while the test set is composed by the judgments made over 9,000 sentences by only one annotator. The 9,200 sentences were evenly distributed across the 400 annotators. In the training set, a word is considered to be complex if at least one of the 20 annotators judged them so, thus reproducing a scenario that captures one of the biggest challenges in lexical simplification: predicting the vocabulary limitations of individuals based on the overall limitations of a group. This dataset is one of the few datasets available for CWI, another example is the one by Yimam et al. (2017). We build ensemble classifiers taking the output of systems that participated in the SemEval CWI task as input. This approach is equivalent to training multiple classifiers and combining them using ensembles. Our first goal is to build highperformance classifiers using plurality voting. Our second goal is to estimate the theoretical upper bound performance given the output of the systems that participated in the SemEval CWI competition using the oracle classifier. Following Malmasi et al. (2015) and Goutte et al. (2016) we use two approaches: 2.2 3.1 Plurality Voting: This approach selects th"
W17-5910,W15-0620,1,0.877257,"nvestigate whether human annotation correlates to the systems’ performance by carefully analyzing the samples of multiple annotators. Although in the shared task complexity was modeled as a binary classification task, we pose that lexical complexity should actually be seen in a continuum spectrum. Intuitively, words that are labeled as complex more often should be easier to be predicted by CWI systems. This hypothesis is investigated in Section 3.3. To the best of our knowledge, no evaluation of this kind has been carried out for CWI. The most similar analyses to ours have been carried out by Malmasi et al. (2015) for native language identification and by Goutte et al. (2016) for language variety identification. Introduction Lexical complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI"
W17-5910,S16-1155,1,0.693934,"Missing"
W17-5910,S16-1152,0,0.123637,"Missing"
W17-5910,C16-1069,1,0.841348,"cal complexity plays a crucial role in reading comprehension. Several NLP systems have been developed to simplify texts to second language learners (Petersen and Ostendorf, 2007) and to native speakers with low literacy levels (Specia, 2010) and reading disabilities (Rello et al., 2013). Identifying which words are likely to be considered complex by a given target population is an important task in many text simplification pipelines called complex word identification (CWI). CWI has been addressed as a stand-alone task (Shardlow, 2013) and as part of studies in lexical and text simplification (Paetzold, 2016). The recent SemEval 2016 Task 11 on Complex Word Identification – henceforth SemEval CWI – addressed this challenge by providing participants with a manually annotated dataset for this purpose (Paetzold and Specia, 2016a). In the SemEval CWI dataset, words in context were tagged as complex or non-complex, that is, difficult to be understood by non-native English speakers, or not. Participating teams used this dataset to train classi2 Methods and Experiments In this section we present the data, the methods, and an overview of the experiments we propose in this paper. The goal of the experiment"
W18-0507,W14-1206,0,0.0720755,"Missing"
W18-0507,W18-0519,0,0.153197,"Missing"
W18-0507,S16-1156,0,0.0850516,"Missing"
W18-0507,S16-1151,0,0.0753176,"Missing"
W18-0507,W18-0539,1,0.893727,"Missing"
W18-0507,W18-0538,0,0.100445,"lower log-probability for complex words. The systems submitted performed the best out of all systems for the cross-lingual task (the French dataset) both for the binary and probabilistic classification tasks, showing a promising direction in the creation of CWI dataset for new languages. 1 74 https://code.google.com/archive/p/word2vec/ tant features. Their best system shows an average performance compared to the other systems in the shared task for the monolingual English binary classification track. NLP-CIC present systems for the English and Spanish multilingual binary classification tasks (Aroyehun et al., 2018). The feature sets include morphological features such as frequency counts of target word on large corpora such as Wikipedia and Simple Wikipedia, syntactic and lexical features, psycholinguistic features from the MRC psycholinguistic database and entity features using the OpenNLP and CoreNLP tools, and word embedding distance as a feature which is computed between the target word and the sentence. Tree learners such as Random Forest, Gradient Boosted, and Tree Ensembles are used to train different classifiers. Furthermore, a deep learning approach based on 2D convolutional (CNN) and word embe"
W18-0507,S16-1148,0,0.0982594,"Missing"
W18-0507,W18-0518,0,0.065579,"on the ensemble techniques where AdaBoost classifier with 5000 estimators achieves the highest results, followed by the bootstrap aggregation classifier of Random Forest. All the features are used for the N EWS and W IKI N EWS datasets, but for the W IKIPEDIA dataset, MCR psycholinguistic features are excluded. For the probabilistic classification task, the same feature setups are used and the Linear Regression algorithm is used to estimate values of targets. CoastalCPH describe systems developed for multilingual and cross-lingual domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both"
W18-0507,W18-0520,0,0.193647,"l domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both for the binary and probabilistic classification tasks. They have used features that are based on the insights of the CWI shared task 2016 (Paetzold and Specia, 2016a) such as lexical features (word length, number of syllables, WordNet features such as the number of synsets), word n-gram and POS tags, and dependency parse relations. In addition, they have used features such as the number of words grammatically related to the target word, psycholinguistic features from the MRC database, CEFR (Common European Framework of Reference fo"
W18-0507,S16-1160,0,0.102164,"Missing"
W18-0507,W18-0540,0,0.0627627,"Missing"
W18-0507,W18-0521,0,0.24579,"layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using th"
W18-0507,S16-1164,0,0.0425763,"Missing"
W18-0507,S16-1149,1,0.538636,"elines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second edition focused on multilingualism providing datasets containing four languages: English, German, French, and Spanish. • English monolingual CWI; • German monolingual CWI; • Spanish monolingual CWI; and • Multilingual CWI with a French test set. For the first three tracks, participants were provided with training and testing data for the same language. For French, participants were provided only with a French test set and no French training data. In the CWI 2016, the task was cast as binary classification. To be able t"
W18-0507,S16-1162,0,0.138871,"Missing"
W18-0507,S16-1158,0,0.0719992,"Missing"
W18-0507,S16-1163,0,0.176167,"Missing"
W18-0507,D14-1162,0,0.0887949,"earning methods, 2) using the average embedding of target words as an input to a neural network, and 3) modeling the context of the target words using an LSTM. For the feature engineering-based systems, features such as linguistic, psycholinguistic, and language model features were used to train different binary and probabilistic classifiers. Lexical features include word length, number of syllables, and number of senses, hypernyms, and hyponyms in WordNet. For N-gram features, probabilities of the n-gram containing the target words were For embedding-based systems, a pre-trained GloVe model (Pennington et al., 2014) was used to get the vector representations of target words. For MWE, the average of the vectors is used. In the first approach, the resulting vector is passed on to a neural network with two ReLu layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For t"
W18-0507,S16-1154,1,0.847385,"Missing"
W18-0507,W18-0541,0,0.102296,"Missing"
W18-0507,S16-1153,1,0.879975,"Missing"
W18-0507,S16-1161,0,0.200613,"Missing"
W18-0507,S16-1147,0,0.032877,"Missing"
W18-0507,S16-1157,0,0.212408,"Missing"
W18-0507,I11-1017,0,0.0188214,"that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using the scikit-learn library. Feature ablation shows that both the length, frequency, and probability features (based on corpu"
W18-0507,P13-3015,0,0.247331,"ative speakers. To train their systems, participants received a labeled training set where words in context were annotated regarding their complexity. One month later, an unlabeled test set was provided and participating teams were required to upload their predictions for evaluation. More information about the data collection is presented in Section 3. Given the multilingual dataset provided, the CWI challenge was divided into four tracks: Introduction The most common first step in lexical simplification pipelines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second e"
W18-0507,S16-1152,0,0.262443,"Missing"
W18-0507,S16-1159,0,0.0576998,"Missing"
W18-0507,L16-1035,1,0.906663,"Missing"
W18-0507,W18-0522,0,0.230666,"n dependency relation, frequency features based on the BNC, Wikipedia, and Dale and Chall list corpora, number of synsets and senses in WordNet, and so on. The experiment is conducted using the Weka machine learning framework using the Support vector machine (with linear and radial basis function kernels), Na¨ıve Bayes, Logistic Regression, Random Tree, and Random Forest classification algorithms. The final experiments employ Support Vector Machines and Random Forest classifiers. CFILT IITB Developed ensemble-based classification systems for the English monolingual binary classification task (Wani et al., 2018). Lexical features based on WordNet for the target word are extracted as follows: 1) Degree of Polysemy: number of senses of the target word in WordNet, 2) Hyponym and Hypernym Tree Depth: the position of the word in WordNet’s hierarchical tree, and 3) Holonym and Meronym Counts: based on the relationship of the target word to its components (meronyms) or to the things it is contained in (Holonym’s). Additional feature classes include size-based features such as word count, word length, vowel counts, and syllable counts. They also use vocabulary-based features such as Ogden Basic (from Ogden’s"
W18-0507,S16-1146,0,0.112937,"Missing"
W18-0507,I17-2068,1,0.54282,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,yimam-etal-2017-multilingual,1,0.591517,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,W17-5910,1,0.857961,"Missing"
W18-0507,S16-1155,1,0.729801,"Missing"
W18-0507,S16-1150,0,\N,Missing
W18-0534,W17-5007,1,0.740618,"as presented in Table 3. COPLE2 LEIRIA PEAPL2 TOTAL Texts 1,058 Tokens 201,921 Types 9,373 TTR 0.05 330 57,358 4,504 0.08 480 1,868 121,138 380,417 6,808 20,685 0.06 0.05 Table 1: Distribution of the dataset: Number of texts, tokens, types, and type/token ratio (TTER) per source corpus. Related Work NLI has attracted a lot of attention in recent years. Due to the availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline results for this task. Finally, as N"
W18-0534,boyd-etal-2014-merlin,0,0.0457443,"Missing"
W18-0534,padro-stanilovsky-2012-freeling,0,0.245629,"Missing"
W18-0534,E14-3013,0,0.0214445,"d: • Computer-aided Language Learning (CALL): CALL software has been developed for Portuguese (Marujo et al., 2009). Further improvements in these tools can take advantage of the training material available in NLI-PT for a number of purposes such as L1-tailored exercise design. • Grammatical error detection and correction: as discussed in Zampieri and Tan (2014), a known challenge in this task is acquiring suitable training data to account for the variation of errors present in non-native texts. One of the strategies developed to cope with this problem is to generate artificial training data (Felice and Yuan, 2014). Augmenting training data using a suitable annotated dataset such as NLI-PT can improve the quality of existing grammatical error correction systems for Portuguese. 5 Conclusion and Future Work This paper presented NLI-PT, the first Portuguese dataset compiled for NLI. NLI-PT contains 1,868 texts written by speakers of 15 L1s amounting to over 380,000 tokens. As discussed in Section 4, NLI-PT opens several avenues for future research. It can be used for different research purposes beyond NLI such as grammatical error correction and CALL. An experiment with the texts written by the speakers of"
W18-0534,W16-6502,0,0.0608381,"Missing"
W18-0534,W14-3625,1,0.839027,"LI has attracted a lot of attention in recent years. Due to the availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline results for this task. Finally, as NLI-PT can be used in other applications besides NLI, it is important to point out that a number of studies have been published on educational NLP applications for Portuguese and on the 6 NLI-PT is available http://www.clul.ulisboa.pt/en/resources-en/11resources/894-nli-pt-a-portuguese-native-languageide"
W18-0534,W13-1706,0,0.0208356,") Leiria corpus, and (iii) PEAPL27 as presented in Table 3. COPLE2 LEIRIA PEAPL2 TOTAL Texts 1,058 Tokens 201,921 Types 9,373 TTR 0.05 330 57,358 4,504 0.08 480 1,868 121,138 380,417 6,808 20,685 0.06 0.05 Table 1: Distribution of the dataset: Number of texts, tokens, types, and type/token ratio (TTER) per source corpus. Related Work NLI has attracted a lot of attention in recent years. Due to the availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline resu"
W18-0534,E14-4019,1,0.846479,"LI has attracted a lot of attention in recent years. Due to the availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline results for this task. Finally, as NLI-PT can be used in other applications besides NLI, it is important to point out that a number of studies have been published on educational NLP applications for Portuguese and on the 6 NLI-PT is available http://www.clul.ulisboa.pt/en/resources-en/11resources/894-nli-pt-a-portuguese-native-languageide"
W18-0534,W15-0614,1,0.832559,"availability of suitable data, as discussed earlier, this attention has been particularly focused on English. The most notable examples are the two editions of the NLI shared task organized in 2013 (Tetreault et al., 2013) and 2017 (Malmasi et al., 2017). Even though most NLI research has been carried out on English data, an important research trend in recent years has been the application of NLI methods to other languages, as discussed in Malmasi and Dras (2015). Recent NLI studies on languages other than English include Arabic (Malmasi and Dras, 2014a) and Chinese (Malmasi and Dras, 2014b; Wang et al., 2015). To the best of our knowledge, no study has been published on Portuguese and the NLI-PT dataset opens new possibilities of research for Portuguese. In Section 4.1 we present the first simple baseline results for this task. Finally, as NLI-PT can be used in other applications besides NLI, it is important to point out that a number of studies have been published on educational NLP applications for Portuguese and on the 6 NLI-PT is available http://www.clul.ulisboa.pt/en/resources-en/11resources/894-nli-pt-a-portuguese-native-languageidentification-dataset Corpus Description The three corpora co"
W18-3901,W18-3913,0,0.217545,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3919,0,0.254393,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3932,0,0.129693,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W17-1223,0,0.131933,"data to reduce the transcriber effects seen last year. 7 • The LaMa system is a blend (weighted vote) of eight classifiers being stochastic gradient descent (hinge and modified Huber), multinomial Na¨ıve Bayes, both counts and tf-idf, FastText, and modified Kneser-Ney smoothing. The classifiers were trained using word n-grams (1-6) and character n-grams (1-8). The hyperparameters were determined with cross-validation and searching on the development set. • XAC system is a refined version of the n-gram-based Bayesline system described in last year’s XAC submission to the VarDial shared tasks (Barbaresi, 2017), and previously used as a baseline for the DSL shared task (Tan et al., 2014). The XAC team achieved their best results using a Na¨ıve Bayes classifier. • The GDI classification system is based on an ensemble of multiple SVM classifiers. The system was trained on various word- and character-level features. • The dkosmajac system is based on a normalized Euclidean distance measure. The distances are calculated between a sample and each class profile. The class profiles are generated by selecting the most frequent features for each class, which results in profiles that are of the same length fo"
W18-3901,W18-3918,0,0.0590619,"Missing"
W18-3901,W18-3925,0,0.142653,"Missing"
W18-3901,W17-1214,0,0.138486,"was part of the first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions gen"
W18-3901,W18-3909,0,0.15091,"Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive evaluation exercise with four shared tasks in 2017. This year, the VarDial workshop featured the second edition of the VarDial evaluation campaign w"
W18-3901,W18-3933,1,0.889149,"Missing"
W18-3901,W18-3920,1,0.880795,"Missing"
W18-3901,W18-3926,0,0.0553879,"Missing"
W18-3901,W18-3921,0,0.054375,"Missing"
W18-3901,W17-1225,0,0.134424,"r of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layers, which represents the prob"
W18-3901,W16-4818,0,0.048944,", duration (Dur.), in number of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layer"
W18-3901,W18-3915,0,0.0733204,"Missing"
W18-3901,W18-3929,0,0.411409,"Missing"
W18-3901,W18-3907,0,0.178349,"Missing"
W18-3901,W18-3922,0,0.0589219,"Missing"
W18-3901,W18-3928,0,0.0556119,"Missing"
W18-3901,kumar-2012-challenges,1,0.824307,"ed printed stories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Osl"
W18-3901,kumar-2014-developing,1,0.832345,"ories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Oslo team submit"
W18-3901,W14-0405,1,0.912975,"Missing"
W18-3901,W18-3917,1,0.897664,"Missing"
W18-3901,L16-1242,1,0.902855,"Missing"
W18-3901,L16-1676,1,0.914666,"Missing"
W18-3901,W16-4814,1,0.86288,"-Oslo system (C ¸ o¨ ltekin et al., 2018) is trained on word and character n-grams using a single SVM classifier, which is fine-tuned using cross-validation. It is similar to the submissions by the same authors to previous VarDial shared tasks (C¸o¨ ltekin and Rama, 2017; C¸o¨ ltekin and Rama, 2016). They also tried an approach based on RNN, which worked worse. • Arabic Identification system is based on an ensemble of SVM classifiers trained on character and word n-grams. The approach is similar to the systems ranked second and first in the previous two ADI tasks (Malmasi and Zampieri, 2017a; Malmasi and Zampieri, 2016). 5 5.3 Results Six teams submitted runs for the ADI shared task and the results are shown in Table 3. The best result, an F1 score of 0.589, was achieved by UnibucKernel,1 followed by safina, with an F1 score of 0.575. The following three teams are tied for the third place as they are not statistically different. Rank 1 2 3 3 3 4 Team F1 (Macro) UnibucKernel safina BZU SYSTRAN T¨ubingen-Oslo Arabic Identification 0.589 0.576 0.534 0.529 0.514 0.500 Table 3: ADI results: ranked taking statistical significance into account. 5.4 Summary We introduced multi-phoneme representation for the dialecta"
W18-3901,W17-1222,1,0.833986,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W17-1220,1,0.80558,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W16-4801,1,0.733081,"Missing"
W18-3901,W18-3927,0,0.0554913,"Missing"
W18-3901,W18-3914,0,0.175754,"ES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive eva"
W18-3901,W18-3924,0,0.0833746,"and the number of submissions varied widely across the tasks, ranging from 6 entries for ADI and MTT to 12 entries for DFS. Table 1 lists the participating teams, the shared tasks they took part in, and a reference to the system description paper. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participate"
W18-3901,L16-1641,1,0.509642,"Missing"
W18-3901,W17-1224,1,0.744067,"Missing"
W18-3901,W18-3923,1,0.879634,"Missing"
W18-3901,W14-5307,1,0.857122,"Missing"
W18-3901,W15-5401,1,0.855743,"Missing"
W18-3901,W17-1201,1,0.607978,"Missing"
W18-3920,W16-4819,0,0.0223457,"om 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dat"
W18-3920,W15-5410,0,0.0278754,"gs are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and n"
W18-3920,D14-1069,0,0.0726343,"Missing"
W18-3920,W17-1213,0,0.0135081,"tween Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 20"
W18-3920,L16-1284,1,0.827322,"ombination. With the aid of Hindi speakers, in Section 5.1 we presented a concise error analysis of the misclassified instances of the development set. We observed a few interesting patterns in the misclassified instances, most notably that many of the misclassified sentences were too short, containing only one, two or three words, and that several of them contained only named entities. making it very challenging for classifiers to identify the language of these instances. Another issue discussed in Section 5.1, is that some instances could not be discriminated by native speakers, as noted by Goutte et al. (2016). To cope with these instances one possible direction for future work is to allow a multi-label classification setup in which sentences could be assign to more than one category if annotators labeled them as such. In future work we would like to explore and compare our methods to other high performance methods for this task. In particular, we would like to try an implementation of the token-based back-off method proposed by the SUKI team. As evidenced in Section 5, SUKI’s system achieved substantially higher performance than the other methods in this competition. Acknowledgements We would like"
W18-3920,W15-5408,0,0.088526,"., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dataset made available by the organizers of the Indo-Ary"
W18-3920,W16-4820,0,0.0514295,"e of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dataset made available by the organizers of the Indo-Aryan Language Identification"
W18-3920,W15-5407,1,0.925336,"he DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed description of our system is presented in Section 4. 3 Data The dataset made available by the organizers of the Indo-Aryan Language Identification (ILI) task comprises five similar languages spoken"
W18-3920,W16-4801,1,0.936687,", 2018) and in previous work (Tiedemann and Ljubešić, 2012; Goutte et al., 2016), discriminating between similar languages is one of the main challenges in automatic language identification. State-of-the-art n-gram-based language identification systems are able to discriminate between unrelated languages with very high performance but very often struggle to discriminate between similar languages. This challenge motivated the organization of recent evaluation campaigns such as the TweetLID (Zubiaga et al., 2016) which included languages spoken in the Iberian peninsula and the DSL shared tasks (Malmasi et al., 2016b; Zampieri et al., 2015) which included groups of similar languages such as Malay and Indonesian, Bulgarian and Macedonian, and Bosnian, Croatian, and Serbian as well as groups of language varieties such as Brazilian and European Portuguese. In this paper we revisit the problem of discriminating between similar languages presenting a system to discriminate between five languages of the Indo-Aryan family: Hindi, Braj Bhasha, Awadhi, Bhojpuri, and Magahi. Inspired by systems that performed well in past editions of the DSL shared task such as the one by Malmasi and Dras (2015), we developed a sy"
W18-3920,W14-5314,0,0.0166142,"identification of very similar languages in multilingual settings are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks"
W18-3920,W14-5318,0,0.017647,"shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiainen et al., 2015; Jauhiainen et al., 2016), and classifier ensembles (Malmasi and Dras, 2015), the approach we apply in this paper. Classifier ensembles showed very good performance not only in language identification but also in similar tasks. Therefore, we build on the experience of our previous work and improve the system that we have previously applied to similar tasks, namely author profiling (Ciobanu et al., 2017) and native language identification (Zampieri et al., 2017a). A detailed desc"
W18-3920,C12-1160,0,0.0763191,"Missing"
W18-3920,W15-4415,0,0.0311268,"aterial or external resource. 4 Methodology Following our aforementioned previous work (Ciobanu et al., 2017), we built a classification system based on SVM ensembles using the same methodology proposed by Malmasi and Dras (2015). The purpose of using classification ensembles is to improve the overall performance and robustness by combining the results of multiple classifiers. Such systems have proved successful not only in NLI and dialect identification, but also in various text classification tasks, such as complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015). The classifiers can differ in a wide range of aspects; for example, algorithms, training data, features or parameters. We implemented our system using the Scikit-learn (Pedregosa et al., 2011) machine learning library, with each classifier in the ensemble using a different type of features. For the individual classifiers, we employed the SVM implementation based on the Liblinear library (Fan et al., 2008), LinearSVC1 , with a linear kernel. This implementation has the advantage of scaling well to large number of samples. For the ensemble, we employed the majority rule VotingClassifier2 , whi"
W18-3920,W14-5307,1,0.895397,", Croatian, Montenegrin, and Serbian (Ljubesic and Kranjcic, 2015). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 178 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 178–184 Santa Fe, New Mexico, USA, August 20, 2018. A first attempting of benchmarking the identification of very similar languages in multilingual settings are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word"
W18-3920,W15-5401,1,0.921353,"Missing"
W18-3920,W17-5045,1,0.84077,"bian (Ljubesic and Kranjcic, 2015). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/ 178 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 178–184 Santa Fe, New Mexico, USA, August 20, 2018. A first attempting of benchmarking the identification of very similar languages in multilingual settings are the aforementioned Discriminating between Similar Languages (DSL) shared tasks. The DSL tasks have been organized from 2014 (Zampieri et al., 2014) to 2017 (Zampieri et al., 2017b) within the scope of the VarDial workshop series. Four versions of the DSLCC dataset (Tan et al., 2014) have been released containing short excerpts of journalistic texts written in similar languages and language varieties. In the four editions of the DSL shared task a variety of computation methods have been tested. This includes Maximum Entropy (Porta and Sancho, 2014), Prediction by Partial Matching (PPM) (Bobicev, 2015), language model perplexity (Gamallo et al., 2017), SVMs (Purver, 2014), Convolution Neural Networks (CNNs) (Belinkov and Glass, 2016), word-based back-off models (Jauhiai"
W18-3920,W18-3901,1,0.883574,"Missing"
W18-3933,W17-1223,0,0.191393,"and dataset were similar to those of the 2018 edition presented in more detail in Section 3. The results of the 2017 edition along with a reference to each system description paper are presented in Table 1. Rank 1 2 3 4 5 6 7 8 9 10 Team MAZA CECL CLUZH qcri mit unibuckernel tubasfs ahaqst Citius Ixa Imaxin XAC Bayesline deepCybErNet F1 (weighted) 0.662 0.661 0.653 0.639 0.637 0.626 0.614 0.612 0.605 0.263 Reference (Malmasi and Zampieri, 2017b) (Bestgen, 2017) (Clematide and Makarov, 2017) (Ionescu and Butnaru, 2017) (C¸o¨ ltekin and Rama, 2017) (Hanani et al., 2017) (Gamallo et al., 2017) (Barbaresi, 2017) - Table 1: GDI shared task 2017: Closed submission results. The ten teams who competed in the first GDI challenge applied different computational methods to approach the task. These include linear SVM classifiers (C¸o¨ ltekin and Rama, 2017; Bestgen, 2017), string kernels (Ionescu and Butnaru, 2017), Naive Bayes classifiers (Barbaresi, 2017), and SVM ensembles (Malmasi and Zampieri, 2017b), which achieved the first place in 2017. For this reason, this is the approach we apply in our GDI identification system. 3 Data In this paper we used only the dataset provided by the GDI organizers. The da"
W18-3933,W17-1214,0,0.0935605,"kshop on NLP for Similar Languages, Varieties and Dialects, pages 288–294 Santa Fe, New Mexico, USA, August 20, 2018. The GDI shared task 2017 setup and dataset were similar to those of the 2018 edition presented in more detail in Section 3. The results of the 2017 edition along with a reference to each system description paper are presented in Table 1. Rank 1 2 3 4 5 6 7 8 9 10 Team MAZA CECL CLUZH qcri mit unibuckernel tubasfs ahaqst Citius Ixa Imaxin XAC Bayesline deepCybErNet F1 (weighted) 0.662 0.661 0.653 0.639 0.637 0.626 0.614 0.612 0.605 0.263 Reference (Malmasi and Zampieri, 2017b) (Bestgen, 2017) (Clematide and Makarov, 2017) (Ionescu and Butnaru, 2017) (C¸o¨ ltekin and Rama, 2017) (Hanani et al., 2017) (Gamallo et al., 2017) (Barbaresi, 2017) - Table 1: GDI shared task 2017: Closed submission results. The ten teams who competed in the first GDI challenge applied different computational methods to approach the task. These include linear SVM classifiers (C¸o¨ ltekin and Rama, 2017; Bestgen, 2017), string kernels (Ionescu and Butnaru, 2017), Naive Bayes classifiers (Barbaresi, 2017), and SVM ensembles (Malmasi and Zampieri, 2017b), which achieved the first place in 2017. For this reason"
W18-3933,W17-1221,0,0.355937,"Missing"
W18-3933,W17-1213,0,0.0231086,"Missing"
W18-3933,W17-1211,0,0.0363003,"Missing"
W18-3933,W17-1225,0,0.061586,"and Dialects, pages 288–294 Santa Fe, New Mexico, USA, August 20, 2018. The GDI shared task 2017 setup and dataset were similar to those of the 2018 edition presented in more detail in Section 3. The results of the 2017 edition along with a reference to each system description paper are presented in Table 1. Rank 1 2 3 4 5 6 7 8 9 10 Team MAZA CECL CLUZH qcri mit unibuckernel tubasfs ahaqst Citius Ixa Imaxin XAC Bayesline deepCybErNet F1 (weighted) 0.662 0.661 0.653 0.639 0.637 0.626 0.614 0.612 0.605 0.263 Reference (Malmasi and Zampieri, 2017b) (Bestgen, 2017) (Clematide and Makarov, 2017) (Ionescu and Butnaru, 2017) (C¸o¨ ltekin and Rama, 2017) (Hanani et al., 2017) (Gamallo et al., 2017) (Barbaresi, 2017) - Table 1: GDI shared task 2017: Closed submission results. The ten teams who competed in the first GDI challenge applied different computational methods to approach the task. These include linear SVM classifiers (C¸o¨ ltekin and Rama, 2017; Bestgen, 2017), string kernels (Ionescu and Butnaru, 2017), Naive Bayes classifiers (Barbaresi, 2017), and SVM ensembles (Malmasi and Zampieri, 2017b), which achieved the first place in 2017. For this reason, this is the approach we apply in our GDI identification"
W18-3933,U13-1003,0,0.0738265,"Missing"
W18-3933,W15-5407,1,0.849532,"ning the four dialects plus a ‘surprise’ dialect not included in the training set. We opted to participate only in the first track which contained only previously ‘seen’ dialects. The dataset comprise nearly 25,000 instances divided in training, development, and test partitions as presented in Table 2. Partition Training Development Test Total Instances 14,647 4,659 5,543 24,849 Table 2: Instances in the GDI dataset 2018. 4 Methodology The system that we propose for the GDI shared task consists of an ensemble of classifiers, namely SVMs. In this approach, we employ the methodology proposed by Malmasi and Dras (2015). Ensembles of classifiers are deemed useful when there are disagreements between the comprising classifiers, which can use different features, training data, algorithms or parameters. The scope of the ensemble is to combine the results of the classifiers in such a way that the overall performance is improved 1 http://www.spur.uzh.ch/en/departments/research/textgroup/ArchiMob.html 289 over the individual performances of the classifiers. Ensembles have proven useful in various tasks, such as complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015)"
W18-3933,W17-1222,1,0.894163,", and finally the first German Dialect Identification (GDI) shard task in 2017 (Zampieri et al., 2017). The GDI shared task 2017 preceded the second GDI shared task (Zampieri et al., 2018) in which our team, GDI classification, participated. In this paper we describe the GDI classification system trained to identify four dialects of (Swiss) German. The GDI dataset included speech transcripts from speakers from Basel, Bern, Lucerne, and Zurich. The system is based on an ensemble of multiple SVM classifiers trained on words and characters as features. Our approach is inspired by the approach of Malmasi and Zampieri (2017b), which was ranked first in the first edition of the GDI task and also performed well on identifying dialects of Arabic (Malmasi and Zampieri, 2017a). We build on the experience of previous work of members of the GDI classification team improving a system that we have previously applied to a similar classification task, namely author profiling (Ciobanu et al., 2017). 2 Related Work: The First GDI Shared Task There have been a few studies on German dialect identification published before the first GDI shared task, using different corpora and evaluation methods (Scherrer and Rambow, 2010; Holl"
W18-3933,W17-1220,1,0.817949,", and finally the first German Dialect Identification (GDI) shard task in 2017 (Zampieri et al., 2017). The GDI shared task 2017 preceded the second GDI shared task (Zampieri et al., 2018) in which our team, GDI classification, participated. In this paper we describe the GDI classification system trained to identify four dialects of (Swiss) German. The GDI dataset included speech transcripts from speakers from Basel, Bern, Lucerne, and Zurich. The system is based on an ensemble of multiple SVM classifiers trained on words and characters as features. Our approach is inspired by the approach of Malmasi and Zampieri (2017b), which was ranked first in the first edition of the GDI task and also performed well on identifying dialects of Arabic (Malmasi and Zampieri, 2017a). We build on the experience of previous work of members of the GDI classification team improving a system that we have previously applied to a similar classification task, namely author profiling (Ciobanu et al., 2017). 2 Related Work: The First GDI Shared Task There have been a few studies on German dialect identification published before the first GDI shared task, using different corpora and evaluation methods (Scherrer and Rambow, 2010; Holl"
W18-3933,W16-4801,1,0.893288,"Missing"
W18-3933,L16-1641,0,0.215341,"Missing"
W18-3933,D10-1112,0,0.0217023,"h of Malmasi and Zampieri (2017b), which was ranked first in the first edition of the GDI task and also performed well on identifying dialects of Arabic (Malmasi and Zampieri, 2017a). We build on the experience of previous work of members of the GDI classification team improving a system that we have previously applied to a similar classification task, namely author profiling (Ciobanu et al., 2017). 2 Related Work: The First GDI Shared Task There have been a few studies on German dialect identification published before the first GDI shared task, using different corpora and evaluation methods (Scherrer and Rambow, 2010; Hollenstein and Aepli, 2015). To the best of our knowledge, the first GDI shared task organized in 2017 was the first attempt to provide a benchmark for this task. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 288 Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 288–294 Santa Fe, New Mexico, USA, August 20, 2018. The GDI shared task 2017 setup and dataset were similar to those of the 2018 edition presented in more detail in Section 3. The results"
W18-3933,W14-5313,0,0.0606865,"Missing"
W18-3933,W15-4415,0,0.0268304,"asi and Dras (2015). Ensembles of classifiers are deemed useful when there are disagreements between the comprising classifiers, which can use different features, training data, algorithms or parameters. The scope of the ensemble is to combine the results of the classifiers in such a way that the overall performance is improved 1 http://www.spur.uzh.ch/en/departments/research/textgroup/ArchiMob.html 289 over the individual performances of the classifiers. Ensembles have proven useful in various tasks, such as complex word identification (Malmasi et al., 2016a) and grammatical error diagnosis (Xiang et al., 2015). To distinguish the classifiers, we employ a different type of features for each of them. After obtaining predictions from each classifier, they need to be combined, to obtain the final predictions of the ensemble. To implement this system we used the Scikit-learn (Pedregosa et al., 2011) library. For the individual classifiers, we used LinearSVC,2 an SVM implementation based on the Liblinear library (Fan et al., 2008), with a linear kernel. For the ensemble, we used the VotingClassifier,3 with a majority rule fusion method: for each instance, the class that has been predicted by the majority"
W18-3933,W14-5307,0,0.102897,"Missing"
W18-3933,W15-5401,0,0.246759,"Missing"
W18-3933,W17-1201,1,0.902646,"Missing"
W18-3933,W18-3901,1,0.890697,"Missing"
W18-3933,L16-1522,1,\N,Missing
W18-3933,W17-1218,0,\N,Missing
W18-4401,W18-4411,0,0.269919,"Missing"
W18-4401,W18-4417,0,0.0963664,"Missing"
W18-4401,W18-4416,0,0.18561,"Missing"
W18-4401,W18-4409,0,0.0701163,"Missing"
W18-4401,R15-1086,0,0.178547,"Missing"
W18-4401,W18-4407,1,0.878228,"Missing"
W18-4401,L18-1226,1,0.762344,"Missing"
W18-4401,W18-4415,0,0.0689419,"Missing"
W18-4401,W18-4410,0,0.0536961,"Missing"
W18-4401,malmasi-zampieri-2017-detecting,1,0.349626,"eople. It is therefore important that preventive measures can be taken to cope with abusive behaviour aggression online. One of the strategies to cope with aggressive behaviour online is to manually monitor and moderate user-generated content, however, the amount and pace at which new data is being created on the web has rendered manual methods of moderation and intervention almost completely impractical. As such the use (semi-) automatic methods to identify such behaviour has become important and has attracted more attention from the research community in recent years (Davidson et al., 2017; Malmasi and Zampieri, 2017). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1 Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying, pages 1–11 Santa Fe, USA, August 25, 2018. This paper reports the results of the first Shared Task on Aggression Identification which was organised jointly with the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018. 2 Related Work Verbal aggression per se has been rarely explored within the field of Natural Language Processing. However,"
W18-4401,K15-1032,0,0.0438624,"kshop on Trolling, Aggression and Cyberbullying, pages 1–11 Santa Fe, USA, August 25, 2018. This paper reports the results of the first Shared Task on Aggression Identification which was organised jointly with the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018. 2 Related Work Verbal aggression per se has been rarely explored within the field of Natural Language Processing. However, previous research in the field has been carried out to automatically recognise several related behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greev"
W18-4401,W18-4423,0,0.110195,"Missing"
W18-4401,W18-4406,0,0.0471652,"Missing"
W18-4401,I13-1066,0,0.2398,"nta Fe, USA, August 25, 2018. This paper reports the results of the first Shared Task on Aggression Identification which was organised jointly with the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018. 2 Related Work Verbal aggression per se has been rarely explored within the field of Natural Language Processing. However, previous research in the field has been carried out to automatically recognise several related behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004) and others. In addition"
W18-4401,W18-4419,0,0.0576791,"Missing"
W18-4401,W18-4414,0,0.0438083,"Missing"
W18-4401,W18-4404,0,0.205006,"Missing"
W18-4401,W18-4403,0,0.0269422,"Missing"
W18-4401,W18-4418,0,0.101033,"Missing"
W18-4401,W18-4408,0,0.071919,"Missing"
W18-4401,W18-4402,0,0.0564524,"Missing"
W18-4401,W17-1101,0,0.284481,"ted behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004) and others. In addition to these, there have been some pragmatic studies on behaviour like trolling (Hardaker, 2010; Hardaker, 2013). This huge interest in the field from different perspectives has created a conglomeration of terminologies as well as understandings of the phenomenon. On the one hand, this provides us with a very rich and extensive insight into the phenomena yet, on the other hand, it has also created a theoretical gap in the understanding of interrelationship"
W18-4401,W18-4421,0,0.0392755,"Missing"
W18-4401,N16-2013,0,0.160048,"recognise several related behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004) and others. In addition to these, there have been some pragmatic studies on behaviour like trolling (Hardaker, 2010; Hardaker, 2013). This huge interest in the field from different perspectives has created a conglomeration of terminologies as well as understandings of the phenomenon. On the one hand, this provides us with a very rich and extensive insight into the phenomena yet, on the other hand, it has also created a theoretical gap in the underst"
W18-4401,W17-3012,0,0.433423,"llying (TRAC - 1) at COLING 2018. 2 Related Work Verbal aggression per se has been rarely explored within the field of Natural Language Processing. However, previous research in the field has been carried out to automatically recognise several related behaviour such as trolling (Cambria et al., 2010; Kumar et al., 2014; Mojica, 2016; Mihaylov et al., 2015) , cyberbullying (Dinakar et al., 2012; Nitta et al., 2013; Dadvar et al., 2013; Dadvar et al., 2014; Hee et al., 2015), flaming / insults (Sax, 2016; Nitin et al., 2012), abusive / offensive language (Chen et al., 2012; Nobata et al., 2016; Waseem et al., 2017), hate speech (Pinkesh Badjatiya and Varma, 2017; Burnap and Williams, 2014; Davidson et al., 2017; Vigna et al., 2017; Djuric et al., 2015; Fortana, 2017; Gitari et al., 2015; Malmasi and Zampieri, 2018; Waseem and Hovy, 2016; Schmidt and Wiegand, 2017), radicalization (Agarwal and Sureka, 2015; Agarwal and Sureka, 2017), racism (Greevy and Smeaton, 2004; Greevy, 2004) and others. In addition to these, there have been some pragmatic studies on behaviour like trolling (Hardaker, 2010; Hardaker, 2013). This huge interest in the field from different perspectives has created a conglomeration of t"
W19-1401,W19-1402,0,0.268136,"Missing"
W19-1401,P19-1068,1,0.913629,"guages and asked to predict the valid morphological analyses for a seventh, unseen language. In the “Semi-Closed” track, the process was the same, only participants were provided with additional raw data by the organisers. This was in the form of raw text Wikipedia dumps, bilingual dictionaries from the Apertium project and any treebanks available in the known languages from the Universal Dependencies project. Moldavian vs. Romanian Cross-dialect Topic identification (MRC): In the Moldavian vs. Romanian Cross-topic Identification shared task, we provided participants with the MOROCO data set (Butnaru and Ionescu, 2019) which contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports, and tech. The samples are pre-processed in order to eliminate named entities. For each sample, the data set provides corresponding dialectal and category labels. To this end, we proposed three subtasks for the 2019 VarDial Evaluation Campaign. The first sub-task was a binary classification by dialect task, in which a classification model is required to discriminate between the Moldavian and the Romanian dialec"
W19-1401,W18-3929,1,0.894916,"Missing"
W19-1401,W19-1413,1,0.836296,"Missing"
W19-1401,W19-1416,0,0.056458,"Missing"
W19-1401,W18-3907,1,0.895659,"Missing"
W19-1401,Y96-1018,1,0.197287,"k. 5.5 Summary Three teams participated in this first iteration of the cross-lingual analysis task. Two of the teams employed variations of neural encoderdecoder systems. Apart from lemmatization performance, it proved to be difficult to attain consistent improvements over the neural baseline systems. However, the suffix stripping approach used by the HSE team did deliver clear improvements in lemmatization for both Turkic and Romance languages. 6 6.1 Dataset Texts to distinguish between the two variations were compiled from the two existing corpora of news: Sinica Corpus for Taiwan Mandarin (Chen et al., 1996) and LCMC (The Lancaster Corpus of Mandarin Chinese, (McEnery and Xiao, 2003)) for Mainland Mandarin. Both corpora are segmented and tokenized. We remove the punctuation and unify the orthography used to eliminate orthographic cues. Since both corpora are balanced corpora, our initial thought was to provide genre-aware classification. However, inspection of both corpora suggested the genres were not defined in the same way and are not distributed homogeneously. In the next edition this idea may be exploited by using some additional resources as genre vs. regional variations which is an importa"
W19-1401,W19-1419,1,0.847943,"Missing"
W19-1401,W19-1414,0,0.0601978,"Missing"
W19-1401,W16-4801,1,0.6692,"Missing"
W19-1401,W19-1420,0,0.0913091,"ces from newspapers for each Mandarin variety. The main task is to determine if a sentence is written in the Mandarin Cuneiform Language Identification (CLI): This shared task focused on discriminating between languages and dialects originally written using the cuneiform script. The task included 2 dif2 Team Adaptcenter BAM dkosmajac DTeam SharifCL ghpaetzold gretelliz92 ekh IUCL HSE itsalexyang lonewolf MineriaUNAM NRC-CNRC R2I LIS PZ SC-UPB situx SUKI tearsofjoy T¨ubingenOslo Twist Bytes Total GDI CMA DMT X MRC CLI X X System Description Papers (Butnaru, 2019) X X X X X X (Tudoreanu, 2019) (Doostmohammadi and Nassajian, 2019) X X (Hu et al., 2019) (Mikhailov et al., 2019) (Yang and Xiang, 2019) X X X X X X X X (Bernier-Colborne et al., 2019) (Chifu, 2019) (Paetzold and Zampieri, 2019) (Onose and Cercel, 2019) X X X X X X X 7 5 X 8 X X 6 3 (Jauhiainen et al., 2019b) (Wu et al., 2019) (C¸o¨ ltekin and Barnes, 2019) (Benites et al., 2019) 14 Table 1: The teams that participated in the Third VarDial Evaluation Campaign. took part in, and a reference to each of the 14 system description papers published in the VarDial workshop proceedings. ferent languages: Sumerian and Akkadian. Furthermore, the Akkadian language was"
W19-1401,W19-1415,0,0.0347692,"Missing"
W19-1401,L18-1550,0,0.0287077,"sed on a majority voting scheme applied on five classification models: kNearest Neighbors, Logistic Regression, Support Vector Machines, Neural Networks and Random Forests. For the first and the third runs, the models are trained on both training and development sets. For the second run, the model is trained only on the training set. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting"
W19-1401,W18-4802,0,0.0115539,"zers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form whic"
W19-1401,W19-1417,0,0.0619352,"Missing"
W19-1401,E17-2034,0,0.0280356,"e-art for this task, however, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjective"
W19-1401,D18-1135,1,0.824726,"016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ionescu and Butnaru, 2018). • Binary classification by dialect (subtask 1) – the task is to discriminate between the Moldavian and the Romanian dialects. • MD→RO cross-dialect multi-class categorization by topic (subtask 2) – the task is to classify the samples written in the Romanian dialect into six topics, using a model trained on samples written in the Moldavian dialect. • RO→MD cross-dialect multi-class categorization by topic (subtask 3) – the task is to classify the samples written in the Moldavian dialect into six topics, using a model trained on samples written in the Romanian dialect. 7.2 Participants and App"
W19-1401,W19-1409,1,0.877823,"Missing"
W19-1401,W19-1418,0,0.0615736,"Missing"
W19-1401,N16-1174,0,0.0228023,"t. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ione"
W19-1401,W19-1423,1,0.825557,"e ranking for subtask 3, as shown in Table 8. 7.4 Dataset Summary We proposed three MRC subtasks for VarDial 2019. Three participants submitted runs for all three subtasks, and another two participants submitted runs only for subtask 1. Two teams (DTeam 5 12 http://oracc.museum.upenn.edu Language or Dialect Sumerian Old Babylonian Middle Babylonian peripheral Standard Babylonian Neo-Babylonian Late Babylonian Neo-Assyrian two systems in more detail. The PZ team used a SVM metaclassifier ensemble of several linear SVM classifiers trained using character n-gram and character skip-gram features. Paetzold and Zampieri (2019) give further details. The SharifCL team submitted three runs and their best performing system was an ensemble of a SVM and a NB classifier (Doostmohammadi and Nassajian, 2019). The ghpaetzold team submitted only one run using 2-layer compositional recurrent neural network that learns numerical representations of sentences based on their words, and of words based on their characters. Their system is described in more detail by Paetzold and Zampieri (2019). The ekh team used a sum of relative frequencies of character bigrams together with a penalty value for those bigrams or unigrams that were"
W19-1401,W17-1201,1,0.803354,"Missing"
W19-1401,L16-1641,1,0.880892,"Missing"
W19-1401,W14-5307,1,0.821127,"Missing"
W19-1401,W18-0209,1,0.707638,"r, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then"
W19-1401,W19-0301,1,0.916428,"task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form which did not include at least one"
W19-1401,E14-2006,0,0.0437089,"Missing"
W19-1401,W19-1422,0,0.0730531,"Missing"
W19-1401,W19-1412,0,0.0741523,"Missing"
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
