2002.tmi-papers.14,W99-0606,0,0.0689983,"Missing"
2002.tmi-papers.14,A00-2020,0,0.0829102,"achine translation. We describe the modality corpus in Section 2, the method of corpus correction in Section 3, and our experiments on corpus correction in Section 4. 2 Modality Corpus for Machine Translation In this section, we describe the modality corpus. A part of the modality corpus is shown in Figure 1. It is composed of a Japanese-English bilingual corpus; each English sentence can include two types of tags: 1 There is no previous paper on error correction in corpora. In terms of error detection in corpora, there has been research using boosting or anomaly detection (Abney et al. 1999; Eskin 2000). , kono kodomo wa aa ieba kou iu kara koniku-rashii This child always talks back to me, and this &lt;v>is&lt;/v> why I &lt;vj>hate&lt;/vj> him. d kare ga aa okubyou da to wa omowanakatta I &lt;v>did not think&lt;/v> he was so timid. c aa isogashikute wa yasumu hima mo nai hazu da Such a busy man as he &lt;v>cannot have&lt;/v> any spare time. Figure 1: Part of the modality corpus • The English main verb phrase is tagged with &lt;v>. • The English verb phrase corresponding to the Japanese main verb phrase is tagged with &lt;vj>. The symbols at the beginning of each Japanese sentence, such as “c” and “d”, indicate a category"
2002.tmi-papers.14,1999.tmi-1.7,1,0.874862,"Missing"
2002.tmi-papers.14,P94-1013,0,0.00968044,"f each category/feature pair as calculated from p˜(a, b) are the same as those from p(a, b) (this corresponds to Equation (1).) These estimated values are not so sparse. We can thus use the above assumption for calculating p(a, b). Furthermore, we maximize the entropy of the distribution of p˜(a, b) to obtain one solution of p˜(a, b), because using only Equation 1 produces several solutions for p˜(a, b). Maximizing the entropy has the effect of making the distribution more uniform and is considered to be a good solution for data sparseness problems. • Method based on the decision-list method (Yarowsky 1994) In this method, the probability of each category is calculated using one of the features, f j (∈ F,  ≤ j ≤ k). The probability that produces category a in context b is given by the following equation: p(a|b) = p(a|f max ), (3) such that f max is defined by f max = argmaxf j ∈F maxai ∈A p˜(ai |f j ), (4) where p˜(ai |f j ) is the occurrence rate of category a i when the context has feature f j. In this paper, we used the following items as features, which are the context when the probabilities are calculated; 26 (= 5 + 10 + 10 + 1) features appear in each English sentence: • the strings of 1-"
2005.mtsummit-papers.10,J97-2004,0,0.152501,"d (&c&&)). (3) Posstra expresses the possibility estimated by and Aaug as alignment results. using the traditional Chinese characters. 5.1.2 Algorithm for broadening coverage Let WJ ( WC ) denote the list of words Bilingual Dictionary j ∈ WJ (c ∈ WC ) that are still not aligned. In this A translation dictionary can help to identify the translation relations. Let C j denote the Chinese phase, we only consider one to one alignment. ~ ~ For j (∈ WJ ) , we estimate the possibility of j translation set of j . We can estimate the possibility of j being aligned with &c&& using the following formula (Ker and Chang, 1997). Possdic ( j ,&c&&) = max Sim(c' ,&c&&). c '∈C j being aligned with c~ (∈ WC ) as follows. ~ For an alignment candidate ( j , c~ ) , we estimate its likelihood by taking the established alignments into account. Here we consider four established alignments: the two alignments that are the nearest ~ to j on the left and right and the two alignments that are the nearest to c~ on the left and right. Null0 , Null0 ) and First, add ( (4) Possdic expresses the possibility estimated by using a translation dictionary. An automatically built Japanese-Chinese dictionary is used here, which was built fro"
2005.mtsummit-papers.10,maekawa-etal-2000-spontaneous,1,0.906267,"Missing"
2005.mtsummit-papers.10,P01-1067,0,0.0308519,"Missing"
2005.mtsummit-papers.10,C94-2209,0,0.0409686,"Annotation on Chinese Sentences For Chinese morphological analysis, we used the analyser developed by Peking University, where the research on definition of Chinese words and the criteria of word segmentation has been conducted for over ten years. The achievements include a grammatical knowledge base of contemporary Chinese, an automatic morphological analyser, and an annotated People’s Daily Corpus. Since the definition and tagset are widely used in Chinese language processing, we also took the criteria as the basis of our guidelines. A morphological analyzer developed by Peking University (Zhou and Yu, 1994) was applied for automatic annotation of the Chinese sentences and then the automatically tagged sentences were revised by humans. An annotated sentence is illustrated in Figure 3, which is the Chinese sentence in Ex. 1 in Section 2. The interface of the tool is shown in Figure 4 and Figure 5. S-ID: 950104141-008 这些/r 俄军/j 士兵/n 均/d 为/v 十九/m 岁/q 左右/m 的/u 年青人/n ，/w 他们/r 甚至/d 连/p 回答/v 问题/n 的/u 气力/n 也/d 没有/v 。/w Figure 3 An annotated Chinese sentence 4.3 Tool for Manual Revision We developed a tool to assist annotators in revision. The tool has both Japanese and Chinese versions. Here, we introduc"
2005.mtsummit-papers.10,W04-2208,1,\N,Missing
2005.mtsummit-papers.18,J93-2003,0,0.0071722,"of the multi-aligner. 1 Introduction In a parallel corpus, automatic word alignment is to identify the translation relations between the words in a source sentence and those in a target sentence. A word-aligned parallel corpus has many applications, such as machine translation, machineaided translation, bilingual lexicography, and wordsense disambiguation. For these applications, much research on automatic word alignment has been conducted and reported. The statistics-based approach is widely studied (Och and Ney, 2003), and is mainly based on the research of statistical machine translation (Brown et al., 1993). However, this approach incorrectly aligns less frequently occurring words when statistically significant evidence is not available. Instead of word-based statistics, Ker proposed a class-based approach by using lexicon resources (Ker and Chang, 1997). Based on this idea, various 2 Japanese-Chinese Parallel Corpus The corpus we used in this study consists of 38,383 Japanese sentences from Mainichi newspaper and their Chinese translations. The corpus has been morphological annotated (word segmented and part-of-speech tagged) in the first phase of the project. For Japanese morphological 133 spe"
2005.mtsummit-papers.18,P00-1050,0,0.0436575,"Missing"
2005.mtsummit-papers.18,J97-2004,0,0.207324,"such as machine translation, machineaided translation, bilingual lexicography, and wordsense disambiguation. For these applications, much research on automatic word alignment has been conducted and reported. The statistics-based approach is widely studied (Och and Ney, 2003), and is mainly based on the research of statistical machine translation (Brown et al., 1993). However, this approach incorrectly aligns less frequently occurring words when statistically significant evidence is not available. Instead of word-based statistics, Ker proposed a class-based approach by using lexicon resources (Ker and Chang, 1997). Based on this idea, various 2 Japanese-Chinese Parallel Corpus The corpus we used in this study consists of 38,383 Japanese sentences from Mainichi newspaper and their Chinese translations. The corpus has been morphological annotated (word segmented and part-of-speech tagged) in the first phase of the project. For Japanese morphological 133 special case when k = 0 . Actually, the case of more-to-one has also been considered in the study. For simplicity of describtion, however, only the case of one-to-more is described here. Three kinds of lexical resources used for the estimation are describ"
2005.mtsummit-papers.18,maekawa-etal-2000-spontaneous,1,0.796201,"Missing"
2005.mtsummit-papers.18,J03-1002,0,0.0088417,"statistics-based aligner at the same time. Quantitative results confirmed the effectiveness of the multi-aligner. 1 Introduction In a parallel corpus, automatic word alignment is to identify the translation relations between the words in a source sentence and those in a target sentence. A word-aligned parallel corpus has many applications, such as machine translation, machineaided translation, bilingual lexicography, and wordsense disambiguation. For these applications, much research on automatic word alignment has been conducted and reported. The statistics-based approach is widely studied (Och and Ney, 2003), and is mainly based on the research of statistical machine translation (Brown et al., 1993). However, this approach incorrectly aligns less frequently occurring words when statistically significant evidence is not available. Instead of word-based statistics, Ker proposed a class-based approach by using lexicon resources (Ker and Chang, 1997). Based on this idea, various 2 Japanese-Chinese Parallel Corpus The corpus we used in this study consists of 38,383 Japanese sentences from Mainichi newspaper and their Chinese translations. The corpus has been morphological annotated (word segmented and"
2005.mtsummit-papers.18,C94-2209,0,0.0449107,"ir Chinese translations. The corpus has been morphological annotated (word segmented and part-of-speech tagged) in the first phase of the project. For Japanese morphological 133 special case when k = 0 . Actually, the case of more-to-one has also been considered in the study. For simplicity of describtion, however, only the case of one-to-more is described here. Three kinds of lexical resources used for the estimation are described below . annotation, the definition of the Corpus of Spontaneous Japanese was adopted (Maekawa, 2000). For Chinese, the definition of Peking University was adopted (Zhou and Yu, 1994). The average lengths of the sentences on both sides are about 30 words. The study, word alignment, aims to assist to word alignment annotation, which is a task in the second phase of the project. 3 Orthography About half of Japanese words contain kanji, the Chinese characters used in Japanese writing. We call them kanji words. Japanese words may also contain hiragana or katakana, which are phonetic characters. Because some kanji words were adapted directly from China, their Chinese translations are the same as the words themselves. For example, the Chinese translations for the Japanese words"
2007.mtsummit-papers.73,C94-2209,0,0.128581,"Missing"
C00-1074,W96-0102,0,0.0512099,"Missing"
C00-1074,C94-1027,0,0.0855273,"Missing"
C00-1074,J94-2001,0,\N,Missing
C00-1074,J95-4004,0,\N,Missing
C00-1082,J96-1002,0,0.00692912,"of information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combinati"
C00-1082,W98-1118,0,0.0131898,"tic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combination of features. Unlike the decision-tree method, as a result w"
C00-1082,W95-0107,0,0.0509382,"hashi, for example, made 146 rules for bunsetsu identi cation (Kurohashi, 1998). In an attempt to reduce the number of manhours, we used machine-learning methods for bunsetsu identi cation. Because it was not clear which machine-learning method would be the one most appropriate for bunsetsu identi cation, so we tried a variety of them. In this paper we report experiments comparing four machine-learning methods (decision tree, maximum entropy, example-based, and decision list methods) and our new methods using category-exclusive rules. 1 Bunsetsu identi cation is a problem similar to chunking (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) in other languages. 2 Bunsetsu identi cation problem We conducted experiments on the following supervised learning methods for identifying bunsetsu:  Decision tree method  Maximum entropy method  Example-based method (use of similarity)  Decision list (use of probability and frequency)  Method 1 (use of exclusive rules)  Method 2 (use of exclusive rules with the highest similarity). In general, bunsetsu identi cation is done after morphological and before syntactic analysis. Morphological analysis corresponds to part-of-speech tagging in English. Japanese synta"
C00-1082,W96-0213,0,0.0136307,"major POS, (ii) minor POS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combination of features. Unl"
C00-1082,W97-0301,0,0.0125915,"or POS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combination of features. Unlike the decision-tr"
C00-1082,E99-1023,0,0.0134246,"46 rules for bunsetsu identi cation (Kurohashi, 1998). In an attempt to reduce the number of manhours, we used machine-learning methods for bunsetsu identi cation. Because it was not clear which machine-learning method would be the one most appropriate for bunsetsu identi cation, so we tried a variety of them. In this paper we report experiments comparing four machine-learning methods (decision tree, maximum entropy, example-based, and decision list methods) and our new methods using category-exclusive rules. 1 Bunsetsu identi cation is a problem similar to chunking (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) in other languages. 2 Bunsetsu identi cation problem We conducted experiments on the following supervised learning methods for identifying bunsetsu:  Decision tree method  Maximum entropy method  Example-based method (use of similarity)  Decision list (use of probability and frequency)  Method 1 (use of exclusive rules)  Method 2 (use of exclusive rules with the highest similarity). In general, bunsetsu identi cation is done after morphological and before syntactic analysis. Morphological analysis corresponds to part-of-speech tagging in English. Japanese syntactic structures are usuall"
C00-1082,E99-1026,1,0.925638,"t for analyzing Japanese sentences. In experiments comparing the four previously available machinelearning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best. 1 Introduction This paper is about machine learning methods for identifying bunsetsus, which correspond to English phrasal units such as noun phrases and prepositional phrases. Since Japanese syntactic analysis is usually done after bunsetsu identi cation (Uchimoto et al., 1999), identifying bunsetsu is important for analyzing Japanese sentences. The conventional studies on bunsetsu identi cation1 have used hand-made rules (Kameda, 1995; Kurohashi, 1998), but bunsetsu identi cation is not an easy task. Conventional studies used many hand-made rules developed at the cost of many man-hours. Kurohashi, for example, made 146 rules for bunsetsu identi cation (Kurohashi, 1998). In an attempt to reduce the number of manhours, we used machine-learning methods for bunsetsu identi cation. Because it was not clear which machine-learning method would be the one most appropriate"
C00-1082,P94-1013,0,0.0437241,"| Symbol Punctuation 2 2 Figure 5: Example of levels of similarity but are expanded by combining all the features, and are stored in a one-dimensional list. A priority order is de ned in a certain way and all of the rules are arranged in this order. The decision-list method searches for rules from the top of the list and analyzes a particular problem by using only the rst applicable rule. In this study we used in the decision-list method the same 152 types of patterns that were used in the maximum-entropy method. To determine the priority order of the rules, we referred to Yarowsky&apos;s method (Yarowsky, 1994) and Nishiokayama&apos;s method (Nishiokayama et al., 1998) and used the probability and frequency of each rule as measures of this priority order. When multiple rules had the same probability, the rules were arranged in order of their frequency. Suppose, for example, that Pattern A Noun: Normal Noun; Particle: Case-Particle: none: wo; Verb: Normal Form: 217; Symbol: Punctuation&quot; occurs 13 times in a learning set and that ten of the occurrences include the inserted partition mark. Suppose also that Pattern B Noun; Particle; Verb; Symbol&quot; occurs 123 times in a learning set and that 90 of the occur"
C00-2126,J96-1002,0,\N,Missing
C00-2126,P99-1018,0,\N,Missing
C02-1060,P94-1038,0,\N,Missing
C02-1060,P93-1022,0,\N,Missing
C02-1060,P90-1034,0,\N,Missing
C02-1060,P97-1009,0,\N,Missing
C02-1060,P98-2148,0,\N,Missing
C02-1060,C98-2143,0,\N,Missing
C04-1165,P90-1034,0,0.139456,"ocused on classifying the semantic relationship between abstract nouns and adjectives (Nemoto 1969, Takahashi 1975). We constructed linguistic data by extracting semantic relations between abstract nouns and adjectives from corpus data and classifying abstract nouns based on adjective similarity using a self-organizing semantic map (SOM), which is a neural network model (Kohonen 1995). The relative proximity of words in the semantic map indicates their relative similarity. In previous research, word meanings have been statistically modeled based on syntactic information derived from a corpus. Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. These methods are useful for the organization of words deep within a hierarchy, but do not seem to provide a solution for the top levels of the hierarchy. To find an objective hierarchical word structure, we utilize the complementary similarity measure (CSM), which estimates a one-to-many relation, such as superordinate–subordinate relations (Hagita and Sawaki 1995, Yamamoto and Umemura 2002). In this paper we propose an automated method for constructing adjective h"
C04-1165,W00-0110,1,0.825311,"e., the goat, and “hana (nose)” in (b) indicates part of something, i.e., the elephant. He recognized abstract nouns in (a) as a hyperonym of the attribute that the predicative adjectives express. Nemoto (1969) identified expressions such as “iro ga akai (the color is red)” and “hayasa ga hayai (the speed is fast)” as a kind of meaning repetition, or tautology. In this paper we define such abstract nouns that co-occur with adjectives as adjective hyperonyms. We semi-automatically extracted from corpora 365 abstract nouns used as this kind of head noun, according to the procedures described in Kanzaki et al. (2000). We collected abstract nouns from two year's worth of articles from the Mainichi Shinbun newspaper, and extracted adjectives co-occurring with abstract nouns in the Abstract nouns are located in the semantic map based on the similarity of co-occurring adjectives after iteratively learning over input data. In this research, we focus on abstract nouns co-occurring with adjectives. In the semantic map, there are 365 abstract nouns co-occurring with adjectives. The similarities between the 365 abstract nouns are determined according to the number of common co-occurring adjectives. We made a list"
C04-1165,P93-1023,0,\N,Missing
C98-2127,H90-1055,0,0.0759493,"Missing"
C98-2127,W96-0102,0,0.0420088,"Missing"
C98-2127,J94-2001,0,0.284868,"Missing"
C98-2127,C94-1027,0,0.0333278,"ements are adopted in the inputs for tagging and that there are 50 POSs. The n-gram models must estimate 50 r = 7.8e + 11 n-grams, while the single-neuro tagger with the longest input uses 805 only 70,000 weights, which can be calculated by nipt • nhid + nhid • ltopt where ?~,ipt, ~Zhid, and nopt are, respectively, the number of units in the input, the hidden, and the output layers, and nhid is set to be nipt/2. T h a t neuro models require few parameters may offer another advantage: their performance is less affected by a small amount of training d a t a than that of the statistical methods (Schmid, 1994). Neuro taggers also offer last tagging compared to other models, although its training stage is longer. 5 Experimental Results The Thai corpus used in tile computer experiments contains 10,452 sentences that are randomly divided into two sets: one with 8,322 sentences for training and another with 2,130 sentences for testing. The training and testing sets contain, respectively, 22,311 and 6,7117 ambiguous words that serve as more than one POS and were used for training and testing. Because there are 47 types of POSs in Thai (Charoenporn et al., 1997), n in (6), (10), and (14) was set at 47. T"
I05-2015,J97-2004,0,0.0332403,"he annotated data to improve the performance of automatic word alignment. We will also investigate a method to automatically identify phrase alignments from the annotated word alignment and a method to automatically discover the syntactic structures on the Chinese side from the annotated phrase alignments. Annotation of word alignment Since automatic word alignment techniques cannot reach as high a level as the morphological analyses, we adopt a practical method of using multiple aligners. One aligner is a lexical knowledge-based approach, which was implemented by us based on the work of Ker (Ker and Chang, 1997). Another aligner is the well-known GIZA++ toolkit, which is a statistics-based approach. For GIZA++, two directions were adopted: the Chinese sentences were used as source sentences and the Japanese sentences as target sentences, and vice versa. The results produced by the lexical knowledgebased aligner, C → J of GIZA++, and J → C of GIZA++ were selected in a majority decision. If an alignment result was produced by two or three aligners at the same time, the result was accepted. Otherwise, was abandoned. In this way, we aimed to utilize the results of each aligner and maintain high precision"
I05-2015,maekawa-etal-2000-spontaneous,1,0.909793,"Missing"
I05-2015,P01-1067,0,0.0246433,"Missing"
I05-2015,C94-2209,0,0.0395746,"Annotation on Chinese Sentences For Chinese morphological analysis, we used the analyser developed by Peking University, where the research on definition of Chinese words and the criteria of word segmentation has been conducted for over ten years. The achievements include a grammatical knowledge base of contemporary Chinese, an automatic morphological analyser, and an annotated People’s Daily Corpus. Since the definition and tagset are widely used in Chinese language processing, we also took the criteria as the basis of our guidelines. A morphological analyzer developed by Peking University (Zhou and Yu, 1994) was applied for automatic annotation of the Chinese sentences and then the automatically tagged sentences were revised by humans. An annotated sentence is illustrated in Figure 3, which is the Chinese sentence in Ex. 1 in Section 2. The interface of the tool is shown in Figure 4 and Figure 5. S-ID: 950104141-008 这些/r 俄军/j 士兵/n 均/d 为/v 十九/m 岁/q 左右/m 的/u 年青人/n ，/w 他们/r 甚至/d 连/p 回答/v 问题/n 的/u 气力/n 也/d 没有/v 。/w Figure 3 An annotated Chinese sentence 4.3 Tool for Manual Revision We developed a tool to assist annotators in revision. The tool has both Japanese and Chinese versions. Here, we introduc"
I05-2015,W04-2208,1,\N,Missing
I05-2024,P96-1003,0,0.0357172,"osed method was much higher than that of the conventional TFIDF method when the process was focused on retrieving highly relevant documents, suggesting that the proposed method might be especially suited to information retrieval tasks in which precision is more critical than recall. 1 Introduction Information retrieval (IR) has been studied since an earlier stage [e.g., (Menzel, 1966)] and several kinds of basic retrieval models have been proposed (Salton and Buckley, 1988) and a number of improved IR systems based on these models have been developed by adopting various NLP techniques [e.g., (Evans and Zhai, 1996; Mitra et al., 1997; Mandara, et al., 1998; Murata, et al., 2000)]. However, an epoch-making technique 138 1 There have been a number of studies of SOM on data mining and visualization [e.g., (Kohonen, et al., 2000)] since the WEBSOM was developed in 1996. To our knowledge, however, these works mainly focused on confirming the capabilities of SOM in the self-organization and/or in the visualization. In this study, we slot the SOM-based processing into a practical IR system that enables visualization of the IR while at the same time improving its precision. The another feature of our study dif"
I05-2024,W98-0704,0,0.0308187,"he conventional TFIDF method when the process was focused on retrieving highly relevant documents, suggesting that the proposed method might be especially suited to information retrieval tasks in which precision is more critical than recall. 1 Introduction Information retrieval (IR) has been studied since an earlier stage [e.g., (Menzel, 1966)] and several kinds of basic retrieval models have been proposed (Salton and Buckley, 1988) and a number of improved IR systems based on these models have been developed by adopting various NLP techniques [e.g., (Evans and Zhai, 1996; Mitra et al., 1997; Mandara, et al., 1998; Murata, et al., 2000)]. However, an epoch-making technique 138 1 There have been a number of studies of SOM on data mining and visualization [e.g., (Kohonen, et al., 2000)] since the WEBSOM was developed in 1996. To our knowledge, however, these works mainly focused on confirming the capabilities of SOM in the self-organization and/or in the visualization. In this study, we slot the SOM-based processing into a practical IR system that enables visualization of the IR while at the same time improving its precision. The another feature of our study differing from others is that we performed com"
I08-2100,P02-1054,0,0.036911,"and “Tokyo is also one of Japan’s 47 prefectures”, from Websites, newspaper articles, or encyclopedias. The system then outputs “Tokyo” as the correct answer. We believe question-answering systems will become a more convenient alternative to other systems designed for information retrieval and a basic component of future artificial intelligence systems. Numerous researchers have recently been attracted to this important topic. These researchers have produced many interesting studies on question-answering systems (Kupiec, 1993; Ittycheriah et al., 2001; Clarke et al., 2001; Dumis et al., 2002; Magnini et al., 2002; Moldovan et al., 2003). Evaluation conferences and contests on question-answering systems have also been held. In particular, the U.S.A. has held the Text REtrieval Conferences (TREC) (TREC-10 committee, 2001), and Japan has hosted the QuestionAnswering Challenges (QAC) (National Institute of Informatics, 2002) at NTCIR (NII Test Collection for IR Systems ) 3. These conferences and contests have aimed at improving question-answering systems. The researchers who participate in these create question-answering systems that they then use to answer the same questions, and each system’s performanc"
I08-2100,N04-1008,0,0.0336121,"systems that they then use to answer the same questions, and each system’s performance is then evaluated to yield possible improvements. We addressed non-factoid question answering in NTCIR-6 QAC-4. For example, when the question was “Why are people opposed to the Private Information Protection Law?” the system retrieved sentences based on terms appearing in the question and output an answer using the retrieved sentences. Numerous studies have addressed issues that are involved in the answering of non-factoid questions (Berger et al., 2000; Blair-Goldensohn et al., 2003; 727 Xu et al., 2003; Soricut and Brill, 2004; Han et al., 2005; Morooka and Fukumoto, 2006; Maehara et al., 2006; Asada, 2006). We constructed a system for answering nonfactoid Japanese questions for QAC-4. We used methods of passage retrieval for the system. We extracted paragraphs based on terms from an input question and output them as the preferred answers. We classified the non-factoid questions into six categories. We used a particular method for each category. For example, we increased the scores of paragraphs including the word “reason” for questions including the word “why.” We performed experiments using the NTCIR-6 QAC-4 data"
kanzaki-etal-2006-semantic,C04-1161,0,\N,Missing
kanzaki-etal-2006-semantic,C02-1144,0,\N,Missing
kanzaki-etal-2006-semantic,C92-2082,0,\N,Missing
kanzaki-etal-2006-semantic,P99-1016,0,\N,Missing
kanzaki-etal-2006-semantic,P90-1034,0,\N,Missing
kanzaki-etal-2006-semantic,P99-1063,1,\N,Missing
kanzaki-etal-2006-semantic,P99-1008,0,\N,Missing
kanzaki-etal-2006-semantic,P02-1029,0,\N,Missing
kanzaki-etal-2006-semantic,P93-1023,0,\N,Missing
kanzaki-etal-2006-semantic,N04-1041,0,\N,Missing
ma-etal-2008-selection,H05-1059,0,\N,Missing
ma-etal-2008-selection,W05-1514,0,\N,Missing
ma-etal-2008-selection,P98-1069,0,\N,Missing
ma-etal-2008-selection,C98-1066,0,\N,Missing
ma-etal-2008-selection,P03-1010,1,\N,Missing
P00-1042,W98-1120,0,\N,Missing
P00-1042,M98-1018,0,\N,Missing
P00-1042,W98-1118,0,\N,Missing
P00-1042,W96-0213,0,\N,Missing
P00-1042,M95-1012,0,\N,Missing
P00-1042,E99-1026,1,\N,Missing
P00-1042,J96-1002,0,\N,Missing
P00-1042,J95-4004,0,\N,Missing
P00-1042,M98-1006,0,\N,Missing
P00-1042,M95-1013,0,\N,Missing
P00-1042,A97-1029,0,\N,Missing
P98-2131,P92-1004,1,0.801198,"Missing"
P98-2131,A97-1001,0,0.0214321,"3, we examine the dialogue processing requirement in a complex scenario involving multiple users and multiple simultaneous dialogues of diverse types. We describe how our architecture supports implementations of such a scenario. Finally, we describe two implemented spoken dialogue systems that embody this architecture (Section 4). Introduction We present an architecture for spoken dialogue systems for both human-computer interaction and computer mediation or analysis of human dialogue. The architecture shares many components with those of existing spoken dialogue systems, such as CommandTalk (Moore et al. 1997), Galaxy (Goddeau et al. 1994), TRAINS (Allen et al. 1995), Verbmobil (Wahlster 1993), Waxholm (Carlson 1996), and others. Our architecture is distinguished from these in its treatment of discourse-level processing. 1 Component Tasks of Discourse Processing We divide discourse-level processing into three component tasks: Dialogue Management, Context Tracking, and Pragmatic Adaptation. 1.1 Dialogue Management The Dialogue Manager is an oversight module whose purpose is to facilitate the interaction between dialogue participants. In a user-initiated system, the dialogue manager directs the proce"
P98-2131,W96-0102,0,\N,Missing
P98-2131,H90-1055,0,\N,Missing
P98-2131,J94-2001,0,\N,Missing
P98-2131,C94-1027,0,\N,Missing
S01-1033,W00-0730,0,0.0142784,"kernel functions. We have used the following polynomial function exclusively. K(x,y) =(x·y+l)d (9) C and d are constants set by experimentation. For all of the experiments reported in this paper, C was fixed as 1 and d was fixed as 2. A set of Xi that satisfies O:i &gt; 0 is called a support vector (SVs) 4 . The summation portion of Equation (4) was calculated using only the examples that were support vectors. Support vector machine methods are capable of handling data consisting of two categories. In general, data consisting of more than two categories is handled by using the pair-wise method (Kudoh and Matsumoto, 2000). In this method, for data consisting of N categories, pairs of two different categories (N (N1)/2 pairs) are constructed. The better cate1rn Figure 1, the circles in the broken lines indicate support vectors. gory is determined by using a 2-category classifier (in this paper, a support vector machine 5 was used as the 2-category classifier), and the correct category is finally determined by ""voting"" on the N(N-1)/2 pairs that result from analysis using the 2-category classifier. The support vector machine method is, in fact, performed by combining the support vector machine and pair-wise meth"
W00-0110,C96-1026,0,0.0253457,"el. 1 Introduction Pustejovsky (Pustejovsky, 1995) proposed the theory of a generative lexicon as a framework by which meanings of words are expressed in one unified representation. This kind ofgenerativity would be very useful for NLP, especially if it is applicable to the complex semantic structures represented by various modification relations. In our previous research on adjectives (Isahara and Kanzaki, 1999) we used Pustejovsky's theory to classify adjectives in Japanese. In this paper we take the first steps in a similar classification of the Japanese &quot;noun + NO&quot; construction. Bouillon (Bouillon, 1996) applied this theory to the adnominal constituent of mental states. Saint-Dizier (Saint-Dizier, 1998) discussed adjectives in French. Isahara and Kanzaki (Isahara and Kanzaki, 1999) treated a much wider range of phenomena of adnominal constituents. They classified the semantic roles of adnominal constituents in .Japanese. where many parts of speech act as adnominal constituents, and discussed a for59 mal treatment of their semantic roles. In their research, adnominal constituents, mainly adjectives which function as adverbials, are discussed. The present paper describes the similarities and di"
W00-0110,P99-1063,1,0.835997,"r also proposes an objective method of classifying these constructs using a large amount of linguistic data. The feasibility of this was verified with a selforganizing semantic map based on a neural network model. 1 Introduction Pustejovsky (Pustejovsky, 1995) proposed the theory of a generative lexicon as a framework by which meanings of words are expressed in one unified representation. This kind ofgenerativity would be very useful for NLP, especially if it is applicable to the complex semantic structures represented by various modification relations. In our previous research on adjectives (Isahara and Kanzaki, 1999) we used Pustejovsky's theory to classify adjectives in Japanese. In this paper we take the first steps in a similar classification of the Japanese &quot;noun + NO&quot; construction. Bouillon (Bouillon, 1996) applied this theory to the adnominal constituent of mental states. Saint-Dizier (Saint-Dizier, 1998) discussed adjectives in French. Isahara and Kanzaki (Isahara and Kanzaki, 1999) treated a much wider range of phenomena of adnominal constituents. They classified the semantic roles of adnominal constituents in .Japanese. where many parts of speech act as adnominal constituents, and discussed a for"
W00-0110,P98-2187,0,0.017927,"a framework by which meanings of words are expressed in one unified representation. This kind ofgenerativity would be very useful for NLP, especially if it is applicable to the complex semantic structures represented by various modification relations. In our previous research on adjectives (Isahara and Kanzaki, 1999) we used Pustejovsky's theory to classify adjectives in Japanese. In this paper we take the first steps in a similar classification of the Japanese &quot;noun + NO&quot; construction. Bouillon (Bouillon, 1996) applied this theory to the adnominal constituent of mental states. Saint-Dizier (Saint-Dizier, 1998) discussed adjectives in French. Isahara and Kanzaki (Isahara and Kanzaki, 1999) treated a much wider range of phenomena of adnominal constituents. They classified the semantic roles of adnominal constituents in .Japanese. where many parts of speech act as adnominal constituents, and discussed a for59 mal treatment of their semantic roles. In their research, adnominal constituents, mainly adjectives which function as adverbials, are discussed. The present paper describes the similarities and differences among adnominal constituents, i.e. adjectives and &quot;noun + NO t (in English &quot;of + noun&quot;)&quot; st"
W00-0110,C98-2182,0,\N,Missing
W01-1415,1999.tmi-1.7,1,\N,Missing
W01-1415,A97-1015,0,\N,Missing
W01-1415,C00-1082,1,\N,Missing
W01-1415,W00-0730,0,\N,Missing
W02-1107,P93-1023,0,0.0854817,"Missing"
W02-1107,W00-0110,1,\N,Missing
W02-1107,P90-1034,0,\N,Missing
W02-1107,P99-1063,1,\N,Missing
W03-1714,J93-2003,0,\N,Missing
W03-1714,C02-1060,1,\N,Missing
W03-1714,C88-1016,0,\N,Missing
W03-1714,C92-2101,0,\N,Missing
W03-1714,P95-1033,0,\N,Missing
W03-1714,P93-1004,0,\N,Missing
W03-1714,C02-1032,0,\N,Missing
W06-0201,I05-2043,1,0.873246,"Missing"
Y05-1014,W00-1303,0,0.012695,"nai (do not), shinakatta (did not). • Feature Set 2 This set consisted of all of the morphemes in each of the input sentences, e.g., kyou (today), watashi (I), wa (topic-marker particle), hashiru (run). 1 This corpus was made in our previous studies (Murata et al., 2002b; Murata et al., 2005). We found that support vector machines were more accurate than other kinds of machine learning methods such as the decision-list method and maximum entropy method (Murata et al., 2001). In addition, the use of support vector machines has been found to be effective in many studies (Taira and Haruno, 2001; Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Murata et al., 2002a). Therefore, we used support vector machines in our translation systems. The detailed parameter settings we used are described in our previous paper (Murata et al., 2001). 2 Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. Table 1: Occurrence rates of correct categories for tense, aspect, and modality. Category Occurrence rate present 0.65 (516/800) past 0.45 (356/800) prefect 0.32 (259/800) “can” 0.11 (90/800) “will” 0.11 (87/800) progressive 0.10 (82/800) imperative 0.09 (74/800) “should” 0.07 ("
Y05-1014,W01-1415,1,0.931958,"(one category) 5. participial constructions (one category) 6. verb ellipses (one category) 7. interjections or greeting sentences (one category) We used 800 sentences extracted from a corpus1 containing 40,198 sentences for the evaluation. We calculated the accuracy rates of six translation systems on the market and our new translation systems and examined the error patterns in the results. The six translation systems were the latest of leading translation system companies as of October 2003. Our systems for translating tense, aspect, and modality are based on support vector machines (SVMs) (Murata et al., 2001).2 They translate Japanese tense, aspect, and modality expressions into English. They detect categories of tense, aspect, and modality previously defined from English expressions. The categories are detected as a categorization problem by SVMs (Cristianini and Shawe-Taylor, 2000; Kudoh, 2000). However, an SVM can handle only two categories at a time. Therefore, we used a pairwise method in addition to the SVM to handle more than two categories (Moreira and Mayoraz, 1998). As training sentences, we used the sentences remaining after eliminating the 800 evaluation sentences from the 40,198-sente"
Y05-1014,2002.tmi-papers.14,1,0.747648,"Missing"
Y11-1062,J93-1006,0,0.383731,"Missing"
Y11-1062,W96-0107,0,0.101135,"s in phrase-level support is more complicated. See Ma et al. (2009) for details. The non-English examples are also rendered in (1) alphabetized form, (2) English glosses, and (3) English translation (or grammar explanations if there are no proper translations) complying with the PACLIC 25 Paper Submission Guidelines. The English translations, however, will be skipped if they appear in the pairs of the parallel translation expressions. 578 3 Extraction of Parallel Translation Expressions 3.1 Related work The earlier studies most closely related to our work were done by Sainoo et al. (2003) and Kitamura and Matsumoto (1996; 2005). The principal techniques used in those studies were used as our baseline in this study and are described in Sec. 3.2.1. In the study of Sainoo et al., the research object was limited to a small corpus of 8,500 Japanese-English parallel sentences, which were examples from a number of bilingual dictionaries, and only a couple dozen parallel translation expressions were extracted. The first study of Kitamura and Matsumoto (1996), on the other hand, used Japanese-English parallel corpora of three distinct domains: a computer manual, a scientific journal, and business correspondence letter"
Y11-1062,ma-etal-2008-selection,1,0.912621,"Keywords: parallel translation expression, extraction, rule, lexical information, Englishwriting support 1 Introduction Non-native speakers often have problems explaining ideas or presenting achievements in written English, in part because of the large amount of time needed to determine which possible translation of an expression most suits the context. Trying to develop English-writing support tools that will enable non-native speakers to produce nearly perfect English sentences for mixed English-Japanese sentences–in which expressions without know translations are simply written in Japanese–Ma et al. (2008; 2009) have developed systems that can provide support at the word and phrase levels. That is, the given Japanese parts in the mixed English-Japanese sentences can be words or phrases. For phrase-level support in those systems, the Japanese parts are extracted from the mixed sentences and segmented into words, the candidate English equivalents of the segmented Japanese words are identified by searching through a Japanese-English dictionary, and the best equivalents of the Japanese phrases are selected from the combinations of the candidate translations of the single words. This kind of suppor"
Y11-1062,C02-1020,0,0.125433,"ore not easy to scale up. In Kimura and Matsumoto’s second study (2005) they confirmed the interim results manually, used more linguistic resources than they did in their earlier study, and introduced a method for halving the extraction time by dividing a corpus into quarters. Their method is still hard to scale up, however, because the repetition processing is costly and the results can be manually confirmed only in small-scale extraction. Among other interesting studies on the extraction of Japanese-English parallel translation expressions are those done by Yamamoto and Matsumoto (2000) and Sato and Saito (2002-1; 20022). The methods proposed in those studies, however, like those proposed in the earlier studies mentioned above, have low precisions and recalls and also cannot be scaled up. These methods are thus not practical for use in the extraction of large-scale parallel translation expressions. In a study by Sato and Saito (2002-1), for example, even though the test corpus used for extraction consisted of only about 3,000 parallel sentences, whereas the training corpus consisted of 30,287 parallel sentences, more than ten times the number in the test corpus, the extraction precision was still le"
Y11-1062,W04-2208,1,0.800665,"atching list”. In this way, “matching list” has six words and and the “nonmatching list” has none. Since the “matching list” has more words than the “non-matching list”, the parallel translation expression passes. Furthermore, if this parallel translation expression and the other passing parallel translation expressions are in many-to-many relationships, the one with the largest number of words in the “matching list” is selected. 4 Experiments 4.1 Experimental setup 4.1.1 Data and tools The aligned Japanese-English parallel corpora that we used in the experiments consisted of the NICT corpus (Uchimoto et al., 2004), the JENNAD corpus, and the aligned Reuters corpus (Utiyama and Isahara, 2003). The NICT corpus is composed of sentence-aligned Mainichi newspaper articles (Japanese) and their translations done by professional translators and has about 40,000 pairs of parallel sentences. The JENNAD corpus is composed of automatically sentence-aligned Yomiuri newspaper articles (Japanese) and Daily Yomiuri newspaper articles (English) and has about 180,000 pairs of parallel sentences. The aligned Reuters corpus is composed of automatically sentence-aligned Reuters Japanese and English news articles and has ab"
Y11-1062,P03-1010,0,0.0246159,"atching list” has none. Since the “matching list” has more words than the “non-matching list”, the parallel translation expression passes. Furthermore, if this parallel translation expression and the other passing parallel translation expressions are in many-to-many relationships, the one with the largest number of words in the “matching list” is selected. 4 Experiments 4.1 Experimental setup 4.1.1 Data and tools The aligned Japanese-English parallel corpora that we used in the experiments consisted of the NICT corpus (Uchimoto et al., 2004), the JENNAD corpus, and the aligned Reuters corpus (Utiyama and Isahara, 2003). The NICT corpus is composed of sentence-aligned Mainichi newspaper articles (Japanese) and their translations done by professional translators and has about 40,000 pairs of parallel sentences. The JENNAD corpus is composed of automatically sentence-aligned Yomiuri newspaper articles (Japanese) and Daily Yomiuri newspaper articles (English) and has about 180,000 pairs of parallel sentences. The aligned Reuters corpus is composed of automatically sentence-aligned Reuters Japanese and English news articles and has about 70,000 pairs of parallel sentences. After the overlapping sentences were ex"
Y11-1062,C00-2135,0,0.221094,"uming and the extraction is therefore not easy to scale up. In Kimura and Matsumoto’s second study (2005) they confirmed the interim results manually, used more linguistic resources than they did in their earlier study, and introduced a method for halving the extraction time by dividing a corpus into quarters. Their method is still hard to scale up, however, because the repetition processing is costly and the results can be manually confirmed only in small-scale extraction. Among other interesting studies on the extraction of Japanese-English parallel translation expressions are those done by Yamamoto and Matsumoto (2000) and Sato and Saito (2002-1; 20022). The methods proposed in those studies, however, like those proposed in the earlier studies mentioned above, have low precisions and recalls and also cannot be scaled up. These methods are thus not practical for use in the extraction of large-scale parallel translation expressions. In a study by Sato and Saito (2002-1), for example, even though the test corpus used for extraction consisted of only about 3,000 parallel sentences, whereas the training corpus consisted of 30,287 parallel sentences, more than ten times the number in the test corpus, the extracti"
Y14-1040,C12-1018,0,0.138398,"Missing"
Y14-1040,D13-1137,0,0.171736,"Missing"
Y14-1040,P13-1088,0,0.0589727,"Missing"
Y14-1040,D13-1176,0,0.0863825,"Missing"
Y14-1040,W13-3512,0,0.0678898,"Missing"
Y14-1040,D13-1170,0,0.01092,"Missing"
Y14-1040,D13-1144,0,0.154897,"Missing"
Y14-1040,D13-1141,0,0.0508547,"Missing"
Y14-1040,D13-1106,0,\N,Missing
Y14-1040,P13-1078,0,\N,Missing
Y16-3001,C12-1018,0,0.0443621,"Missing"
Y16-3001,D13-1137,0,0.0258921,"Missing"
Y16-3001,P13-1088,0,0.0497777,"Missing"
Y16-3001,D13-1176,0,0.0360953,"s.mynavi.jp/news/2010/07/05/028/) In recent years, on the other hand, deep learning/neural network techniques have attracted a great deal of attention in various ﬁelds and have been successfully applied not only in speech recognition (Li et al., 2013) and image recognition (Krizhevsky et al., 2012) tasks but also in NLP tasks including morphology & syntax (Billingsley and Curran, 2012; Hermann and Blunsom, 2013; Luong et al., 2013; Socher et al., 2013a), semantics (Hashimoto et al., 2013; Srivastava et al., 2013; Tsubaki et al., 2013), machine translation (Auli et al., 2013; Liu et al., 2013; Kalchbrenner and Blunsom, 2013; Zou et al., 2013), text classiﬁcation (Glorot et al., 2011), information retrieval (Huang et al., 2013; Salakhutdinov and Hinton, 2009), and others (Seide et al., 2011; Socher et al., 2011; Socher et al., 2013b). Moreover, a uniﬁed neural network architecture and learning algorithm has also been proposed that can be applied to various NLP tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Collobert et al., 2011). How30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 20"
Y16-3001,W13-3512,0,0.0603939,"Missing"
Y16-3001,Y14-1040,1,0.0925179,"October 28-30, 2016 309 ever, there have been no studies on applying deep learning to information retrieval support tasks. It is therefore necessary to conﬁrm whether deep learning is more effective than other conventional machine learning methods in this task. Two objectives were cited above. One was to develop an effective method for predicting suitable retrieval terms and the other was to determine whether deep learning is more effective than other conventional machine learning methods, i.e., multi-layer perceptron (MLP) and support vector machines (SVM), in such NLP tasks. On this basis, Ma et al. (2014) proposed a method to predict retrieval terms in computer-related ﬁelds using machine learning methods with deep belief networks (DBN) (Hinton et al., 2006; Lee et al., 2009; Bengio et al., 2007; Bengio, 2009; Bengio et al., 2013). In small-scale experiments they showed that using DBN resulted in higher prediction precision than using either a multi-layer perceptron (MLP) or support vector machines (SVM). To evaluate their proposed method more reliably, the ﬁrst thing we must do is scale up the experiments. In general, it is not easy to obtain large training data, particularly labeled data for"
Y16-3001,D13-1170,0,0.0054021,"Missing"
Y16-3001,D13-1144,0,0.0213488,"lty deciding on the proper retrieval terms. (http://www.garbagenews.net/archives/1466626.html) (http://news.mynavi.jp/news/2010/07/05/028/) In recent years, on the other hand, deep learning/neural network techniques have attracted a great deal of attention in various ﬁelds and have been successfully applied not only in speech recognition (Li et al., 2013) and image recognition (Krizhevsky et al., 2012) tasks but also in NLP tasks including morphology & syntax (Billingsley and Curran, 2012; Hermann and Blunsom, 2013; Luong et al., 2013; Socher et al., 2013a), semantics (Hashimoto et al., 2013; Srivastava et al., 2013; Tsubaki et al., 2013), machine translation (Auli et al., 2013; Liu et al., 2013; Kalchbrenner and Blunsom, 2013; Zou et al., 2013), text classiﬁcation (Glorot et al., 2011), information retrieval (Huang et al., 2013; Salakhutdinov and Hinton, 2009), and others (Seide et al., 2011; Socher et al., 2011; Socher et al., 2013b). Moreover, a uniﬁed neural network architecture and learning algorithm has also been proposed that can be applied to various NLP tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Collobert et al., 2011). How30th Pacific"
Y16-3001,D13-1141,0,0.0176806,") In recent years, on the other hand, deep learning/neural network techniques have attracted a great deal of attention in various ﬁelds and have been successfully applied not only in speech recognition (Li et al., 2013) and image recognition (Krizhevsky et al., 2012) tasks but also in NLP tasks including morphology & syntax (Billingsley and Curran, 2012; Hermann and Blunsom, 2013; Luong et al., 2013; Socher et al., 2013a), semantics (Hashimoto et al., 2013; Srivastava et al., 2013; Tsubaki et al., 2013), machine translation (Auli et al., 2013; Liu et al., 2013; Kalchbrenner and Blunsom, 2013; Zou et al., 2013), text classiﬁcation (Glorot et al., 2011), information retrieval (Huang et al., 2013; Salakhutdinov and Hinton, 2009), and others (Seide et al., 2011; Socher et al., 2011; Socher et al., 2013b). Moreover, a uniﬁed neural network architecture and learning algorithm has also been proposed that can be applied to various NLP tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Collobert et al., 2011). How30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 309 ever, there"
Y16-3001,D13-1106,0,\N,Missing
Y16-3001,P13-1078,0,\N,Missing
zhang-etal-2008-word,W04-2208,1,\N,Missing
zhang-etal-2008-word,I05-2015,1,\N,Missing
zhang-etal-2008-word,2005.mtsummit-papers.10,1,\N,Missing
zhang-etal-2008-word,maekawa-etal-2000-spontaneous,1,\N,Missing
