2020.alw-1.9,D19-6115,0,0.0283306,"of abuse. Wiegand et al. (2019) argue that the type of sampling strategy introduces bias into the dataset, and we can assume that the two sampling strategies create different biases: Random boosted sampling may create a bias towards specific authors but with a widespread range of topics, and biased topic sampling may create a bias towards specific topics, and potentially specific authors. However, we are often unaware of the exact biases present in such datasets. This is important because first results on debiasing datasets show that these methods work best when we know which bias is present (He et al., 2019). In our work, we focus on reproducing the results by Wiegand et al. (2019) and providing a closer look at the different sampling strategies. While Wiegand et al. (2019) normalize performance by using a single classifier on all datasets, they do not normalize across different text types. Thus, the two sampling strategies have been used on different datasets, which leaves open the question to what degree the differences in bias are due to textual characteristics (Wikipedia talkpages, Twitter feed, Facebook posts), or to the sampling strategies. Consequently, we repeat their experiments applying"
2020.alw-1.9,W18-4401,0,0.0453118,"Missing"
2020.alw-1.9,D18-1302,0,0.16045,"Missing"
2020.alw-1.9,2020.restup-1.4,0,0.0466053,"Missing"
2020.alw-1.9,P19-1163,0,0.0377493,"ics. They suggest concentrating on controversies and describe two specific methods: For Twitter data, they suggest using the most frequent hashtags over a time period. And for Reddit, they suggest using posts that have a similar number of up- and down-votes, a sign for the controversial nature of these posts. Park et al. (2018) discuss methods to decrease the gender bias in abusive language detection. They suggest 3 methods for debiasing, which successfully reduce gender bias in their experiments: debiasing word embeddings, gender swap data augmentation, and fine-tuning using a larger corpus. Sap et al. (2019), in contrast, focus on racial bias, which is originally introduced by annotator’s insensitivities to African-American English (AAE), but is then propagated via a trained classifier learning this bias. Sap et al. (2019) show that priming the annotators for dialect and race of the tweet’s producer results in fewer AAE posts being labeled abusive. Davidson et al. (2019) provide a more in-depth analysis, showing that the bias also holds when comparing tweets containing the keywords “n*gga” and “b*tch”. 1. Does repeated sampling from a dataset change characteristics of the data? We first need to i"
2020.alw-1.9,W12-2103,0,0.372507,"the original Kaggle dataset also used in their experiments (see section 4.1 for details on the datasets), which is originally based on boosted random sampling. Additionally, we repeat the experiment on another, larger Kaggle dataset for abusive language detection. 3. How dependent are results on the topic used for sampling? Since the original Kaggle dataset is based on Wikipedia talkpages and thus covers topics different from the one covered in other datasets, we could not use the list of topics used by previous approaches for biased topic sampling (Kumar et al., 2018; Waseem and Hovy, 2016; Warner and Hirschberg, 2012). This leads to 71 There are also approaches to eliminate bias from datasets. For example, Badjatiya et al. (2019) present a method to identify and replace bias sensitive words. 4 negative polar expressions and annotated for abusive terms via crowdsourcing. This lexicon was used in a classifier to create the extended lexicon. However, a manual inspection showed that many of the words in the base lexicon were not offensive. For this reason, we created a manually-vetted version of this lexicon3 . Three native speakers were asked to rate each word in the base lexicon as either non-abusive, mildly"
2020.alw-1.9,N16-2013,0,0.281076,"ame underlying dataset, the original Kaggle dataset also used in their experiments (see section 4.1 for details on the datasets), which is originally based on boosted random sampling. Additionally, we repeat the experiment on another, larger Kaggle dataset for abusive language detection. 3. How dependent are results on the topic used for sampling? Since the original Kaggle dataset is based on Wikipedia talkpages and thus covers topics different from the one covered in other datasets, we could not use the list of topics used by previous approaches for biased topic sampling (Kumar et al., 2018; Waseem and Hovy, 2016; Warner and Hirschberg, 2012). This leads to 71 There are also approaches to eliminate bias from datasets. For example, Badjatiya et al. (2019) present a method to identify and replace bias sensitive words. 4 negative polar expressions and annotated for abusive terms via crowdsourcing. This lexicon was used in a classifier to create the extended lexicon. However, a manual inspection showed that many of the words in the base lexicon were not offensive. For this reason, we created a manually-vetted version of this lexicon3 . Three native speakers were asked to rate each word in the base lexicon"
2020.alw-1.9,N19-1060,0,0.483356,"overall on classifier performance. We are interested in understanding this interaction better. Wiegand et al. (2019) distinguish between boosted random sampling and biased topic sampling. Boosted random sampling is based on a complete sample, for example all tweets of a specific time frame. Then, the number of abusive posts is boosted using different methods, for example by adding more posts by users who have been blocked for being abusive. Biased topic sampling, in contrast, samples posts from specific topics, such as soccer or Islam, which are known to cause a considerable amount of abuse. Wiegand et al. (2019) argue that the type of sampling strategy introduces bias into the dataset, and we can assume that the two sampling strategies create different biases: Random boosted sampling may create a bias towards specific authors but with a widespread range of topics, and biased topic sampling may create a bias towards specific topics, and potentially specific authors. However, we are often unaware of the exact biases present in such datasets. This is important because first results on debiasing datasets show that these methods work best when we know which bias is present (He et al., 2019). In our work,"
2020.alw-1.9,N18-1095,0,0.0373971,"Missing"
2020.lrec-1.625,W19-3515,0,0.0142454,"del: We concatenated the outputs of the 64 dimension hidden layers of the Brown cluster embeddings BiLSTM model and that of the word embeddings BiLSTM model. This merged model consists of 128 dimension hidden layers, which are then fed to 64 dimension hidden layers with relu activation, then to the output layer. 4.4. Features Brown cluster features are represented as discussed in Section 4, but, besides the normal tokenization, we also experimented with word piece tokenization. The latter method of tokenization has been used as basic tokens for recent transformer models (Devlin et al., 2018). Bodapati et al. (2019) show that using word piece as features can boost the performance of abusive language detection. We used the BERT tokenizer to process the data and generated the word piece input for Brown clustering. Besides adopting Brown clusters as features, we also examined the performance of character n-grams and word ngrams for comparison. Character n-grams are widely used in traditional machine learning methods and outperform other surface features in many tasks (Waseem and Hovy, 2016; Malmasi and Zampieri, 2018) that show high tolerance to spelling errors and variations in tweets (Schmidt and Wiegand,"
2020.lrec-1.625,J92-4003,0,0.500284,"d words, or if they generalize over specific domains in which hate speech is common. The Waseem data set (Waseem and Hovy, 2016), for example, was sampled based on specific topics that had created large amounts of hate speech, such as football or an Australian cooking show. Since the approaches using word embeddings often require more data and compute power than most people have access to, it is impossible or at least difficult to train them on more specific data. We approach the problem of data sparsity in offensive language detection from a slightly different angle: We use Brown clustering (Brown et al., 1992; Liang, 2005) to create generalized word representations. Brown clusters, while not necessarily competitive to embeddings in a range of tasks, have the advantage that they can be trained on smaller data sets, and training times are faster than training times for neural networks. Thus, we can investigate whether it makes sense to have more specialized word representations, which provide a good balance between generalizing over the different forms of a word and losing a distinction between offensive and non-offensive uses. More generally, we investigate the following questions: 1. When creating"
2020.lrec-1.625,W18-4401,0,0.0200173,"acter n-grams are the most common surface features used in these tasks, and also the most successful ones (Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Malmasi and Zampieri, 2018). Malmasi and Zampieri (2018) found character 4-grams to outperform other features, such as word n-gram and Brown clusters. Waseem and Hovy (2016) report that their best results are based on a model built with character n-gram and gender features. More recent work shows that neural network models using pre-trained word representations significantly advanced the state of the art in offensive language detection: Kumar et al. (2018) perform a benchmark analysis of the first shared task on aggression identification and show that half of the top 15 systems use neural network models. Besides traditional neural network models, transformer-based language models like Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) achieve state of the art performance in the SemEval 2019 shared task: OffensEval (Zampieri et al., 2019b). Seven out of the 10 highest performing systems adopt BERT models with variations in the parameters for offensive language identification tasks (Zampieri et al., 2019b). Warne"
2020.lrec-1.625,S19-2011,0,0.0688218,"012; Malmasi and Zampieri, 2018)). One of the challenges that these approaches face is data sparsity since such shallow features do not generalize well: We may not have seen the exact word that characterizes a tweet as hate speech, but we may have seen similar or related words in training. This problem has been approached by using word embeddings. However, most of these approaches use pre-trained models, trained on large sets of English language data (Peters et al., 2018; Devlin et al., 2018; Mikolov et al., 2018). Such models currently provide state of the art models (Badjatiya et al., 2017; Liu et al., 2019; Zhu et al., 2019; Mishra et al., 2019). Since these embeddings were not created from data sets specifically targeting the detection of hate speech, it is not clear if they actually generalize over hate speech related words, or if they generalize over specific domains in which hate speech is common. The Waseem data set (Waseem and Hovy, 2016), for example, was sampled based on specific topics that had created large amounts of hate speech, such as football or an Australian cooking show. Since the approaches using word embeddings often require more data and compute power than most people have a"
2020.lrec-1.625,L18-1008,0,0.035675,"th a set of shallow lexical features such as word or character ngrams (e.g., (Warner and Hirschberg, 2012; Malmasi and Zampieri, 2018)). One of the challenges that these approaches face is data sparsity since such shallow features do not generalize well: We may not have seen the exact word that characterizes a tweet as hate speech, but we may have seen similar or related words in training. This problem has been approached by using word embeddings. However, most of these approaches use pre-trained models, trained on large sets of English language data (Peters et al., 2018; Devlin et al., 2018; Mikolov et al., 2018). Such models currently provide state of the art models (Badjatiya et al., 2017; Liu et al., 2019; Zhu et al., 2019; Mishra et al., 2019). Since these embeddings were not created from data sets specifically targeting the detection of hate speech, it is not clear if they actually generalize over hate speech related words, or if they generalize over specific domains in which hate speech is common. The Waseem data set (Waseem and Hovy, 2016), for example, was sampled based on specific topics that had created large amounts of hate speech, such as football or an Australian cooking show. Since the a"
2020.lrec-1.625,N19-1221,0,0.151366,"ne of the challenges that these approaches face is data sparsity since such shallow features do not generalize well: We may not have seen the exact word that characterizes a tweet as hate speech, but we may have seen similar or related words in training. This problem has been approached by using word embeddings. However, most of these approaches use pre-trained models, trained on large sets of English language data (Peters et al., 2018; Devlin et al., 2018; Mikolov et al., 2018). Such models currently provide state of the art models (Badjatiya et al., 2017; Liu et al., 2019; Zhu et al., 2019; Mishra et al., 2019). Since these embeddings were not created from data sets specifically targeting the detection of hate speech, it is not clear if they actually generalize over hate speech related words, or if they generalize over specific domains in which hate speech is common. The Waseem data set (Waseem and Hovy, 2016), for example, was sampled based on specific topics that had created large amounts of hate speech, such as football or an Australian cooking show. Since the approaches using word embeddings often require more data and compute power than most people have access to, it is impossible or at least d"
2020.lrec-1.625,N13-1039,0,0.1284,"Missing"
2020.lrec-1.625,N18-1202,0,0.0446634,"ed on using machine learning algorithms with a set of shallow lexical features such as word or character ngrams (e.g., (Warner and Hirschberg, 2012; Malmasi and Zampieri, 2018)). One of the challenges that these approaches face is data sparsity since such shallow features do not generalize well: We may not have seen the exact word that characterizes a tweet as hate speech, but we may have seen similar or related words in training. This problem has been approached by using word embeddings. However, most of these approaches use pre-trained models, trained on large sets of English language data (Peters et al., 2018; Devlin et al., 2018; Mikolov et al., 2018). Such models currently provide state of the art models (Badjatiya et al., 2017; Liu et al., 2019; Zhu et al., 2019; Mishra et al., 2019). Since these embeddings were not created from data sets specifically targeting the detection of hate speech, it is not clear if they actually generalize over hate speech related words, or if they generalize over specific domains in which hate speech is common. The Waseem data set (Waseem and Hovy, 2016), for example, was sampled based on specific topics that had created large amounts of hate speech, such as footbal"
2020.lrec-1.625,W17-1101,0,0.0163226,"dapati et al. (2019) show that using word piece as features can boost the performance of abusive language detection. We used the BERT tokenizer to process the data and generated the word piece input for Brown clustering. Besides adopting Brown clusters as features, we also examined the performance of character n-grams and word ngrams for comparison. Character n-grams are widely used in traditional machine learning methods and outperform other surface features in many tasks (Waseem and Hovy, 2016; Malmasi and Zampieri, 2018) that show high tolerance to spelling errors and variations in tweets (Schmidt and Wiegand, 2017). Word n-gram also performed well in previous studies, for example, the study by Warner and Hirschberg (2012) indicates that unigrams outperformed other combinations of word or character n-gram in detecting abusive language. After running preliminary experiments, we decided to adopt unigram Brown cluster IDs (minimal 2 occurrences), word unigrams (minimal 2 occurrences), and character 1-4 grams (minimal 2 occurrences) to build the bag of words models. When we merge different types of features including words and n-grams, we use feature selection on the combined feature set, which has been show"
2020.lrec-1.625,D11-1014,0,0.642249,"cluster features in hate speech identification tasks. They build SVM classifiers with template-based features including ngrams, POS tags, and Brown clusters. However, they obtain the best results using only unigram features. Wester et al. (2016) examines the performance of various types of linguistic features in threat detection, a task with many similarities to hate speech detection. They also utilize Brown cluster features as one of their semantic features. These previous studies indicate that classifiers trained on simple Brown clusters cannot outperform those trained on surface features. Socher et al. (2011) argue that the failure of Brown clusters in sentiment analysis tasks is because ”they do not capture sentiment information (good and bad are usually in the same cluster) and cannot be modified via backpropagation.” However, we also find that most of the studies using Brown clusters in the sentiment analysis tasks only use existing Brown clusters trained on the general genre, which can cause an insensitivity in capturing sentiment information. 3. 3.1. Data Offensive Language Detection Data Sets For consistency with previous work, we use two data sets to examine the performance of domain-specif"
2020.lrec-1.625,R19-1132,1,0.8861,"Missing"
2020.lrec-1.625,W12-2103,0,0.590753,"s, the increase in the use of social media has led to an increase in offensive language. Social media often allow anonymous access, and users increasingly use this anonymity to publicize aggressive or offensive attitudes. Because of the vast volume of data produced on social media platforms every day, we need automated methods that can detect hate speech without restricting people’s right to freedom of expression. Research on offensive language detection has mostly concentrated on using machine learning algorithms with a set of shallow lexical features such as word or character ngrams (e.g., (Warner and Hirschberg, 2012; Malmasi and Zampieri, 2018)). One of the challenges that these approaches face is data sparsity since such shallow features do not generalize well: We may not have seen the exact word that characterizes a tweet as hate speech, but we may have seen similar or related words in training. This problem has been approached by using word embeddings. However, most of these approaches use pre-trained models, trained on large sets of English language data (Peters et al., 2018; Devlin et al., 2018; Mikolov et al., 2018). Such models currently provide state of the art models (Badjatiya et al., 2017; Liu"
2020.lrec-1.625,N16-2013,0,0.282857,"mbeddings. However, most of these approaches use pre-trained models, trained on large sets of English language data (Peters et al., 2018; Devlin et al., 2018; Mikolov et al., 2018). Such models currently provide state of the art models (Badjatiya et al., 2017; Liu et al., 2019; Zhu et al., 2019; Mishra et al., 2019). Since these embeddings were not created from data sets specifically targeting the detection of hate speech, it is not clear if they actually generalize over hate speech related words, or if they generalize over specific domains in which hate speech is common. The Waseem data set (Waseem and Hovy, 2016), for example, was sampled based on specific topics that had created large amounts of hate speech, such as football or an Australian cooking show. Since the approaches using word embeddings often require more data and compute power than most people have access to, it is impossible or at least difficult to train them on more specific data. We approach the problem of data sparsity in offensive language detection from a slightly different angle: We use Brown clustering (Brown et al., 1992; Liang, 2005) to create generalized word representations. Brown clusters, while not necessarily competitive t"
2020.lrec-1.625,W16-0413,0,0.018303,"from Transformers (BERT) (Devlin et al., 2018) achieve state of the art performance in the SemEval 2019 shared task: OffensEval (Zampieri et al., 2019b). Seven out of the 10 highest performing systems adopt BERT models with variations in the parameters for offensive language identification tasks (Zampieri et al., 2019b). Warner and Hirschberg (2012) first introduce Brown cluster features in hate speech identification tasks. They build SVM classifiers with template-based features including ngrams, POS tags, and Brown clusters. However, they obtain the best results using only unigram features. Wester et al. (2016) examines the performance of various types of linguistic features in threat detection, a task with many similarities to hate speech detection. They also utilize Brown cluster features as one of their semantic features. These previous studies indicate that classifiers trained on simple Brown clusters cannot outperform those trained on surface features. Socher et al. (2011) argue that the failure of Brown clusters in sentiment analysis tasks is because ”they do not capture sentiment information (good and bad are usually in the same cluster) and cannot be modified via backpropagation.” However, w"
2020.lrec-1.625,N19-1060,0,0.0337913,"Missing"
2020.lrec-1.625,N19-1144,0,0.333829,"der features. More recent work shows that neural network models using pre-trained word representations significantly advanced the state of the art in offensive language detection: Kumar et al. (2018) perform a benchmark analysis of the first shared task on aggression identification and show that half of the top 15 systems use neural network models. Besides traditional neural network models, transformer-based language models like Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) achieve state of the art performance in the SemEval 2019 shared task: OffensEval (Zampieri et al., 2019b). Seven out of the 10 highest performing systems adopt BERT models with variations in the parameters for offensive language identification tasks (Zampieri et al., 2019b). Warner and Hirschberg (2012) first introduce Brown cluster features in hate speech identification tasks. They build SVM classifiers with template-based features including ngrams, POS tags, and Brown clusters. However, they obtain the best results using only unigram features. Wester et al. (2016) examines the performance of various types of linguistic features in threat detection, a task with many similarities to hate speech"
2020.lrec-1.625,S19-2010,0,0.305689,"der features. More recent work shows that neural network models using pre-trained word representations significantly advanced the state of the art in offensive language detection: Kumar et al. (2018) perform a benchmark analysis of the first shared task on aggression identification and show that half of the top 15 systems use neural network models. Besides traditional neural network models, transformer-based language models like Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) achieve state of the art performance in the SemEval 2019 shared task: OffensEval (Zampieri et al., 2019b). Seven out of the 10 highest performing systems adopt BERT models with variations in the parameters for offensive language identification tasks (Zampieri et al., 2019b). Warner and Hirschberg (2012) first introduce Brown cluster features in hate speech identification tasks. They build SVM classifiers with template-based features including ngrams, POS tags, and Brown clusters. However, they obtain the best results using only unigram features. Wester et al. (2016) examines the performance of various types of linguistic features in threat detection, a task with many similarities to hate speech"
2020.lrec-1.625,S19-2138,1,0.844133,"Missing"
2020.tlt-1.4,C18-1139,0,0.0256342,"Missing"
2020.tlt-1.4,Q17-1010,0,0.0184627,"features in its joint probability model, the neural approach does not use the coarse tag for making a decision about a specific word. Instead, it uses coarse grained tagging to provide a better initialization for the network. A standard method to obtain a better initialization would be to use off-the-shelf embeddings, which have been trained on a large data set of texts. For a low-resource language like Chaghatay, this is is not an option as such embeddings do not exist, and insufficient data is available to create traditional word embeddings like Word2Vec (Mikolov et al., 2013) and FastText (Bojanowski et al., 2017). Instead, we train a coarse-grained part of speech tagger and then transfer that model to fine-grained tagging by optimizing it on the more challenging task. This will provide a better weight initialization, similar to that provided by external embeddings. 3.3 Utilizing Training Data in Different Structure Formats Since the corpus annotation process has evolved over time (see Section 2.2), we have manually annotated data in three different formats with regard to the marking of units: sentence segmented, phrase segmented, and line segmented. Since our task is POS tagging, we assign one label p"
2020.tlt-1.4,Y09-1013,0,0.114424,"Missing"
2020.tlt-1.4,W16-5206,0,0.0278855,"ble. However, language model pretraining can still be useful for part of speech tagging in Chaghatay. Because the overall annotation process in the Chaghatay corpus is quite time consuming, a considerable number of texts have been transliterated but not linguistically annotated yet. 9 518 structures have been transliterated but, as discussed in Section 4.1, roughly half this number of structures have annotations. These 9 518 structures are used to train the simple language models discussed in Section 4.2.4. 4.1.4 The Modern Uyghur Corpus For the modern Uyghur data, we use the Uyghur Treebank (Eli et al., 2016), which is part of the Universal Dependencies (UD) project (McDonald et al., 2013). This treebank uses Universal POS tags, conforming to the UD annotation standards. The Universal POS tagset is a very coarse tagset consisting of 17 POS tags. The Uyghur Dependency treebank uses only 16 of those. The Uyghur treebank is substantially larger than the Chaghatay data we are working with. In total, there are 3 459 sentences and 40 236 words. The data is divided into train, development, and test portions by the treebank creators. Only the training portion is used for pretraining our Chaghatay model wi"
2020.tlt-1.4,gahbiche-braham-etal-2012-joint,0,0.201604,"Additionally, since Chaghatay is no longer spoken, there is only a limited amount of textual data available, restricting our ability to train a language model or use semi-supervised strategies. Finally, the POS tagset is large and includes a detailed analysis of morphological features. This leads us to consider the following questions: 3.1 Choice of Classifier Given the combination of a small training set and a large POS tagset, the choice of classifier is not obvious. We decided to focus on two approaches that have been shown to be successful in POS tagging: Conditional Random Fields (CRF) (Gahbiche-Braham et al., 2012) and Recurrent Neural Networks (RNN) (Shao et al., 2017). RNNs are considered state of the art, but it is well known that they work best when they have access to large amounts of training data (Horsmann and Zesch, 2017). Figure 3: Example of an unpunctuated example. Figure 4: Example of a phrase element. CRFs may be more amenable to small training data sets, but they may not scale up to a large label set (Horsmann and Zesch, 2017). Additionally, neural models can be pretrained on additional data from other domains and then optimized on our small training set. 3.2 Utilizing Coarse Grained POS T"
2020.tlt-1.4,D17-1076,0,0.0277022,"arge and includes a detailed analysis of morphological features. This leads us to consider the following questions: 3.1 Choice of Classifier Given the combination of a small training set and a large POS tagset, the choice of classifier is not obvious. We decided to focus on two approaches that have been shown to be successful in POS tagging: Conditional Random Fields (CRF) (Gahbiche-Braham et al., 2012) and Recurrent Neural Networks (RNN) (Shao et al., 2017). RNNs are considered state of the art, but it is well known that they work best when they have access to large amounts of training data (Horsmann and Zesch, 2017). Figure 3: Example of an unpunctuated example. Figure 4: Example of a phrase element. CRFs may be more amenable to small training data sets, but they may not scale up to a large label set (Horsmann and Zesch, 2017). Additionally, neural models can be pretrained on additional data from other domains and then optimized on our small training set. 3.2 Utilizing Coarse Grained POS Tagging as Preprocessing The large tagset in this corpus is ideal for corpus-based analysis but provides challenges for statistical taggers. We investigate methods to overcome the challenges of a large tagset by using co"
2020.tlt-1.4,2020.acl-main.156,0,0.0278612,"mented documents. However, we only have very few of those, which raises the question whether we can use the other types of data to augment the training set. Does the additional data help guide the POS tagger, or is the missing information about sentence boundaries detrimental for the POS tagger? Does the difference in segmentation have any effect on POS tagging, or does the need for data override the need for sentence boundary information? 3.4 Pre-Training the RNN For neural sequence tagging architectures, language model pre-training has been shown to be beneficial (Peters et al., 2018; Ortiz Suárez et al., 2020). In contrast to the CRF model, the transfer learning approach used for the neural network can be adapted from a variety of different initial tasks. We investigate whether this method can be used successfully in a setting where we have access to very little data in the target language. Since we do not have much additional data for Chaghatay, we experiment with two settings: 1) We use data from Chaghatay’s modern relative, Uyghur, in the assumption that Uyghur is close enough to Chaghatay to provide a good starting point for the POS tagger. 2) We also experiment with pretraining the RNN using l"
2020.tlt-1.4,N18-1202,0,0.0264225,"t data are the sentence segmented documents. However, we only have very few of those, which raises the question whether we can use the other types of data to augment the training set. Does the additional data help guide the POS tagger, or is the missing information about sentence boundaries detrimental for the POS tagger? Does the difference in segmentation have any effect on POS tagging, or does the need for data override the need for sentence boundary information? 3.4 Pre-Training the RNN For neural sequence tagging architectures, language model pre-training has been shown to be beneficial (Peters et al., 2018; Ortiz Suárez et al., 2020). In contrast to the CRF model, the transfer learning approach used for the neural network can be adapted from a variety of different initial tasks. We investigate whether this method can be used successfully in a setting where we have access to very little data in the target language. Since we do not have much additional data for Chaghatay, we experiment with two settings: 1) We use data from Chaghatay’s modern relative, Uyghur, in the assumption that Uyghur is close enough to Chaghatay to provide a good starting point for the POS tagger. 2) We also experiment with"
2020.tlt-1.4,I17-1018,0,0.160294,"limited amount of textual data available, restricting our ability to train a language model or use semi-supervised strategies. Finally, the POS tagset is large and includes a detailed analysis of morphological features. This leads us to consider the following questions: 3.1 Choice of Classifier Given the combination of a small training set and a large POS tagset, the choice of classifier is not obvious. We decided to focus on two approaches that have been shown to be successful in POS tagging: Conditional Random Fields (CRF) (Gahbiche-Braham et al., 2012) and Recurrent Neural Networks (RNN) (Shao et al., 2017). RNNs are considered state of the art, but it is well known that they work best when they have access to large amounts of training data (Horsmann and Zesch, 2017). Figure 3: Example of an unpunctuated example. Figure 4: Example of a phrase element. CRFs may be more amenable to small training data sets, but they may not scale up to a large label set (Horsmann and Zesch, 2017). Additionally, neural models can be pretrained on additional data from other domains and then optimized on our small training set. 3.2 Utilizing Coarse Grained POS Tagging as Preprocessing The large tagset in this corpus"
2020.udw-1.23,W16-5403,0,0.0281401,"so used in other syntactic contexts, as an attributive function. We consider this usage a homograph of the case marker, assuming that it is influenced by Chinese. I.e., i is a modifier particle PART instead of a case marker ADP. The attributive function of i occurs frequently in Cabcal News. There are two types of attributive functions: In the first case, i marks adjectival modifiers. In Figure 4, sˇahvrun ‘cold’ is an adjective and directly modifies mujilen ‘heart’, but there is an i without obvious function. We assume that this is borrowed from Chinese. We follow the Chinese-HK UD treebank (Leung et al., 2016; Wong et al., 2017) and annotate the adjective as the head of the particle i. The particle i is treated as a mark:rel dependent of the adjective. In the second case, i marks adverbial modifiers. In Figure 5, ten is a noun, meaning ‘pole, extreme’. The following particle i marks it to be an adverbial modifier of the adjective amba ‘big’, describing the degree of the adjective. Thus ten is an adjunct depending on the adjective with the relation obl, and i depends on ten with the relation mark:adv indicating that the noun functions as an adverbial modifier. 4.3 Topic Marker oci Xibe uses the can"
2020.udw-1.23,W17-7604,1,0.838203,"lation. Tokenization assumes that all words are separated by spaces or punctuation. We annotated each word with its lemma, UTS part of speech tag, morphological features, and dependency annotation. For the first 544 sentences, the annotation work was carried out by two annotators. The first annotator annotated 464 sentences, and the second annotator annotated 80 sentences. The 80 sentences by the second annotator were checked by the first annotator to keep the annotation consistent. As for the second part of the data, the annotation was performed by the first annotator. We used UD Annotatrix (Tyers et al., 2017) to facilitate our annotation. 3.3 Transliteration The writing system of Xibe is untypical in that its writing direction is from top to bottom, from left to right. The Xibe script is based on Manchu script with slight modifications, which uses traditional Mongolian letters. Xibe letters have different forms: Most of the letters have three forms at initial, medial, or final position, but some letters just have one or two forms. In Table 1, all letters but ng ᡢ are the initial forms. For ng ᡢ, we show the final form since it cannot occur in initial position. Modern written Xibe has 5 vowels, 19"
2020.udw-1.23,W17-6530,0,0.0207762,"tactic contexts, as an attributive function. We consider this usage a homograph of the case marker, assuming that it is influenced by Chinese. I.e., i is a modifier particle PART instead of a case marker ADP. The attributive function of i occurs frequently in Cabcal News. There are two types of attributive functions: In the first case, i marks adjectival modifiers. In Figure 4, sˇahvrun ‘cold’ is an adjective and directly modifies mujilen ‘heart’, but there is an i without obvious function. We assume that this is borrowed from Chinese. We follow the Chinese-HK UD treebank (Leung et al., 2016; Wong et al., 2017) and annotate the adjective as the head of the particle i. The particle i is treated as a mark:rel dependent of the adjective. In the second case, i marks adverbial modifiers. In Figure 5, ten is a noun, meaning ‘pole, extreme’. The following particle i marks it to be an adverbial modifier of the adjective amba ‘big’, describing the degree of the adjective. Thus ten is an adjunct depending on the adjective with the relation obl, and i depends on ten with the relation mark:adv indicating that the noun functions as an adverbial modifier. 4.3 Topic Marker oci Xibe uses the canonical word order of"
2021.iwpt-1.10,E17-1015,0,0.131528,"y achieved proportionally better results as expected with neural models (Fried et al., 2019). One simple reason can be attributed to the fact that we have severe limitations in terms of existing data, data sizes, and the imbalance between the two treebanks typically present in domain adaptation settings. Multi-task learning (MTL; Caruana, 1997) allows for joint learning, which can help facilitate cross information sharing between tasks. This has proven particularly beneficial for tasks that have large data imbalances, with the smaller data tasks benefiting substantially more (Johansson, 2013; Benton et al., 2017; Ruder et al., 2019), and should thus also be useful in domain adaptation. We define domain adaptation as an MTL problem where the two tasks correspond to training on 2. How effective is loss weighting in addressing the data imbalance? Can we optimize both MTL tasks given the data imbalance? 3. Is it more important to address the data imbalance or the differences between domains for successful domain adaptation? 2 2.1 Related Work MTL in Parsing MTL inherently allows for the joint learning of tasks. Learning related tasks, such as POS tagging and dependency parsing, has been shown to be benef"
2021.iwpt-1.10,E17-2026,0,0.125229,"such as POS tagging and dependency parsing, has been shown to be beneficial (Bohnet and Nivre, 2012; Zhang and Weiss, 93 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 93–105 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2.3 2016). A practical assumption is that there is shared information that can be beneficial, particularly if the tasks are closely related. While MTL has resulted in improvements across many tasks and settings, an STL can still outperform an MTL model (Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017; Liang et al., 2020). Reasons for such a lack of increase or even degradation in performance for a certain task may be found in negative transfer as tasks may learn at different rates, and a single task may dominate the learning (Lee et al., 2016), or poor scheduling may result in catastrophic forgetting (French, 1999). Another key fact is the correct choice of tasks. However, it is not clear how to best select tasks. Auxiliary task label distributions (Mart´ınez Alonso and Plank, 2017), the learning curve of the primary task (Bingel and Søgaard, 2017), the difficulty of the auxiliary task (L"
2021.iwpt-1.10,D12-1133,0,0.0166546,"et al., 2019), and should thus also be useful in domain adaptation. We define domain adaptation as an MTL problem where the two tasks correspond to training on 2. How effective is loss weighting in addressing the data imbalance? Can we optimize both MTL tasks given the data imbalance? 3. Is it more important to address the data imbalance or the differences between domains for successful domain adaptation? 2 2.1 Related Work MTL in Parsing MTL inherently allows for the joint learning of tasks. Learning related tasks, such as POS tagging and dependency parsing, has been shown to be beneficial (Bohnet and Nivre, 2012; Zhang and Weiss, 93 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 93–105 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2.3 2016). A practical assumption is that there is shared information that can be beneficial, particularly if the tasks are closely related. While MTL has resulted in improvements across many tasks and settings, an STL can still outperform an MTL model (Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017; Liang et al., 2020). Reasons for such a lack of increase or even degradat"
2021.iwpt-1.10,W19-7723,0,0.0453953,"Missing"
2021.iwpt-1.10,N19-1423,0,0.110838,"to learn task specific information, we apply task-specific biaffine attention layers to the MLP output to produce scores for both arcs and labels. A more detailed description of the parser architecture can be found in Sayyed and Dakota (2021). We modify the PyTorch (Paszke et al., 2019) implementation of the biaffine parser provided by Zhang et al. (2020)2 , to implement our MTL parser3 . We retain many of the default hyperparameters used in the original base parser. Table 2 lists the parameters which we have changed. Word and POS embeddings are initialized randomly. For the BERT embeddings (Devlin et al., 2019), a scalar mixture of the last four layers of BERT is passed through a linear layer to produce BERT embeddings of the specified dimension. For our experiments, we focus on German and Italian, since both languages have smaller treebanks based on Twitter data, which will allow us to avoid domain differences in the smaller domain. We use treebanks annotated with Universal Dependencies V2.7 (Nivre et al., 2020). For German, we use GSD, which is based on news, reviews, and Wikipedia pages, and tweeDe (Rehbein et al., 2019) as the Twitter treebank. For Italian, we use ISDT and ParTUT, which consist"
2021.iwpt-1.10,P17-2054,0,0.0182384,"ative transfer as tasks may learn at different rates, and a single task may dominate the learning (Lee et al., 2016), or poor scheduling may result in catastrophic forgetting (French, 1999). Another key fact is the correct choice of tasks. However, it is not clear how to best select tasks. Auxiliary task label distributions (Mart´ınez Alonso and Plank, 2017), the learning curve of the primary task (Bingel and Søgaard, 2017), the difficulty of the auxiliary task (Liebel and K¨orner, 2018), the relationship between the data of the tasks in terms of size (Luong et al., 2015; Benton et al., 2017; Augenstein and Søgaard, 2017; Schulz et al., 2018) and properties (Wu et al., 2020), among other findings1 , have all shown to influence the effectiveness of MTL. One way to mitigate the negative transfer is to give different weights to the tasks, helping to maximize the contributions for the more pertinent tasks and lessen the impact of sub-optimal tasks (Lee et al., 2016, 2018). Such strategies have shown promise in computer vision, where optimal loss weights can allow an MTL model to improve over a corresponding STL when it would otherwise show a degradation in performance (Kendall et al., 2018). Winata et al. (2018)"
2021.iwpt-1.10,P15-2139,0,0.031091,"ing, and chunking. Our experiments focus on improving parsing in a domain adaptation setting using MTL plus separate Neural networks have increased the ability and ease by which models can exploit information sharing. Much recent parsing research has examined the impact of parameter sharing across treebanks and languages (Ammar et al., 2016; Kitaev et al., 2019), though often not explicitly within an MTL setup, where different treebanks/languages are treated as multiple tasks. Soft sharing of parameters has proven effective on treebanks of the same annotation style in lower resource settings (Duong et al., 2015) as well as for multiple treebanks of the same language when hard sharing all other parameters (Stymne et al., 2018). However, sharing too many parameters between unrelated languages has been shown not to be beneficial (de Lhoneux et al., 2018). More explicit MTL settings with treebanks representing different individual tasks have proven successful in array of settings across languages and architectures (Guo et al., 2016; Johansson and Adesam, 2020; Kankanampati et al., 2020). 2.2 MTL Performance MTL in Domain Adaptation Much recent work has resulted in significant gains in domain adaptation a"
2021.iwpt-1.10,P19-1031,0,0.104934,"s performance in an in-domain setting. Given loss weighting in MTL, we can improve results for both parsers. 1 ¨ Sandra Kubler Indiana University skuebler@indiana.edu Introduction 1. How does the MTL parser handle different levels of data imbalance? This assumes that in a domain adaptation setting, normally a small in-domain treebank is combined with a large out-of-domain treebank. Domain adaption in syntactic parsing is still a significant challenge. While recent work has shown steady improvements, we have not necessarily achieved proportionally better results as expected with neural models (Fried et al., 2019). One simple reason can be attributed to the fact that we have severe limitations in terms of existing data, data sizes, and the imbalance between the two treebanks typically present in domain adaptation settings. Multi-task learning (MTL; Caruana, 1997) allows for joint learning, which can help facilitate cross information sharing between tasks. This has proven particularly beneficial for tasks that have large data imbalances, with the smaller data tasks benefiting substantially more (Johansson, 2013; Benton et al., 2017; Ruder et al., 2019), and should thus also be useful in domain adaptatio"
2021.iwpt-1.10,N19-1355,0,0.133059,"n adaption for chunking for English. Using hyperlinks as a form of weak supervision was used by Søgaard (2017) to improve several NLP tasks both in-domain and out-of-domain, including chunking, for both English and Quechua. Peng and Dredze (2017) use an MTL setting to leverage Chinese word segmentation and NER across two domains, news and social media. They share lower levels but retain domain specific projection layers with task specific models. Results outperform disjoint adaption methods and suffer less from diminishing returns as training sizes increase. 1 See (Søgaard and Goldberg, 2016; Guo et al., 2019; Schr¨oder and Biemann, 2020) for more discussion. 94 German Italian GSD tweeDe ISDT ` TWITTIRO PoSTWITA ParTUT Train 13 814 1 000 13 121 1 000 1 000 1 000 Dev 799 150 564 144 150 150 Hyperparameters Embedding Dimensions POS Tag Embedding Dimension Bert Mapping Dimenstion Number of BERT Layers Used Number of LSTM Layers LSTM Hidden Layer Dimension Optimizer Patience Batch Size Learning Rate Test 977 151 482 142 150 150 Table 1: Treebank sizes (number of sentences). loss weighting to improve both tasks. 3 3.1 Table 2: Hyperparameter settings for MTL parser. Methodology Treebanks used. These Bi"
2021.iwpt-1.10,C16-1002,0,0.0192469,"ent treebanks/languages are treated as multiple tasks. Soft sharing of parameters has proven effective on treebanks of the same annotation style in lower resource settings (Duong et al., 2015) as well as for multiple treebanks of the same language when hard sharing all other parameters (Stymne et al., 2018). However, sharing too many parameters between unrelated languages has been shown not to be beneficial (de Lhoneux et al., 2018). More explicit MTL settings with treebanks representing different individual tasks have proven successful in array of settings across languages and architectures (Guo et al., 2016; Johansson and Adesam, 2020; Kankanampati et al., 2020). 2.2 MTL Performance MTL in Domain Adaptation Much recent work has resulted in significant gains in domain adaptation across several languages via the direct or indirect transfer of parameters and embeddings for languages such as Chinese (Li et al., 2019, 2020), English (Joshi et al., 2018; Fried et al., 2019), Finnish (Virtanen et al., 2019), and French (Martin et al., 2020). More explicit MTL work by Søgaard and Goldberg (2016) found that lower level tasks are best kept at lower layers, as the shared representations benefit from the se"
2021.iwpt-1.10,Q18-1017,0,0.0200638,"e time, even when highly imbalanced, for in-domain and out-of-domain experiments for both German and Italian. We conclude that while domain differences certainly play a factor, data imbalance appears to have more influence on parser performance. In the future, our experiments need to be extended to a wider range of languages and target domains. Additionally, we will investigate strategies for dynamic learning of weights (Guo et al., 2019; Liu et al., 2019; Ming et al., 2019; Yim and Kim, 2020) for determining optimal loss weighting automatically, as well as more complex scheduling approaches (Kiperwasser and Ballesteros, 2018; Guo et al., 2018; S´ebastien et al., 2018) to further improve performance. Acknowledgements The authors would like to thank Ines Rehbein for providing the tweeDe treebank, members of the Uppsala Parsing Group: Artur Kulmizev, Joakim Nivre, and Sara Stymne for their feedback, as well as the anonymous reviewers for their comments. This research was supported in part by Lilly Endowment, Inc., through its support for the Indiana University Pervasive Technology Institute. The first author is supported by the Swedish strategic research programme eSSENCE. In Proceedings of the 55th Annual Meeting o"
2021.iwpt-1.10,P19-1340,0,0.0135365,"By using different weights for the different level tasks, starting with higher weights for lower tasks before incrementally increasing weights to higher level tasks during training, they achieve a noticeable error reduction in POS tagging, dependency parsing, and chunking. Our experiments focus on improving parsing in a domain adaptation setting using MTL plus separate Neural networks have increased the ability and ease by which models can exploit information sharing. Much recent parsing research has examined the impact of parameter sharing across treebanks and languages (Ammar et al., 2016; Kitaev et al., 2019), though often not explicitly within an MTL setup, where different treebanks/languages are treated as multiple tasks. Soft sharing of parameters has proven effective on treebanks of the same annotation style in lower resource settings (Duong et al., 2015) as well as for multiple treebanks of the same language when hard sharing all other parameters (Stymne et al., 2018). However, sharing too many parameters between unrelated languages has been shown not to be beneficial (de Lhoneux et al., 2018). More explicit MTL settings with treebanks representing different individual tasks have proven succe"
2021.iwpt-1.10,D18-1543,0,0.0314797,"Missing"
2021.iwpt-1.10,N13-1013,0,0.0249921,"ve not necessarily achieved proportionally better results as expected with neural models (Fried et al., 2019). One simple reason can be attributed to the fact that we have severe limitations in terms of existing data, data sizes, and the imbalance between the two treebanks typically present in domain adaptation settings. Multi-task learning (MTL; Caruana, 1997) allows for joint learning, which can help facilitate cross information sharing between tasks. This has proven particularly beneficial for tasks that have large data imbalances, with the smaller data tasks benefiting substantially more (Johansson, 2013; Benton et al., 2017; Ruder et al., 2019), and should thus also be useful in domain adaptation. We define domain adaptation as an MTL problem where the two tasks correspond to training on 2. How effective is loss weighting in addressing the data imbalance? Can we optimize both MTL tasks given the data imbalance? 3. Is it more important to address the data imbalance or the differences between domains for successful domain adaptation? 2 2.1 Related Work MTL in Parsing MTL inherently allows for the joint learning of tasks. Learning related tasks, such as POS tagging and dependency parsing, has b"
2021.iwpt-1.10,2020.coling-main.338,0,0.084536,"Missing"
2021.iwpt-1.10,2020.lrec-1.642,0,0.0269307,"uages are treated as multiple tasks. Soft sharing of parameters has proven effective on treebanks of the same annotation style in lower resource settings (Duong et al., 2015) as well as for multiple treebanks of the same language when hard sharing all other parameters (Stymne et al., 2018). However, sharing too many parameters between unrelated languages has been shown not to be beneficial (de Lhoneux et al., 2018). More explicit MTL settings with treebanks representing different individual tasks have proven successful in array of settings across languages and architectures (Guo et al., 2016; Johansson and Adesam, 2020; Kankanampati et al., 2020). 2.2 MTL Performance MTL in Domain Adaptation Much recent work has resulted in significant gains in domain adaptation across several languages via the direct or indirect transfer of parameters and embeddings for languages such as Chinese (Li et al., 2019, 2020), English (Joshi et al., 2018; Fried et al., 2019), Finnish (Virtanen et al., 2019), and French (Martin et al., 2020). More explicit MTL work by Søgaard and Goldberg (2016) found that lower level tasks are best kept at lower layers, as the shared representations benefit from the sequence of information learne"
2021.iwpt-1.10,P19-1229,0,0.0208603,"wever, sharing too many parameters between unrelated languages has been shown not to be beneficial (de Lhoneux et al., 2018). More explicit MTL settings with treebanks representing different individual tasks have proven successful in array of settings across languages and architectures (Guo et al., 2016; Johansson and Adesam, 2020; Kankanampati et al., 2020). 2.2 MTL Performance MTL in Domain Adaptation Much recent work has resulted in significant gains in domain adaptation across several languages via the direct or indirect transfer of parameters and embeddings for languages such as Chinese (Li et al., 2019, 2020), English (Joshi et al., 2018; Fried et al., 2019), Finnish (Virtanen et al., 2019), and French (Martin et al., 2020). More explicit MTL work by Søgaard and Goldberg (2016) found that lower level tasks are best kept at lower layers, as the shared representations benefit from the sequence of information learned, with the approach demonstrating success in domain adaption for chunking for English. Using hyperlinks as a form of weak supervision was used by Søgaard (2017) to improve several NLP tasks both in-domain and out-of-domain, including chunking, for both English and Quechua. Peng and"
2021.iwpt-1.10,P18-1110,0,0.0537204,"Missing"
2021.iwpt-1.10,2020.coling-main.225,0,0.0744853,"Missing"
2021.iwpt-1.10,E17-1005,0,0.0255277,"Learning related tasks, such as POS tagging and dependency parsing, has been shown to be beneficial (Bohnet and Nivre, 2012; Zhang and Weiss, 93 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 93–105 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2.3 2016). A practical assumption is that there is shared information that can be beneficial, particularly if the tasks are closely related. While MTL has resulted in improvements across many tasks and settings, an STL can still outperform an MTL model (Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017; Liang et al., 2020). Reasons for such a lack of increase or even degradation in performance for a certain task may be found in negative transfer as tasks may learn at different rates, and a single task may dominate the learning (Lee et al., 2016), or poor scheduling may result in catastrophic forgetting (French, 1999). Another key fact is the correct choice of tasks. However, it is not clear how to best select tasks. Auxiliary task label distributions (Mart´ınez Alonso and Plank, 2017), the learning curve of the primary task (Bingel and Søgaard, 2017), the difficult"
2021.iwpt-1.10,2020.lrec-1.497,0,0.0258389,"Missing"
2021.iwpt-1.10,W17-2612,0,0.0224846,"l., 2019, 2020), English (Joshi et al., 2018; Fried et al., 2019), Finnish (Virtanen et al., 2019), and French (Martin et al., 2020). More explicit MTL work by Søgaard and Goldberg (2016) found that lower level tasks are best kept at lower layers, as the shared representations benefit from the sequence of information learned, with the approach demonstrating success in domain adaption for chunking for English. Using hyperlinks as a form of weak supervision was used by Søgaard (2017) to improve several NLP tasks both in-domain and out-of-domain, including chunking, for both English and Quechua. Peng and Dredze (2017) use an MTL setting to leverage Chinese word segmentation and NER across two domains, news and social media. They share lower levels but retain domain specific projection layers with task specific models. Results outperform disjoint adaption methods and suffer less from diminishing returns as training sizes increase. 1 See (Søgaard and Goldberg, 2016; Guo et al., 2019; Schr¨oder and Biemann, 2020) for more discussion. 94 German Italian GSD tweeDe ISDT ` TWITTIRO PoSTWITA ParTUT Train 13 814 1 000 13 121 1 000 1 000 1 000 Dev 799 150 564 144 150 150 Hyperparameters Embedding Dimensions POS Tag"
2021.iwpt-1.10,W19-7811,0,0.0237439,"d. Word and POS embeddings are initialized randomly. For the BERT embeddings (Devlin et al., 2019), a scalar mixture of the last four layers of BERT is passed through a linear layer to produce BERT embeddings of the specified dimension. For our experiments, we focus on German and Italian, since both languages have smaller treebanks based on Twitter data, which will allow us to avoid domain differences in the smaller domain. We use treebanks annotated with Universal Dependencies V2.7 (Nivre et al., 2020). For German, we use GSD, which is based on news, reviews, and Wikipedia pages, and tweeDe (Rehbein et al., 2019) as the Twitter treebank. For Italian, we use ISDT and ParTUT, which consist of legal, news, and Wikipedia ` (Cignarella et al., 2019) and texts, plus TWITTIRO PoSTWITA (Sanguinetti et al., 2018) as the Twitter treebanks. Table 1 shows the sizes of the treebanks used in our experiments. For tweeDe we used the first 1 000 sentences for train, and the following 150 and 151 sentences for dev and test respectively. In order to account for treebank size variations of the Twitter treebanks, we limit the maximal sizes ` and PoSTWITA of train and dev for TWITTIRO to the first 1 000 train sentences and"
2021.iwpt-1.10,L18-1279,0,0.0192725,"e BERT embeddings of the specified dimension. For our experiments, we focus on German and Italian, since both languages have smaller treebanks based on Twitter data, which will allow us to avoid domain differences in the smaller domain. We use treebanks annotated with Universal Dependencies V2.7 (Nivre et al., 2020). For German, we use GSD, which is based on news, reviews, and Wikipedia pages, and tweeDe (Rehbein et al., 2019) as the Twitter treebank. For Italian, we use ISDT and ParTUT, which consist of legal, news, and Wikipedia ` (Cignarella et al., 2019) and texts, plus TWITTIRO PoSTWITA (Sanguinetti et al., 2018) as the Twitter treebanks. Table 1 shows the sizes of the treebanks used in our experiments. For tweeDe we used the first 1 000 sentences for train, and the following 150 and 151 sentences for dev and test respectively. In order to account for treebank size variations of the Twitter treebanks, we limit the maximal sizes ` and PoSTWITA of train and dev for TWITTIRO to the first 1 000 train sentences and 150 dev and test sentences respectively, but we do not reduce the GSD or ISDT treebanks. For the in-domain experiments in section 6, we also use the ParTUT treebank, since it covers domains simi"
2021.iwpt-1.10,2021.findings-acl.305,1,0.795658,"a dimension-reducing Multi-layered Perceptrons (MLP) to strip away arc and relationship information deemed irrelevant for the task at hand. We implement two MLP schemes, one in which we share the MLP layers across tasks (shared-MLP; left part of Figure 1 ) and the other in which each task has its own MLP layers (unshared; right part of Figure 1). Finally, in order for the model to learn task specific information, we apply task-specific biaffine attention layers to the MLP output to produce scores for both arcs and labels. A more detailed description of the parser architecture can be found in Sayyed and Dakota (2021). We modify the PyTorch (Paszke et al., 2019) implementation of the biaffine parser provided by Zhang et al. (2020)2 , to implement our MTL parser3 . We retain many of the default hyperparameters used in the original base parser. Table 2 lists the parameters which we have changed. Word and POS embeddings are initialized randomly. For the BERT embeddings (Devlin et al., 2019), a scalar mixture of the last four layers of BERT is passed through a linear layer to produce BERT embeddings of the specified dimension. For our experiments, we focus on German and Italian, since both languages have small"
2021.iwpt-1.10,2020.acl-main.268,0,0.0630706,"Missing"
2021.iwpt-1.10,N18-2006,0,0.0180093,"arn at different rates, and a single task may dominate the learning (Lee et al., 2016), or poor scheduling may result in catastrophic forgetting (French, 1999). Another key fact is the correct choice of tasks. However, it is not clear how to best select tasks. Auxiliary task label distributions (Mart´ınez Alonso and Plank, 2017), the learning curve of the primary task (Bingel and Søgaard, 2017), the difficulty of the auxiliary task (Liebel and K¨orner, 2018), the relationship between the data of the tasks in terms of size (Luong et al., 2015; Benton et al., 2017; Augenstein and Søgaard, 2017; Schulz et al., 2018) and properties (Wu et al., 2020), among other findings1 , have all shown to influence the effectiveness of MTL. One way to mitigate the negative transfer is to give different weights to the tasks, helping to maximize the contributions for the more pertinent tasks and lessen the impact of sub-optimal tasks (Lee et al., 2016, 2018). Such strategies have shown promise in computer vision, where optimal loss weights can allow an MTL model to improve over a corresponding STL when it would otherwise show a degradation in performance (Kendall et al., 2018). Winata et al. (2018) weighted losses for la"
2021.iwpt-1.10,W17-6310,0,0.232292,"tagging and dependency parsing, has been shown to be beneficial (Bohnet and Nivre, 2012; Zhang and Weiss, 93 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 93–105 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics 2.3 2016). A practical assumption is that there is shared information that can be beneficial, particularly if the tasks are closely related. While MTL has resulted in improvements across many tasks and settings, an STL can still outperform an MTL model (Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017; Liang et al., 2020). Reasons for such a lack of increase or even degradation in performance for a certain task may be found in negative transfer as tasks may learn at different rates, and a single task may dominate the learning (Lee et al., 2016), or poor scheduling may result in catastrophic forgetting (French, 1999). Another key fact is the correct choice of tasks. However, it is not clear how to best select tasks. Auxiliary task label distributions (Mart´ınez Alonso and Plank, 2017), the learning curve of the primary task (Bingel and Søgaard, 2017), the difficulty of the auxiliary task (L"
2021.iwpt-1.10,P16-2038,0,0.0287788,"representing different individual tasks have proven successful in array of settings across languages and architectures (Guo et al., 2016; Johansson and Adesam, 2020; Kankanampati et al., 2020). 2.2 MTL Performance MTL in Domain Adaptation Much recent work has resulted in significant gains in domain adaptation across several languages via the direct or indirect transfer of parameters and embeddings for languages such as Chinese (Li et al., 2019, 2020), English (Joshi et al., 2018; Fried et al., 2019), Finnish (Virtanen et al., 2019), and French (Martin et al., 2020). More explicit MTL work by Søgaard and Goldberg (2016) found that lower level tasks are best kept at lower layers, as the shared representations benefit from the sequence of information learned, with the approach demonstrating success in domain adaption for chunking for English. Using hyperlinks as a form of weak supervision was used by Søgaard (2017) to improve several NLP tasks both in-domain and out-of-domain, including chunking, for both English and Quechua. Peng and Dredze (2017) use an MTL setting to leverage Chinese word segmentation and NER across two domains, news and social media. They share lower levels but retain domain specific proje"
2021.iwpt-1.10,P18-2098,0,0.0639097,"Missing"
2021.iwpt-1.10,W18-3207,0,0.0279559,"ein and Søgaard, 2017; Schulz et al., 2018) and properties (Wu et al., 2020), among other findings1 , have all shown to influence the effectiveness of MTL. One way to mitigate the negative transfer is to give different weights to the tasks, helping to maximize the contributions for the more pertinent tasks and lessen the impact of sub-optimal tasks (Lee et al., 2016, 2018). Such strategies have shown promise in computer vision, where optimal loss weights can allow an MTL model to improve over a corresponding STL when it would otherwise show a degradation in performance (Kendall et al., 2018). Winata et al. (2018) weighted losses for language modeling and POS tagging in an MTL setting, finding a lower weight to language modeling yielded a reduction in perplexity in modeling codeswitching between Chinese and English. A multitask supervised pretraining adaption strategy using a hierarchical architecture that learns multiple tasks on a source domain before fine-tuning them on the target was implemented by Meftah et al. (2020). By using different weights for the different level tasks, starting with higher weights for lower tasks before incrementally increasing weights to higher level tasks during training,"
2021.iwpt-1.10,K18-2001,0,0.0445441,"Missing"
2021.iwpt-1.10,2020.acl-main.302,0,0.0225681,"or the task at hand. We implement two MLP schemes, one in which we share the MLP layers across tasks (shared-MLP; left part of Figure 1 ) and the other in which each task has its own MLP layers (unshared; right part of Figure 1). Finally, in order for the model to learn task specific information, we apply task-specific biaffine attention layers to the MLP output to produce scores for both arcs and labels. A more detailed description of the parser architecture can be found in Sayyed and Dakota (2021). We modify the PyTorch (Paszke et al., 2019) implementation of the biaffine parser provided by Zhang et al. (2020)2 , to implement our MTL parser3 . We retain many of the default hyperparameters used in the original base parser. Table 2 lists the parameters which we have changed. Word and POS embeddings are initialized randomly. For the BERT embeddings (Devlin et al., 2019), a scalar mixture of the last four layers of BERT is passed through a linear layer to produce BERT embeddings of the specified dimension. For our experiments, we focus on German and Italian, since both languages have smaller treebanks based on Twitter data, which will allow us to avoid domain differences in the smaller domain. We use t"
2021.iwpt-1.10,P16-1147,0,0.0494517,"Missing"
2021.latechclfl-1.19,W06-0903,0,0.0494186,"d RNN outperform paragraph vectors and conventional machine learning methods. Recently, contextualized embedding models have been introduced to lexical semantic change studies (Hu et al., 2019; Giulianelli et al., 2020; Tseng et al., 2020), with promising results. Related Work Studies in Time Period Classification 2.2 Digital Resources of Historical Chinese Studies using language models to capture temporal Even though we see an increase of work on Classiinformation in contemporary diachronic texts date cal Chinese processing, researchers also are aware back to the 2000s (de Jong et al., 2005; Dalli and Wilks, 2006), they demonstrate that machine learn- of the lack of Chinese diachronic resources, which hinders the research process to some extent (Hamiling systems with textual features can successfully predict the publication time span of documents. ton et al., 2016; Zinin and Xu, 2020). Unlike other understudied languages, many Chinese texts are The shared-task on Diachronic Text Evaluation in SemEval 2015 (Popescu and Strapparava, 2015) already digitized, but only a few digitized texts are provided resources and publicity for the task of au- free to access and process. Most of the datasets are designed"
2021.latechclfl-1.19,N19-1423,0,0.0181218,"atures will help us capture subtle semantic relations and infrequent phrases. We use Gensim10 with 300-dimension skip-grams with negative sampling word embeddings (SGNS) trained on Siku Quanshu ’Complete Library in Four Sections’ (Li et al., 2018), Siku Quanshu is the largest encyclopedia in Ancient China, it contains 3503 books, around 800 million tokens (character). The final feature representation is the average of the individual character vectors in a single instance. 4.3 Contextualized Embeddings Features Instead of using static embeddings, in contextualized language models such as BERT (Devlin et al., 2019), the same word is represented differently according to its contexts. This is promising for lexical change detection, but it has not been applied to the classification of time periods. Here we use Sentence BERT (SBERT) (Reimers and 10 Gurevych, 2019) to embed the whole instance. Using siamese and triplet network structures, Sentence BERT can efficiently generate the sentence embeddings from pre-trained models of the BERT model family. From the different pretrained models, we chose Guwen-RoBertA11 , a RoBertA model (Liu et al., 2019) trained on a large collection of Classical Chinese, which is"
2021.latechclfl-1.19,P16-1141,0,0.0296942,". We report results of the different feature representation methods and present an error analysis in section 6. In section 7, we analyze the classification for neighboring dynasties, focusing on important features drawn from the training data. We conclude in section 8. 2 2.1 Zampieri et al. (2016) built an SVM classifier with word and POS n-grams to classify Portuguese texts from 1600 to 1900. Meanwhile, in the related area of lexical semantic change, many studies use word embeddings trained from deep learning models to model the language change and successfully track the semantic shift (e.g. Hamilton et al., 2016; Rodda et al., 2017; Rodman, 2020). Liebeskind and Liebeskind (2020) first implemented deep learning methods in time period classification tasks. They show that CNN and RNN outperform paragraph vectors and conventional machine learning methods. Recently, contextualized embedding models have been introduced to lexical semantic change studies (Hu et al., 2019; Giulianelli et al., 2020; Tseng et al., 2020), with promising results. Related Work Studies in Time Period Classification 2.2 Digital Resources of Historical Chinese Studies using language models to capture temporal Even though we see an"
2021.latechclfl-1.19,P19-1379,0,0.0232907,"guese texts from 1600 to 1900. Meanwhile, in the related area of lexical semantic change, many studies use word embeddings trained from deep learning models to model the language change and successfully track the semantic shift (e.g. Hamilton et al., 2016; Rodda et al., 2017; Rodman, 2020). Liebeskind and Liebeskind (2020) first implemented deep learning methods in time period classification tasks. They show that CNN and RNN outperform paragraph vectors and conventional machine learning methods. Recently, contextualized embedding models have been introduced to lexical semantic change studies (Hu et al., 2019; Giulianelli et al., 2020; Tseng et al., 2020), with promising results. Related Work Studies in Time Period Classification 2.2 Digital Resources of Historical Chinese Studies using language models to capture temporal Even though we see an increase of work on Classiinformation in contemporary diachronic texts date cal Chinese processing, researchers also are aware back to the 2000s (de Jong et al., 2005; Dalli and Wilks, 2006), they demonstrate that machine learn- of the lack of Chinese diachronic resources, which hinders the research process to some extent (Hamiling systems with textual featu"
2021.latechclfl-1.19,P18-2023,0,0.012441,"ngs vectors are in comparison to other feature types in the task of dating historical texts, which is different from the problem addressed in previous studies. Assuming that the small size of our dataset would negatively influence the quality of embedding models trained on the dataset, we use pretrained character embeddings instead. We expect that these pretrained features will help us capture subtle semantic relations and infrequent phrases. We use Gensim10 with 300-dimension skip-grams with negative sampling word embeddings (SGNS) trained on Siku Quanshu ’Complete Library in Four Sections’ (Li et al., 2018), Siku Quanshu is the largest encyclopedia in Ancient China, it contains 3503 books, around 800 million tokens (character). The final feature representation is the average of the individual character vectors in a single instance. 4.3 Contextualized Embeddings Features Instead of using static embeddings, in contextualized language models such as BERT (Devlin et al., 2019), the same word is represented differently according to its contexts. This is promising for lexical change detection, but it has not been applied to the classification of time periods. Here we use Sentence BERT (SBERT) (Reimers"
2021.latechclfl-1.19,2021.ccl-1.108,0,0.0588564,"Missing"
2021.latechclfl-1.19,S15-2147,0,0.145184,"g the time periods of a text. These Traditional studies of language change largely rely feature representation methods have been shown efon close reading and require researchers to have fective in previous studies of dating historical texts solid training in understanding historical texts. As or lexical semantic change (Zampieri et al., 2016; a wealth of historical data has been digitized, there has been a surge in recent years in using compu- Schlechtweg et al., 2020; Giulianelli et al., 2020). tational methods to investigate language change Our next question looks closer at the interre(e.g. Popescu and Strapparava, 2015; Hamilton lation between text dating and language change. et al., 2016; Rodda et al., 2017). However, most of The classification results provide useful informathese studies are limited to a small number of Indo- tion, but they cannot directly tell us which features European languages. Many languages, especially drive language change. Since language change is typologically very different ones, are understudied. an accumulating process and usually happens in One task that often precedes an investigation neighboring time periods (dynasties), we use a task of language change is automatic text dat"
2021.latechclfl-1.19,D19-1410,0,0.0391644,"Missing"
2021.latechclfl-1.19,S15-2148,0,0.0227981,"fferent languages tend to have different classification stan- Chinese Dynasty Histories for diachronic research. However, the dynasty histories are mainly literary dards, and this is closely related to the language in Chinese and normally written by a small number of question and the goal of the task. Regarding feature engineering in dating texts, authors per dynasty within a short period of time. lexical features including character and word n- Thus, they cannot represent the language properties grams are most widely used in classifying time pe- through the whole dynasty. riods. For example, Szymanski and Lynch (2015) found that character n-grams are more effective 1 http://www.ancientbooks.cn/ 2 compared to other features such as POS n-grams, http://hanchi.ihp.sinica.edu.tw/ 3 word n-grams, and syntactic phrase-structure rules. http://lingcorpus.iis.sinica.edu.tw/ancient/ 169 2.3 Linguistic Periodization of Chinese For Chinese, different opinions exist wrt. periodization. The most widely accepted framework, proposed by Wang (1958) and Xiang (1993) and accepted by Dong (2019), splits Chinese into four major time periods: Old Chinese (pre-Qin Dynasty), Middle Chinese (Three Kingdoms and Jin Dynasty to Song"
2021.latechclfl-1.19,2020.coling-main.258,0,0.0242501,"n the related area of lexical semantic change, many studies use word embeddings trained from deep learning models to model the language change and successfully track the semantic shift (e.g. Hamilton et al., 2016; Rodda et al., 2017; Rodman, 2020). Liebeskind and Liebeskind (2020) first implemented deep learning methods in time period classification tasks. They show that CNN and RNN outperform paragraph vectors and conventional machine learning methods. Recently, contextualized embedding models have been introduced to lexical semantic change studies (Hu et al., 2019; Giulianelli et al., 2020; Tseng et al., 2020), with promising results. Related Work Studies in Time Period Classification 2.2 Digital Resources of Historical Chinese Studies using language models to capture temporal Even though we see an increase of work on Classiinformation in contemporary diachronic texts date cal Chinese processing, researchers also are aware back to the 2000s (de Jong et al., 2005; Dalli and Wilks, 2006), they demonstrate that machine learn- of the lack of Chinese diachronic resources, which hinders the research process to some extent (Hamiling systems with textual features can successfully predict the publication ti"
2021.latechclfl-1.19,L16-1647,0,0.265034,"vations and assumptions made by historical from century to century (Norman, 1988). More linguists. specifically, we investigate the performance of different feature representation methods (char/word n1 Introduction gram, word2vec, contextualized embeddings models) in classifying the time periods of a text. These Traditional studies of language change largely rely feature representation methods have been shown efon close reading and require researchers to have fective in previous studies of dating historical texts solid training in understanding historical texts. As or lexical semantic change (Zampieri et al., 2016; a wealth of historical data has been digitized, there has been a surge in recent years in using compu- Schlechtweg et al., 2020; Giulianelli et al., 2020). tational methods to investigate language change Our next question looks closer at the interre(e.g. Popescu and Strapparava, 2015; Hamilton lation between text dating and language change. et al., 2016; Rodda et al., 2017). However, most of The classification results provide useful informathese studies are limited to a small number of Indo- tion, but they cannot directly tell us which features European languages. Many languages, especially"
2021.latechclfl-1.19,2020.lrec-1.98,0,0.103196,"tudies in Time Period Classification 2.2 Digital Resources of Historical Chinese Studies using language models to capture temporal Even though we see an increase of work on Classiinformation in contemporary diachronic texts date cal Chinese processing, researchers also are aware back to the 2000s (de Jong et al., 2005; Dalli and Wilks, 2006), they demonstrate that machine learn- of the lack of Chinese diachronic resources, which hinders the research process to some extent (Hamiling systems with textual features can successfully predict the publication time span of documents. ton et al., 2016; Zinin and Xu, 2020). Unlike other understudied languages, many Chinese texts are The shared-task on Diachronic Text Evaluation in SemEval 2015 (Popescu and Strapparava, 2015) already digitized, but only a few digitized texts are provided resources and publicity for the task of au- free to access and process. Most of the datasets are designed for close reading but not for an NLP tomatic historical text dating. The task was to build automatic systems to identify the time period of En- purposes. There are well-designed POS tagged glish news snippets from 1700 to 2010 in three sub- diachronic corpora and high-qualit"
2021.latechclfl-1.19,2020.semeval-1.1,0,0.0236195,"e the performance of different feature representation methods (char/word n1 Introduction gram, word2vec, contextualized embeddings models) in classifying the time periods of a text. These Traditional studies of language change largely rely feature representation methods have been shown efon close reading and require researchers to have fective in previous studies of dating historical texts solid training in understanding historical texts. As or lexical semantic change (Zampieri et al., 2016; a wealth of historical data has been digitized, there has been a surge in recent years in using compu- Schlechtweg et al., 2020; Giulianelli et al., 2020). tational methods to investigate language change Our next question looks closer at the interre(e.g. Popescu and Strapparava, 2015; Hamilton lation between text dating and language change. et al., 2016; Rodda et al., 2017). However, most of The classification results provide useful informathese studies are limited to a small number of Indo- tion, but they cannot directly tell us which features European languages. Many languages, especially drive language change. Since language change is typologically very different ones, are understudied. an accumulating process and"
2021.scil-1.29,D16-1001,0,0.139652,"wn factor. We perform an analysis of how span-based parsing performs across 11 treebanks in order to examine the overall behavior of this parsing approach and the effect of the treebanks’ specific annotations on results. We find that the parser tends to prefer flatter trees, but the approach works well because it is robust enough to adapt to differences in annotation schemes across treebanks and languages. 1 Introduction Constituency parsing has slowly shifted from the once dominant PCFG-based grammar approaches to span-based neural approaches with a high degree of success (Hall et al., 2014; Cross and Huang, 2016; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019). Such a parser is not restricted to the grammar extracted from training data anymore, but instead learns which constituents to group and which new constituent label to assign from its current configuration. This is parallel to data-driven dependency parsing (K¨ubler et al., 2009) in that the parser learns parsing actions rather than probabilities of rules; and it has been used in shift-reduce constituency parsing (Cross ¨ Sandra Kubler Indiana University skuebler@indiana.edu and Huang, 2016) Not being limited by the grammar rul"
2021.scil-1.29,dakota-kubler-2017-towards,1,0.893812,"Missing"
2021.scil-1.29,N18-1091,0,0.0340419,"Missing"
2021.scil-1.29,P14-1022,0,0.0936065,"l rules is an unknown factor. We perform an analysis of how span-based parsing performs across 11 treebanks in order to examine the overall behavior of this parsing approach and the effect of the treebanks’ specific annotations on results. We find that the parser tends to prefer flatter trees, but the approach works well because it is robust enough to adapt to differences in annotation schemes across treebanks and languages. 1 Introduction Constituency parsing has slowly shifted from the once dominant PCFG-based grammar approaches to span-based neural approaches with a high degree of success (Hall et al., 2014; Cross and Huang, 2016; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019). Such a parser is not restricted to the grammar extracted from training data anymore, but instead learns which constituents to group and which new constituent label to assign from its current configuration. This is parallel to data-driven dependency parsing (K¨ubler et al., 2009) in that the parser learns parsing actions rather than probabilities of rules; and it has been used in shift-reduce constituency parsing (Cross ¨ Sandra Kubler Indiana University skuebler@indiana.edu and Huang, 2016) Not being lim"
2021.scil-1.29,N19-1419,0,0.0462868,"Missing"
2021.scil-1.29,P82-1020,0,0.778478,"Missing"
2021.scil-1.29,P19-1340,0,0.138579,"tract Constituency parsing is generally evaluated superficially, particularly in a multiple language setting, with only F-scores being reported. As new state-of-the-art chart-based parsers have resulted in a transition from traditional PCFG-based grammars to span-based approaches (Stern et al., 2017; Gaddy et al., 2018), we do not have a good understanding of how such fundamentally different approaches interact with various treebanks as results show improvements across treebanks (Kitaev and Klein, 2018), but it is unclear what influence annotation schemes have on various treebank performance (Kitaev et al., 2019). In particular, a span-based parser’s capability of creating novel rules is an unknown factor. We perform an analysis of how span-based parsing performs across 11 treebanks in order to examine the overall behavior of this parsing approach and the effect of the treebanks’ specific annotations on results. We find that the parser tends to prefer flatter trees, but the approach works well because it is robust enough to adapt to differences in annotation schemes across treebanks and languages. 1 Introduction Constituency parsing has slowly shifted from the once dominant PCFG-based grammar approach"
2021.scil-1.29,P18-1249,0,0.0612564,"ativity of a Span-Based Neural Constituency Parser Daniel Dakota Uppsala University daniel.dakota@lingfil.uu.se Abstract Constituency parsing is generally evaluated superficially, particularly in a multiple language setting, with only F-scores being reported. As new state-of-the-art chart-based parsers have resulted in a transition from traditional PCFG-based grammars to span-based approaches (Stern et al., 2017; Gaddy et al., 2018), we do not have a good understanding of how such fundamentally different approaches interact with various treebanks as results show improvements across treebanks (Kitaev and Klein, 2018), but it is unclear what influence annotation schemes have on various treebank performance (Kitaev et al., 2019). In particular, a span-based parser’s capability of creating novel rules is an unknown factor. We perform an analysis of how span-based parsing performs across 11 treebanks in order to examine the overall behavior of this parsing approach and the effect of the treebanks’ specific annotations on results. We find that the parser tends to prefer flatter trees, but the approach works well because it is robust enough to adapt to differences in annotation schemes across treebanks and lang"
2021.scil-1.29,P03-1054,0,0.0377933,"ng early on by Sagae and Lavie (2005). However, it has only recently been implemented in chart-based parsers (Stern et al., 2017; Gaddy et al., 2018). Traditional chart parsers based on PCFG grammars had one major limitation: They assumed a complete grammar, extracted from the training data. This worked well for languages with a low ratio of unique rules, where most of the rules were observed in the training data. However, for languages with annotation schemes preferring flat trees, this was not true. The first approach to address this limitation was (horizontal) Markovization (Collins, 1999; Klein and Manning, 2003), which allowed the parser a strictly limited type of creating new rules. This pivotal advantages of current span-based neural parsers allows them more freedom in creating novel rules by decoupling decisions on spans from a strict set of possible spans, and also decoubling decisions on spans from their constituent labels (Cross and Huang, 2016). This freedom has been made possible empirically by the use of LSTMs (Hochreiter and Schmidhuber, 1997), which provide a rich context for making span decisions. Span-based parsing encodes a constituency tree on the span level rather than the word level."
2021.scil-1.29,E17-2050,0,0.0288657,"Missing"
2021.scil-1.29,2020.acl-main.375,0,0.0472396,"Missing"
2021.scil-1.29,D12-1096,0,0.0136611,"g the various interactions between a parsing approach and different annotation schemes. One of the earliest in-depth investigations was performed by Levy and Manning (2003) on the Penn Chinese Treebank. They found that many errors are due to annotation decisions or ambiguities. In other words, annotation decisions play an important role in understanding global parsing decisions. This is further emphasized by K¨ubler (2005) and Rehbein and van Genabith (2007) in their discussions on the comparability of performance and evaluation based on two German treebanks with different annotation schemes. Kummerfeld et al. (2012, 2013) performed a set of analyses on the English PTB and Chinese PTB respectively by developing a visualization tool for errors.2 This provided deeper insights into the behavior of various constituency parsers in order to identify systematic linguistic errors. Errors were mainly PP and clause attachment for English. Additionally, gold POS tags did not lead to an improvement in verb-argument attachment and coordination scope in Chinese. This is supported by Dakota (2018) who found that improvements on the POS level on the German TiGer treebank (Brants et al., 2004) did not necessarily equally"
2021.scil-1.29,P13-2018,0,0.0337377,"Missing"
2021.scil-1.29,P03-1056,0,0.157075,"differences between the annotation schemes for the individual languages. While the standard evaluation consists of reporting EVALB metrics (Sekine and Collins, 1997), there are investigations into single linguistic phenomena (e.g., discontinuous constituents (van Cranenburgh et al., 2016) or prepositional attachment (de Kok et al., 2017)) in order to isolate and analyze specific behaviors. This means there is often a lack of depth in understanding the various interactions between a parsing approach and different annotation schemes. One of the earliest in-depth investigations was performed by Levy and Manning (2003) on the Penn Chinese Treebank. They found that many errors are due to annotation decisions or ambiguities. In other words, annotation decisions play an important role in understanding global parsing decisions. This is further emphasized by K¨ubler (2005) and Rehbein and van Genabith (2007) in their discussions on the comparability of performance and evaluation based on two German treebanks with different annotation schemes. Kummerfeld et al. (2012, 2013) performed a set of analyses on the English PTB and Chinese PTB respectively by developing a visualization tool for errors.2 This provided dee"
2021.scil-1.29,J93-2004,0,0.0782653,".e., including the mother node and the sequence of daughter nodes; we use ‘span’ to refer to a sequence of daughters. 323 Proceedings of the Society for Computation in Linguistics (SCiL) 2021, pages 323-333. Held on-line February 14-19, 2021 rules across a range of treebanks. More specifically, we investigate which novel rules the parser can create correctly, and which types of errors the parser makes when it creates incorrect rules. We use the state-of-the-art Berkeley Neural Parser (Kitaev et al., 2019) on the SPMRL data set (Seddah et al., 2013, 2014), and we add the English Penn Treebank (Marcus et al., 1993) and the Penn Chinese Treebank (Xue et al., 2005) to cover typologically different languages. We begin with brief summaries of span-based constituency parsing and in-depth evaluations in section 2 before describing our methodology in section 3. We then present an overview of treebank statistics in section 4, followed by an analysis of known rules in section 5, and of novel rules in section 6, before concluding in section 7. 2 Previous Work 2.1 Span-Based Neural Constituency Parsing The shift from grammar-based parsing to datadriven parsing has first been implemented for transition-based parsin"
2021.scil-1.29,C18-1260,0,0.022017,"in order to identify systematic linguistic errors. Errors were mainly PP and clause attachment for English. Additionally, gold POS tags did not lead to an improvement in verb-argument attachment and coordination scope in Chinese. This is supported by Dakota (2018) who found that improvements on the POS level on the German TiGer treebank (Brants et al., 2004) did not necessarily equally percolate to higher level projections. Many performance aspects can be attributed to various treebank annotation decisions and are not easily overcome, particularly if the annotations are inconsistent. Work by Nguyen et al. (2018) investigated the low resource language of Vietnamese by comparing several non-neural and neural parsers to identify sources of common errors. While POS tagging errors did contribute to problems for specific phrase types, they were not the predominant issue in regards to NPs and PPs, rather inherent ambiguity in the language was the main cause. In the same vein, work by Kim and Park (2020) for Korean shows that neural parsers improve substantially in error reduction over their non-neural counterparts, but the most common remaining errors are still those related to long distance attachments. 3"
2021.scil-1.29,W03-3017,0,0.179884,"d the Penn Chinese Treebank (Xue et al., 2005) to cover typologically different languages. We begin with brief summaries of span-based constituency parsing and in-depth evaluations in section 2 before describing our methodology in section 3. We then present an overview of treebank statistics in section 4, followed by an analysis of known rules in section 5, and of novel rules in section 6, before concluding in section 7. 2 Previous Work 2.1 Span-Based Neural Constituency Parsing The shift from grammar-based parsing to datadriven parsing has first been implemented for transition-based parsing (Nivre, 2003) and was adapted for shift-reduce constituency parsing early on by Sagae and Lavie (2005). However, it has only recently been implemented in chart-based parsers (Stern et al., 2017; Gaddy et al., 2018). Traditional chart parsers based on PCFG grammars had one major limitation: They assumed a complete grammar, extracted from the training data. This worked well for languages with a low ratio of unique rules, where most of the rules were observed in the training data. However, for languages with annotation schemes preferring flat trees, this was not true. The first approach to address this limita"
2021.scil-1.29,N18-1202,0,0.0163026,"Missing"
2021.scil-1.29,N07-1051,0,0.0164995,"tatistics Before we can start looking at parser performance, we need a better understanding of the differences between treebanks. In Table 1 we present rule statistics and evaluation across the train5k and test500 sets. Given the nature of our data splits, there are no adequate direct comparisons that can be made with the performance, with the closest being Dakota and K¨ubler (2017) who examined the variation in performance evaluation when randomly selecting 500 test sentences on a subset of languages in the SPRML dataset, the English PTB, and Chinese PTB; however, they used a PCFG-LA parser (Petrov and Klein, 2007) and no optimization on a dev set was performed. A first look at the numbers in Table 1 shows radically different numbers of rules present in the various sets, despite very similar set sizes. The same holds for the number of rule types (i.e., a rule S → NP VP would count as one rule type regardless of how often it occurs). Thus for Arabic, although there are almost 230 000 total rules in train, there are only roughly 4 200 rule types to which the parser is exposed to during training. We also provide the ratio of rule types to total rules and number of constituent types (Const; e.g., NP or VP)"
2021.scil-1.29,D07-1066,0,0.117973,"Missing"
2021.scil-1.29,W05-1513,0,0.0286143,"languages. We begin with brief summaries of span-based constituency parsing and in-depth evaluations in section 2 before describing our methodology in section 3. We then present an overview of treebank statistics in section 4, followed by an analysis of known rules in section 5, and of novel rules in section 6, before concluding in section 7. 2 Previous Work 2.1 Span-Based Neural Constituency Parsing The shift from grammar-based parsing to datadriven parsing has first been implemented for transition-based parsing (Nivre, 2003) and was adapted for shift-reduce constituency parsing early on by Sagae and Lavie (2005). However, it has only recently been implemented in chart-based parsers (Stern et al., 2017; Gaddy et al., 2018). Traditional chart parsers based on PCFG grammars had one major limitation: They assumed a complete grammar, extracted from the training data. This worked well for languages with a low ratio of unique rules, where most of the rules were observed in the training data. However, for languages with annotation schemes preferring flat trees, this was not true. The first approach to address this limitation was (horizontal) Markovization (Collins, 1999; Klein and Manning, 2003), which allow"
2021.scil-1.29,W14-6111,1,0.799026,"Missing"
2021.scil-1.29,P17-1076,0,0.0827301,"ations in section 2 before describing our methodology in section 3. We then present an overview of treebank statistics in section 4, followed by an analysis of known rules in section 5, and of novel rules in section 6, before concluding in section 7. 2 Previous Work 2.1 Span-Based Neural Constituency Parsing The shift from grammar-based parsing to datadriven parsing has first been implemented for transition-based parsing (Nivre, 2003) and was adapted for shift-reduce constituency parsing early on by Sagae and Lavie (2005). However, it has only recently been implemented in chart-based parsers (Stern et al., 2017; Gaddy et al., 2018). Traditional chart parsers based on PCFG grammars had one major limitation: They assumed a complete grammar, extracted from the training data. This worked well for languages with a low ratio of unique rules, where most of the rules were observed in the training data. However, for languages with annotation schemes preferring flat trees, this was not true. The first approach to address this limitation was (horizontal) Markovization (Collins, 1999; Klein and Manning, 2003), which allowed the parser a strictly limited type of creating new rules. This pivotal advantages of cur"
D07-1096,D07-1119,0,0.635846,"Missing"
D07-1096,D07-1120,0,0.0179351,"Missing"
D07-1096,W06-1615,1,0.130888,"d annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for multiple languages is already a challenging problem, which is only exacerbated when these resources must be drawn from multiple and diverse domains. As a result, the only language that could be feasibly tested in the domain adaptation track was English. The setup for the domain adaptation track was as follows. Participants were prov"
D07-1096,W06-2920,0,0.770442,"d provide a first analysis of these results. 1 Introduction Previous shared tasks of the Conference on Computational Natural Language Learning (CoNLL) have been devoted to chunking (1999, 2000), clause identification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005). In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006). In dependency-based syntactic parsing, the task is to derive a syntactic structure for an input sentence by identifying the syntactic head of each word in the sentence. This defines a dependency graph, where In this year’s shared task, we continue to explore data-driven methods for multilingual dependency parsing, but we add a new dimension by also introducing the problem of domain adaptation. The way this was done was by having two separate tracks: a multilingual track using essentially the same setup as last year, but with partly different languages, and a domain adaptation track, where th"
D07-1096,D07-1121,0,0.0305771,"Missing"
D07-1096,D07-1101,0,0.829681,"Missing"
D07-1096,W01-0521,0,0.119247,"and Turkish. The treebanks from 2 The reason for having an upper bound on the training set size was the fact that, in 2006, some participants could not train on all the data for some languages because of time limitations. Similar considerations also led to the decision to have a smaller number of languages this year (ten, as opposed to thirteen). 917 which the data sets were extracted are described in section 3. 2.3 Domain Adaptation Track One well known characteristic of data-driven parsing systems is that they typically perform much worse on data that does not come from the training domain (Gildea, 2001). Due to the large overhead in annotating text with deep syntactic parse trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantia"
D07-1096,A00-2018,0,0.0790907,"test data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters. Participants in the multilingual track were expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One"
D07-1096,W04-3237,0,0.0190919,"dapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target doma"
D07-1096,D07-1097,1,0.748997,"Missing"
D07-1096,D07-1122,0,0.0269928,"Missing"
D07-1096,P97-1003,0,0.129198,"neralize to unseen test data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters. Participants in the multilingual track were expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz an"
D07-1096,D07-1112,0,0.583145,"Missing"
D07-1096,D07-1102,0,0.0310581,"Missing"
D07-1096,W07-2416,0,0.767751,"he test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turkish requires the basic tokens in parsing to be inflectional groups (IGs) rather than words. IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files). Sentences do not necessarily have a unique root; most internal punctuation and a few foreign word"
D07-1096,D07-1123,0,0.0858787,"he test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turkish requires the basic tokens in parsing to be inflectional groups (IGs) rather than words. IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files). Sentences do not necessarily have a unique root; most internal punctuation and a few foreign word"
D07-1096,W02-2016,0,0.338207,"expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors inv"
D07-1096,W04-3111,1,0.307142,"task. A new test set of about 9,000 tokens was provided by G¨uls¸en Eryi˘git (Eryi˘git, 2007), who also handled the conversion to the CoNLL format, which means that we could use 919 Domain Adaptation Track As mentioned previously, the source data is drawn from a corpus of news, specifically the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). This data set is identical to the English training set from the multilingual track (see section 3.1). For the target domains we used three different labeled data sets. The first two were annotated as part of the PennBioIE project (Kulick et al., 2004) and consist of sentences drawn from either biomedical or chemical research abstracts. Like the source WSJ corpus, this data is annotated using the Penn Treebank phrase structure scheme. To convert these sets to dependency structures we used the same procedure as before (Johansson and Nugues, 2007a). Additional care was taken to remove sentences that contained non-WSJ part-of-speech tags or non-terminals (e.g., HYPH part-of-speech tag indicating a hyphen). Furthermore, the annotation scheme for gaps and traces was made consistent with the Penn Treebank wherever possible. As already mentioned,"
D07-1096,D07-1098,0,0.0447748,"Missing"
D07-1096,D07-1124,0,0.0197253,"Missing"
D07-1096,J93-2004,0,0.0597121,"dependency annotation, just as for PADT. It was also used in the shared task 2006, but there are two important changes compared to last year. First, version 2.0 of PDT was used instead of version 1.0, and a conversion script was created by Zdenek Zabokrtsky, using the new XMLbased format of PDT 2.0. Secondly, due to the upper bound on training set size, only sections 1–3 of PDT constitute the training data, which amounts to some 450,000 tokens. The test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turk"
D07-1096,N04-1001,0,0.244784,"se trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated res"
D07-1096,D07-1125,0,0.020143,"Missing"
D07-1096,P06-1043,0,0.463102,"scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for multiple languages is already a challenging problem, which is only exacerbated when these resources must be drawn from multiple and diverse domains. As a result, the only language that could be feasibly tested in the domain adaptation track was English. The setup for the domain adaptation track was as follo"
D07-1096,N03-1027,0,0.115089,"text with deep syntactic parse trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter set"
D07-1096,D07-1013,1,0.545996,"Missing"
D07-1096,N06-2033,0,0.645238,"Missing"
D07-1096,E06-1011,1,0.349109,"the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an important problem for future research. In order to provide an extended empirical foundation"
D07-1096,D07-1111,0,0.734777,"Missing"
D07-1096,H05-1066,1,0.508532,"Missing"
D07-1096,D07-1126,0,0.0372423,"Missing"
D07-1096,W06-2934,1,0.278986,"Missing"
D07-1096,D07-1128,0,0.0204553,"Missing"
D07-1096,D07-1129,0,0.0286186,"Missing"
D07-1096,W06-2902,0,0.016467,"g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for"
D07-1096,D07-1099,0,0.322183,"Missing"
D07-1096,D07-1130,0,0.0262275,"Missing"
D07-1096,D07-1131,0,0.0212092,"Missing"
D07-1096,W03-3023,0,0.882665,"ults for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an i"
D07-1096,P06-1085,0,\N,Missing
D07-1096,D07-1127,0,\N,Missing
E09-1047,J94-4001,0,0.309468,"Missing"
E09-1047,P92-1003,0,0.519481,"Missing"
E09-1047,C02-1131,0,0.0357558,"Missing"
E09-1047,W08-1005,0,0.0522758,"). BitPar is an efficient implementation of an Earley style parser that uses bit vectors. However, BitPar cannot handle pre-bracketed input. For this reason, we used LoPar for the experiments where such input was required. LoPar, as it is used here, is a pure PCFG parser, which allows the input to be partially bracketed. We are aware that the results that can be obtained by pure PCFG parsers are not state of the art as reported in the shared task of the ACL 2008 Workshop on Parsing German (K¨ubler, 2008). While BitPar reaches an F-score of 69.76 (see next section), the best performing parser (Petrov and Klein, 2008) reaches an Fscore of 83.97 on T¨uBa-D/Z (but with a different split of training and test data). However, our experiments require certain features in the parsers, namely the capability to provide n-best analyses and to parse pre-bracketed input. To our knowledge, the parsers that took part in the shared task do not provide these features. Should they become available, the methods presented here could be applied to such parsers. We see no reason why our The Treebank The data source used for the experiments is the T¨ubingen Treebank of Written German (T¨uBaD/Z) (Telljohann et al., 2005). T¨uBa-D"
E09-1047,P05-1022,0,0.303414,"Missing"
E09-1047,C04-1024,0,0.14491,"Missing"
E09-1047,J05-1003,0,0.31486,"phenomena are particularly hard for pure PCFG parsing, due to the independence assumption inherent in the statistical models for PCFGs. Sentence (4) has the following Viterbi parse: methods should not be able to improve the results of these parsers further. Since we are interested in parsing coordinations, all experiments are conducted with gold POS tags, so as to abstract away from POS tagging errors. Although the treebank contains morphological information, this type of information is not used in the experiments presented here. The reranking experiments were conducted using the reranker by Collins and Koo (2005). This reranker uses a set of candidate parses for a sentence and reranks them based on a set of features that are extracted from the trees. The reranker uses a boosting method based on the approach by Freund et al. (1998). We used a similar feature set to the one Collins and Koo used; the following types of features were included: rules, bigrams, grandparent rules, grandparent bigrams, lexical bigrams, two-level rules, two-level bigrams, trigrams, head-modifiers, PPs, and distance for headmodifier relations, as well as all feature types involving rules extended by closed class lexicalization."
E09-1047,telljohann-etal-2004-tuba,1,0.905351,"Missing"
E09-1047,P07-1086,0,0.815425,"error analysis of his WSJ parsing results that coordination is one of the most frequent cases of incorrect parses, particularly if the conjuncts involved are complex. He manages to reduce errors for simple cases of NP coordination by introducing a special phrasal category of base NPs. In the experiments presented above, no explicit distinction is made between simple and complex cases of coordination, and no transformations are performed on the treebank annotations used for training. Our experiment 1, reranking 50-best parses, is similar to the approaches of Charniak and Johnson (2005) and of Hogan (2007). However, it differs from their experiments in two crucial ways: 1) Compared to Charniak and Johnson, who use 1.1 mio. features, our feature set is appr. five times larger (more than 5 mio. features), with the same threshold of at least five occurrences in the training set. 2) Both Hogan and Charniak and Johnson use special features for coordinate structures, such as a Boolean feature for marking parallelism (Charniak and Johnson) or for distinguishing between coordination of base NPs and coordination of complex conjuncts (Hogan), while our approach refrains from such special-purpose features"
E09-1047,J03-4003,0,\N,Missing
E09-1047,W08-1008,1,\N,Missing
E17-1033,W06-1615,0,0.219657,"ur work is comparable to domain adaptation since we create experts to tag and parse heterogeneous datasets. The work in this is area is largely driven by the unavailability of examples from the target domain. Our work focuses on creating experts using topic modeling which will be able to tag and parse target domain sentences belonging to a specific topic. Compared to POS tagging, there has been significant work on domain adaptation in dependency parsing. Dredze et al. (2007) found that problems in domain adaptation are compounded by differences in the annotation schemes between the treebanks. Blitzer et al. (2006) experimented with structural correspondence learning (SCL), which focuses on finding frequently occurring pivot features that occur commonly across domains in the unlabeled data but equally characterize source and target domains. Similar to our work, Blitzer et al. used the WSJ as the source and MEDLINE abstracts as the target domain. They established that SCL reaches better results in both POS tagging and parsing than supervised and semi-supervised learning even when there is no training data available in the target domain. For POS tagging, Clark et al. (2003) applied an agreement-based and"
E17-1033,C10-1011,0,0.0497441,"Missing"
E17-1033,W03-0407,0,0.24723,"chemes between the treebanks. Blitzer et al. (2006) experimented with structural correspondence learning (SCL), which focuses on finding frequently occurring pivot features that occur commonly across domains in the unlabeled data but equally characterize source and target domains. Similar to our work, Blitzer et al. used the WSJ as the source and MEDLINE abstracts as the target domain. They established that SCL reaches better results in both POS tagging and parsing than supervised and semi-supervised learning even when there is no training data available in the target domain. For POS tagging, Clark et al. (2003) applied an agreement-based and a baseline co-training method by using a Markov model tagger and a maximum entropy tagger. In case of the baseline, all the sentences from one tagger are added to train the other whereas in the agreement-based method, both taggers have to reach to the same decision for a sentence to be added to the training. K¨ubler and Baucom (2011) used a similar concept but with three different taggers and showed that selecting sentences as well as sequences of words for which all taggers agree yield the highest gains. Sagae and Tsujii (2007) emulate a single iteration of cot"
E17-1033,N09-1068,0,0.0394499,"Missing"
E17-1033,W07-2416,0,0.0153129,"L 2007 shared task on domain adaptation for dependency parsing, Attardi et al. (2007) used a tree revision method that corrects the mistakes caused by the base parser for the target domain. Later, Kawahara and Uchimoto (2008) employed a single parser approach using a second order MST parser and combining labeled 349 from Medline, and it is annotated on different linguistic levels, including POS tags, syntax, coreference, and events, among others. We use GENIA 1.0 trees (Ohta et al., 2002) created in the Penn Treebank format1 . Both treebanks were converted to dependencies using pennconverter (Johansson and Nugues, 2007). For our experiments, we need a balanced data set, both for the training and the test set. Since GENIA is rather small and since there is no standard data split for GENIA, we decided to extract the last 850 sentences for the test set. The remaining 17 181 sentences are used for training. For WSJ, we chose the same number of sentences for both training and the test set, the training sentences are selected randomly from sections 02-21 and the test sentences from section 22. 4.2 determine for which expert(s) the sentence is relevant. There are several ways of determining the best expert, see bel"
E17-1033,D07-1119,0,0.118133,"ent is used as features for their similarity metrics. parsing experts to that of the previous work done in the area. However, our work is comparable to domain adaptation since we create experts to tag and parse heterogeneous datasets. The work in this is area is largely driven by the unavailability of examples from the target domain. Our work focuses on creating experts using topic modeling which will be able to tag and parse target domain sentences belonging to a specific topic. Compared to POS tagging, there has been significant work on domain adaptation in dependency parsing. Dredze et al. (2007) found that problems in domain adaptation are compounded by differences in the annotation schemes between the treebanks. Blitzer et al. (2006) experimented with structural correspondence learning (SCL), which focuses on finding frequently occurring pivot features that occur commonly across domains in the unlabeled data but equally characterize source and target domains. Similar to our work, Blitzer et al. used the WSJ as the source and MEDLINE abstracts as the target domain. They established that SCL reaches better results in both POS tagging and parsing than supervised and semi-supervised lea"
E17-1033,I08-2097,0,0.425467,"nd Tsujii, 2004). Both corpora use the Penn Treebank POS tagset (Santorini, 1990) with minor differences: The tagset used in GENIA is based on the Penn Treebank tagset, but it uses the tags for proper names and symbols only in very restricted contexts. For the WSJ corpus, we extract the POS annotation from the syntactically annotated corpus. The GENIA Corpus comprises biomedical abstracts In the CoNLL 2007 shared task on domain adaptation for dependency parsing, Attardi et al. (2007) used a tree revision method that corrects the mistakes caused by the base parser for the target domain. Later, Kawahara and Uchimoto (2008) employed a single parser approach using a second order MST parser and combining labeled 349 from Medline, and it is annotated on different linguistic levels, including POS tags, syntax, coreference, and events, among others. We use GENIA 1.0 trees (Ohta et al., 2002) created in the Penn Treebank format1 . Both treebanks were converted to dependencies using pennconverter (Johansson and Nugues, 2007). For our experiments, we need a balanced data set, both for the training and the test set. Since GENIA is rather small and since there is no standard data split for GENIA, we decided to extract the"
E17-1033,tateisi-tsujii-2004-part,0,0.839695,"nal Linguistics tion. However, this advantage will be offset by a smaller training set since we split into more sets. expert would learn specific information about its own genre. We determine these experts by performing topic modeling on sentences and then train an expert on the sentences of the topic. We group sentences based on their most probable topic. To test our hypothesis that topic modeling can serve to group sentences into topics, we create a mixed dataset from the financial domain (using the Penn Treebank (Marcus et al., 1994)) and from the biomedical domain (using the GENIA Corpus (Tateisi and Tsujii, 2004)) such that the new handcrafted corpus consists of sentences from both domains in equal measure. Consequently, there is a clear difference in the genres in our corpus, and we have gold standard topic information. We perform topic modeling on training and test data simultaneously: We assign a test sentence to the topic with the highest probability. This means that we currently simplify the problem of assigning new sentences to topics. In the future, we plan to assign new sentences to topics based their similarity to sentences in the topics created during training, following the work by Plank an"
E17-1033,R11-1006,1,0.9344,"Missing"
E17-1033,H94-1020,0,0.321254,"s 347–355, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics tion. However, this advantage will be offset by a smaller training set since we split into more sets. expert would learn specific information about its own genre. We determine these experts by performing topic modeling on sentences and then train an expert on the sentences of the topic. We group sentences based on their most probable topic. To test our hypothesis that topic modeling can serve to group sentences into topics, we create a mixed dataset from the financial domain (using the Penn Treebank (Marcus et al., 1994)) and from the biomedical domain (using the GENIA Corpus (Tateisi and Tsujii, 2004)) such that the new handcrafted corpus consists of sentences from both domains in equal measure. Consequently, there is a clear difference in the genres in our corpus, and we have gold standard topic information. We perform topic modeling on training and test data simultaneously: We assign a test sentence to the topic with the highest probability. This means that we currently simplify the problem of assigning new sentences to topics. In the future, we plan to assign new sentences to topics based their similarity"
E17-1033,W16-6316,1,0.615758,"Missing"
E17-1033,P11-1157,0,0.580525,"Missing"
E17-1033,D07-1111,0,0.163435,"n the target domain. For POS tagging, Clark et al. (2003) applied an agreement-based and a baseline co-training method by using a Markov model tagger and a maximum entropy tagger. In case of the baseline, all the sentences from one tagger are added to train the other whereas in the agreement-based method, both taggers have to reach to the same decision for a sentence to be added to the training. K¨ubler and Baucom (2011) used a similar concept but with three different taggers and showed that selecting sentences as well as sequences of words for which all taggers agree yield the highest gains. Sagae and Tsujii (2007) emulate a single iteration of cotraining by using MaxEnt and SVM, selecting the sentences where both models agreed and adding these sentences to the training set. Their approach reached the highest results on the domain adaptation task of CoNLL 2007 (Nivre et al., 2007). 4 4.1 Experimental Setup Data Sets For our experiments, we use the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1994) and the GENIA Corpus (Tateisi and Tsujii, 2004). Both corpora use the Penn Treebank POS tagset (Santorini, 1990) with minor differences: The tagset used in GENIA is based on the Penn"
E17-1033,A00-1031,0,\N,Missing
E17-1033,D07-1096,1,\N,Missing
E17-1033,N10-1004,0,\N,Missing
eberhard-etal-2010-indiana,N07-1051,0,\N,Missing
eberhard-etal-2010-indiana,J93-2004,0,\N,Missing
eberhard-etal-2010-indiana,J97-1002,0,\N,Missing
eberhard-etal-2010-indiana,W07-2416,0,\N,Missing
gilmanov-etal-2014-swift,ahrenberg-etal-2002-system,0,\N,Missing
gilmanov-etal-2014-swift,C00-2163,0,\N,Missing
gilmanov-etal-2014-swift,smith-jahr-2000-cairo,0,\N,Missing
gilmanov-etal-2014-swift,E03-1086,0,\N,Missing
gilmanov-etal-2014-swift,N01-1026,0,\N,Missing
gilmanov-etal-2014-swift,P08-4006,0,\N,Missing
gilmanov-etal-2014-swift,W05-0818,0,\N,Missing
gilmanov-etal-2014-swift,J97-3002,0,\N,Missing
gilmanov-etal-2014-swift,N06-1014,0,\N,Missing
H01-1072,C00-1011,0,0.164592,"of each level serving as input for the next higher level. The first level is part-of-speech (POS) tagging of the input string with the help of the bigram tagger LIKELY [10].1 The parts of speech serve as pre-terminal elements for the next step, i.e. the chunk analysis. Chunk parsing is carried out by an adapted version of Abney’s [2] scol parser, which is realized as a cascade of finite-state transducers. The chunks, which extend if possible to the simplex clause level, are then remodeled into complete trees in the tree construction level. The tree construction is similar to the DOP approach [3, 4] in that it uses complete tree structures instead of rules. Contrary to Bod, we do not make use of probabilities and do not allow tree cuts, instead we only use the complete trees and minimal tree modifications. Thus the number of possible combinations of partial trees is strictly controlled. The resulting parser is highly efficient (3770 English sentences took 106.5 seconds to parse on an Ultra Sparc 10). 3. CHUNK PARSING AND TREE CONSTRUCTION The division of labor between the chunking and tree construction modules can best be illustrated by an example. . 1 The inventory of POS tags is based"
H01-1072,W98-1207,0,0.23362,"mmatical function or constituents with several possible functions may be found so that an additional classifier is needed for selecting the most appropriate assignment (cf. [6]). The second approach, which we have chosen, is to regard the complete parse trees as classes so that the task is defined as the selection of the most similar tree from the instance base. Since in 2 All trees in this contribution follow the data format for trees defined by the NEGRA project of the Sonderforschungsbereich 378 at the University of the Saarland, Saarbr¨ucken. They were printed by the NEGRA annotation tool [5]. 3 Memory-based learning has recently been applied to a variety of NLP classification tasks, including part-of-speech tagging, noun phrase chunking, grapheme-phoneme conversion, word sense disambiguation, and pp attachment (see [9], [14], [15] for details). construct tree(chunk list, treebank): while (chunk list is not empty) do remove first chunk from chunk list process chunk(chunk, treebank) Figure 3: Pseudo-code for tree construction, main routine. process chunk(chunk, treebank): words := string yield(chunk) tree := complete match(words, treebank) if (tree is not empty) then output(tree) e"
H01-1072,W99-0629,0,0.0323722,"ed as a classification task. There are two fundamentally different, possible approaches: the one is to split parsing up into different subtasks, that is, one needs separate classifiers for each functional category and for each level in a recursive structure. Since the classifiers for the functional categories as well as the individual decisions of the classifiers are independent, multiple or no candidates for a specific grammatical function or constituents with several possible functions may be found so that an additional classifier is needed for selecting the most appropriate assignment (cf. [6]). The second approach, which we have chosen, is to regard the complete parse trees as classes so that the task is defined as the selection of the most similar tree from the instance base. Since in 2 All trees in this contribution follow the data format for trees defined by the NEGRA project of the Sonderforschungsbereich 378 at the University of the Saarland, Saarbr¨ucken. They were printed by the NEGRA annotation tool [5]. 3 Memory-based learning has recently been applied to a variety of NLP classification tasks, including part-of-speech tagging, noun phrase chunking, grapheme-phoneme conver"
H01-1072,P79-1022,0,0.0504144,"rd the complete parse trees as classes so that the task is defined as the selection of the most similar tree from the instance base. Since in 2 All trees in this contribution follow the data format for trees defined by the NEGRA project of the Sonderforschungsbereich 378 at the University of the Saarland, Saarbr¨ucken. They were printed by the NEGRA annotation tool [5]. 3 Memory-based learning has recently been applied to a variety of NLP classification tasks, including part-of-speech tagging, noun phrase chunking, grapheme-phoneme conversion, word sense disambiguation, and pp attachment (see [9], [14], [15] for details). construct tree(chunk list, treebank): while (chunk list is not empty) do remove first chunk from chunk list process chunk(chunk, treebank) Figure 3: Pseudo-code for tree construction, main routine. process chunk(chunk, treebank): words := string yield(chunk) tree := complete match(words, treebank) if (tree is not empty) then output(tree) else tree := partial match(words, treebank) if (tree is not empty) then if (tree = postfix of chunk) then tree1 := attach next chunk(tree, treebank) if (tree is not empty) then tree := tree1 if ((chunk - tree) is not empty) then tree"
H01-1072,W97-1016,0,0.238208,"lete parse trees as classes so that the task is defined as the selection of the most similar tree from the instance base. Since in 2 All trees in this contribution follow the data format for trees defined by the NEGRA project of the Sonderforschungsbereich 378 at the University of the Saarland, Saarbr¨ucken. They were printed by the NEGRA annotation tool [5]. 3 Memory-based learning has recently been applied to a variety of NLP classification tasks, including part-of-speech tagging, noun phrase chunking, grapheme-phoneme conversion, word sense disambiguation, and pp attachment (see [9], [14], [15] for details). construct tree(chunk list, treebank): while (chunk list is not empty) do remove first chunk from chunk list process chunk(chunk, treebank) Figure 3: Pseudo-code for tree construction, main routine. process chunk(chunk, treebank): words := string yield(chunk) tree := complete match(words, treebank) if (tree is not empty) then output(tree) else tree := partial match(words, treebank) if (tree is not empty) then if (tree = postfix of chunk) then tree1 := attach next chunk(tree, treebank) if (tree is not empty) then tree := tree1 if ((chunk - tree) is not empty) then tree := extend t"
H01-1072,J03-4003,0,\N,Missing
hinrichs-etal-2002-hybrid,borin-2000-something,0,\N,Missing
hinrichs-etal-2002-hybrid,W96-0102,0,\N,Missing
hinrichs-etal-2002-hybrid,W99-0707,0,\N,Missing
hinrichs-etal-2002-hybrid,H01-1072,1,\N,Missing
hinrichs-etal-2002-hybrid,W98-1207,0,\N,Missing
hinrichs-etal-2002-hybrid,E93-1007,0,\N,Missing
hinrichs-etal-2002-hybrid,A97-1012,0,\N,Missing
hinrichs-etal-2002-hybrid,A00-1033,0,\N,Missing
hinrichs-etal-2002-hybrid,C00-1046,0,\N,Missing
hinrichs-etal-2002-hybrid,P01-1045,1,\N,Missing
hinrichs-etal-2002-hybrid,grover-etal-2000-lt,0,\N,Missing
hinrichs-etal-2002-hybrid,P98-1081,0,\N,Missing
hinrichs-etal-2002-hybrid,C98-1078,0,\N,Missing
J06-4007,daelemans-hoste-2002-evaluation,0,0.0261408,"t from the training data. RIPPER (Cohen 1995), the other classifier used in this chapter, is a typical eager learning approach: It is a rule-induction algorithm, which displays the opposite behavior to TiMBL: a complex learning strategy and simple, efficient classification. The results presented in this chapter show that deleting examples from the training data is harmful for classification, supporting the hypothesis that lazy learning has a fitting bias for natural language problems. However, this seems to be a little too straightforward. Here, one would expect a reference to the findings of Daelemans and Hoste (2002), which show that parameter and feature 560 Book Reviews optimization often result in larger differences in accuracy than differences between learning algorithms. Chapter 7 returns to more practical problems: parameter optimization and more advanced types of classifier combinations to handle sequence learning. Parameter optimization is one of the more tedious aspects of the work in machine learning. Researchers often spend a significant amount of time on finding the optimal parameter settings for a specific algorithm and problem. Wrapped progressive sampling is one method to automate this proc"
J06-4007,W95-0107,0,0.0153393,"time during classification, but they may also affect classification accuracy. Unfortunately, the presentation of the first example suffers from unreadable phonetic transcriptions throughout the chapter. Whereas Chapter 4 analyzes linguistic problems, which are easily described in terms of classification, chapter 5 approaches a problem of sequence learning: partial parsing. For this task, phrase and clause boundaries must be found. In order to apply classification methods to sequence learning, the problem must be redefined as assigning tags to words or word combinations, so-called IOB tagging (Ramshaw and Marcus 1995). This tagging provides information as to whether a word constitutes a boundary or not. One advantage of using MBLP for such problems lies in the fact that different types of information, including long-distance information, can be included without modification of the original algorithm. In Chapter 6, Daelemans and van den Bosch investigate the difference between lazy and eager learning. As noted earlier, TiMBL is a typical example of lazy learning since it does not abstract from the training data. RIPPER (Cohen 1995), the other classifier used in this chapter, is a typical eager learning appr"
J13-1003,P05-1038,0,0.0571577,"Missing"
J13-1003,P08-1067,0,0.0549641,"Missing"
J13-1003,P03-1054,0,0.0158058,"odern Standard Arabic (Semitic) and French (Romance), the last article of this special issue, by Green et al., may be seen as an applications paper, treating the task of MWE recognition as a side effect of a joint model for parsing and MWE identiﬁcation. The key problem here is knowing what to consider a minimal unit for parsing, and how to handle parsing in realistic scenarios where MWEs have not yet been identiﬁed. The authors present two parsing models for such a task: a factored model including a factored lexicon that integrates morphological knowledge into the Stanford Parser word model (Klein and Manning 2003), and a Dirichlet Process Tree Substitution Grammar based model (Cohn, Blunsom, and Goldwater 2010). The latter can be roughly described as Data Oriented Parsing (Bod 1992; Bod, Scha, and Sima’an 2003) in a Bayesian framework, extended to include speciﬁc features that ease the extraction of tree fragments matching MWEs. Interestingly, those very different models do provide the same range of performance when confronted with predicted morphology input. Additional important challenges that are exposed in the context of this study concern the design of experiments for cross-linguistic comparison i"
J13-1003,P95-1037,0,0.302286,"Missing"
J13-1003,J93-2004,0,0.0463151,"Missing"
J13-1003,P06-1055,0,0.078628,"een, de Marneffe, and Manning 2013 Kallmeyer and Maier 2013 Fraser et al. 2013 Goldberg and Elhadad 2013 Seeker and Kuhn 2013 Seeker and Kuhn 2013 Tsarfaty et al. Parsing Morphologically Rich Languages: predicate-argument constraints allows the authors to obtain more substantial gains from morphology. Fraser et al. also focus on parsing German, though in a constituency-based setting. They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set of manual treebank annotations that bring the treebank grammar performance to the level of automatically predicted states learned by Petrov et al. (2006). As in the previous study, syncretism is shown to cause ambiguity that hurts parsing performance. To combat this added ambiguity, they use external information sources. In particular, they show different ways of using information from monolingual and bilingual data sets in a re-ranking framework for improving parsing accuracy. The bilingual approach is inspired by machine translation studies and exploits the variation in marking the same grammatical functions differently across languages for increasing the conﬁdence of a disambiguation decision in one language by observing a parallel non-ambi"
J13-1003,C04-1024,0,0.0319019,"French German Hebrew Hungarian 18 Constituency-Based Dependency-Based Green, de Marneffe, and Manning 2013 Marton, Habash, and Rambow 2013 Seeker and Kuhn 2013 Green, de Marneffe, and Manning 2013 Kallmeyer and Maier 2013 Fraser et al. 2013 Goldberg and Elhadad 2013 Seeker and Kuhn 2013 Seeker and Kuhn 2013 Tsarfaty et al. Parsing Morphologically Rich Languages: predicate-argument constraints allows the authors to obtain more substantial gains from morphology. Fraser et al. also focus on parsing German, though in a constituency-based setting. They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set of manual treebank annotations that bring the treebank grammar performance to the level of automatically predicted states learned by Petrov et al. (2006). As in the previous study, syncretism is shown to cause ambiguity that hurts parsing performance. To combat this added ambiguity, they use external information sources. In particular, they show different ways of using information from monolingual and bilingual data sets in a re-ranking framework for improving parsing accuracy. The bilingual approach is inspired by machine translation studies and exploits the variation in mar"
J13-1003,A97-1014,0,0.288535,"Missing"
J13-1003,W10-1401,1,0.919998,"Missing"
J13-1003,W07-2219,1,0.883448,"Missing"
J13-1003,A00-2018,0,\N,Missing
J13-1003,C10-1011,0,\N,Missing
J13-1003,P97-1003,0,\N,Missing
J13-1003,W06-2920,0,\N,Missing
J13-1003,W08-2102,0,\N,Missing
J13-1003,P05-1022,0,\N,Missing
J13-1003,P08-1109,0,\N,Missing
J13-1003,C92-3126,0,\N,Missing
J13-1003,P99-1065,0,\N,Missing
J13-1003,P03-1013,0,\N,Missing
J13-1003,D07-1096,1,\N,Missing
J13-1003,N10-1115,0,\N,Missing
K17-2001,E14-1060,1,0.855884,"Missing"
K17-2001,P17-1136,0,0.0562959,"Missing"
K17-2001,N15-1107,1,0.861491,"Missing"
K17-2001,chrupala-etal-2008-learning,0,0.152497,"Missing"
K17-2001,W16-2004,0,0.0654088,"Missing"
K17-2001,P14-2102,1,0.910119,"Missing"
K17-2001,Q15-1031,1,0.919531,"Missing"
K17-2001,P16-1156,1,0.87299,"Missing"
K17-2001,K17-2002,0,0.132442,"Missing"
K17-2001,E17-2120,1,0.859022,"Missing"
K17-2001,E17-1049,1,0.894341,"Missing"
K17-2001,N07-1048,0,0.21796,"Missing"
K17-2001,P17-1182,1,0.876831,"Missing"
K17-2001,D09-1011,1,0.888474,"Missing"
K17-2001,D08-1113,1,0.874358,"Missing"
K17-2001,P16-2090,0,0.46072,"Missing"
K17-2001,N13-1138,0,0.147728,"Missing"
K17-2001,P08-1115,0,0.089787,"Missing"
K17-2001,L16-1498,1,0.792213,"Missing"
K17-2001,N16-1077,1,0.812854,"Missing"
K17-2001,W10-2211,0,0.123726,"Missing"
K17-2001,W16-2006,0,0.0673676,"Missing"
K17-2001,P82-1020,0,0.75423,"Missing"
K17-2001,P08-1103,0,0.0541683,"Missing"
K17-2001,D15-1272,1,0.92636,"Missing"
K17-2001,D14-1095,0,0.0938069,"Missing"
K17-2001,K17-2011,0,0.0350181,"Missing"
K17-2001,N15-1093,0,0.19098,"Missing"
K17-2001,K17-2008,0,0.055504,"Missing"
K17-2001,K17-2010,0,0.158704,"Missing"
K17-2001,W16-2007,0,\N,Missing
K17-2001,L16-1497,1,\N,Missing
K17-2001,K17-2012,0,\N,Missing
K17-2001,K17-2003,0,\N,Missing
K17-2001,P17-1029,0,\N,Missing
kubler-etal-2008-compare,daum-etal-2004-automatic,0,\N,Missing
kubler-etal-2008-compare,brants-hansen-2002-developments,0,\N,Missing
kubler-etal-2008-compare,A97-1014,0,\N,Missing
kubler-etal-2008-compare,C04-1024,0,\N,Missing
kubler-etal-2008-compare,W06-1614,1,\N,Missing
kubler-etal-2008-compare,W07-1506,0,\N,Missing
kubler-etal-2008-compare,P03-1054,0,\N,Missing
kubler-etal-2008-compare,D07-1066,1,\N,Missing
kubler-etal-2008-compare,P06-3004,1,\N,Missing
kubler-etal-2008-compare,C04-1056,0,\N,Missing
kubler-etal-2008-compare,emms-2008-tree,0,\N,Missing
L18-1293,E14-1060,1,0.914644,"Missing"
L18-1293,P16-1156,1,0.911216,"Missing"
L18-1293,N07-1048,0,0.136877,"Missing"
L18-1293,N13-1138,0,0.235978,"Missing"
L18-1293,P08-1115,0,0.0456449,"Missing"
L18-1293,N16-1077,1,0.880807,"Missing"
L18-1293,L16-1498,1,0.945257,"The Universal Morphology (UniMorph) project, centered at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University is a collaborative effort to improve how NLP systems handle complex morphology across the world’s languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. Kirov et al. (2016) introduced version 1.0 of the UniMorph morphological database, created by extracting and normalizing the inflectional paradigms included in Wiktionary (www.wiktionary.org), a large, broadly multi-lingual crowd-sourced collection of lexical data. This paper describes UniMorph 2.0. It details improvements in Wiktionary extraction and annotation, as well as normalization of non-Wiktionary resources, leading to a much higher quality morphological database. The new dataset spans 52 languages representing a range of language families. As in UniMorph 1.0, we provide paradigms from highlyinflected op"
L18-1293,D14-1095,0,0.116143,"Missing"
L18-1293,N15-1093,0,0.150689,"Missing"
L18-1293,Q15-1026,0,0.0836395,"Missing"
L18-1293,P15-2111,1,0.852807,"rsing and normalization of Wiktionary. Wiktionary is a broadly multilingual resource with many crowd-sourced morphological paradigms in the form of custom HTML tables. Figure 1 illustrates the challenge associated with extracting this data. Wiktionary is designed for human, rather than machine readability, and authors have extensive freedom in formatting data. This leads to wildly differing table layouts across languages which need to be converted to a consistent tabular format. The extraction process developed for UniMorph 1.0 relied heavily on statistical, visual, and positional heuristics (Sylak-Glassman et al., 2015b) to: 1. Determine which entries in an HTML table are inflected forms and which are grammatical descriptors. 2. Link each inflected form with its appropriate descriptors. 3. Convert each set of linked descriptors into a universal feature annotation schema, described in detail in Sylak-Glassman (2016).1 This led to a large dataset of 952,530 unique noun, verb, and adjective lemmas across 350 languages. Unfortunately, 1 pdf unimorph.github.io/doc/unimorph-schema. Figure 1: Paradigm extraction and normalization. the UniMorph 1.0 dataset was very error-prone due to the inability of our heuristics"
L18-1293,zeman-2008-reusable,0,0.0233658,"Each group represents a different type of paradigm (e.g., regular verb). For each group, a sample table was selected, and an annotator replaced each inflected form in the table with the appropriate UniMorph features. All annotation was compliant with the UniMorph Schema, which was designed to represent the full range of semantic distinctions that can be captured by inflectional morphology in any language (SylakGlassman et al., 2015a). The schema is similar in form and spirit to other tagset universalization efforts, such as the Universal Dependencies Project (Choi et al., 2015) and Interset (Zeman, 2008), but is designed specifically for typological completeness for inflectional morphology, including a focus on the morphology of especially low-resource languages. It includes over 200 base features distributed among 23 dimensions of meaning (i.e., morphological categories), including both common dimensions like tense and aspect as well as rarer dimensions like evidentiality and switch-reference. Despite the high coverage of the UniMorph tagset, for UniMorph 2.0, annotators were allowed to employ additional ‘language specific’ LGSPEC(1, 2, 3, etc.) features to mark any missing distinctions, or"
L18-1293,W16-2002,1,\N,Missing
maier-etal-2014-discosuite,D12-1133,0,\N,Missing
maier-etal-2014-discosuite,E12-1006,0,\N,Missing
maier-etal-2014-discosuite,J93-2004,0,\N,Missing
maier-etal-2014-discosuite,E06-1011,0,\N,Missing
maier-etal-2014-discosuite,W03-2404,0,\N,Missing
maier-etal-2014-discosuite,C10-1061,1,\N,Missing
maier-etal-2014-discosuite,C96-2120,0,\N,Missing
maier-etal-2014-discosuite,P11-2037,0,\N,Missing
maier-etal-2014-discosuite,H05-1066,0,\N,Missing
maier-etal-2014-discosuite,E14-1039,0,\N,Missing
maier-etal-2014-discosuite,seeker-kuhn-2012-making,0,\N,Missing
maier-etal-2014-discosuite,E12-1047,0,\N,Missing
maier-etal-2014-discosuite,emms-2008-tree,0,\N,Missing
maier-etal-2014-discosuite,Y95-1009,0,\N,Missing
mohamed-kubler-2010-arabic,W96-0102,0,\N,Missing
mohamed-kubler-2010-arabic,H94-1049,0,\N,Missing
mohamed-kubler-2010-arabic,N04-4038,0,\N,Missing
mohamed-kubler-2010-arabic,P05-1071,0,\N,Missing
mukherjee-kubler-2017-similarity,C10-1011,0,\N,Missing
mukherjee-kubler-2017-similarity,W03-0407,0,\N,Missing
mukherjee-kubler-2017-similarity,H94-1020,0,\N,Missing
mukherjee-kubler-2017-similarity,D07-1111,0,\N,Missing
mukherjee-kubler-2017-similarity,P11-1157,0,\N,Missing
mukherjee-kubler-2017-similarity,R11-1006,1,\N,Missing
mukherjee-kubler-2017-similarity,tateisi-tsujii-2004-part,0,\N,Missing
mukherjee-kubler-2017-similarity,I08-2097,0,\N,Missing
mukherjee-kubler-2017-similarity,E17-1033,1,\N,Missing
mukherjee-kubler-2017-similarity,N10-1004,0,\N,Missing
mukherjee-kubler-2017-similarity,W16-6316,1,\N,Missing
N10-1105,W96-0102,0,0.227426,"ond experiment uses an automatic segmenter as a pre-processing component to the POS tagger. This means that the accuracy of the segmenter is also the upper limit of the POS tagger since errors in segmentation inevitably lead to errors in POS tagging. The last experiment uses full words and complex POS tags. The purpose of this experiment is to determine whether it is possible to tag complete words without segmentation. The segmenter and the two POS taggers use memory-based learning. For segmentation, we use TiMBL (Daelemans and van den Bosch, 2005); for POS tagging MBT, a memory-based tagger (Daelemans et al., 1996). Memory-based learning is a lazy learning paradigm that does not abstract over the training data. During classification, the k nearest neighbors to a new example are retrieved from the training data, and the class that was assigned to the majority of the neighbors is assigned to the new example. MBT uses TiMBL as classifier; it offers the possibility to use words from both sides of the focus word as well as previous tagging decisions and ambitags as features. An ambitag is a combination of all POS tags of the ambiguity class of the word. Word segmentation is defined as a per-letter classifica"
N10-1105,N04-4038,0,0.604324,"Missing"
N10-1105,P05-1071,0,0.355215,"Missing"
N10-1105,mohamed-kubler-2010-arabic,1,0.285926,"Missing"
P01-1045,W97-1016,0,\N,Missing
P01-1045,W97-0307,0,\N,Missing
P01-1045,H01-1072,1,\N,Missing
P01-1045,C94-1061,0,\N,Missing
P01-1045,J03-4003,0,\N,Missing
P01-1045,H94-1020,0,\N,Missing
P01-1045,C00-1011,0,\N,Missing
P01-1045,lesmo-lombardo-2000-automatic,0,\N,Missing
P01-1045,A97-1011,0,\N,Missing
R09-1037,W04-0831,0,0.0311982,"lgaria, pages 197–202 the manually prepared one. Another interesting remark is the fact that employing web data reaches better results than unsupervised approaches but the final system performance is still far lower than the performance of fully supervised systems. The development of fully supervised WSD systems, however, has been given considerably more attention than the automatic acquisition of sense-tagged corpora. This is proven by approaches as the ones presented by Mihalcea et al. in the overview of the SensEval-3 English lexical sample competition [13] na¨ıve Bayes systems (e.g. htsa3 [6]), systems based on kernel methods (e.g. IRTS-Kernels [19]; nusels [8]), based on the Lesk algorithm [9] (e.g. wsdiit [18]), maximum entropy models (e.g. Cymfony [15]) and many others (e.g. Prob0 [17] and clr04-ls [10]). Those systems constitute the set of best performing systems from the SensEval-3 evaluation exercise. 3 The system for WSD In order to examine evidence for our hypothesis that only automatically annotated examples of high quality can be used successfully, we designed a memory-based learning system, which we used to train multiple wordexperts/classifiers, first on the SensEval t"
R09-1037,W04-0834,0,0.119947,"emark is the fact that employing web data reaches better results than unsupervised approaches but the final system performance is still far lower than the performance of fully supervised systems. The development of fully supervised WSD systems, however, has been given considerably more attention than the automatic acquisition of sense-tagged corpora. This is proven by approaches as the ones presented by Mihalcea et al. in the overview of the SensEval-3 English lexical sample competition [13] na¨ıve Bayes systems (e.g. htsa3 [6]), systems based on kernel methods (e.g. IRTS-Kernels [19]; nusels [8]), based on the Lesk algorithm [9] (e.g. wsdiit [18]), maximum entropy models (e.g. Cymfony [15]) and many others (e.g. Prob0 [17] and clr04-ls [10]). Those systems constitute the set of best performing systems from the SensEval-3 evaluation exercise. 3 The system for WSD In order to examine evidence for our hypothesis that only automatically annotated examples of high quality can be used successfully, we designed a memory-based learning system, which we used to train multiple wordexperts/classifiers, first on the SensEval training data, and in a second step with the automatically acquired dat"
R09-1037,W04-0803,0,0.0185072,"han the performance of fully supervised systems. The development of fully supervised WSD systems, however, has been given considerably more attention than the automatic acquisition of sense-tagged corpora. This is proven by approaches as the ones presented by Mihalcea et al. in the overview of the SensEval-3 English lexical sample competition [13] na¨ıve Bayes systems (e.g. htsa3 [6]), systems based on kernel methods (e.g. IRTS-Kernels [19]; nusels [8]), based on the Lesk algorithm [9] (e.g. wsdiit [18]), maximum entropy models (e.g. Cymfony [15]) and many others (e.g. Prob0 [17] and clr04-ls [10]). Those systems constitute the set of best performing systems from the SensEval-3 evaluation exercise. 3 The system for WSD In order to examine evidence for our hypothesis that only automatically annotated examples of high quality can be used successfully, we designed a memory-based learning system, which we used to train multiple wordexperts/classifiers, first on the SensEval training data, and in a second step with the automatically acquired data. The remainder of the section is structured as follows: In section 3.1, we will describe the supervised baseline system, section 3.2 describes the"
R09-1037,C02-1039,0,0.0280637,"CP0 CP1 CP2 CP3 NA NB VA VB PA PB TP -3 from TW TP -2 from TW TP -1 from TW TW TP 1 from TW TP 2 from TW TP 3 from TW POS of TP -3 from TW POS of TP -2 from TW POS of TP -1 from TW POS of target word POS of TP 1 from TW POS of TP 2 from TW POS of TP 3 from TW first noun after TW first noun before TW first verb after TW first verb before TW first preposition after TW first preposition before TW Table 1: Featured used for the word-experts (TP x is the token at position x and TW is the target word) 3.2 System Optimization Since memory-based learning is sensitive to irrelevant features (cf. e.g. [11]), it is important to optimize features for each word-expert. Following Mihalcea [11] and Dinu and K¨ ubler [3], we used forward and backward feature selection. This resulted in a considerable reduction in the number of features actually used for individual words as well as in a considerable improvement in accuracy (see section 4.1 for details). We also performed a non-exhaustive optimization of system parameters. The most straightforward parameters that can be optimized for a k -NN approach are the distance metric and the values for k representing the k nearest neighbors used for classifying"
R09-1037,W04-2405,0,0.192878,"Missing"
R09-1037,W04-0807,0,0.629295,"l Conference RANLP 2009 - Borovets, Bulgaria, pages 197–202 the manually prepared one. Another interesting remark is the fact that employing web data reaches better results than unsupervised approaches but the final system performance is still far lower than the performance of fully supervised systems. The development of fully supervised WSD systems, however, has been given considerably more attention than the automatic acquisition of sense-tagged corpora. This is proven by approaches as the ones presented by Mihalcea et al. in the overview of the SensEval-3 English lexical sample competition [13] na¨ıve Bayes systems (e.g. htsa3 [6]), systems based on kernel methods (e.g. IRTS-Kernels [19]; nusels [8]), based on the Lesk algorithm [9] (e.g. wsdiit [18]), maximum entropy models (e.g. Cymfony [15]) and many others (e.g. Prob0 [17] and clr04-ls [10]). Those systems constitute the set of best performing systems from the SensEval-3 evaluation exercise. 3 The system for WSD In order to examine evidence for our hypothesis that only automatically annotated examples of high quality can be used successfully, we designed a memory-based learning system, which we used to train multiple wordexperts"
R09-1037,P96-1006,0,0.0607547,"ction 3.1, we will describe the supervised baseline system, section 3.2 describes the feature and parameter optimization, section 3.3 the data sets that were collected, and section 3.4 the preprocessing steps for the new data sets. 3.1 Table 1. As shown in section 4.1, these features also give competitive results for English when compared to systems participating in SensEval-3. The supervised baseline system The supervised system uses memory-based learning, in the implementation of the Tilburg MemoryBased Learner (TiMBL) [2]. Memory-based learning is a member of the k-nearest neighbor (k -NN) [14] paradigm. This approach bases the classification of a new instance on the k most similar instances found in the training data. It has been shown to be successful for a range of problems in NLP [1]. Daelemans et al. [1] argue that MBL has a suitable bias for such problems because it allows learning from atypical and lowfrequency events, thus enabling a principled approach to the treatment of exceptions and sub-regularities in language. Another advantage of MBL lies in the fact that it can work with features with a large number of different values. This allows the usage of complete words as fea"
R09-1037,W04-0846,0,0.0126,"ut the final system performance is still far lower than the performance of fully supervised systems. The development of fully supervised WSD systems, however, has been given considerably more attention than the automatic acquisition of sense-tagged corpora. This is proven by approaches as the ones presented by Mihalcea et al. in the overview of the SensEval-3 English lexical sample competition [13] na¨ıve Bayes systems (e.g. htsa3 [6]), systems based on kernel methods (e.g. IRTS-Kernels [19]; nusels [8]), based on the Lesk algorithm [9] (e.g. wsdiit [18]), maximum entropy models (e.g. Cymfony [15]) and many others (e.g. Prob0 [17] and clr04-ls [10]). Those systems constitute the set of best performing systems from the SensEval-3 evaluation exercise. 3 The system for WSD In order to examine evidence for our hypothesis that only automatically annotated examples of high quality can be used successfully, we designed a memory-based learning system, which we used to train multiple wordexperts/classifiers, first on the SensEval training data, and in a second step with the automatically acquired data. The remainder of the section is structured as follows: In section 3.1, we will describe the s"
R09-1037,P05-3019,0,0.0124804,"punctuation was stripped off, words were tokenized, and meta characters were deleted. Additionally, we used a POS tagger to assign morpho-syntactic annotation to the words. For this purpose, we used the POS tagger by Tsuruoka and Tsujii [20], which is accurate and very efficient, a definite concern with regard to the text size of the newly acquired examples. In a next step, the examples from the dictionaries and corpora had to be annotated for senses. For this purpose we used the generalized framework for WSD - WordNet::SenseRelate::TargetWord9 developed by Pedersen, Patwardhan, and Banerjee [16]. WordNet::SenseRelate::TargetWord uses a modification of the Lesk algorithm [9] that also includes glosses of related words from WordNet. The best sense for a word is then selected based on its semantic relatedness to these words from the context and from WordNet. Based on a manual evaluation of a small data set, we chose the local module for determining relatedness. The reason for not using self-training, as Mihalcea [12] did, was that we wanted to avoid a bias towards 9 199 Source Senseval-3 Dictionaries BNC ukWaC http://www.d.umn.edu/ tpederse/senserelate.html majority senses in the traini"
R09-1037,W04-0852,0,0.0301037,"still far lower than the performance of fully supervised systems. The development of fully supervised WSD systems, however, has been given considerably more attention than the automatic acquisition of sense-tagged corpora. This is proven by approaches as the ones presented by Mihalcea et al. in the overview of the SensEval-3 English lexical sample competition [13] na¨ıve Bayes systems (e.g. htsa3 [6]), systems based on kernel methods (e.g. IRTS-Kernels [19]; nusels [8]), based on the Lesk algorithm [9] (e.g. wsdiit [18]), maximum entropy models (e.g. Cymfony [15]) and many others (e.g. Prob0 [17] and clr04-ls [10]). Those systems constitute the set of best performing systems from the SensEval-3 evaluation exercise. 3 The system for WSD In order to examine evidence for our hypothesis that only automatically annotated examples of high quality can be used successfully, we designed a memory-based learning system, which we used to train multiple wordexperts/classifiers, first on the SensEval training data, and in a second step with the automatically acquired data. The remainder of the section is structured as follows: In section 3.1, we will describe the supervised baseline system, section"
R09-1037,W04-0853,0,0.0212159,"etter results than unsupervised approaches but the final system performance is still far lower than the performance of fully supervised systems. The development of fully supervised WSD systems, however, has been given considerably more attention than the automatic acquisition of sense-tagged corpora. This is proven by approaches as the ones presented by Mihalcea et al. in the overview of the SensEval-3 English lexical sample competition [13] na¨ıve Bayes systems (e.g. htsa3 [6]), systems based on kernel methods (e.g. IRTS-Kernels [19]; nusels [8]), based on the Lesk algorithm [9] (e.g. wsdiit [18]), maximum entropy models (e.g. Cymfony [15]) and many others (e.g. Prob0 [17] and clr04-ls [10]). Those systems constitute the set of best performing systems from the SensEval-3 evaluation exercise. 3 The system for WSD In order to examine evidence for our hypothesis that only automatically annotated examples of high quality can be used successfully, we designed a memory-based learning system, which we used to train multiple wordexperts/classifiers, first on the SensEval training data, and in a second step with the automatically acquired data. The remainder of the section is structured as fol"
R09-1037,W04-0856,0,0.0291506,"interesting remark is the fact that employing web data reaches better results than unsupervised approaches but the final system performance is still far lower than the performance of fully supervised systems. The development of fully supervised WSD systems, however, has been given considerably more attention than the automatic acquisition of sense-tagged corpora. This is proven by approaches as the ones presented by Mihalcea et al. in the overview of the SensEval-3 English lexical sample competition [13] na¨ıve Bayes systems (e.g. htsa3 [6]), systems based on kernel methods (e.g. IRTS-Kernels [19]; nusels [8]), based on the Lesk algorithm [9] (e.g. wsdiit [18]), maximum entropy models (e.g. Cymfony [15]) and many others (e.g. Prob0 [17] and clr04-ls [10]). Those systems constitute the set of best performing systems from the SensEval-3 evaluation exercise. 3 The system for WSD In order to examine evidence for our hypothesis that only automatically annotated examples of high quality can be used successfully, we designed a memory-based learning system, which we used to train multiple wordexperts/classifiers, first on the SensEval training data, and in a second step with the automatically"
R09-1037,H05-1059,0,0.014026,"amounts to approximately 183 000 instances for training. For testing, we used the original test set from the SensEval-3 competition to make our results comparable to previous work. 3.4 Data preprocessing For the additional examples, we had to preprocess the text to change the representation so that it was comparable to the SensEval examples. For preprocessing, punctuation was stripped off, words were tokenized, and meta characters were deleted. Additionally, we used a POS tagger to assign morpho-syntactic annotation to the words. For this purpose, we used the POS tagger by Tsuruoka and Tsujii [20], which is accurate and very efficient, a definite concern with regard to the text size of the newly acquired examples. In a next step, the examples from the dictionaries and corpora had to be annotated for senses. For this purpose we used the generalized framework for WSD - WordNet::SenseRelate::TargetWord9 developed by Pedersen, Patwardhan, and Banerjee [16]. WordNet::SenseRelate::TargetWord uses a modification of the Lesk algorithm [9] that also includes glosses of related words from WordNet. The best sense for a word is then selected based on its semantic relatedness to these words from th"
R09-1037,P95-1026,0,0.570438,"Missing"
R09-1047,2007.mtsummit-papers.20,0,0.0579439,"Missing"
R09-1047,W02-0504,0,0.43356,"Missing"
R09-1047,N07-2014,0,0.196101,"Missing"
R09-1047,W05-0711,0,0.393399,"Missing"
R09-1047,W09-0804,0,0.30028,"Missing"
R09-1047,P06-1073,0,0.755089,"in-vitro evaluation, i.e. on data from the same treebank on which it was trained. After we obtain a Sandra K¨ ubler Indiana University skuebler @indiana.edu reliable system for the treebank data, we will test it to determine how well it works in an in-vivo evaluation, i.e. on real-world data. More importantly, in the second step, we will investigate how the system needs to be modified in order to produce reliable diacritizations in the in-vivo situation. In the first step, we concentrate on investigating how important different types of context are for diacritization. We follow Zitouni et al. [16] in defining diacritization as a classification problem in which we decide for each character in the undiacritized word whether it is followed by a short vowel. Additionally, the classification task includes the shadda and sokoon (lack of a vowel). The shadda is consonant gemination and is usually found in combination with another vowel, thus resulting in 3 classes,˜a,˜i,˜u. The shadda plays an important role in the interpretation of Arabic words because it is used, inter alia, to discriminate between the base form of a verb and its causative form: kataba (to write), kat˜aba (to make write). A"
R09-1085,P02-1045,0,0.0186614,"8 0.650 0.607 0.627 0.604 0.610 0.602 0.559 0.587 0.491 0.562 maximum entropy prec. recall F-score 0.637 0.414 0.502 0.380 0.737 0.501 0.443 0.801 0.570 0.485 0.730 0.583 0.514 0.667 0.581 0.526 0.656 0.584 0.548 0.614 0.579 0.561 0.580 0.570 0.594 0.513 0.550 0.630 0.430 0.511 Table 3: Results of different classifiers with instance sampling process: Does the success translate to other settings with other classifiers? For this reason, we repeated the experiments with two more classifiers. We chose classifiers that have been successfully used for coreference resolution: a decision tree learner [4, 7, 10], and a maximum entropy learner [6, 8]. For both classifiers, we used the Weka [14] implementations, J48 and logistic regression respectively. We were planning to include a SVM classifier in the set, but training times proved prohibitive. In order to be able to estimate the effects of instance sampling on the different classifiers, we kept the whole system and data sets constant and changed only the classifiers. This means that all classifiers are trained on exactly the same folds in the 10-fold CV and on the same feature sets. We are aware that the feature set that proved optimal for the memo"
R09-1085,P04-1020,0,0.019057,"9 0.587 0.491 0.562 maximum entropy prec. recall F-score 0.637 0.414 0.502 0.380 0.737 0.501 0.443 0.801 0.570 0.485 0.730 0.583 0.514 0.667 0.581 0.526 0.656 0.584 0.548 0.614 0.579 0.561 0.580 0.570 0.594 0.513 0.550 0.630 0.430 0.511 Table 3: Results of different classifiers with instance sampling process: Does the success translate to other settings with other classifiers? For this reason, we repeated the experiments with two more classifiers. We chose classifiers that have been successfully used for coreference resolution: a decision tree learner [4, 7, 10], and a maximum entropy learner [6, 8]. For both classifiers, we used the Weka [14] implementations, J48 and logistic regression respectively. We were planning to include a SVM classifier in the set, but training times proved prohibitive. In order to be able to estimate the effects of instance sampling on the different classifiers, we kept the whole system and data sets constant and changed only the classifiers. This means that all classifiers are trained on exactly the same folds in the 10-fold CV and on the same feature sets. We are aware that the feature set that proved optimal for the memory-based classifier may not guarantee"
R09-1085,W02-1008,0,0.114388,"8 0.650 0.607 0.627 0.604 0.610 0.602 0.559 0.587 0.491 0.562 maximum entropy prec. recall F-score 0.637 0.414 0.502 0.380 0.737 0.501 0.443 0.801 0.570 0.485 0.730 0.583 0.514 0.667 0.581 0.526 0.656 0.584 0.548 0.614 0.579 0.561 0.580 0.570 0.594 0.513 0.550 0.630 0.430 0.511 Table 3: Results of different classifiers with instance sampling process: Does the success translate to other settings with other classifiers? For this reason, we repeated the experiments with two more classifiers. We chose classifiers that have been successfully used for coreference resolution: a decision tree learner [4, 7, 10], and a maximum entropy learner [6, 8]. For both classifiers, we used the Weka [14] implementations, J48 and logistic regression respectively. We were planning to include a SVM classifier in the set, but training times proved prohibitive. In order to be able to estimate the effects of instance sampling on the different classifiers, we kept the whole system and data sets constant and changed only the classifiers. This means that all classifiers are trained on exactly the same folds in the 10-fold CV and on the same feature sets. We are aware that the feature set that proved optimal for the memo"
R09-1085,E06-2015,0,0.0133139,"9 0.587 0.491 0.562 maximum entropy prec. recall F-score 0.637 0.414 0.502 0.380 0.737 0.501 0.443 0.801 0.570 0.485 0.730 0.583 0.514 0.667 0.581 0.526 0.656 0.584 0.548 0.614 0.579 0.561 0.580 0.570 0.594 0.513 0.550 0.630 0.430 0.511 Table 3: Results of different classifiers with instance sampling process: Does the success translate to other settings with other classifiers? For this reason, we repeated the experiments with two more classifiers. We chose classifiers that have been successfully used for coreference resolution: a decision tree learner [4, 7, 10], and a maximum entropy learner [6, 8]. For both classifiers, we used the Weka [14] implementations, J48 and logistic regression respectively. We were planning to include a SVM classifier in the set, but training times proved prohibitive. In order to be able to estimate the effects of instance sampling on the different classifiers, we kept the whole system and data sets constant and changed only the classifiers. This means that all classifiers are trained on exactly the same folds in the 10-fold CV and on the same feature sets. We are aware that the feature set that proved optimal for the memory-based classifier may not guarantee"
R09-1085,J01-4004,0,0.771969,"set. In our case, this means restricting the number of negative examples while the positive ones remain unchanged. In the case of coreference resolution for definite noun phrases, in which case all preceding markables of a Rachael Cantrell Indiana University rcantrel @indiana.edu coreference chain are used for positive examples, sampling those positive examples can have a positive effect, too, as Ng and Cardie [7] show. One often-used linguistically motivated approach to restrict the negative examples is to use only the markables between the pronoun and the actual antecedent. (cf. for example [7, 10]) Another possibility is to sample the negative examples randomly until a certain, predefined ratio is reached. To our knowledge, no systematic comparison of sampling methods has been performed. This is the aim of the work presented here. For these experiments, we used a system for pronoun resolution for German [3, 15] as the basis. The system combines a rule-based morphological pre-filter with a pronoun resolution module based on a classifier. In the original system (cf. section 5), memory-based learning was used. For this reason, we first investigated the full range of sampling methods consi"
R09-1085,telljohann-etal-2004-tuba,1,0.781946,"t and discuss the results for the following classifiers: a memory-based classifier, a decision tree classifier, and a maximum entropy classifier. 2 Pronoun resolution: Task description The first step in pronoun resolution is a syntactic analysis, which provides the pronouns and their possible antecedents, so-called markables. Since we are only interested in the influence of instance sampling, we used gold standard data to identify the pronouns and the markables. The syntactic information as well as the referential information is taken from the T¨ ubingen Treebank of Written German, T¨ uBa-D/Z [11] (for more details cf. section 4). In machine learning approaches, the task of pronoun resolution is normally defined as a classification task. Normally, this leads to the approach of pairing each anaphoric pronoun with all markables preceding it in a certain window in turn. Then for each pronounmarkable pair, the classifier decides whether there is an anaphoric relation between the two (cf. e.g. [9, 10]). We follow this approach. 478 International Conference RANLP 2009 - Borovets, Bulgaria, pages 478–483 German is morphologically richer than English, for which most of the work has been done,"
R09-1085,M95-1005,0,0.11555,"Missing"
R09-1085,D07-1057,0,0.0382645,"classifier’s decisions, this sampling method was carried out individually for each of the classifiers used in section 6.3, thus resulting in different sampling ratios for the individual classifiers. This method results in a comparatively low sampling ratio of 1:0.96 for memory-based learning, 1:0.83 for decision-tree learning, and 1:0.70 for maximum entropy learning. Random Sampling is a method in which first the ratio is determined, then negative examples are randomly chosen (without replacement) until the ratio is reached. This method has been used successfully, for example, by Zhao and Ng [17] for Chinese zero pronoun resolution. Since our data was prefiltered by a morphological filter, our original ratio was considerable lower than theirs: We used sampling ratios between 1:1 and 1:4.29 (the full data set). 4 The data Since we are only interested in the influence of instance sampling, we used gold standard data for the syntactic identification of the pronouns and the possible antecedents (markables). As our gold data source, we used the newspaper corpus T¨ ubingen Treebank of Written German (T¨ uBa-D/Z) [12], which is based on German newspaper articles from the newspaper die tagesz"
R11-1006,eberhard-etal-2010-indiana,1,0.889282,"Missing"
R11-1006,gimenez-marquez-2004-svmtool,0,0.264395,"Missing"
R11-1006,P06-1043,0,0.198788,"Missing"
R11-1006,W06-1615,0,0.539317,"aggers both agree, and naive co-training, where all sentences from one tagger are added to the training of the other, with no filter. For small sets of seed sentences, both types of co-training improve accuracy, with the higher quality, smaller training set from agreement-based co-training performing slightly better. The authors also report results for using naive co-training after the taggers were already trained on large amounts of manually annotated data. Naive co-training did not improve the taggers when trained in such a way (the authors leave agreement based co-training to future work). Blitzer et al. (2006) investigate domain adaptation for POS tagging using the method of structural correspondence learning (SCL). SCL pro42 4 Experimental Setup 4.2 4.1 We use three POS taggers: TnT (Brants, 2000), MElt (Denis and Sagot, 2009), and SVMTool (Gim´enez and M`arquez, 2004). These taggers were chosen because they represent the state of the art for single-direction taggers and also because they use different approaches to POS tagging and thus have different biases. Our assumption is that the different biases will result in different types of POS tagging mistakes. Data Sets We use three corpora: the Penn"
R11-1006,W96-0213,0,0.522043,"only the actual transcriptions. This corpus serves as our unannotated, indomain training corpus. POS Taggers TnT (Brants, 2000) is a trigram Markov model POS tagger with state-of-the-art treatment of unknown words. TnT generates files containing lexical and transition frequencies and thus provides us with the option of including new trigrams directly into the trained model. The Maximum-Entropy Lexicon-Enriched Tagger (MElt) (Denis and Sagot, 2009) is a conditional sequence maximum entropy POS tagger that uses a set of lexical and context features, which are a superset of the features used by Ratnaparkhi (1996) and Toutanova and Manning (2000). The CReST Corpus (Eberhard et al., 2010) is a multi-modal corpus consisting of 7 dialogues, comprising 11 317 words in 1 977 sentences. Similar in domain to the HCRC corpus, it represents cooperative dialogues, but is based on a slightly different task: one of the participants is located in a search environment, while the other is outside but has access to a map of the environment. The participants need to collaborate to fulfill their tasks (locating objects in the environment and placing objects on the map). CReST is annotated for POS, syntactic dependency a"
R11-1006,D08-1050,0,0.0583167,"Missing"
R11-1006,A00-1031,0,0.634487,"e accuracy, with the higher quality, smaller training set from agreement-based co-training performing slightly better. The authors also report results for using naive co-training after the taggers were already trained on large amounts of manually annotated data. Naive co-training did not improve the taggers when trained in such a way (the authors leave agreement based co-training to future work). Blitzer et al. (2006) investigate domain adaptation for POS tagging using the method of structural correspondence learning (SCL). SCL pro42 4 Experimental Setup 4.2 4.1 We use three POS taggers: TnT (Brants, 2000), MElt (Denis and Sagot, 2009), and SVMTool (Gim´enez and M`arquez, 2004). These taggers were chosen because they represent the state of the art for single-direction taggers and also because they use different approaches to POS tagging and thus have different biases. Our assumption is that the different biases will result in different types of POS tagging mistakes. Data Sets We use three corpora: the Penn Treebank for the source domain; the HCRC Map Task Corpus (Thompson et al., 1996) for additional training in the cooperative dialogue domain; and the CReST corpus (Eberhard et al., 2010) for e"
R11-1006,C08-1015,0,0.0330639,"Missing"
R11-1006,W00-1308,0,0.0824409,"riptions. This corpus serves as our unannotated, indomain training corpus. POS Taggers TnT (Brants, 2000) is a trigram Markov model POS tagger with state-of-the-art treatment of unknown words. TnT generates files containing lexical and transition frequencies and thus provides us with the option of including new trigrams directly into the trained model. The Maximum-Entropy Lexicon-Enriched Tagger (MElt) (Denis and Sagot, 2009) is a conditional sequence maximum entropy POS tagger that uses a set of lexical and context features, which are a superset of the features used by Ratnaparkhi (1996) and Toutanova and Manning (2000). The CReST Corpus (Eberhard et al., 2010) is a multi-modal corpus consisting of 7 dialogues, comprising 11 317 words in 1 977 sentences. Similar in domain to the HCRC corpus, it represents cooperative dialogues, but is based on a slightly different task: one of the participants is located in a search environment, while the other is outside but has access to a map of the environment. The participants need to collaborate to fulfill their tasks (locating objects in the environment and placing objects on the map). CReST is annotated for POS, syntactic dependency and constituency, disfluency, and"
R11-1006,W03-0407,0,0.138602,"method of using agreement between taggers was originally used by van Halteren et al. (2001) to improve tagger performance. We investigate the following questions: 1) How does the number of agreeing POS taggers influence the accuracy of the final tagger? 2) Should we select only complete sentences or add all trigrams on which the taggers agree? Lifting the restriction that the taggers agree on complete sentences will increase the size of the training set. 3) Do we need the full Penn Treebank training set, or does this large training set dominate the smaller training set from the target domain? Clark et al. (2003) use the results of one POS tagger on unannotated data to inform the training of another tagger in a semi-supervised setting using a co-training routine with a Markov model tagger and a maximum entropy tagger. The authors test both agreement-based co-training, where the sentences are added to training only if the taggers both agree, and naive co-training, where all sentences from one tagger are added to the training of the other, with no filter. For small sets of seed sentences, both types of co-training improve accuracy, with the higher quality, smaller training set from agreement-based co-tr"
R11-1006,J01-2002,0,0.283744,"Missing"
R11-1006,P07-1033,0,0.237371,"Missing"
R11-1006,Y09-1013,0,0.0563481,"higher quality, smaller training set from agreement-based co-training performing slightly better. The authors also report results for using naive co-training after the taggers were already trained on large amounts of manually annotated data. Naive co-training did not improve the taggers when trained in such a way (the authors leave agreement based co-training to future work). Blitzer et al. (2006) investigate domain adaptation for POS tagging using the method of structural correspondence learning (SCL). SCL pro42 4 Experimental Setup 4.2 4.1 We use three POS taggers: TnT (Brants, 2000), MElt (Denis and Sagot, 2009), and SVMTool (Gim´enez and M`arquez, 2004). These taggers were chosen because they represent the state of the art for single-direction taggers and also because they use different approaches to POS tagging and thus have different biases. Our assumption is that the different biases will result in different types of POS tagging mistakes. Data Sets We use three corpora: the Penn Treebank for the source domain; the HCRC Map Task Corpus (Thompson et al., 1996) for additional training in the cooperative dialogue domain; and the CReST corpus (Eberhard et al., 2010) for evaluation in the target domain"
R11-1006,J93-2004,0,\N,Missing
R11-1006,H93-1005,0,\N,Missing
R11-1006,D07-1112,0,\N,Missing
R11-1008,W07-2416,0,0.0166155,". (2) The extrinsic evaluation, which compares the two parsers in the NLU architecture, is also based on in-domain training data. Data sets: For the intrinsic evaluation, we used the Penn Treebank. For the constituent experiments, we used the treebank with grammatical functions since the semantic construction requires this information. The only exception is the experiment using the Berkeley parser with the Penn Treebank: Because of memory restrictions, we could not use grammatical functions. For the dependency parser, we used a dependency version of the Penn Treebank created by pennconverter (Johansson and Nugues, 2007). For the in-domain experiments (intrinsic and extrinsic), we used CReST (Eberhard et al., 2010), a corpus of natural language dialogues obtained from recordings of humans performing a cooperative, remote search task. The multi-modal corpus contains the speech signals and transcriptions of the dialogues, which are additionally annotated for dialogue structure, disfluencies, POS, and syntax. The syntactic annotation covers both constituent annotation based on the Penn Treebank annotation scheme and dependencies based on the dependency version of the Penn Treebank. The corpus consists of 7 dialo"
R11-1008,P01-1005,0,0.0564643,"Missing"
R11-1008,J93-2004,0,0.0374984,"the (partial) parses. While these aspects are often of secondary importance in many NLU systems, they are essential to a robotic NLU architecture. Since we experiment with a new corpus that has not been used in parsing research yet, we also present an intrinsic evaluation to give a reference point to put the parsers’ performance into perspective with regard to previous work. More specifically, we investigate two points: (1) Given that spoken commands to robots are considerably shorter and less complex than newspaper sentences, is it possible to use existing resources, i.e., the Penn Treebank (Marcus et al., 1993), for training the parsers without a major decrease in accuracy? And (2), are constituent or dependency parsers better suited for the NLU architecture described above, in terms of accuracy and speed? To answer these questions, we carried out two experiments: (1) The intrinsic evaluation. This is split into two parts: one that compares constituent and dependency parsers on our test data when both parsers were trained on the Penn Treebank; and one that compares the parsers trained on a small in-domain set. (2) The extrinsic evaluation, which compares the two parsers in the NLU architecture, is a"
R11-1008,W08-1004,0,0.016489,"t trees is less straightforward since it is more difficult to automatically identify the head of a phrase, and to connect the arguments in the same way. We use a slightly different method: each node in the none of these measures is ideal: the ParsEval measures have been widely criticized because they favor flat annotation schemes and harshly punish attachment errors (Carroll et al., 1998). Additionally, there is no evaluation scheme that can compare the performance of constituent and dependency parsers, or parsers using different underlying grammars. Converting constituents into dependencies (Boyd and Meurers, 2008), evens out differences between underlying grammars. However, it is well known that the conversion into a different format is not straightforward. Clark and Curran (2007), who convert the CCGBank to DepBank, report an F-score of 68.7 for the conversion on gold data. Conversions into dependencies have been evaluated on the treebank side (Rehbein and van Genabith, 2007), but not on the parser side; yet, the latter is critical since parser errors result in unpredicted structures and thus conversion errors. Intrinsic parsing quality has been shown to be insufficient for comparing parsers, and addi"
R11-1008,W03-2806,0,0.0386848,"Missing"
R11-1008,R09-2002,1,0.901612,"Missing"
R11-1008,N07-1051,0,0.0782877,"rly, fragmented input from the speech recognizer may not lead to any parsable sequence of words, again likely resulting in incorrect robot behavior. Hence, we need an extrinsic evaluation to determine the utility and performance of a parser in the context of other NLU components at the level of semantics and action execution. To this end, we introduce an evaluation architecture that can be used for extrinsic evaluations of NLU components and demonstrate its utility for parser evaluation using state-of-the-art parsers for each of the two main parsing paradigms: the Berkeley constituent parser (Petrov and Klein, 2007) and MaltParser (Nivre et al., 2007b), a dependency parser. The evaluation compares intrinsic and extrinsic measures on the CReST corpus (Eberhard et al., 2010), which is representative of a broad class of collaborative instructionbased tasks envisioned for future robots (e.g., in search and rescue missions). To our knowledge, no previous extrinsic parser evaluation used conversions to semantic/action representations, which can be performed for different parser types and are thus ideally suited for comparing parsing frameworks. Moreover, no previous work has presented a combined intrinsic-extr"
R11-1008,P07-1032,0,0.0319867,"different method: each node in the none of these measures is ideal: the ParsEval measures have been widely criticized because they favor flat annotation schemes and harshly punish attachment errors (Carroll et al., 1998). Additionally, there is no evaluation scheme that can compare the performance of constituent and dependency parsers, or parsers using different underlying grammars. Converting constituents into dependencies (Boyd and Meurers, 2008), evens out differences between underlying grammars. However, it is well known that the conversion into a different format is not straightforward. Clark and Curran (2007), who convert the CCGBank to DepBank, report an F-score of 68.7 for the conversion on gold data. Conversions into dependencies have been evaluated on the treebank side (Rehbein and van Genabith, 2007), but not on the parser side; yet, the latter is critical since parser errors result in unpredicted structures and thus conversion errors. Intrinsic parsing quality has been shown to be insufficient for comparing parsers, and adding extrinsic measures to the evaluation can lead to inconclusive results, in comparing two dependency parsers (Moll´a and Hutchinson, 2003), three constituent parsers (Pr"
R11-1008,D07-1066,0,0.0753633,"Missing"
R11-1008,eberhard-etal-2010-indiana,1,0.840326,"Missing"
R11-1008,D07-1096,1,\N,Missing
R11-1036,N07-1030,0,0.0233072,", two major shared tasks were concerned with coreference resolution: the S EM E VAL 2010 task 1 “Coreference Resolution in Multiple Languages” (Recasens et al., 2010) and the C O NLL shared task 2011 “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011). Both shared tasks introduced a new element into the definition of coreference resolution: The detection of mentions. Previous to these shared tasks, the availability of gold standard mentions was often assumed, and research concentrated on the resolution of coreference relationships between mentions. (e.g. (Luo et al., 2004; Denis and Baldridge, 2007)). However, in many approaches to coreference resolution, the problem is even more restricted, and the coreference resolution component expects only such Desislava Zhekova University of Bremen zhekova@uni-bremen.de mentions that are coreferent in the present context, i.e. no singletons are present in the data. “Singleton” is a cover term for mentions that are never coreferent, such as in in general or on the contrary, and mentions that are potentially coreferent but occur only once in a document. If the extraction of mentions is part of the task definition, then filtering singletons is general"
R11-1036,N06-2015,0,0.0514455,") - (1) (2)|(3|(4|(5 5) (6)|4) (7)|(8|(9) (10)|(11)|8)|3) (12) (13) (14 14) - Table 3: An example sentence from the C O NLL shared task data set. data sets of the S EM E VAL and the C O NLL shared task. We investigate different strategies of handling singletons, and their influence on results of a robust coreference resolution system, UBIU. 3 The Shared Task English Data Sets Both shared tasks for coreference resolution in the last year, the S EM E VAL 2010 task 1 (Recasens et al., 2010) and the C O NLL shared task 2011 (Pradhan et al., 2011), included an English data set, based on OntoNotes (Hovy et al., 2006). However, both data sets differ in the texts selected for their assembly as well as in the annotations on the gold standard. We discuss these differences below. 3.1 The S EM E VAL English Data Set The S EM E VAL task 1 (Recasens et al., 2010) aimed at the evaluation and comparison of coreference resolution systems in a multilingual environment targeting six languages (Catalan, Dutch, English, German, Italian, Spanish). The main focus of the task was on system portability across different languages and the importance of various linguistic annotations for the system performance for all language"
R11-1036,P04-1018,0,0.0355991,". In the last year, two major shared tasks were concerned with coreference resolution: the S EM E VAL 2010 task 1 “Coreference Resolution in Multiple Languages” (Recasens et al., 2010) and the C O NLL shared task 2011 “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011). Both shared tasks introduced a new element into the definition of coreference resolution: The detection of mentions. Previous to these shared tasks, the availability of gold standard mentions was often assumed, and research concentrated on the resolution of coreference relationships between mentions. (e.g. (Luo et al., 2004; Denis and Baldridge, 2007)). However, in many approaches to coreference resolution, the problem is even more restricted, and the coreference resolution component expects only such Desislava Zhekova University of Bremen zhekova@uni-bremen.de mentions that are coreferent in the present context, i.e. no singletons are present in the data. “Singleton” is a cover term for mentions that are never coreferent, such as in in general or on the contrary, and mentions that are potentially coreferent but occur only once in a document. If the extraction of mentions is part of the task definition, then fil"
R11-1036,H05-1004,0,0.367856,"Missing"
R11-1036,W11-1901,0,0.455302,"overall evaluation – in an experiment where the coreference resolution results remain unchanged over the different settings. 1 Introduction In the last decade, the task of Coreference Resolution has become an important enterprise in Natural Language Processing. At the same time, the need for proper benchmarking increased over time. In the last year, two major shared tasks were concerned with coreference resolution: the S EM E VAL 2010 task 1 “Coreference Resolution in Multiple Languages” (Recasens et al., 2010) and the C O NLL shared task 2011 “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011). Both shared tasks introduced a new element into the definition of coreference resolution: The detection of mentions. Previous to these shared tasks, the availability of gold standard mentions was often assumed, and research concentrated on the resolution of coreference relationships between mentions. (e.g. (Luo et al., 2004; Denis and Baldridge, 2007)). However, in many approaches to coreference resolution, the problem is even more restricted, and the coreference resolution component expects only such Desislava Zhekova University of Bremen zhekova@uni-bremen.de mentions that are coreferent i"
R11-1036,D09-1101,0,0.0143532,"ent, additional mentions are included for each verb with a predicate lemma. The feature extractor creates a feature vector for each possible pair of a mention and all its possible antecedents in a context of 3 sentences. Since mentions are represented by their syntactic head, the module uses a heuristic to select the rightmost noun in a noun phrase. However, since postmodifying prepositional phrases may be present in the mention, the noun may not be followed by a preposition. Initially, UBIU used a wide set of features (Zhekova and K¨ubler, 2010), which constitutes a subset of the features by Rahman and Ng (2009). Our experiments in the C O NLL 2011 shared task (Zhekova and K¨ubler, 2011) showed that adding additional information, such as WordNet or number/gender information, does not improve performance for our system when applied on the C O NLL data set. For this reason, we use the basic feature set shown in table 4. Another important step is to separate singleton mentions from coreferent ones since only the latter are annotated in OntoNotes. Our mention extractor overgenerates in that it extracts all possible mentions, and only after classification, the system can decide which mentions are singleto"
R11-1036,W09-2411,0,0.0346917,"Missing"
R11-1036,P09-1074,0,0.0168023,"Desislava Zhekova University of Bremen zhekova@uni-bremen.de mentions that are coreferent in the present context, i.e. no singletons are present in the data. “Singleton” is a cover term for mentions that are never coreferent, such as in in general or on the contrary, and mentions that are potentially coreferent but occur only once in a document. If the extraction of mentions is part of the task definition, then filtering singletons is generally necessary since methods for mention identification often overgenerate and produce all noun phrases (NPs), including all singletons. Twinless mentions (Stoyanov et al., 2009) are mentions that have been identified by a coreference resolution system but are not included in the gold data, or vice versa. Twinless mentions can lead to considerable changes in overall system performance, and Stoyanov et al. (2009) report that at that time B3 was not prepared to handle them. For the C O NLL shared task, the metrics were updated to obtain ”better alignment for B3 and CEAF so that the gold standard set and the system output have the same number of mentions” (p.c. S. Pradhan). In this paper, we investigate how the presence of singletons in either gold standard or in the sys"
R11-1036,M95-1005,0,0.236412,"Missing"
R11-1036,S10-1019,1,0.886392,"Missing"
R11-1036,W11-1918,1,0.81815,"Missing"
R11-1036,D08-1067,0,\N,Missing
R11-1036,S10-1001,0,\N,Missing
R13-1001,A00-2013,0,0.435736,"Missing"
R13-1001,W96-0102,0,0.0395104,"earning method that does not abstract rules from the data, but rather keeps all training data. During training, the learner stores the training instances without abstraction. Given a new instance, the classifier finds the k nearest neighbors in the training set and chooses their most frequent class for the new instance. MBL has been shown to have a suitable bias for NLP problems (Daelemans et al., 1999; Daelemans and van den Bosch, 2005) since it does not abstract over irregularities or subregularities. For each of the two classification tasks (i.e., segmentation and POS tagging), we use MBT (Daelemans et al., 1996), a memory-based POS tagger that has access to previous tagging decisions in addition to an expressive feature set. 4.2 5 Segmentation 5.1 Setup We define segmentation as an IOB classification task, where each letter in a word is tagged with a label indicating its place in a segment. The tagset is {B-SEG, I-SEG, O}, where B is a tag assigned to the beginning of a segment, I denotes the inside of a segment, and O spaces between surface form words. Data Sets and Splits We use segmentation and POS data from the Penn Arabic Treebank (PATB) (Maamouri et al., 2004), specifically, we use the followin"
R13-1001,N10-1105,1,0.843769,"Missing"
R13-1001,W95-0107,0,0.361934,"Missing"
R13-1001,N04-4038,1,0.820658,"Missing"
R13-1001,P08-2030,1,0.909449,"Missing"
R13-1001,W10-1401,0,0.122729,"Missing"
R13-1001,P05-1071,0,0.0971224,"Missing"
R13-1001,P09-2056,0,0.0503576,"Missing"
R13-1008,I08-2097,0,0.196589,"Missing"
R13-1008,P03-1054,0,0.0206859,"or sections 2-21 (the standard training set for parsing). Our target domain is dialog text taken from cooperative map tasks. The test corpus consists of Cooperative Remote Search Task (CReST) dialogs, in which a searcher collects and deposits items throughout a search location (a series of connected offices) at the guidance of a director, who has a map of the location and communicates 4.2 Parsers Both experiments use the Berkeley Parser (Petrov et al., 2006). For parser combination, we also use the Bikel Parser (Bikel, 2004) and LoPar (Schmid, 2000), and for co-training, the Stanford Parser (Klein and Manning, 2003). Bikel’s parser is a probabilistic context-free (PCFG) parser with a probability model based on Collins’s model 2 (Collins, 1999); the Berkeley 58 optimal values for n and m were found to be 20 and 500, respectively. Then, we repeated the experiment with the PTB 2-21 training set. We used a single training set for both parsers; i.e., after each cycle, the n-best parsed sentences from each parser were added to a common training set, rather than passed to a unique training set for the opposite parser. Initial experiments showed that the set of n-best ranked sentences was comprised almost entire"
R13-1008,J93-2004,0,0.046995,"ompare two different methods in domain adaptation applied to constituent parsing: parser combination and cotraining, each used to transfer information from the source domain of news to the target domain of natural dialogs, in a setting without annotated data. Both methods outperform the baselines and reach similar results. Parser combination profits most from the large amounts of training data combined with a robust probability model. Co-training, in contrast, relies on a small set of higher quality data. 1 Introduction Research on parsing has mostly concentrated on parsing the Penn Treebank (Marcus et al., 1993). As a consequence, most parsers have probability models that are optimized for the syntactic annotations in this treebank and more generally for the language in the treebank. This means that a parser trained on the Penn Treebank will show a severe degradation in performance when used for parsing data from another domain (McClosky et al., 2010). More recently, research has started on adapting parsers to new domains so that the degradation in parsing is minimized. One of the first venues at which domain adaptation was targeted was the 2007 CoNLL shared task on dependency parsing (Nivre et al.,"
R13-1008,A00-1031,0,0.18266,"ing experiments, we used the Stanford parser instead of Bikel’s because the co-training experiments require the parsers to generate confidence scores for each parse. The Berkeley parser produces such scores, Bikel’s does not. The Berkeley parser’s training was limited to 5 split-merge cycles in order to avoid overfitting to the PTB. In all experiments, sentences longer than 40 words were excluded from training and testing. All parsers were trained on the source domain training sets of PTB sections 2-11 and 2-21. All experiments use gold POS tags for the PTB and CReST. HCRC is tagged with TnT (Brants, 2000), trained on the full PTB. 4.3 4.5 Evaluation For evaluation, we used the standard evalb software1 and report F1 -scores, based on labeled precision and recall. We performed significance tests using Dan Bikel’s Randomized Parsing Evaluation Comparator2 . 5 Results 5.1 Parser Combination For the experiments on parser combination, we report three baselines, one baseline per parser. Then, we investigate agreement across 3 parsers and across 2 parsers. Parser Combination Agreement across 3 parsers. Here, we report results for the following experiments: The three parsers were used to parse the HCRC"
R13-1008,P06-1043,0,0.127846,"Missing"
R13-1008,C08-1015,0,0.0355334,"Missing"
R13-1008,P97-1003,0,0.263643,"set for both parsers; i.e., after each cycle, the n-best parsed sentences from each parser were added to a common training set, rather than passed to a unique training set for the opposite parser. Initial experiments showed that the set of n-best ranked sentences was comprised almost entirely of single-word sentences, leading to a decrease in performance from the baselines. Consequently, we removed all oneword sentences from the raw target domain data. parser performs split-merge cycles on the training data to automatically induce a PCFG with optimized syntactic categories. Collins’ model 2 (Collins, 1997) is a generative model based on bigram probabilities, dependencies between pairs of words, as well as sub-categorization frames for head-words. LoPar was used in concert with these two parsers for the parser combination experiments due to its human accessible grammar files: rule counts can be directly modified and new rules added to the LoPar grammar. Thus, we can add partially agreeing sentences, in the form of individual rules, from the HCRC data. For the cotraining experiments, we used the Stanford parser instead of Bikel’s because the co-training experiments require the parsers to generate"
R13-1008,W12-0707,0,0.039158,"Missing"
R13-1008,P07-1033,0,0.468569,"Missing"
R13-1008,eberhard-etal-2010-indiana,1,0.90499,"Missing"
R13-1008,P06-1055,0,0.0740179,"ical functions not present in our target domain (see below). All experiments use either sections 211 (as in the 2007 CoNLL shared task on domain adaptation), or sections 2-21 (the standard training set for parsing). Our target domain is dialog text taken from cooperative map tasks. The test corpus consists of Cooperative Remote Search Task (CReST) dialogs, in which a searcher collects and deposits items throughout a search location (a series of connected offices) at the guidance of a director, who has a map of the location and communicates 4.2 Parsers Both experiments use the Berkeley Parser (Petrov et al., 2006). For parser combination, we also use the Bikel Parser (Bikel, 2004) and LoPar (Schmid, 2000), and for co-training, the Stanford Parser (Klein and Manning, 2003). Bikel’s parser is a probabilistic context-free (PCFG) parser with a probability model based on Collins’s model 2 (Collins, 1999); the Berkeley 58 optimal values for n and m were found to be 20 and 500, respectively. Then, we repeated the experiment with the PTB 2-21 training set. We used a single training set for both parsers; i.e., after each cycle, the n-best parsed sentences from each parser were added to a common training set, ra"
R13-1008,N09-1068,0,0.132735,"Missing"
R13-1008,P07-1078,0,0.147668,"Missing"
R13-1008,D07-1111,0,0.130733,"Missing"
R13-1008,W10-2606,0,0.100897,"Missing"
R13-1008,J01-2002,0,0.138739,"Missing"
R13-1008,J03-4003,0,\N,Missing
R13-1008,H93-1005,0,\N,Missing
R13-1008,D07-1112,0,\N,Missing
R13-1008,D07-1096,1,\N,Missing
R13-1008,N10-1004,0,\N,Missing
R13-1046,A00-1031,0,0.271796,"Missing"
R13-1046,W08-1301,0,0.0874013,"Missing"
R13-1046,R11-1057,0,0.0184636,"ing, pages 357–364, Hissar, Bulgaria, 7-13 September 2013. Indeed, Petrov and McDonald (2012) mention that for the shared task, “[t]he goal was to build a single system that can robustly parse all domains, rather than to build several domain-specific systems.” Thus, parsing results were not obtained by genre. However, Roux et al. (2012) demonstrated that using a genre classifier, in order to employ specific sub-grammars, helped improve parsing performance. Indeed, the quality and fit of data has been shown for in-domain parsing (e.g. Hwa, 2001), as well as for other genres, such as questions (Dima and Hinrichs, 2011). One common, well-documented ailment of web parsers is the effect of erroneous tags on POS accuracy. Foster et al. (2011a,b), e.g., note that propagation of POS errors is a serious problem, especially for Twitter data. Researchers have thus worked on improving POS tagging for web data, whether by tagger voting (Zhang et al., 2012) or word clustering (Owoputi et al., 2012; Seddah et al., 2012). There are no reports about the impact of the quality of POS tags for training— i.e., whether worse, automatically-derived tags might be an improvement over gold tags—though Søgaard and Plank (2012) note"
R13-1046,N10-1060,0,0.0438138,"sentences from the Penn Treebank is beneficial. However, we do not know how to best select the out-of-domain sentences. Should they be drawn randomly; should they match in size; should the sentences match in terms of parsing difficulty (cf. perplexity)? We explore different ways to match the in-domain data (section 6). 2 Related Work There is a growing body of work on parsing web data, as evidenced by the 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). There have been many techniques employed for improving parsing models, including normalizing the potentially ill-formed text (Foster, 2010; Gadde et al., 2011; Øvrelid and Skjærholt, 2012) and training parsers on unannotated or reannotated data, e.g., self-training or uptraining, (e.g., Seddah et al., 2012; Roux et al., 2012; Foster et al., 2011b,a). Less work has gone into investigating the impact of different genres or on specific details of the sentences given to the parser. 3 Experimental Setup 3.1 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The EWT is comprised of approx. 16"
R13-1046,I11-1100,0,0.157089,"Missing"
R13-1046,W01-0710,0,0.0520135,"7 Proceedings of Recent Advances in Natural Language Processing, pages 357–364, Hissar, Bulgaria, 7-13 September 2013. Indeed, Petrov and McDonald (2012) mention that for the shared task, “[t]he goal was to build a single system that can robustly parse all domains, rather than to build several domain-specific systems.” Thus, parsing results were not obtained by genre. However, Roux et al. (2012) demonstrated that using a genre classifier, in order to employ specific sub-grammars, helped improve parsing performance. Indeed, the quality and fit of data has been shown for in-domain parsing (e.g. Hwa, 2001), as well as for other genres, such as questions (Dima and Hinrichs, 2011). One common, well-documented ailment of web parsers is the effect of erroneous tags on POS accuracy. Foster et al. (2011a,b), e.g., note that propagation of POS errors is a serious problem, especially for Twitter data. Researchers have thus worked on improving POS tagging for web data, whether by tagger voting (Zhang et al., 2012) or word clustering (Owoputi et al., 2012; Seddah et al., 2012). There are no reports about the impact of the quality of POS tags for training— i.e., whether worse, automatically-derived tags m"
R13-1046,W13-1101,1,0.830387,"Missing"
R13-1046,J93-2004,0,0.0421844,"ques employed for improving parsing models, including normalizing the potentially ill-formed text (Foster, 2010; Gadde et al., 2011; Øvrelid and Skjærholt, 2012) and training parsers on unannotated or reannotated data, e.g., self-training or uptraining, (e.g., Seddah et al., 2012; Roux et al., 2012; Foster et al., 2011b,a). Less work has gone into investigating the impact of different genres or on specific details of the sentences given to the parser. 3 Experimental Setup 3.1 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The EWT is comprised of approx. 16 000 sen358 Train WSJ EWT WSJ WSJ+EWT (balanced) tences from weblogs, newsgroups, emails, reviews, and question-answers. Note that our data sets are different from the ones in Khan et al. (2013) since in the previous work we had removed sentences with POS labels AFX and GW. To create training and test sets, we broke the data into the following sets: sibling, context, and non-local features, employing information from words and POS tags. We use its default settings for all experiments. • WSJ testing: sect"
R13-1046,P11-1163,0,0.0311811,"Missing"
R13-1046,P05-1012,0,0.18425,"Missing"
R13-1046,E06-1011,0,0.0827335,"Missing"
R13-1046,W11-2917,0,0.0203212,"generally fits the testing domain, as mentioned above. There has been less work, however, on extracting specific types of sentences which fit the domain well. Bohnet et al. (2012) noticed a problem with parsing fragments and so extracted longer NPs to include in training as standalone sentences. From a different perspective, Søgaard and Plank (2012) weight sentences in the training data rather than selecting a subset, to better match the distribution of the target domain. In general, identifying sentences which are similar to a particular domain is a concept familiar in active learning (e.g., Mirroshandel and Nasr, 2011; Sassano and Kurohashi, 2010), where dissimilar sentences are selected for hand-annotation to improve parsing. 1. Answer: where can I get morcillas in tampa bay , I will like the argentinian type , but I will try anothers please ? 2. Email: Michael : &lt;s&gt; Thanks for putting the paperwork together . &lt;s&gt; I would have interest in meeting if you can present unique investment opportunities that I do n’t have access to now . 3. News: complete with original Magnavox tubes all tubes have been tested they are all good - stereo amp 4. Review: Buyer Beware !! &lt;s&gt; Rusted out and unsafe cars sold here ! 5."
R13-1046,N10-1120,0,0.0194146,"Missing"
R13-1046,C12-2088,0,0.0589846,"s beneficial. However, we do not know how to best select the out-of-domain sentences. Should they be drawn randomly; should they match in size; should the sentences match in terms of parsing difficulty (cf. perplexity)? We explore different ways to match the in-domain data (section 6). 2 Related Work There is a growing body of work on parsing web data, as evidenced by the 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). There have been many techniques employed for improving parsing models, including normalizing the potentially ill-formed text (Foster, 2010; Gadde et al., 2011; Øvrelid and Skjærholt, 2012) and training parsers on unannotated or reannotated data, e.g., self-training or uptraining, (e.g., Seddah et al., 2012; Roux et al., 2012; Foster et al., 2011b,a). Less work has gone into investigating the impact of different genres or on specific details of the sentences given to the parser. 3 Experimental Setup 3.1 Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012). The EWT is comprised of approx. 16 000 sen358 Train WSJ EWT WSJ WSJ+EWT (balanced) t"
R13-1046,P10-1037,0,0.0222344,"main, as mentioned above. There has been less work, however, on extracting specific types of sentences which fit the domain well. Bohnet et al. (2012) noticed a problem with parsing fragments and so extracted longer NPs to include in training as standalone sentences. From a different perspective, Søgaard and Plank (2012) weight sentences in the training data rather than selecting a subset, to better match the distribution of the target domain. In general, identifying sentences which are similar to a particular domain is a concept familiar in active learning (e.g., Mirroshandel and Nasr, 2011; Sassano and Kurohashi, 2010), where dissimilar sentences are selected for hand-annotation to improve parsing. 1. Answer: where can I get morcillas in tampa bay , I will like the argentinian type , but I will try anothers please ? 2. Email: Michael : &lt;s&gt; Thanks for putting the paperwork together . &lt;s&gt; I would have interest in meeting if you can present unique investment opportunities that I do n’t have access to now . 3. News: complete with original Magnavox tubes all tubes have been tested they are all good - stereo amp 4. Review: Buyer Beware !! &lt;s&gt; Rusted out and unsafe cars sold here ! 5. Blog: The Supreme Court annou"
R13-1046,N13-1039,0,\N,Missing
R13-1097,W12-4511,0,0.0554871,"Missing"
R13-1097,W12-4503,0,0.0365442,"Missing"
R13-1097,P02-1014,0,0.0922056,"l., 2003; Levine and Meurers, 2006). However, treebanks are generally annotated in a more surface-oriented and flat annotation, in which heads of phrases are often not marked as such. The Penn Treebank (Marcus et al., 1993), which is the standard for training statistical parsers for English, for example, uses a flat annotation scheme for NPs, as shown in the examples in (1). The annotation in the Penn Treebank for English also served as the model for the annotations in the Penn Arabic and Chinese treebanks. Related Work While there is a bulk of literature on CR for English (Soon et al., 2001; Ng and Cardie, 2002; Ng, 2007, for example), MCR has only been addressed recently. The majority of work in this area was carried out in the context of the two shared tasks, the SemEval-2010 (Recasens et al., 2010) and the CoNLL-2012 (Pradhan et al., 2012) tasks. We focus on MHD for the data from CoNLL-2012. The majority of the systems participating in the two shared tasks used approaches that were fairly language dependent with respect to MHD. In the context of the CoNLL-2012 task, the systems by Chen and Ng (2012), Martschat et al. (2012), and Uryupina et al. (2012) used manually created sets of rules, based on"
R13-1097,W12-4504,0,0.0153568,"treebanks. Related Work While there is a bulk of literature on CR for English (Soon et al., 2001; Ng and Cardie, 2002; Ng, 2007, for example), MCR has only been addressed recently. The majority of work in this area was carried out in the context of the two shared tasks, the SemEval-2010 (Recasens et al., 2010) and the CoNLL-2012 (Pradhan et al., 2012) tasks. We focus on MHD for the data from CoNLL-2012. The majority of the systems participating in the two shared tasks used approaches that were fairly language dependent with respect to MHD. In the context of the CoNLL-2012 task, the systems by Chen and Ng (2012), Martschat et al. (2012), and Uryupina et al. (2012) used manually created sets of rules, based on head-finding models following (Collins, 1999). This means that every language other than English, which is targeted by these systems, would need other, language specific, sets of rules. Bj¨orkelund and Farkas (2012) employed Choi and Palmer (2010)’s percolation rules for Arabic and English and the rules of Zhang and Clark (2011) for Chinese. Li et al. (2012) used the (1) a. [ NP The average seven-day compound yield] b. [ NP [ NP the ceiling] [ PP on [ NP government debt]]] c. [ NP [ NP executive"
R13-1097,W11-1901,0,0.0596052,"rally called mentions. They need to be grouped into equivalence classes so that each class contains only mentions that refer to the same entity. The classes are called coreference chains. The task of CR includes not only the identification of coreference links between mentions, but also the detection of the mentions themselves. This subtask of CR has not been a main topic of interest, since most of the standard data sets for CR contained gold mention information. This situation changed in the most recent shared tasks on the topic of CR: SemEval-2010 Task 1 (Recasens et al., 2010), CoNLL-2011 (Pradhan et al., 2011) and CoNLL2012 (Pradhan et al., 2012). The data distributed by these tasks included syntactic annotations, and it was considered an integral part of the task for the participating systems to develop their own methods to detect mention boundaries. Statistical approaches to the CR problem often recast the task to a binary classification exercise. For the latter, coreference is represented by a decision model, such as the mention-pair model (Soon 747 Proceedings of Recent Advances in Natural Language Processing, pages 747–754, Hissar, Bulgaria, 7-13 September 2013. head-finding rules from Penn2Ma"
R13-1097,W12-4501,0,0.173301,"e grouped into equivalence classes so that each class contains only mentions that refer to the same entity. The classes are called coreference chains. The task of CR includes not only the identification of coreference links between mentions, but also the detection of the mentions themselves. This subtask of CR has not been a main topic of interest, since most of the standard data sets for CR contained gold mention information. This situation changed in the most recent shared tasks on the topic of CR: SemEval-2010 Task 1 (Recasens et al., 2010), CoNLL-2011 (Pradhan et al., 2011) and CoNLL2012 (Pradhan et al., 2012). The data distributed by these tasks included syntactic annotations, and it was considered an integral part of the task for the participating systems to develop their own methods to detect mention boundaries. Statistical approaches to the CR problem often recast the task to a binary classification exercise. For the latter, coreference is represented by a decision model, such as the mention-pair model (Soon 747 Proceedings of Recent Advances in Natural Language Processing, pages 747–754, Hissar, Bulgaria, 7-13 September 2013. head-finding rules from Penn2Malt, for English and for Chinese. The"
R13-1097,D09-1101,0,0.0471459,"Missing"
R13-1097,W09-2411,0,0.0316129,"Missing"
R13-1097,W12-4508,0,0.0144705,"ared tasks used approaches that were fairly language dependent with respect to MHD. In the context of the CoNLL-2012 task, the systems by Chen and Ng (2012), Martschat et al. (2012), and Uryupina et al. (2012) used manually created sets of rules, based on head-finding models following (Collins, 1999). This means that every language other than English, which is targeted by these systems, would need other, language specific, sets of rules. Bj¨orkelund and Farkas (2012) employed Choi and Palmer (2010)’s percolation rules for Arabic and English and the rules of Zhang and Clark (2011) for Chinese. Li et al. (2012) used the (1) a. [ NP The average seven-day compound yield] b. [ NP [ NP the ceiling] [ PP on [ NP government debt]]] c. [ NP [ NP executives] and [ NP their wives]] 748 a) b) NP PP NN/NNP title the titles after the names. Titles and proper names are not the only phrase type that is difficult to be covered by heuristics. Other such types include full person names with the use of given and surname or more complex cases, involving coordinated phrases that need to elicit more than one head. Such complex cases cannot be covered by a simple heuristic, but rather need to be defined via language de"
R13-1097,J01-4004,0,0.136502,"e grammar (Sag et al., 2003; Levine and Meurers, 2006). However, treebanks are generally annotated in a more surface-oriented and flat annotation, in which heads of phrases are often not marked as such. The Penn Treebank (Marcus et al., 1993), which is the standard for training statistical parsers for English, for example, uses a flat annotation scheme for NPs, as shown in the examples in (1). The annotation in the Penn Treebank for English also served as the model for the annotations in the Penn Arabic and Chinese treebanks. Related Work While there is a bulk of literature on CR for English (Soon et al., 2001; Ng and Cardie, 2002; Ng, 2007, for example), MCR has only been addressed recently. The majority of work in this area was carried out in the context of the two shared tasks, the SemEval-2010 (Recasens et al., 2010) and the CoNLL-2012 (Pradhan et al., 2012) tasks. We focus on MHD for the data from CoNLL-2012. The majority of the systems participating in the two shared tasks used approaches that were fairly language dependent with respect to MHD. In the context of the CoNLL-2012 task, the systems by Chen and Ng (2012), Martschat et al. (2012), and Uryupina et al. (2012) used manually created se"
R13-1097,H05-1004,0,0.0193154,"is provided, given a small annotated data set. Table 2: MHD for excerpt data for all languages, Arabic (AR), English (EN), and Chinese (ZH), for all spans of mentions. and uses TiMBL for classification. Since we are more interested in the effects of the MHD methods on the full CR system rather than in the optimal performance that can be achieved by UBIU, we do not aim at language dependent system optimization on any system component. We use the official CoNLL-2012 scorer, which provides five evaluation metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), the two variants of CEAF (Luo, 2005), CEAFE and CEAFM , and BLANC (Recasens and Hovy, 2011). For comparison, we calculate a TOTAL score as the average of the F-score of all metrics. 5.2 Intrinsic Evaluation The results in table 2 show an interesting outcome: HeuristicH, which requires only minimal language specific knowledge, leads to the lowest performance across all three languages, with an F-score of 0.83 for Arabic, 0.90 for Chinese, and 0.67 for English. This outcome shows that for Arabic and Chinese, we reach a very competitive performance with a rather simple heuristic. Remember that both Arabic and Chinese have a clearly"
R13-1097,N04-1032,0,0.0147954,"o a binary classification exercise. For the latter, coreference is represented by a decision model, such as the mention-pair model (Soon 747 Proceedings of Recent Advances in Natural Language Processing, pages 747–754, Hissar, Bulgaria, 7-13 September 2013. head-finding rules from Penn2Malt, for English and for Chinese. The system by Martschat et al. (2012) relies on the Stanford SemanticHeadFinder (also an implementation of the rules by Collins (1999)) for English while the head detection for Chinese is provided by the SunJurafskyChineseHeadFinder (an implementation of the rules presented by Sun and Jurafsky (2004)). Martschat et al. (2012) did not work on CR for Arabic. Uryupina et al. (2012) created their own heuristic rules for the Arabic and Chinese; for English, they used Collins (1999)’s rules. For Arabic, the first noun/pronoun was selected as head; in Chinese, the last noun/pronoun was chosen as the head. Uryupina et al. (2012) also made the observation that the absence of expert linguistic knowledge can become an important obstacle when rules are to be developed manually for each separate language. Additionally, depending on the language, the collection of such rules may be a rather expensive t"
R13-1097,W12-4515,0,0.0577076,"a decision model, such as the mention-pair model (Soon 747 Proceedings of Recent Advances in Natural Language Processing, pages 747–754, Hissar, Bulgaria, 7-13 September 2013. head-finding rules from Penn2Malt, for English and for Chinese. The system by Martschat et al. (2012) relies on the Stanford SemanticHeadFinder (also an implementation of the rules by Collins (1999)) for English while the head detection for Chinese is provided by the SunJurafskyChineseHeadFinder (an implementation of the rules presented by Sun and Jurafsky (2004)). Martschat et al. (2012) did not work on CR for Arabic. Uryupina et al. (2012) created their own heuristic rules for the Arabic and Chinese; for English, they used Collins (1999)’s rules. For Arabic, the first noun/pronoun was selected as head; in Chinese, the last noun/pronoun was chosen as the head. Uryupina et al. (2012) also made the observation that the absence of expert linguistic knowledge can become an important obstacle when rules are to be developed manually for each separate language. Additionally, depending on the language, the collection of such rules may be a rather expensive task. alone subtask of CR, but rather as part of the feature extraction process."
R13-1097,J93-2004,0,0.0417771,"head of the corresponding NP. The major difference lies in the fact that mentions often correspond to maximal rather than to base NPs. If we approach the task of finding the mention heads by identifying syntactic heads, the task would be trivial if we had a full syntactic analysis, as provided in X-bar theory (Chomsky, 1970; Jackendoff, 1977) or in head-driven phrase structure grammar (Sag et al., 2003; Levine and Meurers, 2006). However, treebanks are generally annotated in a more surface-oriented and flat annotation, in which heads of phrases are often not marked as such. The Penn Treebank (Marcus et al., 1993), which is the standard for training statistical parsers for English, for example, uses a flat annotation scheme for NPs, as shown in the examples in (1). The annotation in the Penn Treebank for English also served as the model for the annotations in the Penn Arabic and Chinese treebanks. Related Work While there is a bulk of literature on CR for English (Soon et al., 2001; Ng and Cardie, 2002; Ng, 2007, for example), MCR has only been addressed recently. The majority of work in this area was carried out in the context of the two shared tasks, the SemEval-2010 (Recasens et al., 2010) and the C"
R13-1097,M95-1005,0,0.229207,"endent method that can be employed for any language for which POS information is provided, given a small annotated data set. Table 2: MHD for excerpt data for all languages, Arabic (AR), English (EN), and Chinese (ZH), for all spans of mentions. and uses TiMBL for classification. Since we are more interested in the effects of the MHD methods on the full CR system rather than in the optimal performance that can be achieved by UBIU, we do not aim at language dependent system optimization on any system component. We use the official CoNLL-2012 scorer, which provides five evaluation metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), the two variants of CEAF (Luo, 2005), CEAFE and CEAFM , and BLANC (Recasens and Hovy, 2011). For comparison, we calculate a TOTAL score as the average of the F-score of all metrics. 5.2 Intrinsic Evaluation The results in table 2 show an interesting outcome: HeuristicH, which requires only minimal language specific knowledge, leads to the lowest performance across all three languages, with an F-score of 0.83 for Arabic, 0.90 for Chinese, and 0.67 for English. This outcome shows that for Arabic and Chinese, we reach a very competitive performance with a rather si"
R13-1097,S10-1019,1,0.861727,"Missing"
R13-1097,J03-4003,0,\N,Missing
R13-1097,J11-1005,0,\N,Missing
R13-1097,D08-1067,0,\N,Missing
R13-1097,S10-1001,0,\N,Missing
R19-1132,C18-1139,0,0.0217273,"set of experiments, we use a range of classifiers including random forest, SVM, XGBoost and a neural network approach. For the former three classifiers, we use scikitlearn (Pedregosa et al., 2011), for the neural network architecture tensorflow’s Keras API (Abadi et al., 2016). Based on previous research on related tasks (Park and Fung, 2017; Badjatiya et al., 2017), we experiment with several promising architectures, including fully connected neural networks and convolutional neural networks, along with different word embeddings, with both BERT 1153 and Flair embeddings (Devlin et al., 2018; Akbik et al., 2018), stand-alone and stacked respectively. For the scikit-learn classifiers, we optimized hyper-parameters using grid search. For the neural networks, dropout and batch normalization techniques are applied, and architecture selection and hyper-parameter optimization are done in a nonexhaustive search. Features. We use simple character n-grams along with stemmed word n-grams and dependency parse-derived features. Stemming. Since we were unable to identify a good lemmatizer for German Twitter data, we decided to implement a stemmer for both the English and German data, in order to maintain compatib"
R19-1132,S19-2007,0,0.0829773,"Missing"
R19-1132,W18-4409,0,0.0160538,"buted improvements to a given language (e.g., network features boosting English and Portuguese). Several systems for detecting abusive language in Hindi and English were developed as part of the 2018 Trolling, Aggression and Cyberbullying shared task (Kumar et al., 2018). This shared task used data from Facebook and Twitter. The systems had to label examples as ‘Not Aggressive’, ‘Covertly Aggressive’, and ‘Overtly Aggressive’. Modha et al. (2018) achieved the best results on the Hindi side of the shared task using a convolutional neural network with fastText embeddings (Mikolov et al., 2018). Galery et al. (2018) identified abusive language in the English portion of the corpus as their initial survey found code-switching between Hindi and English. To address this codeswitching, they used fastText embeddings for both languages and a SVD transformation of these two sets of embeddings to generate multilingual embeddings using sub-word units. These multilingual embeddings were used in a GRU-based recurrent neural network. They found that this approach was not effective on their particular data set due to 1152 1 This is the same English corpus used in the present work. English These girls are the equivalen"
R19-1132,D14-1108,0,0.0572723,"Missing"
R19-1132,C10-3009,0,0.0158134,"Missing"
R19-1132,W18-4401,0,0.0928963,"om the “source” language, the implications of our findings are far reaching and necessitate further investigation into the issues of multilingual abusive language detection. The remainder of the paper is structured as follows: We discuss related work in section 2 and the data sets in section 3. Then, we explain our experimental setup and feature sets in section 4. Section 5 presents the results, and section 6 draws conclusions and discusses future work. 2 Related Work Research on abusive language detection has recently drawn much attention, as several recent shared tasks (Basile et al., 2019; Kumar et al., 2018; Wiegand et al., 2018; Zampieri et al., 2019) demonstrate. So far, research has mostly focused on English. For a comprehensive survey of NLP techniques to detect hate speech see (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). Here, we will focus on issues relating to problems in multilingual abusive language detection. There is work on abusive language detection in Arabic, Dutch, and German. For Arabic, Mubarak et al. (2017) developed a data set of abusive language by creating an abusive word list and then splitting their Twitter dump into abusive and nonabusive classes by user, where u"
R19-1132,W18-5113,0,0.0413076,"Missing"
R19-1132,W18-5110,0,0.0427798,"Missing"
R19-1132,W14-5902,1,0.895939,"Missing"
R19-1132,L18-1008,0,0.0580327,"ic user features contributed improvements to a given language (e.g., network features boosting English and Portuguese). Several systems for detecting abusive language in Hindi and English were developed as part of the 2018 Trolling, Aggression and Cyberbullying shared task (Kumar et al., 2018). This shared task used data from Facebook and Twitter. The systems had to label examples as ‘Not Aggressive’, ‘Covertly Aggressive’, and ‘Overtly Aggressive’. Modha et al. (2018) achieved the best results on the Hindi side of the shared task using a convolutional neural network with fastText embeddings (Mikolov et al., 2018). Galery et al. (2018) identified abusive language in the English portion of the corpus as their initial survey found code-switching between Hindi and English. To address this codeswitching, they used fastText embeddings for both languages and a SVD transformation of these two sets of embeddings to generate multilingual embeddings using sub-word units. These multilingual embeddings were used in a GRU-based recurrent neural network. They found that this approach was not effective on their particular data set due to 1152 1 This is the same English corpus used in the present work. English These g"
R19-1132,W18-4423,0,0.0195659,"e tweet) along with a standard set of word and character n-gram features using logistic regression. They noted slight improvements but only specific user features contributed improvements to a given language (e.g., network features boosting English and Portuguese). Several systems for detecting abusive language in Hindi and English were developed as part of the 2018 Trolling, Aggression and Cyberbullying shared task (Kumar et al., 2018). This shared task used data from Facebook and Twitter. The systems had to label examples as ‘Not Aggressive’, ‘Covertly Aggressive’, and ‘Overtly Aggressive’. Modha et al. (2018) achieved the best results on the Hindi side of the shared task using a convolutional neural network with fastText embeddings (Mikolov et al., 2018). Galery et al. (2018) identified abusive language in the English portion of the corpus as their initial survey found code-switching between Hindi and English. To address this codeswitching, they used fastText embeddings for both languages and a SVD transformation of these two sets of embeddings to generate multilingual embeddings using sub-word units. These multilingual embeddings were used in a GRU-based recurrent neural network. They found that"
R19-1132,N16-2013,0,0.0275058,"ned on a subset of features (such as TF-IDF and character ngrams). The predictions were then combined with a maximum entropy model for a final prediction. They found that strategies such as feature selection, sampling, and extensive preprocessing ultimately reduced performance in their ensemble. While most work focuses on identifying abusive language in a specific language, Fehn Unsv˚ag and Gamb¨ack (2018) examined how a single approach can handle abusive language across multiple languages: English, Portuguese, and German. They used a number of existing twitter corpora including the corpus by Waseem and Hovy (2016) for English1 , the one by Ross et al. (2016) for German, and the one by Fortuna (2017) for Portuguese. They also incorporated “user features” (i.e., specific demographic information known about the author of the tweet) along with a standard set of word and character n-gram features using logistic regression. They noted slight improvements but only specific user features contributed improvements to a given language (e.g., network features boosting English and Portuguese). Several systems for detecting abusive language in Hindi and English were developed as part of the 2018 Trolling, Aggression"
R19-1132,W17-3008,0,0.0182931,"nd discusses future work. 2 Related Work Research on abusive language detection has recently drawn much attention, as several recent shared tasks (Basile et al., 2019; Kumar et al., 2018; Wiegand et al., 2018; Zampieri et al., 2019) demonstrate. So far, research has mostly focused on English. For a comprehensive survey of NLP techniques to detect hate speech see (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). Here, we will focus on issues relating to problems in multilingual abusive language detection. There is work on abusive language detection in Arabic, Dutch, and German. For Arabic, Mubarak et al. (2017) developed a data set of abusive language by creating an abusive word list and then splitting their Twitter dump into abusive and nonabusive classes by user, where users are considered abusive if they have used at least one word from the list. They also created an algorithm to extend the list of abusive words. Cyberbullying detection in Dutch social media was performed by Van Hee et al. (2015). They collected data from Ask.fm and annotated it with 7 categories including ‘Threat’, ‘Insult’, and ‘Defamation’. For their classifier, they used a bag-of-words approach for word and character ngrams a"
R19-1132,S19-2010,0,0.0343308,"of our findings are far reaching and necessitate further investigation into the issues of multilingual abusive language detection. The remainder of the paper is structured as follows: We discuss related work in section 2 and the data sets in section 3. Then, we explain our experimental setup and feature sets in section 4. Section 5 presents the results, and section 6 draws conclusions and discusses future work. 2 Related Work Research on abusive language detection has recently drawn much attention, as several recent shared tasks (Basile et al., 2019; Kumar et al., 2018; Wiegand et al., 2018; Zampieri et al., 2019) demonstrate. So far, research has mostly focused on English. For a comprehensive survey of NLP techniques to detect hate speech see (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). Here, we will focus on issues relating to problems in multilingual abusive language detection. There is work on abusive language detection in Arabic, Dutch, and German. For Arabic, Mubarak et al. (2017) developed a data set of abusive language by creating an abusive word list and then splitting their Twitter dump into abusive and nonabusive classes by user, where users are considered abusive if they have used"
R19-1132,W02-1011,0,0.0297879,"language detection has focused on English (Schmidt and 2. Can we determine which types of features are necessary for a classifier? Are the types of features and the number of features comparable across the two languages? 3. The data sets are skewed towards non-abusive language, and research in sentiment analysis has shown that over-sampling methods can improve results (Liu et al., 2014). Thus, do over-sampling methods show consistent results across both languages? 4. For tasks related to sentiment analysis, it is often the case that a classifier learns topic information rather that sentiment (Pang et al., 2002). We investigate whether the two languages show similar effects with regard to topics. 1151 Proceedings of Recent Advances in Natural Language Processing, pages 1151–1160, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_132 Our results show that the data sets differ in their answers to questions 1–3, only showing similarities with regard to topics, leading us to the the preliminary conclusion that we cannot transfer methodology across languages and data sets when the data sets for the individual languages have been collected opportunistically. Since it is highly unli"
R19-1132,W17-3006,0,0.018618,"e classifiers since statistical classifiers tend to predict the majority class. 4 Methodology We have developed two pipelines for detecting abusive language in tweets: One pipeline is trained on German data and the other is trained on English data. Classifiers. For the first set of experiments, we use a range of classifiers including random forest, SVM, XGBoost and a neural network approach. For the former three classifiers, we use scikitlearn (Pedregosa et al., 2011), for the neural network architecture tensorflow’s Keras API (Abadi et al., 2016). Based on previous research on related tasks (Park and Fung, 2017; Badjatiya et al., 2017), we experiment with several promising architectures, including fully connected neural networks and convolutional neural networks, along with different word embeddings, with both BERT 1153 and Flair embeddings (Devlin et al., 2018; Akbik et al., 2018), stand-alone and stacked respectively. For the scikit-learn classifiers, we optimized hyper-parameters using grid search. For the neural networks, dropout and batch normalization techniques are applied, and architecture selection and hyper-parameter optimization are done in a nonexhaustive search. Features. We use simple"
R19-1132,W17-1101,0,0.043077,"er is structured as follows: We discuss related work in section 2 and the data sets in section 3. Then, we explain our experimental setup and feature sets in section 4. Section 5 presents the results, and section 6 draws conclusions and discusses future work. 2 Related Work Research on abusive language detection has recently drawn much attention, as several recent shared tasks (Basile et al., 2019; Kumar et al., 2018; Wiegand et al., 2018; Zampieri et al., 2019) demonstrate. So far, research has mostly focused on English. For a comprehensive survey of NLP techniques to detect hate speech see (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). Here, we will focus on issues relating to problems in multilingual abusive language detection. There is work on abusive language detection in Arabic, Dutch, and German. For Arabic, Mubarak et al. (2017) developed a data set of abusive language by creating an abusive word list and then splitting their Twitter dump into abusive and nonabusive classes by user, where users are considered abusive if they have used at least one word from the list. They also created an algorithm to extend the list of abusive words. Cyberbullying detection in Dutch social media was performe"
R19-1132,R15-1086,0,0.156688,"Missing"
S10-1019,A00-1020,0,0.280518,"Missing"
S10-1019,J05-3004,0,0.013223,"detecting full coreference chains, composed of named entities, pronouns, and full noun phrases which makes use of memory based learning and a feature model following Rahman and Ng (2009). UBIU is evaluated on the task “Coreference Resolution in Multiple Languages” (SemEval Task 1 (Recasens et al., 2010)) in the context of the 5th International Workshop on Semantic Evaluation. 1 UBIU: System Structure Introduction Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf. e.g. (Soon et al., 2001; Ng and Cardie, 2002)). However, machine learning based coreference resolution is only possible for a very small number of languages. In order to make such resources available for a wider range of languages, language independent systems are often regarded as a partial solution. To this day, there have been only a few systems reported that work on multiple languages (Mitkov, 1999; Harabagiu and Maiorano, 2000; Luo and Zitouni, 2005). However, all of those systems were geared towards predefined language sets. In this paper, w"
S10-1019,P98-2143,0,0.215862,"Missing"
S10-1019,P02-1014,0,0.0899511,"hich makes use of memory based learning and a feature model following Rahman and Ng (2009). UBIU is evaluated on the task “Coreference Resolution in Multiple Languages” (SemEval Task 1 (Recasens et al., 2010)) in the context of the 5th International Workshop on Semantic Evaluation. 1 UBIU: System Structure Introduction Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf. e.g. (Soon et al., 2001; Ng and Cardie, 2002)). However, machine learning based coreference resolution is only possible for a very small number of languages. In order to make such resources available for a wider range of languages, language independent systems are often regarded as a partial solution. To this day, there have been only a few systems reported that work on multiple languages (Mitkov, 1999; Harabagiu and Maiorano, 2000; Luo and Zitouni, 2005). However, all of those systems were geared towards predefined language sets. In this paper, we present a language independent system that does require syntactic resources for each langu"
S10-1019,poesio-etal-2002-acquiring,0,0.190586,"Missing"
S10-1019,D09-1101,0,0.78119,"a Kubler Indiana University skuebler@indiana.edu Desislava Zhekova University of Bremen zhekova@uni-bremen.de Abstract 2 The UBIU system aims at being a languageindependent system in that it uses a combination of machine learning, in the form of memory-based learning (MBL) in the implementation of TiMBL (Daelemans et al., 2007), and language independent features. MBL uses a similarity metric to find the k nearest neighbors in the training data in order to classify a new example, and it has been shown to work well for NLP problems (Daelemans and van den Bosch, 2005). Similar to the approach by Rahman and Ng (2009), classification in UBUI is based on mention pairs (having been shown to work well for German (Wunsch, 2009)) and uses as features standard types of linguistic annotation that are available for a wide range of languages and are provided by the task. Figure 1 shows an overview of the system. In preprocessing, we slightly change the formatting of the data in order to make it suitable for the next step in which language dependent feature extraction modules are used, from which the training and test sets for the classification are extracted. Our approach is untypical in that it first extracts the"
S10-1019,W09-2411,0,0.147097,"Missing"
S10-1019,J01-4004,0,0.437039,"full noun phrases which makes use of memory based learning and a feature model following Rahman and Ng (2009). UBIU is evaluated on the task “Coreference Resolution in Multiple Languages” (SemEval Task 1 (Recasens et al., 2010)) in the context of the 5th International Workshop on Semantic Evaluation. 1 UBIU: System Structure Introduction Coreference resolution is a field in which major progress has been made in the last decade. After a concentration on rule-based systems (cf. e.g. (Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf. e.g. (Soon et al., 2001; Ng and Cardie, 2002)). However, machine learning based coreference resolution is only possible for a very small number of languages. In order to make such resources available for a wider range of languages, language independent systems are often regarded as a partial solution. To this day, there have been only a few systems reported that work on multiple languages (Mitkov, 1999; Harabagiu and Maiorano, 2000; Luo and Zitouni, 2005). However, all of those systems were geared towards predefined language sets. In this paper, we present a language independent system that does require syntactic re"
S10-1019,H05-1083,0,\N,Missing
S10-1019,D08-1067,0,\N,Missing
S10-1019,C98-2138,0,\N,Missing
S10-1019,S10-1001,0,\N,Missing
S14-2060,P03-1021,0,0.00422352,"near model, taking the log of each score – the direct and inverse translation probabilities, the LM probability, and the surface and POS SIM scores – and producing a weighted sum. Since the original scores are either probabilities or probability-like (in the range [0, 1]), their logs are negative numbers, and at translation time we return the translation (or n-best) with the highest (least negative) score. This leaves us with the question of how to set the weights for the log-linear model; in this work, we use the ZMERT package (Zaidan, 2009), which implements the MERT optimization algorithm (Och, 2003), iteratively tuning the feature weights by repeatedly requesting n-best lists from the system. We used ZMERT with its default settings, optimizing our system’s BLEU scores on the provided development set. We chose, for convenience, BLEU as a stand-in for the wordlevel accuracy score, as BLEU scores are maximized when the system output matches the reference translations. 4 5 Conclusion We have described our entry for the initial running of the “L2 Writing Assistant” task and explained some possible extensions to our base loglinear model system. In developing the SIM extensions, we faced some i"
S14-2060,E12-1009,0,0.0202846,"Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Ney, 2000) and the"
S14-2060,J13-1004,0,0.0143479,"edings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Ney, 2000) and then extract phrases with t"
S14-2060,C10-1011,0,0.0263733,"ional License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ 356 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356–360, Dublin, Ireland, August 23-24, 2014. Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section"
S14-2060,de-marneffe-etal-2006-generating,0,0.0750578,"Missing"
S14-2060,S13-1035,0,0.0138536,"n, in cases where an L1 phrase contained words that were neither in our training data nor BabelNet (and thus were simply outof-vocabulary for our system), we took the first translation for that phrase, without regard to context, from Google Translate, through the semiautomated Google Docs interface. This approach is not particularly scalable or reproducible, but simulates what a user might do in such a situation. Google Books Syntactic N-grams For English, we also obtained dependency relationships for our word similarity statistics using the arcs dataset of the Google Books Syntactic N-Grams (Goldberg and Orwant, 2013), which has 919M items, each of which is a small “syntactic n-gram”, a term Goldberg and Orwant use to describe short dependency chains, each of which may contain several tokens. This data set does not contain the actual parses of books from the Google Books corpus, but counts of these dependency chains. We converted the longer chains into their component (head, dependent, label) triples and then collated these triples into counts, also for use in the SIM system. 3 System Design As previously mentioned, at run-time, our system decomposes the fragment translation task into two parts: generating"
S14-2060,W11-2123,0,0.0168098,"built with the training scripts packaged with Moses (Koehn et al., 2007). These scripts preprocess the bitext, estimate word alignments with GIZA++ (Och and 3.2 Scoring Candidate Translations via a L2 Language Model To model how well a phrase fits into the L2 context, we score candidates with an n-gram lan1 http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor 357 guage model (LM) trained on a large sample of target-language text. Constructing and querying a large language model is potentially computationally expensive, so here we use the KenLM Language Model Toolkit and its Python interface (Heafield, 2011). Here our models were trained on the Wikipedia text mentioned previously (without filtering long sentences), with KenLM set to 5-grams and the default settings. fitness of the phrase “eat with chopsticks” would be computed as: 3.3 Since we consider the heads and dependents of a target phrase component, these may be situated inside or outside the phrase. Both cases are included in our calculation, thus enabling us to consider a broader, syntactically determined local context of the phrase. By basing the calculation on a single word’s head and dependents, we attempt to avoid data sparseness iss"
S14-2060,P03-1054,0,0.00464979,"or German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.1 To save time during parsing, sentences longer than 25 words are removed. The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013). To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump. Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006). The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences. Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3. Ney, 2000) and then extract phrases with the grow-diag-final-and heuristic. At translation time, we look for the given source-language phrase in the phrase table, and if it is found, we take all translations of that phrase as our candidates. When translating a phrase that is not found in the phrase"
S14-2060,P07-2045,0,0.00570498,"counts of these dependency chains. We converted the longer chains into their component (head, dependent, label) triples and then collated these triples into counts, also for use in the SIM system. 3 System Design As previously mentioned, at run-time, our system decomposes the fragment translation task into two parts: generating many possible candidate translations, then scoring and ranking them in the targetlanguage context. 3.1 Constructing Candidate Translations As a starting point, we use phrase tables constructed in typical SMT fashion, built with the training scripts packaged with Moses (Koehn et al., 2007). These scripts preprocess the bitext, estimate word alignments with GIZA++ (Och and 3.2 Scoring Candidate Translations via a L2 Language Model To model how well a phrase fits into the L2 context, we score candidates with an n-gram lan1 http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor 357 guage model (LM) trained on a large sample of target-language text. Constructing and querying a large language model is potentially computationally expensive, so here we use the KenLM Language Model Toolkit and its Python interface (Heafield, 2011). Here our models were trained on the Wikipedia text ment"
S14-2060,2005.mtsummit-papers.11,0,0.00753831,"ng collocational relationships between tokens anywhere in the L2 context and the proposed fragment translations. Our system proceeds in several stages: (1) looking up or constructing candidate translations for the L1 fragment, (2) scoring candidate translations via a language model of the L2, (3) scoring candidate translations with a dependency-driven word similarity measure (Lin, 1998) (which we call SIM), and (4) combining the previous scores in a log-linear model to arrive at a final n-best list. Step 1 models transfer knowledge between Europarl The Europarl Parallel Corpus (Europarl, v7) (Koehn, 2005) is a corpus of proceedings of the European Parliament, containing 21 European languages with sentence alignments. From this corpus, we build phrase tables for English-Spanish, English-German, FrenchEnglish, Dutch-English. BabelNet In the cases where the constructed phrase tables do not contain a translation for a source phrase, we need to back off to smaller phrases and find candidate translations for these components. To better handle sparsity, we extend look-up using the multilingual dictionary BabelNet, v2.0 (Navigli and Ponzetto, 2012) as a way to find translation candidates. This work is"
S14-2060,P00-1056,0,0.048067,"Missing"
S16-1064,W11-1701,0,0.0144464,"the shared task (see Mohammad et al. (2016) for details about the shared task), we interpret it as a variant of sentiment analysis and adopt an approach that combines shallow lexical features with an ensemble of different supervised machine learning classifiers. Previous work has shown that using “arguing” features based on an arguing lexicon along with modal verbs and targets identified via syntactic rules (Somasundaran and Wiebe, 2010); finding polarized relations between aspects and topics (Somasundaran and Wiebe, 2009); adding semantic frames (Hasan and Ng, 2013) and contextual features (Anand et al., 2011) generally improve results. Since some of these features do not generalize across targets (Anand et al., 2011), and since we have an additional challenge in processing Twitter data, we rely on unigram features and word vectors. This means that our 2.1 Preprocessing Preprocessing mostly consists of tokenization. During tokenization, we normalize capitalization, and all punctuation signs are separated except for @ and #, as these symbols indicate hashtags and handles. We extract frequency counts of each token in the entire corpus and in each stance (Favor, Against, None) per target for use in th"
S16-1064,I13-1191,0,0.0113344,"s in favor of or against a specific issue. In the shared task (see Mohammad et al. (2016) for details about the shared task), we interpret it as a variant of sentiment analysis and adopt an approach that combines shallow lexical features with an ensemble of different supervised machine learning classifiers. Previous work has shown that using “arguing” features based on an arguing lexicon along with modal verbs and targets identified via syntactic rules (Somasundaran and Wiebe, 2010); finding polarized relations between aspects and topics (Somasundaran and Wiebe, 2009); adding semantic frames (Hasan and Ng, 2013) and contextual features (Anand et al., 2011) generally improve results. Since some of these features do not generalize across targets (Anand et al., 2011), and since we have an additional challenge in processing Twitter data, we rely on unigram features and word vectors. This means that our 2.1 Preprocessing Preprocessing mostly consists of tokenization. During tokenization, we normalize capitalization, and all punctuation signs are separated except for @ and #, as these symbols indicate hashtags and handles. We extract frequency counts of each token in the entire corpus and in each stance (F"
S16-1064,D14-1108,0,0.0302988,"generalize across targets (Anand et al., 2011), and since we have an additional challenge in processing Twitter data, we rely on unigram features and word vectors. This means that our 2.1 Preprocessing Preprocessing mostly consists of tokenization. During tokenization, we normalize capitalization, and all punctuation signs are separated except for @ and #, as these symbols indicate hashtags and handles. We extract frequency counts of each token in the entire corpus and in each stance (Favor, Against, None) per target for use in the feature selection process. We experimented with TWEEBOPARSER (Kong et al., 2014), a dependency parser specifically designed for Twitter data, to extract dependency relations among words. We extract POS tags, multiword expressions, and dependency triples from the parses. However, due to the feature sparsity, none of them improved over unigrams. Thus, they are not used in the final systems. 2.2 Features One of the major decisions in developing a machine learning system for stance detection lies in 394 Proceedings of SemEval-2016, pages 394–400, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Model GBDT random forest SVM ensembleG en"
S16-1064,W14-5902,1,0.796707,"Missing"
S16-1064,S16-1003,0,0.0288689,"he IUCL system, based on supervised learning, for the shared task on stance detection. Our official submission, the random forest model, reaches a score of 63.60, and is ranked 6th out of 19 teams. We also use gradient boosting decision trees and SVM and merge all classifiers into an ensemble method. Our analysis shows that random forest is good at retrieving minority classes and gradient boosting majority classes. The strengths of different classifiers wrt. precision and recall complement each other in the ensemble. 1 2 Methods We use the data sets provided by the SemEval-2016 shared task 6 (Mohammad et al., 2016). Introduction Stance detection is a difficult task since it often requires reasoning in order to determine whether an utterance is in favor of or against a specific issue. In the shared task (see Mohammad et al. (2016) for details about the shared task), we interpret it as a variant of sentiment analysis and adopt an approach that combines shallow lexical features with an ensemble of different supervised machine learning classifiers. Previous work has shown that using “arguing” features based on an arguing lexicon along with modal verbs and targets identified via syntactic rules (Somasundaran"
S16-1064,D14-1162,0,0.0794968,"Missing"
S16-1064,P09-1026,0,0.0200404,"reasoning in order to determine whether an utterance is in favor of or against a specific issue. In the shared task (see Mohammad et al. (2016) for details about the shared task), we interpret it as a variant of sentiment analysis and adopt an approach that combines shallow lexical features with an ensemble of different supervised machine learning classifiers. Previous work has shown that using “arguing” features based on an arguing lexicon along with modal verbs and targets identified via syntactic rules (Somasundaran and Wiebe, 2010); finding polarized relations between aspects and topics (Somasundaran and Wiebe, 2009); adding semantic frames (Hasan and Ng, 2013) and contextual features (Anand et al., 2011) generally improve results. Since some of these features do not generalize across targets (Anand et al., 2011), and since we have an additional challenge in processing Twitter data, we rely on unigram features and word vectors. This means that our 2.1 Preprocessing Preprocessing mostly consists of tokenization. During tokenization, we normalize capitalization, and all punctuation signs are separated except for @ and #, as these symbols indicate hashtags and handles. We extract frequency counts of each tok"
S16-1064,W10-0214,0,0.0199282,"t al., 2016). Introduction Stance detection is a difficult task since it often requires reasoning in order to determine whether an utterance is in favor of or against a specific issue. In the shared task (see Mohammad et al. (2016) for details about the shared task), we interpret it as a variant of sentiment analysis and adopt an approach that combines shallow lexical features with an ensemble of different supervised machine learning classifiers. Previous work has shown that using “arguing” features based on an arguing lexicon along with modal verbs and targets identified via syntactic rules (Somasundaran and Wiebe, 2010); finding polarized relations between aspects and topics (Somasundaran and Wiebe, 2009); adding semantic frames (Hasan and Ng, 2013) and contextual features (Anand et al., 2011) generally improve results. Since some of these features do not generalize across targets (Anand et al., 2011), and since we have an additional challenge in processing Twitter data, we rely on unigram features and word vectors. This means that our 2.1 Preprocessing Preprocessing mostly consists of tokenization. During tokenization, we normalize capitalization, and all punctuation signs are separated except for @ and #,"
S19-2138,W18-4417,0,0.176788,"Missing"
S19-2138,D14-1162,0,0.0860861,"n to achieve good performance in offensive language identification tasks (Badjatiya et al., 2017). Benchmarks of the first shared task on aggression identification (Kumar et al., 2018) show that half of the top 15 systems are trained on neural networks. Using pre-trained word embeddings for feature extraction has been shown to be highly effective in multiple NLP tasks. Traditional word embeddings are extracted from shallow neural networks trained on a large swathes of texts required to learn the contextual representations of words. Examples include skip-grams (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, these embeddings are learned from an aggregation of all possiIntroduction With the increased influence of social media on modern society, large amounts of user-generated content emerge on the internet. Besides the exchange of ideas, we also see an exponential increase of aggressive and potentially harmful content, for example, hate speech. If we consider the amount of user-generated data, it is impractical to manually identify the malicious speech. Thus we need to develop methods to detect offensive speech automatically through computational models. However, this task is challenging"
S19-2138,N18-1202,0,0.0296065,"r Computational Linguistics by empirical validation on the trial data, to utilize different methods for the subtasks, namely, BERT embeddings for subtask A, and an SVM classifier for subtasks B and C. The data collection method used to compile the dataset in OffensEval is described by Zampieri et al. (2019a). We used the official training data and trial data provided by the shared task to train the classifier. Our implmentations can be found at: https://github.com/zytian9/SemEval-2019-Task-6. ble word contexts, which may gloss over semantic nuances in representations. Recent models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) significantly advanced the state-of-the-art in language modeling by learning context-sensitive representations of words. ELMo goes beyond word embeddings by learning representations that are functions of the entire input sentence (Peters et al., 2018). However, ELMo is still considered shallow with two bidirectional LSTM layers, and more recent transformer based language models such as the OpenAI Generative Pre-trained Transformer (GPT) (Radford et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) have been exten"
S19-2138,W17-1101,0,0.217681,"ubtask A, we fine-tuned a BERT based classifier to detect abusive content in tweets, achieving a macro F1 score of 0.8136 on the test data, thus reaching the 3rd rank out of 103 submissions. In subtasks B and C, we used a linear SVM with selected character n-gram features. For subtask C, our system could identify the target of abuse with a macro F1 score of 0.5243, ranking it 27th out of 65 submissions. 1 ¨ Sandra Kubler Department of Linguistics Indiana University Bloomington, IN, USA skuebler@indiana.edu 2 Related Work Detecting offensive language online is becoming more and more important (Schmidt and Wiegand, 2017; Founta et al., 2018; Malmasi and Zampieri, 2018). To build an effective classifier, one of the major problems is to find the appropriate features. Normally, two types of features are utilized: surface features like n-grams and word representations trained by neural network. Most offensive language classifiers are trained on different types of surface features with approaches like SVM (Malmasi and Zampieri, 2018; Arroyo-Fern´andez et al., 2018), Random Forest (Burnap and Williams, 2015), and Logistic Regression (Davidson et al., 2017). Recently, word embeddings trained in neural networks have"
S19-2138,W18-4401,0,0.284559,"tures are utilized: surface features like n-grams and word representations trained by neural network. Most offensive language classifiers are trained on different types of surface features with approaches like SVM (Malmasi and Zampieri, 2018; Arroyo-Fern´andez et al., 2018), Random Forest (Burnap and Williams, 2015), and Logistic Regression (Davidson et al., 2017). Recently, word embeddings trained in neural networks have been shown to achieve good performance in offensive language identification tasks (Badjatiya et al., 2017). Benchmarks of the first shared task on aggression identification (Kumar et al., 2018) show that half of the top 15 systems are trained on neural networks. Using pre-trained word embeddings for feature extraction has been shown to be highly effective in multiple NLP tasks. Traditional word embeddings are extracted from shallow neural networks trained on a large swathes of texts required to learn the contextual representations of words. Examples include skip-grams (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). However, these embeddings are learned from an aggregation of all possiIntroduction With the increased influence of social media on modern society, large amoun"
S19-2138,N19-1144,0,0.0481999,"perform well, either because of limited training data or because we did not find the appropriate hyperparameters. Thus we use an SVM classifier 788 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 788–795 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics by empirical validation on the trial data, to utilize different methods for the subtasks, namely, BERT embeddings for subtask A, and an SVM classifier for subtasks B and C. The data collection method used to compile the dataset in OffensEval is described by Zampieri et al. (2019a). We used the official training data and trial data provided by the shared task to train the classifier. Our implmentations can be found at: https://github.com/zytian9/SemEval-2019-Task-6. ble word contexts, which may gloss over semantic nuances in representations. Recent models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) significantly advanced the state-of-the-art in language modeling by learning context-sensitive representations of words. ELMo goes beyond word embeddings by learning representations that are functions of the entire input sentence (Peters et al., 2018). Ho"
S19-2138,S19-2010,0,0.0593769,"perform well, either because of limited training data or because we did not find the appropriate hyperparameters. Thus we use an SVM classifier 788 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 788–795 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics by empirical validation on the trial data, to utilize different methods for the subtasks, namely, BERT embeddings for subtask A, and an SVM classifier for subtasks B and C. The data collection method used to compile the dataset in OffensEval is described by Zampieri et al. (2019a). We used the official training data and trial data provided by the shared task to train the classifier. Our implmentations can be found at: https://github.com/zytian9/SemEval-2019-Task-6. ble word contexts, which may gloss over semantic nuances in representations. Recent models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) significantly advanced the state-of-the-art in language modeling by learning context-sensitive representations of words. ELMo goes beyond word embeddings by learning representations that are functions of the entire input sentence (Peters et al., 2018). Ho"
telljohann-etal-2004-tuba,W98-1207,0,\N,Missing
telljohann-etal-2004-tuba,C02-1131,0,\N,Missing
W05-0303,J00-4005,0,0.287918,"Missing"
W05-0303,P89-1032,0,0.135142,"annot be anaphorically related to the object NP Ihre Schulkameradin Cassie Bernall since they are coarguments of the same verb. However, the possessive pronoun ihre and the subject pronoun sie of the subordinate clause, can be and, in fact, are anaphorically related, since they are not co-arguments of the same verb. This can be directly inferred from the treebank annotation, specifically from the sentence structure and the grammatical function information encoded on the edge labels. Most published computational algorithms of anaphora resolution, including (Hobbs, 1978; Lappin and Leass, 1994; Ingria and Stallard, 1989), rely on such binding-constraint filters to minimize the set of potential antecedents for pronouns and reflexives. As already pointed out, the sample sentence contains four markables: one possessive pronoun Ihre, two occurrences of the pronoun sie and one complex NP Ihre Schulkameradin Cassie Bernall. The latter NP is a good example of SYN-RA’s longest-match principle for identifying markables. In case of complex NPs, the entire NP counts as a markable, but so do its subconstituents – in the case at hand, particularly the possessive pronoun ihre. All of this information can be directly derive"
W05-0303,J94-4002,0,0.0249672,"the main clause, sie, cannot be anaphorically related to the object NP Ihre Schulkameradin Cassie Bernall since they are coarguments of the same verb. However, the possessive pronoun ihre and the subject pronoun sie of the subordinate clause, can be and, in fact, are anaphorically related, since they are not co-arguments of the same verb. This can be directly inferred from the treebank annotation, specifically from the sentence structure and the grammatical function information encoded on the edge labels. Most published computational algorithms of anaphora resolution, including (Hobbs, 1978; Lappin and Leass, 1994; Ingria and Stallard, 1989), rely on such binding-constraint filters to minimize the set of potential antecedents for pronouns and reflexives. As already pointed out, the sample sentence contains four markables: one possessive pronoun Ihre, two occurrences of the pronoun sie and one complex NP Ihre Schulkameradin Cassie Bernall. The latter NP is a good example of SYN-RA’s longest-match principle for identifying markables. In case of complex NPs, the entire NP counts as a markable, but so do its subconstituents – in the case at hand, particularly the possessive pronoun ihre. All of this inform"
W05-0303,W03-2117,0,0.162936,"es and anaphoric uses with an NP antecedent, the pronoun es can also be used in cases of event anaphora as in sentence (9). Here es refers to the event of Jochen’s winning the lottery. Currently, the annotation in SYNRA is restricted to NP anaphora and therefore event anaphors such as in sentence (9) remain unannotated for anaphora. (9) Jochen hat im Lotto gewonnen. Aber er Jochen has in the lottery won. But he weiss es noch nicht. knows it yet not. ’Jochen has won the lottery. But he does not know it yet.’ The annotation of such relations is performed manually with the annotation tool MMAX (Müller and Strube, 2003). Its graphical user interface allows for easy selection of the relevant markables and the accompanying relation between the contextually dependent expression and its antecedent. 15 3 Automatic Extraction of Markables and of Semantic Information Annotation of referential relations involves two main tasks: the identification of markables, i.e., identifying the class of expressions that can enter into referential relations, and the identification of the particular referential relations that two or more expressions enter into. Identification of markables requires at least partial syntactic annota"
W05-0303,W99-0309,0,0.0309254,"tion 1 See e.g. nlp.cs.nyu.edu/meyers/pie-in-the-sky/ analysis5. This section introduces the inventory of referential relations adopted in the SYN-RA project. We define referential relations as a cover-term for all contextually dependent reference relations. The inventory of such relations adopted for SYN-RA is inspired by the annotation scheme first developed in the MATE project (Davies et al., 1998). However, it takes a cautious approach in that it only adopts those referential relations from MATE for which the developers of MATE report a sufficiently high level of interannotator agreement (Poesio et al., 1999). SYN-RA currently uses the following subset of relations: coreferential, anaphoric, cataphoric, bound, split antecedent, instance, and expletive. The potential markables are definite NPs, personal pronouns, relative, reflexive, and reciprocal pronouns, demonstrative, indefinite and possessive pronouns. There is a second research effort under way at the European Media Laboratory Heidelberg, which also annotates German text corpora and dialog data with referential relations. Since their corpora are not publicly available, it is difficult to verify their inventory of referential relations. Kouch"
W05-0303,P04-2010,0,\N,Missing
W06-1110,Y01-1032,0,0.0530383,"Missing"
W06-1110,W96-0102,0,0.0396919,"erality can be reached, depending on which types of information are used to determine the similarity between input sentence and training sentences. The results are such that it is possible to construct a case-based parser. The optimal setting out of those presented here need to be determined empirically. 1 Introduction Linguistic similarity has often been used as a bias in machine learning approaches to Computational Linguistics problems. The success of applying memory-based learning to problems such as POS tagging, named-entity recognition, partial parsing, or word sense disambiguation (cf. (Daelemans et al., 1996; Daelemans et al., 1999; Mooney, 1996; Tjong Kim Sang, 2002; Veenstra et al., 2000)) shows that the bias of this similarity-based approach is suitable for processing natural language problems. In (K¨ubler, 2004a; K¨ubler, 2004b), we extended the application of memory-based learning to full scale parsing, a problem which cannot easily be described as a classification problem. In this approach, the most similar sentence is found in the 2 A Memory-Based Parser The parser in (K¨ubler, 2004a; K¨ubler, 2004b) approaches parsing as the task of finding a complete syntax tree rather than incrementally"
W06-1110,telljohann-etal-2004-tuba,1,0.890699,"Missing"
W06-1110,W02-2025,0,0.0129238,"to determine the similarity between input sentence and training sentences. The results are such that it is possible to construct a case-based parser. The optimal setting out of those presented here need to be determined empirically. 1 Introduction Linguistic similarity has often been used as a bias in machine learning approaches to Computational Linguistics problems. The success of applying memory-based learning to problems such as POS tagging, named-entity recognition, partial parsing, or word sense disambiguation (cf. (Daelemans et al., 1996; Daelemans et al., 1999; Mooney, 1996; Tjong Kim Sang, 2002; Veenstra et al., 2000)) shows that the bias of this similarity-based approach is suitable for processing natural language problems. In (K¨ubler, 2004a; K¨ubler, 2004b), we extended the application of memory-based learning to full scale parsing, a problem which cannot easily be described as a classification problem. In this approach, the most similar sentence is found in the 2 A Memory-Based Parser The parser in (K¨ubler, 2004a; K¨ubler, 2004b) approaches parsing as the task of finding a complete syntax tree rather than incrementally building the tree by rule applications, as in standard PCFG"
W06-1110,W04-2407,0,0.0539465,"Missing"
W06-1110,W96-0208,0,0.038627,"of information are used to determine the similarity between input sentence and training sentences. The results are such that it is possible to construct a case-based parser. The optimal setting out of those presented here need to be determined empirically. 1 Introduction Linguistic similarity has often been used as a bias in machine learning approaches to Computational Linguistics problems. The success of applying memory-based learning to problems such as POS tagging, named-entity recognition, partial parsing, or word sense disambiguation (cf. (Daelemans et al., 1996; Daelemans et al., 1999; Mooney, 1996; Tjong Kim Sang, 2002; Veenstra et al., 2000)) shows that the bias of this similarity-based approach is suitable for processing natural language problems. In (K¨ubler, 2004a; K¨ubler, 2004b), we extended the application of memory-based learning to full scale parsing, a problem which cannot easily be described as a classification problem. In this approach, the most similar sentence is found in the 2 A Memory-Based Parser The parser in (K¨ubler, 2004a; K¨ubler, 2004b) approaches parsing as the task of finding a complete syntax tree rather than incrementally building the tree by rule application"
W06-1110,C02-1131,0,0.0511327,"Missing"
W06-1614,A97-1014,0,\N,Missing
W06-1614,J06-2001,0,\N,Missing
W06-1614,J03-4003,0,\N,Missing
W06-1614,P03-1054,0,\N,Missing
W06-1614,P03-1014,0,\N,Missing
W06-1614,P06-3004,1,\N,Missing
W06-1614,C04-1056,0,\N,Missing
W06-1614,P03-1056,0,\N,Missing
W06-1614,P05-1039,0,\N,Missing
W06-1614,P03-1013,0,\N,Missing
W08-1008,bosco-etal-2000-building,0,0.0249893,"different treebanks and between constituent and dependency representations. In this paper, we describe the task and the data sets. In addition, we provide an overview of the test results and a first analysis. 1 Introduction German is one of the very few languages for which more than one syntactically annotated resource exists. Other languages for which this is the case include English (with the Penn treebank (Marcus et al., 1993), the Susanne Corpus (Sampson, 1993), and the British section of the ICE Corpus (Wallis and Nelson, 2006)) and Italian (with ISST (Montegmagni et al., 2000) and TUT (Bosco et al., 2000)). The three German treebanks are Negra (Skut et al., 1998), T IGER (Brants et al., 2002), and T¨uBa-D/Z (Hinrichs et al., 2004). We will concentrate on T IGER and T¨uBa-D/Z here; Negra is annotated with an annotation scheme very similar to T IGER but is smaller. In contrast to other languages, these two treebanks are similar on many levels: Both treebanks are based on newspaper text, both use the STTS part of speech (POS) tagset (Thielen and Schiller, 1994), and both use an annotation ∗ I am very grateful to Gerald Penn, who suggested this workshop and the shared task, took over the biggest p"
W08-1008,daum-etal-2004-automatic,0,0.138715,"Data The constituent representations from both treebanks were converted into dependencies. The conversion aimed at finding dependency representations for both treebanks that are as similar to each other as possible. Complete identity is impossible because the treebanks contain different levels of distinction for different phenomena. The conversion is based on the original formats of the treebanks including crossing branches. The target dependency format was defined based on the dependency grammar by Foth (2003). For the conversion, we used pre-existing dependency converters for T IGER trees (Daum et al., 2004) and for T¨uBa-D/Z trees (Versley, 2005). The dependency representations of the trees in Figures 1 and 3 are shown in Figures 4 and 5. Note that the long-distance relationships are converted into non-projective dependencies. 5 Submissions and Results The shared task drew submissions from 3 groups: the Berkeley group, the Stanford group, and the V¨axj¨o group. Four more groups or individuals had registered but did not submit any data. The submitted systems and results are described in detail in papers in this volume (Petrov and Klein, 2008; Rafferty and Manning, 2008; Hall and Nivre, 2008). All"
W08-1008,W08-1007,0,0.0821368,"trees (Daum et al., 2004) and for T¨uBa-D/Z trees (Versley, 2005). The dependency representations of the trees in Figures 1 and 3 are shown in Figures 4 and 5. Note that the long-distance relationships are converted into non-projective dependencies. 5 Submissions and Results The shared task drew submissions from 3 groups: the Berkeley group, the Stanford group, and the V¨axj¨o group. Four more groups or individuals had registered but did not submit any data. The submitted systems and results are described in detail in papers in this volume (Petrov and Klein, 2008; Rafferty and Manning, 2008; Hall and Nivre, 2008). All three systems submitted results for the constituent task. For the dependency task, the V¨axj¨o group had the only submission. For this reason, we will concentrate on the analysis of the constituent results and will mention the dependency results only shortly. REL SUBJ AUX SUBJ PP PP DET PN ATTR OBJP PP PN ATTR ATTR PN DET AUX PN DET Beim M. Gipfel ist die sprichw. bayer. Gem. von einem Bild verdr¨angt worden, das im Worts. an einen P.staat erinnert. Figure 4: T IGER dependency annotation. AUX REL SUBJ PP SUBJ PN ADV ADV DET AUX ATTR OBJD In Bremen sind bisher nur Fakten geschaffen worden"
W08-1008,W06-1614,1,0.83609,"Missing"
W08-1008,P06-3004,0,0.0273214,"categories, such as for noun phrases, etc. However, this is not possible for T IGER and T¨uBa-D/Z. On the one 60 hand, the range of phenomena described as noun phrases, for example, is different in the two treebanks. The most obvious difference in annotation schemes is that T¨uBa-D/Z annotates unary branching structures while T IGER does not. As a consequence, in T¨uBa-D/Z, all pronouns and substituting demonstratives are annotated as noun phrases; in T IGER , they are attached directly to the next higher node (cf. the relative pronouns, POS tag PRELS, in Figures 1 and 3). K¨ubler (2005) and Maier (2006) suggest a method for comparing such different annotation schemes by approximating them stepwise so that the decisions which result in major changes can be isolated. They come to the conclusion that the differences between the two annotation schemes is a least partially due to inconsistencies introduced into T IGER style annotations during the resolution of crossing branches. However, even this method cannot give any indication which annotation scheme provides more useful information for systems that use such parses as input. To answer this question, an in vivo evaluation would be necessary. I"
W08-1008,J93-2004,0,0.0310755,". The ACL 2008 Workshop on Parsing German features a shared task on parsing German. The goal of the shared task was to find reasons for the radically different behavior of parsers on the different treebanks and between constituent and dependency representations. In this paper, we describe the task and the data sets. In addition, we provide an overview of the test results and a first analysis. 1 Introduction German is one of the very few languages for which more than one syntactically annotated resource exists. Other languages for which this is the case include English (with the Penn treebank (Marcus et al., 1993), the Susanne Corpus (Sampson, 1993), and the British section of the ICE Corpus (Wallis and Nelson, 2006)) and Italian (with ISST (Montegmagni et al., 2000) and TUT (Bosco et al., 2000)). The three German treebanks are Negra (Skut et al., 1998), T IGER (Brants et al., 2002), and T¨uBa-D/Z (Hinrichs et al., 2004). We will concentrate on T IGER and T¨uBa-D/Z here; Negra is annotated with an annotation scheme very similar to T IGER but is smaller. In contrast to other languages, these two treebanks are similar on many levels: Both treebanks are based on newspaper text, both use the STTS part of s"
W08-1008,W00-1903,0,0.0223183,"ifferent behavior of parsers on the different treebanks and between constituent and dependency representations. In this paper, we describe the task and the data sets. In addition, we provide an overview of the test results and a first analysis. 1 Introduction German is one of the very few languages for which more than one syntactically annotated resource exists. Other languages for which this is the case include English (with the Penn treebank (Marcus et al., 1993), the Susanne Corpus (Sampson, 1993), and the British section of the ICE Corpus (Wallis and Nelson, 2006)) and Italian (with ISST (Montegmagni et al., 2000) and TUT (Bosco et al., 2000)). The three German treebanks are Negra (Skut et al., 1998), T IGER (Brants et al., 2002), and T¨uBa-D/Z (Hinrichs et al., 2004). We will concentrate on T IGER and T¨uBa-D/Z here; Negra is annotated with an annotation scheme very similar to T IGER but is smaller. In contrast to other languages, these two treebanks are similar on many levels: Both treebanks are based on newspaper text, both use the STTS part of speech (POS) tagset (Thielen and Schiller, 1994), and both use an annotation ∗ I am very grateful to Gerald Penn, who suggested this workshop and the shared"
W08-1008,W08-1005,0,0.0603772,"e used pre-existing dependency converters for T IGER trees (Daum et al., 2004) and for T¨uBa-D/Z trees (Versley, 2005). The dependency representations of the trees in Figures 1 and 3 are shown in Figures 4 and 5. Note that the long-distance relationships are converted into non-projective dependencies. 5 Submissions and Results The shared task drew submissions from 3 groups: the Berkeley group, the Stanford group, and the V¨axj¨o group. Four more groups or individuals had registered but did not submit any data. The submitted systems and results are described in detail in papers in this volume (Petrov and Klein, 2008; Rafferty and Manning, 2008; Hall and Nivre, 2008). All three systems submitted results for the constituent task. For the dependency task, the V¨axj¨o group had the only submission. For this reason, we will concentrate on the analysis of the constituent results and will mention the dependency results only shortly. REL SUBJ AUX SUBJ PP PP DET PN ATTR OBJP PP PN ATTR ATTR PN DET AUX PN DET Beim M. Gipfel ist die sprichw. bayer. Gem. von einem Bild verdr¨angt worden, das im Worts. an einen P.staat erinnert. Figure 4: T IGER dependency annotation. AUX REL SUBJ PP SUBJ PN ADV ADV DET AUX ATTR OBJD"
W08-1008,W08-1006,0,0.0914538,"ndency converters for T IGER trees (Daum et al., 2004) and for T¨uBa-D/Z trees (Versley, 2005). The dependency representations of the trees in Figures 1 and 3 are shown in Figures 4 and 5. Note that the long-distance relationships are converted into non-projective dependencies. 5 Submissions and Results The shared task drew submissions from 3 groups: the Berkeley group, the Stanford group, and the V¨axj¨o group. Four more groups or individuals had registered but did not submit any data. The submitted systems and results are described in detail in papers in this volume (Petrov and Klein, 2008; Rafferty and Manning, 2008; Hall and Nivre, 2008). All three systems submitted results for the constituent task. For the dependency task, the V¨axj¨o group had the only submission. For this reason, we will concentrate on the analysis of the constituent results and will mention the dependency results only shortly. REL SUBJ AUX SUBJ PP PP DET PN ATTR OBJP PP PN ATTR ATTR PN DET AUX PN DET Beim M. Gipfel ist die sprichw. bayer. Gem. von einem Bild verdr¨angt worden, das im Worts. an einen P.staat erinnert. Figure 4: T IGER dependency annotation. AUX REL SUBJ PP SUBJ PN ADV ADV DET AUX ATTR OBJD In Bremen sind bisher nur F"
W08-1008,D07-1066,0,0.0799192,"Missing"
W08-1008,D07-1096,1,\N,Missing
W10-1821,kermes-evert-2002-yac,0,0.0352578,"he CoNLL shared task definition of chunks is 147 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 147–151, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Figure 1: Treebank annotation for the sentence in (2). useful for machine learning based approaches to chunking since it only requires one level of analysis, which can be represented as IOB-chunking (Tjong Kim Sang and Buchholz, 2000). For English, this definition of chunks has become standard in the literature on machine learning. For German, chunk parsing has been investigated by Kermes and Evert (2002) and by M¨uller (2004). Both approaches used an Abney-style chunk definition. However, there is no corresponding flat chunk representation for German because of the complexity of pre-head modification in German noun phrases. Sentence (1) provides a typical example of this kind. (1) [N C der [N C seinen Sohn] liebende Vater] the his son loving father ‘the father who loves his son’ The structure in (1) violates both the Abneystyle and the CoNLL-style definitions of chunks – Abney’s because it is recursive and the CoNLLstyle definition because of the embedding. A single-level, CoNLL-style chunk a"
W10-1821,W09-3820,0,0.0172775,"t for approaches that require an efficient analysis but not necessarily a complete syntactic analysis. German allows a higher degree of syntactic complexity in prenominal modification of the syntactic head of an NP compared to English. This is particularly evident in written texts annotated in the T¨uBa-D/Z. The complexity of German NPs that causes problems in the conversion to CoNLL-style chunks also affects PCFG parsing approaches to German.The complexity of NPs is one of the phenomena that have been addressed in tree transformation approaches for German parsing (Trushkina, 2004; Ule, 2007; Versley and Rehbein, 2009). The notion of a chunk is orginally due to Abney (1991), who considers chunks as non-recursive phrases which span from the left periphery of a phrase to the phrasal head. Accordingly, the sentence “The woman in the lab coat thought you had bought an expensive book.” is assigned the chunk structure: “[S [NP The woman] [PP in [NP the lab coat] ] [VP thought] ] [S [NP you] [VP had bought] [NP an [ADJP expensive] book]] .”. Abney-style chunk parsing is implemented as cascaded, finite-state transduction (cf. (Abney, 1996; Karlsson et al., 1995)). Notice that cascaded, finite-state transduction all"
W10-1821,C04-1039,0,0.0388403,"Missing"
W10-1821,W95-0107,0,0.0701576,"er is to investigate how the annotation of noun phrases in the T¨ubingen Treebank of Written German (T¨uBa-D/Z) can be transformed into chunks with no internal structure, as proposed in the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Chunk parsing is a form of partial parsing, in which non-recursive phrases are annotated while difficult decisions, such as prepositional phrase attachment, are left unsolved. Flat chunk representations are particularly suitable for machine learning approaches to partial parsing and are inspired by the IOB approach to NP chunking first proposed by Ramshaw and Marcus (1995). They are particularly relevant for approaches that require an efficient analysis but not necessarily a complete syntactic analysis. German allows a higher degree of syntactic complexity in prenominal modification of the syntactic head of an NP compared to English. This is particularly evident in written texts annotated in the T¨uBa-D/Z. The complexity of German NPs that causes problems in the conversion to CoNLL-style chunks also affects PCFG parsing approaches to German.The complexity of NPs is one of the phenomena that have been addressed in tree transformation approaches for German parsin"
W10-1821,telljohann-etal-2004-tuba,1,0.876416,"Missing"
W10-1821,W00-0726,0,0.818297,"unk representation for the T¨ubingen Treebank of Written German, which assumes a flat chunk structure so that each word belongs to at most one chunk. For German, such a chunk definition causes problems in cases of complex prenominal modification. We introduce a flat annotation that can handle these structures via a stranded noun chunk. 1 Introduction The purpose of this paper is to investigate how the annotation of noun phrases in the T¨ubingen Treebank of Written German (T¨uBa-D/Z) can be transformed into chunks with no internal structure, as proposed in the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Chunk parsing is a form of partial parsing, in which non-recursive phrases are annotated while difficult decisions, such as prepositional phrase attachment, are left unsolved. Flat chunk representations are particularly suitable for machine learning approaches to partial parsing and are inspired by the IOB approach to NP chunking first proposed by Ramshaw and Marcus (1995). They are particularly relevant for approaches that require an efficient analysis but not necessarily a complete syntactic analysis. German allows a higher degree of syntactic complexity in prenominal modification of the s"
W11-0323,P07-1056,0,0.0869933,"challenging to detect opinions, opinion detection systems usually borrow opinion-labeled data from other data domains. This is especially common in opinion detection in the blogosphere (Chesley et al., 2006). To evaluate this shallow approach, Aue and Gamon (2005) compared four strategies for utilizing opinion-labeled data from one or more non-target domains and concluded that using non-targeted labeled data without an adaptation strategy is less efficient than using labeled data from the target domain, even when the majority of labels are assigned automatically by a self-training algorithm. Blitzer et al. (2007) and Tan et al. (2009) implemented domain adaptation strategies for sentiment analysis. Although promising, their domain adaptation strategies involved sophisticated and computationally expensive methods for selecting general features to link target and non-target domains. 202 3 Motivation and Objective While SSL is especially attractive for opinion detection because it only requires a small number of labeled examples, the studies described in the previous section have concentrated on simple SSL methods. We intend to fill this research gap by comparing the feasibility and effectiveness of a ra"
W11-0323,E09-1026,0,0.0603311,"Missing"
W11-0323,P04-1035,0,0.254768,"g data (Aue and Gamon, 2005; Takamura et al., 2006). No opinion detection applications of EM-based SSL have been reported in the literature. S3 VMs Semi-Supervised Support Vector Machines (S3 VMs) are a natural extension of SVMs in the semi-supervised spectrum. They are designed to find the maximal margin decision boundary in a vector space containing both labeled and unlabeled examples. Although SVMs are the most favored supervised learning method for opinion detection, S3 VMs have not been used in opinion detection. Graphbased SSL learning has been successfully applied to opinion detection (Pang and Lee, 2004) but is not appropriate for dealing with large scale data sets. 2.3 Domain Adaptation for Opinion Detection When there are few opinion-labeled data in the target domain and/or when the characteristics of the target domain make it challenging to detect opinions, opinion detection systems usually borrow opinion-labeled data from other data domains. This is especially common in opinion detection in the blogosphere (Chesley et al., 2006). To evaluate this shallow approach, Aue and Gamon (2005) compared four strategies for utilizing opinion-labeled data from one or more non-target domains and concl"
W11-0323,W03-1014,0,0.072446,"led data. SSL assumes that, although unlabeled data hold no information about classes (e.g., “opinion” or “non-opinion”), they do contain information about joint distribution over classification features. Therefore, when a limited set of labeled data is available in the target domain, using SSL with unlabeled data is expected to achieve an improvement over supervised learning. Self-training Self-training is the simplest and most commonly adopted form of SSL for opinion detection. Self-training was originally used to facilitate automatic identification of opinion-bearing features. For example, Riloff and Wiebe (2003) proposed a bootstrapping process to automatically identify subjective patterns. Self-training has also been applied directly for identifying subjective sentences by following a standard self-training procedure: (1) train an initial supervised classifier on the labeled data; (2) apply this classifier to unlabeled data and 201 select the most confidently labeled data, as determined by the classifier, to augment the labeled data set; and (3) re-train the classifier by restarting the whole process. Wiebe and Riloff (2005) used a selftrained Na¨ıve Bayes classifier for classifying subjective sente"
W11-0323,E06-1026,0,0.0204582,"to-labeled sentences agreed upon by both classifiers. EM-Based SSL Expectation-Maximization (EM) refers to a class of iterative algorithms for maximum-likelihood estimation when dealing with incomplete data. Nigam et al. (1999) combined EM with a Na¨ıve Bayes classifier to resolve the problem of topical classification, where unlabeled data were treated as incomplete data. The EM-NB SSL algorithm yielded better performance than either an unsupervised lexicon-based approach or a supervised approach for sentiment classification in different data domains, including blog data (Aue and Gamon, 2005; Takamura et al., 2006). No opinion detection applications of EM-based SSL have been reported in the literature. S3 VMs Semi-Supervised Support Vector Machines (S3 VMs) are a natural extension of SVMs in the semi-supervised spectrum. They are designed to find the maximal margin decision boundary in a vector space containing both labeled and unlabeled examples. Although SVMs are the most favored supervised learning method for opinion detection, S3 VMs have not been used in opinion detection. Graphbased SSL learning has been successfully applied to opinion detection (Pang and Lee, 2004) but is not appropriate for deal"
W11-0323,P10-1149,0,0.0648284,"Missing"
W11-0323,P99-1032,0,0.166276,"Missing"
W11-0323,W01-1626,0,0.0129226,"data and that it is more robust than self-training, we evaluate new co-training strategies for opinion detection. Third, to approach domain transfer using SSL, assuming that SSL can overcome the problem of domain-specific features by gradually introducing targeted data and thus diminishing bias from the non-target data set. 4 SSL Experiments Our research treats opinion detection as a binary classification problem with two categories: subjective sentences and objective sentences. It is evaluated in terms of classification accuracy. Since a document is normally a mixture of facts and opinions (Wiebe et al., 2001), sub-document level opinion detection is more useful and meaningful than document-level opinion detection. Thus, we conduct all experiments on the sentence level. The remainder of this section explains the data sets and tools used in this study and presents the experimental design and parameter settings. 4.1 Data Sets Three types of data sets have been explored in opinion detection studies: news articles, online reviews, and online discourse in blogs or discussion forums. These three types of text differ from one another in terms of structure, text genre (e.g., level of formality), and propor"
W11-0323,J04-3002,0,0.032072,"200 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 200–209, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics 2 Background and Related Work There is a wide range of literature on opinion detection. We concentrate here on supervised and semisupervised approaches. 2.1 Supervised Learning for Opinion Detection Supervised learning algorithms that can automatically learn important opinion-bearing features from an annotated corpus have been adopted and investigated for opinion detection and yielded satisfying results (Wiebe et al., 2004; Yu and Hatzivassiloglou, 2003; Zhang and Yu, 2007). With no classification techniques developed specifically for opinion detection, state-of-the-art topical supervised classification algorithms can achieve performance comparable to complex linguistic approaches when using binary values (i.e., presence or absence) and incorporating different types of features. Commonly used opinion-bearing features include bag-of-words, POS tags, ngrams, low frequency words or unique words (Wiebe et al., 2004; Yang et al., 2007), semantically oriented adjectives (e.g., “great”, “poor”) and more complex lingui"
W11-0323,W03-1017,0,0.0756608,"he Fifteenth Conference on Computational Natural Language Learning, pages 200–209, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics 2 Background and Related Work There is a wide range of literature on opinion detection. We concentrate here on supervised and semisupervised approaches. 2.1 Supervised Learning for Opinion Detection Supervised learning algorithms that can automatically learn important opinion-bearing features from an annotated corpus have been adopted and investigated for opinion detection and yielded satisfying results (Wiebe et al., 2004; Yu and Hatzivassiloglou, 2003; Zhang and Yu, 2007). With no classification techniques developed specifically for opinion detection, state-of-the-art topical supervised classification algorithms can achieve performance comparable to complex linguistic approaches when using binary values (i.e., presence or absence) and incorporating different types of features. Commonly used opinion-bearing features include bag-of-words, POS tags, ngrams, low frequency words or unique words (Wiebe et al., 2004; Yang et al., 2007), semantically oriented adjectives (e.g., “great”, “poor”) and more complex linguistic patterns. Both the scale a"
W11-1918,C02-1039,0,0.0372692,"ed by smaller losses in recall. In contrast, CEAFE shows a loss in precision and a similar gain in recall, resulting in a minimal increase in F-score. A comparison of the results for the experiments with the base set as opposed to the extended set in 5 shows that the extended feature set using WordNet information is detrimental to the final results averaged over all metrics while it led to a slight improvement on the mention level. Our assumption is that while in general, the ontological information is useful, the additional information may be a mixture of relevant and irrelevant information. Mihalcea (2002) showed for word sense disambiguation that IM R P Automatic Mention Identification UBIUB 67.27 37.48 auto UBIUE 67.49 37.60 UBIUB 65.92 40.56 gold UBIUE 66.11 40.37 Gold Mention Boundaries UBIUB 67.57 58.66 auto UBIUE 69.19 57.27 UBIUB 67.64 58.75 gold UBIUE 67.72 58.66 B3 F1 R MUC P F1 R P F1 R 48.14 48.29 50.22 50.13 28.75 28.87 31.05 30.84 20.61 20.66 25.57 25.14 24.01 24.08 28.04 27.70 67.17 67.14 64.94 65.07 56.81 56.67 62.23 61.83 61.55 61.46 63.56 63.41 31.67 31.57 33.53 33.23 62.80 62.67 62.88 62.87 34.14 33.48 34.37 34.18 40.43 37.15 40.68 40.40 37.02 35.22 37.26 37.03 54.24 55.47 54."
W11-1918,W11-1901,0,0.0500559,"inds are reliable, but it is also too conservative in assuming coreference relations. For the future, we need to investigate undersampling the negative examples in the training set and more efficient methods for filtering out singletons. 4 Final Results In the following, we present the UBIU system results in two separate settings: using the test set with automatically extracted mentions (section 4.1) and using a test set with gold standard mentions, including singletons (section 4.2). An overview of all sys115 tems participating in the C O NLL-2011 shared task and their results is provided by Pradhan et al. (2011). 4.1 Automatic Mention Identification The final results of UBIU for the test set without gold standard mentions are shown in the first part of table 5. They are separated into results for the coreference resolution module based on automatically annotated linguistic information and the gold annotations. Again, we report results for both the base feature set (UBIUB ) and the extended feature set using WordNet features (UBIUE ). A comparison of the system results on the test and the development set in the UBIUB setting shows that the average Fscore is considerably lower for the test set, 40.46 v"
W11-1918,D09-1101,0,0.218844,"r all of its preceding mentions in a window of 3 sentences. After classification, a filter can optionally be applied to filter out mention pairs that disagree in number, and another filter deletes all mentions that were not assigned an antecedent in classification. Note that the number information was derived from the POS tags and not from the number/gender data provided by the shared task since the POS information proved more reliable in our system. Initially, UBIU was developed to use a wide set of features (Zhekova and K¨ubler, 2010), which constitutes a subset of the features described by Rahman and Ng (2009). For the C O NLL-2011 shared task, we investigated the importance of various additional features that can be included in the feature set used by the memory-based classifier. Thus, we conducted experiments with a base set and an extended feature set, which makes use of lexical semantic features. Base Feature Set Since the original feature set in Zhekova and K¨ubler (2010) contained information that is not easily accessible in the OntoNotes data set (such as grammatical functions), we had to restrict the feature set to information that can be derived solely from POS annotations. Further infor#"
W11-1918,S10-1019,1,0.883525,"Missing"
W11-1918,D08-1067,0,\N,Missing
W12-2011,H01-1052,0,0.0816575,"Missing"
W12-2011,boyd-2010-eagle,0,0.0296141,"nce Hebrew is a less commonly taught language (LCTL), we have few placement exams from which to learn correspondences. Compounding the data sparsity problem is that each piece of data is complex: if a learner produces an erroneous answer, there are potentially a number of ways to analyze it (cf. e.g. (Dickinson, 2011)). An error could feature, for instance, a letter inserted in an irregular verb stem, or between two nouns; any of these properties may be relevant to describing the error (cf. how errors are described in different taxonomies, e.g. (D´ıaz-Negrillo and Fern´andez-Dom´ınguez, 2006; Boyd, 2010)). Specific error types are unlikely to recur, making sparsity even more of a concern. We thus need to develop methods which generalize well, finding the most useful aspects of the data. Our system is an online system to be used at the Hebrew Language Program at our university. The system is intended to semi-automatically place incoming students into the appropriate Hebrew course, i.e., level. As with many exams, the main purpose is to “reduce the number of students who attend an oral interview” (Fulcher, 1997). 2 The Data Exercise type We focus on a scrambled sentence exercise, in which learn"
W12-2011,W11-2838,0,0.0335701,"tures in the future. The aggregation of information also allows us to smooth over sparse features. 1 Introduction and Motivation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human, based on a written exam (e.g. (Fulcher, 1997)). To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels. There is only little work investigating this correspondence formally (see (Hawkins and Filipovi´c, 2010; Alexopoulou e"
W12-2011,W11-1410,1,0.845426,"ation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human, based on a written exam (e.g. (Fulcher, 1997)). To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner levels. There is only little work investigating this correspondence formally (see (Hawkins and Filipovi´c, 2010; Alexopoulou et al., 2010) for discussion) and only on error-annotated English learner corpora. For this reason, we follow a data-driv"
W12-2011,P11-2124,0,0.0139688,"ammatical sentence. Our goal is to move towards freer language production and to analyze language proficiency through more variables, but, in the interest of practicality, we start in a more restricted way. For lesser-resourced languages, there is generally little data and few NLP resources available. For Hebrew, for example, we must create our own pool of 95 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95–104, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008), they are not adapted for dealing with potentially ill-formed learner productions. For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses. Then, the system aligns the learner answer to the gold standard answer and determines the types of deviations. Since Hebrew is a less commonly taught language (LCTL), we have few placement exams from which to learn correspondences. Compounding the data sparsity problem is that each piece of data is complex: if a learner produces an erroneous"
W12-2011,C10-1046,0,0.0281701,"Missing"
W12-2011,P11-1093,0,0.0206146,"nd linguistic constructions which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features. 1 Introduction and Motivation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human, based on a written exam (e.g. (Fulcher, 1997)). To model the decision process automatically, we need to understand how the types of errors, as well as their frequen"
W12-2011,C08-1109,0,0.0156809,"which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features. 1 Introduction and Motivation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human, based on a written exam (e.g. (Fulcher, 1997)). To model the decision process automatically, we need to understand how the types of errors, as well as their frequencies, correspond to learner lev"
W12-2011,P11-1019,0,0.270144,"e account for in our feature selection, learning algorithm, and in the setup. Specifically, we define a two-phase classification process, isolating individual errors and linguistic constructions which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features. 1 Introduction and Motivation Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011). One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008); see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011). Only rarely has there been work on detecting errors in more morphologically-complex languages (Dickinson et al., 2011). In our work, we extend the task to predicting the learner’s level based on the errors, focusing on HePlacing learners into levels is generally done by a human,"
W12-3624,P07-1086,0,0.445015,"n allows the retrieval of a considerable number of coordinate structures beyond the ones having a coordinating conjunction. 1 1.1 Introduction Motivation Coordination is a difficult topic, in terms of linguistic description and analysis as well as for NLP approaches. Most linguistic frameworks still struggle with finding an account for coordination that is descriptively fully adequate (Hartmann, 2000). This is also the reason why coordination is not adequately encoded in the annotation of major treebanks. From an NLP perspective, coordination is one of the major sources for errors in parsing (Hogan, 2007). If parsing of coordinate structures can be improved, overall parsing quality also benefits (K¨ubler et al., 2009). 166 And consequently, downstream NLP applications, such as question answering or machine translation, would benefit as well. However, since linguistic frameworks in general are challenged by the diverse phenomena of coordination, a consistent annotation of coordinate structures, clearly marking the phenomenon as such as well as its scope, is a difficult enterprise. Consequently, this makes the detection of conjuncts and their boundaries a highly non-trivial task. Nevertheless, a"
W12-3624,E09-1047,1,0.936618,"Missing"
W12-3624,H05-1105,0,0.0143213,"1 shows an example sentence with two coordinate structures, the inside one a coordinate noun phrase (NP) with 3 conjuncts, and the outside one a coordinate verb phrase (VP) with two complex conjuncts. These coordinate structures are labeled by ordinary phrasal categories such as VP and NP and can thus not be distinguished at the phrasal level from VPs and NP that do not involve coordination. There are approaches to improving parsing for coordinations, but most of these approaches are restricted to very narrow definitions such as coordinations of noun compounds such as “oil and gas resources” (Nakov and Hearst, 2005), coordinations of symmetrical NPs (Hogan, 2007; Shimbo and Hara, 2007), or coordinations of “A CC B” where A and B are conjuncts, and CC is an overt conjunction (K¨ubler et al., 2009). To our knowledge, there is no attempt at covering all coordination types. One goal of this paper is to demonstrate a wide range of coordination phenomena that have to be taken into account in a thorough treatment of coordinations. We additionally present a proposal for an enhanced annotation of coordination for the Penn Treebank. The annotation is focused on punctuation and allows for an in-depth investigation"
W12-3624,D07-1064,0,0.262261,"ne a coordinate noun phrase (NP) with 3 conjuncts, and the outside one a coordinate verb phrase (VP) with two complex conjuncts. These coordinate structures are labeled by ordinary phrasal categories such as VP and NP and can thus not be distinguished at the phrasal level from VPs and NP that do not involve coordination. There are approaches to improving parsing for coordinations, but most of these approaches are restricted to very narrow definitions such as coordinations of noun compounds such as “oil and gas resources” (Nakov and Hearst, 2005), coordinations of symmetrical NPs (Hogan, 2007; Shimbo and Hara, 2007), or coordinations of “A CC B” where A and B are conjuncts, and CC is an overt conjunction (K¨ubler et al., 2009). To our knowledge, there is no attempt at covering all coordination types. One goal of this paper is to demonstrate a wide range of coordination phenomena that have to be taken into account in a thorough treatment of coordinations. We additionally present a proposal for an enhanced annotation of coordination for the Penn Treebank. The annotation is focused on punctuation and allows for an in-depth investigation of coordinations, for example for linguistic treatments, but also for w"
W12-3624,telljohann-etal-2004-tuba,1,0.831414,"Missing"
W12-3624,J93-2004,0,\N,Missing
W12-4509,W05-0406,0,0.0263091,"no other mentions in the discourse. Because singletons comprise the majority of mentions in a discourse, their presence can have a substantial effect on the performance of machine learning approaches to CR, both because they complicate the learning task and because they heavily skew the proportion in the training data towards negative instances, which can bias the learner towards assuming no coreference relation between pairs of mentions. For this reason, information concerning singletons needs to be incorporated into the CR process so that such mentions can be eliminated from consideration. Boyd et al. (2005), Ng and Cardie (2002), and Evans (2001) experimented with machine learning approaches to detect and/or eliminate singletons, finding that such a module provides an improve90 ment in CR performance provided that the classifier # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 10 11 12 13 14 15 16 17 18 19 Feature Description the depth of the mention in the syntax tree the length of the mention the head token of the mention the POS tag of the head the NE of the head the NE of the mention PR if the head is premodified, PO if it is not; UN otherwise D if the head is in a definite mention; I otherwise the pre"
W12-4509,S10-1021,0,0.0215821,"ks (Daelemans and van den Bosch, 2005). We employ TiMBL (Daelemans et al., 2010), which uses k nearest neighbour classification to assign class labels to the targeted instances. The classifier settings we used were determined by a non-exhaustive search over the development data and are as follows: the IB1 algorithm, similarity is computed based on weighted overlap, gain ratio is used for the relevance weights and the number of nearest neighbors is set to k=3 (cf. (Daelemans et al., 2010) for an explanation of the system parameters). In UBIU, we use a pairwise mention model (Soon et al., 2001; Broscheit et al., 2010) since this model has proven more robust towards multiple languages (Wunsch, 2009) than more elaborate ones. We concentrate on nominal coreference resolution, i.e. we ignore the more unrestricted cases of event coreference. Below, we describe the modules used in UBIU in more detail. 2.1 Preprocessing The preprocessing module oversees the proper formatting of the data for all modules applied in later stages during coreference resolution. During preprocessing, we use the speaker information, if provided, and replace all 1st person singular pronouns from the token position with the information pr"
W12-4509,A00-1020,0,0.0144874,"Missing"
W12-4509,W11-1902,0,0.104446,"Missing"
W12-4509,H05-1083,0,0.0729197,"Missing"
W12-4509,C02-1139,0,0.042249,"n the discourse. Because singletons comprise the majority of mentions in a discourse, their presence can have a substantial effect on the performance of machine learning approaches to CR, both because they complicate the learning task and because they heavily skew the proportion in the training data towards negative instances, which can bias the learner towards assuming no coreference relation between pairs of mentions. For this reason, information concerning singletons needs to be incorporated into the CR process so that such mentions can be eliminated from consideration. Boyd et al. (2005), Ng and Cardie (2002), and Evans (2001) experimented with machine learning approaches to detect and/or eliminate singletons, finding that such a module provides an improve90 ment in CR performance provided that the classifier # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 10 11 12 13 14 15 16 17 18 19 Feature Description the depth of the mention in the syntax tree the length of the mention the head token of the mention the POS tag of the head the NE of the head the NE of the mention PR if the head is premodified, PO if it is not; UN otherwise D if the head is in a definite mention; I otherwise the predicate argument corres"
W12-4509,P04-1020,0,0.027152,"text POS tag of token on position token -1 right context token on position token +1 right context token on position token +2 right context token on position token +3 right context POS tag of token on position token +1 right context POS tag of token on position token +2 right context POS tag of token on position token +3 the syntactic label of the mother node the syntactic label of the grandmother node a concatenation of the labels of the preceding nodes C if the mention is in a PP; else I Table 3: The features used by the singleton classifier. does not eliminate non-singletons too frequently. Ng (2004) additionally compared various feature- and constraint-based approaches to incorporating singleton information into the CR pipeline. Feature-based approaches integrate information from the singleton classifier as features while constraint-based approaches filter singletons from the mention set. Following these works, we include a k nearest neighbor classifier for singleton mentions in UBIU with 19 commonly-used features described below. However, unlike Ng (2004), we use a combination of the feature- and constraint-based approaches to incorporate the classifier’s results. Each training/testing"
W12-4509,W11-1901,0,0.0374559,"Zhekova and K¨ubler, 2010) among them. However, since systems participated across the various languages rather irregularly, Recasens et al. (2010) reported that the data points were too few to allow for a proper comparison between different approaches. Further sig88 nificant issues concerned system portability across the various languages and the respective language tuning, the influence of the quantity and quality of diverse linguistic annotations as well as the performance and behavior of various evaluation metrics. The CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes (Pradhan et al., 2011) targeted unrestricted CR, which aims at identifying nominal coreference but also event coreference, within an English data set from the OntoNotes corpus. Not surprisingly, attempting to include such event mentions had a detrimental effect on overall accuracy, and the best performing systems (e.g., (Lee et al., 2011)) did not attempt event anaphora. The current shared task extends the task definition to three different languages (Arabic, Chinese and English), which can prove challenging for rule-based approaches such as the best performing system from 2011 (Lee et al., 2011). In the current pa"
W12-4509,W12-4501,0,0.0636915,"Missing"
W12-4509,D09-1101,0,0.138096,"on is represented in the feature vector by its syntactic head. The vectors for the pairs are then used by the memory-based learner TiMBL. As anaphoric mentions, we consider all definite phrases; we then create a pair for each anaphor with each mention preceding it within a window of 10 (English, Chinese) or 7 (Arabic) sentences. We consider a shorter window of sentences for Arabic because of its NP-rich syntactic structure and its longer sentences, which leads to an increased number of 91 possible mention pairs. The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009). Before classification, we apply a morphological filter, which excludes vectors that disagree in number or gender (applied only if the respective information is provided or can be deduced from the data). Both the anaphor and the antecedent carry a label assigned to them by the singletons classifier. Yet, we consider as anaphoric only the heads of definite mentions. Including a feature representing the class assigned by the singletons classifier for each anaphor triggers a conservative learner behavior, i.e., fewer positive classes are assigned. Thus, to account for this behavior, we ignore th"
W12-4509,W09-2411,0,0.0320763,"Missing"
W12-4509,J01-4004,0,0.11297,"uage processing tasks (Daelemans and van den Bosch, 2005). We employ TiMBL (Daelemans et al., 2010), which uses k nearest neighbour classification to assign class labels to the targeted instances. The classifier settings we used were determined by a non-exhaustive search over the development data and are as follows: the IB1 algorithm, similarity is computed based on weighted overlap, gain ratio is used for the relevance weights and the number of nearest neighbors is set to k=3 (cf. (Daelemans et al., 2010) for an explanation of the system parameters). In UBIU, we use a pairwise mention model (Soon et al., 2001; Broscheit et al., 2010) since this model has proven more robust towards multiple languages (Wunsch, 2009) than more elaborate ones. We concentrate on nominal coreference resolution, i.e. we ignore the more unrestricted cases of event coreference. Below, we describe the modules used in UBIU in more detail. 2.1 Preprocessing The preprocessing module oversees the proper formatting of the data for all modules applied in later stages during coreference resolution. During preprocessing, we use the speaker information, if provided, and replace all 1st person singular pronouns from the token positio"
W12-4509,S10-1019,1,0.814095,"Missing"
W12-4509,D08-1067,0,\N,Missing
W12-4509,S10-1001,0,\N,Missing
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W14-3912,P96-1041,0,0.013775,"are trained on the provided training data. Lexical probabilities: The lexical probability component consists of a dictionary for each label containing the words found under that label and their relative frequencies. Each word type and its count of tokens are added to the total for each respective label. After training, the probability of a given label emitting a word (i.e., P (word|label)) is derived from these counts. To handle out-of-vocabulary words, we use Chen-Goodman “one-count” smoothing, which approximates the probabilities of unknown words as compared to the occurrence of singletons (Chen and Goodman, 1996). Character n-gram probabilities: The characterbased n-gram model serves mostly as a back-off in case a word is out-of-vocabulary, in which case the lexical probability may not be reliable. However, it also provides important information in the case of mixed words, which may use morphology from one language added to a stem from the other one. In this setting, unigrams are not informative. For this reason, we select longer n-grams, with n ranging between 2 and 5. Character n-gram probabilities are calculated as follows: For each training set, the words in that training set are sorted into lists"
W14-3912,P05-1045,0,0.0577003,"Missing"
W14-3912,N13-1131,0,0.0839953,"rd that could exist in either language, e.g., no in the Spanish-English data). Traditionally, language identification is performed on the document level, i.e., on longer segments of text than what is available in tweets. These methods are based on variants of character n-grams. Seminal work in this area is by Beesley (1988) and Grefenstette (1995). Lui and Baldwin (2014) showed that character n-grams also perform on Twitter messages. One of a few recent approaches working on individual words is by King et al. (2014), who worked on historical data; see also work by Nguyen and Dogruz (2013) and King and Abney (2013). Our system is an adaptation of a Markov model, which integrates lexical, character n-gram, and label transition probabilities (all trained on the provided data) in addition to the output of pre-existing NER tools. All the information sources are weighted in the Markov model. One advantage of our approach is that it is languageindependent. We use the exact same architecture for all language pairs, and the only difference for the individual language pairs lies in a manual, non-exhaustive search for the best weights. Our results show that the approach works well for the one language pair with d"
W14-3912,W14-1303,0,0.0143217,"ons) also common. Named entities (ne) are also frequent, and accounting for them adds a significant complication to the task. Less common are mixed (to account for words that may e.g., apply L1 morphology to an L2 word), and ambiguous (to cover a word that could exist in either language, e.g., no in the Spanish-English data). Traditionally, language identification is performed on the document level, i.e., on longer segments of text than what is available in tweets. These methods are based on variants of character n-grams. Seminal work in this area is by Beesley (1988) and Grefenstette (1995). Lui and Baldwin (2014) showed that character n-grams also perform on Twitter messages. One of a few recent approaches working on individual words is by King et al. (2014), who worked on historical data; see also work by Nguyen and Dogruz (2013) and King and Abney (2013). Our system is an adaptation of a Markov model, which integrates lexical, character n-gram, and label transition probabilities (all trained on the provided data) in addition to the output of pre-existing NER tools. All the information sources are weighted in the Markov model. One advantage of our approach is that it is languageindependent. We use th"
W14-3912,D13-1084,0,0.0385764,"and ambiguous (to cover a word that could exist in either language, e.g., no in the Spanish-English data). Traditionally, language identification is performed on the document level, i.e., on longer segments of text than what is available in tweets. These methods are based on variants of character n-grams. Seminal work in this area is by Beesley (1988) and Grefenstette (1995). Lui and Baldwin (2014) showed that character n-grams also perform on Twitter messages. One of a few recent approaches working on individual words is by King et al. (2014), who worked on historical data; see also work by Nguyen and Dogruz (2013) and King and Abney (2013). Our system is an adaptation of a Markov model, which integrates lexical, character n-gram, and label transition probabilities (all trained on the provided data) in addition to the output of pre-existing NER tools. All the information sources are weighted in the Markov model. One advantage of our approach is that it is languageindependent. We use the exact same architecture for all language pairs, and the only difference for the individual language pairs lies in a manual, non-exhaustive search for the best weights. Our results show that the approach works well for th"
W14-3912,D11-1141,0,0.0161445,"Missing"
W14-3912,W14-3907,0,0.135251,"Missing"
W14-5902,W12-5303,0,0.0189939,"d. Researchers either generate balanced data sets during data collection, by sampling a certain number of positive and negative reviews, or by selecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the movie review data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2–11, Dublin, Ireland, August 24 2014. and Savoy (2012), O’Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed competitive feature selection methods evaluated on the movie review data set. However, the generalizability of suc"
W14-5902,W13-1616,0,0.0267353,"ate balanced data sets during data collection, by sampling a certain number of positive and negative reviews, or by selecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the movie review data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2–11, Dublin, Ireland, August 24 2014. and Savoy (2012), O’Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed competitive feature selection methods evaluated on the movie review data set. However, the generalizability of such feature selection methods"
W14-5902,P09-1078,0,0.0190869,"known about an unobserved random variable given an observed variable. It is defined as the entropy of one random variable minus the conditional entropy of the observed variable. Thus, IG is the reduced uncertainty of class S given a feature f : IG = H(S) − H(S|f ) = X X P (f, S)log f ∈{0,1} S∈{0,1} P (f, S) P (f )P (S) Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman (2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed that IG has a high precision with respect to the minority class. Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds of feature f occurring in class S to the odds of it occurring in class S. A value larger than 1 indicates that a feature is positively correlated with class S, a value smaller than 1 indicates it is negatively correlated: OR = log P (f, S)(1 − P (f, S)) P (f, S)(1 − P (f, S)) Brank et al. (2002b) showed that OR requires"
W14-5902,P11-1015,0,0.0474323,"4 of all ratings consist of the highest rating of 5. For other types of user generated content, they opposite may be true. Heavy skewing in data sets is challenging for standard classification algorithms. Therefore, the data sets generally used for research on sentiment analysis are balanced. Researchers either generate balanced data sets during data collection, by sampling a certain number of positive and negative reviews, or by selecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the movie review data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proceedings of the Second Workshop on Natural Langua"
W14-5902,P10-1141,0,0.142691,"eview data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2–11, Dublin, Ireland, August 24 2014. and Savoy (2012), O’Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed competitive feature selection methods evaluated on the movie review data set. However, the generalizability of such feature selection methods to imbalanced data sets, which better represent real world situations, has not been investigated in much detail. Forman (2003) provides an extensive study of feature selection methods for highly imbalanced data sets, but he uses document classification as task. This current paper investigates the robustness of three feature selection methods that Forman (2003) has shown to be successful, as well as two variants of T F ∗ IDF . The three meth"
W14-5902,P04-1035,0,0.0146865,"ith their ratings. In this data set, more than 3/4 of all ratings consist of the highest rating of 5. For other types of user generated content, they opposite may be true. Heavy skewing in data sets is challenging for standard classification algorithms. Therefore, the data sets generally used for research on sentiment analysis are balanced. Researchers either generate balanced data sets during data collection, by sampling a certain number of positive and negative reviews, or by selecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the movie review data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proc"
W14-5902,W02-1011,0,0.0212287,"op words are domain dependent, and some English stop words may be informative for sentiment analysis, and 2) uninformative words that are equally common in both classes will be excluded by feature selection if the method is successful. 3.3 Feature representation Since our focus is on settings with high numbers of features, we use a bag-of-words approach, in which every word represents one feature, and its term frequency serves as its value. Different feature weighting methods, including binary weighting, term frequency, and T F ∗IDF have been adopted in past sentiment analysis studies (e.g., (Pang et al., 2002; Paltoglou and Thelwall, 2010)). (Pang et al., 2002) found that simply using binary feature weighting performed better than using more complicated weightings in a task of classifying positive and negative movie reviews. However, movie reviews are relatively short, so there may not be a large difference between binary features and others. Topic classification usually uses term frequency as feature weighting. T F ∗ IDF and variants were shown to perform better than binary weighting and term frequency for sentiment analysis (Paltoglou and Thelwall, 2010). Since our user rating prediction tasks a"
W14-5903,C04-1121,0,0.0591603,"rformance, whether used alone or with unigrams, which has been confirmed by many following studies. However, Cui et al. (2006) later conjectured that when the training corpus is large enough, adding bigrams to unigrams improved the accuracy of binary product review classification. A great number of diverse features were proven to be beneficial to capture subtle sentiments across studies and a “kitchen sink” approach is often adopted for sentiment analysis (Yang et al., 2008). However, when features are noisy and redundant, researcher have found it beneficial to identify the most telling ones (Gamon, 2004; Ng et al., 2006). While it is useful to differentiate positive and negative reviews, a finer level of distinction can help users better compare online reviews. As a matter of fact, even extra half star ratings can have dramatic economic impact (Anderson and Magruder, 2012). To predict multi-level ratings, either multiclass classification or regression methods can be applied (Koppel and Schler, 2006; Yu et al., 2013). Pang and Lee (2005) have also proposed an alternative meta-algorithm based on metric labeling for predicting three or four sentiment classes for movie reviews. In their experime"
W14-5903,W14-5902,1,0.878793,"Missing"
W14-5903,N03-5008,0,0.047583,"Missing"
W14-5903,P06-2079,0,0.0259587,"ether used alone or with unigrams, which has been confirmed by many following studies. However, Cui et al. (2006) later conjectured that when the training corpus is large enough, adding bigrams to unigrams improved the accuracy of binary product review classification. A great number of diverse features were proven to be beneficial to capture subtle sentiments across studies and a “kitchen sink” approach is often adopted for sentiment analysis (Yang et al., 2008). However, when features are noisy and redundant, researcher have found it beneficial to identify the most telling ones (Gamon, 2004; Ng et al., 2006). While it is useful to differentiate positive and negative reviews, a finer level of distinction can help users better compare online reviews. As a matter of fact, even extra half star ratings can have dramatic economic impact (Anderson and Magruder, 2012). To predict multi-level ratings, either multiclass classification or regression methods can be applied (Koppel and Schler, 2006; Yu et al., 2013). Pang and Lee (2005) have also proposed an alternative meta-algorithm based on metric labeling for predicting three or four sentiment classes for movie reviews. In their experiments, the meta-algo"
W14-5903,P05-1015,0,0.107989,"ed for sentiment analysis (Yang et al., 2008). However, when features are noisy and redundant, researcher have found it beneficial to identify the most telling ones (Gamon, 2004; Ng et al., 2006). While it is useful to differentiate positive and negative reviews, a finer level of distinction can help users better compare online reviews. As a matter of fact, even extra half star ratings can have dramatic economic impact (Anderson and Magruder, 2012). To predict multi-level ratings, either multiclass classification or regression methods can be applied (Koppel and Schler, 2006; Yu et al., 2013). Pang and Lee (2005) have also proposed an alternative meta-algorithm based on metric labeling for predicting three or four sentiment classes for movie reviews. In their experiments, the meta-algorithm outperformed SVMs in either one-versus-all or regression mode. In order to adopt this meta-algorithm, however, one needs to determine an effective review similarity measure, which is not always straightforward. If an item receives multiple reviews and/or comes from multiple sources, an overall rating needs to be generated for this item. Yu et al. (2013) generated this overall rating by treating all the reviews from"
W14-5903,W02-1011,0,0.0176698,"ach dominantly adopts supervised learning algorithms, which treat sentiment analysis as a text classification task. In this case, sentiment features are generated from a pre-labeled corpus. Given the lack of annotated data, semi-supervised learning is adopted (Yu, 2014; Yu and K¨ubler, 2011). For this study, we focus on a specific language domain of online recipe reviews, which has user ratings, thus we choose supervised learning. We also adopt one existing linguistic lexicon to provide extra features for our classification models. The earliest sentiment analysis on online reviews was done by Pang et al. (2002); they applied several supervised learning algorithms to classify online movie reviews into a positive and a negative class. This study found that machine learning methods outperformed human annotators. It also found that bigrams did not improve the classification performance, whether used alone or with unigrams, which has been confirmed by many following studies. However, Cui et al. (2006) later conjectured that when the training corpus is large enough, adding bigrams to unigrams improved the accuracy of binary product review classification. A great number of diverse features were proven to b"
W14-5903,N03-1033,0,0.0616759,"to counter the effect of the wide variance in the number of reviews per recipe, we randomly sampled 10 reviews from recipes with more than 10 reviews. We had performed initial experiments with all reviews, which resulted in only minor differences. At the review level, rare words (unigrams occurring less than four times) were removed for two reasons: 1) Extremely rare words are likely to be noise rather than sentiment-bearing clues; 2) the feature selection method BNS is biased towards rare words; 3) such words do not generalize well. The recipes were then tagged using the Stanford POS Tagger (Toutanova et al., 2003). The data set is severely skewed with regard to the number of recipes per fork: Users seem to be more willing to review good recipes. To lessen the effect of imbalance in the rating classifier, all half fork reviews were added to their corresponding full star reviews (i.e., 1.5 fork was added to the 1 fork data). This resulted in the data split of 10 089 recipes shown in table 1. Even after collapsing the half stars, there is still a very large skewing of the data towards the higher ratings. This means, feature selection is important to mitigate the imbalance to a certain degree. 4.2 Features"
W14-5903,H05-1044,0,0.00633661,"2 Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 12–21, Dublin, Ireland, August 24 2014. During the last decade or more, there has been significant body of sentiment analysis studies on online reviews. Two major approaches exist: lexicon-based and machine learning. A lexicon-based approach requires prior knowledge of important sentiment features to build a list of sentiment-bearing words (or phrases), which are often domain independent. Examples of such lexicons include the Multi-Perspective Question Answering (MPQA) subjectivity lexicon (Wilson et al., 2005) and the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al., 2014). The sentiment of a review is determined by various ways of aggregating information about the sentiment-bearing words (phrases), such as their frequency and sentiment scores. The machine learning approach dominantly adopts supervised learning algorithms, which treat sentiment analysis as a text classification task. In this case, sentiment features are generated from a pre-labeled corpus. Given the lack of annotated data, semi-supervised learning is adopted (Yu, 2014; Yu and K¨ubler, 2011). For this study, we"
W14-5903,W11-0323,1,0.888497,"Missing"
W14-6101,D12-1133,0,0.0126291,"rser effectively performs the POS disambiguation. On these grounds, they present a factorized model for PCFG parsing which separates parsing into a discriminative lexical model (with local features) and the actual parsing model, to be combined with a product-of-experts (Hinton, 1999). Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and parsing can be found. Research has concentrated on languages that require additional segmentation on the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we forego a further overview. 3 The Three Tagset Variants In our experiments, we use three POS tagset variants: The standard Stuttgart-T¨ubingen Tagset (STTS), the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). Since the two treeban"
W14-6101,W07-1506,0,0.0291559,"reprocessing, we follow the standard practices from the parsing community. In both treebanks, punctuation and other material, such as parentheses, are not included in the annotation, but attached to a virtual root node. We attach the respective nodes to the tree using the algorithm described by Maier et al. (2012) so that every sentence corresponds to exactly one tree. In a nutshell, this algorithm uses the left and right terminal neighbors as attachment targets. In TiGer, we then remove the crossing branches using a two-stage process. In a first step, we apply the transformation described by Boyd (2007). This transformation introduces a new non-terminal for every continuous block of a discontinuous constituent. We keep a flag on each of the newly introduced nodes that indicates if it dominates the head daughter of the original discontinuous node. Subsequently, we delete all those nodes for which this flag is false.2 For both POS tagging and parsing, we use the same split for training, development, and test. We use the first half of the last 10 000 sentences in TiGer for development and the second half for testing. The remaining 40 472 sentences are used for training. Accordingly, in order to"
W14-6101,A00-1031,0,0.163514,"extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is based on support vector machines, TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 1–14 Dublin, Ireland, August 23-29 201"
W14-6101,W10-1409,0,0.0220527,"sque, French, German, Hebrew, and Hungarian. However, note that Sz´ant´o and Farkas (2014) used the data from the SPMRL shared task 2013, which does not contain grammatical functions in the syntactic annotations. Both approaches found improvements for subsets of morphological features. Other works examine, also within a “pipeline” method, possibilities for ambiguity reduction through modification of tagsets, or of the lexicon by tagset reduction, or through word-clustering. Lakeland (2005) uses lexicalized parsing a` la Collins (1999). Similarly to the more recent work by Koo et al. (2008) or Candito and Seddah (2010), he addresses the question of how to optimally disambiguate for parsing on the lexical level by clustering. A word cluster is thereby seen as an equivalence class of words and assumes to a certain extent the function of a POS tag, but can be adapted to the training data. Le Roux et al. (2012) address the issue of data sparseness on the lexical level with PCFG parsing with the morphologically rich language Spanish. The authors use a reimplementation of the Berkeley parser. They show that parsing results can be improved by simplifying the POS tagset, as well as by lemmatization, since both appr"
W14-6101,I11-1141,0,0.0197709,"hing for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag PRON DET ADP NUM description pronoun determiner, article preposition, postposition numeral tag CONJ PRT . X description conjunction particle punctuation everything else Table 1: The 12 tags of the Universal Tagset. hierarchical, features are decisive for the quality of POS tagging and note that a “pipeline” model does not take this into account since the parser effectively performs the POS disambiguation. On these grounds, they pr"
W14-6101,chrupala-etal-2008-learning,0,0.0307403,"Missing"
W14-6101,P99-1065,0,0.245797,"Missing"
W14-6101,E03-1052,0,0.0163896,"iscriminative lexical model (with local features) and the actual parsing model, to be combined with a product-of-experts (Hinton, 1999). Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and parsing can be found. Research has concentrated on languages that require additional segmentation on the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we forego a further overview. 3 The Three Tagset Variants In our experiments, we use three POS tagset variants: The standard Stuttgart-T¨ubingen Tagset (STTS), the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). Since the two treebanks differ in their morphological annotation, in this variant, the tags differ between the two treebanks: For TiGer, we have 783 possible complex"
W14-6101,W11-3802,0,0.0565312,"Missing"
W14-6101,gimenez-marquez-2004-svmtool,0,0.0798508,"Missing"
W14-6101,P08-1043,0,0.02008,"del does not take this into account since the parser effectively performs the POS disambiguation. On these grounds, they present a factorized model for PCFG parsing which separates parsing into a discriminative lexical model (with local features) and the actual parsing model, to be combined with a product-of-experts (Hinton, 1999). Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and parsing can be found. Research has concentrated on languages that require additional segmentation on the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we forego a further overview. 3 The Three Tagset Variants In our experiments, we use three POS tagset variants: The standard Stuttgart-T¨ubingen Tagset (STTS), the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also includes morphological information from the t"
W14-6101,I11-1136,0,0.0223608,"ng and note that a “pipeline” model does not take this into account since the parser effectively performs the POS disambiguation. On these grounds, they present a factorized model for PCFG parsing which separates parsing into a discriminative lexical model (with local features) and the actual parsing model, to be combined with a product-of-experts (Hinton, 1999). Particularly in the dependency parsing literature, combined models for simultaneous POS tagging and parsing can be found. Research has concentrated on languages that require additional segmentation on the word level, such as Chinese (Hatori et al., 2011) or Hebrew (Goldberg and Tsarfaty, 2008). A new approach by Bohnet and Nivre (2012) was also evaluated on German. Results for POS tagging and parsing of German by means of a constraint grammar can be found in Daum et al. (2003) as well as in Foth et al. (2005). However, since these approaches are only marginally related to our approach, we forego a further overview. 3 The Three Tagset Variants In our experiments, we use three POS tagset variants: The standard Stuttgart-T¨ubingen Tagset (STTS), the Universal Tagset (UTS) (Petrov et al., 2012), and an extended version of the STTS that also inclu"
W14-6101,P08-1068,0,0.0413383,"ic, the latter for Basque, French, German, Hebrew, and Hungarian. However, note that Sz´ant´o and Farkas (2014) used the data from the SPMRL shared task 2013, which does not contain grammatical functions in the syntactic annotations. Both approaches found improvements for subsets of morphological features. Other works examine, also within a “pipeline” method, possibilities for ambiguity reduction through modification of tagsets, or of the lexicon by tagset reduction, or through word-clustering. Lakeland (2005) uses lexicalized parsing a` la Collins (1999). Similarly to the more recent work by Koo et al. (2008) or Candito and Seddah (2010), he addresses the question of how to optimally disambiguate for parsing on the lexical level by clustering. A word cluster is thereby seen as an equivalence class of words and assumes to a certain extent the function of a POS tag, but can be adapted to the training data. Le Roux et al. (2012) address the issue of data sparseness on the lexical level with PCFG parsing with the morphologically rich language Spanish. The authors use a reimplementation of the Berkeley parser. They show that parsing results can be improved by simplifying the POS tagset, as well as by l"
W14-6101,W06-1614,1,0.869625,"Missing"
W14-6101,P10-1052,0,0.0507193,"Missing"
W14-6101,W12-3408,0,0.0366199,"Missing"
W14-6101,W12-4615,1,0.850612,"nsiderably in the syntactic annotation scheme. While TiGer uses a very flat annotation involving crossing branches, the annotations in T¨uBa-D/Z are more hierarchical, and long distance relations are modeled via grammatical function labels rather than via attachment. Figures 1 and 2 show examples. For preprocessing, we follow the standard practices from the parsing community. In both treebanks, punctuation and other material, such as parentheses, are not included in the annotation, but attached to a virtual root node. We attach the respective nodes to the tree using the algorithm described by Maier et al. (2012) so that every sentence corresponds to exactly one tree. In a nutshell, this algorithm uses the left and right terminal neighbors as attachment targets. In TiGer, we then remove the crossing branches using a two-stage process. In a first step, we apply the transformation described by Boyd (2007). This transformation introduces a new non-terminal for every continuous block of a discontinuous constituent. We keep a flag on each of the newly introduced nodes that indicates if it dominates the head daughter of the original discontinuous node. Subsequently, we delete all those nodes for which this"
W14-6101,J13-1008,0,0.123556,"been done on investigating different tagsets for individual languages. Collins et al. (1999) adapt the parser of Collins (1999) for the Czech Prague Dependency Treebank. Using an external lexicon to reduce data sparseness for word forms did not result in any improvement, but adding case to the POS tagset had a positive effect. Seddah et al. (2009) investigate the use of different parsers on French. They also investigate two tagsets with different granularity and come to the conclusion that the finer grained tagset leads to higher parser performance. The work that is closest to ours is work by Marton et al. (2013), who investigate the optimal POS tagset for parsing Arabic. They come to the conclusion that adding definiteness, person, number, gender, and lemma information to the POS tagset improve parsing accuracy. Both Dehdari et al. (2011) and Sz´ant´o and Farkas (2014) investigate automatic methods for selecting the best subset of morphological features, the former for Arabic, the latter for Basque, French, German, Hebrew, and Hungarian. However, note that Sz´ant´o and Farkas (2014) used the data from the SPMRL shared task 2013, which does not contain grammatical functions in the syntactic annotation"
W14-6101,N07-1051,0,0.212026,"cal variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is based on support vector machines, TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 1–14 Dublin, Ireland, August 23-29 2014. Our findings for POS tagging show that Morfette reaches the highest accuracy on UTS and overall on unknown words while TnT reaches the best performance for STTS and the RF-Tagger fo"
W14-6101,W08-1005,0,0.0214698,"d distinctions on POS level decrease parsing performance. 1 Introduction German is a non-configurational language with a moderately free word order in combination with a case system. The case of a noun phrase complement generally is a direct indicator of the phrase’s grammatical function. For this reason, a morphological analysis seems to be a prerequisite for a syntactic analysis. However, in computational linguistics, parsing was developed for English without the use of morphological information, and this same architecture is used for other languages, including German (K¨ubler et al., 2006; Petrov and Klein, 2008). An easy way of introducing morphological information into parsing, without modifying the architecture, is to attach morphology to the part-of-speech (POS) tagset. However, this makes POS tagging more complex and thus more difficult. In this paper, we investigate the following questions: 1) How well do the different POS taggers work with tagsets of a varying level of morphological granularity? 2) Do the differences in POS tagger performance translate into similar differences in parsing quality? Complementary POS tagging results and preliminary parsing results have been published in German in"
W14-6101,petrov-etal-2012-universal,0,0.165752,"ological granularity? 2) Do the differences in POS tagger performance translate into similar differences in parsing quality? Complementary POS tagging results and preliminary parsing results have been published in German in K¨ubler and Maier (2013). Our experiments are based on two different treebanks for German, TiGer (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2012). Both treebanks are based on the same POS tagset, the StuttgartT¨ubingen Tagset (STTS) (Schiller et al., 1995). We perform experiments with three variants of the tagset: The standard STTS, the Universal Tagset (UTS) (Petrov et al., 2012) (a language-independent tagset), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is"
W14-6101,W96-0213,0,0.0617186,". (2008) overcome this deficit by automatically searching for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag PRON DET ADP NUM description pronoun determiner, article preposition, postposition numeral tag CONJ PRT . X description conjunction particle punctuation everything else Table 1: The 12 tags of the Universal Tagset. hierarchical, features are decisive for the quality of POS tagging and note that a “pipeline” model does not take this into account since the parser effectively perf"
W14-6101,C08-1098,0,0.0165306,"et, the StuttgartT¨ubingen Tagset (STTS) (Schiller et al., 1995). We perform experiments with three variants of the tagset: The standard STTS, the Universal Tagset (UTS) (Petrov et al., 2012) (a language-independent tagset), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is based on support vector machines, TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License detail"
W14-6101,W09-3824,0,0.0671138,"Missing"
W14-6101,J13-1004,0,0.0149644,"be seen as an equivalence class of words. Since in the “pipeline” approach, the parse tree is built on POS tags, it is possible that a POS tagset is optimal from a linguistic point of view, but that its behavior is not optimal with respect to parsing results, because relevant lexical information is hidden from the parse tree by the POS tagset. While Koo et al. (2008) overcome this deficit by automatically searching for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag PRON DET ADP NUM descri"
W14-6101,E14-1015,0,0.0302323,"Missing"
W14-6101,W00-1308,0,0.071398,"deficit by automatically searching for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag PRON DET ADP NUM description pronoun determiner, article preposition, postposition numeral tag CONJ PRT . X description conjunction particle punctuation everything else Table 1: The 12 tags of the Universal Tagset. hierarchical, features are decisive for the quality of POS tagging and note that a “pipeline” model does not take this into account since the parser effectively performs the POS disambiguation. On t"
W14-6101,N03-1033,0,0.00776892,"ith three variants of the tagset: The standard STTS, the Universal Tagset (UTS) (Petrov et al., 2012) (a language-independent tagset), and an extended version of the STTS that also includes morphological information from the treebanks (STTSmorph). STTS consists of 54 tags, UTS uses 12 basic tags, and the morphological variants of the STTS comprise 783 and 524 POS tags respectively. We use a wide range of POS taggers, which are based on different strategies: Morfette (Chrupala et al., 2008) and RF-Tagger (Schmid and Laws, 2008) are designed for large morphological tagsets, the Stanford tagger (Toutanova et al., 2003) is based on a maximum entropy model, SVMTool (Gim´enez and M`arquez, 2004) is based on support vector machines, TnT (Brants, 2000) is a Markov model trigram tagger, and Wapiti (Lavergne et al., 2010) a conditional random field tagger. For our parsing experiments, we use the Berkeley parser (Petrov and Klein, 2007b; Petrov and Klein, 2007a). This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 First Joint Workshop on Statistical Parsin"
W14-6101,W09-3820,0,0.0250328,"dy mentioned, a POS tag can be seen as an equivalence class of words. Since in the “pipeline” approach, the parse tree is built on POS tags, it is possible that a POS tagset is optimal from a linguistic point of view, but that its behavior is not optimal with respect to parsing results, because relevant lexical information is hidden from the parse tree by the POS tagset. While Koo et al. (2008) overcome this deficit by automatically searching for “better” clusters, other works copy certain lexical information into the actual tree, e.g., by using grammatical function annotation (Versley, 2005; Versley and Rehbein, 2009). Seeker and Kuhn (2013) complement the “pipeline” model (using a dependency parser (Bohnet, 2010)) by an additional component that uses case information as a filter for the parser. They achieve improvements for Hungarian, German and Czech. A number of works develop models for simultaneous POS tagging or morphological segmentation and parsing. Based on work by Ratnaparkhi (1996) and Toutanova and Manning (2000), Chen and Kit (2011) investigate disambiguation on the lexical level. They assume that local, i.e., sequential but not 2 tag NOUN VERB ADJ ADV description noun verb adjective adverb tag"
W14-6101,J03-4003,0,\N,Missing
W14-6111,W13-4916,0,0.216734,"Missing"
W14-6111,H91-1060,0,0.0351043,"hological features for each input segment were not. In the raw setting, there was no gold information, i.e., morphological segmentation, POS tags and morphological features for each input token had to be predicted as part of the parsing task. To lower the entry cost, participants were provided with reasonable baseline (if not state-of-the-art) morphological predictions (either disambiguated – in most cases– or ambiguous prediction in lattice forms). As a consequence of the raw scenario, it was not possible to (only) rely on the accepted parsing metrics, labeled bracket evaluation via E VALB1 (Black et al., 1991), Leaf-Ancestor (Sampson and Babarczy, 2003) for constituents and C O NLL X’s Labeled/Unlabeled Attachment Score for dependencies (Buchholz and Marsi, 2006). When the segmentation of words into input tokens is not given, there may be discrepancies on the lexical levels, which neither E VALB and L EAF -A NCESTOR nor LAS/UAS are prepared to handle. Thus, we also used TedEval, a distance-based metric that evaluates a morphosyntactic structure as a complete whole (Tsarfaty et al., 2012b). Note that given the workload brought to the participants, we did not try to enforce function label evaluation"
W14-6111,J92-4003,0,0.188544,"description of the unlabeled-data preparation that is required in the context of parsing MRLs, and the core labeled data that is used in conjunction with it. 4.1 SPMRL Unlabeled Data Set One of the common problems when dealing with morphologically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks,"
W14-6111,W06-2920,0,0.830006,"aselines for parsing morphologically rich languages (MRLs). The goals were both to provide a focal point for researchers interested in parsing MRLs and consequently to advance the state of the art in this area of research. The shared task focused on parsing nine morphologically rich languages, from different typological language families, in both a constituent-based and a dependency-based format. The set of nine typologically diverse languages comprised data sets for Arabic, Basque, French, German, Hebrew, Hungarian, Korean, Polish, and Swedish. Compared to previous multilingual shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), the SPMRL shared task targeted parsing in realistic evaluation scenarios, in which the analysis of morphologically ambiguous input tokens is not known in advance. An additional novelty of the SPMRL shared task is that it allowed for both a dependency-based and a constituentbased parse representation. This setting relied on an intricate and careful data preparation process which ensured consistency between the constituent and the dependency version by aligning the two representation types at the token level and at the level of part-of-speech tags. For all languages, we pr"
W14-6111,W09-3821,0,0.0148692,"Missing"
W14-6111,W10-1409,1,0.562386,"t is used in conjunction with it. 4.1 SPMRL Unlabeled Data Set One of the common problems when dealing with morphologically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexic"
W14-6111,W13-4909,0,0.0198006,"Missing"
W14-6111,W13-4905,1,0.815015,"e Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the Modern Hebrew Treebank (Sima’an et al., 2001), with the Goldberg (2011) dependency version, in turn aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the"
W14-6111,E09-1038,1,0.836445,"gically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table 2 presents basic facts about the data sets. Details on the unlabeled data and their pre-annotations wi"
W14-6111,C10-1045,0,0.0646224,"X X parsed X* X X* X X X X* X X Table 2: Unlabeled data set properties.*: made available mid-july 4.2 SPMRL Core Labeled Data Set In order to provide a faithful evaluation of the impact of these additional sets of unlabeled data, we used the exact same data sets for training and testing as in the previous edition. Specifically, we used an Arabic data set, originally provided by the LDC (Maamouri et al., 2004), in a dependency form, derived from the Columbia Catib Treebank (Habash and Roth, 2009; Habash et al., 2009) and in a constituency instance, following the Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the T"
W14-6111,P09-2056,0,0.0145597,"re newswire wiki (edited) balanced size (tree tokens) 120M 150M 120M 205M 160M 100M 40M 100M 24M morph X* X X+mwe X X X X X X parsed X* X X* X X X X* X X Table 2: Unlabeled data set properties.*: made available mid-july 4.2 SPMRL Core Labeled Data Set In order to provide a faithful evaluation of the impact of these additional sets of unlabeled data, we used the exact same data sets for training and testing as in the previous edition. Specifically, we used an Arabic data set, originally provided by the LDC (Maamouri et al., 2004), in a dependency form, derived from the Columbia Catib Treebank (Habash and Roth, 2009; Habash et al., 2009) and in a constituency instance, following the Stanford pre-processing scheme (Green and Manning, 2010) and extended according to the SPMRL 2013 extension scheme (Seddah et al., 2013). For Basque, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both insta"
W14-6111,P08-2015,0,0.0097065,"when dealing with morphologically rich languages (MRLs) is lexical data sparseness due to the high level of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table 2 presents basic facts about the data sets. Details on the unlabele"
W14-6111,mengel-lezius-2000-xml,0,0.179115,"reebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in ´ (Woli´nski et al., 2011; Swidzi´ nski and Woli´nski, 2010; Wr´oblewska, 2012). Compared to the last year’s edition, we added explicit feature names in the relevant data fields. The Swedish data originate from (Nivre et al., 2006), we added function labels extracted from the original Swedish XML data. Note that in addition to constituency and dependency versions, the Polish, German and Swedish data sets are also available in the Tiger XML format (Mengel and Lezius, 2000), allowing a direct representation of discontinuous structures in their phrase-based structures. 5 Conclusion At the time of writing this short introduction, the shared task is ongoing, and neither results nor the final submitting teams are known. At this point, we can say that 15 teams registered for the 2014 shared task edition, indicating an increased awareness of and continued interest in the topic of the shared task. Results, cross-parser and cross-data analysis, and shared task description papers will be made available at http://www.spmrl.org/spmrl2014-sharedtask.html. Acknowledgments We"
W14-6111,nivre-etal-2006-talbanken05,0,0.0181359,"match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in ´ (Woli´nski et al., 2011; Swidzi´ nski and Woli´nski, 2010; Wr´oblewska, 2012). Compared to the last year’s edition, we added explicit feature names in the relevant data fields. The Swedish data originate from (Nivre et al., 2006), we added function labels extracted from the original Swedish XML data. Note that in addition to constituency and dependency versions, the Polish, German and Swedish data sets are also available in the Tiger XML format (Mengel and Lezius, 2000), allowing a direct representation of discontinuous structures in their phrase-based structures. 5 Conclusion At the time of writing this short introduction, the shared task is ongoing, and neither results nor the final submitting teams are known. At this point, we can say that 15 teams registered for the 2014 shared task edition, indicating an increase"
W14-6111,W11-3808,0,0.0159627,"l of variation in word forms (Tsarfaty et al., 2010; Tsarfaty et al., 2012c). The use of large, unlabeled corpora in a semi-supervised setting, in addition to the relatively small MRL data sets, can become a valid option to overcome such issues. For instance, using Brown clusters (Brown et al., 1992) has been shown to boost the performance of a PCFG-LA based parser for French (Candito and Crabb´e, 2009; Candito and Seddah, 2010). External lexical acquisition was successfully used for Arabic (Habash, 2008) and Hebrew (Goldberg et al., 2009), self-training increased accuracy for parsing German (Rehbein, 2011), and more recently, the use of word embeddings led to some promising results for some MRLs (Cirik and S¸ensoy, 2013). By releasing large, unlabeled data sets and by providing accurate pre-annotation in a format directly compatible with models trained on the SPMRL Shared Task treebanks, we hope to foster the development of interesting and feature-rich parsing models that build on larger, morphologically rich, lexicons. Table 2 presents basic facts about the data sets. Details on the unlabeled data and their pre-annotations will be provided in (Seddah et al., 2014). Note that we could not ensur"
W14-6111,seeker-kuhn-2012-making,0,0.0136617,"ue, the data was provided by Aduriz et al. (2003) in both dependency and constituency, we removed sentences with non-projective trees so both instances could be aligned at the token level. Regarding French, we used a new instance of the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the Modern Hebrew Treebank (Sima’an et al., 2001), with the Goldberg (2011) dependency version, in turn aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Po"
W14-6111,W10-1401,1,0.903368,"Missing"
W14-6111,E12-1006,1,0.924838,"enario, it was not possible to (only) rely on the accepted parsing metrics, labeled bracket evaluation via E VALB1 (Black et al., 1991), Leaf-Ancestor (Sampson and Babarczy, 2003) for constituents and C O NLL X’s Labeled/Unlabeled Attachment Score for dependencies (Buchholz and Marsi, 2006). When the segmentation of words into input tokens is not given, there may be discrepancies on the lexical levels, which neither E VALB and L EAF -A NCESTOR nor LAS/UAS are prepared to handle. Thus, we also used TedEval, a distance-based metric that evaluates a morphosyntactic structure as a complete whole (Tsarfaty et al., 2012b). Note that given the workload brought to the participants, we did not try to enforce function label evaluation for constituent parsing. We hope that further shared tasks will try to generalize such an evaluation. Indeed, having predicted function labels would ease labeled T ED E VAL evaluation and favor a full parsing chain evaluation. Nevertheless, the choice of T ED E VAL allowed us to go beyond the standard cross-parser evaluation within one setting and approach cross-framework (constituent vs. dependency (Tsarfaty et al., 2012a)) and cross-language evaluation, thus pushing the envelope"
W14-6111,P13-2103,1,0.454845,"the French Treebank (Abeill´e et al., 2003) that includes multi-word expression (MWE) annotations, annotated at the morpho-syntactic level in both instances. Predicted MWEs were added this year, using the same tools as Constant et al. (2013). The German data are based on the Tiger corpus (Brants et al., 2002), and converted to constituent and dependency following (Seeker and Kuhn, 2012). The Hebrew data set is based on the Modern Hebrew Treebank (Sima’an et al., 2001), with the Goldberg (2011) dependency version, in turn aligned with the phrase structure instance described in (Tsarfaty, 2010; Tsarfaty, 2013). Note that in order to match the Hebrew unlabeled data encoding, the Hebrew treebank was converted back to UTF-8. The Hungarian data are derived from the Szeged treebank (Csendes et al., 2005; Vincze et al., 2010), while the Korean data originate from the Kaist Treebank (Choi et al., 1994) which was converted to dependency for the SPMRL shared task by Choi (2013). The Polish treebank we used is described in ´ (Woli´nski et al., 2011; Swidzi´ nski and Woli´nski, 2010; Wr´oblewska, 2012). Compared to the last year’s edition, we added explicit feature names in the relevant data fields. The Swedi"
W14-6111,J13-1003,1,\N,Missing
W14-6111,C08-1112,1,\N,Missing
W14-6111,P12-2002,1,\N,Missing
W14-6111,W13-4917,1,\N,Missing
W14-6111,vincze-etal-2010-hungarian,0,\N,Missing
W14-6111,D07-1096,1,\N,Missing
W15-0701,P10-1061,0,0.0241141,"ework to include multiple source languages. In previous work (Scrivner and K¨ubler, 2012), we used another cross-language transfer method, based on the work by Feldman and Hana (2010), to annotate the Flamenca corpus with POS and syntactic annotations. This method does not require parallel texts, it rather uses resources such as lexical or POS taggers from closely related languages. Our annotations were based on resources from Old French (Martineau et al., 2010) and modern Catalan (Civit et al., 2006). In machine translation, the transfer of sentiment analysis is common in machine translation.Kim et al. (2010) use a machine translation system to map subjectivity lexica from English to other languages. In word sense disambiguation, word alignment has been used as a bridge (Tufis, 2007) based on the assumption that the translated word shares the same sense with the original word. A similar method was used for sentiment transfer from a resource-rich language to a resource-poor language (Mihalcea et al., 2007). 4 Romance of Flamenca Medieval Provenc¸al literature is well known for its lyric poetry of troubadours. There remains, however, a small number of non-lyric provenc¸al texts, such as Romance of F"
W15-0701,P07-1123,0,0.0129323,"ns were based on resources from Old French (Martineau et al., 2010) and modern Catalan (Civit et al., 2006). In machine translation, the transfer of sentiment analysis is common in machine translation.Kim et al. (2010) use a machine translation system to map subjectivity lexica from English to other languages. In word sense disambiguation, word alignment has been used as a bridge (Tufis, 2007) based on the assumption that the translated word shares the same sense with the original word. A similar method was used for sentiment transfer from a resource-rich language to a resource-poor language (Mihalcea et al., 2007). 4 Romance of Flamenca Medieval Provenc¸al literature is well known for its lyric poetry of troubadours. There remains, however, a small number of non-lyric provenc¸al texts, such as Romance of Flamenca, Girart de Rossilho and Daurel et Beto, that have not received much attention. In this project we focus on Romance of Flamenca, which can be faithfully described as “the one universally acknowledged masterpiece of Old Occitan narrative” (Fleischmann, 1995). This anonymous romance, written in the 13th century, presents an artistic amalgam of fabliau, courtly romance, troubadours lyrics, and nar"
W15-0701,W11-1709,0,0.0224002,"s to take advantage of English resources in combination with the word alignment. I.e., we can annotate emotions in English, which is easy enough to do given a lexicon and an undergraduate student. In a second step, we transfer the annotation via the word alignments to the source language. Below, we will describe our emotion annotation transfer method. First, we compiled a word list from the English version Flamenca and removed common function words. We then used the NRC emotion lexicon4 , which consists of words and their associations with 8 emotions as well as positive or negative sentiment. Mohammad and Yang (2011) created this lexicon by using frequent words from the 1911 Roget Thesaurus5 , which were annotated by 5 human annotators. For our application, we focus on the 8 emotion associations: anger, anticipation, disgust, fear, joy, sadness, surprise and trust. Since the emotion annotation is not neutral with regard to context, several emotions can be assigned to the same token in the NRC lexicon, as shown in (4). (4) abandon abandon lose lose lose fear sadness anger disgust fear As we can see, the word abandon has two associations, namely with fear and sadness, and the word lose has three possible as"
W15-0701,C00-2163,0,0.443733,"Occitan. Section 7 introduces emotion queries via ANNIS, a freely available on-line search platform, and visualization methods with motion charts in GoogleViz. Section 8 draws general conclusions and provides an outlook on future steps 2 for the project. 2 Using Corpora in Historical Linguistics Parallel corpora are collections of two or more texts consisting of an original text (source) and its translation(s). Generally, such parallel corpora are annotated for word alignment between the source text and its translation. Word alignment can be carried out automatically via tools such as GIZA++ (Och and Ney, 2000). Monolingual historical corpora are undoubtedly valuable linguistic resources. It is not uncommon, however, to encounter different spellings and other lexical variations in historical texts. Not knowing an exact spelling or just a simple language barrier may hinder data collection. Parallel corpora can assist in such situations through Historic Document Retrieval (Koolen et al., 2006), which allows researchers to query via the modern translation rather than via the older language. Given the common assumption that “translation equivalents are likely to be inserted in the same or very similar,"
W15-0701,W12-1007,0,0.0188791,"o look at the overall emotional content, by querying for any words that have emotion; in other words, they are not annotated as “None” for emotion (query: emotion ! = ”N one”). We can then perform a frequency analysis of the results. The frequency distribution is shown in Figure 4, where we see that trust and joy are the most common emotions. In recent years, visual and dynamic applications in corpus and literature studies have become more important, thus showing a focus on non-technical users. For example, (Moretti, 2005) advocates the use of maps, trees, and graphs in the literary analysis. Oelke et al. (2012) suggest person network representation, showing relations between characters. In addition, they also use fingerprint plots, which allow to compare the mentioning of various categories, e.g. male, female, across several novels. Hilpert (2011) introduces dynamic motion Figure 2: Search for joy with the speaker Father. Figure 3: Resulting visualization for the query in Figure 2. charts as a visualization methodology to the literary and linguistic studies. These charts are common in the socio-economic statistic field and are capable of visualizing in motion the development of a phenomenon in quest"
W15-0701,R09-2011,0,0.0119932,"tic variation. With a parallel corpus, it is possible to search for the explicit form in a translated text and then observe the use or omission of that form in the original text. For instance, in his study of Biblia Medieval, a parallel corpus of Old Spanish Bible translations, Enrique-Arias (2013) analyzes discourse markers and observes instances of zero-marking in Old Spanish by searching for explicit Hebrew markers. Furthermore, such corpora can be used to investigate the convergence universal in machine translation, a correlation between zero anaphora and the degree of text explicitation (Rello and Ilisei, 2009). We argue that the addition of a translation into a modern language is a simple and intuitive way of giving access to historical texts. This is a useful 3 tool not only for historians and historical linguists but also for a lay audience since the translation provides access to the meaning without introducing additional hurdles such as by a semantic annotation. In section 4, we will present our corpus of choice, the Romance of Flamenca, an Old Occitan poem, along with the parallel version that includes a modern English translation. Then, we will present an approach to annotate this corpus for"
W15-0701,P08-1084,0,0.019122,"annotation. Cross-language transfer has been previously applied to languages with parallel corpora and bilingual lexica (Yarowsky and Ngai, 2001; Hwa et al., 2005). This approach uses parallel text and word alignment to transfer the annotation from one language to the next. Yarowsky and Ngai (2001) show the transfer of a coarse grained POS tagset and base noun phrases from English to French. Yarowsky et al. (2001) extend the approach to Spanish and Czech, and they include named entity tagging. Hwa et al. (2005) use a similar approach to transfer dependency annotations from English to Chinese. Snyder and Barzilay (2008) extend the approach to unsupervised annotation of morphology in Semitic languages via a hierarchical Bayesian network. And Snyder et al. (2009) extend the framework to include multiple source languages. In previous work (Scrivner and K¨ubler, 2012), we used another cross-language transfer method, based on the work by Feldman and Hana (2010), to annotate the Flamenca corpus with POS and syntactic annotations. This method does not require parallel texts, it rather uses resources such as lexical or POS taggers from closely related languages. Our annotations were based on resources from Old Frenc"
W15-0701,P09-1009,0,0.019741,"et al., 2005). This approach uses parallel text and word alignment to transfer the annotation from one language to the next. Yarowsky and Ngai (2001) show the transfer of a coarse grained POS tagset and base noun phrases from English to French. Yarowsky et al. (2001) extend the approach to Spanish and Czech, and they include named entity tagging. Hwa et al. (2005) use a similar approach to transfer dependency annotations from English to Chinese. Snyder and Barzilay (2008) extend the approach to unsupervised annotation of morphology in Semitic languages via a hierarchical Bayesian network. And Snyder et al. (2009) extend the framework to include multiple source languages. In previous work (Scrivner and K¨ubler, 2012), we used another cross-language transfer method, based on the work by Feldman and Hana (2010), to annotate the Flamenca corpus with POS and syntactic annotations. This method does not require parallel texts, it rather uses resources such as lexical or POS taggers from closely related languages. Our annotations were based on resources from Old French (Martineau et al., 2010) and modern Catalan (Civit et al., 2006). In machine translation, the transfer of sentiment analysis is common in mach"
W15-0701,N01-1026,0,0.0629022,"the Romance of Flamenca, an Old Occitan poem, along with the parallel version that includes a modern English translation. Then, we will present an approach to annotate this corpus for emotions, using non-experts working on the English part and then transferring the annotation to the source language. 3 Using Alignment Methods for Annotating Resource-Poor Languages Linguistic annotation often involves a great amount of manual labor, which is often not feasible for low-resourced languages. Instead, we can use a method from computational linguistics, namely cross-language transfer, as proposed by Yarowsky and Ngai (2001). This method does not involve any resources in the target language, neither training data, a large lexicon, nor time-consuming manual annotation. Cross-language transfer has been previously applied to languages with parallel corpora and bilingual lexica (Yarowsky and Ngai, 2001; Hwa et al., 2005). This approach uses parallel text and word alignment to transfer the annotation from one language to the next. Yarowsky and Ngai (2001) show the transfer of a coarse grained POS tagset and base noun phrases from English to French. Yarowsky et al. (2001) extend the approach to Spanish and Czech, and t"
W15-0701,H01-1035,0,0.122368,"amely cross-language transfer, as proposed by Yarowsky and Ngai (2001). This method does not involve any resources in the target language, neither training data, a large lexicon, nor time-consuming manual annotation. Cross-language transfer has been previously applied to languages with parallel corpora and bilingual lexica (Yarowsky and Ngai, 2001; Hwa et al., 2005). This approach uses parallel text and word alignment to transfer the annotation from one language to the next. Yarowsky and Ngai (2001) show the transfer of a coarse grained POS tagset and base noun phrases from English to French. Yarowsky et al. (2001) extend the approach to Spanish and Czech, and they include named entity tagging. Hwa et al. (2005) use a similar approach to transfer dependency annotations from English to Chinese. Snyder and Barzilay (2008) extend the approach to unsupervised annotation of morphology in Semitic languages via a hierarchical Bayesian network. And Snyder et al. (2009) extend the framework to include multiple source languages. In previous work (Scrivner and K¨ubler, 2012), we used another cross-language transfer method, based on the work by Feldman and Hana (2010), to annotate the Flamenca corpus with POS and s"
W16-6316,W06-1615,0,0.282143,"no filter. K¨ubler and Baucom (2011) expanded on this method and used three different taggers to annotate additional data and then select those sentences for which the taggers agree. They found that adding not only complete sentences but also sequences of words where the taggers agree re122 sults in the highest gains. Khan et al. (2013) investigated the situation where some annotated target data is available. They focused on optimizing the balance between source and target sentences. They found that selecting sentences that are the most similar to the target data results in the highest gains. Blitzer et al. (2006) developed structural correspondence learning, which learns correspondences between two domains in settings where a small set of target sentences is available as well as in an unsupervised setting. They show an improved performance for POS tagging and for parsing when using the adapted POS tagger. 4 4.1 Experimental Setup Data Sets For our experiments, we use the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1994) and the GENIA Corpus (version 3.02) (Tateisi and Tsujii, 2004). Both corpora use the Penn Treebank POS tagset (Santorini, 1990) with minor differences, as de"
W16-6316,A00-1031,0,0.589231,"Missing"
W16-6316,W03-0407,0,0.60644,"We will additionally look at the confusion matrices. 3 Related Work We are not aware of any research that directly compares to the research presented here. The closest area is domain adaptation. For this reason, we will cover work on domain adaptation for POS tagging here. However, more work has been done on domain adaptation for parsing. The work in that area seems to fall into two categories: “frustratingly easy” when some annotated data from the target domain is available (Daum´e III, 2007) and “frustratingly hard” if no such target data is available (Dredze et al., 2007). For POS tagging, Clark et al. (2003) used the results of one POS tagger on unannotated data to inform the training of another tagger in a semisupervised setting using a co-training routine with a Markov model tagger and a maximum entropy tagger. The authors tested both agreement-based co-training, where the sentences are added to training only if the taggers both agree, and naive co-training, where all sentences from one tagger are added to the training of the other, with no filter. K¨ubler and Baucom (2011) expanded on this method and used three different taggers to annotate additional data and then select those sentences for w"
W16-6316,P07-1033,0,0.116647,"Missing"
W16-6316,R13-1046,1,0.901265,"Missing"
W16-6316,R11-1006,1,0.826309,"Missing"
W16-6316,H94-1020,0,0.905623,"ould have headlines with an elliptical sentence structure. For this reason, we assume that the POS tagger can reach a higher accuracy if we can split the data sets into more homogeneous subsets and then train individual expert POS taggers, specialized for individual subsets. We determine these homogeneous subsets by using topic modeling. Since topic modeling is unsupervised, the sentences will be divided into sets based on similarity. This similarity may be based on similarity of content, but it can also be based on similarity on the structural level. For example, if we use the Penn Treebank (Marcus et al., 1994), we could assume that one topic consists of sentences reporting changes of the stock market while another topic consists of sentences about the earthquake in San Francisco. Yet, another topic may consist of mostly questions. Our current research concentrates on answering the four questions described below. In this investigation, we use a setting in which we perform topic modeling jointly on the training and test data. This is a simplification of the problem since this means that we would have to create new experts every time a new sentence needs to be tagged. We assume that we can rerun the t"
W16-6316,W07-1024,0,0.0321203,"ntence with the corresponding POS tagging expert. We show that using topic model experts enhances the accuracy of POS tagging by around half a percent point on average over the random baseline, and the 2-topic hard clustering model and the 10-topic soft clustering model improve over the full training set. 1 Introduction Part-of-speech (POS) tagging is the task of assigning word classes to lexical items and is often considered a solved problem. However, even though we can reach high accuracies on the Penn Treebank, POS taggers are sensitive to differences in genre (cf. e.g. (Khan et al., 2013; Miller et al., 2007; Søgaard, 2013)). In the current research, we investigate a novel way of adapting POS taggers to different genres, but also to specific lexical and syntactic characteristics of texts. We propose to use topic modeling, an unsupervised soft clustering method that clusters documents, or sentences in our case, into a distribution of individual top120 ics. We interpret the topics as specialized training sets, which are used to train a POS tagging expert for each topic. Test sentences are also clustered into the same topics, and each test sentence is annotated by the corresponding POS tagging exper"
W16-6316,P11-1157,0,0.418823,"Missing"
W16-6316,N13-1077,0,0.0150278,"sponding POS tagging expert. We show that using topic model experts enhances the accuracy of POS tagging by around half a percent point on average over the random baseline, and the 2-topic hard clustering model and the 10-topic soft clustering model improve over the full training set. 1 Introduction Part-of-speech (POS) tagging is the task of assigning word classes to lexical items and is often considered a solved problem. However, even though we can reach high accuracies on the Penn Treebank, POS taggers are sensitive to differences in genre (cf. e.g. (Khan et al., 2013; Miller et al., 2007; Søgaard, 2013)). In the current research, we investigate a novel way of adapting POS taggers to different genres, but also to specific lexical and syntactic characteristics of texts. We propose to use topic modeling, an unsupervised soft clustering method that clusters documents, or sentences in our case, into a distribution of individual top120 ics. We interpret the topics as specialized training sets, which are used to train a POS tagging expert for each topic. Test sentences are also clustered into the same topics, and each test sentence is annotated by the corresponding POS tagging expert. We investigat"
W16-6316,tateisi-tsujii-2004-part,0,0.838063,". 2.1 Question 1: Do Topic Models Provide Information from which POS Tagging Experts can Profit? The first question is concerned with determining whether the data splits we obtain from topic modeling are meaningful for creating POS tagging experts. In other words, do the topics that we can generate in an unsupervised manner pro121 vide a specialization that has an effect on POS tagging? In order to investigate this question, we manually generate a two-topic corpus by combining data from the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1994) and from the GENIA corpus (Tateisi and Tsujii, 2004). The WSJ covers financial news while GENIA uses Medline abstracts as its textual basis. As a consequence, we have sentences from two different genres, but also slight variations in the POS tagsets. The tagset used in GENIA is based on the Penn Treebank tagset, but it uses the tags for proper names and symbols only in very restricted contexts. This setup allows us to test whether the topic modeler is able to distinguish the two genres, and whether POS tagging experts can profit from this separation. 2.2 Question 2: Can we use Soft Clusters of the Topic Models? The first set of experiments uses"
W17-5046,W13-1728,0,0.0161966,"ll experiments, we use the C-Support Vector Classifier implementation in scikit-learn with a linear kernel.5 For feature values, rather than using a frequency count matrix for features in the document, the TF-IDF score for the term is used instead. TF is the term frequency, i.e., the number of occurrences of a term in a document. IDF is the inverse document frequency, which is calculated by dividing the total number of documents in a corpus by the number of documents containing term t (if t is not equal 0). The application of TF-IDF weighting has been used to good effect in previous research (Gebre et al., 2013). The benefit of using TF-IDF weighting in this task is that it dampens the effect of terms that are well dispersed throughout the corpus while emphasizing terms that occur less frequently and only within a smaller set of documents. For NLI, TFIDF weighting is useful for capturing and amplifying the effects of vocabulary choices that are L1specific. When using binary features, for example, only the presence or absence of a feature is recorded. This effectively weights rare features, such as low frequency words and spelling errors, the same as common features, such as function words. In contras"
W17-5046,W13-1706,0,0.0727021,"eristics of their L1 onto L2. For example, a Japanese L1 speaker may transfer the rigid CV syllable structure onto English and epenthesize vowels into consonant clusters, which may also be reflected in writing. Thus, having phonetic information may prove useful in an NLI classification task. However, we need to make sure that the features we add can be acquired from text and do not contribute to data sparsity. For the IUCL system in the Native Language Identification Shared Task (Mal2 Related Work The first NLI Shared Task was part of the 2013 Building Educational Applications (BEA) workshop (Tetreault et al., 2013). Participants received the training portion of the TOEFL11 corpus and were asked to identify the native language of the essay writer from among a closed set of 11 languages available in the corpus. Scoring was based on classification accuracy on an unseen test set in 3 tasks: 1 closed training task where only training data provided in the TOEFL11 corpus could be used, and 2 open training tasks. In the first open training task, researchers could use any training data except for the TOEFL11 corpus. In the second task, they could use any training data including the TOEFL11 corpus. Both character"
W17-5046,W13-1714,0,0.484014,"Missing"
W17-5046,W07-0602,0,0.506087,"Missing"
W17-5046,U09-1008,0,0.601887,"Missing"
W17-5046,D11-1148,0,0.0238224,"of articulation in the mouth. The approach abstracts over issues of 1-to-many sound-symbol correspondence in English. For example, the sound /ks/ can be written as both <ks&gt; and <x&gt; as in tacks and tax. Soundex converts the two spellings to two different representations. In contrast, the NYSIIS and the Metaphone family of algorithms (Philips, 1990, 2000) go a step further to incorporate more of the peculiarities of English spelling, which is necessary for better mappings of homophones. Additionally, a number of studies have used syntactic features: context-free grammar (CFG) production rules (Wong and Dras, 2011; Bykh and Meurers, 2014), Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012), and Stanford Dependencies (Malmasi and Cahill, 2015). 1 Features were generated using the Fuzzy library https://pypi.python.org/pypi/Fuzzy 406 Figure 1: Distribution of data in the development set by number of essays per prompt for each L1. ing characters for longer words or appending zeros until there are 3 digits for shorter words. Table 1 shows an example sentence from the training set written by a Turkish speaker converted to keys using Soundex and the other algorithms in this paper. One of t"
W17-5046,W15-0606,0,0.460017,"can be written as both <ks&gt; and <x&gt; as in tacks and tax. Soundex converts the two spellings to two different representations. In contrast, the NYSIIS and the Metaphone family of algorithms (Philips, 1990, 2000) go a step further to incorporate more of the peculiarities of English spelling, which is necessary for better mappings of homophones. Additionally, a number of studies have used syntactic features: context-free grammar (CFG) production rules (Wong and Dras, 2011; Bykh and Meurers, 2014), Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012), and Stanford Dependencies (Malmasi and Cahill, 2015). 1 Features were generated using the Fuzzy library https://pypi.python.org/pypi/Fuzzy 406 Figure 1: Distribution of data in the development set by number of essays per prompt for each L1. ing characters for longer words or appending zeros until there are 3 digits for shorter words. Table 1 shows an example sentence from the training set written by a Turkish speaker converted to keys using Soundex and the other algorithms in this paper. One of the the advantages of the Soundex algorithm is that it is easy to implement the small number of rules mapping from letters to numbers. However, since it"
W17-5046,W17-1201,0,0.0982964,"Missing"
W17-5046,W17-5007,0,0.0420942,"Missing"
W17-5046,W16-4801,0,0.0543631,"Missing"
W17-5046,P12-2038,0,0.0254885,"l correspondence in English. For example, the sound /ks/ can be written as both <ks&gt; and <x&gt; as in tacks and tax. Soundex converts the two spellings to two different representations. In contrast, the NYSIIS and the Metaphone family of algorithms (Philips, 1990, 2000) go a step further to incorporate more of the peculiarities of English spelling, which is necessary for better mappings of homophones. Additionally, a number of studies have used syntactic features: context-free grammar (CFG) production rules (Wong and Dras, 2011; Bykh and Meurers, 2014), Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012), and Stanford Dependencies (Malmasi and Cahill, 2015). 1 Features were generated using the Fuzzy library https://pypi.python.org/pypi/Fuzzy 406 Figure 1: Distribution of data in the development set by number of essays per prompt for each L1. ing characters for longer words or appending zeros until there are 3 digits for shorter words. Table 1 shows an example sentence from the training set written by a Turkish speaker converted to keys using Soundex and the other algorithms in this paper. One of the the advantages of the Soundex algorithm is that it is easy to implement the small number of ru"
W18-1603,C14-1185,0,0.0177835,"atures. They discovered, for example, that translated text use significantly more pronouns than the original texts, across all genres. But they were unable to investigate the syntactic contexts in which those overused pronouns occur most often. For them, syntactic features were examined through word n-grams, similar to previous studies on Indo-European languages, but no text classification task was carried out. 2.2 # texts news LCMC ZCTC 88 88 general prose 206 206 science fiction total 80 80 111 111 485 485 Table 1: Distribution of texts across genres and Meurers, 2014; Wong and Dras, 2011). Bykh and Meurers (2014) showed that using a form of normalized counts of lexicalized CFG rules plus ngrams as features in an ensemble model performed better than all other previous systems. Wong and Dras (2011) reported that using unlexicalized CFG rules (except for function words) from two parsers yielded statistically higher accuracy than simple lexical features (function words, character and POS n-grams). Other approaches have used rules of tree substitution grammar (TSG) (Post and Bergsma, 2013; Swanson and Charniak, 2012) in NLI. Swanson and Charniak (2012) compared the results of CFG rules and two variants of"
W18-1603,hu-etal-2017-non,1,0.841562,"Missing"
W18-1603,P13-2150,0,0.0224718,"tion total 80 80 111 111 485 485 Table 1: Distribution of texts across genres and Meurers, 2014; Wong and Dras, 2011). Bykh and Meurers (2014) showed that using a form of normalized counts of lexicalized CFG rules plus ngrams as features in an ensemble model performed better than all other previous systems. Wong and Dras (2011) reported that using unlexicalized CFG rules (except for function words) from two parsers yielded statistically higher accuracy than simple lexical features (function words, character and POS n-grams). Other approaches have used rules of tree substitution grammar (TSG) (Post and Bergsma, 2013; Swanson and Charniak, 2012) in NLI. Swanson and Charniak (2012) compared the results of CFG rules and two variants of TSG rules and showed that TSG rules obtained through Bayesian methods reached the best results. Nevertheless, such deep syntactic features are rarely used, if at all, in the identification of translated texts. This is the gap that we hope to fill. 3 3.1 Experimental Setup Dataset We use the comparable corpus by Xiao and Hu (2015), which is composed of 500 original Chinese texts from the Lancaster Corpus of Modern Chinese (LCMC), and another 500 human translated Chinese texts"
W18-1603,P11-1132,0,0.0581723,"Missing"
W18-1603,P09-2012,0,0.103485,"e from content words. Thus no lexical rules are considered, and POS tags are considered to be the leaf nodes (Figure 2). We experiment with subtrees of depth up to 3 since the number of subtrees grows exponentially as the depth increases. With depth 3, we are already facing more than 1 billion features. Performing subtree extraction and feature selection becomes difficult and time consuming. Also note that CFGRs are essentially subtrees of depth 1. So with increasing maximum depth of subtrees, we test fewer local relations in constituent parses. In the future, we plan to use Bayesian methods (Post and Gildea, 2009) to sample from all the subtrees. We also conduct separate experiments using subtrees headed by a specific label (we only look at NP, VP, IP, and CP, since they are the most frequent types of subtrees). For example, using NP subtrees as features will inform us how important the noun phrase structure is in identifying translationese. Context-Free Grammar Context-free grammar rules (CFGR) We use the count of each CFG rule extracted from the parse trees. Subtrees Subtrees are defined as any part of the constituent tree of any depth, closely following the data-oriented parsing (DOP) paradigm (Bod"
W18-1603,J12-4004,0,0.0577125,"Missing"
W18-1603,D11-1008,0,0.0117499,"syntactic features, we use various forms of constituent and dependency parses of the sentences. We extract the following features based on either type of parse using the CoreNLP parser with its pre-trained parsing model. 3.3.1 PN 照 一起 together take We remove URLs and headlines, normalize irregular ellipsis (e.g. “。。。”, “....”) to “. . . . . . ”, change all half-width punctuations to full-width, so that our text is compatible with the Chinese Penn Treebank (Xue et al., 2005), which is the training data for the Stanford CoreNLP parser (Manning et al., 2014) used in our study. 3.3 PU VP NP 2009; Sangati and Zuidema, 2011; Swanson and Charniak, 2012) in that we do not include any lexical information in order to exclude topical influence from content words. Thus no lexical rules are considered, and POS tags are considered to be the leaf nodes (Figure 2). We experiment with subtrees of depth up to 3 since the number of subtrees grows exponentially as the depth increases. With depth 3, we are already facing more than 1 billion features. Performing subtree extraction and feature selection becomes difficult and time consuming. Also note that CFGRs are essentially subtrees of depth 1. So with increasing maximum dept"
W18-1603,levy-andrew-2006-tregex,0,0.0329427,"Missing"
W18-1603,P12-2038,0,0.126885,"485 485 Table 1: Distribution of texts across genres and Meurers, 2014; Wong and Dras, 2011). Bykh and Meurers (2014) showed that using a form of normalized counts of lexicalized CFG rules plus ngrams as features in an ensemble model performed better than all other previous systems. Wong and Dras (2011) reported that using unlexicalized CFG rules (except for function words) from two parsers yielded statistically higher accuracy than simple lexical features (function words, character and POS n-grams). Other approaches have used rules of tree substitution grammar (TSG) (Post and Bergsma, 2013; Swanson and Charniak, 2012) in NLI. Swanson and Charniak (2012) compared the results of CFG rules and two variants of TSG rules and showed that TSG rules obtained through Bayesian methods reached the best results. Nevertheless, such deep syntactic features are rarely used, if at all, in the identification of translated texts. This is the gap that we hope to fill. 3 3.1 Experimental Setup Dataset We use the comparable corpus by Xiao and Hu (2015), which is composed of 500 original Chinese texts from the Lancaster Corpus of Modern Chinese (LCMC), and another 500 human translated Chinese texts from the Zhejiang-University"
W18-1603,W14-5902,1,0.901149,"Missing"
W18-1603,P14-1069,0,0.0277097,"ic punctuation “、”. Our methodology can, in principle, be applied to any stylistic comparisons in the digital humanities, and can yield stylistic insights much deeper than the pioneering work of Mosteller and Wallace (1963). In future work, we will investigate tree substitution grammar (TSG), which extracts even deeper constituent trees (c.f. Post and Gildea, 2009), and detailed feature interpretation for phrases headed by other tags (ADJP, PP, etc.) and for specific genres. It is also desirable to improve the accuracy of constituent parsers for Chinese, along the lines of (Wang et al., 2013; Wang and Xue, 2014; Hu et al., 2017), since accurate syntactic trees are the prerequisite for accurate feature interpretation. While the parser in this study works well, better parsers will undoubtedly be a plus. are not in the top 100 features (the highest ranking 191, VV DOBJ 它 it). Thus we see the two types of syntactic features (constituent trees and dependency trees) converging to the same conclusion. If we look at the pronoun issue from the opposite side, a reasonable consequence would be that in original texts, more common nouns should serve as the subject, which is indeed what we find. VV NSUBJ NN predi"
W18-1603,S16-1064,1,0.899798,"Missing"
W18-1603,P13-2110,0,0.0126462,"ns26 Chinese-specific punctuation “、”. Our methodology can, in principle, be applied to any stylistic comparisons in the digital humanities, and can yield stylistic insights much deeper than the pioneering work of Mosteller and Wallace (1963). In future work, we will investigate tree substitution grammar (TSG), which extracts even deeper constituent trees (c.f. Post and Gildea, 2009), and detailed feature interpretation for phrases headed by other tags (ADJP, PP, etc.) and for specific genres. It is also desirable to improve the accuracy of constituent parsers for Chinese, along the lines of (Wang et al., 2013; Wang and Xue, 2014; Hu et al., 2017), since accurate syntactic trees are the prerequisite for accurate feature interpretation. While the parser in this study works well, better parsers will undoubtedly be a plus. are not in the top 100 features (the highest ranking 191, VV DOBJ 它 it). Thus we see the two types of syntactic features (constituent trees and dependency trees) converging to the same conclusion. If we look at the pronoun issue from the opposite side, a reasonable consequence would be that in original texts, more common nouns should serve as the subject, which is indeed what we fin"
W18-1603,D11-1148,0,0.211087,"y of mostly lexical features. They discovered, for example, that translated text use significantly more pronouns than the original texts, across all genres. But they were unable to investigate the syntactic contexts in which those overused pronouns occur most often. For them, syntactic features were examined through word n-grams, similar to previous studies on Indo-European languages, but no text classification task was carried out. 2.2 # texts news LCMC ZCTC 88 88 general prose 206 206 science fiction total 80 80 111 111 485 485 Table 1: Distribution of texts across genres and Meurers, 2014; Wong and Dras, 2011). Bykh and Meurers (2014) showed that using a form of normalized counts of lexicalized CFG rules plus ngrams as features in an ensemble model performed better than all other previous systems. Wong and Dras (2011) reported that using unlexicalized CFG rules (except for function words) from two parsers yielded statistically higher accuracy than simple lexical features (function words, character and POS n-grams). Other approaches have used rules of tree substitution grammar (TSG) (Post and Bergsma, 2013; Swanson and Charniak, 2012) in NLI. Swanson and Charniak (2012) compared the results of CFG r"
W18-1603,mcenery-xiao-2004-lancaster,0,0.794004,"composed of 500 original Chinese texts from the Lancaster Corpus of Modern Chinese (LCMC), and another 500 human translated Chinese texts from the Zhejiang-University Corpus of Translated Chinese (ZCTC). All texts are of similar lengths (~2000 words), and from different genres. There are four broad genres: news, general prose, science, and fiction (see Table 1), and 15 second-level categories. We exclude texts from the second-level categories “science fiction” and “humor” (both under fiction) since they only have 6 and 9 texts respectively, which is not enough for a classification task. LCMC (McEnery and Xiao, 2004) was originally designed for “synchronic studies of Chinese and the contrastive studies of Chinese and English” (see Xiao and Hu, 2015, chapter 4.2). It includes written Chinese sampled from 1989 to 1993, amounting to about one million words. ZCTC was created specifically for translation Syntactic Features in Text Classification Although n-gram features are more prevalent in text-classification tasks, deep syntactic features have been found useful as well. In the Native Language Identification (NLI) literature, which in many respects is similar to the task of detecting translations, various fo"
W18-1603,W03-1730,0,0.201239,"Missing"
W18-1603,P14-5010,0,\N,Missing
W18-1603,D11-1034,0,\N,Missing
W98-1203,1995.iwpt-1.15,0,0.0601732,"Missing"
