W12-2006,{HOO} 2012: A Report on the Preposition and Determiner Error Correction Shared Task,2012,8,91,1,1,42377,robert dale,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"Incorrect usage of prepositions and determiners constitute the most common types of errors made by non-native speakers of English. It is not surprising, then, that there has been a significant amount of work directed towards the automated detection and correction of such errors. However, to date, the use of different data sets and different task definitions has made it difficult to compare work on the topic. This paper reports on the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners, where systems developed by 14 teams from around the world were evaluated on the same previously unseen errorful text."
dale-narroway-2012-framework,A Framework for Evaluating Text Correction,2012,6,9,1,1,42377,robert dale,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Computer-based aids for writing assistance have been around since at least the early 1980s, focussing primarily on aspects such as spelling, grammar and style. The potential audience for such tools is very large indeed, and this is a clear case where we might expect to see language processing applications having a significant real-world impact. However, existing comparative evaluations of applications in this space are often no more than impressionistic and anecdotal reviews of commercial offerings as found in software magazines, making it hard to determine which approaches are superior. More rigorous evaluation in the scholarly literature has been held back in particular by the absence of shared datasets of texts marked-up with errors, and the lack of an agreed evaluation framework. Significant collections of publicly available data are now appearing; this paper describes a complementary evaluation framework, which has been piloted in the Helping Our Own shared task. The approach, which uses stand-off annotations for representing edits to text, can be used in a wide variety of text-correction tasks, and easily accommodates different error tagsets."
W11-2806,The Impact of Visual Context on the Content of Referring Expressions,2011,23,7,2,0,44148,henriette viethen,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"Traditional approaches to referring expression generation (REG) have taken as a fundamental requirement the need to distinguish the intended referent from other entities in the context. It seems obvious that this should be a necessary condition for successful reference; but we suggest that a number of recent investigations cast doubt on the significance of this aspect of reference. In the present paper, we look at the role of visual context in determining the content of a referring expression, and come to the conclusion that, at least in the referential scenarios underlying our data, visual context appears not to be a major factor in content determination for reference. We discuss the implications of this surprising finding."
W11-2828,Detecting Interesting Event Sequences for Sports Reporting,2011,15,8,3,0,25263,franccois lareau,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"Hand-crafted approaches to content determination are expensive to port to new domains. Machine-learned approaches, on the other hand, tend to be limited to relatively simple selection of items from data sets. We observe that in time series domains, textual descriptions often aggregate a series of events into a compact description. We present a simple technique for automatically determining sequences of events that are worth reporting, and evaluate its effectiveness."
W11-2838,Helping Our Own: The {HOO} 2011 Pilot Shared Task,2011,2,114,1,1,42377,robert dale,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"The aim of the Helping Our Own (HOO) Shared Task is to promote the development of automated tools and techniques that can assist authors in the writing task, with a specific focus on writing within the natural language processing community. This paper reports on the results of a pilot run of the shared task, in which six teams participated. We describe the nature of the task and the data used, report on the results achieved, and discuss some of the things we learned that will guide future versions of the task."
W11-2702,{GRE}3{D}7: A Corpus of Distinguishing Descriptions for Objects in Visual Scenes,2011,29,29,2,1,40967,jette viethen,Proceedings of the {UCNLG}+{E}val: Language Generation and Evaluation Workshop,0,"Recent years have seen a trend towards empirically motivated and more data-driven approaches in the field of referring expression generation (REG). Much of this work has focussed on initial reference to objects in visual scenes. While this scenario of use is one of the strongest contenders for real-world applications of referring expression generation, existing data sets still only embody very simple stimulus scenes. To move this research forward, we require data sets built around increasingly complex scenes, and we need much larger data sets to accommodate their higher dimensionality. To control the complexity, we also need to adopt a hypothesis-driven approach to scene design. In this paper, we describe GRE3D7, the largest corpus of human-produced distinguishing descriptions available to date, discuss the hypotheses that underlie its design, and offer a number of analyses of the 4480 descriptions it contains."
U11-1013,Collocations in Multilingual Natural Language Generation: Lexical Functions meet {L}exical {F}unctional {G}rammar,2011,27,6,4,0,25263,franccois lareau,Proceedings of the Australasian Language Technology Association Workshop 2011,0,"In a collocation, the choice of one lexical item depends on the choice made for another. This poses a problem for simple approaches to lexicalisation in natural language generation systems. In the Meaning-Text framework, recurrent patterns of collocations have been characterised by lexical functions, which offer an elegant way of describing these relationships. Previous work has shown that using lexical functions in the context of multilingual natural language generation allows for a more efficient development of linguistic resources. We propose a way to encode lexical functions in the Lexical Functional Grammar framework."
D11-1107,Generating Subsequent Reference in Shared Visual Scenes: Computation vs Re-Use,2011,23,5,2,1,40967,jette viethen,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Traditional computational approaches to referring expression generation operate in a deliberate manner, choosing the attributes to be included on the basis of their ability to distinguish the intended referent from its distractors. However, work in psycholinguistics suggests that speakers align their referring expressions with those used previously in the discourse, implying less deliberate choice and more subconscious reuse. This raises the question as to which is a more accurate characterisation of what people do. Using a corpus of dialogues containing 16,358 referring expressions, we explore this question via the generation of subsequent references in shared visual scenes. We use a machine learning approach to referring expression generation and demonstrate that incorporating features that correspond to the computational tradition does not match human referring behaviour as well as using features corresponding to the process of alignment. The results support the view that the traditional model of referring expression generation that is widely assumed in work on natural language generation may not in fact be correct; our analysis may also help explain the oft-observed redundancy found in human-produced referring expressions."
W10-4233,Report on the Second {NLG} Challenge on Generating Instructions in Virtual Environments ({GIVE}-2),2010,15,49,6,0.2326,987,alexander koller,Proceedings of the 6th International Natural Language Generation Conference,0,"We describe the second installment of the Challenge on Generating Instructions in Virtual Environments (GIVE-2), a shared task for the NLG community which took place in 2009--10. We evaluated seven NLG systems by connecting them to 1825 users over the Internet, and report the results of this evaluation in terms of objective and subjective measures."
W10-4236,Helping Our Own: Text Massaging for Computational Linguistics as a New Shared Task,2010,8,39,1,1,42377,robert dale,Proceedings of the 6th International Natural Language Generation Conference,0,"In this paper, we propose a new shared task called HOO: Helping Our Own. The aim is to use tools and techniques developed in computational linguistics to help people writing about computational linguistics. We describe a text-to-text generation scenario that poses challenging research questions, and delivers practical outcomes that are useful in the first case to our own community and potentially much more widely. Two specific factors make us optimistic that this task will generate useful outcomes: one is the availability of the ACL Anthology, a large corpus of the target text type; the other is that CL researchers who are non-native speakers of English will be motivated to use prototype systems, providing informed and precise feedback in large quantity. We lay out our plans in detail and invite comment and critique with the aim of improving the nature of the planned exercise."
U10-1013,Speaker-Dependent Variation in Content Selection for Referring Expression Generation,2010,32,20,2,1,40967,jette viethen,Proceedings of the Australasian Language Technology Association Workshop 2010,0,"In this paper we describe machine learning experiments that aim to characterise the content selection process for distinguishing descriptions. Our experiments are based on two large corpora of humanproduced descriptions of objects in relatively small visual scenes; the referring expressions are annotated with their semantic content. The visual context of reference is widely considered to be a primary determinant of content in referring expression generation, so we explore whether a model can be trained to predict the collection of descriptive attributes that should be used in a given situation. Our experiments demonstrate that speaker-specific preferences play a much more important role than existing approaches to referring expression generation acknowledge."
U10-1015,Repurposing Corpora for Speech Repair Detection: Two Experiments,2010,13,1,3,0,44667,simon zwarts,Proceedings of the Australasian Language Technology Association Workshop 2010,0,"Unrehearsed spoken language often contains many disfluencies. If we want to correctly interpret the content of spoken language, we need to be able to detect these disfluencies and deal with them appropriately. In the work described here, we use a statistical noisy channel model to detect disfluencies in transcripts of spoken language. Like all statistical approaches, this is naturally very data-hungry; however, corpora containing transcripts of unrehearsed spoken language with disfluencies annotated are a scarce resource, which makes training difficult. We address this issue in the following ways: First, since written textual corpora are much more abundant than speech corpora, we see whether using a large text corpus to increase the data available to our language model component delivers an improvement. Second, given that most spoken language corpora are not annotated with disfluencies, we explore the use of Expectation Maximisation to mark the disfluencies in such corpora, so as to increase the data availability for our complete model. In neither case do we see an improvement in our results. We discuss these results and the possible reasons for the negative outcome."
N10-1142,Detecting Emails Containing Requests for Action,2010,27,31,2,1,45825,andrew lampert,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Automatically finding email messages that contain requests for action can provide valuable assistance to users who otherwise struggle to give appropriate attention to the actionable tasks in their inbox. As a speech act classification task, however, automatically recognising requests in free text is particularly challenging. The problem is compounded by the fact that typical emails contain extraneous material that makes it difficult to isolate the content that is directed to the recipient of the email message. In this paper, we report on an email classification system which identifies messages containing requests; we then show how, by segmenting the content of email messages into different functional zones and then considering only content in a small number of message zones when detecting requests, we can improve the accuracy of message-level automated request classification to 83.76%, a relative increase of 15.9%. This represents an error reduction of 41% compared with the same request classifier deployed without email zoning."
viethen-etal-2010-dialogue,Dialogue Reference in a Visual Domain,2010,16,5,3,1,40967,jette viethen,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"A central purpose of referring expressions is to distinguish intended referents from other entities that are in the context; but how is this context determined? This paper draws a distinction between discourse context âother entities that have been mentioned in the dialogueâ and visual context âvisually available objects near the intended referent. It explores how these two different aspects of context have an impact on subsequent reference in a dialogic situation where the speakers share both discourse and visual context. In addition we take into account the impact of the reference history âforms of reference used previously in the discourseâ on forming what have been called conceptual pacts. By comparing the output of different parameter settings in our model to a data set of human-produced referring expressions, we determine that an approach to subsequent reference based on conceptual pacts provides a better explanation of our data than previously proposed algorithmic approaches which compute a new distinguishing description for the intended referent every time it is mentioned."
D10-1089,{W}iki{W}ars: A New Corpus for Research on Temporal Expressions,2010,15,48,2,1,46402,pawel mazur,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"The reliable extraction of knowledge from text requires an appropriate treatment of the time at which reported events take place. Unfortunately, there are very few annotated data sets that support the development of techniques for event time-stamping and tracking the progression of time through a narrative. In this paper, we present a new corpus of temporally-rich documents sourced from English Wikipedia, which we have annotated with TIMEX2 tags. The corpus contains around 120000 tokens, and 2600 TIMEX2 expressions, thus comparing favourably in size to other existing corpora used in these areas. We describe the preparation of the corpus, and compare the profile of the data with other existing temporally annotated corpora. We also report the results obtained when we use DANTE, our temporal expression tagger, to process this corpus, and point to where further work is required. The corpus is publicly available for research purposes."
C10-1154,Detecting Speech Repairs Incrementally Using a Noisy Channel Approach,2010,7,22,3,0,44667,simon zwarts,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Unrehearsed spoken language often contains disfluencies. In order to correctly interpret a spoken utterance, any such disfluencies must be identified and removed or otherwise dealt with. Operating on transcripts of speech which contain disfluencies, our particular focus here is the identification and correction of speech repairs using a noisy channel model. Our aim is to develop a high-accuracy mechanism that can identify speech repairs in an incremental fashion, as the utterance is processed word-by-word.n n We also address the issue of the evaluation of such incremental systems. We propose a novel approach to evaluation, which evaluates performance in detecting and correcting disfluencies incrementally, rather than only assessing performance once the processing of an utterance is complete. This demonstrates some shortcomings in our basic incremental model, and so we then demonstrate a technique that improves performance on the detection of disfluencies as they happen."
W09-3606,Designing a Citation-Sensitive Research Tool: An Initial Study of Browsing-Specific Information Needs,2009,-1,-1,4,1,3122,stephen wan,Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries ({NLPIR}4{DL}),0,None
W09-0609,Referring Expression Generation through Attribute-Based Heuristics,2009,16,41,1,1,42377,robert dale,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"In this paper, we explore a corpus of human-produced referring expressions to see to what extent we can learn the referential behaviour the corpus represents. Despite a wide variation in the way subjects refer across a set of ten stimuli, we demonstrate that component elements of the referring expression generation process appear to generalise across participants to a significant degree. This leads us to propose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic."
W09-0628,Report on the {F}irst {NLG} {C}hallenge on {G}enerating {I}nstructions in {V}irtual {E}nvironments ({GIVE}),2009,7,30,5,0,45130,donna byron,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"We describe the first installment of the Challenge on Generating Instructions in Virtual Environments (GIVE), a new shared task for the NLG community. We motivate the design of the challenge, describe how we carried it out, and discuss the results of the system evaluation."
P09-2076,Validating the web-based evaluation of {NLG} systems,2009,7,12,5,0.2326,987,alexander koller,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"The GIVE Challenge is a recent shared task in which NLG systems are evaluated over the Internet. In this paper, we validate this novel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment. We find that the results delivered by both methods are consistent, but the Internet-based approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carry out."
E09-2009,The Software Architecture for the First Challenge on Generating Instructions in Virtual Environments,2009,40,6,4,0.2326,987,alexander koller,Proceedings of the Demonstrations Session at {EACL} 2009,0,"The GIVE Challenge is a new Internet-based evaluation effort for natural language generation systems. In this paper, we motivate and describe the software infrastructure that we developed to support this challenge."
E09-1097,Improving Grammaticality in Statistical Sentence Generation: Introducing a Dependency Spanning Tree Algorithm with an Argument Satisfaction Model,2009,22,28,3,1,3122,stephen wan,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Abstract-like text summarisation requires a means of producing novel summary sentences. In order to improve the grammaticality of the generated sentence, we model a global (sentence) level syntactic structure. We couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words. We also introduce a new search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree. We treat the allocation of modifiers to heads as a weighted bipartite graph matching (or assignment) problem, a well studied problem in graph theory. Using BLEU to measure performance on a string regeneration task, we found an improvement, illustrating the benefit of the spanning tree approach armed with an argument satisfaction model."
D09-1096,Segmenting Email Message Text into Zones,2009,11,18,2,1,45825,andrew lampert,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In the early days of email, widely-used conventions for indicating quoted reply content and email signatures made it easy to segment email messages into their functional parts. Today, the explosion of different email formats and styles, coupled with the ad hoc ways in which people vary the structure and layout of their messages, means that simple techniques for identifying quoted replies that used to yield 95% accuracy now find less than 10% of such content. In this paper, we describe Zebra, an SVM-based system for segmenting the body text of email messages into nine zone types based on graphic, orthographic and lexical cues. Zebra performs this task with an accuracy of 87.01%; when the number of zones is abstracted to two or three zone classes, this increases to 93.60% and 91.53% respectively."
2009.mtsummit-posters.15,{U}nited {N}ations General Assembly Resolutions: A Six-Language Parallel Corpus,2009,-1,-1,2,0,47471,alexandre rafalovitch,Proceedings of Machine Translation Summit XII: Posters,0,None
W08-1109,The Use of Spatial Relations in Referring Expression Generation,2008,23,80,2,1,40967,jette viethen,Proceedings of the Fifth International Natural Language Generation Conference,0,"There is a prevailing assumption in the literature on referring expression generation that relations are used in descriptions only 'as a last resort', typically on the basis that including the second entity in the relation introduces an additional cognitive load for either speaker or hearer. In this paper, we describe an experiemt that attempts to test this assumption; we determine that, even in simple scenes where the use of relations is not strictly required in order to identify an entity, relations are in fact often used. We draw some conclusions as to what this means for the development of algorithms for the generation of referring expressions."
U08-1009,Requests and Commitments in Email are More Complex Than You Think: Eight Reasons to be Cautious,2008,-1,-1,2,1,45825,andrew lampert,Proceedings of the Australasian Language Technology Association Workshop 2008,0,None
U08-1020,Generating Relational References: What Makes a Difference?,2008,17,1,2,1,40967,jette viethen,Proceedings of the Australasian Language Technology Association Workshop 2008,0,"When we describe an object in order to enable a listener to identify it, we often do so by indicating the location of that object with respect to other objects in a scene. This requires the use of a relational referring expression; while these are very common, they are relatively unexplored in work on referring expression generation. In this paper, we describe an experiment in which we gathered data on how humans use relational referring expressions in simple scenes, with the aim of identifying the factors that make a difference to the ways in which humans construct referring expressions."
bird-etal-2008-acl,The {ACL} {A}nthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics,2008,15,134,2,0,8953,steven bird,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics. Its primary purpose is to serve as a reference repository of research results, but we believe that it can also be an object of study and a platform for research in its own right. We describe an enriched and standardized reference corpus derived from the ACL Anthology that can be used for research in scholarly document processing. This corpus, which we call the ACL Anthology Reference Corpus (ACL ARC), brings together the recent activities of a number of research groups around the world. Our goal is to make the corpus widely available, and to encourage other researchers to use it as a standard testbed for experiments in both bibliographic and bibliometric research."
viethen-etal-2008-controlling,Controlling Redundancy in Referring Expressions,2008,16,16,2,1,40967,jette viethen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Krahmer et al.Âs (2003) graph-based framework provides an elegant and flexible approach to the generation of referring expressions. In this paper, we present the first reported study that systematically investigates how to tune the parameters of the graph-based framework on the basis of a corpus of human-generated descriptions. We focus in particular on replicating the redundant nature of human referring expressions, whereby properties not strictly necessary for identifying a referent are nonetheless included in descriptions. We show how statistics derived from the corpus data can be integrated to boost the frameworkÂs performance over a non-stochastic baseline."
J08-4008,Last Words: What{'}s the Future for \\textit{Computational Linguistics},2008,-1,-1,1,1,42377,robert dale,Computational Linguistics,0,None
D08-1057,Seed and Grow: {A}ugmenting Statistically Generated Summary Sentences using Schematic Word Patterns,2008,21,7,2,1,3122,stephen wan,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We examine the problem of content selection in statistical novel sentence generation. Our approach models the processes performed by professional editors when incorporating material from additional sentences to support some initially chosen key summary sentence, a process we refer to as Sentence Augmentation. We propose and evaluate a method called Seed and Grow for selecting such auxiliary information. Additionally, we argue that this can be performed using schemata, as represented by word-pair co-occurrences, and demonstrate its use in statistical summary sentence generation. Evaluation results are supportive, indicating that a schemata model significantly improves over the baseline."
C08-1070,What{'}s the Date? High Accuracy Interpretation of Weekday Names,2008,13,14,2,1,46402,pawel mazur,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In this paper we present a study on the interpretation of weekday names in texts. Our algorithm for assigning a date to a weekday name achieves 95.91% accuracy on a test data set based on the ACE 2005 Training Corpus, outperforming previously reported techniques run against this same data. We also provide the first detailed comparison of various approaches to the problem using this test data set, employing re-implementations of key techniques from the literature and a range of additional heuristic-based approaches."
W07-2318,Capturing Acceptable Variation in Distinguishing Descriptions,2007,10,1,2,1,40967,jette viethen,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"Almost all existing referring expression generation algorithms aim to find one best referring expression for a given intended referent. However, human-produced data demonstrates that, for any given entity, many perfectly acceptable referring expressions exist. At the same time, it is not the case that all logically possible descriptions are acceptable; so, if we remove the requirement to produce only one best solution, how do we avoid generating undesirable descriptions? Our aim in this paper is to sketch a framework that allows us to capture constraints on referring expression generation, so that the set of logically possible descriptions can be reduced to just those that are acceptable."
U07-1012,Charting Democracy Across Parsers,2007,15,6,2,0,28619,scott nowson,Proceedings of the Australasian Language Technology Workshop 2007,0,"Different parsers trained on the same corpus deliver different results, both in terms of overall performance and in terms of the individual analyses they provide. In particular, for any given sentence, one parser may provide a correct analysis, while another will produce an incorrect analysis; but when faced with a different sentence, the first parser may be in error while the second is correct. In this paper, we leverage this observation by exploring how the results of a number of different parsers may be combined to provide a better performance than any single parser. The method involves constructing a chart that contains edges contributed by a collection of parsers, with a simple voting mechanism to choose the most preferred constituents; this provides a significant improvement in performance over any individual parser. More sophisticated voting mechanisms are also discussed."
P07-1044,{GLEU}: Automatic Evaluation of Sentence-Level Fluency,2007,16,33,4,0,49192,andrew mutton,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"In evaluating the output of language technology applicationsxe2x80x94MT, natural language generation, summarisationxe2x80x94automatic evaluation techniques generally conflate measurement of faithfulness to source content with fluency of the resulting text. In this paper we develop an automatic evaluation metric to estimate fluency alone, by examining the use of parser outputs as metrics, and show that they correlate with human judgements of generated text fluency. We then develop a machine learner based on these, and show that this performs better than the individual parser metrics, approaching a lower bound on human performance. We finally look at different language models for generating sentences, and show that while individual parser metrics can be xe2x80x98fooledxe2x80x99 depending on generation method, the machine learner provides a consistent estimator of fluency."
W06-1410,Algorithms for Generating Referring Expressions: Do They Do What People Do?,2006,23,58,2,1,40967,jette viethen,Proceedings of the Fourth International Natural Language Generation Conference,0,"The natural language generation literature provides many algorithms for the generation of referring expressions. In this paper, we explore the question of whether these algorithms actually produce the kinds of expressions that people produce. We compare the output of three existing algorithms against a data set consisting of human-generated referring expressions, and identify a number of significant differences between what people do and what these algorithms do. On the basis of these observations, we suggest some ways forward that attempt to address these differences."
W06-1418,Introduction to the {INLG}{'}06 Special Session on Sharing Data and Comparative Evaluation,2006,0,0,2,0,26421,anja belz,Proceedings of the Fourth International Natural Language Generation Conference,0,"The idea for this special session had its origins in discussions with many different members of the NLG community at the 2005 Workshop on Using Corpora for Natural Language Generation (UCNLG'05, held in conjunction with the Corpus Linguistics 2005 conference at the University of Birmingham in July 2005), and subsequently at the 10th European Natural Language Generation Workshop (ENLG'05, held at the University of Aberdeen in August 2005). At the latter event, the excitement about introducing shared tasks was infectious: the topic hijacked several of the organised discussion groups, it was the focus of conversation at many tables during lunch-breaks, and even the end of the conference didn't put a stop to it, with discussions carrying right on until the taxis to the airport arrived."
W06-0902,Local Semantics in the Interpretation of Temporal Expressions,2006,7,10,1,1,42377,robert dale,Proceedings of the Workshop on Annotating and Reasoning about Time and Events,0,"Work on the interpretation of temporal expressions in text has generally been pursued in one of two paradigms: the formal semantics approach, where an attempt is made to provide a well-grounded theoretical basis for the interpretation of these expressions, and the more pragmatically-focused approach represented by the development of the TIMEX2 standard, with its origins in work in information extraction. The former emphasises formal elegance and consistency; the latter emphasises broad coverage for practical applications. In this paper, we report on the development of a framework that attempts to integrate insights from both perspectives, with the aim of achieving broad coverage of the domain in a well-grounded manner from a formal perspective. We focus in particular on the development of a compact notation for representing the semantics of underspecified temporal expressions that can be used to provide component-level evaluation of systems that interpret such expressions."
U06-1007,Classifying Speech Acts using Verbal Response Modes,2006,24,21,2,1,45825,andrew lampert,Proceedings of the Australasian Language Technology Workshop 2006,0,"The driving vision for our work is to provide intelligent, automated assistance to users in understanding the status of their email conversations. Our approach is to create tools that enable the detection and connection of speech acts across email messages. We thus require a mechanism for tagging email utterances with some indication of their dialogic function. However, existing dialog act taxonomies as used in computational linguistics tend to be too taskor application-specific for the wide range of acts we find represented in email conversation. The Verbal Response Modes (VRM) taxonomy of speech acts, widely applied for discourse analysis in linguistics and psychology, is distinguished from other speech act taxonomies by its construction from crosscutting principles of classification, which ensure universal applicability across any domain of discourse. The taxonomy categorises on two dimensions, characterised as literal meaning and pragmatic meaning. In this paper, we describe a statistical classifier that automatically identifies the literal meaning category of utterances using the VRM classification. We achieve an accuracy of 60.8% using linguistic features derived from VRMxe2x80x99s human annotation guidelines. Accuracy is improved to 79.8% using additional features."
U06-1017,Towards the Evaluation of Referring Expression Generation,2006,-1,-1,2,1,40967,jette viethen,Proceedings of the Australasian Language Technology Workshop 2006,0,None
U06-1019,Using Dependency-Based Features to Take the {'}Para-farce{'} out of Paraphrase,2006,19,103,3,1,3122,stephen wan,Proceedings of the Australasian Language Technology Workshop 2006,0,"As research in text-to-text paraphrase generation progresses, it has the potential to improve the quality of generated text. However, the use of paraphrase generation methods creates a secondary problem. We must ensure that generated novel sentences are not inconsistent with the text from which it was generated. We propose a machine learning approach be used to filter out inconsistent novel sentences, or False Paraphrases. To train such a filter, we use the Microsoft Research Paraphrase corpus and investigate whether features based on syntactic dependencies can aid us in this task. Like Finch et al. (2005), we obtain a classification accuracy of 75.6%, the best known performance for this corpus. We also examine the strengths and weaknesses of dependency based features and conclude that they may be useful in more accurately classifying cases of False Paraphrase."
P06-4009,An Intermediate Representation for the Interpretation of Temporal Expressions,2006,5,5,2,1,46402,pawel mazur,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"The interpretation of temporal expressions in text is an important constituent task for many practical natural language processing tasks, including question-answering, information extraction and text summarisation. Although temporal expressions have long been studied in the research literature, it is only more recently, with the impetus provided by exercises like the ACE Program, that attention has been directed to broad-coverage, implemented systems. In this paper, we describe our approach to intermediate semantic representations in the interpretation of temporal expressions."
mazur-dale-2006-named,Named Entity Extraction with Conjunction Disambiguation,2006,9,2,2,1,46402,pawel mazur,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The recognition of named entities is now a well-developed area, with a range of symbolic and machine learning techniques that deliver high accuracy extraction and categorisation of a variety of entity types. However, there are still some named entity phenomena that present problems for existing techniques; in particular, relatively little work has explored the disambiguation of conjunctions appearing in candidate named entity strings. We demonstrate that there are in fact four distinct uses of conjunctions in the context of named entities; we present some experiments using machine-learned classifiers to disambiguate the different uses of the conjunction, with 85{\%} of test examples being correctly classified."
W05-1628,Searching for Grammaticality: Propagating Dependencies in the {V}iterbi Algorithm,2005,12,11,2,1,3122,stephen wan,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,None
U05-1004,Disambiguating Conjunctions in Named Entities,2005,5,0,2,1,46402,pawel mazur,Proceedings of the Australasian Language Technology Workshop 2005,0,"The recognition of named entities is now a welldeveloped area, with a range of symbolic and machine learning techniques that deliver high accuracy identification and categorisation of a variety of entity types. However, there are still some named entity phenomena that present problems for existing techniques; in particular, relatively little work has explored the disambiguation of conjunctions appearing in candidate named entity strings. We demonstrate that there are in fact four distinct uses of conjunctions in the context of named entities; we present the results of some experiments using machine-learned classifiers to disambiguate the dierent uses of the conjunction, with 81.73% of test examples being correctly classified. We provide some discussion and analysis of the problem of conjunction in named entities, and we show that there are some cases which are ambiguous even for humans."
I05-5012,Towards Statistical Paraphrase Generation: Preliminary Evaluations of Grammaticality,2005,11,7,3,1,3122,stephen wan,Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005),0,"Summary sentences are often paraphrases of existing sentences. They may be made up of recycled fragments of text taken from important sentences in an input document. We investigate the use of a statistical sentence generation technique that recombines words probabilistically in order to create new sentences. Given a set of event-related sentences, we use an extended version of the Viterbi algorithm which employs dependency relation and bigram probabilities to find the most probable summary sentence. Using precision and recall metrics for verb arguments as a measure of grammaticality, we find that our system performs better than a bigram baseline, producing fewer spurious verb arguments."
U04-1010,Referring Expression Generation as a Search Problem,2004,15,1,2,0,16528,bernd bohnet,Proceedings of the Australasian Language Technology Workshop 2004,0,"One of the most widely explored issues in natural language generation is the generation of referring expressions (gre): given an entity we want to refer to, how do we work out the content of a referring expression that uniquely identifies the intended referent? Over the last 15 years, a number of authors have proposed a wide range of algorithms for addressing different aspects of this problem, but the different approaches taken have made it very difficult to compare and contrast the algorithms provided in any meaningful way. In this paper, we propose a characterisation of the problem of referring expression generation as a search problem; this allows us to recast existing algorithms in a way that makes their similarities and differences clear."
W03-1202,Using Thematic Information in Statistical Headline Generation,2003,21,17,4,1,3122,stephen wan,Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering,0,"We explore the problem of single sentence summarisation. In the news domain, such a summary might resemble a headline. The headline generation system we present uses Singular Value Decomposition (SVD) to guide the generation of a headline towards the theme that best represents the document to be summarised. In doing so, the intuition is that the generated summary will more accurately reflect the content of the source document. This paper presents SVD as an alternative method to determine if a word is a suitable candidate for inclusion in the headline. The results of a recall based evaluation comparing three different strategies to word selection, indicate that thematic information does help improve recall."
U03-1002,{`}One{'}-anaphora and the case for discourse-driven referring expression generation,2003,12,4,1,1,42377,robert dale,Proceedings of the Australasian Language Technology Workshop 2003,0,"Conventional approaches to the generation of referring expressions place the task within a pipelined architecture, typically somewhere between text planning and linguistic realisation. In this paper, we look at the issues that arise in generating one-anaphoric referring expressions; examination of this task causes us to reflect on the current predominant architectural models for natural language generation, and leads us to suggest an alternative architecture where decisions that influence forms of reference happen much earlier in the process of natural language generation."
U03-1012,Straight to the point: Discovering themes for summary generation,2003,19,3,4,1,3122,stephen wan,Proceedings of the Australasian Language Technology Workshop 2003,0,"This paper presents our approach to the problem of single sentence summarisation. We investigate the use of Singular Value Decomposition (SVD) to guide the generation of a summary towards the theme that is the focus of the document to be summarised. In doing so, the intuition is that the generated summary will more accurately reflect the content of the source document. Currently, we operate in the news domain and at present, our summaries are modelled on headlines. This paper presents SVD as an alternative method to determine if a word is a suitable candidate for inclusion in the headline. The results of a recall based evaluation comparing three different strategies to word selection, indicate that thematic information does help improve recall."
W02-1112,Using the {W}ord{N}et Hierarchy for Associative Anaphora Resolution,2002,10,7,2,0,53207,josef meyer,{COLING}-02: {SEMANET}: Building and Using Semantic Networks,0,"In this paper, we explore how the taxonomic inheritance hierarchy in a semantic net can contribute to the resolution of associative anaphoric expressions. We present the results of some preliminary experiments and discuss both their implications and the scope for improvements to the technique."
W02-0104,Evangelising Language Technology: A Practically-Focussed Undergraduate Program,2002,6,1,1,1,42377,robert dale,Proceedings of the {ACL}-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,0,"This paper describes an undergraduate program in Language Technology that we have developed at Macquarie University. We question the industrial relevance of much that is taught in NLP courses, and emphasize the need for a practical orientation as a means to growing the size of the field. We argue that a more evangelical approach, both with regard to students and industry, is required. The paper provides an overview of the material we cover, and makes some observations for the future on the basis of our experiences so far."
C00-1003,Selectional Restrictions in {HPSG},2000,10,10,2,0,66,ion androutsopoulos,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Selectional restrictions are semantic sortal constraints imposed on the participants of linguistic constructions to capture contextually-dependent constraints on interpretation. Despite their limitations, selectional restrictions have proven very useful in natural language applications, where they have been used frequently in word sense disambiguation, syntactic disambiguation, and anaphora resolution. Given their practical value, we explore two methods to incorporate selectional restrictions in the HPSG theory, assuming that the reader is familiar with HPSG. The first method employs HPSG's BACKGROUND feature and a constraint-satisfaction component pipe-lined after the parser. The second method uses subsorts of referential indices, and blocks readings that violate selectional restrictions during parsing. While theoretically less satisfactory, we have found the second method particularly useful in the development of practical systems."
J98-3001,Introduction to the Special Issue on Natural Language Generation,1998,31,19,1,1,42377,robert dale,Computational Linguistics,0,"There are two sides to natural language processing. On the one hand, work in natural language understanding is concerned with the mapping from some surface representation of linguistic material expressed as speech or text--to an underlying representation of the meaning carried by that surface representation. But there is also the question of how one maps from some underlying representation of meaning into text or speech: this is the domain of natural language generation. Whether our end-goal is the construction of artifacts that use natural languages intelligently, the formal characterization of phenomena in human languages, or the computational modeling of the human language processing mechanism, we cannot ignore the fact that language is both spoken (or written) and heard (or read). Both are equally large and important problems, but the literature contains much less work on natural language generation (NLG) than it does on natural language understanding (NLU). There are many reasons why this might be so, although clearly an important one is that researchers in natural language understanding in some sense start out with a more well-defined task: the input is known, and there is a lot of it around. This is not the case in natural language generation: there, it is the desired output that is known, but the input is an unknown; and while the world is awash with text waiting to be processed, there are fewer instances of what we might consider appropriate inputs for the process of natural language generation. For researchers in the field, this highlights the fundamental question that always has to be asked: What do we generate from? Despite this problem, the natural language generation community is a thriving one, with a research base that has been developing steadily--although perhaps at a slower pace because of the smaller size of the community--for just as long as work in natural language understanding. It should not be forgotten that much of NLP has its origins in the early work on machine translation in the 1950s; and that to carry out machine translation, one has to not only analyze existing texts but also to generate new ones. The early machine translation experiments, however, did not recognize the problems that give modern work in NLG its particular character. The first significant pieces of work in the field appeared during the 1970s; in particular, Goldman's work on the problem of lexicalizing underlying conceptual material (Goldman 1974) and"
W96-0417,Strategies for Comparison in Encyclopedia Descriptions,1996,13,13,2,0,45549,maria milosavljevic,Eighth International Natural Language Generation Workshop,0,"Comparisons are typically employed to distinguish similar entities, or to illustrate a property of an entity by referring to another comxe2x80xa2 monly known entity which shares that property. Based on an analysis of a corpus of encyclopaedia texts, we define three types of comparisons and outline some strategies for applying these in the generation of entity descriptions. We describe how these comparison strategies are used within the PEBA-II hypertext generation system to generate descriptions of animals. 1 I n t r o d u c t i o n a n d A i m s In this paper, we outline some strategies for comparison which we use in PEBA-II, a hypertext generation system which produces encyclopaedia descriptions of entities as World Wide Web ( w w w ) documents, based on an underlying taxonomic knowledge base. PEBA-II is part of a larger research programme built around the idea of an intelligent on-line encyclopaedia, where the descriptions produced by the system vary for different users and at different times. Our work is grounded in the domain of animal descriptions, although similar issues arise in many other domains. Comparisons are widespread within existing encyclopaedia descriptions. In particular, when describing a new concept to a user, a comparison may be made with reference to other known concepts or ideas, enabling the hearer to more easily process and understand the new material (see [Milosavljevic 1996]). So, for example, in the context of animal descriptions, if the user knows about the porcupine and requests a description of the echidna, then we might describe the echidna by highlighting both its similarities to and differences from the porcupine. Clearly this requires us to make use of some notion of a user model, and in a way that is distinct from previous work in user-modelling in text generation. Our aim is to produce texts which introduce new concepts by reference to existing knowledge the user is assumed to have, thus employing the user model to greater advantage; past research (see, in particular, [Paris 1987]) has concentrated on avoiding the production of texts which repeat what the user already knows. By making use of a discourse model, we can also generate comparisons that take account of entities that have been mentioned in the previous discourse. This is particularly important in the context of the dynamic construction of hypertext documents from an underlying representation: by employing text generation techniques, we can produce context-dependent descriptions which vary depending on the information which has already been presented to the user, thus overcoming some of the limitations of hypertext documents which have been constructed simply by breaking an existing linear text into"
W93-0202,Rhetoric and Intentions in Discourse,1993,-1,-1,1,1,42377,robert dale,Intentionality and Structure in Discourse Relations,0,None
C92-2072,Towards Robust {PATR},1992,11,29,2,0,52884,shona douglas,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We report on the initial stages of development of a robust parsing system, to be used as part of The Editor's Assistant, a program that detects and corrects textual errors and infelicities in the area of syntax and style. Our mechanism extends the standard PATR-II formalism by indexing the constraints on rules and abstracting away control of the application of these constraints. This allows independent specification of grouping and ordering of the constraints, which can improve the efficiency of processing, and in conjunction with information specifying whether constraints are necessary or optional, allows detection of syntactic errors."
C92-1038,A Fast Algorithm for the Generation of Referring Expressions,1992,22,91,2,0,5931,ehud reiter,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We simplify previous work in the development of algorithms for the generation of referring expressions while at the same time taking account of psycholinguistic findings and transcript data. The result is a straightforward algorithm that is computationally tractable, sensitive to the preferences of human users, and reasonably domain-independent. We provide a specification of the resources a host system must provide in order to make use of the algorithm, and describe an implementation used in the IDAS system."
J91-4005,Book Reviews: Natural Language Generation in Artificial Intelligence and Computational Linguistics,1991,0,3,1,1,42377,robert dale,Computational Linguistics,0,"This book is a selection of papers presented at the Fourth International Workshop on Natural Language Generation, held in Catalina, California in July of 1988. If there is a central theme to this compilation of articles, it is that natural language generators are no longer relatively straightforward, mechanistic translators which produce text from an internal representation language. Rather, state-of-the-art theories of generation look very much like theories of planning. Many analogies can be made between the generation process and hierarchical planning. The first level of planning in most systems corresponds to the selection of an overall discourse structure. As planning continues, lexical selection is analogous to the task of operator selection. Each lexical choice imposes constraints on the rest of the plan which restrict further choices."
E91-1028,Generating Referring Expressions Involving Relations,1991,12,77,1,1,42377,robert dale,Fifth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper, we review Dale's [1989] algorithm for determining the content of a referring expression. The algorithm, which only permits the use of one-place predicates, it revised and extended to deal with n-ary predicates. We investigate the problem of blocking recursion in complex noun phrases and propose a solution in the context of our algorithm."
P89-1009,Cooking Up Referring Expressions,1989,15,173,1,1,42377,robert dale,27th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes the referring expression generation mechanisms used in EPICURE, a computer program which produces natural language descriptions of cookery recipes. Major features of the system include: an underlying ontology which permits the representation of non-singular entities; a notion of discriminatory power, to determine what properties should be used in a description; and a PATR-like unification grammar to produce surface linguistic strings."
