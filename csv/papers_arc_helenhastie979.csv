2021.splurobonlp-1.2,Learning to Read Maps: Understanding Natural Language Instructions from Unseen Maps,2021,-1,-1,4,0,1046,miltiadis katsakioris,Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics,0,"Robust situated dialog requires the ability to process instructions based on spatial information, which may or may not be available. We propose a model, based on LXMERT, that can extract spatial information from text instructions and attend to landmarks on OpenStreetMap (OSM) referred to in a natural language instruction. Whilst, OSM is a valuable resource, as with any open-sourced data, there is noise and variation in the names referred to on the map, as well as, variation in natural language instructions, hence the need for data-driven methods over rule-based systems. This paper demonstrates that the gold GPS location can be accurately predicted from the natural language instruction and metadata with 72{\%} accuracy for previously seen maps and 64{\%} for unseen maps."
2021.eacl-main.202,A Study of Automatic Metrics for the Evaluation of Natural Language Explanations,2021,-1,-1,3,1,6001,mirunaadriana clinciu,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"As transparency becomes key for robotics and AI, it will be necessary to evaluate the methods through which transparency is provided, including automatically generated natural language (NL) explanations. Here, we explore parallels between the generation of such explanations and the much-studied field of evaluation of Natural Language Generation (NLG). Specifically, we investigate which of the NLG evaluation measures map well to explanations. We present the ExBAN corpus: a crowd-sourced corpus of NL explanations for Bayesian Networks. We run correlations comparing human subjective ratings with NLG automatic measures. We find that embedding-based automatic NLG evaluation methods, such as BERTScore and BLEURT, have a higher correlation with human ratings, compared to word-overlap metrics, such as BLEU and ROUGE. This work has implications for Explainable AI and transparent robotic and autonomous systems."
2020.signlang-1.24,Towards Large-Scale Data Mining for Data-Driven Analysis of Sign Languages,2020,-1,-1,3,0,14845,boris mocialov,"Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives",0,"Access to sign language data is far from adequate. We show that it is possible to collect the data from social networking services such as TikTok, Instagram, and YouTube by applying data filtering to enforce quality standards and by discovering patterns in the filtered data, making it easier to analyse and model. Using our data collection pipeline, we collect and examine the interpretation of songs in both the American Sign Language (ASL) and the Brazilian Sign Language (Libras). We explore their differences and similarities by looking at the co-dependence of the orientation and location phonological parameters."
2020.lrec-1.36,{CRWIZ}: A Framework for Crowdsourcing Real-Time {W}izard-of-{O}z Dialogues,2020,24,0,4,1,4817,francisco garcia,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Large corpora of task-based and open-domain conversational dialogues are hugely valuable in the field of data-driven dialogue systems. Crowdsourcing platforms, such as Amazon Mechanical Turk, have been an effective method for collecting such large amounts of data. However, difficulties arise when task-based dialogues require expert domain knowledge or rapid access to domain-relevant information, such as databases for tourism. This will become even more prevalent as dialogue systems become increasingly ambitious, expanding into tasks with high levels of complexity that require collaboration and forward planning, such as in our domain of emergency response. In this paper, we propose CRWIZ: a framework for collecting real-time Wizard of Oz dialogues through crowdsourcing for collaborative, complex tasks. This framework uses semi-guided dialogue to avoid interactions that breach procedures and processes only known to experts, while enabling the capture of a wide variety of interactions."
W19-8403,A Survey of Explainable {AI} Terminology,2019,-1,-1,2,1,6001,mirunaadriana clinciu,Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019),0,None
W19-1601,Corpus of Multimodal Interaction for Collaborative Planning,2019,0,1,2,0,1046,miltiadis katsakioris,Proceedings of the Combined Workshop on Spatial Language Understanding ({S}p{LU}) and Grounded Communication for Robotics ({R}obo{NLP}),0,"As autonomous systems become more commonplace, we need a way to easily and naturally communicate to them our goals and collaboratively come up with a plan on how to achieve these goals. To this end, we conducted a Wizard of Oz study to gather data and investigate the way operators would collaboratively make plans via a conversational {`}planning assistant{'} for remote autonomous systems. We present here a corpus of 22 dialogs from expert operators, which can be used to train such a system. Data analysis shows that multimodality is key to successful interaction, measured both quantitatively and qualitatively via user feedback."
W18-6511,Explainable Autonomy: A Study of Explanation Styles for Building Clear Mental Models,2018,0,8,6,1,4817,francisco garcia,Proceedings of the 11th International Conference on Natural Language Generation,0,"As unmanned vehicles become more autonomous, it is important to maintain a high level of transparency regarding their behaviour and how they operate. This is particularly important in remote locations where they cannot be directly observed. Here, we describe a method for generating explanations in natural language of autonomous system behaviour and reasoning. Our method involves deriving an interpretable model of autonomy through having an expert {`}speak aloud{'} and providing various levels of detail based on this model. Through an online evaluation study with operators, we show it is best to generate explanations with multiple possible reasons but tersely worded. This work has implications for designing interfaces for autonomy as well as for explainable AI and operator training."
W18-3911,Transfer Learning for {B}ritish {S}ign {L}anguage Modelling,2018,0,0,2,0,14845,boris mocialov,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"Automatic speech recognition and spoken dialogue systems have made great advances through the use of deep machine learning methods. This is partly due to greater computing power but also through the large amount of data available in common languages, such as English. Conversely, research in minority languages, including sign languages, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a model developed for one language is reused as the starting point for a model on a second language, which is less resourced. In this paper, we examine two transfer learning techniques of fine-tuning and layer substitution for language modelling of British Sign Language. Our results show improvement in perplexity when using transfer learning with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus."
W14-4422,Multi-adaptive Natural Language Generation using Principal Component Regression,2014,22,4,2,1,5916,dimitra gkatzia,Proceedings of the 8th International Natural Language Generation Conference ({INLG}),0,"We present FeedbackGen, a system that uses a multi-adaptive approach to Natural Language Generation. With the term xe2x80x98multi-adaptivexe2x80x99, we refer to a system that is able to adapt its content to different user groups simultaneously, in our case adapting to both lecturers and students. We present a novel approach to student feedback generation, which simultaneously takes into account the preferences of lecturers and students when determining the content to be conveyed in a feedback summary. In this framework, we utilise knowledge derived from ratings on feedback summaries by extracting the most relevant features using Principal Component Regression (PCR) analysis. We then model a reward function that is used for training a Reinforcement Learning agent. Our results with students suggest that, from the studentsxe2x80x99 perspective, such an approach can generate more preferable summaries than a purely lecturer-adapted approach."
W14-4336,The {PARLANCE} mobile application for interactive search in {E}nglish and {M}andarin,2014,3,2,1,1,1049,helen hastie,Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL}),0,We demonstrate a mobile application in English and Mandarin to test and evaluate components of the Parlance dialogue system for interactive search under real-world conditions.
P14-1116,Comparing Multi-label Classification with Reinforcement Learning for Summarisation of Time-series Data,2014,35,10,2,1,5916,dimitra gkatzia,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present a novel approach for automatic report generation from time-series data, in the context of student feedback generation. Our proposed methodology treats content selection as a multi-label (ML) classification problem, which takes as input time-series data and outputs a set of templates, while capturing the dependencies between selected templates. We show that this method generates output closer to the feedback that lecturers actually generated, achieving 3.5% higher accuracy and 15% higher F-score than multiple simple classifiers that keep a history of selected templates. Furthermore, we compare a ML classifier with a Reinforcement Learning (RL) approach in simulation and using ratings from real student users. We show that the different methods have different benefits, with ML being more accurate for predicting what was seen in the training data, whereas RL is more exploratory and slightly preferred by the students."
hastie-belz-2014-comparative,A Comparative Evaluation Methodology for {NLG} in Interactive Systems,2014,40,4,1,1,1049,helen hastie,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Interactive systems have become an increasingly important type of application for deployment of NLG technology over recent years. At present, we do not yet have commonly agreed terminology or methodology for evaluating NLG within interactive systems. In this paper, we take steps towards addressing this gap by presenting a set of principles for designing new evaluations in our comparative evaluation methodology. We start with presenting a categorisation framework, giving an overview of different categories of evaluation measures, in order to provide standard terminology for categorising existing and new evaluation techniques. Background on existing evaluation methodologies for NLG and interactive systems is presented. The comparative evaluation methodology is presented. Finally, a methodology for comparative evaluation of NLG components embedded within interactive systems is presented in terms of the comparative evaluation methodology, using a specific task for illustrative purposes."
E14-4041,Finding middle ground? Multi-objective Natural Language Generation from time-series data,2014,15,6,2,1,5916,dimitra gkatzia,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"A Natural Language Generation (NLG) system is able to generate text from nonlinguistic data, ideally personalising the content to a userxe2x80x99s specific needs. In some cases, however, there are multiple stakeholders with their own individual goals, needs and preferences. In this paper, we explore the feasibility of combining the preferences of two different user groups, lecturers and students, when generating summaries in the context of student feedback generation. The preferences of each user group are modelled as a multivariate optimisation function, therefore the task of generation is seen as a multi-objective (MO) optimisation task, where the two functions are combined into one. This initial study shows that treating the preferences of each user group equally smooths the weights of the MO function, in a way that preferred content of the user groups is not presented in the generated summary."
E14-1074,Cluster-based Prediction of User Ratings for Stylistic Surface Realisation,2014,32,15,3,1,26443,nina dethlefs,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Surface realisations typically depend on their target style and audience. A challenge in estimating a stylistic realiser from data is that humans vary significantly in their subjective perceptions of linguistic forms and styles, leading to almost no correlation between ratings of the same utterance. We address this problem in two steps. First, we estimate a mapping function between the linguistic features of a corpus of utterances and their human style ratings. Users are partitioned into clusters based on the similarity of their ratings, so that ratings for new utterances can be estimated, even for new, unknown users. In a second step, the estimated model is used to re-rank the outputs of a number of surface realisers to produce stylistically adaptive output. Results confirm that the generated styles are recognisable to human judges and that predictive models based on clusters of users lead to better rating predictions than models based on an average population of users."
W13-4026,"Demonstration of the {PARLANCE} system: a data-driven incremental, spoken dialogue system for interactive search",2013,6,21,1,1,1049,helen hastie,Proceedings of the {SIGDIAL} 2013 Conference,0,"The Parlance system for interactive search processes dialogue at a microturn level, displaying dialogue phenomena that play a vital role in human spoken conversation. These dialogue phenomena include more natural turn-taking through rapid system responses, generation of backchannels, and user barge-ins. The Parlance demonstration system dierentiates from other incremental systems in that it is data-driven with an infrastructure that scales well."
W13-4047,Impact of {ASR} N-Best Information on {B}ayesian Dialogue Act Recognition,2013,20,5,3,1,30450,heriberto cuayahuitl,Proceedings of the {SIGDIAL} 2013 Conference,0,"A challenge in dialogue act recognition is the mapping from noisy user inputs to dialogue acts. In this paper we describe an approach for re-ranking dialogue act hypotheses based on Bayesian classifiers that incorporate dialogue history and Automatic Speech Recognition (ASR) N-best information. We report results based on the Letxe2x80x99s Go dialogue corpora that show (1) that including ASR N-best information results in improved dialogue act recognition performance (7% accuracy), and (2) that competitive results can be obtained from as early as the first system dialogue act, reducing the need to wait for subsequent system dialogue acts."
W13-4058,Demonstration of the {E}mote{W}izard of {O}z Interface for Empathic Robotic Tutors,2013,5,3,3,0,40763,shweta bhargava,Proceedings of the {SIGDIAL} 2013 Conference,0,We present a Wizard of Oz (WoZ) environment that was designed to build an artificial embodied intelligent tutoring system (ITS) that is capable of empathic conversations with school pupils aged between 10-13. We describe the components and the data that we plan to collect using the environment.
W13-2115,Generating Student Feedback from Time-Series Data Using Reinforcement Learning,2013,24,12,2,1,5916,dimitra gkatzia,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"We describe a statistical Natural Language Generation (NLG) method for summarisation of time-series data in the context of feedback generation for students. In this paper, we initially present a method for collecting time-series data from students (e.g. marks, lectures attended) and use example feedback from lecturers in a datadriven approach to content selection. We show a novel way of constructing a reward function for our Reinforcement Learning agent that is informed by the lecturersxe2x80x99 method of providing feedback. We evaluate our system with undergraduate students by comparing it to three baseline systems: a rule-based system, lecturerconstructed summaries and a Brute Force system. Our evaluation shows that the feedback generated by our learning agent is viewed by students to be as good as the feedback from the lecturers. Our findings suggest that the learning agent needs to take into account both the student and lecturersxe2x80x99 preferences."
P13-1123,Conditional Random Fields for Responsive Surface Realisation using Global Features,2013,25,21,2,1,26443,nina dethlefs,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher (up to 14%) than those for taking only local context into account."
W12-1808,Incremental Spoken Dialogue Systems: Tools and Data,2012,8,4,1,1,1049,helen hastie,{NAACL}-{HLT} Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data ({SDCTD} 2012),0,"Strict-turn taking models of dialogue do not accurately model human incremental processing, where users can process partial input and plan partial utterances in parallel. We discuss the current state of the art in incremental systems and propose tools and data required for further advances in the field of Incremental Spoken Dialogue Systems."
W12-1509,Optimising Incremental Generation for Spoken Dialogue Systems: Reducing the Need for Fillers,2012,23,16,2,1,26443,nina dethlefs,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"Recent studies have shown that incremental systems are perceived as more reactive, natural, and easier to use than non-incremental systems. However, previous work on incremental NLG has not employed recent advances in statistical optimisation using machine learning. This paper combines the two approaches, showing how the update, revoke and purge operations typically used in incremental approaches can be implemented as state transitions in a Markov Decision Process. We design a model of incremental NLG that generates output based on micro-turn interpretations of the user's utterances and is able to optimise its decisions using statistical machine learning. We present a proof-of-concept study in the domain of Information Presentation (IP), where a learning agent faces the trade-off of whether to present information as soon as it is available (for high reactiveness) or else to wait until input ASR hypotheses are more reliable. Results show that the agent learns to avoid long waiting times, fillers and self-corrections, by re-ordering content based on its confidence."
D12-1008,Optimising Incremental Dialogue Decisions Using Information Density for Interactive Systems,2012,37,30,2,1,26443,nina dethlefs,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Incremental processing allows system designers to address several discourse phenomena that have previously been somewhat neglected in interactive systems, such as backchannels or barge-ins, but that can enhance the responsiveness and naturalness of systems. Unfortunately, prior work has focused largely on deterministic incremental decision making, rendering system behaviour less flexible and adaptive than is desirable. We present a novel approach to incremental decision making that is based on Hierarchical Reinforcement Learning to achieve an interactive optimisation of Information Presentation (IP) strategies, allowing the system to generate and comprehend backchannels and barge-ins, by employing the recent psycholinguistic hypothesis of information density (ID) (Jaeger, 2010). Results in terms of average rewards and a human rating study show that our learnt strategy outperforms several baselines that are not sensitive to ID by more than 23%."
W11-2002,Spoken Dialog Challenge 2010: Comparison of Live and Control Test Results,2011,8,49,4,0,4130,alan black,Proceedings of the {SIGDIAL} 2011 Conference,0,"The Spoken Dialog Challenge 2010 was an exercise to investigate how different spoken dialog systems perform on the same task. The existing Let's Go Pittsburgh Bus Information System was used as a task and four teams provided systems that were first tested in controlled conditions with speech researchers as users. The three most stable systems were then deployed to real callers. This paper presents the results of the live tests, and compares them with the control test results. Results show considerable variation both between systems and between the control and live tests. Interestingly, relatively high task completion for controlled tests did not always predict relatively high task completion for live tests. Moreover, even though the systems were quite different in their designs, we saw very similar correlations between word error rate and task completion for all the systems. The dialog data collected is available to the research community."
W11-2017,{``}The day after the day after tomorrow?{''} A machine learning approach to adaptive temporal expression generation: training and evaluation with real users,2011,15,18,2,0.428571,38858,srinivasan janarthanam,Proceedings of the {SIGDIAL} 2011 Conference,0,"Generating Temporal Expressions (TE) that are easy to understand, unambiguous, and reasonably short is a challenge for humans and Spoken Dialogue Systems. Rather than developing hand-written decision rules, we adopt a data-driven approach by collecting user feedback on a variety of possible TEs in terms of task success, ambiguity, and user preference. The data collected in this work is freely available to the research community. These data were then used to train a simulated user and a reinforcement learning policy that learns an adaptive Temporal Expression generation strategy for a variety of contexts. We evaluate our learned policy both in simulation and with real users and show that this data-driven adaptive policy is a significant improvement over a rule-based adaptive policy, leading to a 24% increase in perceived task completion, while showing a small increase in actual task completion, and a 16% decrease in call duration. This means that dialogues are more efficient and that users are also more confident about the appointment that they have agreed with the system."
W09-3922,"Automatic Generation of Information State Update Dialogue Systems that Dynamically Create Voice {XML}, as Demonstrated on the i{P}hone",2009,5,4,1,1,1049,helen hastie,Proceedings of the {SIGDIAL} 2009 Conference,0,"We demonstrate DUDE (Dialogue and Understanding Development Environment), a prototype development environment that automatically generates dialogue systems from business-user resources and databases. These generated spoken dialogue systems (SDS) are then deployed on an industry standard Voice XML platform. Specifically, the deployed system works by dynamically generating context-sensitive Voice XML pages. The dialogue move of each page is determined in real time by the dialogue manager, which is an Information State Update engine. Firstly, we will demonstrate the development environment which includes automatic generation of speech recognition grammars for robust interpretation of spontaneous speech, and uses the application database to generate lexical entries and grammar rules. A simple graphical interface allows users (i.e. developers) to easily and quickly create and the modify SDS without the need for expensive service providers. Secondly, we will demonstrate the deployed system which enables participants to call up and speak to the SDS recently created. We will also show a pre-built application running on the iPhone and Google Android phone for searching for places such as restaurants, hotels and museums."
C08-3005,{``}Build Your Own{''} Spoken Dialogue Systems: Automatically Generating {ISU} Dialogue Systems from Business User Resources,2008,7,2,3,0,2551,oliver lemon,Coling 2008: Companion volume: Demonstrations,0,"Building effective spoken dialogue systems (SDS) is currently a complex task requiring expert knowledge. Our tools give control of SDS application development to non-experts, who need only use a Graphical User Interface or GUI to develop state-of-the-art Information State Update (ISU) dialogue systems. Behind the GUI is a set of Advanced Dialogue Tools (ADT) that generate complete SDS based on Business User Resources. These resources include a database and a Process Model that captures the structure of an application, for example, banking or restaurant information. Also generated are speech recognition Language Models and grammars for robust interpretation of spontaneous speech. We will demonstrate how our simple GUI allows developers to easily and quickly create and modify SDS without the need for expensive speech application service providers. This demonstration shows the interface, the ADT components, and discusses some of the research issues involved. We also show an example application built with the tools: a tourist information system running on an ultra-mobile PC."
W07-0312,{WIRE}: A Wearable Spoken Language Understanding System for the Military,2007,5,2,1,1,1049,helen hastie,Proceedings of the Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies,0,"In this paper, we present the WIRE system for human intelligence reporting and discuss challenges of deploying spoken language understanding systems for the military, particularly for dismounted warfighters. Using the PARADISE evaluation paradigm, we show that performance models derived using standard metrics can account for 68% of the variance of User Satisfaction. We discuss the implication of these results and how the evaluation paradigm may be modified for the military domain."
W03-0706,The Pragmatics of Taking a Spoken Language System Out of the Laboratory,2003,3,3,2,0,52739,jody daniels,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Research Directions in Dialogue Processing,0,"Lockheed Martin's Advanced Technology Laboratories has been designing, developing, testing, and evaluating spoken language understanding systems in several unique operational environments over the past five years. Through these experiences we have encountered numerous challenges in making each system become an integral part of a user's operations. In this paper, we discuss these challenges and report how we overcame them with respect to a number of domains."
P02-1049,What{'}s the Trouble: Automatically Identifying Problematic Dialogues in {DARPA} Communicator Dialogue Systems,2002,10,17,1,1,1049,helen hastie,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Spoken dialogue systems promise efficient and natural access to information services from any phone. Recently, spoken dialogue systems for widely used applications such as email, travel information, and customer care have moved from research labs into commercial use. These applications can receive millions of calls a month. This huge amount of spoken dialogue data has led to a need for fully automatic methods for selecting a subset of caller dialogues that are most likely to be useful for further system improvement, to be stored, transcribed and further analyzed. This paper reports results on automatically training a Problematic Dialogue Identifier to classify problematic human-computer dialogues using a corpus of 1242 DARPA Communicator dialogues in the travel planning domain. We show that using fully automatic features we can identify classes of problematic dialogues with accuracies from 67% to 89%."
hastie-etal-2002-automatic,Automatic Evaluation: Using a {DATE} Dialogue Act Tagger for User Satisfaction and Task Completion Prediction,2002,14,13,1,1,1049,helen hastie,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The objective of the DARPA Communicator project is to support rapid, cost-effective development of multi-modal speech-enabled dialogue systems with advanced conversational capabilities. During the course of the Communicator program, we have been involved in developing methods for measuring progress towards the program goals and assessing advances in the component technologies required to achieve such goals. Our goal has been to develop a lightweight evaluation paradigm for heterogeneous systems. In this paper, we utilize the Communicator evaluation corpus from 2001 and build on previous work applying the PARADISE evaluation framework to establish a baseline for fully automatic system evaluation. We train a regression tree to predict User Satisfaction using a random 80 of the dialogues for training. The metrics (features) we use for prediction are a fully automatic Task Success Measure, Efficiency Measures, and System Dialogue Act Behaviors extracted from the dialogue logfiles using the DATE (Dialogue Act Tagging for Evaluation) tagging scheme. The learned tree with the DATE metrics has a correlation of 0.614 ( of 0.376) with the actual user satisfaction values for the held out test set, while the learned tree without the DATE metrics has a correlation of 0.595 ( of 0.35)."
