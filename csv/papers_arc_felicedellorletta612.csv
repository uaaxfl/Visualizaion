2021.gem-1.2,Human Perception in Natural Language Generation,2021,-1,-1,3,1,6234,lorenzo mattei,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",0,"We ask subjects whether they perceive as human-produced a bunch of texts, some of which are actually human-written, while others are automatically generated. We use this data to fine-tune a GPT-2 model to push it to generate more human-like texts, and observe that this fine-tuned model produces texts that are indeed perceived more human-like than the original model. Contextually, we show that our automatic evaluation strategy well correlates with human judgements. We also run a linguistic analysis to unveil the characteristics of human- vs machine-perceived language."
2021.deelio-1.5,What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity,2021,-1,-1,3,1,789,alessio miaschi,Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,0,"This paper presents an investigation aimed at studying how the linguistic structure of a sentence affects the perplexity of two of the most popular Neural Language Models (NLMs), BERT and GPT-2. We first compare the sentence-level likelihood computed with BERT and the GPT-2{'}s perplexity showing that the two metrics are correlated. In addition, we exploit linguistic features capturing a wide set of morpho-syntactic and syntactic phenomena showing how they contribute to predict the perplexity of the two NLMs."
2021.deelio-1.6,How Do {BERT} Embeddings Organize Linguistic Knowledge?,2021,-1,-1,3,0,11250,giovanni puccetti,Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,0,"Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties."
2021.cmcl-1.5,That Looks Hard: Characterizing Linguistic Complexity in Humans and Language Models,2021,-1,-1,3,0,791,gabriele sarti,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,0,"This paper investigates the relationship between two complementary perspectives in the human assessment of sentence complexity and how they are modeled in a neural language model (NLM). The first perspective takes into account multiple online behavioral metrics obtained from eye-tracking recordings. The second one concerns the offline perception of complexity measured by explicit human judgments. Using a broad spectrum of linguistic features modeling lexical, morpho-syntactic, and syntactic properties of sentences, we perform a comprehensive analysis of linguistic phenomena associated with the two complexity viewpoints and report similarities and differences. We then show the effectiveness of linguistic features when explicitly leveraged by a regression model for predicting sentence complexity and compare its results with the ones obtained by a fine-tuned neural language model. We finally probe the NLM{'}s linguistic competence before and after fine-tuning, highlighting how linguistic information encoded in representations changes when the model learns to predict complexity."
2021.cmcl-1.23,Sentence Complexity in Context,2021,-1,-1,3,0,11550,benedetta iavarone,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,0,"We study the influence of context on how humans evaluate the complexity of a sentence in English. We collect a new dataset of sentences, where each sentence is rated for perceived complexity within different contextual windows. We carry out an in-depth analysis to detect which linguistic features correlate more with complexity judgments and with the degree of agreement among annotators. We train several regression models, using either explicit linguistic features or contextualized word embeddings, to predict the mean complexity values assigned to sentences in the different contextual windows, as well as their standard deviation. Results show that models leveraging explicit features capturing morphosyntactic and syntactic phenomena perform always better, especially when they have access to features extracted from all contextual sentences."
2020.repl4nlp-1.15,Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation,2020,-1,-1,2,1,789,alessio miaschi,Proceedings of the 5th Workshop on Representation Learning for NLP,0,"In this paper we present a comparison between the linguistic knowledge encoded in the internal representations of a contextual Language Model (BERT) and a contextual-independent one (Word2vec). We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that, although BERT is capable of understanding the full context of each word in an input sequence, the implicit knowledge encoded in its aggregated sentence representations is still comparable to that of a contextual-independent model. We also find that BERT is able to encode sentence-level properties even within single-word embeddings, obtaining comparable or even superior results than those obtained with sentence representations."
2020.lrec-1.114,{``}Voices of the Great War{''}: A Richly Annotated Corpus of {I}talian Texts on the First World War,2020,-1,-1,4,0,16843,federico boschetti,Proceedings of the 12th Language Resources and Evaluation Conference,0,"{``}Voices of the Great War{''} is the first large corpus of Italian historical texts dating back to the period of First World War. This corpus differs from other existing resources in several respects. First, from the linguistic point of view it gives account of the wide range of varieties in which Italian was articulated in that period, namely from a diastratic (educated vs. uneducated writers), diaphasic (low/informal vs. high/formal registers) and diatopic (regional varieties, dialects) points of view. From the historical perspective, through a collection of texts belonging to different genres it represents different views on the war and the various styles of narrating war events and experiences. The final corpus is balanced along various dimensions, corresponding to the textual genre, the language variety used, the author type and the typology of conveyed contents. The corpus is fully annotated with lemmas, part-of-speech, terminology, and named entities. Significant corpus samples representative of the different {``}voices{''} have also been enriched with meta-linguistic and syntactic information. The layer of syntactic annotation forms the first nucleus of an Italian historical treebank complying with the Universal Dependencies standard. The paper illustrates the final resource, the methodology and tools used to build it, and the Web Interface for navigating it."
2020.lrec-1.828,Invisible to People but not to Machines: Evaluation of Style-aware {H}eadline{G}eneration in Absence of Reliable Human Judgment,2020,-1,-1,3,1,6234,lorenzo mattei,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We automatically generate headlines that are expected to comply with the specific styles of two different Italian newspapers. Through a data alignment strategy and different training/testing settings, we aim at decoupling content from style and preserve the latter in generation. In order to evaluate the generated headlines{'} quality in terms of their specific newspaper-compliance, we devise a fine-grained evaluation strategy based on automatic classification. We observe that our models do indeed learn newspaper-specific style. Importantly, we also observe that humans aren{'}t reliable judges for this task, since although familiar with the newspapers, they are not able to discern their specific styles even in the original human-written headlines. The utility of automatic evaluation goes therefore beyond saving the costs and hurdles of manual annotation, and deserves particular care in its design."
2020.lrec-1.883,Profiling-{UD}: a Tool for Linguistic Profiling of Texts,2020,-1,-1,3,1,11248,dominique brunato,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we introduce Profiling{--}UD, a new text analysis tool inspired to the principles of linguistic profiling that can support language variation research from different perspectives. It allows the extraction of more than 130 features, spanning across different levels of linguistic description. Beyond the large number of features that can be monitored, a main novelty of Profiling{--}UD is that it has been specifically devised to be multilingual since it is based on the Universal Dependencies framework. In the second part of the paper, we demonstrate the effectiveness of these features in a number of theoretical and applicative studies in which they were successfully used for text and author profiling."
2020.evalnlgeval-1.5,On the interaction of automatic evaluation and task framing in headline style transfer,2020,-1,-1,4,1,6234,lorenzo mattei,Proceedings of the 1st Workshop on Evaluating NLG Evaluation,0,"An ongoing debate in the NLG community concerns the best way to evaluate systems, with human evaluation often being considered the most reliable method, compared to corpus-based metrics. However, tasks involving subtle textual differences, such as style transfer, tend to be hard for humans to perform. In this paper, we propose an evaluation method for this task based on purposely-trained classifiers, showing that it better reflects system differences than traditional metrics such as BLEU."
2020.coling-main.65,Linguistic Profiling of a Neural Language Model,2020,-1,-1,3,1,789,alessio miaschi,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper we investigate the linguistic knowledge learned by a Neural Language Model (NLM) before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT{'}s capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence."
2020.bea-1.9,Tracking the Evolution of Written Language Competence in {L}2 {S}panish Learners,2020,-1,-1,4,1,789,alessio miaschi,Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"In this paper we present an NLP-based approach for tracking the evolution of written language competence in L2 Spanish learners using a wide range of linguistic features automatically extracted from students{'} written productions. Beyond reporting classification results for different scenarios, we explore the connection between the most predictive features and the teaching curriculum, finding that our set of linguistic features often reflect the explicit instructions that students receive during each course."
W19-4430,Linguistically-Driven Strategy for Concept Prerequisites Learning on {I}talian,2019,0,0,4,1,789,alessio miaschi,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"We present a new concept prerequisite learning method for Learning Object (LO) ordering that exploits only linguistic features extracted from textual educational resources. The method was tested in a cross- and in- domain scenario both for Italian and English. Additionally, we performed experiments based on a incremental training strategy to study the impact of the training set size on the classifier performances. The paper also introduces ITA-PREREQ, to the best of our knowledge the first Italian dataset annotated with prerequisite relations between pairs of educational concepts, and describe the automatic strategy devised to build it."
W18-6001,Assessing the Impact of Incremental Error Detection and Correction. A Case Study on the {I}talian {U}niversal {D}ependency Treebank,2018,0,0,2,1,24180,chiara alzetta,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),0,"Detection and correction of errors and inconsistencies in {``}gold treebanks{''} are becoming more and more central topics of corpus annotation. The paper illustrates a new incremental method for enhancing treebanks, with particular emphasis on the extension of error patterns across different textual genres and registers. Impact and role of corrections have been assessed in a dependency parsing experiment carried out with four different parsers, whose results are promising. For both evaluation datasets, the performance of parsers increases, in terms of the standard LAS and UAS measures and of a more focused measure taking into account only relations involved in error patterns, and at the level of individual dependencies."
L18-1719,{U}niversal {D}ependencies and Quantitative Typological Trends. A Case Study on Word Order,2018,0,0,2,1,24180,chiara alzetta,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1289,Is this Sentence Difficult? Do you Agree?,2018,0,3,3,1,11248,dominique brunato,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we present a crowdsourcing-based approach to model the human perception of sentence complexity. We collect a large corpus of sentences rated with judgments of complexity for two typologically-different languages, Italian and English. We test our approach in two experimental scenarios aimed to investigate the contribution of a wide set of lexical, morpho-syntactic and syntactic phenomena in predicting i) the degree of agreement among annotators independently from the assigned judgment and ii) the perception of sentence complexity."
W17-7624,Dangerous Relations in Dependency Treebanks,2017,0,0,2,1,24180,chiara alzetta,Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories,0,None
W17-6505,On the order of Words in {I}talian: a Study on Genre vs Complexity,2017,-1,-1,2,1,11248,dominique brunato,Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017),0,None
W17-5049,Stacked Sentence-Document Classifier Approach for Improving Native Language Identification,2017,0,4,2,1,18355,andrea cimino,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper, we describe the approach of the ItaliaNLP Lab team to native language identification and discuss the results we submitted as participants to the essay track of NLI Shared Task 2017. We introduce for the first time a 2-stacked sentence-document architecture for native language identification that is able to exploit both local sentence information and a wide set of general-purpose features qualifying the lexical and grammatical structure of the whole document. When evaluated on the official test set, our sentence-document stacked architecture obtained the best result among all the participants of the essay track with an F1 score of 0.8818."
L16-1014,{CI}t{A}: an {L}1 {I}talian Learners Corpus to Study the Development of Writing Competence,2016,0,1,3,0,34747,alessia barbagli,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we present the CItA corpus (Corpus Italiano di Apprendenti L1), a collection of essays written by Italian L1 learners collected during the first and second year of lower secondary school. The corpus was built in the framework of an interdisciplinary study jointly carried out by computational linguistics and experimental pedagogists and aimed at tracking the development of written language competence over the years and students{'} background information."
D16-1034,{P}a{CCSS}-{IT}: A Parallel Corpus of Complex-Simple Sentences for Automatic Text Simplification,2016,13,2,3,1,11248,dominique brunato,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-2618,{NLP}{--}Based Readability Assessment of Health{--}Related Texts: a Case Study on {I}talian Informed Consent Forms,2015,31,2,3,0,11249,giulia venturi,Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis,0,"The paper illustrates the results of a case study aimed at investigating and enhancing the accessibility of Italian healthxe2x80x93 related documents by relying on advanced NLP techniques, with particular attention to informed consent forms. Results achieved show that the features automatically extracted from the linguistically annotated text and ranging across different levels of linguistic description have a high discriminative power in order to guarantee a reliable readability assessment."
W15-1604,Design and Annotation of the First {I}talian Corpus for Text Simplification,2015,31,3,2,1,11248,dominique brunato,Proceedings of The 9th Linguistic Annotation Workshop,0,"In this paper, we present design and construction of the first Italian corpus for automatic and semixe2x80x90automatic text simplification. In line with current approaches, we propose a new annotation scheme specifically conceived to identify the typology of changes an original sentence undergoes when it is manually simplified. Such a scheme has been applied to two aligned Italian corpora, containing original texts with corresponding simplified versions, selected as representative of two different manual simplification strategies and addressing different target reader populations. Each corpus was annotated with the operations foreseen in the annotation scheme, covering different levels of linguistic description. Annotation results were analysed with the final aim of capturing peculiarities and differences of the different simplification strategies pursued in the two corpora."
W14-1820,Assessing the Readability of Sentences: Which Corpora and Features?,2014,37,10,1,1,6236,felice dellorletta,Proceedings of the Ninth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"The paper investigates the problem of sentence readability assessment, which is modelled as a classification task, with a specific view to text simplification. In particular, it addresses two open issues connected with it, i.e. the corpora to be used for training, and the identification of the most effective features to determine sentence readability. An existing readability assessment tool developed for Italian was specialized at the level of training corpus and learning algorithm. A maximum entropyxe2x80x93based feature selection and ranking algorithm (grafting) was used to identify to the most relevant features: it turned out that assessing the readability of sentences is a complex task, requiring a high number of features, mainly syntactic ones."
W14-0406,The {PAIS{\\`A}} Corpus of {I}talian Web Texts,2014,-1,-1,6,0,3005,verena lyding,Proceedings of the 9th Web as Corpus Workshop ({W}a{C}-9),0,None
dellorletta-etal-2014-t2k,{T}2{K}{\\textasciicircum}2: a System for Automatically Extracting and Organizing Knowledge from Texts,2014,21,14,1,1,6236,felice dellorletta,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we present T2K{\textasciicircum}2, a suite of tools for automatically extracting domainâspecific knowledge from collections of Italian and English texts. T2K{\textasciicircum}2 (TextâToâKnowledge v2) relies on a battery of tools for Natural Language Processing (NLP), statistical text analysis and machine learning which are dynamically integrated to provide an accurate and incremental representation of the content of vast repositories of unstructured documents. Extracted knowledge ranges from domainâspecific entities and named entities to the relations connecting them and can be used for indexing document collections with respect to different information types. T2K{\textasciicircum}2 also includes Âlinguistic profilingÂ functionalities aimed at supporting the user in constructing the acquisition corpus, e.g. in selecting texts belonging to the same genre or characterized by the same degree of specialization or in monitoring the Âadded valueÂ of newly inserted documents. T2K{\textasciicircum}2 is a web application which can be accessed from any browser through a personal account which has been tested in a wide range of domains."
W13-1906,Unsupervised Linguistically-Driven Reliable Dependency Parses Detection and Self-Training for Adaptation to the Biomedical Domain,2013,23,3,1,1,6236,felice dellorletta,Proceedings of the 2013 Workshop on Biomedical Natural Language Processing,0,"In this paper, a new selfxe2x80x90training method for domain adaptation is illustrated, where the selection of reliable parses is carried out by an unsupervised linguisticallyxe2x80x90 driven algorithm, ULISSE. The method has been tested on biomedical texts with results showing a significant improvement with respect to considered baselines, which demonstrates its ability to capture both reliability of parses and domainxe2x80x90 specificity of linguistic constructions."
W13-1727,Linguistic Profiling based on General{--}purpose Features and Native Language Identification,2013,24,6,2,1,18355,andrea cimino,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper, we describe our approach to native language identification and discuss the results we submitted as participants to the First NLI Shared Task. By resorting to a wide set of generalxe2x80x90purpose features qualifying the lexical and grammatical structure of a text, rather than to ad hoc features specifically selected for the NLI task, we achieved encouraging results, which show that the proposed approach is generalxe2x80x90purpose and portable across different tasks, domains and languages."
R13-1025,Linguistic Profiling of Texts Across Textual Genres and Readability Levels. An Exploratory Study on {I}talian Fictional Prose,2013,29,2,1,1,6236,felice dellorletta,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"In this paper we present a case study focusing on the literature genre, in particular on Italian fictional prose, aimed at identifying the features characterizing this text type. Identified features were tested in two classification tasks, i.e. by genre and by readability, with promising results. Interestingly, the same multixe2x80x93level set of linguistic features turned out to reliably capture variation within and across textual genres."
W12-5812,Genre-oriented Readability Assessment: a Case Study,2012,-1,-1,1,1,6236,felice dellorletta,Proceedings of the Workshop on Speech and Language Processing Tools in Education,0,None
W11-2308,{READ}{--}{IT}: Assessing Readability of {I}talian Texts with a View to Text Simplification,2011,32,54,1,1,6236,felice dellorletta,Proceedings of the Second Workshop on Speech and Language Processing for Assistive Technologies,0,"In this paper, we propose a new approach to readability assessment with a specific view to the task of text simplification: the intended audience includes people with low literacy skills and/or with mild cognitive impairment. READ-IT represents the first advanced readability assessment tool for what concerns Italian, which combines traditional raw text features with lexical, morpho-syntactic and syntactic information. In READ-IT readability assessment is carried out with respect to both documents and sentences where the latter represents an important novelty of the proposed approach creating the prerequisites for aligning the readability assessment step with the text simplification process. READ-IT shows a high accuracy in the document classification task and promising results in the sentence classification scenario."
W11-0314,{ULISSE}: an Unsupervised Algorithm for Detecting Reliable Dependency Parses,2011,28,10,1,1,6236,felice dellorletta,Proceedings of the Fifteenth Conference on Computational Natural Language Learning,0,"In this paper we present ULISSE, an unsupervised linguistically--driven algorithm to select reliable parses from the output of a dependency parser. Different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains. In all cases, ULISSE appears to outperform the baseline algorithms."
W10-3711,Contrastive Filtering of Domain-Specific Multi-Word Terms from Different Types of Corpora,2010,4,10,2,0,10600,francesca bonin,Proceedings of the 2010 Workshop on Multiword Expressions: from Theory to Applications,0,In this paper we tackle the challenging task of Multi-word term (MWT) extraction from different types of specialized corpora. Contrastive filtering of previously extracted MWTs results in a considerable increment of acquired domainspecific terms.
passarotti-dellorletta-2010-improvements,"Improvements in Parsing the Index {T}homisticus Treebank. Revision, Combination and a Feature Model for Medieval {L}atin",2010,19,14,2,0,16574,marco passarotti,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The creation of language resources for less-resourced languages like the historical ones benefits from the exploitation of language-independent tools and methods developed over the years by many projects for modern languages. Along these lines, a number of treebanks for historical languages started recently to arise, including treebanks for Latin. Among the Latin treebanks, the Index Thomisticus Treebank is a 68,000 token dependency treebank based on the Index Thomisticus by Roberto Busa SJ, which contains the opera omnia of Thomas Aquinas (118 texts) as well as 61 texts by other authors related to Thomas, for a total of approximately 11 million tokens. In this paper, we describe a number of modifications that we applied to the dependency parser DeSR, in order to improve the parsing accuracy rates on the Index Thomisticus Treebank. First, we adapted the parser to the specific processing of Medieval Latin, defining an ad-hoc configuration of its features. Then, in order to improve the accuracy rates provided by DeSR, we applied a revision parsing method and we combined the outputs produced by different algorithms. This allowed us to improve accuracy rates substantially, reaching results that are well beyond the state of the art of parsing for Latin."
bosco-etal-2010-comparing,Comparing the Influence of Different Treebank Annotations on Dependency Parsing,2010,11,16,5,0,17906,cristina bosco,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"As the interest of the NLP community grows to develop several treebanks also for languages other than English, we observe efforts towards evaluating the impact of different annotation strategies used to represent particular languages or with reference to particular tasks. This paper contributes to the debate on the influence of resources used for the training and development on the performance of parsing systems. It presents a comparative analysis of the results achieved by three different dependency parsers developed and tested with respect to two treebanks for the Italian language, namely TUT and ISST--TANL, which differ significantly at the level of both corpus composition and adopted dependency representations."
bonin-etal-2010-contrastive,A Contrastive Approach to Multi-word Extraction from Domain-specific Corpora,2010,0,21,2,0,10600,francesca bonin,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we present a novel approach to multi-word terminology extraction combining a well-known automatic term recognition approach, the C--NC value method, with a contrastive ranking technique, aimed at refining obtained results either by filtering noise due to common words or by discerning between semantically different types of terms within heterogeneous terminologies. Differently from other contrastive methods proposed in the literature that focus on single terms to overcome the multi-word terms' sparsity problem, the proposed contrastive function is able to handle variation in low frequency events by directly operating on pre-selected multi-word terms. This methodology has been tested in two case studies carried out in the History of Art and Legal domains. Evaluation of achieved results showed that the proposed two--stage approach improves significantly multi--word term extraction results. In particular, for what concerns the legal domain it provides an answer to a well-known problem in the semi--automatic construction of legal ontologies, namely that of singling out law terms from terms of the specific domain being regulated."
N09-2066,Reverse Revision and Linear Tree Combination for Dependency Parsing,2009,14,49,2,1,5833,giuseppe attardi,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"Deterministic transition-based Shift/Reduce dependency parsers make often mistakes in the analysis of long span dependencies (McDonald & Nivre, 2007)."
W08-2138,{D}e{SRL}: A Linear-Time Semantic Role Labeling System,2008,9,23,3,0,37169,massimiliano ciaramita,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"This paper describes the DeSRL system, a joined effort of Yahoo! Research Barcelona and Universita di Pisa for the CoNLL-2008 Shared Task (Surdeanu et al., 2008). The system is characterized by an efficient pipeline of linear complexity components, each carrying out a different sub-task. Classifier errors and ambiguities are addressed with several strategies: revision models, voting, and reranking. The system participated in the closed challenge ranking third in the complete problem evaluation with the following scores: 82.06 labeled macro F1 for the overall task, 86.6 labeled attachment for syntactic dependencies, and 77.5 labeled F1 for semantic dependencies."
D07-1119,Multilingual Dependency Parsing and Domain Adaptation using {D}e{SR},2007,21,32,2,1,5833,giuseppe attardi,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We describe our experiments using the DeSR parser in the multilingual and domain adaptation tracks of the CoNLL 2007 shared task. DeSR implements an incremental deterministic Shift/Reduce parsing algorithm, using specific rules to handle non-projective dependencies. For the multilingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language. For the domain adaptation track we applied a tree revision method which learns how to correct the mistakes made by the base parser on the adaptation domain."
W06-0604,Probing the Space of Grammatical Variation: Induction of Cross-Lingual Grammatical Constraints from Treebanks,2006,11,2,1,1,6236,felice dellorletta,Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006,0,"The paper reports on a detailed quantitative analysis of distributional language data of both Italian and Czech, highlighting the relative contribution of a number of distributed grammatical factors to sentence-based identification of subjects and direct objects. The work uses a Maximum Entropy model of stochastic resolution of conflicting grammatical constraints and is demonstrably capable of putting explanatory theoretical accounts to the test of usage-based empirical verification."
dellorletta-etal-2006-searching,Searching treebanks for functional constraints: cross-lingual experiments in grammatical relation assignment,2006,12,0,1,1,6236,felice dellorletta,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The paper reports on a detailed quantitative analysis of distributional language data of both Italian and Czech, highlighting the relative contribution of a number of distributed grammatical factors to sentence-based identification of subjects and direct objects. The work is based on a Maximum Entropy model of stochastic resolution of grammatical conflicting constraints, and is demonstrably capable of putting explanatory theoretical accounts to the challenging test of an extensive, usage-based empirical verification."
W05-0509,Climbing the Path to Grammar: A Maximum Entropy Model of Subject/Object Learning,2005,21,14,1,1,6236,felice dellorletta,Proceedings of the Workshop on Psychocomputational Models of Human Language Acquisition,0,"In this paper, we discuss an application of Maximum Entropy to modeling the acquisition of subject and object processing in Italian. The model is able to learn from corpus data a set of experimentally and theoretically well-motivated linguistic constraints, as well as their relative salience in Italian grammar development and processing. The model is also shown to acquire robust syntactic generalizations by relying on the evidence provided by a small number of high token frequency verbs only. These results are consistent with current research focusing on the role of high frequency verbs in allowing children to converge on the most salient constraints in the grammar."
