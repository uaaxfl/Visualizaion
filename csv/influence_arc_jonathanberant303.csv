2020.acl-main.495,N16-1181,0,0.171489,"d field goal. The Texans tried to cut the lead with QB Matt Schaub getting a 8-yard TD pass to WR Andre Johnson, but the Titans would pull away with RB Javon Ringer throwing a 7-yard TD pass . The Texans tried to come back into the game in the fourth quarter, but only came away with Schaub throwing a 12-yard TD pass to WR Kevin Walter. relocate[who threw] find-max-num filter [the second half] find [touchdown pass] Figure 2: An example for a mapping of an utterance to a gold program and a perfect execution in a reasoning problem from NLVR2 (top) and DROP (bottom). Neural module networks (NMNs; Andreas et al., 2016) parse an input utterance into an executable program composed of learnable modules that are designed to perform atomic reasoning tasks and can be composed to perform complex reasoning against an unstructured context. NMNs are appealing since their output is interpretable; they provide a logical meaning representation of the utterance and also the outputs of the intermediate steps (modules) to reach the final answer. However, because module parameters are typically learned from end-task supervision only, it is possible that the program will not be a faithful explanation of the behaviour of the"
2020.acl-main.495,D17-1160,1,0.826889,"d outputs a distribution over the passage tokens and find-num outputs a distribution over the numbers in the passage. We extend their model and introduce additional modules; addition and subtraction to add or subtract passage numbers, and extract-answer which directly predicts an answer span from the representations of passage tokens without any explicit compositional reasoning. We use BERT-base (Devlin et al., 2019) to encode the input question and passage. The Text-NMN does not have access to gold programs, and thus we implement a parser as an encoder-decoder model with attention similar to Krishnamurthy et al. (2017), which takes the utterance as input, and outputs a linearized abstract syntax tree of the predicted program. 5596 3 Module-wise Faithfulness Neural module networks (NMNs) facilitate interpretability of their predictions via the reasoning steps in the structured program and providing the outputs of those intermediate steps during execution. For example, in Figure 2, all reasoning steps taken by both the Visual-NMN and Text-NMN can be discerned from the program and the intermediate module outputs. However, because module parameters are learned from an end-task, there is no guarantee that the mo"
2020.acl-main.495,P19-1416,1,0.915983,"Missing"
2020.acl-main.495,Q19-1016,0,0.0333891,"Missing"
2020.acl-main.495,C00-2137,0,\N,Missing
2020.acl-main.495,P17-2034,0,\N,Missing
2020.acl-main.495,D18-1259,0,\N,Missing
2020.acl-main.495,N19-1246,1,\N,Missing
2020.acl-main.495,N19-1423,0,\N,Missing
2020.acl-main.495,P19-1644,0,\N,Missing
2020.acl-main.495,D19-1002,0,\N,Missing
2020.acl-main.495,D19-1455,0,\N,Missing
2020.acl-main.495,2020.acl-main.386,0,\N,Missing
2020.acl-main.89,D19-1609,0,0.231029,", and 67.3 → 69.3 EM for MS-TAG. This shows that G EN BERT can be used as a drop-in replacement for BERT, when numerical reasoning is needed. To summarize, we have empirically shown that one can inject numerical reasoning skills into a pre-trained LM, resulting in good performance on DROP, generalization to MWP, while maintaining high performance on standard RC datasets. Moreover, the resulting weights can be used for initializing numerical reasoning models. 953 6 Related Work Most NRoT models designed for DROP are extractive QA models augmented with specialized modules (§2). Two recent work (Andor et al., 2019; Chen et al., 2020) take a more symbolic approach and output a symbolic program augmented with operations over text. In our work, numerical computations are latent and performed internally by the model. A related line of work has been analyzing the mathematical reasoning abilities of neural models over text (Wallace et al., 2019; Rozen et al., 2019; Ravichander et al., 2019), and on arithmetic problems (Saxton et al., 2019; Amini et al., 2019; Lample and Charton, 2020). Designing pre-training tasks to teach LMs additional skills has been applied by Huang et al. (2019), who designed cross-ling"
2020.acl-main.89,N19-1423,0,0.308529,"ining steps over large amounts of synthetic numerical data (ND) and textual data (TD); (b) we further fine-tune the model over either numerical reasoning datasets (DROP, MAWPS) or reading comprehension datasets (SQ UAD). text, compute their difference, and generate the tokens corresponding to the result, which generally do not appear in the input passage. Introduction Recently, models trained on large amounts of data with a language modeling (LM) objective, have shown great promise in natural language processing, exhibiting surprising amounts of knowledge and information (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Petroni et al., 2019; Hewitt and Manning, 2019). However, high-level skills, such as the ability to perform numerical reasoning over text, can be challenging to capture with a LM objective only. Consider the example in Table 1. To solve the first question (Q1), a model must capture the value of numbers in the ∗ extractive generative These authors contributed equally. To make the task more manageable, state-of-theart models have employed specialized architectures, restricting the space of possible numerical computations to a limited set. Modules were design"
2020.acl-main.89,N19-1246,0,0.362915,"asoning over text, can be challenging to capture with a LM objective only. Consider the example in Table 1. To solve the first question (Q1), a model must capture the value of numbers in the ∗ extractive generative These authors contributed equally. To make the task more manageable, state-of-theart models have employed specialized architectures, restricting the space of possible numerical computations to a limited set. Modules were designed for counting (but only until ‘9’) and for addition and subtraction (but of 2-3 numbers only). Such models perform well on existing datasets, such as DROP (Dua et al., 2019), but do not generalize to unsupported computations, which will require modifying the model architecture. Moreover, current models marginalize at training time over all numerical expressions that evaluate to the correct answer. Since the number of such expressions grows exponentially, scaling these approaches to arbitrary computations entails using non-differentiable operations (sampling or computing top-K numerical expressions), which can lead to training difficulties. 946 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946–958 c July 5 - 10, 202"
2020.acl-main.89,N19-1419,0,0.0140844,"a (TD); (b) we further fine-tune the model over either numerical reasoning datasets (DROP, MAWPS) or reading comprehension datasets (SQ UAD). text, compute their difference, and generate the tokens corresponding to the result, which generally do not appear in the input passage. Introduction Recently, models trained on large amounts of data with a language modeling (LM) objective, have shown great promise in natural language processing, exhibiting surprising amounts of knowledge and information (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Petroni et al., 2019; Hewitt and Manning, 2019). However, high-level skills, such as the ability to perform numerical reasoning over text, can be challenging to capture with a LM objective only. Consider the example in Table 1. To solve the first question (Q1), a model must capture the value of numbers in the ∗ extractive generative These authors contributed equally. To make the task more manageable, state-of-theart models have employed specialized architectures, restricting the space of possible numerical computations to a limited set. Modules were designed for counting (but only until ‘9’) and for addition and subtraction (but of 2-3 num"
2020.acl-main.89,D14-1058,0,0.42861,"in the context. Specifically, the answer is either: (a) a span (or list of spans) from the context c or question q, or (b) a number that is the result of some computation (see examples in Table 1). Two natural, yet opposing, approaches lend themselves to tackling NRoT: (a) A symbolic approach: a model can read the question and context, output a numerical expression and evaluate the answer with an external symbolic calculator. This approach is a particular case of semantic parsing (Kamath and Das, 2019), and was common in early NRoT datasets (Koncel-Kedziorski et al., 2015; Roy and Roth, 2015; Hosseini et al., 2014). How947 ever, it suffers from several drawbacks. First, beFinally, for deciding which answer head to use cause numerical expressions are discrete and their for a given input, a type head FFtyp (L) outputs space grows combinatorially, the model must learn a probability distribution phead (h |q, c) (using a to search in this space using non-differentiable op- FFN). Thus the model probability for an answer is X erations, which are usually difficult to optimize. p(a |q, c) = phead (h |c, q) · p(a |c, q, h). Second, numerical expressions are limited to nuh∈heads merical answers, while in DROP ofte"
2020.acl-main.89,D19-1170,0,0.349427,"rate the model can only count up to ‘9’, and numerical it. While this makes training straightforward, the model must learn to perform numerical computa- computations are restricted to signed combinations of a few numbers. Second, expanding the space tions from the relatively small target dataset. We empirically show in §3 that this leads to low per- of supported numerical computations is non-trivial, because training involves marginalizing over all formance in general. expressions that lead to the correct answer. Since As a compromise, most NRoT models (Dua et al., 2019; Kinley and Lin, 2019; Hu et al., 2019; the space of numerical expressions grows expoEfrat et al., 2019) have taken a hybrid approach: nentially, expanding this space quickly leads to a difficult search problem. Third, delegating nuthey augment standard extractive QA models with merical computations to an external symbolic calspecialized modules for handling a limited set of culator leads to modeling challenges, since there numerical computations. We briefly describe this could be interactions between text and numerical architecture, as it is the basis for our model in §3. computation: Consider the DROP question “How Given a quest"
2020.acl-main.89,D19-1252,0,0.0189745,"es (§2). Two recent work (Andor et al., 2019; Chen et al., 2020) take a more symbolic approach and output a symbolic program augmented with operations over text. In our work, numerical computations are latent and performed internally by the model. A related line of work has been analyzing the mathematical reasoning abilities of neural models over text (Wallace et al., 2019; Rozen et al., 2019; Ravichander et al., 2019), and on arithmetic problems (Saxton et al., 2019; Amini et al., 2019; Lample and Charton, 2020). Designing pre-training tasks to teach LMs additional skills has been applied by Huang et al. (2019), who designed cross-lingual pre-training tasks to teach better mappings between languages, and Lee et al. (2019), who introduced the Inverse Cloze Task to pre-train an information retriever. 7 Conclusions Large pre-trained LMs lack high-level skills such as numerical reasoning. Consequently, current models that perform numerical reasoning over a pretrained LM resorted to customized modules with limited flexibility. In this work, we propose a general method for injecting additional skills into LMs, assuming automatic data generation is possible. We apply our approach to the task of numerical r"
2020.acl-main.89,Q15-1042,0,0.160656,"umerical computation using the entities and numbers in the context. Specifically, the answer is either: (a) a span (or list of spans) from the context c or question q, or (b) a number that is the result of some computation (see examples in Table 1). Two natural, yet opposing, approaches lend themselves to tackling NRoT: (a) A symbolic approach: a model can read the question and context, output a numerical expression and evaluate the answer with an external symbolic calculator. This approach is a particular case of semantic parsing (Kamath and Das, 2019), and was common in early NRoT datasets (Koncel-Kedziorski et al., 2015; Roy and Roth, 2015; Hosseini et al., 2014). How947 ever, it suffers from several drawbacks. First, beFinally, for deciding which answer head to use cause numerical expressions are discrete and their for a given input, a type head FFtyp (L) outputs space grows combinatorially, the model must learn a probability distribution phead (h |q, c) (using a to search in this space using non-differentiable op- FFN). Thus the model probability for an answer is X erations, which are usually difficult to optimize. p(a |q, c) = phead (h |c, q) · p(a |c, q, h). Second, numerical expressions are limited to n"
2020.acl-main.89,N16-1136,0,0.0786859,"Missing"
2020.acl-main.89,P19-1612,0,0.0178801,"c program augmented with operations over text. In our work, numerical computations are latent and performed internally by the model. A related line of work has been analyzing the mathematical reasoning abilities of neural models over text (Wallace et al., 2019; Rozen et al., 2019; Ravichander et al., 2019), and on arithmetic problems (Saxton et al., 2019; Amini et al., 2019; Lample and Charton, 2020). Designing pre-training tasks to teach LMs additional skills has been applied by Huang et al. (2019), who designed cross-lingual pre-training tasks to teach better mappings between languages, and Lee et al. (2019), who introduced the Inverse Cloze Task to pre-train an information retriever. 7 Conclusions Large pre-trained LMs lack high-level skills such as numerical reasoning. Consequently, current models that perform numerical reasoning over a pretrained LM resorted to customized modules with limited flexibility. In this work, we propose a general method for injecting additional skills into LMs, assuming automatic data generation is possible. We apply our approach to the task of numerical reasoning over text, using a general-purpose model called G EN BERT, and a simple framework for generating large a"
2020.acl-main.89,2021.ccl-1.108,0,0.168414,"Missing"
2020.acl-main.89,N18-1202,0,0.0243416,"a) We add two pre-training steps over large amounts of synthetic numerical data (ND) and textual data (TD); (b) we further fine-tune the model over either numerical reasoning datasets (DROP, MAWPS) or reading comprehension datasets (SQ UAD). text, compute their difference, and generate the tokens corresponding to the result, which generally do not appear in the input passage. Introduction Recently, models trained on large amounts of data with a language modeling (LM) objective, have shown great promise in natural language processing, exhibiting surprising amounts of knowledge and information (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Petroni et al., 2019; Hewitt and Manning, 2019). However, high-level skills, such as the ability to perform numerical reasoning over text, can be challenging to capture with a LM objective only. Consider the example in Table 1. To solve the first question (Q1), a model must capture the value of numbers in the ∗ extractive generative These authors contributed equally. To make the task more manageable, state-of-theart models have employed specialized architectures, restricting the space of possible numerical computations to a limited set"
2020.acl-main.89,D19-1250,0,0.0251621,"a (ND) and textual data (TD); (b) we further fine-tune the model over either numerical reasoning datasets (DROP, MAWPS) or reading comprehension datasets (SQ UAD). text, compute their difference, and generate the tokens corresponding to the result, which generally do not appear in the input passage. Introduction Recently, models trained on large amounts of data with a language modeling (LM) objective, have shown great promise in natural language processing, exhibiting surprising amounts of knowledge and information (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Petroni et al., 2019; Hewitt and Manning, 2019). However, high-level skills, such as the ability to perform numerical reasoning over text, can be challenging to capture with a LM objective only. Consider the example in Table 1. To solve the first question (Q1), a model must capture the value of numbers in the ∗ extractive generative These authors contributed equally. To make the task more manageable, state-of-theart models have employed specialized architectures, restricting the space of possible numerical computations to a limited set. Modules were designed for counting (but only until ‘9’) and for addition and"
2020.acl-main.89,D16-1264,0,0.393925,"difficulties. 946 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 946–958 c July 5 - 10, 2020. 2020 Association for Computational Linguistics (b) Pre-training on these tasks provides G EN BERT with 1) skills to reach performance that matches state-of-the-art models of comparable size on DROP (Dua et al., 2019), a standard numerical reasoning dataset, as well as 2) the ability to generalize to math word problem (MWP) datasets (Koncel-Kedziorski et al., 2016). (c) G EN BERT learns these numerical skills while maintaining high performance on SQuAD (Rajpurkar et al., 2016), a standard reading comprehension dataset. (d) Initializing models for numerical reasoning with G EN BERT’s weights improves their original performance. To conclude, in this work we address the problem of injecting LMs with numerical reasoning skills. Our contributions are: • A method for injecting skills into pre-trained LMs, given that automatic data generation is possible. • G EN BERT, an architecture for pre-trained LM with generative and extractive abilities. • A framework for generating numerical and textual synthetic data for numerical reasoning. Our code and data can be downloaded fro"
2020.acl-main.89,K19-1033,0,0.0509916,"s. Moreover, the resulting weights can be used for initializing numerical reasoning models. 953 6 Related Work Most NRoT models designed for DROP are extractive QA models augmented with specialized modules (§2). Two recent work (Andor et al., 2019; Chen et al., 2020) take a more symbolic approach and output a symbolic program augmented with operations over text. In our work, numerical computations are latent and performed internally by the model. A related line of work has been analyzing the mathematical reasoning abilities of neural models over text (Wallace et al., 2019; Rozen et al., 2019; Ravichander et al., 2019), and on arithmetic problems (Saxton et al., 2019; Amini et al., 2019; Lample and Charton, 2020). Designing pre-training tasks to teach LMs additional skills has been applied by Huang et al. (2019), who designed cross-lingual pre-training tasks to teach better mappings between languages, and Lee et al. (2019), who introduced the Inverse Cloze Task to pre-train an information retriever. 7 Conclusions Large pre-trained LMs lack high-level skills such as numerical reasoning. Consequently, current models that perform numerical reasoning over a pretrained LM resorted to customized modules with limi"
2020.acl-main.89,Q15-1001,0,0.128264,"Missing"
2020.acl-main.89,K19-1019,0,0.0404515,"standard RC datasets. Moreover, the resulting weights can be used for initializing numerical reasoning models. 953 6 Related Work Most NRoT models designed for DROP are extractive QA models augmented with specialized modules (§2). Two recent work (Andor et al., 2019; Chen et al., 2020) take a more symbolic approach and output a symbolic program augmented with operations over text. In our work, numerical computations are latent and performed internally by the model. A related line of work has been analyzing the mathematical reasoning abilities of neural models over text (Wallace et al., 2019; Rozen et al., 2019; Ravichander et al., 2019), and on arithmetic problems (Saxton et al., 2019; Amini et al., 2019; Lample and Charton, 2020). Designing pre-training tasks to teach LMs additional skills has been applied by Huang et al. (2019), who designed cross-lingual pre-training tasks to teach better mappings between languages, and Lee et al. (2019), who introduced the Inverse Cloze Task to pre-train an information retriever. 7 Conclusions Large pre-trained LMs lack high-level skills such as numerical reasoning. Consequently, current models that perform numerical reasoning over a pretrained LM resorted to c"
2020.acl-main.89,D19-1534,0,0.402875,"er all ways in which the answer can be predicted: pdec (ai+1 |a0 , ..ai , c, q). i=0 As we have a generative model, we can remove the specialized count and arithmetic heads from §2. Thus, the type head FFtyp (Henc ) outputs a distribution (pq , pc , pdec ) over the context span, question span, and decoder heads. To improve pre-training on the numeric data (§4), we make two additional modifications. Digit Tokenization (DT) Conventional wordpiece tokenization treats numbers no differently than any other token. However, computing the value of numbers should be simpler when using digits directly (Wallace et al., 2019). Hence, we tokenize numbers digit-by-digit. For example, a wordpiece ##d1 · · · dk where di ∈ {0,...,9} is further split into ##d1 , ..., ##dk . We show in §5.1 that this substantially improves sample complexity when training to perform numerical operations. eSEP en e1 eCLS pdec where W is a parameter matrix (Hendrycks and Gimpel, 2016; Ba et al., 2016). Analogously, we add FFdec to the decoder. To further distinguish the encoder and decoder, we use distinct start and end tokens for input and output sequences. Given m answer tokens a = (a1 , . . . , am ), we form an output sequence with m + 2"
2020.emnlp-main.248,Q19-1026,0,0.0475102,"Missing"
2020.emnlp-main.248,2021.ccl-1.108,0,0.182691,"Missing"
2020.emnlp-main.248,D18-1538,0,0.0274496,"Missing"
2020.emnlp-main.248,D19-1609,0,0.0312582,"n the input, it is not a single span. For example, in Figure 1 the answer includes two people who appear as noncontiguous spans in the context. Existing models (Seo et al., 2017; Dua et al., 2019) are by design unable to provide the correct answer to such multispan questions. While most work has largely ignored this issue, recent work has taken initial steps towards handling multi-span questions. Hu et al. (2019) proposed to predict the number of output spans for each question, and used a non-differentiable inference procedure to find them in the text, leading to a complex training procedure. Andor et al. (2019) proposed a Merge operation that merges spans, but is constrained to at most 2 spans. Chen et al. (2020) proposed a non-differentiable symbolic approach which outputs programs that compose single-span extractions. In this work, we propose a simple and fully differentiable architecture for handling multi-span questions that evades the aforementioned shortcomings, and outperforms prior work. Similar to Yao et al. (2013), who used a linear model over treebased features, we cast question answering as a sequence tagging task, predicting for each token whether it is part of the answer. At test time,"
2020.emnlp-main.248,D19-1606,0,0.0295542,"Missing"
2020.emnlp-main.248,N19-1423,0,0.0355342,"the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3074–3080, c November 16–20, 2020. 2020 Association for Computational Linguistics bining the single-span and multi-span architectures further improves performance by 2.1 EM on DROP, surpassing results by other span-extraction methods on both datasets. 2 Background: Single-span Model Setup Given a training set of question-contextanswer triplets (qi , ci , ai )N i=1 , our goal is to learn a function that maps a question-context pair (q, c) to an answer a. We briefly review the standard singlespan architecture for RC (Devlin et al., 2019), which we build upon. First, we encode the question and context with a pre-trained language model, such as BERT (Devlin et al., 2019): h = Encoder([q, c]), where h = (h1 , . . . , hm ) is a sequence of contextualized representations for all input tokens. Then, two parameterized functions (feed-forward networks), fstart (hi ) and fend (hi ), are used to compute a score for each token, corresponding to whether that token is the start or the end of the answer. Last, the start and end probability for each token i is computed as follows: pstart = softmax (fstart (h1 ), . . . , fstart (hm ))i , i p"
2020.emnlp-main.248,N19-1246,0,0.459498,"2016; Seo et al., 2017; Yu et al., 2018; Kwiatkowski et al., 2019), RC is set up as an extractive task, where the answer is constrained to be a single span from the input. This makes learning easier, since the model does not need to generate text abstractively, while still being expressive enough to capture a large set of questions. However, for some questions, while the answer is indeed extractive, i.e., contained in the input, it is not a single span. For example, in Figure 1 the answer includes two people who appear as noncontiguous spans in the context. Existing models (Seo et al., 2017; Dua et al., 2019) are by design unable to provide the correct answer to such multispan questions. While most work has largely ignored this issue, recent work has taken initial steps towards handling multi-span questions. Hu et al. (2019) proposed to predict the number of output spans for each question, and used a non-differentiable inference procedure to find them in the text, leading to a complex training procedure. Andor et al. (2019) proposed a Merge operation that merges spans, but is constrained to at most 2 spans. Chen et al. (2020) proposed a non-differentiable symbolic approach which outputs programs t"
2020.emnlp-main.248,D19-1170,0,0.11251,"not need to generate text abstractively, while still being expressive enough to capture a large set of questions. However, for some questions, while the answer is indeed extractive, i.e., contained in the input, it is not a single span. For example, in Figure 1 the answer includes two people who appear as noncontiguous spans in the context. Existing models (Seo et al., 2017; Dua et al., 2019) are by design unable to provide the correct answer to such multispan questions. While most work has largely ignored this issue, recent work has taken initial steps towards handling multi-span questions. Hu et al. (2019) proposed to predict the number of output spans for each question, and used a non-differentiable inference procedure to find them in the text, leading to a complex training procedure. Andor et al. (2019) proposed a Merge operation that merges spans, but is constrained to at most 2 spans. Chen et al. (2020) proposed a non-differentiable symbolic approach which outputs programs that compose single-span extractions. In this work, we propose a simple and fully differentiable architecture for handling multi-span questions that evades the aforementioned shortcomings, and outperforms prior work. Simi"
2020.emnlp-main.248,D16-1264,0,0.115107,"Missing"
2020.emnlp-main.248,W95-0107,0,0.455312,"(fstart (h1 ), . . . , fstart (hm ))i , i pend = softmax (fend (h1 ), . . . , fend (hm ))i , i where both pstart , pend ∈ Rm×1 . Training is done by minimizing cross entropy of the start and end indices of the gold span, and at test time the answer span is extracted by finding the indices (s, e): end (s, e) = arg max pstart s pe . s≤e 3 3.1 Multi-span Model Span Extraction as Sequence Tagging Extracting a variable number of spans from an input text is standard in many natural language processing tasks, such as Named Entity Recognition (NER) and is commonly cast as a sequence tagging problem (Ramshaw and Marcus, 1995). Here we apply this approach to multi-span questions. Our model uses the same contextualized representations h, but rather than predicting start and end probabilities, it outputs a probability distribution over a set of tags for each token. We experiment with two tagging schemes. First, the wellknown BIO tagging (Sang, 2000; Huang et al., 2015), in which B denotes the first token of an output span, I denotes subsequent tokens in a span, and O denotes tokens that are not part of an output span. In addition, we experiment with a simpler IO tagging scheme, where words are tagged as either part o"
2020.emnlp-main.248,N13-1106,0,0.0752189,"Missing"
2020.emnlp-main.248,2020.emnlp-main.582,0,0.0149239,"re (TASE: TAg-based Span Extraction) with the traditional single-span extraction (SSE), as well as to each separately. Comparison to previous models For a fair comparison with prior work on DROP, we also train our model initialized with BERTLARGE , as all prior work used it as an encoder. On DROP, TASEBIO+SSE (BERTLARGE ) outperforms all prior models that handle multi-span questions, improving by at least 3.2 EM points. On multi-span questions, we dramatically improve performance over BERT-C ALC and MTMSN, while obtaining similar performance to NeRd. On Q UOREF, compared to CorefRoBERTaLARGE (Ye et al., 2020) which uses the same method as MTMSN for multi-span extraction, we achieve a substantial improvement of over 20 EM on multi-span questions and an improvement of 4.5 EM and 3.2 F1 on the full development set, where the best results are achieved when using solely our multi-span architecture with IO-tagging. Comparing span extraction architectures Table 1 also shows that in both DROP and Q UOREF, replacing the single-span extraction architecture with our multi-span extraction results in dramatic improvement in multi-span question performance, while single-span question performance is either maint"
2020.findings-emnlp.117,W07-2441,0,0.0137691,"1. UD Parsing Finally, we discuss dependency parsing in the universal dependencies (UD) formalism (Nivre et al., 2016). We look at dependency parsing to show that contrast sets apply not only to modern “high-level” NLP tasks but also to longstanding linguistic analysis tasks. We first chose a specific type of attachment ambiguity to target: the classic problem of prepositional phrase (PP) attachment (Collins and Brooks, 1995), e.g. We ate spaghetti with forks versus We ate spaghetti with meatballs. We use a subset of the English UD treebanks: GUM (Zeldes, 2017), the English portion of LinES (Ahrenberg, 2007), the English portion of ParTUT (Sanguinetti and Bosco, 2015), and the dependency-annotated English Web Treebank (Silveira et al., 2014). We searched these treebanks for sentences that include a potentially structurally ambiguous attachment from the head of a PP to either a noun or a verb. We then perturbed these sentences by altering one of their noun phrases such that the semantics of the perturbed sentence required a different attachment for the PP. We then re-annotated these perturbed sentences to indicate the new attachment(s). Summary While the overall process we recommend for constructi"
2020.findings-emnlp.117,D15-1075,0,0.0445289,"ow frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distribut"
2020.findings-emnlp.117,W18-6433,0,0.0341655,"Missing"
2020.findings-emnlp.117,D19-1606,1,0.924571,"round a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates"
2020.findings-emnlp.117,N19-1423,0,0.0110664,"st sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set de"
2020.findings-emnlp.117,N19-1246,1,0.944307,"arts of the gaps around a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while pe"
2020.findings-emnlp.117,D18-1407,1,0.898157,"uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011)"
2020.findings-emnlp.117,D19-1107,1,0.925439,"l language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps"
2020.findings-emnlp.117,P18-2103,0,0.0419316,"ded minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed inst"
2020.findings-emnlp.117,N18-2017,1,0.922108,"uction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in th"
2020.findings-emnlp.117,D19-1170,0,0.0120239,"work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set design, etc. On IMDb and PERSPECTRUM, the model achieves a reasonably high consistency, suggesting that, while there is definitely still room fo"
2020.findings-emnlp.117,D17-1263,0,0.0339764,"h to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring lo"
2020.findings-emnlp.117,D17-1215,0,0.560991,"rks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Lev"
2020.findings-emnlp.117,D19-1423,0,0.0613359,"Missing"
2020.findings-emnlp.117,2020.emnlp-main.12,1,0.714022,"ple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits simil"
2020.findings-emnlp.117,W17-5401,0,0.118381,"Missing"
2020.findings-emnlp.117,P19-1554,1,0.81477,"ctive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot confirm that a model does align with it. This is because annotators cannot exhaustively label all inputs near a pivot and thus a contrast set will necessarily be incomplete. However, note that this problem is not unique to contrast sets—similar issues hold for the original test set as well as adversarial test sets (Jia and Liang, 2017), challenge sets (Naik et al., 2018), and input perturbations (Ribeiro et al., 2018a; Feng et al., 2018). See Feng et al. (2019) for a detailed discussion of how dataset analysis methods only have negative predictive power. Dataset-Specific Instantiations The process for creating contrast sets is dataset-specific: although we present general guidelines that hold across many tasks, experts must still characterize the type of phenomena each individual dataset is intended to capture. Fortunately, the original dataset authors should already have thought deeply about such phenomena. Hence, creating contrast sets should be well-defined and relatively straightforward. 3 How to Create Contrast Sets Here, we walk through our pr"
2020.findings-emnlp.117,D19-5808,1,0.8991,"n the contrast sets (not including the original example). We report percentage accuracy for NLVR2, IMDb, PERSPECTRUM, MATRES, and BoolQ; F1 scores for DROP and Q UOREF; Exact Match (EM) scores for ROPES and MC-TACO; and unlabeled attachment score on modified attachments for the UD English dataset. We also report contrast consistency: the percentage of the “# Sets” contrast sets for which a model’s predictions are correct for all examples in the set (including the original example). More details on datasets, models, and metrics can be found in §A and §B. • Quoref (Dasigi et al., 2019) • ROPES (Lin et al., 2019) • BoolQ (Clark et al., 2019) • MC-TACO (Zhou et al., 2019) We choose these datasets because they span a variety of tasks (e.g., reading comprehension, sentiment analysis, visual reasoning) and input-output formats (e.g., classification, span extraction, structured prediction). We include high-level tasks for which dataset artifacts are known to be prevalent, as well as longstanding formalism-based tasks, where data artifacts have been less of an issue (or at least have been less well-studied). 4.2 Contrast Set Construction The contrast sets were constructed by NLP researchers who were deeply"
2020.findings-emnlp.117,2021.ccl-1.108,0,0.144976,"Missing"
2020.findings-emnlp.117,P11-1015,0,0.07085,"ena they are most interested in studying and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ"
2020.findings-emnlp.117,J93-2004,0,0.068933,"ation: Two similarly-colored and similarly-posed chow dogs are face to face in one image. Figure 1: An example contrast set for NLVR2 (Suhr and Artzi, 2019). The label for the original example is T RUE and the label for all of the perturbed examples is FALSE. The contrast set allows probing of a model’s decision boundary local to examples in the test set, which better evaluates whether the model has captured the relevant phenomena than standard metrics on i.i.d. test data. Introduction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input"
2020.findings-emnlp.117,D18-1151,0,0.0148944,"1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in systematic gaps. Thus contrast sets often require less effort to create, and they remain grounded i"
2020.findings-emnlp.117,N19-1314,0,0.0179195,"the task definition than a random selection of input / output pairs. 2.3 Contrast sets in practice Given these definitions, we now turn to the actual construction of contrast sets in practical NLP settings. There were two things left unspecified in the definitions above: the distance function d to use in discrete input spaces, and the method for sampling from a local decision boundary. While there has been some work trying to formally characterize dis2 In this discussion we are talking about the true decision boundary, not a model’s decision boundary. tances for adversarial robustness in NLP (Michel et al., 2019; Jia et al., 2019), we find it more useful in our setting to simply rely on expert judgments to generate a similar but meaningfully different x0 given x, addressing both the distance function and the sampling method. Future work could try to give formal treatments of these issues, but we believe expert judgments are sufficient to make initial progress in improving our evaluation methodologies. And while expertcrafted contrast sets can only give us an upper bound on a model’s local alignment with the true decision boundary, an upper bound on local alignment is often more informative than a pot"
2020.findings-emnlp.117,P19-1416,1,0.832263,"dissolute alcoholic. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap"
2020.findings-emnlp.117,C18-1198,0,0.264897,"nt work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that dataset authors manually perturb instances fro"
2020.findings-emnlp.117,D19-1642,1,0.874527,"Missing"
2020.findings-emnlp.117,P18-1122,1,0.846317,"and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ 339 70 RoBERTa 86.1 71.1 (–15.0) 59.0 MC-"
2020.findings-emnlp.117,P19-1459,0,0.0145999,"uestion: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be e"
2020.findings-emnlp.117,N18-1202,1,0.523856,"es from the different contrast sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model a"
2020.findings-emnlp.117,S18-2023,0,0.0681783,"Missing"
2020.findings-emnlp.117,P19-1621,1,0.860914,"Missing"
2020.findings-emnlp.117,P18-1079,1,0.679043,"of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that data"
2020.findings-emnlp.117,N18-2002,0,0.0515743,"Missing"
2020.findings-emnlp.117,E17-2060,0,0.0150328,"ds have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in system"
2020.findings-emnlp.117,2020.acl-main.468,0,0.0236124,"ing set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distributional biases in the data that was chosen for annotation, as well as numerous other biases that are more subtle and harder to discern (Shah et al., 2020). Completely removing these gaps in the initial data collection process would be ideal, but is likely impossible—language has too much inherent variability in a very high-dimensional space. Instead, we use contrast sets to fill in gaps in the test data to give more thorough evaluations than what the original data provides. 1309 2.2 Definitions We begin by defining a decision boundary as a partition of some space into labels.2 This partition can be represented by the set of all points in the space with their associated labels: {(x, y)}. This definition differs somewhat from the canonical defini"
2020.findings-emnlp.117,silveira-etal-2014-gold,0,0.0413118,"Missing"
2020.findings-emnlp.117,P19-1644,1,0.918073,"hanging questions asking for counts to questions asking for sets (How many countries. . . to Which countries. . . ). Finally, we changed the ordering of events. A large number of questions about war paragraphs ask which of two events happened first. We changed (1) the order the events were asked about in the question, (2) the order that the events showed up in the passage, and (3) the dates associated with each event to swap their temporal order. NLVR2 We next consider NLVR2, a dataset where a model is given a sentence about two provided images and must determine whether the sentence is true (Suhr et al., 2019). The data collection process encouraged highly compositional language, which was intended to require understanding the relationships between objects, properties of objects, and counting. We constructed NLVR2 contrast sets by modifying the sentence or replacing one of the images with freely-licensed images from web searches. For example, we might change The left image contains twice the number of dogs as the right image to The left image contains three times the number of dogs as the right image. Similarly, given an image pair with four dogs in the left and two dogs in the right, we can replac"
2020.findings-emnlp.117,D19-1608,1,0.822267,"esources that we have created, is giving a simple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various pheno"
2020.findings-emnlp.117,D18-1009,0,0.0260643,"is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that"
2020.findings-emnlp.117,P19-1472,0,0.0134417,"annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that model. 2.5 Limitations of Contrast Sets Solely Negative Predictive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot"
2020.findings-emnlp.225,P18-1033,0,0.073795,"output program. Nevertheless, prior work (FineganDollak et al., 2018; Herzig and Berant, 2019; Keysers et al., 2020) has shown that data splits that require generalizing to new program templates result in drastic loss of performance. However, past work did not investigate how different modeling choices interact with compositional generalization. In this paper, we thoroughly analyze the impact of different modeling choices on compositional generalization in 5 semantic parsing datasets—four that are text-to-SQL datasets, and DROP, a dataset for executing programs over text paragraphs. Following Finegan-Dollak et al. (2018), we examine performance on a compositional split, where target programs are partitioned into “program templates”, and templates appearing at test time are unobserved at training time. We examine the effect of standard practices, such as contextualized representations (§3.1) and grammar-based decoding (§3.2). Moreover, we propose novel extensions to decoder attention (§3.3), the component responsible for aligning sub-structures in the question and program: (a) supervising attention based on precomputed token alignments, (b) attending over constituency spans, and (c) encouraging the decoder 248"
2020.findings-emnlp.225,P19-1444,0,0.0236547,"trained from (x, z) pairs. Similar to Finegan-Dollak et al. (2018), our baseline semantic parser is a standard sequence-tosequence model (Dong and Lapata, 2016) that encodes the question x with a BiLSTM encoder (Hochreiter and Schmidhuber, 1997) over GloVe embeddings (Pennington et al., 2014), and decodes the program z token-by-token from left to right with an attention-based LSTM decoder (Bahdanau et al., 2015). 3.1 Contextualized Representations Pre-trained contextualized representations revolutionized natural language processing in recent years, and semantic parsing has been no exception (Guo et al., 2019; Wang et al., 2019). We hypothesize that better representations for question tokens should improve compositional generalization, because they reduce language variability and thus may help improve the mapping from input to output tokens. We evaluate the effect of using ELM O (Peters et al., 2018) and BERT (Devlin et al., 2019) to represent question tokens.1 3.2 Grammar-Based Decoding A unique property of semantic parsing, compared to other generation tasks, is that programs have a clear hierarchical structure that is based on the target formal language. Decoding the output program token-by-tok"
2020.findings-emnlp.225,D17-1160,1,0.829696,".1 3.2 Grammar-Based Decoding A unique property of semantic parsing, compared to other generation tasks, is that programs have a clear hierarchical structure that is based on the target formal language. Decoding the output program token-by-token from left to right (Dong and Lapata, 2016; Jia and Liang, 2016) can thus generate programs that are not syntactically valid, and the model must effectively learn the syntax of the target language at training time. Grammar-based decoding resolves this issue and has been shown to consistently improve in-distribution performance (Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). In grammar-based decoding, the decoder outputs the abstract syntax tree of the program based on a formal grammar of the target language. At each step, a production rule from the grammar is chosen, eventually outputting a topdown left-to-right linearization of the program tree. Because decoding is constrained by the grammar, the model outputs only valid programs. We refer the reader to the aforementioned papers for details on grammar-based decoding. Compositional generalization involves combining known sub-structures in novel ways. In grammar-based decoding, the structu"
2020.findings-emnlp.225,D17-1018,0,0.0244953,"rd phrase “do for a living” aligns to the KB relation Profession. Allowing the model to directly attend to multi-token phrases could induce more meaningful alignments that improve compositional generalization. Here, rather than computing an attention distribution over input tokens (x1 , . . . xn ), we compute a distribution over the set of spans corresponding to all constituents (including all tokens) as predicted by an off-the-shelf constituency parser (Joshi et al., 2018). Spans are represented using a self-attention mechanism over the hidden representations of the tokens in the span, as in Lee et al. (2017). (c) Coverage Questions at test time are sometimes similar to training questions, but include new information expressed by a few tokens. A model that memorizes a mapping from question templates to programs can ignore this new information, hampering compositional generalization. To encourage models to attend to the entire question, we add the attention-coverage mechanism from See et al. (2017) to our model. Specifically, at each decoding step the decoder holds a coverage vector c = (c1 , . . . , cn ), where ci corresponds to the sum of attention probabilities over xi in all previous time steps"
2020.tacl-1.13,P13-1023,0,0.0507346,"Missing"
2020.tacl-1.13,N19-1027,0,0.0194134,"the number of examples in the original dataset and in BREAK. Numbers of high-level QDMRs are denoted by high. Question Collection Questions in BREAK were randomly sampled from ten QA datasets over the following tasks (Table 3): • Semantic Parsing: Mapping natural language utterances into formal queries, to be executed on a target KB (Price, 1990; Zelle and Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018). • Reading Comprehension (RC): Questions that require understanding of a text passage by reasoning over multiple sentences (Talmor and Berant, 2018; Yang et al., 2018; Dua et al., 2019; Abujabal et al., 2019). • Visual Question Answering (VQA): Questions over images that require both visual and numerical reasoning skills (Johnson et al., 2017; Suhr et al., 2019). All questions collected were composed by human annotators.3 HOTPOTQA questions were all sampled from the hard split of the dataset. steps, where they are only allowed to use words from a lexicon Lx , which contains: (a) words appearing in the question (or their automatically computed inflections), (b) words from a small pre-defined list of 66 function word such as, ‘if’, ‘on’, ‘for each’, or (c) reference tokens that refer to the results"
2020.tacl-1.13,N16-1181,0,0.249915,"Missing"
2020.tacl-1.13,P15-1127,0,0.0288983,"l details in §7.2). COMBINED and BREAKRC were compared to COMBINEDR and BREAKRCR , which use the rulebased decompositions. We observe that QDMR lead to substantially higher performance when compared to the rule-based decompositions. 7 Issuing an IR query over each ‘‘content word’’ in the question, instead of each noun phrase, led to poor results. 192 Thus, QDMR can be viewed as an intermediate representation between a natural language question and an executable query. Such intermediate representations have already been discussed in prior work on semantic parsing. Kwiatkowski et al. (2013) and Choi et al. (2015) used underspecified logical forms as an intermediate representation. Guo et al. (2019) proposed a two-stage approach, separating between learning an intermediate text-to-SQL representation and the actual mapping to schema items. Works in the database community have particularly targeted the mapping of intermediate query representations into DB grounded queries, using schema mapping and join path inference (Androutsopoulos et al., 1995; Li et al., 2014; Baik et al., 2019). We argue that QDMR can be used as an easy-to-annotate representation in such semantic parsers, bridging between natural la"
2020.tacl-1.13,W10-2903,0,0.0397036,"models should improve performance and generalization in tasks that require multi-step reasoning or that do not have access to substantial amounts of data. In this work we propose question understanding as a standalone language understanding task. We introduce a formalism for representing the meaning of questions that relies on question decomposition, and is agnostic to the information source. Our formalism, Question Decomposition Meaning Representation (QDMR), is inspired by database query languages (SQL; SPARQL), and by semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al., 2010), in which questions are given full meaning representations. We express complex questions via simple (‘‘atomic’’) questions that can be executed in sequence to answer the original question. Each atomic question can be mapped into a small set of formal operations, where each operation either selects a set of entities, retrieves information about Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes th"
2020.tacl-1.13,H94-1010,0,0.464354,"Missing"
2020.tacl-1.13,N19-1423,0,0.0339406,"o-logical forms, which can be used as 190 5 QDMR for Open-domain QA A natural setup for QDMR is in answering complex questions that require multiple reasoning steps. We compare models that exploit question decompositions to baselines that do not. We use the open-domain QA (‘‘full-wiki"") setting of the HOTPOTQA dataset (Yang et al., 2018): Given a question, the QA model retrieves the relevant Wikipedia paragraphs and answers the question using these paragraphs. 5.1 Experimental Setup We compare BREAKRC, a model that utilizes question decomposition to BERTQA, a standard QA model, based on BERT (Devlin et al., 2019), and present COMBINED, an approach that enjoys the benefits of both models. BREAKRC Algorithm 1 describes the BREAKRC model, which uses high-level QDMR structures for answering open-domain multi-hop questions. We assume access to an Information Retrieval (IR) model and an RC model, and denote by ANSWER(·) a function that takes a question as input, runs the IR model to obtain paragraphs, and then feeds those paragraphs as context for an RC model that returns a distribution over answers. Given an input QDMR, s = hs1 , ..., sn i, iterate over s step-by-step and perform the following. First, we e"
2020.tacl-1.13,W13-2322,0,0.0713232,"x. Domain-independent intermediate representations for semantic parsers were proposed by Kwiatkowski et al. (2013) and Reddy et al. (2016). As there is no consensus on the ideal meaning representation for semantic parsing, representations are often chosen based on the particular execution setup: SQL is used for relational databases (Yu et al., 2018), SPARQL for graph KBs (Yih et al., 2016), while other ad-hoc languages are used based on the task at hand. We frame QDMR as an easy-to-annotate formalism that can be potentially converted to other representations, depending on the task. Last, AMR (Banarescu et al., 2013) is a meaning representation for sentences. Instead of representing general language, QDMR represents questions, which are important for QA systems, and for probing models for reasoning. questions from 10 datasets and 3 modalities (DB, images, text). We presented the utility of QDMR for both open-domain question answering and semantic parsing, and constructed a QDMR parser with reasonable performance. QDMR proposes a promising direction for modeling question understanding, which we believe will be useful for multiple tasks in which reasoning is probed through questions. Acknowledgments This wo"
2020.tacl-1.13,N19-1246,1,0.922364,"it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query lan"
2020.tacl-1.13,P17-1171,0,0.0297332,"the reference to the previous step in si with its already computed answer, and then run ANSWER(·). For FILTER steps,6 we use a simple rule to extract a ‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraph"
2020.tacl-1.13,N19-1405,0,0.0304319,"ems from the design of QDMR as a domain-agnostic meaning representation (§2). QDMR abstracts away from a concrete KB schema by assuming an underlying ‘‘idealized’’ KB, which contains all of its arguments. Figure 6: Examples of PROJECT and COMPARISON questions in HOTPOTQA (high-level QDMR). Model BERTQA BREAKRCP BREAKRCG PROJECT COMPARISON EM F1 IR EM F1 IR 22.8 31.0 31.6 42.9 51.7 75.8 25.4 33.7 52.9 34.7 50.4 68.9 32.2 41.9 59.8 44.5 57.6 78.0 Table 7: Results on PROJECT and COMPARISON questions from HOTPOTQA development set. to reasoning shortcuts, i.e. they necessitate multistep reasoning (Chen and Durrett, 2019; Jiang and Bansal, 2019; Min et al., 2019a). In Table 7 we report BREAKRC results on these question types, where it notably outperforms BERTQA. Ablations In BREAKRC, multiple IR queries are issued, one at each step. To examine whether these multiple queries were the cause for performance gains, we built IR-NP, a model that issues multiple IR queries, one for each noun phrase in the question. Similar to COMBINED, the question and union of retrieved paragraphs are given as input to BERTQA. We observe that COMBINED substantially outperforms IR-NP, indicating that the structure of QDMR, rather th"
2020.tacl-1.13,P16-1154,0,0.0275769,"authors generate sequences of simple questions which crowdworkers paraphrase into a compositional question. Questions in BREAK are composed by humans, and are then decomposed to QDMR. Table 8: The decomposition rules of RULEBASED. Rules are based on dependency labels, part-ofspeech tags and coreference edges. Text fragments used for decomposition are in boldface. • S2SDYNAMIC: SEQ2SEQ with a dynamic output vocabulary restricted to the closed set of tokens Lx available to crowd-workers (see §3). • COPYNET: SEQ2SEQ with an added copy mechanism that allows copying tokens from the input sequence (Gu et al., 2016). 7.3 Results Table 9 presents model performance on BREAK. Neural models outperform the RULEBASED baseline and perform reasonably well, with COPYNET obtaining the best scores across all metrics. This can be attributed to most of the tokens in a QDMR parse being copied from the original question. Semantic Formalism Annotation Labeling corpora with a semantic formalism has often been reserved for expert annotators (Dahl et al., 1994; Zelle and Mooney, 1996; Abend and Rappoport, 2013; Yu et al., 2018). Recent work has focused on cheaply eliciting quality annotations from nonexperts through crowds"
2020.tacl-1.13,P19-1262,0,0.0376279,"DMR as a domain-agnostic meaning representation (§2). QDMR abstracts away from a concrete KB schema by assuming an underlying ‘‘idealized’’ KB, which contains all of its arguments. Figure 6: Examples of PROJECT and COMPARISON questions in HOTPOTQA (high-level QDMR). Model BERTQA BREAKRCP BREAKRCG PROJECT COMPARISON EM F1 IR EM F1 IR 22.8 31.0 31.6 42.9 51.7 75.8 25.4 33.7 52.9 34.7 50.4 68.9 32.2 41.9 59.8 44.5 57.6 78.0 Table 7: Results on PROJECT and COMPARISON questions from HOTPOTQA development set. to reasoning shortcuts, i.e. they necessitate multistep reasoning (Chen and Durrett, 2019; Jiang and Bansal, 2019; Min et al., 2019a). In Table 7 we report BREAKRC results on these question types, where it notably outperforms BERTQA. Ablations In BREAKRC, multiple IR queries are issued, one at each step. To examine whether these multiple queries were the cause for performance gains, we built IR-NP, a model that issues multiple IR queries, one for each noun phrase in the question. Similar to COMBINED, the question and union of retrieved paragraphs are given as input to BERTQA. We observe that COMBINED substantially outperforms IR-NP, indicating that the structure of QDMR, rather than multiple IR queries,"
2020.tacl-1.13,P19-1444,0,0.0312823,"use the rulebased decompositions. We observe that QDMR lead to substantially higher performance when compared to the rule-based decompositions. 7 Issuing an IR query over each ‘‘content word’’ in the question, instead of each noun phrase, led to poor results. 192 Thus, QDMR can be viewed as an intermediate representation between a natural language question and an executable query. Such intermediate representations have already been discussed in prior work on semantic parsing. Kwiatkowski et al. (2013) and Choi et al. (2015) used underspecified logical forms as an intermediate representation. Guo et al. (2019) proposed a two-stage approach, separating between learning an intermediate text-to-SQL representation and the actual mapping to schema items. Works in the database community have particularly targeted the mapping of intermediate query representations into DB grounded queries, using schema mapping and join path inference (Androutsopoulos et al., 1995; Li et al., 2014; Baik et al., 2019). We argue that QDMR can be used as an easy-to-annotate representation in such semantic parsers, bridging between natural language and full logical forms. 7 QDMR Parsing We now present evaluation metrics and mod"
2020.tacl-1.13,D18-1239,0,0.0422473,"Missing"
2020.tacl-1.13,D13-1161,0,0.0328552,"ency tree of the question (full details in §7.2). COMBINED and BREAKRC were compared to COMBINEDR and BREAKRCR , which use the rulebased decompositions. We observe that QDMR lead to substantially higher performance when compared to the rule-based decompositions. 7 Issuing an IR query over each ‘‘content word’’ in the question, instead of each noun phrase, led to poor results. 192 Thus, QDMR can be viewed as an intermediate representation between a natural language question and an executable query. Such intermediate representations have already been discussed in prior work on semantic parsing. Kwiatkowski et al. (2013) and Choi et al. (2015) used underspecified logical forms as an intermediate representation. Guo et al. (2019) proposed a two-stage approach, separating between learning an intermediate text-to-SQL representation and the actual mapping to schema items. Works in the database community have particularly targeted the mapping of intermediate query representations into DB grounded queries, using schema mapping and join path inference (Androutsopoulos et al., 1995; Li et al., 2014; Baik et al., 2019). We argue that QDMR can be used as an easy-to-annotate representation in such semantic parsers, brid"
2020.tacl-1.13,Q19-1026,0,0.0790769,"Missing"
2020.tacl-1.13,D16-1258,0,0.062318,"Missing"
2020.tacl-1.13,P17-1089,0,0.0546298,"Missing"
2020.tacl-1.13,J13-2005,0,0.0690178,"Missing"
2020.tacl-1.13,N18-2089,0,0.0398159,"Missing"
2020.tacl-1.13,P17-1167,0,0.0532124,"Missing"
2020.tacl-1.13,P19-1416,1,0.939891,"ER steps,6 we use a simple rule to extract a ‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraphs are fed to a pretrained BERT-based RC model (Min et al., 2019b) to answer x. In contrast to BREAKRC, tha"
2020.tacl-1.13,P19-1613,0,0.228693,"ER steps,6 we use a simple rule to extract a ‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraphs are fed to a pretrained BERT-based RC model (Min et al., 2019b) to answer x. In contrast to BREAKRC, tha"
2020.tacl-1.13,Q18-1021,0,0.0375168,"nverted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query languages (Chamberlin and Boyce, 1"
2020.tacl-1.13,P15-1142,0,0.0707424,"n semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query languages (Chamberlin and Boyce, 1974), the same intuition can be applied to other modalities, such as images and"
2020.tacl-1.13,Q16-1029,0,0.0219436,"sition can be further decomposed. To measure such variations, we introduce two types of evaluation metrics. Sequence-based metrics treat the decomposition as a sequence of tokens, applying standard text generation metrics. As such metrics ignore the QDMR graph structure, we also use graph-based metrics that compare the predicted graph Gˆs to the gold QDMR graph Gs (see §2). Sequence-based scores, where higher values are better, are denoted by ⇑. Graph-based scores, where lower values are better, are denoted by ⇓. • Exact Match ⇑: Measures exact match between s and ˆs, either 0 or 1. • SARI ⇑ (Xu et al., 2016): SARI is commonly used in tasks such as text simplification. Given s, we consider the sets of added, deleted, and kept n-grams when mapping the question x to s. We compute these three sets for both s and ˆs using the standard of up to 4-grams, then average (a) the F1 for added n-grams between s and ˆs, (b) the F1 for kept n-grams, and (c) the precision for the deleted n-grams. cost. GED computes the minimal-cost graph edit path required for transitioning from Gs to Gˆs (and vice versa), normalized by max(|Gs |, |Gˆs |). Operation costs are 1 for insertion and deletion of nodes and edges. The"
2020.tacl-1.13,H90-1020,0,0.415169,"81 7,982 32,164 13,935 29,680 13,517 11,214 5,520 34,689 96,567 23,066 2,988, 2,991high 10,230, 10,262high 10,575high 83,978 Figure 4: User interface for decomposing a complex question that uses a closed lexicon of tokens. Table 3: The QA datasets in BREAK. Lists the number of examples in the original dataset and in BREAK. Numbers of high-level QDMRs are denoted by high. Question Collection Questions in BREAK were randomly sampled from ten QA datasets over the following tasks (Table 3): • Semantic Parsing: Mapping natural language utterances into formal queries, to be executed on a target KB (Price, 1990; Zelle and Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018). • Reading Comprehension (RC): Questions that require understanding of a text passage by reasoning over multiple sentences (Talmor and Berant, 2018; Yang et al., 2018; Dua et al., 2019; Abujabal et al., 2019). • Visual Question Answering (VQA): Questions over images that require both visual and numerical reasoning skills (Johnson et al., 2017; Suhr et al., 2019). All questions collected were composed by human annotators.3 HOTPOTQA questions were all sampled from the hard split of the dataset. steps, where they are only allowed t"
2020.tacl-1.13,D18-1259,0,0.13256,"Missing"
2020.tacl-1.13,D19-1261,0,0.0255197,"to BREAKRC, that is trained on SQUAD, BERTQA is trained on the target dataset (HOTPOTQA), giving it an advantage over BREAKRC. A COMBINED Approach Last, we present an approach that combines the strengths of BREAKRC and BERTQA. In this approach, we use the QDMR decomposition to improve retrieval only. Given a question x and its QDMR s, we run BREAKRC on s, but in addition to storing answers, we also store all the paragraphs retrieved by the IR model. We then run BERTQA on the question x and the top-10 paragraphs retrieved by BREAKRC, sorted by their IR ranking. This approach resembles that of Qi et al. (2019). The advantage of COMBINED is that we do not need to develop an answering procedure for each QDMR operator separately, which involves dif6 INTERSECTION steps are handled in a manner similar to FILTER, but we omit the exact description for brevity. 191 Model BERTQA BREAKRCP BREAKRCG COMBINEDP COMBINEDG IR-NP BREAKRCR COMBINEDR EM 33.6 28.8 34.6 38.3 41.2 31.7 18.9 32.7 HOTPOTQA F1 43.3 37.7 44.6 49.3 52.4 41.2 26.5 42.6 IR 46.3 52.5 59.2 52.5 59.2 40.8 40.3 40.3 Table 6: Open-domain QA results on HOTPOTQA. ferent discrete operations such as comparison and intersection. Instead, we use BREAKRC"
2020.tacl-1.13,P16-2033,0,0.0282538,"s = hs1 , . . . , sn i do 4: op ← OPTYPE(si ) 5: ref s ← REFERENCEDSTEPS(si ) 6: if op is SELECT then 7: ans ← ANSWER(si ) 8: else if op is FILTER then 9: sˆi ← EXTRACTQUESTION(si ) 10: anstmp ← ANSWER(ˆ si ) 11: ans ← INTERSECT(anstmp , ansrs[ref s[0]]) 12: else if op is COMPARISON then 13: ans ← COMPARESTEPS(ref s,s) 14: else ⊲ op is PROJECT 15: sˆi ← SUBSTITUTEREF (si , ansrs[ref s[0]]) 16: ans ← ANSWER(ˆ si ) 17: ansrs[i] ← ans 18: return ansrs[n] Figure 5: Examples and justifications of expert judgment on collected QDMRs in BREAK. a cheap intermediate representation for semantic parsers (Yih et al., 2016), further discussed in §6. QDMR operator (f i ) and its arguments from each step (si ). To infer these formal representations, we developed an algorithm that goes over the QDMR structure step-by-step, and for each step si , uses a set of predefined templates to identify f i and its arguments, expressed in si . This results in an execution graph (Figure 2), where the execution result of a parent node serves as input to its child. Figure 1 presents three QDMR decompositions along with the formal graphs output by our algorithm (lower box). Each node lists its operator (e.g., GROUP), its constant"
2020.tacl-1.13,D16-1264,0,0.0946606,"‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraphs are fed to a pretrained BERT-based RC model (Min et al., 2019b) to answer x. In contrast to BREAKRC, that is trained on SQUAD, BERTQA is trained on the targ"
2020.tacl-1.13,D18-1425,0,0.0326117,"567 23,066 2,988, 2,991high 10,230, 10,262high 10,575high 83,978 Figure 4: User interface for decomposing a complex question that uses a closed lexicon of tokens. Table 3: The QA datasets in BREAK. Lists the number of examples in the original dataset and in BREAK. Numbers of high-level QDMRs are denoted by high. Question Collection Questions in BREAK were randomly sampled from ten QA datasets over the following tasks (Table 3): • Semantic Parsing: Mapping natural language utterances into formal queries, to be executed on a target KB (Price, 1990; Zelle and Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018). • Reading Comprehension (RC): Questions that require understanding of a text passage by reasoning over multiple sentences (Talmor and Berant, 2018; Yang et al., 2018; Dua et al., 2019; Abujabal et al., 2019). • Visual Question Answering (VQA): Questions over images that require both visual and numerical reasoning skills (Johnson et al., 2017; Suhr et al., 2019). All questions collected were composed by human annotators.3 HOTPOTQA questions were all sampled from the hard split of the dataset. steps, where they are only allowed to use words from a lexicon Lx , which contains: (a) words appeari"
2020.tacl-1.13,Q16-1010,0,0.0640017,"Missing"
2020.tacl-1.13,N18-1059,1,0.945937,"SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query languages (Chamberlin and Boyce, 1974), the same intuition"
2020.tacl-1.13,P11-1060,0,\N,Missing
2020.tacl-1.13,W18-2501,1,\N,Missing
2020.tacl-1.13,P19-1644,0,\N,Missing
2020.tacl-1.48,P17-1152,0,0.0900964,"Missing"
2020.tacl-1.48,N19-1423,0,0.108827,"Missing"
2020.tacl-1.48,Q19-1030,1,0.843319,"Missing"
2020.tacl-1.48,P19-1388,1,0.88036,"Missing"
2020.tacl-1.48,D19-1275,0,0.0713234,"one verify whether pretrained representations hold information that is useful for a particular task? Past work mostly resorted to fixing the representations and finetuning a simple, often linear, randomly initialized probe, to determine whether the representations hold relevant information (Ettinger et al., 2016; Adi et al., 2016; Belinkov and Glass, 2019; Hewitt and Manning, 2019; Wallace et al., 2019; Rozen et al., 2019; Peters et al., 2018b; Warstadt et al., 2019). However, it is difficult to determine whether success is due to the pre-trained representations or due to fine-tuning itself (Hewitt and Liang, 2019). To handle this challenge, we include multiple controls that improve our understanding of the results. Our ‘‘purest’’ setup is zero-shot: We cast tasks in the masked LM format, and use a pre-trained LM without any fine-tuning. For example, given Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such"
2020.tacl-1.48,Q16-1037,1,0.865635,"Missing"
2020.tacl-1.48,2020.acl-main.441,0,0.0620945,"ins FOODTYPE.’’ the LM always predicts ‘‘sometimes’’. Overall, we find models do not perform well. Reporting bias (Gordon and Van Durme, 2013) may play a role in the inability to correctly determine that ‘‘A rhinoceros NEVER has fur.’’ Interestingly, behavioral research conducted on blind humans shows they exhibit a similar bias (Kim et al., 2019). 4.3 Do LMs Capture Negation? Ideally, the presence of the word ‘‘not’’ should affect the prediction of a masked token. However, Several recent works have shown that LMs do not take into account the presence of negation in sentences (Ettinger, 2019; Nie et al., 2020; Kassner and Sch¨utze, 2020). Here, we add to this literature, by probing whether LMs can properly use negation in the context of synonyms vs. antonyms. 4.4 Can LMs handle conjunctions of facts? We present two probes where a model should understand the reasoning expressed by the word and. Property conjunction CONCEPTNET is a Knowledge-Base that describes the properties of millions of concepts through its (subject, 751 Model RoBERTa-L BERT-WWM BERT-L BERT-B RoBERTa-B Baseline LEARNCURVE LANGSENSE WS MAX pert nolang 49 46 48 47 40 39 87 80 75 71 57 49 2 0 2 2 0 0 4 1 5 1 0 0 Table 8: Results fo"
2020.tacl-1.48,K19-1019,0,0.0497465,"g the sizes of different objects. Understanding what is missing from current LMs may help design datasets and objectives that will endow models with the missing capabilities. However, how does one verify whether pretrained representations hold information that is useful for a particular task? Past work mostly resorted to fixing the representations and finetuning a simple, often linear, randomly initialized probe, to determine whether the representations hold relevant information (Ettinger et al., 2016; Adi et al., 2016; Belinkov and Glass, 2019; Hewitt and Manning, 2019; Wallace et al., 2019; Rozen et al., 2019; Peters et al., 2018b; Warstadt et al., 2019). However, it is difficult to determine whether success is due to the pre-trained representations or due to fine-tuning itself (Hewitt and Liang, 2019). To handle this challenge, we include multiple controls that improve our understanding of the results. Our ‘‘purest’’ setup is zero-shot: We cast tasks in the masked LM format, and use a pre-trained LM without any fine-tuning. For example, given Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to unde"
2020.tacl-1.48,N19-1421,1,0.880809,"M) and multichoice question answering (MC-QA). The default setup is MC-MLM, used for tasks where the answer set is small, consistent across the different questions, and each answer appears as a single item in the word-piece vocabulary.2 The MC-QA setup is used when the answer set substantially varies between questions, and many of the answers have more than one word piece. MC-QA Constructing a MC-MLM probe limits the answer candidates to a single token from the word-piece vocabulary. To relax this we use in two tasks the standard setup for answering multichoice questions with pre-trained LMs (Talmor et al., 2019; Mihaylov et al., 2018). Given a question q and candidate answers a1 , . . . , aK , we compute for each candidate answer ak representations h(k) from the input tokens ‘‘[CLS] q [SEP] ak [SEP]’’. Then the probability over answers is obtained using the multichoice QA head: 2 Vocabularies of LMs such as BERT and ROBERTA contain word-pieces, which are sub-word units that are frequent in the training corpus. For details see Sennrich et al. (2016). (k ) l(k) = F FQA (h1 ), p = softmax(l(1) , . . . , l(K ) ), 745 where F FQA is a 1-hidden layer MLP that is run over the [CLS] (first) token of an answ"
2020.tacl-1.48,P19-1452,0,0.0785555,"ion and Learning-curve Metrics Learning curves are informative, but inspecting many learning curves can be difficult. Thus, we summarize them using two aggregate statistics. We report: (a) MAX, that is, the maximal accuracy on the learning curve, used to estimate how well the model can handle the task given the limited amount of examples. (b) The metric WS, which is a weighted average of accuracies across the learning curve, where higher weights are given to points where N is small.3 WS is related to the area under the accuracy curve, and to the online code metric, proposed by Yogatama et al. (2019) and Blier and Ollivier (2018). The linearly decreasing weights emphasizes our focus on performance given little training data, as it highlights what was encoded by the model before fine-tuning. 3 We use the decreasing weights W = (0.23, 0.2, 0.17, 0.14, 0.11, 0.08, 0.07). 747 For AGE-COMPARE, the solid lines in Figure 2B illustrate the learning curves of ROBERTA-L and BERT-WWM, and Table 2 shows the aggregate statistics. We fine-tune the model by replacing AGE-1 and AGE-2 with values between 43 and 120, but test with values between 15 and 38, to guarantee that the model generalizes to values"
2021.acl-long.239,2020.tacl-1.30,0,0.0256716,"uate for question answering. Overall, our results highlight the importance of designing objectives and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concatenated to the context. The [QUESTION] token’s representation is then used to sele"
2021.acl-long.239,D19-1606,0,0.0380564,"Missing"
2021.acl-long.239,N19-1423,0,0.456229,"n SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting.1 1 80.0 F1 60.0 20.0 0.0 16 64 128 256 Figure 1: Performance of SpanBERT (red) and RoBERTa (yellow) base-size models on SQuAD, given different amounts of training examples. Our model (Splinter, green) dramatically improves performance. SpanBERT-base trained on the full training data of SQuAD (blue, dashed) is shown for reference. The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019). While this approach is effective, and sometimes exceeds human performance, its success is based on the assumption that large quantities of annotated question answering examples are available. For instance, both SQuAD (Rajpurkar et al., 2016, 2018) and Natural Questions (Kwiatkowski et al., 2019) contain an order of 100,000 question and ∗ 32 # Examples Introduction Equal contribution. Our code, models, and datasets are publicly available: https://github.com/oriram/splinter. 1 40.0 answer pairs in their training data. This assumption quickly becomes unrea"
2021.acl-long.239,N19-1246,0,0.0414153,"Missing"
2021.acl-long.239,D19-5801,0,0.521048,"tation to select the answer span. The QASS layer seamlessly integrates with fine-tuning on real question-answer pairs. We simply append the [QUESTION] token to the input question, and use the QASS layer to select the answer span (Figure 3). This is unlike existing models for span selection, which do not include an explicit question representation. The compatibility between pretraining and fine-tuning makes Splinter an effective few-shot learner. Splinter exhibits surprisingly high performance given only a few training examples throughout a variety of benchmarks from the MRQA 2019 shared task (Fisch et al., 2019). For example, Splinter-base achieves 72.7 F1 on SQuAD with only 128 examples, outperforming all baselines by a very wide margin. An ablation study shows that the pretraining method and the QASS layer itself (even without pretraining) both contribute to improved performance. Analysis indicates that Splinter’s representations change significantly less during fine-tuning compared to the baselines, suggesting that our pretraining is more adequate for question answering. Overall, our results highlight the importance of designing objectives and architectures in the few-shot setting, where an approp"
2021.acl-long.239,2021.acl-long.295,0,0.066632,"), while the other models’ representations seem to change drastically. This suggests that fine-tuning with even 128 questionanswering examples makes significant modifications to the functionality of pretrained masked language models. Splinter’s pretraining, on the other hand, is much more similar to the fine-tuning task, resulting in much more modest changes to the produced vector representations. 7 Related Work The remarkable results of GPT-3 (Brown et al., 2020) have inspired a renewed interest in few-shot learning. While some work focuses on classification tasks (Schick and Sch¨utze, 2020; Gao et al., 2021), our work investigates few-shot learning in the context of extractive question answering. One approach to this problem is to create synthetic text-question-answer examples. Both Lewis et al. (2019) and Glass et al. (2020) use the traditional NLP pipeline to select noun phrases and named entities in Wikipedia paragraphs as potential answers, which are then masked from the context to create pseudo-questions. Lewis et al. (2019) use methods from unsupervised machine translation to translate the pseudo-questions into real ones, while Glass et al. (2020) keep the pseudo-questions but use informati"
2021.acl-long.239,2020.acl-main.247,0,0.127054,"guage models. Splinter’s pretraining, on the other hand, is much more similar to the fine-tuning task, resulting in much more modest changes to the produced vector representations. 7 Related Work The remarkable results of GPT-3 (Brown et al., 2020) have inspired a renewed interest in few-shot learning. While some work focuses on classification tasks (Schick and Sch¨utze, 2020; Gao et al., 2021), our work investigates few-shot learning in the context of extractive question answering. One approach to this problem is to create synthetic text-question-answer examples. Both Lewis et al. (2019) and Glass et al. (2020) use the traditional NLP pipeline to select noun phrases and named entities in Wikipedia paragraphs as potential answers, which are then masked from the context to create pseudo-questions. Lewis et al. (2019) use methods from unsupervised machine translation to translate the pseudo-questions into real ones, while Glass et al. (2020) keep the pseudo-questions but use information retrieval to find new text passages that can answer them. Both works assume access to language- and domain-specific NLP tools such as part-of-speech taggers, syntactic parsers, and named-entity recognizers, which might"
2021.acl-long.239,2020.tacl-1.5,1,0.93661,"training examples), while maintaining competitive performance in the high-resource setting.1 1 80.0 F1 60.0 20.0 0.0 16 64 128 256 Figure 1: Performance of SpanBERT (red) and RoBERTa (yellow) base-size models on SQuAD, given different amounts of training examples. Our model (Splinter, green) dramatically improves performance. SpanBERT-base trained on the full training data of SQuAD (blue, dashed) is shown for reference. The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019). While this approach is effective, and sometimes exceeds human performance, its success is based on the assumption that large quantities of annotated question answering examples are available. For instance, both SQuAD (Rajpurkar et al., 2016, 2018) and Natural Questions (Kwiatkowski et al., 2019) contain an order of 100,000 question and ∗ 32 # Examples Introduction Equal contribution. Our code, models, and datasets are publicly available: https://github.com/oriram/splinter. 1 40.0 answer pairs in their training data. This assumption quickly becomes unrealistic as we venture"
2021.acl-long.239,P17-1147,0,0.0559974,"ntext. 4 A Few-Shot QA Benchmark To evaluate how pretrained models work when only a small amount of labeled data is available for finetuning, we simulate various low-data scenarios by sampling subsets of training examples from larger datasets. We use a subset of the MRQA 2019 shared task (Fisch et al., 2019), which contains extractive question answering datasets in a unified format, where the answer is a single span in the given text passage. Split I of the MRQA shared task contains 6 large question answering datasets: SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). For each dataset, we sample smaller training datasets from the original training set with sizes changing on a 3069 logarithmic scale, from 16 to 1,024 examples. To reduce variance, for each training set size, we sample 5 training sets using different random seeds and report average performance across training sets. We also experiment with fine-tuning the models on the full training sets. Since Split I of the MRQA shared task does not contain test sets, we evaluate using the official"
2021.acl-long.239,D19-1439,0,0.130029,"producing contextualized to2 In practice, only some sets of recurring spans are processed; see Cluster Selection below. 3068 ken representations. The model is then tasked with predicting the start and end positions of the answer given each [QUESTION] token representation. In Figure 2b, we observe four instances of this prediction task: two for the “Roosevelt” cluster, one for the “Allied countries” cluster, and one for “Declaration by United Nations”. Taking advantage of recurring words in a passage (restricted to nouns or named entities) was proposed in past work as a signal for coreference (Kocijan et al., 2019; Ye et al., 2020). We further discuss this connection in Section 7. Span Filtering To focus pretraining on semantically meaningful spans, we use the following definition for “spans”, which filters out recurring spans that are likely to be uninformative: (1) spans must begin and end at word boundaries, (2) we consider only maximal recurring spans, (3) spans containing only stop words are ignored, (4) spans are limited to a maximum of 10 tokens. These simple heuristic filters do not require a model, as opposed to masking schemes in related work (Glass et al., 2020; Ye et al., 2020; Guu et al.,"
2021.acl-long.239,Q19-1026,0,0.26222,"roves performance. SpanBERT-base trained on the full training data of SQuAD (blue, dashed) is shown for reference. The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019). While this approach is effective, and sometimes exceeds human performance, its success is based on the assumption that large quantities of annotated question answering examples are available. For instance, both SQuAD (Rajpurkar et al., 2016, 2018) and Natural Questions (Kwiatkowski et al., 2019) contain an order of 100,000 question and ∗ 32 # Examples Introduction Equal contribution. Our code, models, and datasets are publicly available: https://github.com/oriram/splinter. 1 40.0 answer pairs in their training data. This assumption quickly becomes unrealistic as we venture outside the lab conditions of English Wikipedia, and attempt to crowdsource question-answer pairs in other languages or domains of expertise (Tsatsaronis et al., 2015; Kembhavi et al., 2017). How do question answering models fare in the more practical case, where an in-house annotation effort can only produce a cou"
2021.acl-long.239,K17-1034,1,0.842402,"esigning objectives and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concatenated to the context. The [QUESTION] token’s representation is then used to select the answer span. such as BERT (Devlin et al., 2019), and adds two paramete"
2021.acl-long.239,P19-1484,0,0.0615571,"of pretrained masked language models. Splinter’s pretraining, on the other hand, is much more similar to the fine-tuning task, resulting in much more modest changes to the produced vector representations. 7 Related Work The remarkable results of GPT-3 (Brown et al., 2020) have inspired a renewed interest in few-shot learning. While some work focuses on classification tasks (Schick and Sch¨utze, 2020; Gao et al., 2021), our work investigates few-shot learning in the context of extractive question answering. One approach to this problem is to create synthetic text-question-answer examples. Both Lewis et al. (2019) and Glass et al. (2020) use the traditional NLP pipeline to select noun phrases and named entities in Wikipedia paragraphs as potential answers, which are then masked from the context to create pseudo-questions. Lewis et al. (2019) use methods from unsupervised machine translation to translate the pseudo-questions into real ones, while Glass et al. (2020) keep the pseudo-questions but use information retrieval to find new text passages that can answer them. Both works assume access to language- and domain-specific NLP tools such as part-of-speech taggers, syntactic parsers, and named-entity r"
2021.acl-long.239,2020.acl-main.653,0,0.0220653,"raining is more adequate for question answering. Overall, our results highlight the importance of designing objectives and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concatenated to the context. The [QUESTION] token’s representation"
2021.acl-long.239,P18-2124,0,0.0297517,"and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concatenated to the context. The [QUESTION] token’s representation is then used to select the answer span. such as BERT (Devlin et al., 2019), and adds two parameter vectors s, e to the pre"
2021.acl-long.239,D16-1264,0,0.810559,"xamples. Our model (Splinter, green) dramatically improves performance. SpanBERT-base trained on the full training data of SQuAD (blue, dashed) is shown for reference. The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019). While this approach is effective, and sometimes exceeds human performance, its success is based on the assumption that large quantities of annotated question answering examples are available. For instance, both SQuAD (Rajpurkar et al., 2016, 2018) and Natural Questions (Kwiatkowski et al., 2019) contain an order of 100,000 question and ∗ 32 # Examples Introduction Equal contribution. Our code, models, and datasets are publicly available: https://github.com/oriram/splinter. 1 40.0 answer pairs in their training data. This assumption quickly becomes unrealistic as we venture outside the lab conditions of English Wikipedia, and attempt to crowdsource question-answer pairs in other languages or domains of expertise (Tsatsaronis et al., 2015; Kembhavi et al., 2017). How do question answering models fare in the more practical case, wh"
2021.acl-long.239,W17-2623,0,0.450597,"ine-tuning compared to the baselines, suggesting that our pretraining is more adequate for question answering. Overall, our results highlight the importance of designing objectives and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concaten"
2021.acl-long.74,D13-1160,1,0.6734,"as [matte [block ?]], as in the figure, or [[matte block] ?]. This phenomena is common in CLEVR and CLOSURE, as span trees tend to be deep, and thus have more ambiguity. 4.5 Limitations Our approach assumes a one-to-one mapping between domain constants and their manifestation as phrases in language. This leads to strong results on compositional generalization, but hurts the flexibility that is sometimes necessary in semantic parsing. For example, in some cases predicates do not align explicitly to a phrase in the utterance or appear several times in the program but only once in the utterance (Berant et al., 2013; Pasupat and Liang, 2015). This is evident in text-to-SQL parsing, where an utterance such as “What is the minimum, and maximum age of all singers from France?” is mapped to SELECT min(age) , max(age) FROM singer WHERE country=’France’. Here, the constant age is mentioned only once in language (but twice in the program), and country is not mentioned at all. Thus, our approach is more suitable for formalisms where there is tighter alignment between the natural and formal language. In addition, while we handle a class of nonprojective trees (§3.4), there are other nonprojective structures that"
2021.acl-long.74,2020.emnlp-main.219,0,0.0877479,"al., 2017). 3.4 Non-Projective Trees Our span-based parser assumes composition can only be done for adjacent spans that form together a contiguous span. However, this assumption does not always hold (Liang et al., 2011). For example, in Figure 3, while the predicate pop_1 should combine with the predicate state, the spans they align to (“people” and “state” respectively) are not contiguous, as they are separated by “most”, which contributes the semantics of a superlative. In constituency parsing, such non-projective structures are treated by adding rules to the grammar G (Maier et al., 2012; Corro, 2020; Stanojevi´c and Steedman, 2020). We identify one specific class of non-projective structures that is frequent in semantic parsing (Figure 3), and expand the grammar G and the CKY Algorithm to support this structure. Specifically, we add the ternary grammar rule join := join join join. During CKY, when calculating the top-K trees for spans (i, j) (line 6 in Alg. 1), we also consider the following top-K scores for the non-terminal join: max [s’(xij , join) + π(i, s1 , join) s1 ∈i...(j−2) s2 ∈(s1 +1)...(j−1) + π(s1 + 1, s2 , join) + π(s2 + 1, j, join)]. These additional trees are created by goi"
2021.acl-long.74,N19-1423,0,0.0207791,"and training procedure of our model (S PAN BASED SP), assuming we are given for every utterance x a gold tree T , for which program(T) = z. Similar to Pasupat et al. (2019), we minimize the negative log-likelihood − log p(T ) (Eq. 1) for the gold tree T . The loss decomposes over spans into cross-entropy terms for every span (i, j). This effectively results in multi-class problem, where for every span xi:j we predict a category c ∈ C. Training in this setup is trivial and does not require any structured inference. Concretely, the architecture of S PAN BASED SP is based on a BERT-base encoder (Devlin et al., 2019) that yields a contextual representation hi ∈ Rhdim for each token xi in the input utterance. We represent each span (i, j) by concatenating its start and end representations [hi ; hj ], and apply a 1hidden layer network to produce a real-valued score s(xi:j , c) for a span (i, j) and category c: A Span-based Semantic Parser s(xi:j , c) = [W2 relu(W1 [hi ; hj ])]ind(c) , Span-based parsing had success in both syntactic (Stern et al., 2017; Kitaev and Klein, 2018) and semantic parsing (Pasupat et al., 2019). The intuition is that modern sequence encoders are powerful, and thus we can predict a"
2021.acl-long.74,P16-1004,0,0.44765,"ge accuracy. Pasupat et al., 2019), equipped with the advantages of modern neural architectures. Our model, S PAN 1 Introduction BASED SP, predicts for every span in the input a category, which is either a constant from the unThe most dominant approach in recent years for semantic parsing, the task of mapping a natural lan- derlying knowledge-base, a composition category, or a null category. Given the category predictions guage utterance to an executable program, has been for all spans, we can construct a tree over the input based on sequence-to-sequence (seq2seq) models (Jia and Liang, 2016; Dong and Lapata, 2016; utterance and deterministically compute the output Wang et al., 2020, inter alia). In these models, program. For example, in Figure 1, the category for the output program is decoded step-by-step (au- the tree node covering the span “New York borders ?” is the composition category join, indicating toregressively), using an attention mechanism that the composition of the predicate next_to_1 with softly ties output tokens to the utterance. Despite the success of seq2seq models, recently, the entity stateid(’new york’). Finegan-Dollak et al. (2018) and Keysers et al. Categories are predicted for"
2021.acl-long.74,P18-1033,0,0.611517,"nce-to-sequence (seq2seq) models (Jia and Liang, 2016; Dong and Lapata, 2016; utterance and deterministically compute the output Wang et al., 2020, inter alia). In these models, program. For example, in Figure 1, the category for the output program is decoded step-by-step (au- the tree node covering the span “New York borders ?” is the composition category join, indicating toregressively), using an attention mechanism that the composition of the predicate next_to_1 with softly ties output tokens to the utterance. Despite the success of seq2seq models, recently, the entity stateid(’new york’). Finegan-Dollak et al. (2018) and Keysers et al. Categories are predicted for each span indepen(2020) and Herzig and Berant (2019) demonstrated dently, resulting in a very simple training procedure. that such models fail at compositional generaliza- CKY is used at inference time to find the best span tion, that is, they do not generalize to program tree, which is a tree with a category predicted at structures that were not seen at training time. For every node. The output program is computed from 908 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Co"
2021.acl-long.74,2020.acl-main.398,1,0.844473,"100 100 - 100 100 - 100 100 - 97.0 99.4 - 96.7 99.3 - 98.9 98.5 - 98.8 88.6 - 88.3 88.3 85.0 86.1 78.9 80.0 93.3 86.7 90.0 82.2 65.9 80.2 95.0 90.0 93.3 63.6 41.4 59.3 100 100 100 100 100 100 100 96.8 100 96.7 91.2 86.4 100 81.8 96.7 68.6 E ND 2E ND S PAN BASED SP -lexicon -non projective +gold trees Table 2: Denotation accuracies for all models, including S PAN BASED SP ablations. For both CLEVR splits, S PAN BASED SP only trains on 10K examples, in comparison to 695K for the baselines. for development. 4.2 the context can be an image (Hudson and Manning, 2018; Perez et al., 2018), a table (Herzig et al., 2020), etc. Because CLEVR and CLOSURE have a closed set of 28 possible answers and a short context (the scene), they are a good fit for end-to-end approaches. To check whether end-to-end models generalize compositionally, we implement the following model. We use BERT-base to encode the concatenation of the input x to a representation of all objects in the scene. Each scene object is represented by adding learned embeddings of all of its attributes: shape, material, size, color, and relative positional rank (from left to right, and from front to back). We fine-tune the model on the training set usin"
2021.acl-long.74,P16-1002,0,0.321501,"17; 61.0 → 88.9 average accuracy. Pasupat et al., 2019), equipped with the advantages of modern neural architectures. Our model, S PAN 1 Introduction BASED SP, predicts for every span in the input a category, which is either a constant from the unThe most dominant approach in recent years for semantic parsing, the task of mapping a natural lan- derlying knowledge-base, a composition category, or a null category. Given the category predictions guage utterance to an executable program, has been for all spans, we can construct a tree over the input based on sequence-to-sequence (seq2seq) models (Jia and Liang, 2016; Dong and Lapata, 2016; utterance and deterministically compute the output Wang et al., 2020, inter alia). In these models, program. For example, in Figure 1, the category for the output program is decoded step-by-step (au- the tree node covering the span “New York borders ?” is the composition category join, indicating toregressively), using an attention mechanism that the composition of the predicate next_to_1 with softly ties output tokens to the utterance. Despite the success of seq2seq models, recently, the entity stateid(’new york’). Finegan-Dollak et al. (2018) and Keysers et al. Categ"
2021.acl-long.74,W18-2501,0,0.0244416,"Missing"
2021.acl-long.74,P16-1154,0,0.0314411,"to right, and from front to back). We fine-tune the model on the training set using cross-entropy loss, where the [CLS] token is used to predict the answer. Baselines S EQ 2S EQ Similar to Finegan-Dollak et al. (2018), our baseline parser is a standard seq2seq model (Jia and Liang, 2016) that encodes the utterance x with a BiLSTM encoder over pre-trained GloVe (Pennington et al., 2014) or ELM O (Peters et al., 2018) embeddings, and decodes the program with an attention-based LSTM decoder (Bahdanau et al., 2015) assisted by a copying mechanism for handling entities unseen during training time (Gu et al., 2016). BERT2S EQ Same as S EQ 2S EQ, but we replace the BiLSTM encoder with BERT-base, which is identical to the encoder of S PAN BASED SP. GRAMMAR Grammar-based decoding has been shown to improve performance on IID splits (Krishnamurthy et al., 2017; Yin and Neubig, 2017). Because decoding is constrained by the grammar, the model outputs only valid programs, which could potentially improve performance on compositional splits. We use the grammar from (Wong and Mooney, 2007) for G EO Q UERY, and write grammars for SCAN-SP and CLEVR + CLOSURE. The model architecture is identical to S EQ 2S EQ. BART W"
2021.acl-long.74,D18-1239,0,0.0178782,"er than trees. More importantly, we show that this approach leads to dramatic gains in compositional generalization compared to autoregressive parsers. In recent years, work on compositional generalization in semantic parsing mainly focused on the poor performance of parsers in compositional splits (Finegan-Dollak et al., 2018), creating new datasets that require compositional generalization (Keysers et al., 2020; Lake and Baroni, 2018; Bahdanau et al., 2019), and proposing specialized architectures mainly for the SCAN task (Lake, 2019; Nye et al., 2020; Gordon et al., 2020; Liu et al., 2020; Gupta and Lewis, 2018). In this work we present a general-purpose architecture for semantic parsing that incorporates an inductive bias towards compositional generalization. Finally, concurrently to us, Shaw et al. (2020) induced a synchronous grammar over program and utterance pairs and used it to introduce a compositional bias, showing certain improvements over compositional splits. 6 We thank Ben Bogin, Nitish Gupta, Matt Gardner and the anonymous reviewers for their constructive feedback, useful comments and suggestions. This work was completed in partial fulfillment for the PhD degree of the first author, whic"
2021.acl-long.74,P17-1097,0,0.0141066,"ring spans (i, s) and (s, j) only if their sub-programs zi:s and zs:j can compose according to z. For instance, in Figure 1, a span with the sub-program capital can only compose with a span with the sub-program loc_2(·). After running this constrained CKY procedure we return the highest scoring tree that yields the correct program, ∗ , if one is found. We then treat the span strucTtrain ∗ ture of Ttrain as labels for training the parameters of S PAN BASED SP. Past work on weakly-supervised semantic parsing often used maximum marginal likelihood, especially when training from denotations only (Guu et al., 2017). In this work, we found hard-EM to be simple and sufficient, since we are given the program z that provides a rich signal for guiding search in the space of latent trees. largest_one join: pop_1 join: state seq2seq models, for predicting entities unseen during training. In the second lexicon we manually add no more than two examples of language phrases for each constant in Σ. E.g., for the predicate next_to_1, we update the lexicon to include lexicon[“border”] = lexicon[“borders”] = next_to_1. This requires minimal manual work (if no language phrases are available), but is done only once, and"
2021.acl-long.74,D19-1394,1,0.829096,"ally compute the output Wang et al., 2020, inter alia). In these models, program. For example, in Figure 1, the category for the output program is decoded step-by-step (au- the tree node covering the span “New York borders ?” is the composition category join, indicating toregressively), using an attention mechanism that the composition of the predicate next_to_1 with softly ties output tokens to the utterance. Despite the success of seq2seq models, recently, the entity stateid(’new york’). Finegan-Dollak et al. (2018) and Keysers et al. Categories are predicted for each span indepen(2020) and Herzig and Berant (2019) demonstrated dently, resulting in a very simple training procedure. that such models fail at compositional generaliza- CKY is used at inference time to find the best span tion, that is, they do not generalize to program tree, which is a tree with a category predicted at structures that were not seen at training time. For every node. The output program is computed from 908 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 908–921 August 1–6, 2021. ©2021 Association for Comput"
2021.acl-long.74,P18-1249,0,0.0186326,"trivial and does not require any structured inference. Concretely, the architecture of S PAN BASED SP is based on a BERT-base encoder (Devlin et al., 2019) that yields a contextual representation hi ∈ Rhdim for each token xi in the input utterance. We represent each span (i, j) by concatenating its start and end representations [hi ; hj ], and apply a 1hidden layer network to produce a real-valued score s(xi:j , c) for a span (i, j) and category c: A Span-based Semantic Parser s(xi:j , c) = [W2 relu(W1 [hi ; hj ])]ind(c) , Span-based parsing had success in both syntactic (Stern et al., 2017; Kitaev and Klein, 2018) and semantic parsing (Pasupat et al., 2019). The intuition is that modern sequence encoders are powerful, and thus we can predict a category for every span independently, reducing the role of global structure. This leads to simple and fast training. Specifically, our parser is based on a model pθ (T [i, j] = c), parameterized by θ, that provides for every span (i, j) a distribution over categories c ∈ C. Due to the above independence assumption, the log-likelihood of a tree T is defined as: X log p(T ) = log pθ (T [i, j]), (1) i&lt;j where, similar to Pasupat et al. (2019), the sum is over all s"
2021.acl-long.74,D17-1160,0,0.0133852,"ser is a standard seq2seq model (Jia and Liang, 2016) that encodes the utterance x with a BiLSTM encoder over pre-trained GloVe (Pennington et al., 2014) or ELM O (Peters et al., 2018) embeddings, and decodes the program with an attention-based LSTM decoder (Bahdanau et al., 2015) assisted by a copying mechanism for handling entities unseen during training time (Gu et al., 2016). BERT2S EQ Same as S EQ 2S EQ, but we replace the BiLSTM encoder with BERT-base, which is identical to the encoder of S PAN BASED SP. GRAMMAR Grammar-based decoding has been shown to improve performance on IID splits (Krishnamurthy et al., 2017; Yin and Neubig, 2017). Because decoding is constrained by the grammar, the model outputs only valid programs, which could potentially improve performance on compositional splits. We use the grammar from (Wong and Mooney, 2007) for G EO Q UERY, and write grammars for SCAN-SP and CLEVR + CLOSURE. The model architecture is identical to S EQ 2S EQ. BART We additionally experiment with BART-base (Lewis et al., 2020), a seq2seq model pre-trained as a denoising autoencoder. E ND 2E ND Semantic parsers generate a program that is executed to retrieve an answer. However, other end-to-end models direct"
2021.acl-long.74,2020.acl-main.703,0,0.0263675,"lace the BiLSTM encoder with BERT-base, which is identical to the encoder of S PAN BASED SP. GRAMMAR Grammar-based decoding has been shown to improve performance on IID splits (Krishnamurthy et al., 2017; Yin and Neubig, 2017). Because decoding is constrained by the grammar, the model outputs only valid programs, which could potentially improve performance on compositional splits. We use the grammar from (Wong and Mooney, 2007) for G EO Q UERY, and write grammars for SCAN-SP and CLEVR + CLOSURE. The model architecture is identical to S EQ 2S EQ. BART We additionally experiment with BART-base (Lewis et al., 2020), a seq2seq model pre-trained as a denoising autoencoder. E ND 2E ND Semantic parsers generate a program that is executed to retrieve an answer. However, other end-to-end models directly predict the answer from the context without an executor, where 4.3 Main Results Table 2 shows denotation accuracies for all baselines (top part) and our S PAN BASED SP model (middle part). For S PAN BASED SP, We also ablate the use of the manually constructed lexicon (§3.3) and the non-projective extension to CKY (§3.4), which is relevant only for G EO Q UERY, where nonprojective structures are more frequent."
2021.acl-long.74,D19-1163,0,0.324724,"ional generalization (compositional splits), and is posit that a span-based parser should lead to in contrast to the generalization abilities of humans better compositional generalization. we pro(Fodor and Pylyshyn, 1988). pose S PAN BASED SP, a parser that predicts In this work, we posit that the poor generalizaa span tree over an input utterance, expliction of seq2seq models is due to fact that the input itly encoding how partial programs compose utterance and output program are only tied softly over spans in the input. S PAN BASED SP exthrough attention. We revisit a more traditional tends Pasupat et al. (2019) to be comparable to seq2seq models by (i) training from proapproach for semantic parsing (Zelle and Mooney, grams, without access to gold trees, treating 1996; Zettlemoyer and Collins, 2005; Liang et al., trees as latent variables, (ii) parsing a class 2011), where partial programs are predicted over of non-projective trees through an extension short spans in the utterance, and are composed to to standard CKY. On G EO Q UERY, SCAN build the program for the entire utterance. Such and CLOSURE datasets, S PAN BASED SP perexplicit inductive bias for compositionality should forms similarly to stro"
2021.acl-long.74,P17-1003,1,0.918354,"t is semantically valid. 3.3 Training without Gold Trees We now remove the assumption of access to gold trees at training time, in line with standard supervised semantic parsing, where only the gold program z is given, without its decomposition over x. This can be viewed as a weakly-supervised setting, where the correct span tree is a discrete latent variable. In this setup, our goal is to maximize X log p(z |x) = log p(T ) 911 T :program(T )=z ≈ log argmax T :program(T )=z p(T ). join: largest_one(pop_1(state(all))) Because marginalizing over trees is intractable, we take a hard-EM approach (Liang et al., 2017; Min et al., 2019), and replace the sum over trees with an argmax. More concretely, to approximately solve ∗ , the argmax and find the highest scoring tree, Ttrain we employ a constrained version of Alg. 1, that prunes out trees that cannot generate z. We first remove all predictions of constants that do not appear in z by setting their score to −∞: - state Exact match features The challenge of weaklysupervised parsing is that S PAN BASED SP must learn to map language phrases to constants, and how the span tree is structured. To alleviate the language-to-constant problem we add an exact match"
2021.acl-long.74,P15-1142,0,0.0271829,"as in the figure, or [[matte block] ?]. This phenomena is common in CLEVR and CLOSURE, as span trees tend to be deep, and thus have more ambiguity. 4.5 Limitations Our approach assumes a one-to-one mapping between domain constants and their manifestation as phrases in language. This leads to strong results on compositional generalization, but hurts the flexibility that is sometimes necessary in semantic parsing. For example, in some cases predicates do not align explicitly to a phrase in the utterance or appear several times in the program but only once in the utterance (Berant et al., 2013; Pasupat and Liang, 2015). This is evident in text-to-SQL parsing, where an utterance such as “What is the minimum, and maximum age of all singers from France?” is mapped to SELECT min(age) , max(age) FROM singer WHERE country=’France’. Here, the constant age is mentioned only once in language (but twice in the program), and country is not mentioned at all. Thus, our approach is more suitable for formalisms where there is tighter alignment between the natural and formal language. In addition, while we handle a class of nonprojective trees (§3.4), there are other nonprojective structures that S PAN BASED SP can not 915"
2021.acl-long.74,P11-1060,0,0.339566,"on we manually add no more than two examples of language phrases for each constant in Σ. E.g., for the predicate next_to_1, we update the lexicon to include lexicon[“border”] = lexicon[“borders”] = next_to_1. This requires minimal manual work (if no language phrases are available), but is done only once, and is common in semantic parsing (Zettlemoyer and Collins, 2005; Wang et al., 2015; Liang et al., 2017). 3.4 Non-Projective Trees Our span-based parser assumes composition can only be done for adjacent spans that form together a contiguous span. However, this assumption does not always hold (Liang et al., 2011). For example, in Figure 3, while the predicate pop_1 should combine with the predicate state, the spans they align to (“people” and “state” respectively) are not contiguous, as they are separated by “most”, which contributes the semantics of a superlative. In constituency parsing, such non-projective structures are treated by adding rules to the grammar G (Maier et al., 2012; Corro, 2020; Stanojevi´c and Steedman, 2020). We identify one specific class of non-projective structures that is frequent in semantic parsing (Figure 3), and expand the grammar G and the CKY Algorithm to support this st"
2021.acl-long.74,W18-5413,0,0.0636602,"Missing"
2021.acl-long.74,W12-4615,0,0.030477,"al., 2015; Liang et al., 2017). 3.4 Non-Projective Trees Our span-based parser assumes composition can only be done for adjacent spans that form together a contiguous span. However, this assumption does not always hold (Liang et al., 2011). For example, in Figure 3, while the predicate pop_1 should combine with the predicate state, the spans they align to (“people” and “state” respectively) are not contiguous, as they are separated by “most”, which contributes the semantics of a superlative. In constituency parsing, such non-projective structures are treated by adding rules to the grammar G (Maier et al., 2012; Corro, 2020; Stanojevi´c and Steedman, 2020). We identify one specific class of non-projective structures that is frequent in semantic parsing (Figure 3), and expand the grammar G and the CKY Algorithm to support this structure. Specifically, we add the ternary grammar rule join := join join join. During CKY, when calculating the top-K trees for spans (i, j) (line 6 in Alg. 1), we also consider the following top-K scores for the non-terminal join: max [s’(xij , join) + π(i, s1 , join) s1 ∈i...(j−2) s2 ∈(s1 +1)...(j−1) + π(s1 + 1, s2 , join) + π(s2 + 1, j, join)]. These additional trees are c"
2021.acl-long.74,D19-1284,0,0.0138912,"lid. 3.3 Training without Gold Trees We now remove the assumption of access to gold trees at training time, in line with standard supervised semantic parsing, where only the gold program z is given, without its decomposition over x. This can be viewed as a weakly-supervised setting, where the correct span tree is a discrete latent variable. In this setup, our goal is to maximize X log p(z |x) = log p(T ) 911 T :program(T )=z ≈ log argmax T :program(T )=z p(T ). join: largest_one(pop_1(state(all))) Because marginalizing over trees is intractable, we take a hard-EM approach (Liang et al., 2017; Min et al., 2019), and replace the sum over trees with an argmax. More concretely, to approximately solve ∗ , the argmax and find the highest scoring tree, Ttrain we employ a constrained version of Alg. 1, that prunes out trees that cannot generate z. We first remove all predictions of constants that do not appear in z by setting their score to −∞: - state Exact match features The challenge of weaklysupervised parsing is that S PAN BASED SP must learn to map language phrases to constants, and how the span tree is structured. To alleviate the language-to-constant problem we add an exact match feature, based on"
2021.acl-long.74,D14-1162,0,0.0842728,"Missing"
2021.acl-long.74,N18-1202,0,0.0146289,"epresentation of all objects in the scene. Each scene object is represented by adding learned embeddings of all of its attributes: shape, material, size, color, and relative positional rank (from left to right, and from front to back). We fine-tune the model on the training set using cross-entropy loss, where the [CLS] token is used to predict the answer. Baselines S EQ 2S EQ Similar to Finegan-Dollak et al. (2018), our baseline parser is a standard seq2seq model (Jia and Liang, 2016) that encodes the utterance x with a BiLSTM encoder over pre-trained GloVe (Pennington et al., 2014) or ELM O (Peters et al., 2018) embeddings, and decodes the program with an attention-based LSTM decoder (Bahdanau et al., 2015) assisted by a copying mechanism for handling entities unseen during training time (Gu et al., 2016). BERT2S EQ Same as S EQ 2S EQ, but we replace the BiLSTM encoder with BERT-base, which is identical to the encoder of S PAN BASED SP. GRAMMAR Grammar-based decoding has been shown to improve performance on IID splits (Krishnamurthy et al., 2017; Yin and Neubig, 2017). Because decoding is constrained by the grammar, the model outputs only valid programs, which could potentially improve performance on"
2021.acl-long.74,2020.iwpt-1.12,0,0.0674394,"Missing"
2021.acl-long.74,P17-1076,0,0.178247,"ams are predicted over of non-projective trees through an extension short spans in the utterance, and are composed to to standard CKY. On G EO Q UERY, SCAN build the program for the entire utterance. Such and CLOSURE datasets, S PAN BASED SP perexplicit inductive bias for compositionality should forms similarly to strong seq2seq baselines on encourage compositional generalization. random splits, but dramatically improves perSpecifically, we propose to introduce such inducformance compared to baselines on splits that require compositional generalization: from tive bias via a span-based parser (Stern et al., 2017; 61.0 → 88.9 average accuracy. Pasupat et al., 2019), equipped with the advantages of modern neural architectures. Our model, S PAN 1 Introduction BASED SP, predicts for every span in the input a category, which is either a constant from the unThe most dominant approach in recent years for semantic parsing, the task of mapping a natural lan- derlying knowledge-base, a composition category, or a null category. Given the category predictions guage utterance to an executable program, has been for all spans, we can construct a tree over the input based on sequence-to-sequence (seq2seq) models (Ji"
2021.acl-long.74,2020.acl-main.677,0,0.186327,"neural architectures. Our model, S PAN 1 Introduction BASED SP, predicts for every span in the input a category, which is either a constant from the unThe most dominant approach in recent years for semantic parsing, the task of mapping a natural lan- derlying knowledge-base, a composition category, or a null category. Given the category predictions guage utterance to an executable program, has been for all spans, we can construct a tree over the input based on sequence-to-sequence (seq2seq) models (Jia and Liang, 2016; Dong and Lapata, 2016; utterance and deterministically compute the output Wang et al., 2020, inter alia). In these models, program. For example, in Figure 1, the category for the output program is decoded step-by-step (au- the tree node covering the span “New York borders ?” is the composition category join, indicating toregressively), using an attention mechanism that the composition of the predicate next_to_1 with softly ties output tokens to the utterance. Despite the success of seq2seq models, recently, the entity stateid(’new york’). Finegan-Dollak et al. (2018) and Keysers et al. Categories are predicted for each span indepen(2020) and Herzig and Berant (2019) demonstrated den"
2021.acl-long.74,P15-1129,1,0.892358,"Missing"
2021.acl-long.74,P07-1121,0,0.375687,"ention-based LSTM decoder (Bahdanau et al., 2015) assisted by a copying mechanism for handling entities unseen during training time (Gu et al., 2016). BERT2S EQ Same as S EQ 2S EQ, but we replace the BiLSTM encoder with BERT-base, which is identical to the encoder of S PAN BASED SP. GRAMMAR Grammar-based decoding has been shown to improve performance on IID splits (Krishnamurthy et al., 2017; Yin and Neubig, 2017). Because decoding is constrained by the grammar, the model outputs only valid programs, which could potentially improve performance on compositional splits. We use the grammar from (Wong and Mooney, 2007) for G EO Q UERY, and write grammars for SCAN-SP and CLEVR + CLOSURE. The model architecture is identical to S EQ 2S EQ. BART We additionally experiment with BART-base (Lewis et al., 2020), a seq2seq model pre-trained as a denoising autoencoder. E ND 2E ND Semantic parsers generate a program that is executed to retrieve an answer. However, other end-to-end models directly predict the answer from the context without an executor, where 4.3 Main Results Table 2 shows denotation accuracies for all baselines (top part) and our S PAN BASED SP model (middle part). For S PAN BASED SP, We also ablate t"
2021.acl-long.74,P17-1041,0,0.0125915,"del (Jia and Liang, 2016) that encodes the utterance x with a BiLSTM encoder over pre-trained GloVe (Pennington et al., 2014) or ELM O (Peters et al., 2018) embeddings, and decodes the program with an attention-based LSTM decoder (Bahdanau et al., 2015) assisted by a copying mechanism for handling entities unseen during training time (Gu et al., 2016). BERT2S EQ Same as S EQ 2S EQ, but we replace the BiLSTM encoder with BERT-base, which is identical to the encoder of S PAN BASED SP. GRAMMAR Grammar-based decoding has been shown to improve performance on IID splits (Krishnamurthy et al., 2017; Yin and Neubig, 2017). Because decoding is constrained by the grammar, the model outputs only valid programs, which could potentially improve performance on compositional splits. We use the grammar from (Wong and Mooney, 2007) for G EO Q UERY, and write grammars for SCAN-SP and CLEVR + CLOSURE. The model architecture is identical to S EQ 2S EQ. BART We additionally experiment with BART-base (Lewis et al., 2020), a seq2seq model pre-trained as a denoising autoencoder. E ND 2E ND Semantic parsers generate a program that is executed to retrieve an answer. However, other end-to-end models directly predict the answer f"
2021.eacl-main.25,S17-2001,0,0.0356559,"ing the cosine between the vectors representing two sentences, where each vector is a count vector over the n-grams that appear in the response. We use the reduction from §5 to convert this to a diversity measure. In both metrics, rather than choosing the order of the ngrams, we average over n ∈ {1, . . . , 5}, which we found to outperform any single choice of n. Neural metrics We exploit existing BERT-based models (Devlin et al., 2019) fine-tuned for estimating similarity between two sentences (applying the reduction from §5). BERT-STS; A BERT model fine-tuned on Semantic Textual Similarity (Cer et al., 2017): a collection of sentence pairs annotated with scores from 1-5 denoting their semantic similarity.3 BERT-Score (Zhang et al., 2019a); Originally a quality metric, BERT-Score uses BERT’s embeddings to measure similarity between two senmsim (si , sj ). si ,sj ∈Sc ,i&gt;j This reduction allows us to easily define new diversity metrics based on past work on sentence similarity (Gomaa et al., 2013; Devlin et al., 2019; Zhang et al., 2019a; Reimers and Gurevych, 2019). In §6 we show that both n-gram-based similarity metrics and neural semantic similarity metrics provide useful diversity metrics. 6 6.1"
2021.eacl-main.25,W11-0609,0,0.0142251,"Missing"
2021.eacl-main.25,D19-1410,0,0.0166958,"r estimating similarity between two sentences (applying the reduction from §5). BERT-STS; A BERT model fine-tuned on Semantic Textual Similarity (Cer et al., 2017): a collection of sentence pairs annotated with scores from 1-5 denoting their semantic similarity.3 BERT-Score (Zhang et al., 2019a); Originally a quality metric, BERT-Score uses BERT’s embeddings to measure similarity between two senmsim (si , sj ). si ,sj ∈Sc ,i&gt;j This reduction allows us to easily define new diversity metrics based on past work on sentence similarity (Gomaa et al., 2013; Devlin et al., 2019; Zhang et al., 2019a; Reimers and Gurevych, 2019). In §6 we show that both n-gram-based similarity metrics and neural semantic similarity metrics provide useful diversity metrics. 6 6.1 Evaluated Metrics Experiments NLG Tasks We apply our evaluation procedure on three different English NLG tasks that require diversity. • Story completion (storyGen); We use the ROC Stories dataset (Mostafazadeh et al., 2016), in which the context c is the first four sentences of a story, and the response s is a single sentence that ends the story. We use the contexts C from this data and generate response sets Sc for each context using our testers. The long c"
2021.eacl-main.25,S18-2024,0,0.0282548,"ard evaluation metric for measuring diversity. Thus, different papers evaluate diversity differently (if at all), making it difficult to compare competing approaches (Hashimoto et al., 2019). Having a principled and consensual diversity evaluation metric is hence fundamental for the field of NLG. A key challenge in developing diversity evaluation metrics, is the difficulty in determining their efficacy. Unlike metrics for evaluating the quality of generated text, where one can measure correlation between a metric (such as BLEU (Papineni et al., 2002)) and human judgement (Zhang et al., 2019a; Sagarkar et al., 2018), it is unknown if hu326 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 326–346 April 19 - 23, 2021. ©2021 Association for Computational Linguistics sity, while n-gram-based metrics fail to do so. Due to this gap, we construct a large dataset focused on content-diversity metrics. We release the Metrics for content Diversity (McDiv) benchmark, a challenge for research in diversity evaluation. To conclude, our main contributions are: • A framework for evaluating diversity metrics. • Tests instantiating this framework, measuring"
2021.eacl-main.25,P19-1177,0,0.0222384,"While one can approximate perplexity for such models (Tevet et al., 2019), ideally, a metric should not be tied to a model. Diversity Parameter d Tester / Gd (c) “Very good!” “Fine thank you.” “Couldn’t be better.” Diversity Metric mdiv (Sc,d ) N-gram-based metrics A popular metric is distinct n-grams (Li et al., 2016b), which computes the proportion of unique n-grams out of the total number of n-grams in a set of generated sentences. Duˇsek et al. (2020) calculated Shannon entropy (Manning et al., 1999) based on different n-grams as a measure of lexical diversity. SelfBLEU (Zhu et al., 2018; Shu et al., 2019) measures the BLEU score of a generated sentence with respect to another generated sentence (rather than a gold reference). High average Self-BLEU indicates high similarity between generated sentences and low diversity. In §5 we expand this idea and suggest a reduction from any similarity metric to a diversity metric. By design, n-gram based metrics are sensitive to diversity in the form of language, rather than its meaning. Test Score ρ(mdiv , d) Figure 2: An overview of our diversity metrics evaluation framework. The tester (machine or human) generates a response set (Sc,d ) given a diversit"
2021.eacl-main.25,N19-1233,1,0.722846,"-based metrics, humans are substantially better than any automatic metric at detecting content diversity. This is illustrated in Figure 1, where a human clearly distinguishes between sets that have high (blue) and low (orange) content diver1 https://github.com/GuyTevet/ diversity-eval 327 sampling are popular decoding schemes that tradeoff quality and diversity by ignoring some of the LM probability mass (Holtzman et al., 2019). Last, some NLG models, such as Generative Adversarial Networks (GANs) (Yu et al., 2017) are not language models. While one can approximate perplexity for such models (Tevet et al., 2019), ideally, a metric should not be tied to a model. Diversity Parameter d Tester / Gd (c) “Very good!” “Fine thank you.” “Couldn’t be better.” Diversity Metric mdiv (Sc,d ) N-gram-based metrics A popular metric is distinct n-grams (Li et al., 2016b), which computes the proportion of unique n-grams out of the total number of n-grams in a set of generated sentences. Duˇsek et al. (2020) calculated Shannon entropy (Manning et al., 1999) based on different n-grams as a measure of lexical diversity. SelfBLEU (Zhu et al., 2018; Shu et al., 2019) measures the BLEU score of a generated sentence with re"
2021.eacl-main.25,P19-1191,0,0.0242056,"Missing"
2021.eacl-main.25,P19-1193,0,0.155336,"ither a neural model or a human. A good tester should reliably represent the diversity level quantified by d. As a hypothetical example, c can be a movie name and d represent sentiment diversity, that is, Embedding-based metrics A new line of metrics suggests to embed generated sentences in latent space, then evaluate them in this space. Du and Black (2019) suggest to cluster the embedded sentences with k-means, then use its inertia as a measure for diversity. Recently, Lai et al. (2020) suggested to consider the volume induced by the embedded sentences as a diversity metric. Human evaluation Yang et al. (2019) asked humans to evaluate the internal diversity of a generated essay. Ghandeharioun et al. (2019) let crowdsourcing workers interact with a dialog chat-bot, then asked them to evaluate the diversity of a single conversation. In contrast, this paper focuses on the diversity of different responses given a context, as in Zhang et al. (2019b). To conclude, increasing interest in diversity resulted in multiple proposed diversity metrics. However, there is no consensus on how to evaluate diversity and what each metric actually measures. 3 “How are you today?” c Evaluating Diversity Metrics We now d"
2021.eacl-main.25,P19-1199,0,0.352898,"9), there is no standard evaluation metric for measuring diversity. Thus, different papers evaluate diversity differently (if at all), making it difficult to compare competing approaches (Hashimoto et al., 2019). Having a principled and consensual diversity evaluation metric is hence fundamental for the field of NLG. A key challenge in developing diversity evaluation metrics, is the difficulty in determining their efficacy. Unlike metrics for evaluating the quality of generated text, where one can measure correlation between a metric (such as BLEU (Papineni et al., 2002)) and human judgement (Zhang et al., 2019a; Sagarkar et al., 2018), it is unknown if hu326 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 326–346 April 19 - 23, 2021. ©2021 Association for Computational Linguistics sity, while n-gram-based metrics fail to do so. Due to this gap, we construct a large dataset focused on content-diversity metrics. We release the Metrics for content Diversity (McDiv) benchmark, a challenge for research in diversity evaluation. To conclude, our main contributions are: • A framework for evaluating diversity metrics. • Tests instantiating t"
2021.eacl-main.316,2020.acl-main.747,0,0.0266135,"Missing"
2021.eacl-main.316,N19-1423,0,0.0506725,"e models perform knowledge extraction. 1 Predictor (Off-The-Shelf Pretrained BERT) Q: will & grace is originally aired on [MASK]. Nearest Neighbors BERT Embeddings Vocabulary Rewriter (Identity Pretrained BERT) Q: will & grace was originally aired on [MASK]. Figure 1: The BERTese Model. The model takes an input query, rewrites it, and feeds the output to a pretrained BERT model. The untrained components are marked in green, and the blue component is trained. Introduction Recent work has shown that large pre-trained language models (LM), trained with a masked language modeling (MLM) objective (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Sanh et al., 2019; Conneau et al., 2020), encode substantial amounts of world knowledge in their parameters. This has led to ample research on developing methods for extracting that knowledge (Petroni et al., 2019, 2020; Jiang et al., 2020; Bouraoui et al., 2020). The most straightforward approach is to present the model with a manually-crafted query such as “Dante was born in [MASK]” and check if the model predicts “Florence” in the [MASK] position. However, when this fails, it is difficult to determine if the knowledge is absent from the LM or if the mod"
2021.eacl-main.316,L18-1544,0,0.0141688,"ct word embeddings it received as input (i.e., fine-tuned to the identity mapping). Thus, when training for knowledge extraction, the rewriter is initialized to output exactly the query it received as input. 3620 4 Experiments Experimental setup We conduct our experiments on the LAMA dataset (Petroni et al., 2019; Jiang et al., 2020), a recently introduced unsupervised knowledge-extraction benchmark for pretrained LMs. LAMA is composed of a collection of cloze-style queries about relational facts with a single token answer. As in Jiang et al. (2020), we limit our main experiment to the T-REx (Elsahar et al., 2018) subset. The T-REx dataset is constructed out of 41 relations, each associated with at most 1000 queries, all extracted from Wikidata. For training our model, we use a separate training set, created by Jiang et al. (2020), called T-RExtrain. This dataset is constructed from Wikidata and has no overlap with the original T-REx dataset. We evaluate our model on the complete T-REx dataset. Implementation Details Both the rewriter and the predictor are based on BERTbase with the default settings from the Huggingface (Wolf et al., 2020) platform. We optimize BERTese using AdamW with an initial learn"
2021.eacl-main.316,P19-1610,0,0.0282617,"ented in these models. Our code can be downloaded from https://github.com/ adihaviv/bertese. 2 Related Work Choosing the right language for extracting world knowledge from LMs has attracted much interest recently. First, Petroni et al. (2019) observed that MLMs can complete simple queries with correct factual information. Jiang et al. (2020) and Heinzerling and Inui (2020) then showed that in the zeroshot setting, small variations to such queries can lead to a drop in fact recall. Orthogonally, another line of research focused on query reformulation for standard Question Answering (QA) tasks. Gan and Ng (2019) demonstrated that even minor query modifications can lead to a significant decrease in performance for multiple QA models and tasks. Buck et al. (2017) showed that it is possible to train a neural network to reformulate a question using Reinforcement Learning (RL), optimizing the accuracy of a black-box QA system. Similarly, Nogueira and Cho (2017) used RL to create a query reformulation system that maximizes the recall of a black-box information retrieval engine. Jiang et al. (2020) proposed an ensemble method for query reformulation from LMs, that includes: (1) mining new queries, (2) using"
2021.eacl-main.316,D19-1250,0,0.115399,"Missing"
2021.eacl-main.316,P19-1139,0,0.069962,"Missing"
2021.eacl-main.316,2020.tacl-1.28,0,0.583624,"se Model. The model takes an input query, rewrites it, and feeds the output to a pretrained BERT model. The untrained components are marked in green, and the blue component is trained. Introduction Recent work has shown that large pre-trained language models (LM), trained with a masked language modeling (MLM) objective (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019; Sanh et al., 2019; Conneau et al., 2020), encode substantial amounts of world knowledge in their parameters. This has led to ample research on developing methods for extracting that knowledge (Petroni et al., 2019, 2020; Jiang et al., 2020; Bouraoui et al., 2020). The most straightforward approach is to present the model with a manually-crafted query such as “Dante was born in [MASK]” and check if the model predicts “Florence” in the [MASK] position. However, when this fails, it is difficult to determine if the knowledge is absent from the LM or if the model failed to understand the query itself. For example, the model might return the correct answer if the query is “Dante was born in the city of [MASK]”. Motivated by the above observation, we ask: can we automatically find the best way to “ask” an LM about its knowledge? We re"
2021.eacl-main.316,2021.ccl-1.108,0,0.126637,"Missing"
2021.eacl-main.316,D17-1061,0,0.0288661,"nd Heinzerling and Inui (2020) then showed that in the zeroshot setting, small variations to such queries can lead to a drop in fact recall. Orthogonally, another line of research focused on query reformulation for standard Question Answering (QA) tasks. Gan and Ng (2019) demonstrated that even minor query modifications can lead to a significant decrease in performance for multiple QA models and tasks. Buck et al. (2017) showed that it is possible to train a neural network to reformulate a question using Reinforcement Learning (RL), optimizing the accuracy of a black-box QA system. Similarly, Nogueira and Cho (2017) used RL to create a query reformulation system that maximizes the recall of a black-box information retrieval engine. Jiang et al. (2020) proposed an ensemble method for query reformulation from LMs, that includes: (1) mining new queries, (2) using an off-the-shelf pre-trained translation model to collect additional paraphrased queries with back-translation, and (3) using a re-ranker to select one or more of the new queries. They then feed those queries to BERT to get the masked token prediction. In this work, we take the idea of Jiang et al. (2020) a step forward and train a model in an endt"
2021.emnlp-main.115,D19-1423,0,0.018564,"repeatedly shown to be an effective attack method on pre-trained models (Alzantot et al., 2018; Lei et al., 2019; Ren et al., 2019; Li et al., 2020; Jin et al., 2020; Zang et al., 2020). However, in terms of improving robustness, discrete attacks have thus far been mostly used with offline augmentation (defined below) and have led to limited robustness gains. In this work, we examine the more costly but potentially more beneficial online augmentation. crete attack w.r.t to the model’s current state. Online augmentation has been used to improve robustness in NLP with gradient-based approaches (Jia et al., 2019; Shi et al., 2020; Zhou et al., 2020), but to the best of our knowledge has been overlooked in the context of discrete attacks. In this work, we are the first to propose model-agnostic online augmentation training, which uses automatically generated discrete adversarial attacks to boost overall robustness in NLP models. 3 The Attack Space An attack space for an input with respect to a classification task can be intuitively defined as the set of label-preserving perturbations over the input. A popular attack space S(x), which we adopt, is the space of synonym substitutions (Alzantot et al., 20"
2021.emnlp-main.115,P18-2006,0,0.127765,"st node after the first step (p = 0.6), only tuted in the source node x0 = x. Then, the model from the second-best (p = 0.65). A(·) is run on the frontier, that is, all out-neighbor nodes N (xt ) = {ˆ xt+1 |(xt , x ˆt+1 ) ∈ E}, and the Best-first search (Pearl, 1984) overcomes this at one that minimizes the heuristic scoring function a negligible cost, by holding a min-heap over the is selected: xt+1 := argminxˆ∈N (xt ) sA (ˆ x). nodes of the frontier of the search space (Alg. 1). In While greedy search has been used for character- each step, we pop the next utterance, which assigns flipping (Ebrahimi et al., 2018), it is ill-suited in the lowest probability to the gold label, and push all the space of synonym substitutions. The degree neighbors into the heap. When a promising branch of nodes is high – assuming nrep words can be re- turns out to be sub-optimal, search can resume from placed in the text, each with K possible synonyms, an earlier node to find a better solution, as shown then the out degree is O(nrep · K). This results in in the blue path in Figure 3. To bound the cost an infeasible number of forward passes through the of finding a single adversarial example, we bound attacked model even f"
2021.emnlp-main.115,2020.acl-main.197,0,0.0848279,"Missing"
2021.emnlp-main.115,2020.emnlp-main.498,0,0.0326975,"Missing"
2021.emnlp-main.115,2020.acl-main.245,0,0.0213317,"obust accuracy. Recently, Zhou et al. (2020) proposed a method that does improve robustness, but like other gradient-based methods, it is white-box, does not work well with transformers over subwords, and leads to noisy samples. A similar approach has been taken by Si et al. (2020b) to generate virtual attacks during training by interpolating offline-generated attacks. Defense layers This approach involves adding normalization layers to the input before propagating it to the model, so that different input variations are mapped to the same representation (Wang et al., 2019; Mozes et al., 2020; Jones et al., 2020) . While successful, this approach requires manual engineering and a reduction in model expressivity as the input space is significantly reduced. A similar approach (Zhou et al., 2019) has been to identify adversarial inputs and predict the original un-perturbed input. orthogonal approach to ours was proposed by Garg and Ramakrishnan (2020) and Li et al. (2020), who used the fact that BERT was trained with the masked language modeling objective to predict possible semantic preserving adversarial perturbations over the input tokens, thereby coupling the definition of the attack space with the a"
2021.emnlp-main.115,2020.emnlp-main.500,0,0.226407,"prior methods. Figure 1: Robust accuracy vs. slowdown in training time, comparing different methods to Baseline (purple pentagon); x-axis in logarithmic scale. The popular A DVO FF (blue squares, offline augmentation with adversarial example) is 10x slower than our simple augmentation of 4 (8) random samples (triangles, R AND O FF-4, R AND O FF-8) and achieves similar or worse robust accuracy. Our online augmentation of adversarial examples (A DVO N, yellow circles) significantly improves robust accuracy, but is expensive to train. such perturbations (Alzantot et al., 2018; Jin et al., 2020; Li et al., 2020; Lei et al., 2019; Wallace et al., 2019; Zhang et al., 2020; Garg and Ramakrishnan, 1 Introduction 2020; Si et al., 2020a; Goel et al., 2021). Adversarial examples are inputs that are slightly, Training and evaluating models with adversarial but intentionally, perturbed to create a new exam- examples has had considerable success in computer ple that is misclassified by a model (Szegedy et al., vision, with gradient-based techniques like FGSM 2014). Adversarial examples have attracted im- (Goodfellow et al., 2015) and PGD (Madry et al., mense attention in machine learning (Goodfellow 2018). In"
2021.emnlp-main.115,2021.findings-acl.56,0,0.0790365,"Missing"
2021.emnlp-main.115,2021.findings-acl.137,0,0.0385005,"Missing"
2021.emnlp-main.115,D13-1170,0,0.0110797,"ques. To our knowledge, we are the first to evaluate online augmentation with discrete attacks on a wide range of NLP tasks. Our results show that online augmentation leads to significant improvement in robustness compared to prior work and that simple random augmentation achieves comparable results to the common offline augmentation at a fraction of the complexity and training time. Figure 2: Given a movie review x, the model A is robust to a set of perturbations, while A0 is not. We evaluate model robustness on three datasets: BoolQ (Clark et al., 2019), IMDB (Maas et al., 2011), and SST-2 (Socher et al., 2013), which vary in terms of the target task (question answering and sentiment analysis) and input length. Surprisingly, we find across different tasks (Fig. 1) that augmenting each training example with 4-8 random samples from the synonym substitution space performs as well as (or better than) the commonly used offline augmentation, while being simpler and 10x faster to train. Conversely, online augmentation makes better use of the extra computational cost, and substantially improves robust accuracy compared to offline augmentation. Additionally, our proposed discrete attack algorithm, BFF, outpe"
2021.emnlp-main.115,2020.acl-main.263,0,0.014673,"imperceptible perturbations around imsons. First, they are useful for evaluating model age pixels. Conversely, language is discrete, and robustness, and have revealed that current mod- any perturbation is perceptible. Thus, robust models are over-sensitive to minor perturbations. Sec- els must be invariant to input modifications that ond, adversarial examples can improve robustness: preserve semantics, such as synonym substitutions training on adversarial examples reduces the brittle- (Alzantot et al., 2018; Jin et al., 2020), paraphrasness and over-sensitivity of deep learning models to ing (Tan et al., 2020), or typos (Huang et al., 2019). 1529 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1529–1544 c November 7–11, 2021. 2021 Association for Computational Linguistics Due to this property of language, ample work has been dedicated to developing discrete attacks that generate adversarial examples through combinatorial optimization (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020; Zhou et al., 2020; Zang et al., 2020) . For example, in sentiment analysis, it is common to consider the space of all synonym substitutions, where an adversarial e"
2021.emnlp-main.115,D19-1496,0,0.0179823,"over subwords, and leads to noisy samples. A similar approach has been taken by Si et al. (2020b) to generate virtual attacks during training by interpolating offline-generated attacks. Defense layers This approach involves adding normalization layers to the input before propagating it to the model, so that different input variations are mapped to the same representation (Wang et al., 2019; Mozes et al., 2020; Jones et al., 2020) . While successful, this approach requires manual engineering and a reduction in model expressivity as the input space is significantly reduced. A similar approach (Zhou et al., 2019) has been to identify adversarial inputs and predict the original un-perturbed input. orthogonal approach to ours was proposed by Garg and Ramakrishnan (2020) and Li et al. (2020), who used the fact that BERT was trained with the masked language modeling objective to predict possible semantic preserving adversarial perturbations over the input tokens, thereby coupling the definition of the attack space with the attack strategy. While this approach showed great promise in efficiently generating valid adversarial examples, it does not permit any external constraint on the attack space and thus i"
2021.emnlp-main.115,D19-1221,0,0.0155567,"curacy vs. slowdown in training time, comparing different methods to Baseline (purple pentagon); x-axis in logarithmic scale. The popular A DVO FF (blue squares, offline augmentation with adversarial example) is 10x slower than our simple augmentation of 4 (8) random samples (triangles, R AND O FF-4, R AND O FF-8) and achieves similar or worse robust accuracy. Our online augmentation of adversarial examples (A DVO N, yellow circles) significantly improves robust accuracy, but is expensive to train. such perturbations (Alzantot et al., 2018; Jin et al., 2020; Li et al., 2020; Lei et al., 2019; Wallace et al., 2019; Zhang et al., 2020; Garg and Ramakrishnan, 1 Introduction 2020; Si et al., 2020a; Goel et al., 2021). Adversarial examples are inputs that are slightly, Training and evaluating models with adversarial but intentionally, perturbed to create a new exam- examples has had considerable success in computer ple that is misclassified by a model (Szegedy et al., vision, with gradient-based techniques like FGSM 2014). Adversarial examples have attracted im- (Goodfellow et al., 2015) and PGD (Madry et al., mense attention in machine learning (Goodfellow 2018). In computer vision, adversarial examples e"
2021.emnlp-main.115,2020.acl-main.540,0,0.16746,"ces the brittle- (Alzantot et al., 2018; Jin et al., 2020), paraphrasness and over-sensitivity of deep learning models to ing (Tan et al., 2020), or typos (Huang et al., 2019). 1529 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1529–1544 c November 7–11, 2021. 2021 Association for Computational Linguistics Due to this property of language, ample work has been dedicated to developing discrete attacks that generate adversarial examples through combinatorial optimization (Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020; Zhou et al., 2020; Zang et al., 2020) . For example, in sentiment analysis, it is common to consider the space of all synonym substitutions, where an adversarial example for an input “Such an amazing movie!” might be “Such an extraordinary film” (Fig. 2). This body of work has mostly focused on evaluating robustness, rather than improving it, which naturally led to the development of complex combinatorial search algorithms, whose goal is to find adversarial examples in the exponential space of perturbations. In this work, we address a major research gap in current literature around improving robustness with discrete attacks. Spec"
2021.emnlp-main.446,W19-4828,1,0.924686,"es. The feed-forward layer’s output is thus the weighted sum of its values. Transformer-based language models (Vaswani et al., 2017) are at the core of state-of-the-art natuparameter matrix in the layer corresponds to keys, ral language processing (Devlin et al., 2019; Brown et al., 2020), largely due to the success of self- and the second parameter matrix to values. Figure 1 shows how the keys (first parameter matrix) interattention. While much literature has been devoted act with the input to produce coefficients, which to analyzing the function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Be- are then used to compute a weighted sum of the vallinkov, 2019), they account for only a third of a typ- ues (second parameter matrix) as the output. While the theoretical similarity between feed-forward layical transformer’s parameters (4d2 per layer, where ers and key-value memories has previously been d is the model’s hidden dimension). Most of the parameter budget is spent on position-wise feed- suggested by Sukhbaatar et al. (2019), we take forward layers (8d2 per layer), yet their role re- this observation one step further, and analyze the mains under-explored. What, if so,"
2021.emnlp-main.446,N19-1423,0,0.0246823,"4 x5 self-attention layer Transformer layers Stay with you for a Figure 1: An illustration of how a feed-forward layer emulates a key-value memory. Input vectors (here, x5 ) are multiplied by keys to produce memory coefficients (e.g., the memory coefficient for v1 is 0.2), which then weigh distributions over the output vocabulary, stored in the values. The feed-forward layer’s output is thus the weighted sum of its values. Transformer-based language models (Vaswani et al., 2017) are at the core of state-of-the-art natuparameter matrix in the layer corresponds to keys, ral language processing (Devlin et al., 2019; Brown et al., 2020), largely due to the success of self- and the second parameter matrix to values. Figure 1 shows how the keys (first parameter matrix) interattention. While much literature has been devoted act with the input to produce coefficients, which to analyzing the function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Be- are then used to compute a weighted sum of the vallinkov, 2019), they account for only a third of a typ- ues (second parameter matrix) as the output. While the theoretical similarity between feed-forward layical transformer’s parameters"
2021.emnlp-main.446,2020.emnlp-main.395,0,0.0327808,"t in the final layer. We find that in most cases (66 examples), the output changes to a semantically distant word (e.g., “people” → “same”) and in the rest of the cases (34 examples), the feed-forward layer’s output shifts the residual prediction to a related word (e.g. “later” → “earlier” and “gastric” → “stomach”). This suggests that feed-forward layers tune the residual predictions at varying granularity, even in the last layer of the model. 6 Related Work extensive line of work targeted neuron functionality in general, extracting the properties that neurons and subsets of neurons capture (Durrani et al., 2020; Dalvi et al., 2019; Rethmeier et al., 2020; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importanc"
2021.emnlp-main.446,2020.acl-main.492,0,0.0145535,"xtract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the functionality of memory cells based on examples that trigger maximal activations has been used previously in NLP (Rethmeier et al., 2020) and vision (Erhan et al., 2009). 7 Discussion and Conclusion Understanding how and why transformers work is crucial to many aspects of modern NLP, including model interpretability, data security, and development of better models. Feed-forward layers account for most of a t"
2021.emnlp-main.446,W18-5408,0,0.0230668,"examples), the feed-forward layer’s output shifts the residual prediction to a related word (e.g. “later” → “earlier” and “gastric” → “stomach”). This suggests that feed-forward layers tune the residual predictions at varying granularity, even in the last layer of the model. 6 Related Work extensive line of work targeted neuron functionality in general, extracting the properties that neurons and subsets of neurons capture (Durrani et al., 2020; Dalvi et al., 2019; Rethmeier et al., 2020; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explo"
2021.emnlp-main.446,N19-1112,0,0.0230888,"ixes associated with shallow patterns and semantic patterns (Figure 2), the lower layers (layers 1-9) are dominated by shallow patterns, often with prefixes that share the last word (e.g. k1449 in Table 1). In contrast, the upper layers (layers 10-16) are characterized by more semantic patterns, with prefixes from similar contexts but without clear surface-form similarities (e.g. k16 1935 in Table 1). This observation corroborates recent findings that lower (upper) layers in deep contextualized models encode shallow (semantic) features of the inputs (Peters et al., 2018; Jawahar et al., 2019; Liu et al., 2019). To further test this hypothesis, we sample 1600 random keys (100 keys per layer) and apply local modifications to the top-50 trigger examples of every key. Specifically, we remove either the first, last, or a random token from the input, and measure how this mutation affects the memory coefficient. Figure 3 shows that the model considers the end of an example as more salient than the beginning for predicting the next token. In upper layers, removing the last token has less impact, supporting our conclusion that upper-layer keys are less correlated with shallow patterns. 4 Values Represent Di"
2021.emnlp-main.446,N18-1202,0,0.0194337,"allow patterns Comparing the amount of prefixes associated with shallow patterns and semantic patterns (Figure 2), the lower layers (layers 1-9) are dominated by shallow patterns, often with prefixes that share the last word (e.g. k1449 in Table 1). In contrast, the upper layers (layers 10-16) are characterized by more semantic patterns, with prefixes from similar contexts but without clear surface-form similarities (e.g. k16 1935 in Table 1). This observation corroborates recent findings that lower (upper) layers in deep contextualized models encode shallow (semantic) features of the inputs (Peters et al., 2018; Jawahar et al., 2019; Liu et al., 2019). To further test this hypothesis, we sample 1600 random keys (100 keys per layer) and apply local modifications to the top-50 trigger examples of every key. Specifically, we remove either the first, last, or a random token from the input, and measure how this mutation affects the memory coefficient. Figure 3 shows that the model considers the end of an example as more salient than the beginning for predicting the next token. In upper layers, removing the last token has less impact, supporting our conclusion that upper-layer keys are less correlated wit"
2021.emnlp-main.446,2020.acl-main.270,1,0.701989,"20; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the functionality of memory cells based on examples that trigger maximal activations has been used previously in NLP (Rethmeier et al., 2020) and vision (Erhan et al., 2009). 7 Discussion and Conclusion Understandi"
2021.emnlp-main.446,P19-1452,0,0.0160944,"racting the properties that neurons and subsets of neurons capture (Durrani et al., 2020; Dalvi et al., 2019; Rethmeier et al., 2020; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the functionality of memory cells based on examples that trigger maximal activations"
2021.emnlp-main.446,W19-4808,0,0.0160153,"l. 6 Related Work extensive line of work targeted neuron functionality in general, extracting the properties that neurons and subsets of neurons capture (Durrani et al., 2020; Dalvi et al., 2019; Rethmeier et al., 2020; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the"
2021.emnlp-main.446,P19-1580,0,0.154987,", stored in the values. The feed-forward layer’s output is thus the weighted sum of its values. Transformer-based language models (Vaswani et al., 2017) are at the core of state-of-the-art natuparameter matrix in the layer corresponds to keys, ral language processing (Devlin et al., 2019; Brown et al., 2020), largely due to the success of self- and the second parameter matrix to values. Figure 1 shows how the keys (first parameter matrix) interattention. While much literature has been devoted act with the input to produce coefficients, which to analyzing the function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Be- are then used to compute a weighted sum of the vallinkov, 2019), they account for only a third of a typ- ues (second parameter matrix) as the output. While the theoretical similarity between feed-forward layical transformer’s parameters (4d2 per layer, where ers and key-value memories has previously been d is the model’s hidden dimension). Most of the parameter budget is spent on position-wise feed- suggested by Sukhbaatar et al. (2019), we take forward layers (8d2 per layer), yet their role re- this observation one step further, and analyze the mains under-ex"
2021.emnlp-main.446,D19-1002,0,0.0262223,"from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the functionality of memory cells based on examples that trigger maximal activations has been used previously in NLP (Rethmeier et al., 2020) and vision (Erhan et al., 2009). 7 Discussion and Conclusion Understanding how and why transformers work is crucial to many aspects of modern NLP, including model interpretability, data security, and development of better models. Feed-forward layers account for most of a transformer’s parameters, yet"
2021.emnlp-main.646,2021.emnlp-main.468,0,0.0304162,"luding the question (which never appears in the summary), we can rank the question sentence based on the score of osum . Computing the rank distribution of question sentences, we see (Figure 4) that the distributions of R EADER S UMonly sum and R EADER S UM are significantly different,4 and that questions are ranked higher in R EADER S UM. This shows that the summarization head puts higher emphasis on the question in the multi-head setup. Overall, these results provide evidence that multi-head training pushes osum to perform querybased summarization on inputs from H OTPOT QA. and Singh, 2021; Aghajanyan et al., 2021; Segal et al., 2020; Hu et al., 2019; Clark et al., 2019). To the best of our knowledge, this is the first work that analyzes the outputs of the non-target heads. Previous work used additional output heads to generate explanations for model predictions (Perez et al., 2019; Schuff et al., 2020; Wang et al., 2019). Specifically, recent work has explored utilization of summarization modules for explainable QA (Nishida et al., 2019; Deng et al., 2020). In the context of summarization, Xu and Lapata (2020) have leveraged QA resources for training query-based summarization models. Hierarchies betwe"
2021.emnlp-main.646,P19-1595,0,0.0187195,"an rank the question sentence based on the score of osum . Computing the rank distribution of question sentences, we see (Figure 4) that the distributions of R EADER S UMonly sum and R EADER S UM are significantly different,4 and that questions are ranked higher in R EADER S UM. This shows that the summarization head puts higher emphasis on the question in the multi-head setup. Overall, these results provide evidence that multi-head training pushes osum to perform querybased summarization on inputs from H OTPOT QA. and Singh, 2021; Aghajanyan et al., 2021; Segal et al., 2020; Hu et al., 2019; Clark et al., 2019). To the best of our knowledge, this is the first work that analyzes the outputs of the non-target heads. Previous work used additional output heads to generate explanations for model predictions (Perez et al., 2019; Schuff et al., 2020; Wang et al., 2019). Specifically, recent work has explored utilization of summarization modules for explainable QA (Nishida et al., 2019; Deng et al., 2020). In the context of summarization, Xu and Lapata (2020) have leveraged QA resources for training query-based summarization models. Hierarchies between NLP tasks have also been explored in multi-task models"
2021.emnlp-main.646,N19-1246,0,0.0130472,"Liu et al., 2019b; Nishida et al., 2019; head, steering it to exhibit emergent behaviour, Hu and Singh, 2021). The heads are trained in a which can explain the target head’s predictions, supervised manner, each on labelled data collected or generalize beyond the task the non-target head for the task it performs (Devlin et al., 2019). At was trained for. inference time, the output is read out of a selected We study the “steering effect” in three multitarget head, while the outputs from the other heads head models (Figure 2). In a numerical reading are discarded (Figure 1). comprehension task (Dua et al., 2019), the model What is the nature of predictions made by non- is given a question and paragraph and either uses target heads given inputs directed to the target an extractive head to output an input span, or a genhead? One extreme possibility is that the pre- erative head to generate a number using arithmetic 8201 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8201–8215 c November 7–11, 2021. 2021 Association for Computational Linguistics shall LP 2 ainability / eralization rget head 2983 “4,365”, “7,348” generation multi-span extraction “Edward Davi"
2021.emnlp-main.646,2020.acl-main.89,1,0.901049,"start by examining a combination of generative ments, which are discussed in detail in §4, §5, §6. and extractive heads (Figure 2, left), and analyze Given a model with a target head ot and a steered the spans extracted from the input when the generhead os , our goal is to understand the behaviour of ative head is selected to output the final answer. os on inputs where ot provides the prediction. To this end, we focus on head combinations, where 4.1 Experimental Setting os is expressive enough to explain the outputs of ot , but unlike most prior work aiming to explain Model We take G EN BERT (Geva et al., 2020), by examining model outputs (Perez et al., 2019; a BERT-base model fine-tuned for numerical reaSchuff et al., 2020; Wang et al., 2019), os is not soning, and use it to initialize a variant called explicitly trained for this purpose. Concretely, our M SE G EN BERT, in which the single-span extracanalysis covers three settings, illustrated in Figure 2 tion head is replaced by a multi-span extraction and summarized in Table 1. (MSE) head introduced by Segal et al. (2020), The first setting (Figure 2 left, and §4) consid- which allows extracting multiple spans from the ers a model with generative"
2021.emnlp-main.646,D17-1206,0,0.0217184,"that analyzes the outputs of the non-target heads. Previous work used additional output heads to generate explanations for model predictions (Perez et al., 2019; Schuff et al., 2020; Wang et al., 2019). Specifically, recent work has explored utilization of summarization modules for explainable QA (Nishida et al., 2019; Deng et al., 2020). In the context of summarization, Xu and Lapata (2020) have leveraged QA resources for training query-based summarization models. Hierarchies between NLP tasks have also been explored in multi-task models not based on transformers (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Swayamdipta et al., 2018). Contrary to previous work, the models in this work were not trained to perform the desired behaviour. Instead, explanations and generalized behaviour emerged from training on multiple tasks. A related line of research has focused on developing probes, which are supervised network modules that predict properties from model representations (Conneau et al., 2018; van Aken et al., 2019; Tenney et al., 2019; Liu et al., 2019a). A key challenge with probes is determining whether the information exists in the representation or is learned during probing (Hewitt and Liang,"
2021.emnlp-main.646,N19-1112,0,0.143571,"tween the heads. 1 Introduction In this work, we test whether such interactions The typical framework for training a model in natu- occur in multi-task transformer models, and if nonral language processing to perform multiple tasks is target heads decode useful information given inputs to have a shared pre-trained language model (LM), directed to the target head. We show that multiand add a small, compact neural network, often head training leads to a steering effect, where the termed head, on top of the LM, for each task (Clark target head guides the behaviour of the non-target et al., 2019; Liu et al., 2019b; Nishida et al., 2019; head, steering it to exhibit emergent behaviour, Hu and Singh, 2021). The heads are trained in a which can explain the target head’s predictions, supervised manner, each on labelled data collected or generalize beyond the task the non-target head for the task it performs (Devlin et al., 2019). At was trained for. inference time, the output is read out of a selected We study the “steering effect” in three multitarget head, while the outputs from the other heads head models (Figure 2). In a numerical reading are discarded (Figure 1). comprehension task (Dua et al., 2019)"
2021.emnlp-main.646,P19-1441,0,0.16694,"tween the heads. 1 Introduction In this work, we test whether such interactions The typical framework for training a model in natu- occur in multi-task transformer models, and if nonral language processing to perform multiple tasks is target heads decode useful information given inputs to have a shared pre-trained language model (LM), directed to the target head. We show that multiand add a small, compact neural network, often head training leads to a steering effect, where the termed head, on top of the LM, for each task (Clark target head guides the behaviour of the non-target et al., 2019; Liu et al., 2019b; Nishida et al., 2019; head, steering it to exhibit emergent behaviour, Hu and Singh, 2021). The heads are trained in a which can explain the target head’s predictions, supervised manner, each on labelled data collected or generalize beyond the task the non-target head for the task it performs (Devlin et al., 2019). At was trained for. inference time, the output is read out of a selected We study the “steering effect” in three multitarget head, while the outputs from the other heads head models (Figure 2). In a numerical reading are discarded (Figure 1). comprehension task (Dua et al., 2019)"
2021.emnlp-main.646,D19-1387,0,0.0244515,"It is notable that the performance on H OTPOT QA is similar across the different models, with only a slight deterioration when training only the extraction head (osse ). This is expected as R EADERonly sse is not trained with yes/no questions, which make up a small fraction of H OTPOT QA. 6 osum := FFNNd×1 (hCLS ). Setting 3: Emerging Query-based Summaries The sentences are ranked by their scores and the top-3 highest score sentences are taken as the summary (top-3 because choosing the first 3 sentences of a document is a standard baseline in extractive summarization (Nallapati et al., 2017; Liu and Lapata, 2019)). Implementation details are in A.3. Data The QA heads (osse , otype ) are trained on H OTPOT QA, while the summarization head is trained on the CNN/DAILY M AIL dataset for extractive summarization (Hermann et al., 2015). We use the supporting facts from H OTPOT QA to evaluate the outputs of osum as explanations for predictions of the QA heads. Evaluation metrics Annotated supporting facts and the summary are defined by sentences from the input context. Therefore, given a set T of sentences extracted by osum (|T |= 3) and the set of supporting facts F, we compute the Recall@3 of T against F."
2021.emnlp-main.646,K16-1028,0,0.0621217,"Missing"
2021.emnlp-main.646,P19-1225,0,0.102587,"Introduction In this work, we test whether such interactions The typical framework for training a model in natu- occur in multi-task transformer models, and if nonral language processing to perform multiple tasks is target heads decode useful information given inputs to have a shared pre-trained language model (LM), directed to the target head. We show that multiand add a small, compact neural network, often head training leads to a steering effect, where the termed head, on top of the LM, for each task (Clark target head guides the behaviour of the non-target et al., 2019; Liu et al., 2019b; Nishida et al., 2019; head, steering it to exhibit emergent behaviour, Hu and Singh, 2021). The heads are trained in a which can explain the target head’s predictions, supervised manner, each on labelled data collected or generalize beyond the task the non-target head for the task it performs (Devlin et al., 2019). At was trained for. inference time, the output is read out of a selected We study the “steering effect” in three multitarget head, while the outputs from the other heads head models (Figure 2). In a numerical reading are discarded (Figure 1). comprehension task (Dua et al., 2019), the model What is the"
2021.emnlp-main.646,D19-1244,0,0.333082,"where we modify the representation based on the output of the extractive head, and show this leads to predictable changes in the behaviour of the generative head. Thus, we can use the output of the non-target head to improve interpretability. We observe a similar phenomenon in multi-hop question answering (QA) model (Yang et al., 2018), where a non-target span extraction head outputs supporting evidence for the answer predicted by a classification head (Figure 2, center). This emerging interpretability is considerably different from methods that explicitly train models to output explanations (Perez et al., 2019; Schuff et al., 2020). models, and find that without any dedicated training, non-target heads provide explanations for the predictions of target heads, and exhibit capabilities beyond the ones they were trained for. This extrapolation of skills can be harnessed for many applications. For example, teaching models new skills could by done by training on combinations of tasks different from the target task. This would be useful when labeled data is not available or hard to collect. Also, training an additional head that extracts information from the input could be applied as a general practice f"
2021.emnlp-main.646,2020.acl-demos.14,0,0.0607047,"Missing"
2021.emnlp-main.646,W95-0107,0,0.0218116,"Missing"
2021.emnlp-main.646,2020.emnlp-main.575,0,0.26691,"representation based on the output of the extractive head, and show this leads to predictable changes in the behaviour of the generative head. Thus, we can use the output of the non-target head to improve interpretability. We observe a similar phenomenon in multi-hop question answering (QA) model (Yang et al., 2018), where a non-target span extraction head outputs supporting evidence for the answer predicted by a classification head (Figure 2, center). This emerging interpretability is considerably different from methods that explicitly train models to output explanations (Perez et al., 2019; Schuff et al., 2020). models, and find that without any dedicated training, non-target heads provide explanations for the predictions of target heads, and exhibit capabilities beyond the ones they were trained for. This extrapolation of skills can be harnessed for many applications. For example, teaching models new skills could by done by training on combinations of tasks different from the target task. This would be useful when labeled data is not available or hard to collect. Also, training an additional head that extracts information from the input could be applied as a general practice for model interpretabil"
2021.emnlp-main.646,2020.emnlp-main.248,1,0.77017,"h never appears in the summary), we can rank the question sentence based on the score of osum . Computing the rank distribution of question sentences, we see (Figure 4) that the distributions of R EADER S UMonly sum and R EADER S UM are significantly different,4 and that questions are ranked higher in R EADER S UM. This shows that the summarization head puts higher emphasis on the question in the multi-head setup. Overall, these results provide evidence that multi-head training pushes osum to perform querybased summarization on inputs from H OTPOT QA. and Singh, 2021; Aghajanyan et al., 2021; Segal et al., 2020; Hu et al., 2019; Clark et al., 2019). To the best of our knowledge, this is the first work that analyzes the outputs of the non-target heads. Previous work used additional output heads to generate explanations for model predictions (Perez et al., 2019; Schuff et al., 2020; Wang et al., 2019). Specifically, recent work has explored utilization of summarization modules for explainable QA (Nishida et al., 2019; Deng et al., 2020). In the context of summarization, Xu and Lapata (2020) have leveraged QA resources for training query-based summarization models. Hierarchies between NLP tasks have al"
2021.emnlp-main.646,P16-2038,0,0.0340591,"dge, this is the first work that analyzes the outputs of the non-target heads. Previous work used additional output heads to generate explanations for model predictions (Perez et al., 2019; Schuff et al., 2020; Wang et al., 2019). Specifically, recent work has explored utilization of summarization modules for explainable QA (Nishida et al., 2019; Deng et al., 2020). In the context of summarization, Xu and Lapata (2020) have leveraged QA resources for training query-based summarization models. Hierarchies between NLP tasks have also been explored in multi-task models not based on transformers (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Swayamdipta et al., 2018). Contrary to previous work, the models in this work were not trained to perform the desired behaviour. Instead, explanations and generalized behaviour emerged from training on multiple tasks. A related line of research has focused on developing probes, which are supervised network modules that predict properties from model representations (Conneau et al., 2018; van Aken et al., 2019; Tenney et al., 2019; Liu et al., 2019a). A key challenge with probes is determining whether the information exists in the representation or is learned during pro"
2021.emnlp-main.646,D18-1412,0,0.0187001,"s of the non-target heads. Previous work used additional output heads to generate explanations for model predictions (Perez et al., 2019; Schuff et al., 2020; Wang et al., 2019). Specifically, recent work has explored utilization of summarization modules for explainable QA (Nishida et al., 2019; Deng et al., 2020). In the context of summarization, Xu and Lapata (2020) have leveraged QA resources for training query-based summarization models. Hierarchies between NLP tasks have also been explored in multi-task models not based on transformers (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Swayamdipta et al., 2018). Contrary to previous work, the models in this work were not trained to perform the desired behaviour. Instead, explanations and generalized behaviour emerged from training on multiple tasks. A related line of research has focused on developing probes, which are supervised network modules that predict properties from model representations (Conneau et al., 2018; van Aken et al., 2019; Tenney et al., 2019; Liu et al., 2019a). A key challenge with probes is determining whether the information exists in the representation or is learned during probing (Hewitt and Liang, 2019; Tamkin et al., 2020;"
2021.emnlp-main.646,2020.emnlp-main.296,0,0.0290013,"shes osum to perform querybased summarization on inputs from H OTPOT QA. and Singh, 2021; Aghajanyan et al., 2021; Segal et al., 2020; Hu et al., 2019; Clark et al., 2019). To the best of our knowledge, this is the first work that analyzes the outputs of the non-target heads. Previous work used additional output heads to generate explanations for model predictions (Perez et al., 2019; Schuff et al., 2020; Wang et al., 2019). Specifically, recent work has explored utilization of summarization modules for explainable QA (Nishida et al., 2019; Deng et al., 2020). In the context of summarization, Xu and Lapata (2020) have leveraged QA resources for training query-based summarization models. Hierarchies between NLP tasks have also been explored in multi-task models not based on transformers (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Swayamdipta et al., 2018). Contrary to previous work, the models in this work were not trained to perform the desired behaviour. Instead, explanations and generalized behaviour emerged from training on multiple tasks. A related line of research has focused on developing probes, which are supervised network modules that predict properties from model representations (Co"
2021.emnlp-main.646,D18-1259,0,0.0949412,"as the non-target head, we observe that it tends to output the arguments to the arithmetic operation performed by the decoder, and that successful argument extraction is correlated with higher performance. Moreover, we perform interventions (Woodward, 2005; Elazar et al., 2021), where we modify the representation based on the output of the extractive head, and show this leads to predictable changes in the behaviour of the generative head. Thus, we can use the output of the non-target head to improve interpretability. We observe a similar phenomenon in multi-hop question answering (QA) model (Yang et al., 2018), where a non-target span extraction head outputs supporting evidence for the answer predicted by a classification head (Figure 2, center). This emerging interpretability is considerably different from methods that explicitly train models to output explanations (Perez et al., 2019; Schuff et al., 2020). models, and find that without any dedicated training, non-target heads provide explanations for the predictions of target heads, and exhibit capabilities beyond the ones they were trained for. This extrapolation of skills can be harnessed for many applications. For example, teaching models new"
2021.emnlp-main.646,2020.tacl-1.48,1,0.67895,". Contrary to previous work, the models in this work were not trained to perform the desired behaviour. Instead, explanations and generalized behaviour emerged from training on multiple tasks. A related line of research has focused on developing probes, which are supervised network modules that predict properties from model representations (Conneau et al., 2018; van Aken et al., 2019; Tenney et al., 2019; Liu et al., 2019a). A key challenge with probes is determining whether the information exists in the representation or is learned during probing (Hewitt and Liang, 2019; Tamkin et al., 2020; Talmor et al., 2020). Unlike probes, steered heads are trained in parallel to target heads rather than on a fixed model. Moreover, steered heads are not designed to decode specific properties from representations, but their behaviour naturally extends beyond their training objective. Our findings also relate to explainability methods that highlight parts from the input via the model’s attention (Wiegreffe and Pinter, 2019), and extract rationales through unsupervised training (Lei et al., 2016). The emerging explanations we observe are based on the predictions of a head rather than on internal representations. 8"
2021.emnlp-main.646,2020.findings-emnlp.125,0,0.0232217,"amdipta et al., 2018). Contrary to previous work, the models in this work were not trained to perform the desired behaviour. Instead, explanations and generalized behaviour emerged from training on multiple tasks. A related line of research has focused on developing probes, which are supervised network modules that predict properties from model representations (Conneau et al., 2018; van Aken et al., 2019; Tenney et al., 2019; Liu et al., 2019a). A key challenge with probes is determining whether the information exists in the representation or is learned during probing (Hewitt and Liang, 2019; Tamkin et al., 2020; Talmor et al., 2020). Unlike probes, steered heads are trained in parallel to target heads rather than on a fixed model. Moreover, steered heads are not designed to decode specific properties from representations, but their behaviour naturally extends beyond their training objective. Our findings also relate to explainability methods that highlight parts from the input via the model’s attention (Wiegreffe and Pinter, 2019), and extract rationales through unsupervised training (Lei et al., 2016). The emerging explanations we observe are based on the predictions of a head rather than on intern"
2021.emnlp-main.646,P19-1452,0,0.0130736,"uery-based summarization models. Hierarchies between NLP tasks have also been explored in multi-task models not based on transformers (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Swayamdipta et al., 2018). Contrary to previous work, the models in this work were not trained to perform the desired behaviour. Instead, explanations and generalized behaviour emerged from training on multiple tasks. A related line of research has focused on developing probes, which are supervised network modules that predict properties from model representations (Conneau et al., 2018; van Aken et al., 2019; Tenney et al., 2019; Liu et al., 2019a). A key challenge with probes is determining whether the information exists in the representation or is learned during probing (Hewitt and Liang, 2019; Tamkin et al., 2020; Talmor et al., 2020). Unlike probes, steered heads are trained in parallel to target heads rather than on a fixed model. Moreover, steered heads are not designed to decode specific properties from representations, but their behaviour naturally extends beyond their training objective. Our findings also relate to explainability methods that highlight parts from the input via the model’s attention (Wiegreff"
2021.emnlp-main.646,K19-1065,0,0.155047,"), and analyze Given a model with a target head ot and a steered the spans extracted from the input when the generhead os , our goal is to understand the behaviour of ative head is selected to output the final answer. os on inputs where ot provides the prediction. To this end, we focus on head combinations, where 4.1 Experimental Setting os is expressive enough to explain the outputs of ot , but unlike most prior work aiming to explain Model We take G EN BERT (Geva et al., 2020), by examining model outputs (Perez et al., 2019; a BERT-base model fine-tuned for numerical reaSchuff et al., 2020; Wang et al., 2019), os is not soning, and use it to initialize a variant called explicitly trained for this purpose. Concretely, our M SE G EN BERT, in which the single-span extracanalysis covers three settings, illustrated in Figure 2 tion head is replaced by a multi-span extraction and summarized in Table 1. (MSE) head introduced by Segal et al. (2020), The first setting (Figure 2 left, and §4) consid- which allows extracting multiple spans from the ers a model with generative and extractive heads, input. This is important for supporting extraction trained on the DROP dataset (Dua et al., 2019) of more than o"
2021.emnlp-main.646,D19-1002,0,0.0286433,"l., 2019; Liu et al., 2019a). A key challenge with probes is determining whether the information exists in the representation or is learned during probing (Hewitt and Liang, 2019; Tamkin et al., 2020; Talmor et al., 2020). Unlike probes, steered heads are trained in parallel to target heads rather than on a fixed model. Moreover, steered heads are not designed to decode specific properties from representations, but their behaviour naturally extends beyond their training objective. Our findings also relate to explainability methods that highlight parts from the input via the model’s attention (Wiegreffe and Pinter, 2019), and extract rationales through unsupervised training (Lei et al., 2016). The emerging explanations we observe are based on the predictions of a head rather than on internal representations. 8 Conclusions and Discussion We show that training multiple heads on top of a pre-trained language model creates a steering effect, where the target head influences the behaviour of another head, steering it towards capabilities 7 Related Work beyond its training objective. In three multi-task Transformer models with multiple output heads settings, we find that without any dedicated trainhave been widely"
2021.emnlp-main.753,2021.tacl-1.4,0,0.0922533,"Missing"
2021.emnlp-main.753,D19-1443,0,0.0542344,"Missing"
2021.emnlp-main.774,2021.naacl-main.225,0,0.051292,"Missing"
2021.emnlp-main.843,2020.acl-main.676,0,0.542834,".d setup. Our code and data can be downloaded from http://github.com/inbaroren/ scfg-sampling-for-comp-gen. 2 Problem Setup We assume access to a small dataset of natural language utterances paired with queries, Dtrain = NL {(xi , zi )}N i=1 , and a large pool of synthetic utterNSyn Syn ances paired with queries Dtrain = {(x0i , zi )}i=1 , Syn where NNL  NSyn . In this work, Dtrain is generated with a synchronous context-free grammar, which provides wide coverage of query structures and tight control over the generated queries, but other methods of generating synthetic examples are possible (Andreas, 2020; Guo et al., 2020a; Wang et al., 2021). Table 1 provides examples of natural language utterances, synthetic utterances, and queries in the ThingTalk language, a language designed for virtual assistants used in this work (through the Schema2QA dataset (Xu et al., 2020a,b)). Our goal is to train a model using Dtrain and Syn Dtrain and generalize to a test set Dtest sampled from the same distribution as Dtrain . More imWe evaluate our approach on a new split of the portantly, our model should generalize to a comComp Schema2QA dataset (Xu et al., 2020a), in which it positional test set, Dtest , w"
2021.emnlp-main.843,2021.acl-long.258,0,0.0274057,"reasing lexical diversity can improve outof-distribution generalization. Compositional Generalization In contrast to our work that focuses on sampling synthetic data, many other approaches have been suggested to improve compositional generalization in semantic parsing. These include new or modified model architectures (Li et al., 2019; Gordon et al., 2020; Guo et al., 2020b; Oren et al., 2020; Zheng and Lapata, 2020; Herzig and Berant, 2021; Shaw et al., 2020), pre-trained language models (Furrer et al., 2020), intermediate representations (Herzig et al., 2021), and meta learning (Lake, 2019; Conklin et al., 2021). approach utilizes the structural nature of executable queries, and focuses on biases related to structural diversity. 6 Conclusion In this work, we for the first time explore whether generating large amounts of synthetic data from a synchronous grammar improves compositional generalization, and propose sampling methods that allow for more efficient training by generating structurally-diverse training sets. We find that synthetic data dramatically improves generalization, and moderately improves i.i.d generalization, and that by uniformly sampling abstract templates, we can improve data effic"
2021.emnlp-main.843,2021.naacl-main.105,0,0.0790772,"Missing"
2021.emnlp-main.843,P18-1033,0,0.123721,"1 Introduction inductive bias (Liu et al., 2020; Russin et al., 2020; Semantic parsers map natural language utterances Zheng and Lapata, 2020; Herzig and Berant, 2021), to executable programs (Zelle and Mooney, 1996; and (b) adding training data that will encourage a Zettlemoyer and Collins, 2005). A worrying weak- compositional solution (Akyürek et al., 2021; Guo ness of semantic parsers that has been recently ex- et al., 2020c; Wang et al., 2021; Guo et al., 2020a). posed, is their inability to generalize at test time In the latter approach, typically a model is trained to new compositions (Finegan-Dollak et al., 2018; from labeled data (Jia and Liang, 2016; Yu et al., Lake and Baroni, 2018; Keysers et al., 2020; Kim 2020; Zhong et al., 2020), and is used to later generand Linzen, 2020; Gu et al., 2021). For example, ate new examples. An alternative approach is to use a virtual assistant trained on the examples “Show a manually-built synchronous grammar that autome Thai restaurants that allow pets” and “How matically generates programs paired with synthetic many hotels are there in Tokyo”, might not gener- utterances (Wang et al., 2015; Cheng et al., 2018; alize to “How many hotels in Tokyo allow pets?”. W"
2021.emnlp-main.843,P16-1002,0,0.149588,"Russin et al., 2020; Semantic parsers map natural language utterances Zheng and Lapata, 2020; Herzig and Berant, 2021), to executable programs (Zelle and Mooney, 1996; and (b) adding training data that will encourage a Zettlemoyer and Collins, 2005). A worrying weak- compositional solution (Akyürek et al., 2021; Guo ness of semantic parsers that has been recently ex- et al., 2020c; Wang et al., 2021; Guo et al., 2020a). posed, is their inability to generalize at test time In the latter approach, typically a model is trained to new compositions (Finegan-Dollak et al., 2018; from labeled data (Jia and Liang, 2016; Yu et al., Lake and Baroni, 2018; Keysers et al., 2020; Kim 2020; Zhong et al., 2020), and is used to later generand Linzen, 2020; Gu et al., 2021). For example, ate new examples. An alternative approach is to use a virtual assistant trained on the examples “Show a manually-built synchronous grammar that autome Thai restaurants that allow pets” and “How matically generates programs paired with synthetic many hotels are there in Tokyo”, might not gener- utterances (Wang et al., 2015; Cheng et al., 2018; alize to “How many hotels in Tokyo allow pets?”. Weir et al., 2020). Using a grammar allow"
2021.emnlp-main.843,2020.emnlp-main.731,0,0.0802086,"Missing"
2021.emnlp-main.843,2020.acl-main.703,0,0.0165045,"mple-efficient? Specifˆ Syn , such that ically, can we sample a smaller set D train ˆ Syn | |DSyn |and still improve compositional |D train train generalization. Last, can we do the above while preserving or improving generalization to the i.i.d test set, Dtest ? Answering these questions will be the focus of §3 and §4. 3 Sampling a Structurally-diverse Training Set We first succinctly describe our model and training procedure (§3.1) and then turn to methods for sampling structurally-diverse training sets. 3.1 Model and Training In this work, we start from a pre-trained encoderdecoder model (Lewis et al., 2020), as such models have been shown to provide a good initialization for fine-tuning semantic parsers (Furrer et al., 2020; Herzig et al., 2021). We then train our model in 1 Preliminary experiments on synthetic data pointed that two steps (Yu et al., 2020; Wang et al., 2021). First, abstracting entities only might not lead to a compositional ˆ Syn , split that is challenging enough. on synthetic utterance-query pairs (x0 , z) ∈ D train 10795 Example x0 : z: zabs : please search the hotels with pets allowed ( @Hotel ) filter patsAllowed:Boolean == true ( @table ) filter property:type op entity x0"
2021.emnlp-main.843,D19-1438,0,0.0162425,"-translation (Guo et al., 2020c). Conversely, we generate data from an independent wide-coverage grammar and investigate data-efficient sampling through structured diversity. Outside of semantic parsing, it has been shown in a grounded learning setup (Hill et al., 2020) that increasing lexical diversity can improve outof-distribution generalization. Compositional Generalization In contrast to our work that focuses on sampling synthetic data, many other approaches have been suggested to improve compositional generalization in semantic parsing. These include new or modified model architectures (Li et al., 2019; Gordon et al., 2020; Guo et al., 2020b; Oren et al., 2020; Zheng and Lapata, 2020; Herzig and Berant, 2021; Shaw et al., 2020), pre-trained language models (Furrer et al., 2020), intermediate representations (Herzig et al., 2021), and meta learning (Lake, 2019; Conklin et al., 2021). approach utilizes the structural nature of executable queries, and focuses on biases related to structural diversity. 6 Conclusion In this work, we for the first time explore whether generating large amounts of synthetic data from a synchronous grammar improves compositional generalization, and propose sampling"
2021.emnlp-main.843,2020.findings-emnlp.225,1,0.86557,"hotels having pets allowed ? ( @Hotel ) filter petsAllowed: ( @table ) filter property: Boolean type == op true entity Table 1: Examples from Schema2QA of utterance-query pairs (x,z) with their synthetic utterances (x0 ), in the books and hotels domains. In the bottom example, the abstract template (zabs ) is included. Queries are in abbreviated syntax for clarity. Compositional split We define a compositional split following the abstract template split proposed by Finegan-Dollak et al. (2018), which is commonly used for assessing compositional generalization (Lee et al., 2019; Andreas, 2020; Oren et al., 2020). In this approach, queries are abstracted into templates that correspond to different structures. Templates are then partitioned into disjoint sets (train/development/test), which ensures that test time structures are never observed at training time. While prior work focused on abstracting entities only, by replacing any DB entity with the token entity, in this work we abstract queries into more coarse templates, e.g, table constants are replaced by the token table. Table 2 lists all abstracted query parts and their corresponding abstract token, and Table 1 (bottom) shows an example query z a"
2021.emnlp-main.843,2020.acl-srw.42,0,0.0540481,"Missing"
2021.emnlp-main.843,2020.spnlp-1.3,0,0.0350782,"elated Work synchronous grammar is that we can easily sample a large number of synthetic examples that cover a Data augmentation Previous work studied difwider range of structures. ferent data augmentation techniques to improve NL-S CHEMAO RG comprises data from 6 differ- i.i.d generalization in semantic parsing including ent domains. In Table 10 in Appendix F, we show synchronous grammars (Jia and Liang, 2016; Yu development EM per domain. While the number of et al., 2020; Xu et al., 2020b), target side grammars examples in each domain is small (a few dozen ex- with neural generation models (Tran and Tan, 2020; amples per domain), we still observe similar trends Wang et al., 2021), and pre-training with auxiliary across all domains. tasks (Yin et al., 2020; Deng et al., 2021). In the To summarize, our results suggest that abstract context of compositional generalization, data aug10800 Method U NIFORM +5k +60k +120k +1M CM AX E NT +5k +60k UAT +5k +60k Benign Errors (a) Linking Errors Structural Errors (b) Consistent Queries 7 8 10 11 63 84 76 63 30 8 14 26 15 27 31 37 8 9 62 52 31 39 24 31 8 0 92 91 0 9 33 32 Table 6: Error analysis. (a) a categorization of 40 predictions on the compositional devel"
2021.emnlp-main.843,2021.naacl-main.220,0,0.458854,"sitional generalization. Two high-level approaches have been considered for tackling compositional generalization: (a) designing models with a stronger compositional 1 Introduction inductive bias (Liu et al., 2020; Russin et al., 2020; Semantic parsers map natural language utterances Zheng and Lapata, 2020; Herzig and Berant, 2021), to executable programs (Zelle and Mooney, 1996; and (b) adding training data that will encourage a Zettlemoyer and Collins, 2005). A worrying weak- compositional solution (Akyürek et al., 2021; Guo ness of semantic parsers that has been recently ex- et al., 2020c; Wang et al., 2021; Guo et al., 2020a). posed, is their inability to generalize at test time In the latter approach, typically a model is trained to new compositions (Finegan-Dollak et al., 2018; from labeled data (Jia and Liang, 2016; Yu et al., Lake and Baroni, 2018; Keysers et al., 2020; Kim 2020; Zhong et al., 2020), and is used to later generand Linzen, 2020; Gu et al., 2021). For example, ate new examples. An alternative approach is to use a virtual assistant trained on the examples “Show a manually-built synchronous grammar that autome Thai restaurants that allow pets” and “How matically generates progra"
2021.emnlp-main.843,P15-1129,1,0.845394,"Missing"
2021.emnlp-main.843,2020.emnlp-main.31,0,0.520868,"Cheng et al., 2018; alize to “How many hotels in Tokyo allow pets?”. Weir et al., 2020). Using a grammar allows generThis type of out-of-domain generalization to new ating large amounts of synthetic data that cover a 10793 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10793–10809 c November 7–11, 2021. 2021 Association for Computational Linguistics wide range of program structures. This has been shown to be useful in the i.i.d setup, and combined with paraphrase models, has led to high-accuracy parsers that are trained from synthetic data only (Xu et al., 2020b). In this work, we investigate the potential of using synthetic data, generated from a synchronous grammar, to improve compositional generalization. Tsarkov et al. (2020) have shown that large quantities of such data improve compositional generalization. However, they evaluated on synthetic utterances only, and did not examine generalization to natural language. Moreover, error rates were high in some compositional splits even when the training set was as large as 1M examples. In this work, we ask whether we can strategically sample a small and structurally-diverse training set and improve c"
2021.emnlp-main.843,2020.acl-main.745,0,0.0185134,"difwider range of structures. ferent data augmentation techniques to improve NL-S CHEMAO RG comprises data from 6 differ- i.i.d generalization in semantic parsing including ent domains. In Table 10 in Appendix F, we show synchronous grammars (Jia and Liang, 2016; Yu development EM per domain. While the number of et al., 2020; Xu et al., 2020b), target side grammars examples in each domain is small (a few dozen ex- with neural generation models (Tran and Tan, 2020; amples per domain), we still observe similar trends Wang et al., 2021), and pre-training with auxiliary across all domains. tasks (Yin et al., 2020; Deng et al., 2021). In the To summarize, our results suggest that abstract context of compositional generalization, data aug10800 Method U NIFORM +5k +60k +120k +1M CM AX E NT +5k +60k UAT +5k +60k Benign Errors (a) Linking Errors Structural Errors (b) Consistent Queries 7 8 10 11 63 84 76 63 30 8 14 26 15 27 31 37 8 9 62 52 31 39 24 31 8 0 92 91 0 9 33 32 Table 6: Error analysis. (a) a categorization of 40 predictions on the compositional development set, selected at random. Errors are partitioned by similar patterns to: Benign errors: the prediction is different but semantically equivalent"
2021.naacl-main.29,D13-1160,1,0.650395,"t spans. 4 mantic parsing, where a semantic parser answers We compute this through an efficient tree hashing procedure. See Appendix A. a sequence of questions. For example, given the 316 questions “How many students are living in the dorms?” and then “what are their last names?”, the pronoun “their” refers to a sub-tree from the SQL tree of the first question. Having a representation for such sub-trees can be useful when parsing the second question, in benchmarks such as SPARC (Yu et al., 2019). Another potential benefit of bottom-up parsing is that sub-queries can be executed while parsing (Berant et al., 2013; Liang et al., 2017), which can guide the search procedure. Recently, Odena et al. (2020) proposed such an approach for program synthesis, and showed that conditioning on the results of execution can improve performance. We do not explore this advantage of bottom-up parsing in this work, since executing queries at training time leads to a slow-down during training. S M B O P is a bottom-up semi-autoregressive parser, but it could potentially be modified to be autoregressive by decoding one tree at a time. Past work (Cheng et al., 2019) has shown that the performance of bottom-up and top-down"
2021.naacl-main.29,J19-1002,0,0.0147395,"M B O P leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. S M B O P obtains 71.1 denotation accuracy on S PIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+G RAPPA. 1 Introduction Jonathan Berant Tel Aviv University Allen Institute for AI joberant@cs.tau.ac.il top-down manner (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Bottom-up decoding in semantic parsing has received little attention (Cheng et al., 2019; Odena et al., 2020). In this work, we propose a bottom-up semantic parser, and demonstrate that equipped with recent developments in Transformer-based (Vaswani et al., 2017) architectures, it offers several advantages. From an efficiency perspective, bottom-up parsing can naturally be done semiautoregressively: at each decoding step t, the parser generates in parallel the top-K program sub-trees of depth ≤ t (akin to beam search). This leads to runtime complexity that is logarithmic in the tree size, rather than linear, contributing to the rocketing interest in efficient and greener artifici"
2021.naacl-main.29,2020.acl-main.398,0,0.0190219,"ining time, the entire model is trained jointly using maximum likelihood. We evaluate our model, S M B O P1 (SeMiautoregressive Bottom-up semantic Parser), on S PI - Semantic parsing, the task of mapping natural language utterances into programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al.; Liang et al., 2011), has converged in recent years on a standard encoder-decoder architecture. Recently, meaningful advances emerged on the encoder side, including developments in Transformer-based architectures (Wang et al., 2020a) and new pretraining techniques (Yin et al., 2020; Herzig et al., 2020; Yu et al., 2020; Deng et al., 2020; Shi et al., 2021). Conversely, the decoder has remained roughly constant for years, where the abstract syntax tree of the target program is autoregressively decoded in a 311 1 Rhymes with ‘MMMBop’. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 311–324 June 6–11, 2021. ©2021 Association for Computational Linguistics name actor actor 60 age 60 Represent-beam Prune frontier ... Score-frontier Cross Attention ... What are the names of actors over 60? name ac"
2021.naacl-main.29,D17-1160,0,0.102007,"ly-vacuous partial trees. We apply S M B O P on S PIDER, a challenging zero-shot semantic parsing benchmark, and show that S M B O P leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. S M B O P obtains 71.1 denotation accuracy on S PIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+G RAPPA. 1 Introduction Jonathan Berant Tel Aviv University Allen Institute for AI joberant@cs.tau.ac.il top-down manner (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Bottom-up decoding in semantic parsing has received little attention (Cheng et al., 2019; Odena et al., 2020). In this work, we propose a bottom-up semantic parser, and demonstrate that equipped with recent developments in Transformer-based (Vaswani et al., 2017) architectures, it offers several advantages. From an efficiency perspective, bottom-up parsing can naturally be done semiautoregressively: at each decoding step t, the parser generates in parallel the top-K program sub-trees of depth ≤ t (akin to beam search). This leads to runtime complexity that is logari"
2021.naacl-main.29,P17-1003,1,0.846172,"sing, where a semantic parser answers We compute this through an efficient tree hashing procedure. See Appendix A. a sequence of questions. For example, given the 316 questions “How many students are living in the dorms?” and then “what are their last names?”, the pronoun “their” refers to a sub-tree from the SQL tree of the first question. Having a representation for such sub-trees can be useful when parsing the second question, in benchmarks such as SPARC (Yu et al., 2019). Another potential benefit of bottom-up parsing is that sub-queries can be executed while parsing (Berant et al., 2013; Liang et al., 2017), which can guide the search procedure. Recently, Odena et al. (2020) proposed such an approach for program synthesis, and showed that conditioning on the results of execution can improve performance. We do not explore this advantage of bottom-up parsing in this work, since executing queries at training time leads to a slow-down during training. S M B O P is a bottom-up semi-autoregressive parser, but it could potentially be modified to be autoregressive by decoding one tree at a time. Past work (Cheng et al., 2019) has shown that the performance of bottom-up and top-down autoregressive parser"
2021.naacl-main.29,N19-1423,0,0.00899833,"e of the tokens is an utterance word and another is a table column, a relation will denote if there is a string match between them. The same principle is also applied for representing the self-attention values, where another relation embedding matrix is used. We refer the reader to the RAT-SQL paper for details. Overall, RAT-SQL jointly encodes the utterance, schema, the structure of the schema and alignments between the utterance and schema, and leads to state-of-the-art results in text-to-SQL parsing. RAT-SQL layers are typically stacked on top of a pre-trained language model, such as BERT (Devlin et al., 2019). In this work, we use G RAPPA (Yu et al., 2020), a recent pre-trained model that has obtained state-of-the-art results in text-to-SQL parsing. G RAPPA is based on RO BERTA (Liu et al., 2019), but is further fine-tuned on synthetically generated utterance-query pairs using an objective for aligning the utterance and query. Algorithm 1: S M B O P 8 input: utterance x, schema S x, s ← EncodeRAT (x, S) Z0 ← Top-K schema constants and DB values for t ← 0 . . . T − 1 do Zt0 ← Attention(Zt , x, x) Ft+1 ← Score-frontier(Zt0 ) 0 Ft+1 ← arg maxK (Ft+1 ) 0 Zt+1 ← Represent-beam(Zt , Ft+1 ) 9 return arg"
2021.naacl-main.29,P16-1004,0,0.050081,"Missing"
2021.naacl-main.29,W18-2501,0,0.0171787,"ntion that computes Zt0 and uses the representations in Zt directly to score the frontier. In this setup, the decoder only observes the input question through the 0-high trees in Z0 . • W ITH C NTX R EP.: We use the contextualized representations not only for scoring, but also as input for creating the new trees Zt+1 . This tests if contextualized representations on the beam hurt or improve performance. Experimental setup We encode the input utterance x and the schema S with G RAPPA, consisting of 24 Transformer layers, followed by another 8 RAT-SQL layers, which we implement inside AllenNLP (Gardner et al., 2018). Our beam size is K = 30, and the number of decoding steps is T = 9 at inference time, which is the maximal tree depth on the development set. The transformer used for tree representations has one layer, 8 heads, and dimensionality 256. We train for 60K steps with batch size 60, and perform early stopping based on the development set. Evaluation We evaluate performance with the official S PIDER evaluation script, which computes exact match (EM), i.e., whether the predicted SQL query is identical to the gold query after some query normalization. The evaluation script uses Model RAT-SQL+GP+G RA"
2021.naacl-main.29,P18-1168,1,0.89511,"Missing"
2021.naacl-main.29,P19-1444,0,0.011853,"not change the semantics of the subtree it is applied on. Hence, we can always grow the height of trees in the beam without changing the formal query. For training (which we elaborate on in §3.4), we balance all relational algebra trees in the training set using the KEEP operation, such that the distance from the root to all leaves is equal. For example, in Figure 2, two K EEP operations are used to balance the column actor.name. After tree balancing, all constants and values are at height 0, and the goal of the parser at step t is to generate the gold set of t-high trees. Relational algebra Guo et al. (2019) have shown recently that the mismatch between natural language and SQL leads to parsing difficulties. Therefore, they proposed SemQL, a formal query language with better alignment to natural language. In this work, we follow their intuition, but instead of SemQL, we use the standard query language relational algebra (Codd, 1970). Relational algebra describes queries as trees, where leaves (terminals) are schema constants or DB values, and inner nodes (non-terminals) are operations (see Table 1). Similar to SemQL, its alignment with natural language is better than SQL. However, unlike SemQL, i"
2021.naacl-main.29,P11-1060,0,0.0608812,"m, and the top-K trees are kept (purple). Last, a representation for each of the new K trees is generated and placed in the new beam Zt+1 . After T decoding steps, the parser returns the highest-scoring tree in ZT that corresponds to a full program. Because we have gold trees at training time, the entire model is trained jointly using maximum likelihood. We evaluate our model, S M B O P1 (SeMiautoregressive Bottom-up semantic Parser), on S PI - Semantic parsing, the task of mapping natural language utterances into programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al.; Liang et al., 2011), has converged in recent years on a standard encoder-decoder architecture. Recently, meaningful advances emerged on the encoder side, including developments in Transformer-based architectures (Wang et al., 2020a) and new pretraining techniques (Yin et al., 2020; Herzig et al., 2020; Yu et al., 2020; Deng et al., 2020; Shi et al., 2021). Conversely, the decoder has remained roughly constant for years, where the abstract syntax tree of the target program is autoregressively decoded in a 311 1 Rhymes with ‘MMMBop’. Proceedings of the 2021 Conference of the North American Chapter of the Associati"
2021.naacl-main.29,2020.findings-emnlp.438,0,0.0140058,"oken of a DB value, Pstart (xi ) ∝ exp(w> start xi ), and similarly the probability that a token is the end token of a DB value, Pend (xi ) ∝ exp(w> end xi ), where wstart and wend are parameter vectors. We define the probability of a span (xi , . . . , xj ) to be Pstart (xi ) · Pend (xj ), and place in the beam Z0 the top- K2 input spans, where the representation of a span (xi , xj ) is the average of xi and xj . A current limitation of S M B O P is that it cannot generate DB values that do not appear in the input question. This would require adding a mechanism such as “ BRIDGE ” proposed by Lin et al. (2020). 3.4 Training To specify the loss function, we need to define the supervision signal. Recall that given the gold SQL program, we convert it into a gold balanced relational algebra tree z gold , as explained in §3.1 and Figure 2. This lets us define for every decoding gold step the set of t-high gold sub-trees Zt . For gold example Z0 includes all gold schema constants gold and input spans that match a gold DB value,3 Z1 includes all 1-high gold trees, etc. During training, we apply “bottom-up Teacher Forcing” (Williams and Zipser, 1989), that is, we gold populate4 the beam Zt with all trees f"
2021.naacl-main.29,2021.ccl-1.108,0,0.0312131,"Missing"
2021.naacl-main.29,P17-1105,0,0.126317,"apply S M B O P on S PIDER, a challenging zero-shot semantic parsing benchmark, and show that S M B O P leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. S M B O P obtains 71.1 denotation accuracy on S PIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+G RAPPA. 1 Introduction Jonathan Berant Tel Aviv University Allen Institute for AI joberant@cs.tau.ac.il top-down manner (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Bottom-up decoding in semantic parsing has received little attention (Cheng et al., 2019; Odena et al., 2020). In this work, we propose a bottom-up semantic parser, and demonstrate that equipped with recent developments in Transformer-based (Vaswani et al., 2017) architectures, it offers several advantages. From an efficiency perspective, bottom-up parsing can naturally be done semiautoregressively: at each decoding step t, the parser generates in parallel the top-K program sub-trees of depth ≤ t (akin to beam search). This leads to runtime complexity that is logarithmic in the tree size, ra"
2021.naacl-main.29,D16-1264,0,0.00851107,"rees. While scoring is done with tion fconst (·). Recall that si is a representation of a 315 constant, contextualized by the rest of the schema and the utterance. The function fconst (·) is a feedforward network that scores each schema constant independently: fconst (si ) = wconst tanh (Wconst si ), and the top- K2 constants are placed in Z0 . DB values Because the number of values in the DB is potentially huge, we do not score all DB values. Instead, we learn to detect spans in the question that correspond to DB values. This leads to a setup that is similar to extractive question answering (Rajpurkar et al., 2016), where the model outputs a distribution over input spans, and thus we adopt the architecture commonly used in extractive question answering. Concretely, we compute the probability that a token is the start token of a DB value, Pstart (xi ) ∝ exp(w> start xi ), and similarly the probability that a token is the end token of a DB value, Pend (xi ) ∝ exp(w> end xi ), where wstart and wend are parameter vectors. We define the probability of a span (xi , . . . , xj ) to be Pstart (xi ) · Pend (xj ), and place in the beam Z0 the top- K2 input spans, where the representation of a span (xi , xj ) is t"
2021.naacl-main.29,N18-2074,0,0.0221579,"(i) N (i) {(x , y , S )}i=1 , where x is an utterance, in a final contextualized representation (x, s) = y (i) is its translation to a SQL query, and S (i) is the (x1 , . . . , x|x |, s1 , . . . , s|s |), where si is a vector representing a single schema constant. This contextuschema of the target database (DB), our goal is to alization of x and S leads to better representation learn a model that maps new question-schema pairs and alignment between the utterance and schema. 2 Our code is available at https://github.com/ OhadRubin/SmBop Second, RAT-SQL uses relational-aware self312 attention (Shaw et al., 2018) to encode the structure of the schema and other prior knowledge on relations between encoded tokens. Specifically, given a sequence of token representations (u1 , . . . , u|u |), relational-aware self-attention computes a scalar similarity score between pairs of token representations eij ∝ ui WQ (uj WK + rK ij ). This is identical to standard self-attention (WQ and WK are the query and key parameter matrices), except for the term rK ij , which is an embedding that represents a relation between ui and uj from a closed set of possible relations. For example, if both tokens correspond to schema"
2021.naacl-main.29,2020.acl-main.677,0,0.384226,"that corresponds to a full program. Because we have gold trees at training time, the entire model is trained jointly using maximum likelihood. We evaluate our model, S M B O P1 (SeMiautoregressive Bottom-up semantic Parser), on S PI - Semantic parsing, the task of mapping natural language utterances into programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al.; Liang et al., 2011), has converged in recent years on a standard encoder-decoder architecture. Recently, meaningful advances emerged on the encoder side, including developments in Transformer-based architectures (Wang et al., 2020a) and new pretraining techniques (Yin et al., 2020; Herzig et al., 2020; Yu et al., 2020; Deng et al., 2020; Shi et al., 2021). Conversely, the decoder has remained roughly constant for years, where the abstract syntax tree of the target program is autoregressively decoded in a 311 1 Rhymes with ‘MMMBop’. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 311–324 June 6–11, 2021. ©2021 Association for Computational Linguistics name actor actor 60 age 60 Represent-beam Prune frontier ... Score-fr"
2021.naacl-main.29,P19-1447,0,0.0287035,"Missing"
2021.naacl-main.29,2020.acl-main.745,0,0.010684,"gold trees at training time, the entire model is trained jointly using maximum likelihood. We evaluate our model, S M B O P1 (SeMiautoregressive Bottom-up semantic Parser), on S PI - Semantic parsing, the task of mapping natural language utterances into programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al.; Liang et al., 2011), has converged in recent years on a standard encoder-decoder architecture. Recently, meaningful advances emerged on the encoder side, including developments in Transformer-based architectures (Wang et al., 2020a) and new pretraining techniques (Yin et al., 2020; Herzig et al., 2020; Yu et al., 2020; Deng et al., 2020; Shi et al., 2021). Conversely, the decoder has remained roughly constant for years, where the abstract syntax tree of the target program is autoregressively decoded in a 311 1 Rhymes with ‘MMMBop’. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 311–324 June 6–11, 2021. ©2021 Association for Computational Linguistics name actor actor 60 age 60 Represent-beam Prune frontier ... Score-frontier Cross Attention ... What are the names of ac"
2021.naacl-main.29,D18-1425,0,0.0127275,"ics name actor actor 60 age 60 Represent-beam Prune frontier ... Score-frontier Cross Attention ... What are the names of actors over 60? name actor age 60 Figure 1: An overview of the decoding procedure of S M B O P. Zt is is the beam at step t, Zt0 is the contextualized beam after cross-attention, Ft+1 is the frontier (κ, σ, ≥ are logical operations applied on trees, as explained below), 0 Ft+1 is the pruned frontier, and Zt+1 is the new beam. At the top we see the new trees created in this step. For t = 0 (depicted here), the beam contains the predicted schema constants and DB values. DER (Yu et al., 2018), a challenging zero-shot text-to-SQL dataset. We implement the RATSQL+G RAPPA encoder (Yu et al., 2020), currently the best model on S PIDER, and replace the autoregressive decoder with the semi-autoregressive S M B O P. S M B O P obtains an exact match accuracy of 69.5, comparable to the autoregressive RATSQL+G RAPPA at 69.6 exact match, and to current state-of-the-art at 69.8 exact match (Zhao et al., 2021), which applies additional pretraining. Moreover, S M B O P substantially improves state-of-theart in denotation accuracy, improving performance from 68.3 → 71.1. Importantly, compared to"
2021.naacl-main.29,P17-1041,0,0.138147,"er than for semantically-vacuous partial trees. We apply S M B O P on S PIDER, a challenging zero-shot semantic parsing benchmark, and show that S M B O P leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. S M B O P obtains 71.1 denotation accuracy on S PIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+G RAPPA. 1 Introduction Jonathan Berant Tel Aviv University Allen Institute for AI joberant@cs.tau.ac.il top-down manner (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Bottom-up decoding in semantic parsing has received little attention (Cheng et al., 2019; Odena et al., 2020). In this work, we propose a bottom-up semantic parser, and demonstrate that equipped with recent developments in Transformer-based (Vaswani et al., 2017) architectures, it offers several advantages. From an efficiency perspective, bottom-up parsing can naturally be done semiautoregressively: at each decoding step t, the parser generates in parallel the top-K program sub-trees of depth ≤ t (akin to beam search). This leads to runti"
2021.spnlp-1.2,2020.acl-main.398,0,0.0243574,"uracy on S PIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+G RAPPA. 1 Introduction Semantic parsing, the task of mapping natural language utterances into programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al.; Liang et al., 2011), has converged in recent years on a standard encoder-decoder architecture. Recently, meaningful advances emerged on the encoder side, including developments in Transformer-based architectures (Wang et al., 2020a) and new pretraining techniques (Yin et al., 2020; Herzig et al., 2020; Yu et al., 2020; Deng et al., 2020; Shi et al., 2021). Conversely, the decoder has remained roughly constant for years, where the abstract syntax tree of the target program is autoregressively decoded in a 1 2 Originally published in NAACL 2021. Rhymes with ‘MMMBop’. 12 Proceedings of the 5th Workshop on Structured Prediction for NLP, pages 12–21 August 1–6, 2021. ©2021 Association for Computational Linguistics name actor actor 60 age 60 Represent-beam Prune frontier ... Score-frontier Cross Attention ... What are the names of actors over 60? name actor age 60 Figure 1: An overview of the de"
2021.spnlp-1.2,D13-1160,1,0.654199,"through an efficient tree hashing procedure. See Appendix A. 17 Model RAT-SQL+GP+G RAPPA RAT-SQL+GAP RAT-SQL+G RAPPA RAT-SQL+STRUG BRIDGE+BERT (ensemble) RAT-SQLv3+BERT S M B O P+G RAPPA questions “How many students are living in the dorms?” and then “what are their last names?”, the pronoun “their” refers to a sub-tree from the SQL tree of the first question. Having a representation for such sub-trees can be useful when parsing the second question, in benchmarks such as SPARC (Yu et al., 2019). Another potential benefit of bottom-up parsing is that sub-queries can be executed while parsing (Berant et al., 2013; Liang et al., 2017), which can guide the search procedure. Recently, Odena et al. (2020) proposed such an approach for program synthesis, and showed that conditioning on the results of execution can improve performance. We do not explore this advantage of bottom-up parsing in this work, since executing queries at training time leads to a slow-down during training. S M B O P is a bottom-up semi-autoregressive parser, but it could potentially be modified to be autoregressive by decoding one tree at a time. Past work (Cheng et al., 2019) has shown that the performance of bottom-up and top-down"
2021.spnlp-1.2,D17-1160,0,0.0150601,". This is unlike sequence-to-sequence models where items on the beam are competing hypotheses without any interaction. We now provide the details of our parser. First, we describe the formal language (§3.1), then we provide precise details of our model architecture (§3.2) including beam initialization (§3.3, we describe the training procedure (§3.4), and last, we discuss the properties of S M B O P compared to prior work (§3.5). Autoregressive top-down decoding The prevailing method for decoding in semantic parsing has been grammar-based autoregressive top-down decoding (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017), which guarantees decoding of syntactically valid programs. Specifically, the target program is represented as an abstract syntax tree under the grammar of the formal language, and linearized to a sequence of rules (or actions) using a top-down depth-first traversal. Once the program is represented as a sequence, it can be decoded using a standard sequence-tosequence model with encoder attention (Dong and Lapata, 2016), often combined with beam search. We refer the reader to the aforementioned papers for further details on grammar-based decoding. 14 Operation Set Uni"
2021.spnlp-1.2,J19-1002,0,0.0320816,"Missing"
2021.spnlp-1.2,P17-1003,1,0.847865,"tree hashing procedure. See Appendix A. 17 Model RAT-SQL+GP+G RAPPA RAT-SQL+GAP RAT-SQL+G RAPPA RAT-SQL+STRUG BRIDGE+BERT (ensemble) RAT-SQLv3+BERT S M B O P+G RAPPA questions “How many students are living in the dorms?” and then “what are their last names?”, the pronoun “their” refers to a sub-tree from the SQL tree of the first question. Having a representation for such sub-trees can be useful when parsing the second question, in benchmarks such as SPARC (Yu et al., 2019). Another potential benefit of bottom-up parsing is that sub-queries can be executed while parsing (Berant et al., 2013; Liang et al., 2017), which can guide the search procedure. Recently, Odena et al. (2020) proposed such an approach for program synthesis, and showed that conditioning on the results of execution can improve performance. We do not explore this advantage of bottom-up parsing in this work, since executing queries at training time leads to a slow-down during training. S M B O P is a bottom-up semi-autoregressive parser, but it could potentially be modified to be autoregressive by decoding one tree at a time. Past work (Cheng et al., 2019) has shown that the performance of bottom-up and top-down autoregressive parser"
2021.spnlp-1.2,P11-1060,0,0.0602577,"ply S M B O P on S PIDER, a challenging zero-shot semantic parsing benchmark, and show that S M B O P leads to a 2.2x speed-up in decoding time and a ∼5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. S M B O P obtains 71.1 denotation accuracy on S PIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+G RAPPA. 1 Introduction Semantic parsing, the task of mapping natural language utterances into programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al.; Liang et al., 2011), has converged in recent years on a standard encoder-decoder architecture. Recently, meaningful advances emerged on the encoder side, including developments in Transformer-based architectures (Wang et al., 2020a) and new pretraining techniques (Yin et al., 2020; Herzig et al., 2020; Yu et al., 2020; Deng et al., 2020; Shi et al., 2021). Conversely, the decoder has remained roughly constant for years, where the abstract syntax tree of the target program is autoregressively decoded in a 1 2 Originally published in NAACL 2021. Rhymes with ‘MMMBop’. 12 Proceedings of the 5th Workshop on Structure"
2021.spnlp-1.2,2020.findings-emnlp.438,0,0.160256,"oken of a DB value, Pstart (xi ) ∝ exp(w> start xi ), and similarly the probability that a token is the end token of a DB value, Pend (xi ) ∝ exp(w> end xi ), where wstart and wend are parameter vectors. We define the probability of a span (xi , . . . , xj ) to be Pstart (xi ) · Pend (xj ), and place in the beam Z0 the top- K2 input spans, where the representation of a span (xi , xj ) is the average of xi and xj . A current limitation of S M B O P is that it cannot generate DB values that do not appear in the input question. This would require adding a mechanism such as “ BRIDGE ” proposed by Lin et al. (2020). 3.4 Size Depth 700 300 200 100 0 0 20 40 60 80 100 Figure 4: A histogram showing the distribution of the height of relational algebra trees in S PIDER, and the size of equivalent SQL query trees. the gold tree. Because of teacher forcing, the frontier Ft+1 is guaranteed to contain all gold trees gold Zt+1 . We first apply a softmax over all frontier trees p(znew ) = softmax{s(znew )}znew ∈Ft+1 and then maximize the probabilities of gold trees: T 1 X X log p (zt ) C gold t=0 zt ∈Z t where the loss is normalized by C, the total number of summed terms. In the initial beam, Z0 , the probability"
2021.spnlp-1.2,N19-1423,0,0.0100478,"e of the tokens is an utterance word and another is a table column, a relation will denote if there is a string match between them. The same principle is also applied for representing the self-attention values, where another relation embedding matrix is used. We refer the reader to the RAT-SQL paper for details. Overall, RAT-SQL jointly encodes the utterance, schema, the structure of the schema and alignments between the utterance and schema, and leads to state-of-the-art results in text-to-SQL parsing. RAT-SQL layers are typically stacked on top of a pre-trained language model, such as BERT (Devlin et al., 2019). In this work, we use G RAPPA (Yu et al., 2020), a recent pre-trained model that has obtained state-of-the-art results in text-to-SQL parsing. G RAPPA is based on RO BERTA (Liu et al., 2019), but is further fine-tuned on synthetically generated utterance-query pairs using an objective for aligning the utterance and query. Algorithm 1: S M B O P 8 input: utterance x, schema S x, s ← EncodeRAT (x, S) Z0 ← Top-K schema constants and DB values for t ← 0 . . . T − 1 do Zt0 ← Attention(Zt , x, x) Ft+1 ← Score-frontier(Zt0 ) 0 Ft+1 ← arg maxK (Ft+1 ) 0 Zt+1 ← Represent-beam(Zt , Ft+1 ) 9 return arg"
2021.spnlp-1.2,2021.ccl-1.108,0,0.0332239,"Missing"
2021.spnlp-1.2,P16-1004,0,0.0188988,"own decoding The prevailing method for decoding in semantic parsing has been grammar-based autoregressive top-down decoding (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017), which guarantees decoding of syntactically valid programs. Specifically, the target program is represented as an abstract syntax tree under the grammar of the formal language, and linearized to a sequence of rules (or actions) using a top-down depth-first traversal. Once the program is represented as a sequence, it can be decoded using a standard sequence-tosequence model with encoder attention (Dong and Lapata, 2016), often combined with beam search. We refer the reader to the aforementioned papers for further details on grammar-based decoding. 14 Operation Set Union Set Intersection Set difference Selection Cartesian product Projection And Or Comparison Constant Union Order by Group by Limit In/Not In Like/Not Like Aggregation Distinct Keep Notation ∪ ∩  σ × Π ∧ ∨ {≤ , ≥ , = , 6=} t τasc/dsc γ λ ∈, 6∈ ∼, 6∼ Gagg δ κ Input → Output R×R→R R×R→R R×R→R P ×R→R R×R→R C0 × R → R P ×P →P P ×P →P C ×C →P C0 × C0 → C0 C ×R→R C ×R→R C ×R→R C ×R→P C ×C →P C→C C→C Any → Any Π σ name ≥ σ κ κ actor age 60 (a) Unbalanc"
2021.spnlp-1.2,W18-2501,0,0.0385166,"Missing"
2021.spnlp-1.2,P17-1105,0,0.0188381,"-sequence models where items on the beam are competing hypotheses without any interaction. We now provide the details of our parser. First, we describe the formal language (§3.1), then we provide precise details of our model architecture (§3.2) including beam initialization (§3.3, we describe the training procedure (§3.4), and last, we discuss the properties of S M B O P compared to prior work (§3.5). Autoregressive top-down decoding The prevailing method for decoding in semantic parsing has been grammar-based autoregressive top-down decoding (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017), which guarantees decoding of syntactically valid programs. Specifically, the target program is represented as an abstract syntax tree under the grammar of the formal language, and linearized to a sequence of rules (or actions) using a top-down depth-first traversal. Once the program is represented as a sequence, it can be decoded using a standard sequence-tosequence model with encoder attention (Dong and Lapata, 2016), often combined with beam search. We refer the reader to the aforementioned papers for further details on grammar-based decoding. 14 Operation Set Union Set Intersection Set di"
2021.spnlp-1.2,P19-1444,0,0.067981,"he subtree it is applied on. Hence, we can always grow the height of trees in the beam without changing the formal query. For training (which we elaborate on in §3.4), we balance all relational algebra trees in the training set using the KEEP operation, such that the distance from the root to all leaves is equal. For example, in Figure 2, two K EEP operations are used to balance the column actor.name. After tree balancing, all constants and values are at height 0, and the goal of the parser at step t is to generate the gold set of t-high trees. Representation of Query Trees Relational algebra Guo et al. (2019) have shown recently that the mismatch between natural language and SQL leads to parsing difficulties. Therefore, they proposed SemQL, a formal query language with better alignment to natural language. In this work, we follow their intuition, but instead of SemQL, we use the standard query language relational algebra (Codd, 1970). Relational algebra describes queries as trees, where leaves (terminals) are schema constants or DB values, and inner nodes (non-terminals) are operations (see Table 1). Similar to SemQL, its alignment with natural language is better than SQL. However, unlike SemQL, i"
2021.spnlp-1.2,D16-1264,0,0.00874499,"a simple scoring function fconst (·). Recall that si is a representation of a 16 constant, contextualized by the rest of the schema and the utterance. The function fconst (·) is a feedforward network that scores each schema constant independently: fconst (si ) = wconst tanh (Wconst si ), and the top- K2 constants are placed in Z0 . 600 500 400 DB values Because the number of values in the DB is potentially huge, we do not score all DB values. Instead, we learn to detect spans in the question that correspond to DB values. This leads to a setup that is similar to extractive question answering (Rajpurkar et al., 2016), where the model outputs a distribution over input spans, and thus we adopt the architecture commonly used in extractive question answering. Concretely, we compute the probability that a token is the start token of a DB value, Pstart (xi ) ∝ exp(w> start xi ), and similarly the probability that a token is the end token of a DB value, Pend (xi ) ∝ exp(w> end xi ), where wstart and wend are parameter vectors. We define the probability of a span (xi , . . . , xj ) to be Pstart (xi ) · Pend (xj ), and place in the beam Z0 the top- K2 input spans, where the representation of a span (xi , xj ) is t"
2021.spnlp-1.2,N18-2074,0,0.045873,", x|x |, s1 , . . . , s|s |), where si is a vector representing a single schema constant. This contextualization of x and S leads to better representation and alignment between the utterance and schema. Background Problem definition We focus in this work on text-to-SQL semantic parsing. Given a training set (i) is an utterance, {(x(i) , y (i) , S (i) )}N i=1 , where x y (i) is its translation to a SQL query, and S (i) is the schema of the target database (DB), our goal is to 3 Our code is available at https://github.com/ OhadRubin/SmBop 13 Second, RAT-SQL uses relational-aware selfattention (Shaw et al., 2018) to encode the structure of the schema and other prior knowledge on relations between encoded tokens. Specifically, given a sequence of token representations (u1 , . . . , u|u |), relational-aware self-attention computes a scalar similarity score between pairs of token representations eij ∝ ui WQ (uj WK + rK ij ). This is identical to standard self-attention (WQ and WK are the query and key parameter matrices), except for the term rK ij , which is an embedding that represents a relation between ui and uj from a closed set of possible relations. For example, if both tokens correspond to schema"
2021.spnlp-1.2,D18-1425,0,0.314534,"frontier ... Score-frontier Cross Attention ... What are the names of actors over 60? name actor age 60 Figure 1: An overview of the decoding procedure of S M B O P. Zt is is the beam at step t, Zt0 is the contextualized beam after cross-attention, Ft+1 is the frontier (κ, σ, ≥ are logical operations applied on trees, as explained below), 0 Ft+1 is the pruned frontier, and Zt+1 is the new beam. At the top we see the new trees created in this step. For t = 0 (depicted here), the beam contains the predicted schema constants and DB values. autoregressive Bottom-up semantic Parser), on S PI DER (Yu et al., 2018), a challenging zero-shot text-to-SQL dataset. We implement the RATSQL+G RAPPA encoder (Yu et al., 2020), currently the best model on S PIDER, and replace the autoregressive decoder with the semi-autoregressive S M B O P. S M B O P obtains an exact match accuracy of 69.5, comparable to the autoregressive RATSQL+G RAPPA at 69.6 exact match, and to current state-of-the-art at 69.8 exact match (Zhao et al., 2021), which applies additional pretraining. Moreover, S M B O P substantially improves state-of-theart in denotation accuracy, improving performance from 68.3 → 71.1. Importantly, compared to"
2021.spnlp-1.2,2020.acl-main.677,0,0.635976,"at uses autoregressive decoding. S M B O P obtains 71.1 denotation accuracy on S PIDER, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+G RAPPA. 1 Introduction Semantic parsing, the task of mapping natural language utterances into programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al.; Liang et al., 2011), has converged in recent years on a standard encoder-decoder architecture. Recently, meaningful advances emerged on the encoder side, including developments in Transformer-based architectures (Wang et al., 2020a) and new pretraining techniques (Yin et al., 2020; Herzig et al., 2020; Yu et al., 2020; Deng et al., 2020; Shi et al., 2021). Conversely, the decoder has remained roughly constant for years, where the abstract syntax tree of the target program is autoregressively decoded in a 1 2 Originally published in NAACL 2021. Rhymes with ‘MMMBop’. 12 Proceedings of the 5th Workshop on Structured Prediction for NLP, pages 12–21 August 1–6, 2021. ©2021 Association for Computational Linguistics name actor actor 60 age 60 Represent-beam Prune frontier ... Score-frontier Cross Attention ... What are the na"
2021.spnlp-1.2,P17-1041,0,0.0356668,"t were found at step t. This is unlike sequence-to-sequence models where items on the beam are competing hypotheses without any interaction. We now provide the details of our parser. First, we describe the formal language (§3.1), then we provide precise details of our model architecture (§3.2) including beam initialization (§3.3, we describe the training procedure (§3.4), and last, we discuss the properties of S M B O P compared to prior work (§3.5). Autoregressive top-down decoding The prevailing method for decoding in semantic parsing has been grammar-based autoregressive top-down decoding (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017), which guarantees decoding of syntactically valid programs. Specifically, the target program is represented as an abstract syntax tree under the grammar of the formal language, and linearized to a sequence of rules (or actions) using a top-down depth-first traversal. Once the program is represented as a sequence, it can be decoded using a standard sequence-tosequence model with encoder attention (Dong and Lapata, 2016), often combined with beam search. We refer the reader to the aforementioned papers for further details on grammar-based de"
2021.sustainlp-1.5,D18-1045,0,0.0134401,"rs after casting them into the familiar query-key-value framework. We evaluate the quality of top-k approximation for multi-head attention layers on the Long Range Arena Benchmark, and for feedforward layers of T5 and UnifiedQA on multiple QA datasets. We show our approach leads to accuracy that is nearly-identical to vanilla attention in multiple setups including training from scratch, fine-tuning, and zero-shot inference. 1 Introduction The Transformer architecture (Vaswani et al., 2017) has been successful in a wide range of natural language processing tasks, including machine translation (Edunov et al., 2018), language modeling (Roy et al., 2021), question-answering (Karpukhin et al., 2020), and many more. Transformers pre-trained on large amounts of text with a language modeling (LM) objective, have become the standard in ∗ majority of work done while author was part of IBM AI Residency program. 39 Proceedings of the 2nd Workshop on Simple and Efficient Natural Language Processing, pages 39–52 November 10, 2021. ©201 Association for Computational Linguistics vanilla Performer query-chunking (C=1024) 30 memory (GiB) 20 10 0 2K 4K 6K 8K 10 0 10K 0.40 0.35 0.30 0.25 0.20 top-k-query-chunking (C=1024"
2021.sustainlp-1.5,N10-1000,0,0.132652,"Missing"
2021.sustainlp-1.5,D19-1250,0,0.0383645,"Missing"
2021.sustainlp-1.5,Q18-1023,0,0.0230694,"Missing"
2021.sustainlp-1.5,D17-1082,0,0.0244753,"Missing"
2021.sustainlp-1.5,2020.findings-emnlp.232,0,0.168936,"ue vectors Q, K, V respectively, the dot-product attention function outputs softmax(QK > )V where the softmax function is applied row-wise on the matrix QK > ∈ RL×L of similarity scores of the querykey pairs, leading to an expensive Ω(L2 ) memory requirement. To alleviate this, past work proposed approximation methods for the computation of softmax(QK > ). One major line of research focused on sparse attention variants, where only a few similarity scores are computed per query, and the rest are ignored. Methods differ by which query-key pairs are selected (Child et al., 2019; Ye et al., 2019; Qiu et al., 2020; Roy et al., 2021; Kitaev et al., 2020; Beltagy et al., 2020; Gupta and Berant, 2020; Vyas et al., 2020). A second line of research explored dense variants (Katharopoulos et al., 2020; Wang et al., 2020; Bello, 2021; Tay et al., 2020a) (cf. (Tay et al., 2020b) for a survey). For example, instead of computing the attention scores exactly for only a small number of querykey pairs, (Choromanski et al., 2021) compute an approximation of scores for all pairs. In this work, we adopt the sparse attention approach, but rather than approximating the k most similar key vectors per query vector, we comp"
2021.sustainlp-1.5,D19-5808,0,0.0216781,"Missing"
2021.sustainlp-1.5,P11-1015,0,0.0227684,"at multi-head attention layers. ing zero-shot inference using pre-trained language models (U NIFIED QA) without any training (§4.3). 4.1 Long Range Arena Long Range Arena (Tay et al., 2021) is a recently established benchmark for evaluating the ability of Transformer variants to handle long sequences. It comprises of multiple text classification tasks with inputs containing thousands of tokens (Table 1). In ListOps (Nangia and Bowman, 2018), given a sequence of operations on single-digit integers, the model predicts a single-digit solution modeled as 10-way classification. IMDb movie reviews (Maas et al., 2011) is a character-level binary sentiment classification task. Lastly, in the ACL Anthology Network (AAN) (Radev et al., 2013) task, a character-level model classifies if there is a citation between a pair of papers. For each task, we downloaded and directly used the vanilla Transformer code offered by the authors (Tay et al., 2021) and compared the performance before and after replacing the multi-head attention layers with top-128 attention, using identical hyperparameters for both cases (details in §A.1).2 Test accuracy measured at the training checkpoint with the highest accuracy on the develo"
2021.sustainlp-1.5,P18-2124,0,0.0434834,"Missing"
2021.sustainlp-1.5,D16-1264,0,0.0123558,"t OBQA ROPES 0.5 35.7 36.6 17.0 21.7 82.0 45.0 85.3 59.6 26.3 83.3 61.9 81.2 58.8 54.0 83.1 62.0 79.4 58.0 53.8 Table 3: Exact match scores on development sets. “Finetuning” denotes model was finetuned on the dataset, else it was evaluated directly without any training. All models use vanilla feed-forward layers except the ones that say top-256 (§4.5). Zero-shot Inference with BERT To verify that the plug-and-play property of topk attention also holds at self-attention layers, we downloaded a BERT-large-uncased-whole-wordmasking checkpoint (Devlin et al., 2019) already fine-tuned on SQuAD v1 (Rajpurkar et al., 2016) To summarize, our experiments in §4.1-§4.5 demonstrated that the performance of vanilla attention and top-k attention is comparable at both multi-head attention (§4.1, §4.4) and feed-forward 46 5 Discussion top-k-query-chunking (C=16384, k=512) (dense x dense) top-k-query-chunking (C=16384, k=512) (torch. sparse_coo_tensor x dense) 14 memory (GiB) layers in multiple set-ups including training from scratch (§4.1, §4.2), fine-tuning (§4.5) and zeroshot inference (§4.3, §4.4), while dramatically improving memory usage, as shown in §3. 12 10 time (sec) 8 Related work Our work follows a long line"
2021.sustainlp-1.5,D18-1260,0,0.0235026,"Missing"
2021.sustainlp-1.5,N18-4013,0,0.0196298,"keys, results in a performance very similar to that of vanilla attention. This shows that an exact and sparse top-k solution is a high-quality approximation for vanilla attention at multi-head attention layers. ing zero-shot inference using pre-trained language models (U NIFIED QA) without any training (§4.3). 4.1 Long Range Arena Long Range Arena (Tay et al., 2021) is a recently established benchmark for evaluating the ability of Transformer variants to handle long sequences. It comprises of multiple text classification tasks with inputs containing thousands of tokens (Table 1). In ListOps (Nangia and Bowman, 2018), given a sequence of operations on single-digit integers, the model predicts a single-digit solution modeled as 10-way classification. IMDb movie reviews (Maas et al., 2011) is a character-level binary sentiment classification task. Lastly, in the ACL Anthology Network (AAN) (Radev et al., 2013) task, a character-level model classifies if there is a citation between a pair of papers. For each task, we downloaded and directly used the vanilla Transformer code offered by the authors (Tay et al., 2021) and compared the performance before and after replacing the multi-head attention layers with t"
2021.sustainlp-1.5,D13-1020,0,0.0297203,"Missing"
2021.sustainlp-1.5,N18-1202,0,0.044527,"Missing"
2021.sustainlp-1.5,2020.emnlp-main.437,0,0.0280308,"Missing"
2021.sustainlp-1.5,2021.tacl-1.4,0,0.196842,"respectively, the dot-product attention function outputs softmax(QK > )V where the softmax function is applied row-wise on the matrix QK > ∈ RL×L of similarity scores of the querykey pairs, leading to an expensive Ω(L2 ) memory requirement. To alleviate this, past work proposed approximation methods for the computation of softmax(QK > ). One major line of research focused on sparse attention variants, where only a few similarity scores are computed per query, and the rest are ignored. Methods differ by which query-key pairs are selected (Child et al., 2019; Ye et al., 2019; Qiu et al., 2020; Roy et al., 2021; Kitaev et al., 2020; Beltagy et al., 2020; Gupta and Berant, 2020; Vyas et al., 2020). A second line of research explored dense variants (Katharopoulos et al., 2020; Wang et al., 2020; Bello, 2021; Tay et al., 2020a) (cf. (Tay et al., 2020b) for a survey). For example, instead of computing the attention scores exactly for only a small number of querykey pairs, (Choromanski et al., 2021) compute an approximation of scores for all pairs. In this work, we adopt the sparse attention approach, but rather than approximating the k most similar key vectors per query vector, we compute this quantity"
2021.sustainlp-1.5,N19-1421,1,0.871219,"Missing"
C10-1087,D09-1110,1,0.818491,"ismatching numbers, (b) antonymy and (c) co-hyponymy (coordinate terms), as specified by WordNet. For example, two nodes of the noun distance would be considered incompatible if one is modified by short and the second by its antonym long. Similarly, two modifier co-hyponyms of distance, such as walking and running would also result such an incompatibility. Adding more incompatibility types (e.g. first vs. second flight) may further improve the precision of this method. The Baseline RTE System In this work we used B IU T EE, Bar-Ilan University Textual Entailment Engine (Bar-Haim et al., 2008; Bar-Haim et al., 2009), a state of the art RTE system, as a baseline and as a basis for our discourse-based enhancements. This section describes this system’s architecture; the methods by which it was augmented to address discourse are presented in Section 5. To determine entailment, B IU T EE performs the following main steps: Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution. For the latter purpose, we use OpenNLP and enable the substitution of coreferring terms. This is the only way by which B IU T EE addr"
C10-1087,bensley-hickl-2008-unsupervised,0,0.0453249,"Missing"
C10-1087,P05-1045,0,0.00846969,"ove the precision of this method. The Baseline RTE System In this work we used B IU T EE, Bar-Ilan University Textual Entailment Engine (Bar-Haim et al., 2008; Bar-Haim et al., 2009), a state of the art RTE system, as a baseline and as a basis for our discourse-based enhancements. This section describes this system’s architecture; the methods by which it was augmented to address discourse are presented in Section 5. To determine entailment, B IU T EE performs the following main steps: Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution. For the latter purpose, we use OpenNLP and enable the substitution of coreferring terms. This is the only way by which B IU T EE addresses discourse, representing the state of the art in entailment systems. Entailment-based transformations Given a T-H pair (both represented as dependency parse trees), the system applies a sequence of knowledge-based entailment transformations over T, generating a set of texts which are entailed by it. The goal is to obtain consequent texts which are more similar to H. Based on preliminary results on the development set, in our expe"
C10-1087,W07-1401,1,0.866677,"Missing"
C10-1087,P06-1114,0,0.022269,"a small military submarine. Introduction This paper investigates the problem of recognising textual entailment within discourse. Textual Entailment (TE) is a generic framework for applied semantic inference (Dagan et al., 2009). Under TE, the relationship between a text (T) and a textual assertion (hypothesis, H) is defined such that T entails H if humans reading T would infer that H is most likely true (Dagan et al., 2006). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference. In part, this is the result of the unavailability of adept tools for handling the kind of discourse processing required for inference. In addition in the main TE benchmarks, the Recognising Textual Entailment (RTE) challenges, discourse This example demonstrates a common situation in texts, and is also applicable to the RTE Search task’s setting. Still, little was done by the task’s participants to consider discourse, and sentences were mostly processed independently. Analyzing the Search task"
C10-1087,P09-1083,0,0.034429,"Missing"
C10-1087,P10-1123,1,0.711842,"Missing"
C10-1087,qiu-etal-2004-public,0,0.0233689,"extracting discourse and document-level features based on the classification of each sentence on its own. Our results show that, even when simple solutions are employed, the reliance on discoursebased information is helpful and achieves a significant improvement of results. We analyze the contribution of each component and suggest some future work to better attend to discourse in entailment systems. To our knowledge, this is the most extensive effort thus far to empirically explore the effect of discourse on entailment systems. nominal phrases, using publicly available tools such as JavaRap (Qiu et al., 2004) or OpenNLP1 , e.g. (Bar-Haim et al., 2008). A major step in the RTE challenges towards a more practical setting of text processing applications occurred with the introduction of the Search task in the Fifth RTE challenge (RTE-5). In this task entailing sentences are situated within documents and depend on other sentences for their correct interpretation. Thus, discourse becomes a substantial factor impacting inference. Surprisingly, discourse hardly received any treatment in this task beyond the standard use of coreference resolution (Castillo, 2009; Litkowski, 2009), and an attempt to addres"
C10-1087,E06-1052,1,0.860453,") The Russian navy worked desperately to save a small military submarine. Introduction This paper investigates the problem of recognising textual entailment within discourse. Textual Entailment (TE) is a generic framework for applied semantic inference (Dagan et al., 2009). Under TE, the relationship between a text (T) and a textual assertion (hypothesis, H) is defined such that T entails H if humans reading T would infer that H is most likely true (Dagan et al., 2006). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference. In part, this is the result of the unavailability of adept tools for handling the kind of discourse processing required for inference. In addition in the main TE benchmarks, the Recognising Textual Entailment (RTE) challenges, discourse This example demonstrates a common situation in texts, and is also applicable to the RTE Search task’s setting. Still, little was done by the task’s participants to consider discourse, and sentences were mostly"
C18-1014,P16-1223,0,0.0346491,"ing comprehension (RC), the task of reading documents and answering questions about their content, has attracted immense attention recently. While early work focused on simple questions and short paragraphs (Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017; Onishi et al., 2016), current work is shifting towards more complex questions that require reasoning over long documents (Joshi et al., 2017; Hewlett et al., 2016; Welbl et al., 2017; Koˇcisky et al., 2017). Long documents pose a challenge for current RC models, as they are dominated by recurrent neural networks (RNNs) (Chen et al., 2016; Kadlec et al., 2016; Xiong et al., 2017). RNNs process documents token-by-token, and thus using them for long documents is prohibitive. A common solution is to retrieve part of the document with an IR approach (Chen et al., 2017; Clark and Gardner, 2017) or a cheap model (Watanabe et al., 2017), and run an RNN over the retrieved excerpts. However, as documents become longer and questions become complex, two problems emerge: (a) retrieving all the necessary text with a one-shot IR method when performing complex reasoning becomes harder, and thus thousands of tokens are retrieved (Clark and Ga"
C18-1014,P17-1171,0,0.0305838,"Rajpurkar et al., 2016; Trischler et al., 2017; Onishi et al., 2016), current work is shifting towards more complex questions that require reasoning over long documents (Joshi et al., 2017; Hewlett et al., 2016; Welbl et al., 2017; Koˇcisky et al., 2017). Long documents pose a challenge for current RC models, as they are dominated by recurrent neural networks (RNNs) (Chen et al., 2016; Kadlec et al., 2016; Xiong et al., 2017). RNNs process documents token-by-token, and thus using them for long documents is prohibitive. A common solution is to retrieve part of the document with an IR approach (Chen et al., 2017; Clark and Gardner, 2017) or a cheap model (Watanabe et al., 2017), and run an RNN over the retrieved excerpts. However, as documents become longer and questions become complex, two problems emerge: (a) retrieving all the necessary text with a one-shot IR method when performing complex reasoning becomes harder, and thus thousands of tokens are retrieved (Clark and Gardner, 2017). (b) Running even a cheap model over the document in its entirety becomes expensive (Choi et al., 2017). Humans, in lieu of a mental inverted index, use document structure to guide their search for answers. E.g., the"
C18-1014,D16-1053,0,0.0337262,"servation and answer prediction tokens are encoded with pre-trained word embeddings and trained character embeddings, where character embeddings are followed by a convolutional layer with max pooling, yielding a single vector per token. Each token is then represented by concatenating the word embedding with the character embedding. Question tokens, observation tokens, and answer to are then fed into a BiLSTM and LSTM (Hochreiter and Schmidhuber, 1997) respectively and the LSTM outputs are compressed to a single vector through 2 s is annealed from 1 to 0.5 during training. 166 self attention (Cheng et al., 2016), resulting in one vector for q and one for o. Answer prediction tokens are fed into an LSTM, where the last hidden state is concatenated to the features φz , thus creating a third vector for the answer prediction. We concatenate these three vectors, and pass them through a one layer feed-forward network that then branches to two networks according to the Dueling DQN architecture (Wang et al., 2016). In each branch we also concatenate the navigation features φn . One branch predicts the value of the state Vθ (s) ∈ R, and the other branch predicts the advantage of every action AθP (s, a) ∈ R fo"
C18-1014,D16-1241,0,0.0144654,"etwork (DQN), which strategically samples tree nodes at training time. Empirically we find our algorithm improves question answering performance compared to DQN and a strong information-retrieval (IR) baseline, and that ensembling our model with the IR baseline results in further gains in performance. 1 Introduction Reading comprehension (RC), the task of reading documents and answering questions about their content, has attracted immense attention recently. While early work focused on simple questions and short paragraphs (Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017; Onishi et al., 2016), current work is shifting towards more complex questions that require reasoning over long documents (Joshi et al., 2017; Hewlett et al., 2016; Welbl et al., 2017; Koˇcisky et al., 2017). Long documents pose a challenge for current RC models, as they are dominated by recurrent neural networks (RNNs) (Chen et al., 2016; Kadlec et al., 2016; Xiong et al., 2017). RNNs process documents token-by-token, and thus using them for long documents is prohibitive. A common solution is to retrieve part of the document with an IR approach (Chen et al., 2017; Clark and Gardner, 2017) or a cheap model (Watana"
C18-1014,D16-1264,0,0.0280683,"e, we propose a new algorithm, based on Deep Q-Network (DQN), which strategically samples tree nodes at training time. Empirically we find our algorithm improves question answering performance compared to DQN and a strong information-retrieval (IR) baseline, and that ensembling our model with the IR baseline results in further gains in performance. 1 Introduction Reading comprehension (RC), the task of reading documents and answering questions about their content, has attracted immense attention recently. While early work focused on simple questions and short paragraphs (Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017; Onishi et al., 2016), current work is shifting towards more complex questions that require reasoning over long documents (Joshi et al., 2017; Hewlett et al., 2016; Welbl et al., 2017; Koˇcisky et al., 2017). Long documents pose a challenge for current RC models, as they are dominated by recurrent neural networks (RNNs) (Chen et al., 2016; Kadlec et al., 2016; Xiong et al., 2017). RNNs process documents token-by-token, and thus using them for long documents is prohibitive. A common solution is to retrieve part of the document with an IR approach (Chen et al., 2017; Cla"
C18-1014,N18-2088,1,0.888673,"Missing"
C18-1014,W17-2623,0,0.0125476,"rithm, based on Deep Q-Network (DQN), which strategically samples tree nodes at training time. Empirically we find our algorithm improves question answering performance compared to DQN and a strong information-retrieval (IR) baseline, and that ensembling our model with the IR baseline results in further gains in performance. 1 Introduction Reading comprehension (RC), the task of reading documents and answering questions about their content, has attracted immense attention recently. While early work focused on simple questions and short paragraphs (Hermann et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017; Onishi et al., 2016), current work is shifting towards more complex questions that require reasoning over long documents (Joshi et al., 2017; Hewlett et al., 2016; Welbl et al., 2017; Koˇcisky et al., 2017). Long documents pose a challenge for current RC models, as they are dominated by recurrent neural networks (RNNs) (Chen et al., 2016; Kadlec et al., 2016; Xiong et al., 2017). RNNs process documents token-by-token, and thus using them for long documents is prohibitive. A common solution is to retrieve part of the document with an IR approach (Chen et al., 2017; Clark and Gardner, 2017) or"
C18-1014,W18-1001,0,0.0278316,"Missing"
C18-1014,P17-1172,0,0.0523833,"ocuments is gaining fast momentum recently (Shen et al., 2017). As mentioned, some approaches use IR for reducing the amount of processed text (Chen et al., 2017; Clark and Gardner, 2017), while others use cheap or parallelizable models to handle long documents (Hewlett et al., 2017; Swayamdipta et al., 2018; Wang et al., 2018a). Searching for 169 answers while using a trained RC model as a black-box was also implemented recently in Wang et al. (2018b), for open-domain questions and multiple short evidence texts from the Web. Another thrust has focused on skimming text in a sequential manner (Yu et al., 2017), or designing recurrent architectures that can consume text quickly (Bradbury et al., 2017; Seo et al., 2018; Campos et al., 2018; Yu et al., 2018). However, to the best of our knowledge no work has previously applied these methods to long documents such as Wikipedia pages. In this work we use T RIVIAQA-N O P for evaluation of our navigation based approach and comparison to an IR baseline. While there are various aspects to consider in such evaluation setup, our choice of data was derived mainly by the requirements for long and structured context. Recently, several new datasets such as W IKI"
C18-1014,P15-1065,0,\N,Missing
C18-1014,P16-1145,0,\N,Missing
C18-1014,P17-1147,0,\N,Missing
C18-1014,D17-1214,0,\N,Missing
D09-1110,meyers-etal-2004-cross,0,0.0140164,"ions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y→X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Experimental setting The goal of the second experiment was to assess that compact inference scales well for broad entailment rule bases. In this experiment we used the Bar-Ilan RTE system (Bar-Haim et al., 2009). The system ope"
D09-1110,P08-1077,0,0.0253695,"er such representations is made by applying some kind of transformations or substitutions to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions (KRAQ)1 and the planned evaluatio"
D09-1110,P08-1023,0,0.0735938,"Missing"
D09-1110,P98-1060,0,0.0432183,"d subtree is set as an alternative to existing subtrees. Alternatives are specified locally using d-edges. Packed chart representations for parse forests were introduced in classical parsing algorithms such as CYK and Earley (Jurafsky and Martin, 2008), and have been extended in later work for various purposes (Maxwell III and Kaplan, 1991; Kay, 1996). Alternatives in the parse chart stem from syntactic ambiguities, and are specified locally as the possible decompositions of each phrase into its sub-phrases. Packed representations have been utilized also in transfer-based machine translation. Emele and Dorna (1998) translated packed source language representation to packed target language representation while avoiding unnecessary unpacking during transfer. Unlike our rule application, in their work transfer rules preserve ambiguity stemming from source language, rather than generating new alternatives. Mi et al.(2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. Their transfer rules are tree-to-string, contrary to our tree-to-tree rules, and chaining is not attempted (rules are applied in a single top-down pass over the source forest), and t"
D09-1110,W07-1401,1,0.772809,"ample, the three rule matches presented in Figure 3 are independent. Let us consider explicit inference first. Assume we start with a single tree T with k independent rules matched. Applying k rules will yield 2k trees, since any subset of the rules might be applied to T . Therefore, the time and space complexity of applying k independent rule matches is Ω(2k ). Applying more rules on the newly derived 4 Empirical Evaluation This section reports empirical evaluation of the efficiency of compact inference, tested in the recognizing textual entailment setting using the RTE-3 and RTE-4 datasets (Giampiccolo et al., 2007; Giampiccolo et al., 2009). These datasets consist of (text, hypothesis) pairs, which need to be classified as entailing/non entailing. Our first experiment shows, using a small rule set, that compact inference outperforms explicit inference by orders of magnitude (Section 4.1). The second experiment shows that compact inference scales well to a full-blown RTE setting with several large-scale rule bases, where up to hundreds of rules are applied for a text (Section 4.2). 1061 4.1 Compact vs. Explicit Inference To compare explicit and compact inference we randomly sampled 100 pairs from the RT"
D09-1110,H05-1049,0,0.149823,"Missing"
D09-1110,C08-1043,0,0.0175992,"transformations within RTE systems, as well as on using packed representations in other NLP tasks. RTE Systems Previous RTE systems usually restricted both the type of allowed transformations and the search space. Systems based on lexical (word-based or phrase-based) matching of h in t typically applied only lexical rules (without vari1063 ables), where both sides of the rule are matched directly in t and h (Haghighi et al., 2005; MacCartney et al., 2008). The inference formalism we use is more expressive, allowing also syntactic and lexical-syntactic transformations as well as rule chaining. Hickl (2008) derived from a given (t, h) pair a small set of discourse commitments, which are quite similar to the kind of consequents we derive by our syntactic and lexical-syntactic rules. The commitments were generated by several different tools and techniques, compared to our generic unified inference process, and commitment generation efficiency was not discussed. Braz et al. (2005) presented a semantic inference framework which “augments” the text representation with only the right-hand-side of an applied rule, and in this respect is similar to ours. However, in their work, both rule application and"
D09-1110,N07-1071,0,0.0238686,"s to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions (KRAQ)1 and the planned evaluation of knowledge resources in the forthcoming 5th Recognizing Textual Entailment"
D09-1110,P09-1051,1,0.741292,"for the RTE literature. They include lexical and structural measures for the coverage of H by F, where high coverage is assumed to correlate with entailment, as well as features aiming to detect inconsistencies between F and H such as incompatible arguments for the same predicate or incompatible verb polarity (see below). For a complete feature description, see (Bar-Haim et al., 2009). 4.2 Application to an RTE System Rule Bases In addition to the generic rules described in Section 4.1, the following large-scale sources for entailment rules were used: Wikipeda: We used the lexical rulebase of Shnarch et al. (2009), who extracted rules such as ‘Janis Joplin → singer’ from Wikipedia based on both its metadata (e.g. links and redirects) and text definitions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y→X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment"
D09-1110,C08-1107,1,0.836779,"cal rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y→X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Experimental setting The goal of the second experiment was to assess that compact inference scales well for broad entailment rule bases. In this experiment we used the Bar-Ilan RTE system (Bar-Haim et al., 2009). The system operates in two primary stages: Inference, in which entailment rules are applied to the initial compact f"
D09-1110,W09-2504,1,0.827376,"singer’ from Wikipedia based on both its metadata (e.g. links and redirects) and text definitions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y→X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Experimental setting The goal of the second experiment was to assess that compact inference scales well for broad entailment rule bases. I"
D09-1110,W04-3206,1,0.825126,"commonly, inference over such representations is made by applying some kind of transformations or substitutions to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions ("
D09-1110,P96-1027,0,0.0120575,"lems typically requires specific representations and algorithms, depending on the type of alternatives that should be represented and the specified operations for creating them. In our work, alternatives are created by rule application, where a newly derived subtree is set as an alternative to existing subtrees. Alternatives are specified locally using d-edges. Packed chart representations for parse forests were introduced in classical parsing algorithms such as CYK and Earley (Jurafsky and Martin, 2008), and have been extended in later work for various purposes (Maxwell III and Kaplan, 1991; Kay, 1996). Alternatives in the parse chart stem from syntactic ambiguities, and are specified locally as the possible decompositions of each phrase into its sub-phrases. Packed representations have been utilized also in transfer-based machine translation. Emele and Dorna (1998) translated packed source language representation to packed target language representation while avoiding unnecessary unpacking during transfer. Unlike our rule application, in their work transfer rules preserve ambiguity stemming from source language, rather than generating new alternatives. Mi et al.(2008) applied statistical m"
D09-1110,D08-1084,0,0.0688858,"Missing"
D09-1110,J00-4006,0,\N,Missing
D09-1110,D08-1022,0,\N,Missing
D09-1110,C98-1058,0,\N,Missing
D09-1110,P08-1000,0,\N,Missing
D12-1018,I08-1065,0,0.0159781,"Weisman§ , Jonathan Berant† , Idan Szpektor‡, Ido Dagan§ § Computer Science Department, Bar-Ilan University † The Blavatnik School of Computer Science, Tel Aviv University ‡ Yahoo! Research Israel {weismah1,dagan}@cs.biu.ac.il {jonatha6}@post.tau.ac.il {idan}@yahoo-inc.com Abstract of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper → talk’, ‘win → play’ and ‘buy → own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguis"
D12-1018,W02-2001,0,0.0359612,"callyrelated verbs sharing some linguistic properties (Levin, 1993). One of the most general verb classes are stative vs. event verbs (Jackendoff, 1983). Stative verb, such as ‘love’ and ‘think’, usually describe a state that lasts some time. On the other hand, event verbs, such as ‘run’ and ‘kiss’, describe an action. We hypothesize that verb classes are relevant for determining entailment, for example, that stative verbs are not likely to entail event verbs. 196 Verb generality Verb-particle constructions are multi-word expressions consisting of a head verb and a particle, e.g., switch off (Baldwin and Villavicencio, 2002). We conjecture that the more general a verb is, the more likely it is to appear with many different particles. Detecting verb generality can help us tackle an infamous property of distributional similarity methods, namely, the difficulty in detecting the direction of entailment (Berant et al., 2012). For example, the verb ’cover’ appears with many different particles such as ’up’ and ’for’, while the verb ’coat’ does not. Thus, assuming we have evidence for an entailment relation between the two verbs, this indicator can help us discern the direction of entailment and determine that ‘coat → c"
D12-1018,J12-1003,1,0.714129,"xample, given the sentence “Churros are coated with sugar”, one can use the rule ‘coat → cover’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle → scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in the same document, and a"
D12-1018,P08-1090,0,0.124146,"strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starting with candidate verb pairs based on a distributional similarity measure, the patterns are used to choose a semantic relation per verb pair based on the different patterns this pair instantiates. This method is more precise than distributional similarity approaches, but it is highly susceptible to sparseness issues, since verbs do not typically co-occur within rigid patterns. Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. Mirkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel → book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennac"
D12-1018,W04-3205,0,0.836949,"Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle → scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in the same document, and also in different clauses of the same sentence. In this paper, we claim that on top of standard pattern-based and distributional similarity methods, corpus-base"
D12-1018,P05-1014,1,0.692735,"entailment detection. 2 Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five 195 semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starti"
D12-1018,C92-2082,0,0.134291,"ilarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle → scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in the same document, and also in different clauses of the same sentence. In this paper, we claim that on top of standard pattern-based and distributional similarity methods, corpus-based learning of verb entailment can greatly benefit from exploiting additional linguisticallymotivated cues that are specific to verbs. For instance, when verbs co-occur in different clauses of the same sentence, the syntactic relation between the clauses can be viewed as a proxy for the semantic relation between the verbs. Mo"
D12-1018,P02-1047,0,0.0179155,"harder to discern but coverage is increased. 4.1.1 Sentence-level co-occurrence We next detail features that address co-occurrence of the target verb pair within a sentence. These include our novel linguistically-motivated indicators, as well as features that were adapted from prior work. Discourse markers As discussed in Section 3, discourse markers may signal relations between the main verbs of adjacent clauses. The literature is abundant with taxonomies that classify markers to various discourse relations (Mann and Thompson, 1988; Hovy and Maier, 1993; Knott and Sanders, 1998). Inspired by Marcu and Echihabi (2002), we employ markers that are mapped to four discourse relations ’Contrast’, ’Cause’, ’Condition’ and ’Temporal’, as specified in Table 1. This definition can be viewed as a relaxed version of VerbOcean’s (Chklovski and Pantel, 2004) patterns, although the underlying intuition is different (see Section 3). For a target verb pair (v1 , v2 ) and each discourse relation r, we count the number of times that v1 is the main verb in the main clause, v2 is the main verb in the subordinate clause, and the clauses are connected via a marker mapped to r. For example, given the sentence “You must enroll in"
D12-1018,P06-2075,1,0.926486,"imilarity approaches, but it is highly susceptible to sparseness issues, since verbs do not typically co-occur within rigid patterns. Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. Mirkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel → book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment Mirkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (20"
D12-1018,nivre-etal-2006-maltparser,0,0.00951803,"se and v2 in the main clause. We term the features by the relevant discourse relation, e.g., ‘v1-contrast-v2’ refers to v1 being in the main clause and connected to the subordinate clause via a contrast marker. Dependency relations between clauses As noted in Section 3, the syntactic structure of verb cooccurrence can indicate the existence or lack of entailment. In dependency parsing this may be expressed via the label of the dependency relation connecting the main and subordinate clauses. In our experiments we used the ukWaC corpus1 (Baroni et al., 2009) which was parsed by the MALT parser (Nivre et al., 2006). Hence, we identified three MALT dependency relations that connect a main clause with its subordinate clause. The first relation is the object complement relation ‘obj’. In this case the subordinate clause is an object complement of the main clause. For example, in “it surprised me that the lizard could talk” the verb pair (‘surprise’,‘talk’) is connected by the ‘obj’ relation. The second relation is the adverbial adjunct relation ‘adv’, in which the subordinate clause is adverbial and describes the time, place, manner, etc. of the main clause, e.g., “he gave his consent without thinking abou"
D12-1018,D09-1025,0,0.0192561,"(2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. Mirkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel → book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment Mirkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (2012) utilized various distributional similarity features to identify entailment between lexical-syntactic predicates. In this paper, we follow the supervised approach for semantic relation detection in order to identify"
D12-1018,P02-1006,0,0.127632,"o-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task. 1 Introduction Inference rules are an important building block of many semantic applications, such as Question Answering (Ravichandran and Hovy, 2002) and Information Extraction (Shinyama and Sekine, 2006). For example, given the sentence “Churros are coated with sugar”, one can use the rule ‘coat → cover’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are seman"
D12-1018,D10-1106,0,0.200727,"an Berant† , Idan Szpektor‡, Ido Dagan§ § Computer Science Department, Bar-Ilan University † The Blavatnik School of Computer Science, Tel Aviv University ‡ Yahoo! Research Israel {weismah1,dagan}@cs.biu.ac.il {jonatha6}@post.tau.ac.il {idan}@yahoo-inc.com Abstract of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper → talk’, ‘win → play’ and ‘buy → own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for de"
D12-1018,N06-1039,0,0.0607114,"a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task. 1 Introduction Inference rules are an important building block of many semantic applications, such as Question Answering (Ravichandran and Hovy, 2002) and Information Extraction (Shinyama and Sekine, 2006). For example, given the sentence “Churros are coated with sugar”, one can use the rule ‘coat → cover’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin,"
D12-1018,C08-1107,1,0.938656,"Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five 195 semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starting with candidate verb pai"
D12-1018,W04-3206,1,0.339602,"t the candidate rules ‘vs → vsi ’ and ‘vsi → vs ’ respectively. To reduce noise, we filtered out verb pairs where one of the verbs is an auxiliary or a light verb such as ’do’, ’get’ and ’have’. This step resulted in 812 verb pairs as our dataset6 , which were manually annotated by the authors as representing a valid entailment rule or not. To annotate these pairs, we generally followed the rule-based approach for entailment rule annotation, where a rule ‘v1 → v2 ’ is considered as correct if the annotator could think of reasonable contexts under which the rule holds (Dekang and Pantel, 2001; Szpektor et al., 2004). In total 225 verb pairs were labeled as entailing (the rule ‘v1 → v2 ’ was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule ‘v1 → v2 ’ was judged as incorrect). The InterAnnotator Agreement (IAA) for a random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach (Szpektor et al., 2007). For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (Baroni et al., 2009), which contains 2 billion words. For classification, we utilized SVM-perf’s (Joachims, 2005) linear SVM implementation with defau"
D12-1018,P07-1058,1,0.934416,"annotate these pairs, we generally followed the rule-based approach for entailment rule annotation, where a rule ‘v1 → v2 ’ is considered as correct if the annotator could think of reasonable contexts under which the rule holds (Dekang and Pantel, 2001; Szpektor et al., 2004). In total 225 verb pairs were labeled as entailing (the rule ‘v1 → v2 ’ was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule ‘v1 → v2 ’ was judged as incorrect). The InterAnnotator Agreement (IAA) for a random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach (Szpektor et al., 2007). For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (Baroni et al., 2009), which contains 2 billion words. For classification, we utilized SVM-perf’s (Joachims, 2005) linear SVM implementation with default parameters, and evaluated our model by performing 10-fold cross validation (CV) over the labeled dataset. 5 http://trec.nist.gov/data/reuters/ reuters.html 6 The data set is available at http://www.cs.biu.ac. il/˜nlp/downloads/verb-pair-annotation. html 200 Feature selection and analysis As discussed in Section 4.2, we followed the feature"
D12-1018,P10-3017,0,0.217353,"ed arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. Mirkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel → book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment Mirkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (2012) utilized various distributional similarity features to identify entailment between lexical-syntactic predicates. In this paper, we follow the supervised approach for semantic relation detection in order to identify verb entailment. While we utilize and adapt useful features from prior work, we introduce a diverse set of novel features for the task, effectively combining verb co-occurrence information at the sentence, document"
D12-1018,W03-1011,0,0.161733,"ntially improves verb entailment detection. 2 Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five 195 semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the v"
D12-1018,P06-1107,0,0.0413731,"Motivated Evidence Hila Weisman§ , Jonathan Berant† , Idan Szpektor‡, Ido Dagan§ § Computer Science Department, Bar-Ilan University † The Blavatnik School of Computer Science, Tel Aviv University ‡ Yahoo! Research Israel {weismah1,dagan}@cs.biu.ac.il {jonatha6}@post.tau.ac.il {idan}@yahoo-inc.com Abstract of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper → talk’, ‘win → play’ and ‘buy → own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer no"
D12-1018,W07-1401,1,\N,Missing
D12-1018,P09-1068,0,\N,Missing
D13-1160,D11-1039,0,0.0140545,"ion We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previo"
D13-1160,Q13-1005,0,0.724997,"the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision from a knowledge base (Riedel"
D13-1160,P10-1129,0,0.00937516,"Missing"
D13-1160,P11-1028,0,0.00622826,". For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perf"
D13-1160,P12-1014,0,0.00683997,"s. First, we introduce features indicating when a word of a given POS tag is skipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine 1538 logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2 . Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft"
D13-1160,P13-1042,0,0.526067,"ty In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline. Execute on Database Type.University u Education.BarackObama Type.University bridging Education alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging"
D13-1160,chang-manning-2012-sutime,0,0.0420154,"les (e1 , r, e2 ) (e.g., (“Obama”, “was also born in”, “August 1961”)) 4 Freebase associates each entity with a set of types using the Type property. 1536 were extracted from ClueWeb09 using the ReVerb open IE system (Fader et al., 2011). Lin et al. (2012) released a subset of these triples5 where they were able to substitute the subject arguments with KB entities. We downloaded their dataset and heuristically replaced object arguments with KB entities by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r ∈ R1 and augment it with a type signature [t1 , t2 ] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1 , e2 ) to the extension of F(r[t1 , t2 ]) if the (Freebase) type of e1 (e2 ) is t1 (t2 ). For example, (BarackObama, 1961) is added to F(“born in”[Person, Date]). We perform a similar procedure that uses a Hearst-like pattern (Hearst, 1992) to map phrases to unary predicates. If a text phrase r ∈ R1 matches the pattern “(is|was a|the) x IN”, where IN is a preposition, then we add e1 to F"
D13-1160,P12-1045,0,0.00683741,"” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision"
D13-1160,W10-2903,0,0.0679367,"rates predicates compatible with neighboring predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011),"
D13-1160,D11-1142,0,0.269794,"typed4 phrases R1 (e.g., “born in”[Person,Location]) and predicates R2 (e.g., PlaceOfBirth). For each r ∈ R1 ∪ R2 , we create its extension F(r), which is a set of co-occurring entitypairs (e.g., F(“born in”[Person,Location]) = {(BarackObama, Honolulu), . . . }. The lexicon is generated based on the overlap F(r1 ) ∩ F(r2 ), for r1 ∈ R1 and r2 ∈ R2 . Typed phrases 15 million triples (e1 , r, e2 ) (e.g., (“Obama”, “was also born in”, “August 1961”)) 4 Freebase associates each entity with a set of types using the Type property. 1536 were extracted from ClueWeb09 using the ReVerb open IE system (Fader et al., 2011). Lin et al. (2012) released a subset of these triples5 where they were able to substitute the subject arguments with KB entities. We downloaded their dataset and heuristically replaced object arguments with KB entities by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r ∈ R1 and augment it with a type signature [t1 , t2 ] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1 , e"
D13-1160,P13-1158,0,0.717148,"phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., λx.Marriage.Spouse.(Gender.Female ux)). Learning these composite predicat"
D13-1160,P11-1149,0,0.0503237,"ing predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage"
D13-1160,C92-2082,0,0.0941625,"alking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r ∈ R1 and augment it with a type signature [t1 , t2 ] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1 , e2 ) to the extension of F(r[t1 , t2 ]) if the (Freebase) type of e1 (e2 ) is t1 (t2 ). For example, (BarackObama, 1961) is added to F(“born in”[Person, Date]). We perform a similar procedure that uses a Hearst-like pattern (Hearst, 1992) to map phrases to unary predicates. If a text phrase r ∈ R1 matches the pattern “(is|was a|the) x IN”, where IN is a preposition, then we add e1 to F(x). For (Honolulu, “is a city in”, Hawaii), we extract x = “city 00 and add Honolulu to F(“city”). From the initial 15M triples, we extracted 55,081 typed binary phrases (9,456 untyped) and 6,299 unary phrases. Logical predicates Binary logical predicates contain (i) all KB properties6 and (ii) concatenations of two properties p1 .p2 if the intermediate type represents an event (e.g., the married to relation is represented by Marriage.Spouse). F"
D13-1160,P11-1055,0,0.579056,"ia perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally,"
D13-1160,Q13-1016,0,0.0899754,". The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment pr"
D13-1160,D12-1069,0,0.282435,"thout annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build a coarse alignment between phrases and predicates— an approach similar in spirit to Cai and Yates (2013). However, this alignment only allows us to generate a subset of the desired predicates. Aligning light verbs (e.g., “go”) and prepositions is not very informative due to polysemy, a"
D13-1160,N13-1103,0,0.181619,"h the type Person. (iii) The system sometimes incorrectly draws verbs from subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples"
D13-1160,D10-1119,0,0.0609899,"nt BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical lev"
D13-1160,D11-1140,0,0.133847,"the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build a coarse alignment"
D13-1160,D11-1049,0,0.54058,"taset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., λx.Marriage.Spouse.(Gender.Female ux)). Learning these composite predicates would drastically increase the possible space of logical forms, but we believe that the methods proposed in this paper— alignment via distant supervision and bridging—can provide some traction on this problem. Acknowledgments We would like to thank Thomas Lin, Mausam and Oren Etzioni for providing us with open IE triples that are partially-linked to Freebase, and also Arun Chaganty for helpful co"
D13-1160,P13-1127,0,0.0118652,"system sometimes incorrectly draws verbs from subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) cont"
D13-1160,P11-1060,1,0.607238,"atible with neighboring predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they"
D13-1160,J13-2005,1,0.70636,"ps new questions x to answers y via latent logical forms z and the knowledge base K. 1534 2.1 Knowledge base Let E denote a set of entities (e.g., BarackObama), and let P denote a set of properties (e.g., PlaceOfBirth). A knowledge base K is a set of assertions (e1 , p, e2 ) ∈ E × P × E (e.g., (BarackObama, PlaceOfBirth, Honolulu)). We use the Freebase knowledge base (Google, 2013), which has 41M non-numeric entities, 19K properties, and 596M assertions.1 2.2 Logical forms To query the knowledge base, we use a logical language called Lambda Dependency-Based Compositional Semantics (λ-DCS)—see Liang (2013) for details. For the purposes of this paper, we use a restricted subset called simple λ-DCS, which we will define below for the sake of completeness. The chief motivation of λ-DCS is to produce logical forms that are simpler than lambda calculus forms. For example, λx.∃a.p1 (x, a) ∧ ∃b.p2 (a, b) ∧ p3 (b, e) is expressed compactly in λ-DCS as p1 .p2 .p3 .e. Like DCS (Liang et al., 2011), λ-DCS makes existential quantification implicit, thereby reducing the number of variables. Variables are only used for anaphora and building composite binary predicates; these do not appear in simple λ-DCS. Ea"
D13-1160,W12-3016,0,0.00702754,".g., “born in”[Person,Location]) and predicates R2 (e.g., PlaceOfBirth). For each r ∈ R1 ∪ R2 , we create its extension F(r), which is a set of co-occurring entitypairs (e.g., F(“born in”[Person,Location]) = {(BarackObama, Honolulu), . . . }. The lexicon is generated based on the overlap F(r1 ) ∩ F(r2 ), for r1 ∈ R1 and r2 ∈ R2 . Typed phrases 15 million triples (e1 , r, e2 ) (e.g., (“Obama”, “was also born in”, “August 1961”)) 4 Freebase associates each entity with a set of types using the Type property. 1536 were extracted from ClueWeb09 using the ReVerb open IE system (Fader et al., 2011). Lin et al. (2012) released a subset of these triples5 where they were able to substitute the subject arguments with KB entities. We downloaded their dataset and heuristically replaced object arguments with KB entities by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r ∈ R1 and augment it with a type signature [t1 , t2 ] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1 , e2 ) to the extensio"
D13-1160,D12-1048,0,0.440057,"is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., λx.Marriag"
D13-1160,P05-1012,0,0.014856,"h can only represent one-sided preferences for having more or fewer of a given operation. Indicator features stabilize the model, preferring derivations with a well-balanced inventory of operations. Part-of-speech tag features To guide the composition of predicates, we use POS tags in two ways. First, we introduce features indicating when a word of a given POS tag is skipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine 1538 logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2 . Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in"
D13-1160,P13-1092,0,0.28632,"kipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine 1538 logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2 . Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft, overlapping rules. In contrast, CCG-based methods (Kwiatkowski et"
D13-1160,D12-1042,0,0.527661,"et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has t"
D13-1160,P07-1121,0,0.0270625,"dging Education alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on F"
D13-1160,D12-1035,0,0.0209998,"e goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build a coarse alignment between phrases and predicates— an approach similar in spirit to Cai and Yates (2013). However, this alignment only allows us to generate a subset of the desired predicates. Aligning light v"
D13-1177,D08-1073,0,0.65788,"ens # of events # of non-N ONE relations Avg 3.80 89.98 6.20 5.64 Min 1 19 2 1 Max 15 319 15 24 Table 1: Process statistics over 148 process descriptions. N ONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the S UPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). Importantly, our relation set also includes the relations C AUSES and E NABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. We also added event coreference (S AME) to R. Do et al. (2012) used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. In this paper, we simply treat S AME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference"
D13-1177,P11-1098,0,0.14816,"and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a “one-shot” chance to extract the process structure. We showed in this paper that global structural properties lead to significant improvements in extraction accuracy, and ILP is an effective framework CAUSES CAUSES shifts CA"
D13-1177,N13-1104,0,0.0243669,"ons between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a “one-shot” chance to extract the process structure. We showed in this paper that global structural properties lead to significant improvements in extraction accuracy, and ILP is an effective framework CAUSES CAUSES shifts CAUSES skipped CAUSES deleted COTEMP secrete PREV CAUSES CAUSES CAUSES used duplicated bind CAUSES PREV ENABLES SAME NONE binds Figure 3: Process graph fra"
D13-1177,de-marneffe-etal-2006-generating,1,0.126092,"Missing"
D13-1177,D12-1062,0,0.42201,"turing, economical developments, and various phenomena in life and social sciences can all be viewed as types of processes. Processes are complicated objects; consider for example the biological process of ATP synthesis described in Figure 1. This process involves 12 entities and 8 events. Additionally, it describes relations between events and entities, and the relationship between events (e.g., the second occurrence of the event ‘enter’, causes the event ‘changing’). ∗ Both authors equally contributed to the paper Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task. Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations. In this paper, we formally define the task of process extraction and present automatic extraction methods. Our approach handles an op"
D13-1177,N13-1112,0,0.0818844,"Missing"
D13-1177,P08-2012,1,0.92408,"fier and the gold standard disagree. We are interested in triads that never occur in training data but are predicted by the classifier, and vice versa. Figure 2 illustrates some of the triads found and Equations 8-12 provide the corresponding ILP formulations. Equations 8-10 were formulated as soft constraints (expanding the set K) and were incorporated by defining a reward αk for each triad type.3 On the other hand, Equations 11-12 were formulated as hard constraints to prevent certain structures. 1. S AME transitivity (Figure 2a, Eqn. 8): Coreference transitivity has been used in past work (Finkel and Manning, 2008) and we incorporate it by a constraint that encourages triads that respect transitivity. 2. C AUSE-C OTEMP (Figure 2b, Eqn. 9): If ti causes both tj and tk , then often tj and tk are co-temporal. E.g, in “genetic drift has led to a loss of genetic variation and an increase in the frequency of . . .”, a single event causes two subsequent events that occur simultaneously. 3. C OTEMP transitivity (Figure 2c, Eqn. 10): If ti is co-temporal with tj and tj is co-temporal with tk , then usually ti and tk are either cotemporal or denote the same event. 4. S AME contradiction (Figure 2d, Eqn. 11): If t"
D13-1177,W11-1801,0,0.226726,"hich exploits these structural properties by performing joint inference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure. 1 Process extraction is related to two recent lines of work in Information Extraction – event extraction and timeline construction. Traditional event extraction focuses on identifying a closed set of events within a single sentence. For example, the BioNLP 2009 and 2011 shared tasks (Kim et al., 2009; Kim et al., 2011) consider nine events types related to proteins. In practice, events are currently almost always extracted from a single sentence. Process extraction, on the other hand, is centered around discovering relations between events that span multiple sentences. The set of possible event types in process extraction is also much larger. Introduction A process is defined as a series of inter-related events that involve multiple entities and lead to an end result. Product manufacturing, economical developments, and various phenomena in life and social sciences can all be viewed as types of processes. Pr"
D13-1177,P03-1054,1,0.0154475,"gy” and marking any contiguous sequence of sentences that describes a process, i.e., a series of events that lead towards some objective. Then, each process description was annotated by a biologist. The annotator was first presented with annotation guidelines and annotated 20 descriptions. The annotations were then discussed with the authors, after which all process descriptions were annotated. After training a second biologist, we measured inter-annotator agreement κ = 0.69, on 30 random process descriptions. Process descriptions were parsed with Stanford constituency and dependency parsers (Klein and Manning, 2003; de Marneffe et al., 2006), and 35 process descriptions were set aside as a test set (number of training set trigger pairs: 1932, number of test set trigger pairs: 906). We performed 10fold cross validation over the training set for feature selection and tuning of constraint parameters. For each constraint type (connectivity, chain-structure, and five triad constraints) we introduced a parameter and tuned the seven parameters by coordinatewise ascent, where for hard constraints a binary parameter controls whether the constraint is used, and for soft constraints we attempted 10 different rewar"
D13-1177,P09-1039,0,0.019495,"for a relation r between the trigger pair (ti , tj ) (e.g., θijr = log pijr ), and yijr be the corresponding indicator variable. Our goal is to find an assignment for the indicators y = {yijr |1 ≤ i &lt; j ≤ n, r ∈ R}. With no global constraints this can be formulated as the following ILP: arg max y X θijr yijr (1) ijr s.t.∀i,j X yijr = 1 r where the constraint ensures exactly one relation between each event pair. We now describe constraints that result in a coherent global process structure. Connectivity Our ILP formulation for enforcing connectivity is a minor variation of the one suggested by Martins et al. (2009) for dependency parsing. In our setup, we want P to be a connected undirected graph, and not a directed tree. However, an undirected graph P is connected iff there exists a directed tree that is a subgraph of P when edge directions are ignored. Thus the resulting formulation is almost identical and is based on flow constraints which ensure that there is a path from a designated root in the graph to all other nodes. ¯ be the set R  N ONE. An edge (ti , tj ) is Let R in E iff there is some non-N P ONE relation between ti and tj , i.e. iff yij := ¯ yijr is equal to 1. r∈R For each variable yij w"
D13-1177,D12-1080,1,0.93945,"l developments, and various phenomena in life and social sciences can all be viewed as types of processes. Processes are complicated objects; consider for example the biological process of ATP synthesis described in Figure 1. This process involves 12 entities and 8 events. Additionally, it describes relations between events and entities, and the relationship between events (e.g., the second occurrence of the event ‘enter’, causes the event ‘changing’). ∗ Both authors equally contributed to the paper Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task. Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations. In this paper, we formally define the task of process extraction and present automatic extraction methods. Our approach handles an open set of event types and wo"
D13-1177,P11-1163,1,0.901643,"Missing"
D13-1177,N10-1123,0,0.0211054,"nalized for both the false negative of the correct class (just as before), and also for the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky ("
D13-1177,N12-1008,0,0.0193823,"the predictions of Global and Local are identical. Original text, Left: “... the template shifts . . . , and a part of the template strand is either skipped by the replication machinery or used twice as a template. As a result, a segment of DNA is deleted or duplicated.” Right: “Cells of mating type A secrete a signaling molecule, which can bind to specific receptor proteins on nearby cells. At the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biol"
D13-1177,D11-1001,0,0.0317982,"the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one anothe"
D13-1177,D12-1131,0,0.0299692,"t the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biological process models from text, intelligent tutoring systems, and non-factoid QA systems. In this paper we have presented the task of process extraction, and developed methods for extracting relations between process events. Processes contain events that are tightly coupled through strong dependencies. We have shown that exploiting these structural dependencies and performing joint"
D13-1177,J11-2003,0,0.0141453,"iversity, Stanford Justin Lewis and Brittany Harding University of Washington, Seattle Peter Clark Allen Institute for Artificial Intelligence, Seattle Abstract Automatically extracting the structure of processes from text is crucial for applications that require reasoning, such as non-factoid QA. For instance, answering a question on ATP synthesis, such as “How do H+ ions contribute to the production of ATP?” requires a structure that links H+ ions (Figure 1, sentence 1) to ATP (Figure 1, sentence 4) through a sequence of intermediate events. Such “How?” questions are common on FAQ websites (Surdeanu et al., 2011), which further supports the importance of process extraction. Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How?” and “Why?” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We rep"
D13-1177,J08-2002,1,0.92479,"e gold standard disagree. We are interested in triads that never occur in training data but are predicted by the classifier, and vice versa. Figure 2 illustrates some of the triads found and Equations 8-12 provide the corresponding ILP formulations. Equations 8-10 were formulated as soft constraints (expanding the set K) and were incorporated by defining a reward αk for each triad type.3 On the other hand, Equations 11-12 were formulated as hard constraints to prevent certain structures. 1. S AME transitivity (Figure 2a, Eqn. 8): Coreference transitivity has been used in past work (Finkel and Manning, 2008) and we incorporate it by a constraint that encourages triads that respect transitivity. 2. C AUSE-C OTEMP (Figure 2b, Eqn. 9): If ti causes both tj and tk , then often tj and tk are co-temporal. E.g, in “genetic drift has led to a loss of genetic variation and an increase in the frequency of . . .”, a single event causes two subsequent events that occur simultaneously. 3. C OTEMP transitivity (Figure 2c, Eqn. 10): If ti is co-temporal with tj and tj is co-temporal with tk , then usually ti and tk are either cotemporal or denote the same event. 4. S AME contradiction (Figure 2d, Eqn. 11): If t"
D13-1177,P09-1046,0,0.281107,"E relations Avg 3.80 89.98 6.20 5.64 Min 1 19 2 1 Max 15 319 15 24 Table 1: Process statistics over 148 process descriptions. N ONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the S UPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). Importantly, our relation set also includes the relations C AUSES and E NABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. We also added event coreference (S AME) to R. Do et al. (2012) used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. In this paper, we simply treat S AME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference and temporal relations"
D13-1177,W09-1401,0,\N,Missing
D13-1177,J12-1003,1,\N,Missing
D14-1159,D13-1160,1,0.289643,"contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehensio"
D14-1159,D08-1073,0,0.0214459,"the set of regular expressions that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al.,"
D14-1159,W06-0602,0,0.0205921,"urafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke e"
D14-1159,W10-2903,0,0.0252125,"., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choic"
D14-1159,clarke-etal-2012-nlp,1,0.779928,"Missing"
D14-1159,W02-1001,0,0.0492796,"ossible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2 -regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions. For event relations, we include the features described in Scaria et al. (2013), as well as context features for both triggers, and the dependency path between them, if one exists. 5 Question Answering via Structures This section describes our question answering system that, given a process struc"
D14-1159,D12-1062,0,0.0400506,"d by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank a"
D14-1159,doddington-etal-2004-automatic,0,0.188353,"Missing"
D14-1159,D11-1142,0,0.015657,"in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗ Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in"
D14-1159,P13-1158,0,0.00703221,"ons via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗ Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in biology textbooks such as this excerpt and the question that follows: “. . . Water is split, providing a source of electrons and protons (hydrogen io"
D14-1159,P99-1042,0,0.0251544,"ap natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating machine reading. Hirschman et al. (1999) presented a bag-ofwords approach to retrieving sentences for reading comprehension. Richardson et al. (2013) recently released the MCTest reading comprehension dataset that examines understanding of fictional stories. Their work shares our goal of advancing micro-reading, but they do not focus on process understanding. Developing programs that perform deep reasoning over complex descriptions of processes is an important step on the road to fulfilling the higher goals of machine reading. In this paper, we present an end-to-end system for reading comprehension of paragraphs which describe biolo"
D14-1159,W11-1801,0,0.0399034,"Missing"
D14-1159,Q13-1016,0,0.0240224,"e do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating mach"
D14-1159,P14-1026,0,0.0192904,"he text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology “. . . Water is split, providing a source of electrons and protons (hydrogen ions, H+ ) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of water T HEME Step 1 split absorb E NABLE C AUSE transfer the electrons and hydrogen ions from water T HEME T HEME light ions to an acceptor called NADP+ . . . ” Step 3: Answer = b Q What can the splitting of water lead to? a Light absorption b Transfer of io"
D14-1159,P14-5010,1,0.0110213,"Missing"
D14-1159,P09-1039,0,0.00814841,"iption Every argument candidate and trigger pair has exactly one label. Two arguments of the same trigger cannot overlap. The S AME relation is symmetric. All other relations are anti-symmetric, i.e., for any relation label other than S AME, at most one of (ti , tj ) or (tj , ti ) can take that label and the other is assigned the label NULL - REL. Every trigger can have no more than two arguments with the same label. The same span of text can not be an argument for more than two triggers. The triggers must form a connected graph, framed as flow constraints as in Magnanti and Wolsey (1995) and Martins et al. (2009). If the same span of text is an argument of two triggers, then the triggers must be connected by a relation that is not NULL - REL. This ensures that triggers that share arguments are related. For any trigger, at most one outgoing edge can be labeled S UPER. Table 2: Constraints for joint inference. Formulation Given the two sets of variables, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: max y,z X t,a∈At ,A bt,a,A · yt,a,A + X ct1 ,t2 ,R · zt1 ,t2 ,R t1 ,t2 ,R Here, y and z refer to all the argument and re"
D14-1159,D12-1080,1,0.842646,"m. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow"
D14-1159,N03-1022,0,0.0797767,"igure 1). Our strategy is to treat the process structure as a small knowledge-base. We map each answer along with the question into a structured query that we compare against the structure. The query can prove either the correctness or incorrectness of the answer being considered. That is, either we get a valid match for an answer (proving that the corresponding answer is correct), or we get a refutation in the form of a contradicted causal chain (thus proving that the other answer is correct). This is similar to theorem proving approaches suggested in the past for factoid question answering (Moldovan et al., 2003). The rest of this section is divided into three parts: Section 5.1 defines the queries we use, Section 5.2 describes a rule-based algorithm for converting a question and an answer into a query and finally, 5.3 describes the overall algorithm. 5.1 Queries over Processes We model a query as a directed graph path with regular expressions over edge labels. The bottom right portion of Figure 1 shows examples of queries for our running example. In general, given a question and one of the answer candidates, one end of the path is populated by a trigger/argument found in the question and the other is"
D14-1159,J05-1004,0,0.128022,"tudied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of superv"
D14-1159,J08-2005,0,0.144807,"Missing"
D14-1159,D13-1020,0,0.248644,"ong answer is closer in the text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology “. . . Water is split, providing a source of electrons and protons (hydrogen ions, H+ ) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of water T HEME Step 1 split absorb E NABLE C AUSE transfer the electrons and hydrogen ions from water T HEME T HEME light ions to an acceptor called NADP+ . . . ” Step 3: Answer = b Q What can the splitting of water lead to? a Light abso"
D14-1159,D11-1001,0,0.0772427,"Missing"
D14-1159,W04-2401,0,0.0455766,"es, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: max y,z X t,a∈At ,A bt,a,A · yt,a,A + X ct1 ,t2 ,R · zt1 ,t2 ,R t1 ,t2 ,R Here, y and z refer to all the argument and relation variables respectively. Clearly, all possible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2 -regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions."
D14-1159,D13-1177,1,0.941392,"and the accompanying dataset. We will use the example in Figure 1 as our running example throughout the paper. Our goal is to tackle a complex reading comprehension setting that centers on understanding the underlying meaning of a process description. We target a multiple-choice setting in which each input consists of a paragraph of text describing a biological process, a question, and two possible answers. The goal is to identify the correct answer using the text (Figure 1, left). We used the 148 paragraphs from the textbook Biology (Campbell and Reece, 2005) that were manually identified by Scaria et al. (2013). We extended this set to 200 paragraphs by including additional paragraphs that describe biological processes. Each paragraph in the collection represents a single biological process and describes a set of events, their participants and their interactions. Because we target understanding of paragraph meaning, we use the following desiderata for building the corpus of questions and answers: 1. The questions should focus on the events and entities participating in the process described in the paragraph, and answering the questions should require reasoning about the relations between those event"
D14-1159,N06-1056,0,0.0533243,"and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future res"
D14-1159,P09-1046,0,0.0164746,"s that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contr"
D14-1159,W09-1401,0,\N,Missing
D14-1159,E12-2021,0,\N,Missing
D18-1190,Q13-1005,0,0.0953072,"executable logical forms, is a key paradigm in developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Berant and Liang, 2015). The recent success of conversational interfaces such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana has led to soaring interest in developing methodologies for training semantic parsers quickly in any new domain and from little data. Prior work focused on alleviating data collection by training from weak supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), or developing protocols for fast data collection through paraphrasing (Berant and Liang, 2014; Wang et al., 2015) or a human-in-the-loop (Iyer et al., 2017). However, all these approaches rely on supervised training data in the target domain and ignore data collected previously for other domains. In this paper, we propose an alternative, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as in semantic parsing each dataset is associated with its o"
D18-1190,P14-1133,1,0.815706,"y, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Berant and Liang, 2015). The recent success of conversational interfaces such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana has led to soaring interest in developing methodologies for training semantic parsers quickly in any new domain and from little data. Prior work focused on alleviating data collection by training from weak supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), or developing protocols for fast data collection through paraphrasing (Berant and Liang, 2014; Wang et al., 2015) or a human-in-the-loop (Iyer et al., 2017). However, all these approaches rely on supervised training data in the target domain and ignore data collected previously for other domains. In this paper, we propose an alternative, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as in semantic parsing each dataset is associated with its own knowledge-base (KB) and thus all target domain KB constants (relations and entities) are uno"
D18-1190,Q15-1039,1,0.845012,"e … Field.QA ⊓ Type.Article ⊓ PubYear.2018 Figure 1: A test utterance is delexicalized (1) and mapped to its abstract logical form (2). Slots (“$” variables) are then aligned to the abstract utterance (3), and are filled with the top assignment in terms of local and global scores (4). Logical forms throughout this paper are in λ-DCS (Liang, 2013). Introduction Semantic parsing, the task of mapping natural language utterances into executable logical forms, is a key paradigm in developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Berant and Liang, 2015). The recent success of conversational interfaces such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana has led to soaring interest in developing methodologies for training semantic parsers quickly in any new domain and from little data. Prior work focused on alleviating data collection by training from weak supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), or developing protocols for fast data collection through paraphrasing (Berant and Liang, 2014; Wang et al., 2015) or a human-in-the-loop (Iyer et al., 2017). Ho"
D18-1190,P15-1127,0,0.0216989,"he lexicon with rules. In addition, since some spatial relations in this domain are semantically similar (Length, Width and Height), we found it hard to rank them correctly during inference. This stresses that in our framework, we assume KB constants to be sufficiently distinguishable in the pre-trained embedding space, which is not always the case. 5 Related Work While zero-shot executable semantic parsing is still under-explored, some works focused on the open-vocabulary setting which handles unseen relations by replacing a formal KB with a probabilistic database learned from a text corpus (Choi et al., 2015; Gardner and Krishnamurthy, 2017). Our abstract utterance representation is related to other attempts to generate intermediate representations that improve generalization such as dependency trees (Reddy et al., 2016), syntactic CCG parses (Krishnamurthy and Mitchell, 2015), abstract templates (Abujabal et al., 2017; Goldman et al., 2018) or masked enitites (Dong and Lapata, 2016). Our abstract logical form representation is similar to that Dong and Lapata (2018) used in to guide the decoding of the full logical form. The main difference with our work is that we focus on a comprehensive abstra"
D18-1190,P17-1089,0,0.0554471,"erant and Liang, 2015). The recent success of conversational interfaces such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana has led to soaring interest in developing methodologies for training semantic parsers quickly in any new domain and from little data. Prior work focused on alleviating data collection by training from weak supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), or developing protocols for fast data collection through paraphrasing (Berant and Liang, 2014; Wang et al., 2015) or a human-in-the-loop (Iyer et al., 2017). However, all these approaches rely on supervised training data in the target domain and ignore data collected previously for other domains. In this paper, we propose an alternative, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as in semantic parsing each dataset is associated with its own knowledge-base (KB) and thus all target domain KB constants (relations and entities) are unobserved at training time. Moreover, this is a natural use-case"
D18-1190,P16-1004,0,0.168563,"shot executable semantic parsing is still under-explored, some works focused on the open-vocabulary setting which handles unseen relations by replacing a formal KB with a probabilistic database learned from a text corpus (Choi et al., 2015; Gardner and Krishnamurthy, 2017). Our abstract utterance representation is related to other attempts to generate intermediate representations that improve generalization such as dependency trees (Reddy et al., 2016), syntactic CCG parses (Krishnamurthy and Mitchell, 2015), abstract templates (Abujabal et al., 2017; Goldman et al., 2018) or masked enitites (Dong and Lapata, 2016). Our abstract logical form representation is similar to that Dong and Lapata (2018) used in to guide the decoding of the full logical form. The main difference with our work is that we focus on a comprehensive abstract representation tailored for zero-shot semantic parsing. It is worth mentioning other work that inspected various aspects of zero-shot parsing. Bapna et al. (2017) focused on frame semantic parsing, and assumed that relations appear across different domains to learn a better mapping in the target domain. Also in frame semantic parsing, Ferreira et al. (2015) utilized word embedd"
D18-1190,P16-1002,0,0.207241,"Missing"
D18-1190,P18-1068,0,0.0847672,"open-vocabulary setting which handles unseen relations by replacing a formal KB with a probabilistic database learned from a text corpus (Choi et al., 2015; Gardner and Krishnamurthy, 2017). Our abstract utterance representation is related to other attempts to generate intermediate representations that improve generalization such as dependency trees (Reddy et al., 2016), syntactic CCG parses (Krishnamurthy and Mitchell, 2015), abstract templates (Abujabal et al., 2017; Goldman et al., 2018) or masked enitites (Dong and Lapata, 2016). Our abstract logical form representation is similar to that Dong and Lapata (2018) used in to guide the decoding of the full logical form. The main difference with our work is that we focus on a comprehensive abstract representation tailored for zero-shot semantic parsing. It is worth mentioning other work that inspected various aspects of zero-shot parsing. Bapna et al. (2017) focused on frame semantic parsing, and assumed that relations appear across different domains to learn a better mapping in the target domain. Also in frame semantic parsing, Ferreira et al. (2015) utilized word embeddings to map words to unseen KB relations. Finally, Lake and Baroni (2017) inspected"
D18-1190,Q15-1019,0,0.0238316,"ufficiently distinguishable in the pre-trained embedding space, which is not always the case. 5 Related Work While zero-shot executable semantic parsing is still under-explored, some works focused on the open-vocabulary setting which handles unseen relations by replacing a formal KB with a probabilistic database learned from a text corpus (Choi et al., 2015; Gardner and Krishnamurthy, 2017). Our abstract utterance representation is related to other attempts to generate intermediate representations that improve generalization such as dependency trees (Reddy et al., 2016), syntactic CCG parses (Krishnamurthy and Mitchell, 2015), abstract templates (Abujabal et al., 2017; Goldman et al., 2018) or masked enitites (Dong and Lapata, 2016). Our abstract logical form representation is similar to that Dong and Lapata (2018) used in to guide the decoding of the full logical form. The main difference with our work is that we focus on a comprehensive abstract representation tailored for zero-shot semantic parsing. It is worth mentioning other work that inspected various aspects of zero-shot parsing. Bapna et al. (2017) focused on frame semantic parsing, and assumed that relations appear across different domains to learn a bet"
D18-1190,N13-1073,0,0.0190491,"ate by using a global inference procedure (Section 3.5). Thus, our goal is to learn a model that given an abstract utterance-logical form pair (xabs , z abs ) produces an alignment matrix A, where Aij correabs sponds to the alignment probability p(xabs j |zi ). A central challenge is that no gold alignments are provided in any domain. Therefore, we adopt a “distillation approach”, where we train a supervised model over abstract examples to mimic the predictions of an unsupervised model that has access to the full lexicalized examples. Specifically, we use a standard unsupervised word aligner (Dyer et al., 2013), which takes all d lexicalized examples {(xi , zi )}N i=1 in all D domains and produces an Alignment matrix A∗ for every example, where A∗ij = 1 iff token i in the logical form is aligned to token j in the utterance. Then, we treat A∗ as gold alignments and gener1622 ate examples (xabs , z abs , A∗ ) to train the aligner. Learning alignments over abstract representations is possible, as a slot in a specific context tends to align to specific types of abstract words (e.g., Figure 3 suggests that a relation that is aggregated, often aligns to the NOUN that appears after the NUM in the abstract"
D18-1190,W17-2607,0,0.302302,"pose an alternative, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as in semantic parsing each dataset is associated with its own knowledge-base (KB) and thus all target domain KB constants (relations and entities) are unobserved at training time. Moreover, this is a natural use-case as more and more conversational interfaces are developed in multiple domains. Our approach is motivated by recent work (Herzig and Berant, 2017; Su and Yan, 2017; Fan et al., 2017; Richardson et al., 2018) that showed that while the lexicon and KB constants in dif1619 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1619–1629 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ferent domains vary, the structure of language composition repeats across domains. Therefore, we propose that by abstracting away the domainspecific lexical items of an utterance, we can learn to map the structure of an abstract utterance to an abstract logical form that does not include any domain-specifi"
D18-1190,P18-1168,1,0.843123,"lways the case. 5 Related Work While zero-shot executable semantic parsing is still under-explored, some works focused on the open-vocabulary setting which handles unseen relations by replacing a formal KB with a probabilistic database learned from a text corpus (Choi et al., 2015; Gardner and Krishnamurthy, 2017). Our abstract utterance representation is related to other attempts to generate intermediate representations that improve generalization such as dependency trees (Reddy et al., 2016), syntactic CCG parses (Krishnamurthy and Mitchell, 2015), abstract templates (Abujabal et al., 2017; Goldman et al., 2018) or masked enitites (Dong and Lapata, 2016). Our abstract logical form representation is similar to that Dong and Lapata (2018) used in to guide the decoding of the full logical form. The main difference with our work is that we focus on a comprehensive abstract representation tailored for zero-shot semantic parsing. It is worth mentioning other work that inspected various aspects of zero-shot parsing. Bapna et al. (2017) focused on frame semantic parsing, and assumed that relations appear across different domains to learn a better mapping in the target domain. Also in frame semantic parsing,"
D18-1190,P17-2098,1,0.910016,"ly for other domains. In this paper, we propose an alternative, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as in semantic parsing each dataset is associated with its own knowledge-base (KB) and thus all target domain KB constants (relations and entities) are unobserved at training time. Moreover, this is a natural use-case as more and more conversational interfaces are developed in multiple domains. Our approach is motivated by recent work (Herzig and Berant, 2017; Su and Yan, 2017; Fan et al., 2017; Richardson et al., 2018) that showed that while the lexicon and KB constants in dif1619 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1619–1629 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ferent domains vary, the structure of language composition repeats across domains. Therefore, we propose that by abstracting away the domainspecific lexical items of an utterance, we can learn to map the structure of an abstract utterance to an abstract logical form that"
D18-1190,D13-1161,0,0.0309423,"l language utterances into executable logical forms, is a key paradigm in developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Berant and Liang, 2015). The recent success of conversational interfaces such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana has led to soaring interest in developing methodologies for training semantic parsers quickly in any new domain and from little data. Prior work focused on alleviating data collection by training from weak supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), or developing protocols for fast data collection through paraphrasing (Berant and Liang, 2014; Wang et al., 2015) or a human-in-the-loop (Iyer et al., 2017). However, all these approaches rely on supervised training data in the target domain and ignore data collected previously for other domains. In this paper, we propose an alternative, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as in semantic parsing each da"
D18-1190,D11-1140,0,0.0267044,".15 Author 0.57 Field Venue … Field.QA ⊓ Type.Article ⊓ PubYear.2018 Figure 1: A test utterance is delexicalized (1) and mapped to its abstract logical form (2). Slots (“$” variables) are then aligned to the abstract utterance (3), and are filled with the top assignment in terms of local and global scores (4). Logical forms throughout this paper are in λ-DCS (Liang, 2013). Introduction Semantic parsing, the task of mapping natural language utterances into executable logical forms, is a key paradigm in developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Berant and Liang, 2015). The recent success of conversational interfaces such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana has led to soaring interest in developing methodologies for training semantic parsers quickly in any new domain and from little data. Prior work focused on alleviating data collection by training from weak supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), or developing protocols for fast data collection through paraphrasing (Berant and Liang, 2014; Wang et al., 2015) or a human-in-the-loo"
D18-1190,P11-1060,0,0.133866,"sk of mapping natural language utterances into executable logical forms, is a key paradigm in developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Berant and Liang, 2015). The recent success of conversational interfaces such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana has led to soaring interest in developing methodologies for training semantic parsers quickly in any new domain and from little data. Prior work focused on alleviating data collection by training from weak supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), or developing protocols for fast data collection through paraphrasing (Berant and Liang, 2014; Wang et al., 2015) or a human-in-the-loop (Iyer et al., 2017). However, all these approaches rely on supervised training data in the target domain and ignore data collected previously for other domains. In this paper, we propose an alternative, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as i"
D18-1190,D15-1166,0,0.437527,"al Semantic Parsing Sequence-tosequence models (Sutskever et al., 2014) were recently proposed for semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016). In this setting, a sequence of input language tokens x1 , . . . , xm is mapped to a sequence of output logical tokens z1 , . . . , zn . We briefly review the model by Jia and Liang (2016), which we use as part of our framework, and also as a baseline. The encoder is a BiLSTM (Hochreiter and Schmidhuber, 1997) that converts x1 , . . . , xm into a sequence of context sensitive states. The attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) is an LSTM language model additionally conditioned on the encoder states. Formally, the decoder is defined by: p(zj = w |x, z1:j−1 ) ∝ exp(U [sj , cj ]), sj+1 = LST M ([φ(out) (zj ), cj ], sj ), where sj are decoder states, U and the embedding function φ(out) are the decoder parameters, and the context vector, cj , is the result of global attention (Luong et al., 2015). We also employ attention-based copying (Jia and Liang, 2016), but omit details for brevity. Semantic Parsing over Multiple KBs Recently, Herzig and Berant (2017), Su and Yan (2017) and Fan et al. (2017) proposed to exploit str"
D18-1190,P14-5010,0,0.00481317,"Missing"
D18-1190,D14-1162,0,0.0799367,"Missing"
D18-1190,Q16-1010,0,0.02844,"Missing"
D18-1190,N18-1066,1,0.841268,"e, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as in semantic parsing each dataset is associated with its own knowledge-base (KB) and thus all target domain KB constants (relations and entities) are unobserved at training time. Moreover, this is a natural use-case as more and more conversational interfaces are developed in multiple domains. Our approach is motivated by recent work (Herzig and Berant, 2017; Su and Yan, 2017; Fan et al., 2017; Richardson et al., 2018) that showed that while the lexicon and KB constants in dif1619 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1619–1629 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ferent domains vary, the structure of language composition repeats across domains. Therefore, we propose that by abstracting away the domainspecific lexical items of an utterance, we can learn to map the structure of an abstract utterance to an abstract logical form that does not include any domain-specific KB constants, using data"
D18-1190,D17-1127,0,0.662399,"this paper, we propose an alternative, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as in semantic parsing each dataset is associated with its own knowledge-base (KB) and thus all target domain KB constants (relations and entities) are unobserved at training time. Moreover, this is a natural use-case as more and more conversational interfaces are developed in multiple domains. Our approach is motivated by recent work (Herzig and Berant, 2017; Su and Yan, 2017; Fan et al., 2017; Richardson et al., 2018) that showed that while the lexicon and KB constants in dif1619 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1619–1629 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ferent domains vary, the structure of language composition repeats across domains. Therefore, we propose that by abstracting away the domainspecific lexical items of an utterance, we can learn to map the structure of an abstract utterance to an abstract logical form that does not include"
D18-1190,P15-1129,1,0.893218,"Collins, 2005; Kwiatkowski et al., 2011; Berant and Liang, 2015). The recent success of conversational interfaces such as Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana has led to soaring interest in developing methodologies for training semantic parsers quickly in any new domain and from little data. Prior work focused on alleviating data collection by training from weak supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), or developing protocols for fast data collection through paraphrasing (Berant and Liang, 2014; Wang et al., 2015) or a human-in-the-loop (Iyer et al., 2017). However, all these approaches rely on supervised training data in the target domain and ignore data collected previously for other domains. In this paper, we propose an alternative, zeroshot approach to semantic parsing, where no labeled or unlabeled examples are provided in the target domain, but annotated examples from other domains are available. This is a challenging setup as in semantic parsing each dataset is associated with its own knowledge-base (KB) and thus all target domain KB constants (relations and entities) are unobserved at training"
D18-1190,W10-2903,0,\N,Missing
D18-1190,J13-2005,0,\N,Missing
D19-1107,P18-2114,1,0.829918,"majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators"
D19-1107,D12-1091,0,0.0497685,"Missing"
D19-1107,D15-1075,0,0.0532422,"s. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In t"
D19-1107,N15-4002,0,0.0604716,"Missing"
D19-1107,P18-1128,0,0.0258084,"Missing"
D19-1107,N19-1246,0,0.119133,"Missing"
D19-1107,P18-2103,1,0.854507,"using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by"
D19-1107,N18-2017,0,0.0590183,"ssively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator inf"
D19-1107,N15-1098,0,0.0331621,"o produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generate"
D19-1107,D18-1260,0,0.0869546,"atterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators that were not seen at training time. We observe that often generalization to new annotators fails, and that augmenting the training set with a small number of examples from these annotators substantially increase"
D19-1107,Q18-1040,0,0.0271845,"s raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators that were not seen"
D19-1107,S18-2023,0,0.0357123,"aving only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we"
D19-1107,D16-1264,0,0.0698024,"that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue r"
D19-1107,P18-2124,0,0.029158,"hese annotators substantially increases performance. Taken together, our experiments show that annotator bias exists in current NLU datasets, which can lead to problems in model generalization to new users. Hence, we propose that annotator bias should be monitored at data collection time and to tackle it by having the test set include examples from a disjoint set of annotators. 2 Crowdsourcing Practice Crowdsourcing has become the prominent paradigm for creating NLP datasets (CallisonBurch et al., 2015; Sabou et al., 2014). It has been used for various NLU tasks, including Question Answering (Rajpurkar et al., 2018; Mihaylov 1161 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1161–1166, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 3 Experimental Setup We focus on crowdsourcing tasks where workers produce full-length sentences. We first describe the datasets we test our hypothesis on, and then provide details on the model and training. Datasets We consider recent NLU datasets, for which the annotator IDs are available. • MNLI (matched) (William"
D19-1107,N18-1101,0,0.204621,"an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators that were not seen at training time. We observe that often generalization to new annotators fails, and that augmenting the training set with a small number of examples fro"
D19-1107,P11-1122,0,0.0414743,"models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to"
D19-1107,D13-1020,0,0.0488703,"most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of mo"
D19-1107,sabou-etal-2014-corpus,0,0.0229497,"notators fails, and that augmenting the training set with a small number of examples from these annotators substantially increases performance. Taken together, our experiments show that annotator bias exists in current NLU datasets, which can lead to problems in model generalization to new users. Hence, we propose that annotator bias should be monitored at data collection time and to tackle it by having the test set include examples from a disjoint set of annotators. 2 Crowdsourcing Practice Crowdsourcing has become the prominent paradigm for creating NLP datasets (CallisonBurch et al., 2015; Sabou et al., 2014). It has been used for various NLU tasks, including Question Answering (Rajpurkar et al., 2018; Mihaylov 1161 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1161–1166, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 3 Experimental Setup We focus on crowdsourcing tasks where workers produce full-length sentences. We first describe the datasets we test our hypothesis on, and then provide details on the model and training. Datasets We cons"
D19-1107,K17-1004,0,0.0163273,"ity annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illust"
D19-1107,N19-1421,1,0.810675,"ral model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators that were not seen at training time. We observe that often generalization to new annotators fails, and that augmenting the training set with a small number of examples from these annotators substantially increases performance. Taken together, our experime"
D19-1107,W17-2623,0,0.0270496,"generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understa"
D19-1107,L18-1239,0,0.0274676,"ers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether mod"
D19-1378,P19-1448,1,0.90392,"quent word ‘nation’ is similar to the DB column country which belongs to the table singer, thus selecting the column singer.name from the same table is more likely. Second, the next appearance of the word ‘name’ is next to the phrase ’Hey’, which appears as the value in one of the cells of the column song.name. Assuming a one-toone mapping between words and DB constants, again singer.name is preferred. In this paper, we propose a semantic parser that reasons over the DB structure and question to make a global decision about which DB constants should be used in a query. We extend the parser of Bogin et al. (2019), which learns a representation for the 3659 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3659–3664, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics DB schema at parsing time. First, we perform message-passing through a graph neural network representation of the DB schema, to softly select the set of DB constants that are likely to appear in the output query. Second, we train a model that takes the top-K queries output by the autoregr"
D19-1378,J05-1003,0,0.0672435,"DB constants that appear in P y. We can now P add a relevance loss term global global − v∈Uy log ρv − v∈U ) to the / y log(1−ρv objective. Thus, the parameters of the gating GCN are trained from the relevance loss and the usual decoding loss, a ML objective over the gold sequence of decisions that output the query y. Discriminative re-ranking Global gating provides a more accurate model for softly predicting the correct subset of DB constants. However, parsing is still auto-regressive and performed with a local similarity function. To overcome this, we separately train a discriminative model (Collins and Koo, 2005; Ge and Mooney, 2006; Lu et al., 2008; Fried et al., 2017) to re-rank the top-K queries in the decoder’s output beam. The re-ranker scores each candidate tuple (x, S, yˆ), and thus can globally reason over the entire candidate query yˆ. We focus the re-ranker capacity on the main pain point of zero-shot parsing – the set of DB constants Uyˆ that appear in yˆ. At a high-level (Figure 3), for each candidate we compute a logit syˆ = w&gt; F F (fUyˆ , ealign ), where w is a learned 3661 parameter vector, fUyˆ is a representation for the set Uyˆ, and ealign is a representation for the global alignmen"
D19-1378,N19-1240,0,0.0805353,"Missing"
D19-1378,P17-2025,0,0.0308538,"loss term global global − v∈Uy log ρv − v∈U ) to the / y log(1−ρv objective. Thus, the parameters of the gating GCN are trained from the relevance loss and the usual decoding loss, a ML objective over the gold sequence of decisions that output the query y. Discriminative re-ranking Global gating provides a more accurate model for softly predicting the correct subset of DB constants. However, parsing is still auto-regressive and performed with a local similarity function. To overcome this, we separately train a discriminative model (Collins and Koo, 2005; Ge and Mooney, 2006; Lu et al., 2008; Fried et al., 2017) to re-rank the top-K queries in the decoder’s output beam. The re-ranker scores each candidate tuple (x, S, yˆ), and thus can globally reason over the entire candidate query yˆ. We focus the re-ranker capacity on the main pain point of zero-shot parsing – the set of DB constants Uyˆ that appear in yˆ. At a high-level (Figure 3), for each candidate we compute a logit syˆ = w&gt; F F (fUyˆ , ealign ), where w is a learned 3661 parameter vector, fUyˆ is a representation for the set Uyˆ, and ealign is a representation for the global alignment between question words and DB constants. The re-ranker is"
D19-1378,P06-2034,0,0.0647933,"r in P y. We can now P add a relevance loss term global global − v∈Uy log ρv − v∈U ) to the / y log(1−ρv objective. Thus, the parameters of the gating GCN are trained from the relevance loss and the usual decoding loss, a ML objective over the gold sequence of decisions that output the query y. Discriminative re-ranking Global gating provides a more accurate model for softly predicting the correct subset of DB constants. However, parsing is still auto-regressive and performed with a local similarity function. To overcome this, we separately train a discriminative model (Collins and Koo, 2005; Ge and Mooney, 2006; Lu et al., 2008; Fried et al., 2017) to re-rank the top-K queries in the decoder’s output beam. The re-ranker scores each candidate tuple (x, S, yˆ), and thus can globally reason over the entire candidate query yˆ. We focus the re-ranker capacity on the main pain point of zero-shot parsing – the set of DB constants Uyˆ that appear in yˆ. At a high-level (Figure 3), for each candidate we compute a logit syˆ = w&gt; F F (fUyˆ , ealign ), where w is a learned 3661 parameter vector, fUyˆ is a representation for the set Uyˆ, and ealign is a representation for the global alignment between question wo"
D19-1378,D18-1190,1,0.893009,"Missing"
D19-1378,D17-1160,1,0.868222,"hema S was not seen at training time. A DB schema S includes : (a) A set of DB tables, (b) a set of columns for each table, and (c) a set of foreign key-primary key column pairs where each pair is a relation from a foreign-key in one table to a primary-key in another. Schema tables and columns are termed DB constants, denoted by V. We now describe a recent semantic parser from Bogin et al. (2019), focusing on the components relevant for selecting DB constants. Base Model The base parser is a standard topdown semantic parser with grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Lin et al., 2019). The input question (x1 , . . . , x|x |) is encoded with a BiLSTM, where the hidden states ei of the BiLSTM are used as contextualized representations for the word xi . The output query y is decoded top-down with another LSTM using a SQL grammar, where at each time step a grammar rule is decoded. Our main focus is decoding of DB constants, and we will elaborate on this part. The parser decodes a DB constant whenever the previous step decoded the non-terminals Table or Column. To select the DB constant, it first computes an attention distribution ove"
D19-1378,D08-1082,0,0.0473698,"P add a relevance loss term global global − v∈Uy log ρv − v∈U ) to the / y log(1−ρv objective. Thus, the parameters of the gating GCN are trained from the relevance loss and the usual decoding loss, a ML objective over the gold sequence of decisions that output the query y. Discriminative re-ranking Global gating provides a more accurate model for softly predicting the correct subset of DB constants. However, parsing is still auto-regressive and performed with a local similarity function. To overcome this, we separately train a discriminative model (Collins and Koo, 2005; Ge and Mooney, 2006; Lu et al., 2008; Fried et al., 2017) to re-rank the top-K queries in the decoder’s output beam. The re-ranker scores each candidate tuple (x, S, yˆ), and thus can globally reason over the entire candidate query yˆ. We focus the re-ranker capacity on the main pain point of zero-shot parsing – the set of DB constants Uyˆ that appear in yˆ. At a high-level (Figure 3), for each candidate we compute a logit syˆ = w&gt; F F (fUyˆ , ealign ), where w is a learned 3661 parameter vector, fUyˆ is a representation for the set Uyˆ, and ealign is a representation for the global alignment between question words and DB consta"
D19-1378,P17-1105,0,0.0423086,"ing time. A DB schema S includes : (a) A set of DB tables, (b) a set of columns for each table, and (c) a set of foreign key-primary key column pairs where each pair is a relation from a foreign-key in one table to a primary-key in another. Schema tables and columns are termed DB constants, denoted by V. We now describe a recent semantic parser from Bogin et al. (2019), focusing on the components relevant for selecting DB constants. Base Model The base parser is a standard topdown semantic parser with grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Lin et al., 2019). The input question (x1 , . . . , x|x |) is encoded with a BiLSTM, where the hidden states ei of the BiLSTM are used as contextualized representations for the word xi . The output query y is decoded top-down with another LSTM using a SQL grammar, where at each time step a grammar rule is decoded. Our main focus is decoding of DB constants, and we will elaborate on this part. The parser decodes a DB constant whenever the previous step decoded the non-terminals Table or Column. To select the DB constant, it first computes an attention distribution over the question |x| words"
D19-1378,C18-1280,0,0.0292329,"the decoding sequence, especially in a top-down parser. DB schema encoding In the zero-shot setting, the schema structure of a new DB can affect the output query. To capture DB structure, Bogin et al. (2019) learned a representation hv for every DB constant, which the parser later used at decoding time. This was done by converting the DB schema into a graph, where nodes are DB constants, and edges connect tables and their columns, as well as primary and foreign keys (Figure 2, left). A graph convolutional network (GCN) then learned representations hv for nodes end-to-end (De Cao et al., 2019; Sorokin and Gurevych, 2018). To focus the GCN’s capacity on important nodes, a relevance probability ρv was computed for every node, and used to “gate” the input to the GCN, conditioned on the question. Specifically, given a learned embedding rv for every database constant, (0) the GCN input is hv = ρv · rv . Then, the GCN recurrence is applied for L steps. At each step, nodes re-compute their representation based on the representation of their neighbors, where different edge types are associated with different learned parameters (Li et al., 2016). The final representation (L) of each DB constant is hv = hv . Importantl"
D19-1378,P16-1127,0,0.0321369,"he correct SQL query. Importantly, the schema S was not seen at training time. A DB schema S includes : (a) A set of DB tables, (b) a set of columns for each table, and (c) a set of foreign key-primary key column pairs where each pair is a relation from a foreign-key in one table to a primary-key in another. Schema tables and columns are termed DB constants, denoted by V. We now describe a recent semantic parser from Bogin et al. (2019), focusing on the components relevant for selecting DB constants. Base Model The base parser is a standard topdown semantic parser with grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Lin et al., 2019). The input question (x1 , . . . , x|x |) is encoded with a BiLSTM, where the hidden states ei of the BiLSTM are used as contextualized representations for the word xi . The output query y is decoded top-down with another LSTM using a SQL grammar, where at each time step a grammar rule is decoded. Our main focus is decoding of DB constants, and we will elaborate on this part. The parser decodes a DB constant whenever the previous step decoded the non-terminals Table or Column. To select the DB constan"
D19-1378,P17-1041,0,0.0281408,"y. Importantly, the schema S was not seen at training time. A DB schema S includes : (a) A set of DB tables, (b) a set of columns for each table, and (c) a set of foreign key-primary key column pairs where each pair is a relation from a foreign-key in one table to a primary-key in another. Schema tables and columns are termed DB constants, denoted by V. We now describe a recent semantic parser from Bogin et al. (2019), focusing on the components relevant for selecting DB constants. Base Model The base parser is a standard topdown semantic parser with grammar-based decoding (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Lin et al., 2019). The input question (x1 , . . . , x|x |) is encoded with a BiLSTM, where the hidden states ei of the BiLSTM are used as contextualized representations for the word xi . The output query y is decoded top-down with another LSTM using a SQL grammar, where at each time step a grammar rule is decoded. Our main focus is decoding of DB constants, and we will elaborate on this part. The parser decodes a DB constant whenever the previous step decoded the non-terminals Table or Column. To select the DB constant, it first computes a"
D19-1378,D18-1193,0,0.166296,"Missing"
D19-1378,D18-1425,0,0.122647,"Missing"
D19-1394,Q13-1005,0,0.0785711,"ney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Semantic parsing models rely on supervised training data that pairs natural language utterances with logical forms. Alas, such data does not occur naturally, especially in virtual assistants that are meant to support thousands of different applications and use-cases. Thus, efficient data collection is perhaps the most pressing problem for scalable conversational interfaces. In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including training from denotations (Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), semisupervised learning (Kocisk´y et al., 2016; Yin et al., 2018), human in the loop (Iyer et al., 2017; Lawrence and Riezler, 2018), and training on examples from other domains (Herzig and Berant, 2017, 2018; Su and Yan, 2017). One prominent approach for data collection was introduced by Wang et al. (2015), termed OVERNIGHT. In this method, one automatically generates logical forms for an application from a grammar, paired with pseudo-language utterances. These pseudolanguage utterances are then shown to crowd workers, who are able to understand them and paraphrase them into natural languag"
D19-1394,P17-1152,0,0.0259009,"Missing"
D19-1394,N19-2003,0,0.0119007,"owd workers, who are able to understand them and paraphrase them into natural language, resulting 3810 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3810–3820, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics in a supervised dataset. The OVERNIGHT procedure has been adopted and extended both within semantic parsing (Locascio et al., 2016; Ravichander et al., 2017; Zhong et al., 2017; Cheng et al., 2018), in dialogue (Shah et al., 2018; Damonte et al., 2019) and in visual reasoning (Johnson et al., 2017; Hudson and Manning, 2019). While the OVERNIGHT approach is appealing since it generates training data from scratch, it suffers from a major drawback – training a parser on data generated from OVERNIGHT and testing on utterances collected from a target distribution results in a significant drop in performance (Wang et al., 2015; Ravichander et al., 2017). In this paper, we thoroughly analyze the sources of mismatch between a target distribution and the distribution induced by OVERNIGHT, and propose a new method for overcoming this mismatch. We dec"
D19-1394,P16-1004,0,0.0739832,"Missing"
D19-1394,P18-1033,0,0.311678,"Missing"
D19-1394,P17-2098,1,0.869341,"naturally, especially in virtual assistants that are meant to support thousands of different applications and use-cases. Thus, efficient data collection is perhaps the most pressing problem for scalable conversational interfaces. In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including training from denotations (Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), semisupervised learning (Kocisk´y et al., 2016; Yin et al., 2018), human in the loop (Iyer et al., 2017; Lawrence and Riezler, 2018), and training on examples from other domains (Herzig and Berant, 2017, 2018; Su and Yan, 2017). One prominent approach for data collection was introduced by Wang et al. (2015), termed OVERNIGHT. In this method, one automatically generates logical forms for an application from a grammar, paired with pseudo-language utterances. These pseudolanguage utterances are then shown to crowd workers, who are able to understand them and paraphrase them into natural language, resulting 3810 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3810–3820, c Hong"
D19-1394,D18-1190,1,0.90589,"Missing"
D19-1394,P11-1060,0,0.0491533,"phrase: • number of paper whose keyphrase is parsing • paper whose keyphrase is parsing and that has the largest publication year • paper whose keyphrase is parsing • N/A 3. train ??0? (2) using all aggregated annotations Figure 1: An overview of G R A NNO, a method for annotating unlabeled utterances with their logical forms. Introduction Conversing with a virtual assistant in natural language is one of the most exciting current applications of semantic parsing, the task of mapping natural language utterances to executable logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Semantic parsing models rely on supervised training data that pairs natural language utterances with logical forms. Alas, such data does not occur naturally, especially in virtual assistants that are meant to support thousands of different applications and use-cases. Thus, efficient data collection is perhaps the most pressing problem for scalable conversational interfaces. In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including training from denotations (Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), semisupervised learning (Kocisk"
D19-1394,D16-1197,0,0.0607578,"Missing"
D19-1394,P17-1089,0,0.6377,"ata that pairs natural language utterances with logical forms. Alas, such data does not occur naturally, especially in virtual assistants that are meant to support thousands of different applications and use-cases. Thus, efficient data collection is perhaps the most pressing problem for scalable conversational interfaces. In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including training from denotations (Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), semisupervised learning (Kocisk´y et al., 2016; Yin et al., 2018), human in the loop (Iyer et al., 2017; Lawrence and Riezler, 2018), and training on examples from other domains (Herzig and Berant, 2017, 2018; Su and Yan, 2017). One prominent approach for data collection was introduced by Wang et al. (2015), termed OVERNIGHT. In this method, one automatically generates logical forms for an application from a grammar, paired with pseudo-language utterances. These pseudolanguage utterances are then shown to crowd workers, who are able to understand them and paraphrase them into natural language, resulting 3810 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"
D19-1394,D14-1162,0,0.0838524,"Missing"
D19-1394,P16-1002,0,0.312344,"Missing"
D19-1394,N18-1202,0,0.0117435,"wd workers. We trained a neural semantic parser on both Dnat (named S UPERVISED) and Dlang (named OVERNIGHT- ORACLE - LF) using the original train/development split. Results in Table 4 show that for each domain, a decrease of approximately 9 points in accuracy occurs only due to the language mismatch, even with high-quality workers. This gap is likely to grow when workers are not experienced in paraphrasing, or are unfamiliar with the domain they generate paraphrases for. We additionally observe that when we use GloVe embeddings (Pennington et al., 2014) instead of contextual ELMo embeddings (Peters et al., 2018), the gap is even higher. This shows that better representations reduce the language gap, though it is still substantial. 4 Grammar-driven Annotation To overcome the logical form and language mismatches discussed, we propose in this section a new data generation procedure that does not suffer from these shortcomings. Our procedure, named G R A NNO, relies on two assumptions. First, unlike OVERNIGHT, we assume access to unlabeled ul utterances Xul = {xi }N i=1 . These can typically be found in query logs, or generated by users experimenting with a prototype. Second, we assume a scoring function"
D19-1394,D16-1116,0,0.0690224,"Missing"
D19-1394,P17-1014,0,0.0212334,"ck to detect wrong parses and 3817 sent them to expert annotation. Lawrence and Riezler (2018) and Berant et al. (2019) improved a supervised parser by showing its predictions to users and training from this feedback. Gupta et al. (2018) built a hierarchical annotation scheme for annotating utterances with multiple intents. Labutov et al. (2019) trained a semantic parser through interactive dialogues. Comparing to these works, our proposed method requires no supervised data or expert annotators, and is suitable for rapid development of parsers in multiple domains. In semi-supervised learning, Konstas et al. (2017) used self-training to improve an existing AMR parser. Others used a variational autoencoder by modeling unlabeled utterances (Kocisk´y et al., 2016) or logical forms (Yin et al., 2018) as latent variables. However, empirical gains from the unlabeled data were relatively small compared to annotating more examples. Finally, several papers extended the OVERNIGHT procedure. Ravichander et al. (2017) replaced phrases in the lexicon with images to elicit more natural language from workers. (Cheng et al., 2018) generated more complex compositional structures by splitting the canonical utterances int"
D19-1394,D13-1161,0,0.0236222,"gical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011). Semantic parsing models rely on supervised training data that pairs natural language utterances with logical forms. Alas, such data does not occur naturally, especially in virtual assistants that are meant to support thousands of different applications and use-cases. Thus, efficient data collection is perhaps the most pressing problem for scalable conversational interfaces. In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including training from denotations (Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), semisupervised learning (Kocisk´y et al., 2016; Yin et al., 2018), human in the loop (Iyer et al., 2017; Lawrence and Riezler, 2018), and training on examples from other domains (Herzig and Berant, 2017, 2018; Su and Yan, 2017). One prominent approach for data collection was introduced by Wang et al. (2015), termed OVERNIGHT. In this method, one automatically generates logical forms for an application from a grammar, paired with pseudo-language utterances. These pseudolanguage utterances are then shown to crowd workers, who are able to understand them and paraph"
D19-1394,P18-1169,0,0.158372,"ral language utterances with logical forms. Alas, such data does not occur naturally, especially in virtual assistants that are meant to support thousands of different applications and use-cases. Thus, efficient data collection is perhaps the most pressing problem for scalable conversational interfaces. In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including training from denotations (Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), semisupervised learning (Kocisk´y et al., 2016; Yin et al., 2018), human in the loop (Iyer et al., 2017; Lawrence and Riezler, 2018), and training on examples from other domains (Herzig and Berant, 2017, 2018; Su and Yan, 2017). One prominent approach for data collection was introduced by Wang et al. (2015), termed OVERNIGHT. In this method, one automatically generates logical forms for an application from a grammar, paired with pseudo-language utterances. These pseudolanguage utterances are then shown to crowd workers, who are able to understand them and paraphrase them into natural language, resulting 3810 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joi"
D19-1394,J13-2005,0,0.033424,"Missing"
D19-1394,W17-5545,0,0.326532,"ammar, paired with pseudo-language utterances. These pseudolanguage utterances are then shown to crowd workers, who are able to understand them and paraphrase them into natural language, resulting 3810 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3810–3820, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics in a supervised dataset. The OVERNIGHT procedure has been adopted and extended both within semantic parsing (Locascio et al., 2016; Ravichander et al., 2017; Zhong et al., 2017; Cheng et al., 2018), in dialogue (Shah et al., 2018; Damonte et al., 2019) and in visual reasoning (Johnson et al., 2017; Hudson and Manning, 2019). While the OVERNIGHT approach is appealing since it generates training data from scratch, it suffers from a major drawback – training a parser on data generated from OVERNIGHT and testing on utterances collected from a target distribution results in a significant drop in performance (Wang et al., 2015; Ravichander et al., 2017). In this paper, we thoroughly analyze the sources of mismatch between a target distribution and the"
D19-1394,D17-1127,0,0.101511,"al assistants that are meant to support thousands of different applications and use-cases. Thus, efficient data collection is perhaps the most pressing problem for scalable conversational interfaces. In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including training from denotations (Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), semisupervised learning (Kocisk´y et al., 2016; Yin et al., 2018), human in the loop (Iyer et al., 2017; Lawrence and Riezler, 2018), and training on examples from other domains (Herzig and Berant, 2017, 2018; Su and Yan, 2017). One prominent approach for data collection was introduced by Wang et al. (2015), termed OVERNIGHT. In this method, one automatically generates logical forms for an application from a grammar, paired with pseudo-language utterances. These pseudolanguage utterances are then shown to crowd workers, who are able to understand them and paraphrase them into natural language, resulting 3810 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3810–3820, c Hong Kong, China, November 3–7"
D19-1394,P15-1129,1,0.948404,"use-cases. Thus, efficient data collection is perhaps the most pressing problem for scalable conversational interfaces. In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including training from denotations (Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), semisupervised learning (Kocisk´y et al., 2016; Yin et al., 2018), human in the loop (Iyer et al., 2017; Lawrence and Riezler, 2018), and training on examples from other domains (Herzig and Berant, 2017, 2018; Su and Yan, 2017). One prominent approach for data collection was introduced by Wang et al. (2015), termed OVERNIGHT. In this method, one automatically generates logical forms for an application from a grammar, paired with pseudo-language utterances. These pseudolanguage utterances are then shown to crowd workers, who are able to understand them and paraphrase them into natural language, resulting 3810 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3810–3820, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics in a supervised dataset. T"
D19-1394,N19-4013,0,0.0217214,"Missing"
D19-1394,P18-1070,0,0.387877,"g models rely on supervised training data that pairs natural language utterances with logical forms. Alas, such data does not occur naturally, especially in virtual assistants that are meant to support thousands of different applications and use-cases. Thus, efficient data collection is perhaps the most pressing problem for scalable conversational interfaces. In recent years, many attempts aim to reduce the burden of data collection for semantic parsing, including training from denotations (Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013), semisupervised learning (Kocisk´y et al., 2016; Yin et al., 2018), human in the loop (Iyer et al., 2017; Lawrence and Riezler, 2018), and training on examples from other domains (Herzig and Berant, 2017, 2018; Su and Yan, 2017). One prominent approach for data collection was introduced by Wang et al. (2015), termed OVERNIGHT. In this method, one automatically generates logical forms for an application from a grammar, paired with pseudo-language utterances. These pseudolanguage utterances are then shown to crowd workers, who are able to understand them and paraphrase them into natural language, resulting 3810 Proceedings of the 2019 Conference on Empirical M"
D19-1394,W18-2501,0,\N,Missing
D19-1394,D18-1300,0,\N,Missing
D19-5815,P17-1147,0,0.0660527,"Missing"
D19-5815,W13-2322,0,0.0634753,"Missing"
D19-5815,P16-1223,0,0.0995681,"Missing"
D19-5815,P18-1078,1,0.890843,"Missing"
D19-5815,Q18-1023,0,0.0674018,"Missing"
D19-5815,N19-1300,0,0.0236782,"al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al., 2019) are two examples for such datasets. However, access to such questions is usually limited for most researchers. Communicative aspects There are many communicative aspects of text that a human implicitly understands when reading, and which could be queried in reading comprehension datasets. For instance, is a text intended to be expository, narrative, persuasive, or something else? Did the author succeed in their communicative intent? Was there some deeper metaphorical point in the text? A dataset targeted at these sorts of phenomena could be incredibly interesting, and very challenging. 4 Ways"
D19-5815,Q19-1026,0,0.011731,"swering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al., 2019) are two examples for such datasets. However, access to such questions is usually limited for most researchers. Communicative aspects There are many communicative aspects of text that a human implicitly understands when reading, and which could be queried in reading comprehension datasets. For instance, is a text intended to be expository, narrative, persuasive, or something else? Did the author succeed in their communicative intent? Was there some deeper metaphorical point in the text? A dataset targeted at these sorts of phenomena could be incredibly interest"
D19-5815,P19-1612,0,0.0261817,"a movie script that will be used for answering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al., 2019) are two examples for such datasets. However, access to such questions is usually limited for most researchers. Communicative aspects There are many communicative aspects of text that a human implicitly understands when reading, and which could be queried in reading comprehension datasets. For instance, is a text intended to be expository, narrative, persuasive, or something else? Did the author succeed in their communicative intent? Was there some deeper metaphorical point in the text? A dataset targeted at these"
D19-5815,D19-1606,1,0.90781,"ets that we construct. Paragraph-level structure While the input to a reading comprehension dataset is a paragraph of text, most datasets do not explicitly target questions that require understanding the entire paragraph, or how the sentences fit together into a coherent whole. Some post-hoc analyses attempt to reveal the percentage of questions that require more than one sentence, but it is better to design the datasets from the beginning to obtain questions that look at paragraph- or discourse-level phenomena, such as entity tracking, discourse relations, or pragmatics. For example, Quoref (Dasigi et al., 2019) is a dataset that targets entity tracking and coreference resolution. There are few linguistic formalisms targeting structures larger than a paragraph, but those that do exist, such as rhetorical structure theory (Mann and Thompson, 1988), could form the basis of an interesting and useful reading comprehension dataset. Sentence-level linguistic structure Most existing reading comprehension datasets implicitly target local predicate-argument structures. The incentives involved in the creation of SQuAD encouraged workers to create questions that were close paraphrases of some part of a paragrap"
D19-5815,D19-5808,1,0.90061,"Mary. Mary was just diagnosed with cancer. means also understanding that Bill will be sad. In some sense this can be seen as “grounding” the predicates in the text to some prior knowledge that includes the implications of that predicate, but it also includes the more general notion of reconstructing a model of the world being described by the text. There are two datasets that just scratch 107 the surface of this kind of reading: ShARC (Saeidi et al., 2018) requires reading rules and applying them to questions asked by users, though its format is not standard reading comprehension; and ROPES (Lin et al., 2019), which requires reading descriptions of causes and effects and applying them to situated questions. a similar meaning. Examples include NARRA TIVE QA (Koˇcisk` y et al., 2018), where question authors were shown a summary of a movie script that will be used for answering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which"
D19-5815,N19-1246,1,0.946639,"et local predicate-argument structures. The incentives involved in the creation of SQuAD encouraged workers to create questions that were close paraphrases of some part of a paragraph, replacing a noun phrase with a question word. This, and other cloze-style question construction, encourages very local reasoning that amounts to finding and then understanding the argument structure of a single sentence. This is an important aspect of meaning, but one could construct much harder datasets than this. One direction to push on linguistic structure is to move beyond locating a single sentence. DROP (Dua et al., 2019) largely involves the same level of linguistic structural analysis as SQuAD, but the questions require combining pieces from several parts of the passage, forcing a more comprehensive analysis of the passage contents. A separate direction one could push on sentence-level linguistic structure in reading comprehension is to target other phenomena than predicate argument structure. There are many rich problems in semantic analysis, such as negation scope, distributive vs. non-distributive coordination, factuality, deixis, briding and empty elements, preposition senses, noun compounds, and many mo"
D19-5815,D10-1123,0,0.0140052,"and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and not the full conjunction. On the other hand such relations are desired when requiring a composition to be solved in a question. For example, in the question What is the capital of the largest economy in Europe? we would like the largest economy in Europe to be one answer we can use to modify the question to what is the capital of Germany. 4.5 Adversarial construction 4.7 Minimal question pairs ROPES (Lin et al., 2019) borrowed the idea of “minimal pairs” from linguistic analys"
D19-5815,P19-1416,1,0.897168,"easoning Tasks which require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and not the full conjunction. On the other hand such relations are desired when requiring a composition to be solved in a question. For e"
D19-5815,P18-2124,0,0.0366525,"a et al., 2018). Accordingly, more recent reading comprehension datasets are constructed with several different approaches to prevent such shortcuts in order to foster natural language understanding. 4.1 4.2 “No answer” option Most of the reasoning shortcuts in existing datasets arise due to the fact that the system can assume that the answer is guaranteed to exist in the given passage. Removing this assumption and requiring the system to identify whether the question is even answerable from the passage can prevent such shortcuts. One example of this kind of dataset construction is SQ UAD 2.0(Rajpurkar et al., 2018), which asked annotators to read the given passage and write a question which the passage does not contain the answer to but contains a plausible negative answer. A drawback of this approach is that annotators see the passage when asking the question, which can introduce its own biases and shortcuts. An alternative is to combine a “no answer” option with the approach the previous section, where an annotator writes questions without knowing the answer, and another annotator verifies whether they are answerable by the paired passage. Example datasets include N EWS QA (Trischler et al., 2016)4 ,"
D19-5815,D19-1243,0,0.0520439,"Missing"
D19-5815,Q19-1016,0,0.0223257,"ntial means of avoiding reasoning shortcuts. A person is not able to answer a simple question such as How many? without the additional context of a prior question describing what is being counted. Care needs to be taken with this method, however, as some datasets are amenable to input reduction (Feng et al., 2018), where there is only one plausible answer to such a short question. If done well, however, this method provides additional challenges such as clarification, coreference resolution, and aggregation of pieces scattered across conversation history. Q UAC (Choi et al., 2018) and C O QA (Reddy et al., 2019) are two datasets that focus on such setting. 4.4 4.6 One promising means of removing reasoning shortcuts is to encode those shortcuts into a learned system, and use that system to filter out questions that are too easy during dataset construction. DROP (Dua et al., 2019) and Quoref (Dasigi et al., 2019) used a model trained on SQuAD 1.1 (Rajpurkar et al., 2016) as an “adversarial” baseline when having crowd workers write questions. Because the people could see when the system answered their questions correctly, they learned to ask harder questions. This kind of adversarial construction can in"
D19-5815,D17-1215,0,0.0795702,"Missing"
D19-5815,D13-1020,0,0.0492515,"it is not even clear what it means to understand text, or how to judge whether a machine has achieved success at this task. Much recent research in the natural language processing community has converged on an approach to this problem called machine reading comprehension, where a system is given a passage of text and a natural language question that presumably requires some level of “understanding” of the passage in order to answer. While there have been many papers in the last few years studying this basic problem, as far as we are aware, there is no paper formally justifying this approach 1 Richardson et al. (2013) give a good overview of the early history of this approach, but provide only very little justification. 2 Though SQuAD was not nearly the first reading comprehension dataset, its introduction of the span extraction format was innovative and useful, and most new datasets follow its design. 105 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 105–112 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics so this is the definition we choose, while admitting that it is not perfect. Using natural language questions to test comprehens"
D19-5815,P19-1262,0,0.0173168,"ch require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and not the full conjunction. On the other hand such relations are desired when requiring a composition to be solved in a question. For example, in the question W"
D19-5815,D18-1233,0,0.0558381,"Missing"
D19-5815,P18-1156,0,0.0337327,"the world being described by the text. There are two datasets that just scratch 107 the surface of this kind of reading: ShARC (Saeidi et al., 2018) requires reading rules and applying them to questions asked by users, though its format is not standard reading comprehension; and ROPES (Lin et al., 2019), which requires reading descriptions of causes and effects and applying them to situated questions. a similar meaning. Examples include NARRA TIVE QA (Koˇcisk` y et al., 2018), where question authors were shown a summary of a movie script that will be used for answering questions, and D UO RC (Saha et al., 2018), where questions are authored given a passage that is comparable to the one that will later be employed. Another approach is to collect questions first, and then pair them with a passage, which was done in Q UAC (Choi et al., 2018) or with a distantly collected relevant context, which was the method of choice in T RIVIAQA (Joshi et al., 2017). Last, lexical overlap can be reduced if one has access to natural questions that have been posed by users who do not know the answers and are seeking information (Lee et al., 2019). NATU RAL Q UESTIONS (Kwiatkowski et al., 2019) and B OOL Q (Clark et al"
D19-5815,N18-1059,1,0.835931,"eems unsatisfying. Overall, however, we believe this is a good method that could be used more widely when collecting reading comprehension datasets. Complex reasoning Tasks which require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functi"
D19-5815,P19-1485,1,0.878847,"Missing"
D19-5815,D18-1259,0,0.133395,"l, however, we believe this is a good method that could be used more widely when collecting reading comprehension datasets. Complex reasoning Tasks which require more advanced forms of reasoning are proposed to prevent answering the question from superficial clues. Examples include tasks requiring discrete and arithmetic reasoning (Dua et al., 2019), textbook question answering which requires understanding various forms of knowledge (Clark et al., 2018; Kembhavi et al., 2017) and multi-hop question answering which requires reading multiple distinct pieces of evidence (Talmor and Berant, 2018; Yang et al., 2018). Despite these attempts, it was found that shortcuts still exist in complex reasoning tasks such as multi-hop QA (Min et al., 2019; Jiang and Bansal, 2019), so careful construction of the dataset is necessary. One novel method that may by applied to combat such shortcuts and enforce multi-hop reasoning is to check the semantic relations present in the question. In questions requiring a conjunction to be performed, functional or pseudo functional relations (Lin et al., 2010), such as father or founder, may facilitate arriving at the correct answer by solving only the functional relation and no"
D19-5815,D18-1009,0,0.0295379,"ose shortcuts into a learned system, and use that system to filter out questions that are too easy during dataset construction. DROP (Dua et al., 2019) and Quoref (Dasigi et al., 2019) used a model trained on SQuAD 1.1 (Rajpurkar et al., 2016) as an “adversarial” baseline when having crowd workers write questions. Because the people could see when the system answered their questions correctly, they learned to ask harder questions. This kind of adversarial construction can introduce its own biases, however, especially if the questions being filtered are generated by machines instead of humans (Zellers et al., 2018). This also makes a dataset dependent on another dataset and model in complex ways, which has both positive and negative aspects to it. In some sense, it is a good thing to get a diverse set of reading comprehension questions, and encoding one dataset’s biases into a model to enforce a different distribution for new datasets helps in collecting diverse datasets. If crowd workers end up simply wordsmithing their questions in order to pass the adversary, however, this seems unsatisfying. Overall, however, we believe this is a good method that could be used more widely when collecting reading com"
J12-1003,P04-1051,0,0.084713,"Missing"
J12-1003,P98-1013,0,0.0368907,"Missing"
J12-1003,P10-2045,1,0.837245,"Missing"
J12-1003,P10-1124,1,0.845704,"Missing"
J12-1003,P11-1062,1,0.416945,"Missing"
J12-1003,D07-1017,0,0.113317,"Missing"
J12-1003,J06-1003,0,0.0040761,"lobal methods that perform inference over a larger set of predicates. 2.1 Local Learning Three types of information have primarily been utilized in the past to learn entailment rules between predicates: lexicographic methods, distributional similarity methods, and pattern-based methods. Lexicographic methods use manually prepared knowledge bases that contain information about semantic relations between lexical items. WordNet (Fellbaum 1998b), by far the most widely used resource, speciﬁes relations such as hyponymy, synonymy, derivation, and entailment that can be used for semantic inference (Budanitsky and Hirst 2006). For example, if WordNet speciﬁes that reduce is a hyponym of affect, then one can infer that X reduces Y → X affects Y. WordNet has also been exploited to automatically generate a training set for a hyponym classiﬁer (Snow, Jurafsky, and Ng 2004), and we make a similar use of WordNet in Section 4.1. A drawback of WordNet is that it speciﬁes semantic relations for words and terms but not for more complex expressions. For example, WordNet does not cover a complex predicate such as X causes a reduction in Y. Another drawback of WordNet is that it only supplies semantic relations between lexical"
J12-1003,W04-3205,0,0.887671,"tifying the existence of semantic similarity between predicates, they are often unable to discern the exact type of semantic similarity and speciﬁcally determine whether it is entailment. Pattern-based methods are used to automatically extract pairs of predicates for a speciﬁc semantic relation. Pattern-based methods identify a semantic relation between two predicates by observing that they co-occur in speciﬁc patterns in sentences. For example, from the single proposition He scared and even startled me one might infer that startle is semantically stronger than scare and thus startle → scare. Chklovski and Pantel (2004) manually constructed a few dozen patterns and learned semantic relations between predicates by looking for these patterns on the Web. For example, the pattern X and even Y implies that Y is stronger than X, and the pattern to X and then Y indicates that Y follows X. The main disadvantage of pattern-based methods is that they are based on the co-occurrence of two predicates in a single sentence in a speciﬁc pattern. These events are quite rare and require working on a very large corpus, or preferably, the Web. Pattern-based methods were mainly utilized so far to extract semantic relations betw"
J12-1003,W07-1409,0,0.0182793,"hed line. 3.2 Focused Entailment Graphs In this article we concentrate on learning a type of entailment graph, termed the focused entailment graph. Given a target concept, such as nausea, a focused entailment graph describes the entailment relations between propositional templates for which the target concept is one of the arguments (see Figure 1). Learning such entailment rules in real time for a target concept is useful in scenarios such as information retrieval and question answering, where a user speciﬁes a query about the target concept. The need for such rules has been also motivated by Clark et al. (2007), who investigated what types of knowledge are needed to identify entailment in the context of the RTE challenge, and found that often rules that are speciﬁc to a certain concept are required. Another example for a semantic inference algorithm that is utilized in real time is provided by Do and Roth (2010), who recently described a system that, given two terms, determines the taxonomic relation between them on the ﬂy. Last, we have recently suggested an application that uses focused entailment graphs to present information about a target concept according to a hierarchy of entailment (Berant,"
J12-1003,D10-1107,0,0.268848,"ers, the problem is termed an Integer Linear Program (ILP). ILP has attracted considerable attention recently in several ﬁelds of NLP, such as semantic role labeling, summarization, and parsing (Althaus, Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we formulate the entailment graph learning problem as an ILP, which leads to an optimal solution with respect to the objective function (vs. a greedy optimization algorithm suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP in a related task of learning taxonomic relations between nouns, utilizing constraints between sibling nodes and ancestor–child nodes in small graphs of three nodes. 3. Entailment Graph In this section we deﬁne a structure termed the entailment graph that describes the entailment relations between propositional templates (Section 3.1), and a speciﬁc type of entailment graph, termed the focused entailment graph, that concentrates on entailment relations that are relevant for some pre-deﬁned target concept (Section 3.2). 3.1 Entailment Graph: Deﬁnition and Properties The nodes of an en"
J12-1003,P08-2012,0,0.100708,"ptimal assignment for the d variables in the vector x, such 78 Berant et al. Learning Entailment Relations by Global Graph Structure Optimization that all n linear constraints speciﬁed by the matrix A and the vector b are satisﬁed by this assignment. If the variables are forced to be integers, the problem is termed an Integer Linear Program (ILP). ILP has attracted considerable attention recently in several ﬁelds of NLP, such as semantic role labeling, summarization, and parsing (Althaus, Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we formulate the entailment graph learning problem as an ILP, which leads to an optimal solution with respect to the objective function (vs. a greedy optimization algorithm suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP in a related task of learning taxonomic relations between nouns, utilizing constraints between sibling nodes and ancestor–child nodes in small graphs of three nodes. 3. Entailment Graph In this section we deﬁne a structure termed the entailment graph that describes the entailment relations bet"
J12-1003,N03-1013,0,0.0355236,"at express information that is diverse and orthogonal to the one given by distributional similarity. Therefore, we turn to existing knowledge resources that were created using both manual and automatic methods, expressing various types of linguistic and statistical information that is relevant for entailment prediction: 1. WordNet: contains manually annotated relations such as hypernymy, synonymy, antonymy, derivation, and entailment. 2. VerbOcean11 (Chklovski and Pantel 2004): contains verb relations such as stronger-than and similar that were learned with pattern-based methods. 3. CATVAR12 (Habash and Dorr 2003): contains word derivations such as develop–development. 4. FRED13 (Ben Aharon, Szpektor, and Dagan 2010): contains entailment rules between templates learned automatically from FrameNet. 5. NomLex14 (Macleod et al. 1998): contains English nominalizations including their argument mapping to the corresponding verbal form. 6. BAP15 (Kotlerman et al. 2010): contains directional distributional similarity scores between lexical terms (rather than propositional templates) calculated with the BAP similarity scoring function. Table 12 describes the 16 new features that were generated for each of the g"
J12-1003,P98-2127,0,0.331451,"lates: X ←−− buys −→ Y, X ←− buys −−→ for −−−−−→ Y and X ←−− buys prep pcomp−n −−→ for −−−−−→ Y. For each binary template Lin and Pantel compute two sets of features Fx and Fy , which are the words that instantiate the arguments X and Y, respectively, in a large corpus. Given a template t and its feature set for the X variable Ftx , every fx ∈ Ftx is weighted by the pointwise mutual information between the template and the feature: Pr( f |t) wtx ( fx ) = log Pr( xfx ) , where the probabilities are computed using maximum likelihood over the corpus. Given two templates u and v, the Lin measure (Lin 1998a) is computed for the variable X in the following manner:  u v f ∈Fux ∩Fvx [wx ( f ) + wx ( f )]  Linx (u, v) =  u v f ∈Fux wx ( f ) + f ∈Fvx wx ( f ) (1) The measure is computed analogously for the variable Y and the ﬁnal distributional similarity score, termed DIRT, is the geometric average of the scores for the two variables:  DIRT(u, v) = Linx (u, v) · Liny (u, v) (2) If DIRT(u, v) is high, this means that the templates u and v share many “informative” arguments and so it is possible that u → v. Note, however, that the DIRT similarity measure computes a symmetric score, which is appro"
J12-1003,P09-1039,0,0.0847854,"Missing"
J12-1003,meyers-etal-2004-cross,0,0.0646719,"Missing"
J12-1003,P06-2075,1,0.816453,"Missing"
J12-1003,W06-1616,0,0.0281891,"Rn specify the constraints. In short, we wish to ﬁnd the optimal assignment for the d variables in the vector x, such 78 Berant et al. Learning Entailment Relations by Global Graph Structure Optimization that all n linear constraints speciﬁed by the matrix A and the vector b are satisﬁed by this assignment. If the variables are forced to be integers, the problem is termed an Integer Linear Program (ILP). ILP has attracted considerable attention recently in several ﬁelds of NLP, such as semantic role labeling, summarization, and parsing (Althaus, Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we formulate the entailment graph learning problem as an ILP, which leads to an optimal solution with respect to the objective function (vs. a greedy optimization algorithm suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP in a related task of learning taxonomic relations between nouns, utilizing constraints between sibling nodes and ancestor–child nodes in small graphs of three nodes. 3. Entailment Graph In this section we deﬁne a structure termed the entailment"
J12-1003,W04-2401,0,0.0469741,"A ∈ Rn × Rd and b ∈ Rn specify the constraints. In short, we wish to ﬁnd the optimal assignment for the d variables in the vector x, such 78 Berant et al. Learning Entailment Relations by Global Graph Structure Optimization that all n linear constraints speciﬁed by the matrix A and the vector b are satisﬁed by this assignment. If the variables are forced to be integers, the problem is termed an Integer Linear Program (ILP). ILP has attracted considerable attention recently in several ﬁelds of NLP, such as semantic role labeling, summarization, and parsing (Althaus, Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we formulate the entailment graph learning problem as an ILP, which leads to an optimal solution with respect to the objective function (vs. a greedy optimization algorithm suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP in a related task of learning taxonomic relations between nouns, utilizing constraints between sibling nodes and ancestor–child nodes in small graphs of three nodes. 3. Entailment Graph In this section we deﬁne a structur"
J12-1003,D10-1106,0,0.0838487,"Number 1 by the arguments. One important type of entailment rule speciﬁes entailment between propositional templates, that is, propositions where the arguments are possibly replaced by variables. A rule corresponding to the aforementioned example may be X reduce blood pressure → X affect blood pressure. Because facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task. This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010). Previous work has focused on learning each entailment rule in isolation. It is clear, however, that there are interactions between rules. A prominent phenomenon is that entailment is inherently a transitive relation, and thus the rules X → Y and Y → Z imply the rule X → Z.1 In this article we take advantage of these global interactions to improve entailment rule learning. After reviewing relevant background (Section 2), we describe a structure termed an entailment graph that models entailment relations between propositional templates (Section 3). Next, we motivate and discuss a speciﬁc type"
J12-1003,I05-5011,0,0.0311312,"Missing"
J12-1003,P05-1044,0,0.0109691,"hod proposed by Snow, Jurafsky, and Ng (2004) for training a noun hypernym classiﬁer. It differs in some important aspects, however: First, Snow, Jurafsky, and Ng consider a positive example to be any Wordnet hypernym, irrespective of the distance, whereas we look only at direct hypernyms. This is because predicates are mainly verbs and precision drops quickly when looking at verb hypernyms in WordNet at a longer distance. Second, Snow, Jurafsky, and Ng generate negative examples by looking at any two nouns where one is not the hypernym of the other. In the spirit of “contrastive estimation” (Smith and Eisner 2005), we prefer to generate negative examples that are “hard,” that is, negative examples that, although not entailing, are still semantically similar to positive examples and thus focus the classiﬁer’s attention on determining the boundary of the entailment class. Last, we use a balanced number of positive and negative examples, because classiﬁers tend to perform poorly on the minority class when trained on imbalanced data (Van Hulse, Khoshgoftaar, and Napolitano 2007; Nikulin 2008). (3) Distributional similarity representation. We aim to train a classiﬁer that for an input template pair (t1 , t2"
J12-1003,P06-1101,0,0.18016,"Missing"
J12-1003,N07-1031,0,0.0147238,"Missing"
J12-1003,C08-1107,1,0.654208,"Linguistics Computational Linguistics Volume 38, Number 1 by the arguments. One important type of entailment rule speciﬁes entailment between propositional templates, that is, propositions where the arguments are possibly replaced by variables. A rule corresponding to the aforementioned example may be X reduce blood pressure → X affect blood pressure. Because facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task. This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010). Previous work has focused on learning each entailment rule in isolation. It is clear, however, that there are interactions between rules. A prominent phenomenon is that entailment is inherently a transitive relation, and thus the rules X → Y and Y → Z imply the rule X → Z.1 In this article we take advantage of these global interactions to improve entailment rule learning. After reviewing relevant background (Section 2), we describe a structure termed an entailment graph that models entailment relations between propositional templates (Secti"
J12-1003,W09-2504,1,0.948795,"buy, but does not describe the way in which arguments are mapped: if X pays Y for Z then X buys Z from Y. Thus, using WordNet directly to derive entailment rules between predicates is possible only for semantic relations such as hyponymy and synonymy, where arguments typically preserve their syntactic positions on both sides of the rule. Some knowledge bases try to overcome this difﬁculty: Nomlex (Macleod et al. 1998) is a dictionary that provides the mapping of arguments between verbs and their nominalizations and has been utilized to derive predicative entailment rules (Meyers et al. 2004; Szpektor and Dagan 2009). FrameNet (Baker, Fillmore, and Lowe 1998) is a lexicographic resource that is arranged around “frames”: Each frame corresponds to an event and includes information on the predicates and arguments relevant for that speciﬁc event supplemented with annotated examples that specify argument positions. Consequently, FrameNet was also used to derive entailment rules between predicates (Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Additional manually constructed resources for predicates include PropBank (Kingsbury, Palmer, and Marcus 2002) and VerbNet (Kipper, Dang, and Palmer 2000)"
J12-1003,W04-3206,1,0.72226,"bow 2009; Ben Aharon, Szpektor, and Dagan 2010). Additional manually constructed resources for predicates include PropBank (Kingsbury, Palmer, and Marcus 2002) and VerbNet (Kipper, Dang, and Palmer 2000). Distributional similarity methods are used to learn broad-scale resources, because lexicographic resources tend to have limited coverage. Distributional similarity algorithms employ “the distributional hypothesis” (Harris 1954) and predict a semantic relation between two predicates by comparing the arguments with which they occur. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al. 2004; Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010), which differ in terms of the speciﬁcs of the ways in which predicates are represented, the features that are extracted, and the function used to compute feature vector similarity. Next, we elaborate on some of the prominent methods. 75 Computational Linguistics Volume 38, Number 1 Lin and Pantel (2001) proposed an algorithm that is based on a mutual information criterion. A predicate is represented by a binary template, which is a dependency path between two arguments of a predicate wh"
J12-1003,W03-1011,0,0.056599,"nal distributional similarity measure. In their work, Szpektor and Dagan chose to represent predicates with unary templates, which are identical to binary templates, only they contain a predsubj icate and a single argument, such as: X ←−− buys. Szpektor and Dagan explain that unary templates are more expressive than binary templates, and that some predicates can only be encoded using unary templates. They propose that if for two unary templates u → v, then relatively many of the features of u should be covered by the features of v. This is captured by the asymmetric Cover measure suggested by Weeds and Weir (2003) (we omit the subscript x from Fux and Fvx because in their setting there is only one argument):  u f ∈Fu ∩Fv w ( f ) Cover(u, v) =  u f ∈Fu w ( f ) (3) The ﬁnal directional score, termed BInc (Balanced Inclusion), is the geometric average of the Lin measure and the Cover measure: BInc(u, v) = 76  Lin(u, v) · Cover(u, v) (4) Berant et al. Learning Entailment Relations by Global Graph Structure Optimization Both Lin and Pantel as well as Szpektor and Dagan compute a similarity score for each argument separately, effectively decoupling the arguments from one another. It is clear, however, tha"
J12-1003,C98-1013,0,\N,Missing
J12-1003,2003.mtsummit-systems.9,0,\N,Missing
J12-1003,C98-2122,0,\N,Missing
J15-2003,D14-1059,0,0.0201683,"ance of learning graphs where predicates are marked by their various senses, which will result in a model that can directly benefit from the methods suggested in this article. 6. Conclusions The problem of language variability is at the heart of many semantic applications such as Information Extraction, Question Answering, Semantic Parsing, and more. Consequently, learning broad coverage knowledge bases of entailment rules and paraphrases (Ganitkevitch, Van Durme, and Callison-Burch 2013) has proved crucial for systems that perform inference over textual representations (Stern and Dagan 2012; Angeli and Manning 2014). 257 Computational Linguistics Volume 41, Number 2 In this article, we have presented algorithms for learning entailment rules between predicates that can scale to large sets of predicates. Our work builds on prior work by Berant, Dagan, and Goldberger (2012), who defined the concept of entailment graphs, and formulated entailment rule learning as a graph optimization problem, where the graph has to obey the structural constraint of transitivity. Our main contribution is a heuristic polynomial approximation algorithm that can learn entailment graphs containing tens of thousands of nodes. The"
J15-2003,P10-2045,1,0.817166,"Missing"
J15-2003,P12-1013,1,0.818566,"s. Section 3.2 describes the second step, in which we develop an efficient heuristic approximation based on the assumption that entailment graphs are forest reducible. Section 4 describes experiments on the first data set containing medium-sized entailment graphs with typed predicates. Section 5 presents an empirical evaluation on a large graph containing 20,000 untyped predicates. We also perform a qualitative analysis in Section 5.4 to further elucidate the behavior of our algorithm. Section 6 concludes the article. This article is based on previous work (Berant, Dagan, and Goldberger 2011; Berant et al. 2012), but expands over it in multiple directions. Empirically, we present results on a novel data set (Section 5) that is by orders of magnitude larger than in the past. Algorithmically, we present the Tree-Node-And-Component-Fix algorithm, which is an extension of the Tree-Node-Fix algorithm presented in Berant et al. (2012) and achieves best results in our experimental evaluation. Theoretically, we provide an NP-hardness 223 Computational Linguistics Volume 41, Number 2 proof for the Max-Trans-Forest optimization problem presented in Berant et al. (2012) and an ILP formulation for it. Last, we p"
J15-2003,P10-1124,1,0.927307,"Missing"
J15-2003,P11-1062,1,0.894287,"Missing"
J15-2003,J12-1003,1,0.780195,"s. Section 3.2 describes the second step, in which we develop an efficient heuristic approximation based on the assumption that entailment graphs are forest reducible. Section 4 describes experiments on the first data set containing medium-sized entailment graphs with typed predicates. Section 5 presents an empirical evaluation on a large graph containing 20,000 untyped predicates. We also perform a qualitative analysis in Section 5.4 to further elucidate the behavior of our algorithm. Section 6 concludes the article. This article is based on previous work (Berant, Dagan, and Goldberger 2011; Berant et al. 2012), but expands over it in multiple directions. Empirically, we present results on a novel data set (Section 5) that is by orders of magnitude larger than in the past. Algorithmically, we present the Tree-Node-And-Component-Fix algorithm, which is an extension of the Tree-Node-Fix algorithm presented in Berant et al. (2012) and achieves best results in our experimental evaluation. Theoretically, we provide an NP-hardness 223 Computational Linguistics Volume 41, Number 2 proof for the Max-Trans-Forest optimization problem presented in Berant et al. (2012) and an ILP formulation for it. Last, we p"
J15-2003,P14-1133,1,0.802761,"LI a 00220 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 paradigm for textual inference is Textual Entailment (Dagan et al. 2013). In textual entailment, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text. For example, the text Cyprus was invaded by the Ottoman Empire in 1571 implies the hypothesis The Ottomans attacked Cyprus. Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules"
J15-2003,D07-1017,0,0.080239,"Missing"
J15-2003,J06-1003,0,0.00884565,"utional similarity. We briefly describe the first two and then expand more on distributional similarity, which is the most commonly used source of information. Lexicographic resources are manually built knowledge bases from which semantic information may be extracted. For example, the hyponymy, toponymy, and synonymy relations in WordNet (Fellbaum 1998) can be used to detect entailment between nouns and verbs. Although WordNet is the most popular lexicographic resource, other resources such as CatVar, Nomlex, and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chk"
J15-2003,P11-1098,0,0.0303822,"and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004) used pattern-based methods to generate the commonly used VerbOcean resource. Both lexicographic as well as pattern-based methods suffer from limited coverage. Distributional similarity is therefore used to automatically construct broad-scale resources. Distributional similarity methods are based on the “distributional hypothesis” (Harris 1954) that semantically similar predicates occur with similar arguments. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al."
J15-2003,D11-1003,0,0.0248533,"eger, so many violations of transitivity may occur. The solution when applying LP relaxation is not a transitive graph; we will show in Section 4 that our approximation method is substantially faster. Global inference has gained popularity in recent years in NLP, and a common approximation method that has been extensively utilized is dual decomposition (Sontag, Globerson, and Jaakkola 2011). Dual decomposition has been successfully applied in 226 Berant et al. Efficient Global Learning of Entailment Graphs tasks such as Information Extraction (Reichart and Barzilay 2012), Machine Translation (Chang and Collins 2011), Parsing (Rush et al. 2012), and Named Entity Recognition (Wang, Che, and Manning 2013). To the best of our knowledge, it has not yet been applied for the task of learning entailment relations. The graph decomposition method we present in Section 3.1 can be viewed as an ideal case of dual decomposition, where we can decompose the problem into disjoint components in a way that we do not need to ensure consistency of the results obtained on each component separately. 3. Efficient Inference Our goal is to learn a large knowledge base of entailment rules between natural language predicates. Follo"
J15-2003,W04-3205,0,0.0684946,"e half of the data set for training, resulting in 1,224 positive examples and 2,060 negative examples. Another two training examples are X unable to pay Y ⇒ X owe Y and X own Y ; Y be sold to X. 2. Feature representation. Each pair of predicates (p1 , p2 ) is represented by a feature vector where the first six are distributional similarity features identical to the first six features described in Section 4.2. In addition, for pairs of predicates for which at least one distributional similarity feature is non-zero, we add lexicographic features computed from WordNet (Fellbaum 1998), VerbOcean (Chklovski and Pantel 2004), and CatVar (Habash and Dorr 2003), as well as string-similarity features. Table 2 provides the exact details of these features. A feature is computed for a pair of predicates (p1 , p2 ) where in this context a predicate is a pair (pred,rev): pred is the lexical realization of the predicate, and rev is a Boolean indicating whether arg1 is X and arg2 is Y or vice versa. Overall, each pair of predicates is represented by 27 features. 3. Training. After obtaining a feature representation for every pair of predicates, we train a Gaussian kernel SVM classifier that optimizes F1 (SVMperf implementa"
J15-2003,C10-2029,0,0.0255208,"that are typed (Pantel et al. 2007; Schoenmackers et al. 2010). For example, argument variables in the work of Schoenmackers et al. were restricted to belong to one of 156 types, such as country or profession. A different solution that has attracted substantial attention recently is to represent the various contexts in which a predicate can appear in a low-dimensional latent space (for example, using Latent Dirichlet Allocation [Blei, Ng, and Jordan 2003]) and infer entailment relations between predicates based on the contexts in which they appear (Ritter, Mausam, and Etzioni ´ eaghdha 2010; Dinu and Lapata 2010; Melamud et al. 2013). In the experiments 2010; OS´ presented in this article we will use the representation of Schoenmackers et al. in one experiment, and ignore the problem of predicate ambiguity in the other. 225 Computational Linguistics Volume 41, Number 2 2.2 Global Learning The idea of global learning is that, by jointly learning semantic relations between a large number of natural language phrases, one can use the dependencies between the relations to improve accuracy. A natural way to model that is with a graph where nodes are phrases and edges represent semantic similarity. Snow, Ju"
J15-2003,D10-1107,0,0.0664476,"e local scores they look for the set of edges E that maximizes the objective function (i,j)∈E wij under the constraint that edges respect transitivity. They show that this optimization problem is NP-hard and find an exact solution using an ILP solver over small graphs. They also avoid problems of predicate ambiguity by partially contextualizing the predicates. In this article, we present efficient and scalable heuristic approximation algorithms for the optimization problem they propose. In recent years, there has been substantial work on approximation algorithms for global inference problems. Do and Roth (2010) suggested a method for the related task of learning taxonomic relations between terms. Given a pair of terms, they construct a small graph that contains the two terms and a few other related terms, and then impose constraints on the graph structure. They construct these small graphs because their work is geared towards scenarios where relations are determined on-the-fly for a given pair of terms and no global knowledge base is ever explicitly constructed. Because they independently construct a graph for each pair of terms, their method easily produces solutions where global constraints, such"
J15-2003,D11-1142,0,0.0543923,"Missing"
J15-2003,N13-1092,0,0.0669501,"Missing"
J15-2003,N03-1013,0,0.0145222,"ulting in 1,224 positive examples and 2,060 negative examples. Another two training examples are X unable to pay Y ⇒ X owe Y and X own Y ; Y be sold to X. 2. Feature representation. Each pair of predicates (p1 , p2 ) is represented by a feature vector where the first six are distributional similarity features identical to the first six features described in Section 4.2. In addition, for pairs of predicates for which at least one distributional similarity feature is non-zero, we add lexicographic features computed from WordNet (Fellbaum 1998), VerbOcean (Chklovski and Pantel 2004), and CatVar (Habash and Dorr 2003), as well as string-similarity features. Table 2 provides the exact details of these features. A feature is computed for a pair of predicates (p1 , p2 ) where in this context a predicate is a pair (pred,rev): pred is the lexical realization of the predicate, and rev is a Boolean indicating whether arg1 is X and arg2 is Y or vice versa. Overall, each pair of predicates is represented by 27 features. 3. Training. After obtaining a feature representation for every pair of predicates, we train a Gaussian kernel SVM classifier that optimizes F1 (SVMperf implementation [Joachims 2005]), and tune the"
J15-2003,C92-2082,0,0.220143,"e, other resources such as CatVar, Nomlex, and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004) used pattern-based methods to generate the commonly used VerbOcean resource. Both lexicographic as well as pattern-based methods suffer from limited coverage. Distributional similarity is therefore used to automatically construct broad-scale resources. Distributional similarity methods are based on the “distributional hypothesis” (Harris 1954) that semantically similar predicates occur with similar arguments. Quite a few methods"
J15-2003,P98-2127,0,0.172773,"ariables such as X treat Y. For each binary predicate, Lin and Pantel compute two sets of features Fx and Fy , 224 Berant et al. Efficient Global Learning of Entailment Graphs which are the words that instantiate the arguments X and Y, respectively, in a large corpus. Given a predicate u and its feature set for the X variable Fx , every feature fx ∈ Fx is weighted by pointwise mutual information between the predicate and the feature: Pr( f |u) w( fx ) = log Pr( xfx ) , where the probabilities are computed using maximum likelihood over the corpus. Given two predicates u and v, the Lin measure (Lin 1998) is computed for the variable X in the following manner: P u v f ∈Fux ∩Fvx [wx ( f ) + wx ( f )] P Linx (u, v) = P u v f ∈Fux wx ( f ) + f ∈Fvx wx ( f ) (1) The measure is computed analogously for the variable Y and the final distributional similarity score, termed DIRT, is the geometric average of the scores for the two variables: If DIRT(u, v) is high, this means that the templates u and v share many “informative” arguments and so it is possible that u ⇒ v. Szpektor and Dagan (2008) suggested two modifications to DIRT. First, they looked at unary predicates, that is, predicates with a single"
J15-2003,P09-1039,0,0.0710535,"Missing"
J15-2003,P13-1131,1,0.856715,"on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules x ⇒ y and y ⇒ z imply the rule x ⇒ z. For example, from the rules X reduce nausea ⇒ X help with nausea and X help with nausea ⇒ X associated with nausea we can infer the rule X reduce nausea ⇒ X associated with nausea (Figure 1). Accordingly, Berant, Dagan, and Goldberger (2012) proposed taking advantage of transitivity to improve learning of entailment rules. They modeled learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules that respect transitivit"
J15-2003,meyers-etal-2004-cross,0,0.0323109,"nce, and (3) distributional similarity. We briefly describe the first two and then expand more on distributional similarity, which is the most commonly used source of information. Lexicographic resources are manually built knowledge bases from which semantic information may be extracted. For example, the hyponymy, toponymy, and synonymy relations in WordNet (Fellbaum 1998) can be used to detect entailment between nouns and verbs. Although WordNet is the most popular lexicographic resource, other resources such as CatVar, Nomlex, and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations betw"
J15-2003,D12-1104,0,0.109232,"Missing"
J15-2003,P10-1045,0,0.0128464,"ment variables that are typed (Pantel et al. 2007; Schoenmackers et al. 2010). For example, argument variables in the work of Schoenmackers et al. were restricted to belong to one of 156 types, such as country or profession. A different solution that has attracted substantial attention recently is to represent the various contexts in which a predicate can appear in a low-dimensional latent space (for example, using Latent Dirichlet Allocation [Blei, Ng, and Jordan 2003]) and infer entailment relations between predicates based on the contexts in which they appear (Ritter, Mausam, and Etzioni ´ eaghdha 2010; Dinu and Lapata 2010; Melamud et al. 2013). In the experiments 2010; OS´ presented in this article we will use the representation of Schoenmackers et al. in one experiment, and ignore the problem of predicate ambiguity in the other. 225 Computational Linguistics Volume 41, Number 2 2.2 Global Learning The idea of global learning is that, by jointly learning semantic relations between a large number of natural language phrases, one can use the dependencies between the relations to improve accuracy. A natural way to model that is with a graph where nodes are phrases and edges represent semanti"
J15-2003,N07-1071,0,0.0106022,"However, it is clear that although this alleviates sparsity problems, considering pairs of arguments jointly provides more information. Yates and Etzioni (2009), Schoenmackers et al. (2010), and even earlier, Szpektor et al. (2004), presented methods that compute semantic similarity based on pairs of arguments. A problem common to all local methods presented above is predicate ambiguity— predicates may have different meanings and different entailment relations in different contexts. Some works resolved the problem of ambiguity by representing predicates with argument variables that are typed (Pantel et al. 2007; Schoenmackers et al. 2010). For example, argument variables in the work of Schoenmackers et al. were restricted to belong to one of 156 types, such as country or profession. A different solution that has attracted substantial attention recently is to represent the various contexts in which a predicate can appear in a low-dimensional latent space (for example, using Latent Dirichlet Allocation [Blei, Ng, and Jordan 2003]) and infer entailment relations between predicates based on the contexts in which they appear (Ritter, Mausam, and Etzioni ´ eaghdha 2010; Dinu and Lapata 2010; Melamud et al"
J15-2003,P02-1006,0,0.0363355,"pted for publication: 25 November 2014. doi:10.1162/COLI a 00220 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 paradigm for textual inference is Textual Entailment (Dagan et al. 2013). In textual entailment, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text. For example, the text Cyprus was invaded by the Ottoman Empire in 1571 implies the hypothesis The Ottomans attacked Cyprus. Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inh"
J15-2003,N12-1008,0,0.0193099,"fractional value are rounded to their nearest integer, so many violations of transitivity may occur. The solution when applying LP relaxation is not a transitive graph; we will show in Section 4 that our approximation method is substantially faster. Global inference has gained popularity in recent years in NLP, and a common approximation method that has been extensively utilized is dual decomposition (Sontag, Globerson, and Jaakkola 2011). Dual decomposition has been successfully applied in 226 Berant et al. Efficient Global Learning of Entailment Graphs tasks such as Information Extraction (Reichart and Barzilay 2012), Machine Translation (Chang and Collins 2011), Parsing (Rush et al. 2012), and Named Entity Recognition (Wang, Che, and Manning 2013). To the best of our knowledge, it has not yet been applied for the task of learning entailment relations. The graph decomposition method we present in Section 3.1 can be viewed as an ideal case of dual decomposition, where we can decompose the problem into disjoint components in a way that we do not need to ensure consistency of the results obtained on each component separately. 3. Efficient Inference Our goal is to learn a large knowledge base of entailment ru"
J15-2003,C08-1092,0,0.0334601,"r tuples provided by Schoenmackers et al. We perform the following steps: 1. Training set generation Positive examples are generated using WordNet synonyms and hypernyms. Negative pairs are generated using WordNet direct co-hyponyms (sister terms), but we also utilize Word hyponyms at distance 2. In addition, we generate negative examples by randomly sampling pairs of typed predicates that share the same types. Table 1 provides an example for each type of automatically generated training example. It has been noted in the past that the WordNet verb hierarchy contains a certain amount of noise (Richens 2008; Roth and Frank 2012). However, we use WordNet only to generate examples for training a statistical classifier, and thus we can tolerate some noise in the generated examples. In fact, we have noticed that simple variants in training set generation do not result in substantial differences in classifier performance. 2. Feature representation Each example pair of typed predicates (p1 , p2 ) is represented by a feature vector, where each feature is a distributional similarity score estimating whether p1 entails p2 . We compute 11 distributional similarity scores for each pair of typed predicates,"
J15-2003,W06-1616,0,0.0329696,"ole graph. Finding the undirected edges (Line 1) and computing connected components (Line 2) can be performed in O(V 2 ). Thus, in this case the efficiency of the algorithm is dominated by the application of an ILP solver (Line 4). If the entailment graph decomposes into small components, one could obtain an exact solution with an ILP solver, applied on each component separately, without resorting to any approximation. To further extend scalability in this setting we use a cutting-plane method (Kelley 1960). Cutting-plane methods have been used in the past, for example, in dependency parsing (Riedel and Clarke 2006). The idea is that even if we omit all transitivity constraints, we still expect most transitivity constraints to be satisfied, given a good weighting function w. Thus, it makes sense to avoid specifying the constraints ahead of time, but rather add them when they are violated. This is formalized in Algorithm 2. Line 1 initializes an active set of constraints (ACT). Line 3 applies the ILP solver with the active constraints. Lines 4 and 5 find the violated constraints and add them to the active constraints. The algorithm halts when no constraints are violated. The solution is clearly optimal be"
J15-2003,P10-1044,0,0.0724553,"Missing"
J15-2003,D12-1016,0,0.0266362,"ded by Schoenmackers et al. We perform the following steps: 1. Training set generation Positive examples are generated using WordNet synonyms and hypernyms. Negative pairs are generated using WordNet direct co-hyponyms (sister terms), but we also utilize Word hyponyms at distance 2. In addition, we generate negative examples by randomly sampling pairs of typed predicates that share the same types. Table 1 provides an example for each type of automatically generated training example. It has been noted in the past that the WordNet verb hierarchy contains a certain amount of noise (Richens 2008; Roth and Frank 2012). However, we use WordNet only to generate examples for training a statistical classifier, and thus we can tolerate some noise in the generated examples. In fact, we have noticed that simple variants in training set generation do not result in substantial differences in classifier performance. 2. Feature representation Each example pair of typed predicates (p1 , p2 ) is represented by a feature vector, where each feature is a distributional similarity score estimating whether p1 entails p2 . We compute 11 distributional similarity scores for each pair of typed predicates, based on their argume"
J15-2003,D12-1131,0,0.0566681,"Missing"
J15-2003,D10-1106,0,0.115177,"s QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules x ⇒ y and y ⇒ z imply the rule x ⇒ z. For example, from the rules X reduce nausea ⇒ X help with nausea and X help with nausea ⇒ X associated with nausea we can infer the rule X reduce nausea ⇒ X associated with nausea (Figure 1). Accordingly, Berant, Dagan, and Goldberger (2012) proposed taking advantage of transitivity to improve learning of entailment rules. They modeled learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules th"
J15-2003,I05-5011,0,0.0343162,"Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules x ⇒ y and y ⇒ z imply the rule x ⇒ z. For example, from the rules X reduce nausea ⇒ X help with nausea and X help with nausea ⇒ X associated with nausea we can infer the rule X reduce nausea ⇒ X associated with nausea (Figure 1). Accordingly, Berant, Dagan, and Goldberger (2012) proposed taking advantage of transitivity to improve learning of entailment rules. They modeled learning entailment rules as a graph optimization problem, where nodes ar"
J15-2003,N06-1039,0,0.0160885,"ember 2014. doi:10.1162/COLI a 00220 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 paradigm for textual inference is Textual Entailment (Dagan et al. 2013). In textual entailment, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text. For example, the text Cyprus was invaded by the Ottoman Empire in 1571 implies the hypothesis The Ottomans attacked Cyprus. Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relat"
J15-2003,P06-1101,0,0.206547,"Missing"
J15-2003,P12-3013,1,0.838495,"emphasizes the importance of learning graphs where predicates are marked by their various senses, which will result in a model that can directly benefit from the methods suggested in this article. 6. Conclusions The problem of language variability is at the heart of many semantic applications such as Information Extraction, Question Answering, Semantic Parsing, and more. Consequently, learning broad coverage knowledge bases of entailment rules and paraphrases (Ganitkevitch, Van Durme, and Callison-Burch 2013) has proved crucial for systems that perform inference over textual representations (Stern and Dagan 2012; Angeli and Manning 2014). 257 Computational Linguistics Volume 41, Number 2 In this article, we have presented algorithms for learning entailment rules between predicates that can scale to large sets of predicates. Our work builds on prior work by Berant, Dagan, and Goldberger (2012), who defined the concept of entailment graphs, and formulated entailment rule learning as a graph optimization problem, where the graph has to obey the structural constraint of transitivity. Our main contribution is a heuristic polynomial approximation algorithm that can learn entailment graphs containing tens o"
J15-2003,C08-1107,1,0.933058,"rence applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules x ⇒ y and y ⇒ z imply the rule x ⇒ z. For example, from the rules X reduce nausea ⇒ X help with nausea and X help with nausea ⇒ X associated with nausea we can infer the rule X reduce nausea ⇒ X associated with nausea (Figure 1). Accordingly, Berant, Dagan, and Goldberger (2012) proposed taking advantage of transitivity to improve learning of entailment rules. They modeled learning entailment rules as a graph optimization problem, where nodes are predicates and edges re"
J15-2003,W09-2504,1,0.838143,"fly describe the first two and then expand more on distributional similarity, which is the most commonly used source of information. Lexicographic resources are manually built knowledge bases from which semantic information may be extracted. For example, the hyponymy, toponymy, and synonymy relations in WordNet (Fellbaum 1998) can be used to detect entailment between nouns and verbs. Although WordNet is the most popular lexicographic resource, other resources such as CatVar, Nomlex, and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004)"
J15-2003,W04-3206,1,0.844059,"d Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004) used pattern-based methods to generate the commonly used VerbOcean resource. Both lexicographic as well as pattern-based methods suffer from limited coverage. Distributional similarity is therefore used to automatically construct broad-scale resources. Distributional similarity methods are based on the “distributional hypothesis” (Harris 1954) that semantically similar predicates occur with similar arguments. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al. 2004; Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010), which differ in terms of the specifics of the ways in which predicates are represented, the features that are extracted, and the function used to compute feature vector similarity. Next, we elaborate on the methods that we use in this article. Lin and Pantel (2001) proposed an algorithm for learning paraphrase relations between binary predicates, that is, predicates with two variables such as X treat Y. For each binary predicate, Lin and Pantel compute two sets of features Fx and Fy ,"
J15-2003,W03-1011,0,0.909309,"s high, this means that the templates u and v share many “informative” arguments and so it is possible that u ⇒ v. Szpektor and Dagan (2008) suggested two modifications to DIRT. First, they looked at unary predicates, that is, predicates with a single variable such as X treat. Secondly, they computed a directional score that is more suited for capturing entailment relations compared to the symmetric Lin score. They proposed that if for two unary predicates u ⇒ v, then relatively many of the features of u should be covered by the features of v. This is captured by the asymmetric Cover measure (Weeds and Weir 2003): P u f ∈ Fu ∩ Fv w ( f ) Cover(u, v) = P u f ∈ Fu w ( f ) (2) The final directional score, termed BInc (Balanced Inclusion), is the geometric average of the Lin measure and the Cover measure. Both Lin and Pantel as well as Szpektor and Dagan compute a similarity score using a single argument. However, it is clear that although this alleviates sparsity problems, considering pairs of arguments jointly provides more information. Yates and Etzioni (2009), Schoenmackers et al. (2010), and even earlier, Szpektor et al. (2004), presented methods that compute semantic similarity based on pairs of arg"
J15-2003,D12-1018,1,0.894504,"utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004) used pattern-based methods to generate the commonly used VerbOcean resource. Both lexicographic as well as pattern-based methods suffer from limited coverage. Distributional similarity is therefore used to automatically construct broad-scale resources. Distributional similarity methods are based on the “distributional hypothesis” (Harris 1954) that semantically similar predicates occur with similar arguments. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al. 2004; Bhagat, Pantel,"
J15-2003,P12-2031,1,0.930862,"Missing"
J15-2003,2003.mtsummit-systems.9,0,\N,Missing
J15-2003,C98-2122,0,\N,Missing
K19-1042,P13-1023,0,0.0601246,"Missing"
K19-1042,W13-2322,0,0.0513238,"Missing"
K19-1042,D14-1159,1,0.825604,"novsky2,4 Vivek Srikumar3 Yichu Zhou3 Jonathan Berant1,2 1 Tel-Aviv University, 2 Allen Institute for AI 3 The University of Utah, 4 University of Washington {omri.koshorek,joberant}@cs.tau.ac.il gabis@allenai.org, {flyaway,svivek}@cs.utah.edu Abstract Banarescu et al., 2013), universal conceptual cognitive annotation (UCCA; Abend and Rappoport, 2013), question-answer driven SRL (QA-SRL; He et al., 2015), and universal dependencies (Nivre et al., 2016), as well as domain-specific semantic representations for particular users in fields such as biology (Kim et al., 2009; N´edellec et al., 2013; Berant et al., 2014) and material science (Mysore et al., 2017; Kim et al., 2019). Currently, the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et a"
K19-1042,P18-1174,0,0.302926,"collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work by Liu et al. (2018), given a labeled dataset from some domain, active learning is simulated on this dataset, and a policy is trained to iteratively select the subset of examples that maximizes performance on a development set. Then, this policy is used on a target domain to select unlabeled examples for annotation. If the learned One of the goals of natural language understanding is to develop models that map sentences into meaning representations. However,"
K19-1042,D19-1003,0,0.0858022,"OM S MALL M ODEL O RACLE S MALL M ODEL 110 45.2 43.3 46.6 44.8 44.2 45.2 30.0 40.9 45.1 44.9 51.9 53.8 150 47.2 49.2 48.8 47.4 48.3 47.5 38.2 44.6 48.6 48.8 54.8 56.6 210 50.5 52.9 51.4 52.1 52.5 51.9 41.0 50.1 52.4 51.4 57.3 58.9 290 53.1 56.8 55.8 55.9 55.5 55.1 51.5 54.1 55.6 53.9 59.5 60.3 370 55.8 57.8 57.6 — 58.0 56.7 53.7 — 57.1 57.0 61.4 61.5 510 58.5 60.3 58.6 — 59.8 58.7 57.2 — 59.8 59.2 62.6 63.3 tecture modifications do not expose an advantage of the oracle policy compared to the random one. We did not examine a simpler linear model for span detection, in light of recent findings (Lowell et al., 2019) that it is important to test LTAL with state-of-the-art models, as performance is tied to the specific model being trained. Myopicity We hypothesized that greedily selecting an example that maximizes performance in a specific iteration might be suboptimal in the long run. Because non-greedy selection strategies are computationaly intractable, we perform the following two experiments. First, we examine E PSILON -G REEDY- P, where in each iteration the oracle policy selects the set Cj that maximizes target performance with probability 1 − p and randomly chooses a set with probability p. This is"
K19-1042,D17-1063,0,0.0315033,"proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work by Liu et al. (2018), given a labeled dataset from some domain, active learning is simulated on this dataset, and a policy is trained to iteratively select the subset of examples that maximizes performance on a development set. Then, this policy is used on a target domain to select unlabeled examples for annotation. If the learned One of the goals of natural language understanding is to develop models that map sentenc"
K19-1042,W18-2501,0,0.0604564,"Missing"
K19-1042,W13-2001,0,0.0473772,"Missing"
K19-1042,D15-1076,0,0.133676,"the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work by Liu et al. (2018), given a labeled dataset from some domain, active learning"
K19-1042,L16-1262,0,0.0301266,"Missing"
K19-1042,D16-1258,0,0.0481928,"Missing"
K19-1042,P18-1035,0,0.0219761,"nt et al., 2014) and material science (Mysore et al., 2017; Kim et al., 2019). Currently, the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representations (Stanovsky and Dagan, 2018; Hershcovich et al., 2018), or having non-experts annotate sentences in natural language (He et al., 2015, 2016). One of the classic and natural solutions for reducing annotation costs is to use active learning, an iterative procedure for selecting unlabeled examples which are most likely to improve the performance of a model, and annotating them (Settles, 2009). Recently, learning to actively-learn (LTAL) has been proposed (Fang et al., 2017; Bachman et al., 2017; Liu et al., 2018), where the procedure for selecting unlabeled examples is trained using methods from reinforcement and imitation learning. In recent work b"
K19-1042,S14-2008,0,0.0250035,"ation strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving data efficiency in learning semantic meaning representations is limited. 1 Introduction The task of mapping a natural language sentence into a semantic representation, that is, a structure that represents its meaning, is one of the core goals of natural language processing. This goal has led to the creation of many general-purpose formalisms for representing the structure of language, such as semantic role labeling (SRL; Palmer et al., 2005), semantic dependencies (SDP; Oepen et al., 2014), abstract meaning representation (AMR; 452 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 452–462 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 policy generalizes well, we can reduce the cost of learning semantic representations. Liu et al. (2018) and Vu et al. (2019) have shown that such learned policies significantly reduce annotation costs on both text classification and named entity recognition (NER). Learning to Actively Learn Classic pool-based active learning (Settles, 2009) assumes access to a small label"
K19-1042,J05-1004,0,0.160826,"hile in our setup the stochastic nature of optimization strongly affects the examples selected by the oracle. We conclude that the current applicability of LTAL for improving data efficiency in learning semantic meaning representations is limited. 1 Introduction The task of mapping a natural language sentence into a semantic representation, that is, a structure that represents its meaning, is one of the core goals of natural language processing. This goal has led to the creation of many general-purpose formalisms for representing the structure of language, such as semantic role labeling (SRL; Palmer et al., 2005), semantic dependencies (SDP; Oepen et al., 2014), abstract meaning representation (AMR; 452 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 452–462 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 policy generalizes well, we can reduce the cost of learning semantic representations. Liu et al. (2018) and Vu et al. (2019) have shown that such learned policies significantly reduce annotation costs on both text classification and named entity recognition (NER). Learning to Actively Learn Classic pool-based active learnin"
K19-1042,W09-1401,0,0.0514371,"Representations Omri Koshorek1 Gabriel Stanovsky2,4 Vivek Srikumar3 Yichu Zhou3 Jonathan Berant1,2 1 Tel-Aviv University, 2 Allen Institute for AI 3 The University of Utah, 4 University of Washington {omri.koshorek,joberant}@cs.tau.ac.il gabis@allenai.org, {flyaway,svivek}@cs.utah.edu Abstract Banarescu et al., 2013), universal conceptual cognitive annotation (UCCA; Abend and Rappoport, 2013), question-answer driven SRL (QA-SRL; He et al., 2015), and universal dependencies (Nivre et al., 2016), as well as domain-specific semantic representations for particular users in fields such as biology (Kim et al., 2009; N´edellec et al., 2013; Berant et al., 2014) and material science (Mysore et al., 2017; Kim et al., 2019). Currently, the dominant paradigm for building models that predict such representations is supervised learning, which requires annotating thousands of sentences with their correct structured representation, usually by experts. This arduous data collection is the main bottleneck for building parsers for different users in new domains. Past work has proposed directions for accelerating data collection and improving data efficiency through multi-task learning across different representation"
K19-1042,D14-1162,0,0.0826899,"oder, producing a representation hi for every token. Each span xi∶j is represented by concatenating the respective hidden states: sij = [hi ; hj ]. A fully connected network consumes the span representation sij , and predicts a probability whether the span is an argument or not. To accelerate training, we reduce the number of parameters to 488K by freezing the token embeddings, reducing the number of layers in the encoder, and by shrinking the dimension of both the hidden representations and the binary predicate indicator embedding. Following FitzGerald et al. (2018), we use GLoVe embeddings (Pennington et al., 2014). AllenNLP (Gardner et al., 2018), and experiment with two variants: (1) NER-M ULTILANG: A BiLSTM CRF model (20K parameters) with 40 dimensional multi-lingual word embeddings (Ammar et al., 2016), and (2) NER-L INEAR: A linear CRF model which was originally used by Liu et al. (2018). 5.3 Results Span Detection: Table 2 shows F1 score (the official metric) of the QA-SRL span detector models for different sizes of Slab for BASE O RACLE and the other baselines. Figure 2 (left) shows the relative improvement of the baselines over R AN DOM . We observe that the maximal improvement of BASE O RACLE o"
K19-1042,W03-0419,0,0.169446,"Missing"
K19-1042,D18-1318,0,0.0202569,"representation models. In a large empirical study, Lowell et al. (2019) have recently shown other limitations in active learning. They investigate the performance of active learning across NLP tasks and model architectures, and demonstrate that it does not achieve consistent gains over supervised learning, mostly because the collected samples are beneficial to a specific model architecture, and does not yield better results than random selection when switching to a new architecture. There has been little research regarding active learning of semantic representations. Among the relevant work, Siddhant and Lipton (2018) have shown that uncertainty estimation using dropout and Bayes-By-Backprop (Blundell et al., 2015) achieves good results on the SRL formulation. The improvements in performance due to LTAL approaches on various tasks (Konyushkova et al., 2017; Bachman et al., 2017; Fang et al., 2017; Liu et al., 2018) has raised the question whether learned policies can be applied also to the field of learning semantic representations. 8 Acknowledgements We thank Julian Michael and Oz Anani for their useful comments and feedback. This research was supported by The U.S-Israel Binational Science Foundation gran"
K19-1042,P19-1401,0,0.0204549,"the core goals of natural language processing. This goal has led to the creation of many general-purpose formalisms for representing the structure of language, such as semantic role labeling (SRL; Palmer et al., 2005), semantic dependencies (SDP; Oepen et al., 2014), abstract meaning representation (AMR; 452 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 452–462 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics 2 policy generalizes well, we can reduce the cost of learning semantic representations. Liu et al. (2018) and Vu et al. (2019) have shown that such learned policies significantly reduce annotation costs on both text classification and named entity recognition (NER). Learning to Actively Learn Classic pool-based active learning (Settles, 2009) assumes access to a small labeled dataset Slab and a large pool of unlabeled examples Sunlab for a target task. In each iteration, a heuristic is used to select L unlabeled examples, which are sent to annotation and added to Slab . An example heuristic is uncertainty sampling (Lewis and Gale, 1994), which at each iteration chooses examples that the current model is the least con"
K19-1042,D18-1342,0,0.0483898,"Missing"
K19-1042,P18-1191,0,\N,Missing
N18-1059,Q13-1005,0,0.0259447,"Liang, 2017; Chen et al., 2016) that they mostly excel at matching questions to local contexts, but struggle with questions that require reasoning. Moreover, RC assumes documents with the information relevant for the answer are available – but when questions are complex, even retrieving the documents can be difficult. Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Liang et al., 2011). However, this reliance on a manually-curated KB has limited the coverage and applicability of semantic parsers. In this paper we present a framework for QA that is broad, i.e., it does not assume information is in a KB or in retrieved documents, and compositional, i.e., to compute an answer we must perform some computation or reasoning. Our thesis is that answering simple questions can be achieved Introduction Humans often want to answer complex questions that require reasoning over multiple pieces of evidence,"
N18-1059,P16-1145,0,0.0603824,"CCG (Steedman, 2000), or a join operation in λ-DCS (Liang, 2013), where the string is a function applied to previously-computed values. 3. C ONJ(·, ·): takes two sets and returns their intersection. Other set operations can be defined analogously. As syntactic sugar, we allow C ONJ(·) to take strings as input, which means that we run S IMP QA(·) to obtain a set 4 Dataset Evaluating our framework requires a dataset of broad and complex questions that examine the importance of question decomposition. While many QA datasets have been developed recently (Yang et al., 2015; Rajpurkar et al., 2016; Hewlett et al., 2016; Nguyen et al., 2016; Onishi et al., 2016; Hill et al., 2015; Welbl et al., 2017), they lack a focus on the importance of question decomposition. Most RC datasets contain simple questions that can be answered from a short input document. Recently, T RIVIAQA (Joshi et al., 2017) presented a larger portion of complex questions, but still most do not require reasoning. Moreover, the focus of T RIVIAQA is on answer extraction from documents that are given. We, conversely, highlight question decomposition for finding the relevant documents. Put differently, RC is complementary to question decompos"
N18-1059,C16-1236,0,0.27964,"Missing"
N18-1059,D13-1160,1,0.90721,"Missing"
N18-1059,Q15-1039,1,0.852631,"The arguments of a function are its children sub-trees. To compute an answer, or denotation, from a tree, we recursively apply the function at the root to its children. More formally, given a tree rooted at node t, labeled by the function f , that has children c1 (t), . . . , ck (t), the denotation JtK = f (Jc1 (t)K, . . . , Jck (t)K) is an arbitrary function applied to the denotations of the root’s children. Denotations are computed recursively and the denotation of a string at the leaf is the string itself, i.e., JlK = l. This is closely related to “semantic functions” in semantic parsing (Berant and Liang, 2015), except that we do not in1 We differ training from question-answer pairs for future work. 642 and then perform intersection. The root node in Figure 2 illustrates an application of C ONJ. 4. A DD(·, ·): takes two singleton sets of numbers and returns a set with their addition. Similar functions can be defined analogously. While we support mathematical operations, they were not required in our dataset. teract with a KB, but rather compute directly over the breadth of the web through a search engine. Figure 2 provides an example computation tree for our running example. Notice that words at the"
N18-1059,P16-1002,0,0.0523594,"Missing"
N18-1059,D17-1215,0,0.0274749,"decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset. 1 Figure 1: Given a complex questions q, we decompose the question to a sequence of simple questions q1 , q2 , . . . , use a search engine and a QA model to answer the simple questions, from which we compute the final answer a. Neural models trained over large datasets led to great progress in RC, nearing human-level performance (Wang et al., 2017). However, analysis of models revealed (Jia and Liang, 2017; Chen et al., 2016) that they mostly excel at matching questions to local contexts, but struggle with questions that require reasoning. Moreover, RC assumes documents with the information relevant for the answer are available – but when questions are complex, even retrieving the documents can be difficult. Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Artzi and Z"
N18-1059,P17-1147,0,0.58022,"hat answering simple questions can be achieved Introduction Humans often want to answer complex questions that require reasoning over multiple pieces of evidence, e.g., “From what country is the winner of the Australian Open women’s singles 2008?”. Answering such questions in broad domains can be quite onerous for humans, because it requires searching and integrating information from multiple sources. Recently, interest in question answering (QA) has surged in the context of reading comprehension (RC), where an answer is sought for a question given one or more documents (Hermann et al., 2015; Joshi et al., 2017; Rajpurkar et al., 2016). 641 Proceedings of NAACL-HLT 2018, pages 641–651 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics by combining a search engine with a RC model. Thus, answering complex questions can be addressed by decomposing the question into a sequence of simple questions, and computing the answer from the corresponding answers. Figure 1 illustrates this idea. Our model decomposes the question in the figure into a sequence of simple questions, each is submitted to a search engine, and then an answer is extracted from the search result. Onc"
N18-1059,P16-1223,0,0.029924,"and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset. 1 Figure 1: Given a complex questions q, we decompose the question to a sequence of simple questions q1 , q2 , . . . , use a search engine and a QA model to answer the simple questions, from which we compute the final answer a. Neural models trained over large datasets led to great progress in RC, nearing human-level performance (Wang et al., 2017). However, analysis of models revealed (Jia and Liang, 2017; Chen et al., 2016) that they mostly excel at matching questions to local contexts, but struggle with questions that require reasoning. Moreover, RC assumes documents with the information relevant for the answer are available – but when questions are complex, even retrieving the documents can be difficult. Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Kr"
N18-1059,P17-1171,0,0.153042,"Missing"
N18-1059,D12-1069,0,0.0279233,"6) that they mostly excel at matching questions to local contexts, but struggle with questions that require reasoning. Moreover, RC assumes documents with the information relevant for the answer are available – but when questions are complex, even retrieving the documents can be difficult. Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Liang et al., 2011). However, this reliance on a manually-curated KB has limited the coverage and applicability of semantic parsers. In this paper we present a framework for QA that is broad, i.e., it does not assume information is in a KB or in retrieved documents, and compositional, i.e., to compute an answer we must perform some computation or reasoning. Our thesis is that answering simple questions can be achieved Introduction Humans often want to answer complex questions that require reasoning over multiple pieces of evidence, e.g., “From what country is the wi"
N18-1059,W14-4012,0,0.0173443,"Missing"
N18-1059,D13-1161,0,0.0450778,"ing questions to local contexts, but struggle with questions that require reasoning. Moreover, RC assumes documents with the information relevant for the answer are available – but when questions are complex, even retrieving the documents can be difficult. Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Liang et al., 2011). However, this reliance on a manually-curated KB has limited the coverage and applicability of semantic parsers. In this paper we present a framework for QA that is broad, i.e., it does not assume information is in a KB or in retrieved documents, and compositional, i.e., to compute an answer we must perform some computation or reasoning. Our thesis is that answering simple questions can be achieved Introduction Humans often want to answer complex questions that require reasoning over multiple pieces of evidence, e.g., “From what country is the winner of the Australian Ope"
N18-1059,D17-1082,0,0.0481117,"Missing"
N18-1059,D16-1262,0,0.0458963,"Missing"
N18-1059,J13-2005,0,0.0262828,"ng argument and returns a set of strings or numbers as answer. 2. C OMP(·, ·): This function takes a string containing one unique variable VAR, and a set of answers. E.g., in Figure 2 the first argument is “birthplace of VAR”, and the second argument is “{K EN F OLLETT, A DAM Z AGAJEWSKI}”. The function replaces the variable with each answer string representation and returns their union. Formally, C OMP(q, A) = ∪a∈A S IMP QA(q/a), where q/a denotes the string produced when replacing VAR in q with a. This is similar to function composition in CCG (Steedman, 2000), or a join operation in λ-DCS (Liang, 2013), where the string is a function applied to previously-computed values. 3. C ONJ(·, ·): takes two sets and returns their intersection. Other set operations can be defined analogously. As syntactic sugar, we allow C ONJ(·) to take strings as input, which means that we run S IMP QA(·) to obtain a set 4 Dataset Evaluating our framework requires a dataset of broad and complex questions that examine the importance of question decomposition. While many QA datasets have been developed recently (Yang et al., 2015; Rajpurkar et al., 2016; Hewlett et al., 2016; Nguyen et al., 2016; Onishi et al., 2016;"
N18-1059,P11-1060,0,0.0607752,"texts, but struggle with questions that require reasoning. Moreover, RC assumes documents with the information relevant for the answer are available – but when questions are complex, even retrieving the documents can be difficult. Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Liang et al., 2011). However, this reliance on a manually-curated KB has limited the coverage and applicability of semantic parsers. In this paper we present a framework for QA that is broad, i.e., it does not assume information is in a KB or in retrieved documents, and compositional, i.e., to compute an answer we must perform some computation or reasoning. Our thesis is that answering simple questions can be achieved Introduction Humans often want to answer complex questions that require reasoning over multiple pieces of evidence, e.g., “From what country is the winner of the Australian Open women’s singles 200"
N18-1059,P17-1097,0,0.0226282,"Missing"
N18-1059,P16-2033,0,0.193825,"ntic perspective. Still, developing models that are capable of reasoning is an important direction for natural language understanding and C OM PLEX W EB Q UESTIONS provides an opportunity to develop and evaluate such models. To summarize, each of our examples contains a question, an answer, a SPARQL query (that our models ignore), and all web snippets harvested by our model when attempting to answer the question. This renders C OMPLEX W EB Q UESTIONS useful for both the RC and semantic parsing communities. Dataset collection To generate complex questions we use the dataset W EB Q UESTIONS SP (Yih et al., 2016), which contains 4,737 questions paired with SPARQL queries for Freebase (Bollacker et al., 2008). Questions are broad but simple. Thus, we sample question-query pairs, automatically create more complex SPARQL queries, generate automatically questions that are understandable to AMT workers, and then have them paraphrase those into natural language (similar to Wang et al. (2015)). We compute answers by executing complex SPARQL queries against Freebase, and obtain broad and complex questions. Figure 3 provides an example for this procedure, and we elaborate next. Generating SPARQL queries Given"
N18-1059,D16-1241,0,0.0269862,"n λ-DCS (Liang, 2013), where the string is a function applied to previously-computed values. 3. C ONJ(·, ·): takes two sets and returns their intersection. Other set operations can be defined analogously. As syntactic sugar, we allow C ONJ(·) to take strings as input, which means that we run S IMP QA(·) to obtain a set 4 Dataset Evaluating our framework requires a dataset of broad and complex questions that examine the importance of question decomposition. While many QA datasets have been developed recently (Yang et al., 2015; Rajpurkar et al., 2016; Hewlett et al., 2016; Nguyen et al., 2016; Onishi et al., 2016; Hill et al., 2015; Welbl et al., 2017), they lack a focus on the importance of question decomposition. Most RC datasets contain simple questions that can be answered from a short input document. Recently, T RIVIAQA (Joshi et al., 2017) presented a larger portion of complex questions, but still most do not require reasoning. Moreover, the focus of T RIVIAQA is on answer extraction from documents that are given. We, conversely, highlight question decomposition for finding the relevant documents. Put differently, RC is complementary to question decomposition and can be used as part of the imple"
N18-1059,P15-1142,0,0.0573451,"add such functions to the formal language, answering such questions from the web is cumbersome: we would have to extract a list of entities and a numerical value for each. Instead, we handle such constructions using S IMP QA directly, assuming they are mentioned verbatim on some web document. Similarly, negation questions (“What countries are not in the OECD?”) are difficult to handle when working against a search engine only, as this is an open world setup and we do not hold a closed set of countries over which we can perform set subtraction. In future work, we plan to interface with tables (Pasupat and Liang, 2015) and KBs (Zhong et al., 2017). This will allow us to perform set operations over well-defined sets, and handle in a compositional manner superlatives and comparatives. Formal Language Functions in our formal language take arguments and return values that can be strings (when decomposing or re-phrasing the question), sets of strings, or sets of numbers. Our set of functions includes: 1. S IMP QA(·): Model for answering simple questions, which takes a string argument and returns a set of strings or numbers as answer. 2. C OMP(·, ·): This function takes a string containing one unique variable VAR"
N18-1059,D14-1162,0,0.0849055,"Missing"
N18-1059,D16-1264,0,0.578211,"questions can be achieved Introduction Humans often want to answer complex questions that require reasoning over multiple pieces of evidence, e.g., “From what country is the winner of the Australian Open women’s singles 2008?”. Answering such questions in broad domains can be quite onerous for humans, because it requires searching and integrating information from multiple sources. Recently, interest in question answering (QA) has surged in the context of reading comprehension (RC), where an answer is sought for a question given one or more documents (Hermann et al., 2015; Joshi et al., 2017; Rajpurkar et al., 2016). 641 Proceedings of NAACL-HLT 2018, pages 641–651 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics by combining a search engine with a RC model. Thus, answering complex questions can be addressed by decomposing the question into a sequence of simple questions, and computing the answer from the corresponding answers. Figure 1 illustrates this idea. Our model decomposes the question in the figure into a sequence of simple questions, each is submitted to a search engine, and then an answer is extracted from the search result. Once all answers are gathere"
N18-1059,S17-1020,1,0.724066,"oduced decomposition points that are better than the heuristic, e.g., for “What is the place of birth for the lyricist of Roman Holiday”, S PLIT QA produced “the lyricist of Roman Holiday”, but the heuristic produced “the place of birth for the lyricist of Roman Holiday”. Additional examples of S PLIT QA question decompositions are provided in Table 5. Table 4: precision@1 results on the development set and test set for C OMPLEX W EB Q UESTIONS. position or use S IMP QA in hindsight based on what performs better. 4. RCQA: This is identical to S IMP QA, except that we replace the RC model from Talmor et al. (2017) with the the RC model D OC QA (Clark and Gardner, 2017), whose performance is comparable to state-of-the-art on T RIVIAQA. 5. S PLIT RCQA: This is identical to S PLIT QA, except that we replace the RC model from Talmor et al. (2017) with D OC QA. 6. G OOGLE B OX: We sample 100 random development set questions and check whether Google returns a box that contains one of the correct answers. 7. H UMAN: We sample 100 random development set questions and manually answer the questions with Google’s search engine, including all available information. We limit the amount of time allowed for answering"
N18-1059,P17-1018,0,0.0571283,"ions, C OMPLEX W EB Q UES TIONS , and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset. 1 Figure 1: Given a complex questions q, we decompose the question to a sequence of simple questions q1 , q2 , . . . , use a search engine and a QA model to answer the simple questions, from which we compute the final answer a. Neural models trained over large datasets led to great progress in RC, nearing human-level performance (Wang et al., 2017). However, analysis of models revealed (Jia and Liang, 2017; Chen et al., 2016) that they mostly excel at matching questions to local contexts, but struggle with questions that require reasoning. Moreover, RC assumes documents with the information relevant for the answer are available – but when questions are complex, even retrieving the documents can be difficult. Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) (Zelle a"
N18-1059,P15-1129,1,0.862932,"en attempting to answer the question. This renders C OMPLEX W EB Q UESTIONS useful for both the RC and semantic parsing communities. Dataset collection To generate complex questions we use the dataset W EB Q UESTIONS SP (Yih et al., 2016), which contains 4,737 questions paired with SPARQL queries for Freebase (Bollacker et al., 2008). Questions are broad but simple. Thus, we sample question-query pairs, automatically create more complex SPARQL queries, generate automatically questions that are understandable to AMT workers, and then have them paraphrase those into natural language (similar to Wang et al. (2015)). We compute answers by executing complex SPARQL queries against Freebase, and obtain broad and complex questions. Figure 3 provides an example for this procedure, and we elaborate next. Generating SPARQL queries Given a SPARQL query r, we create four types of more complex queries: conjunctions, superlatives, comparatives, and compositions. Table 1 gives the exact rules for generation. For conjunctions, superlatives, and comparatives, we identify queries in W EB Q UESTIONS SP whose denotation is a set A, |A |≥ 2, and generate a new query r0 whose denotation is a strict subset A0 , A0 ⊂ A, A0"
N18-1059,D15-1237,0,0.0421265,"This is similar to function composition in CCG (Steedman, 2000), or a join operation in λ-DCS (Liang, 2013), where the string is a function applied to previously-computed values. 3. C ONJ(·, ·): takes two sets and returns their intersection. Other set operations can be defined analogously. As syntactic sugar, we allow C ONJ(·) to take strings as input, which means that we run S IMP QA(·) to obtain a set 4 Dataset Evaluating our framework requires a dataset of broad and complex questions that examine the importance of question decomposition. While many QA datasets have been developed recently (Yang et al., 2015; Rajpurkar et al., 2016; Hewlett et al., 2016; Nguyen et al., 2016; Onishi et al., 2016; Hill et al., 2015; Welbl et al., 2017), they lack a focus on the importance of question decomposition. Most RC datasets contain simple questions that can be answered from a short input document. Recently, T RIVIAQA (Joshi et al., 2017) presented a larger portion of complex questions, but still most do not require reasoning. Moreover, the focus of T RIVIAQA is on answer extraction from documents that are given. We, conversely, highlight question decomposition for finding the relevant documents. Put differe"
N18-1066,P17-2098,1,0.837602,"grams from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously. Our ultimate goal is to build polyglot translation models (cf. Johnson et al. (2016)), or models with shared representations that can translate any input text to any output programming language, regardless of whether such language pairs were encountered explicitly during training. Inherent in this task is the challenge of building an efficient polyglot decoder, or a translation mechanism that"
N18-1066,P13-2009,0,0.183632,"and 112,107 paths or component representations over an output vocabulary of 9,324 words. 5.2 Experimental Setup For the technical datasets, the goal is to see if our model generates correct signature representations from unobserved descriptions using exact match. We follow exactly the experimental setup and data splits from Richardson and Kuhn (2017b), and measure the accuracy at 1 (Acc@1), accuracy in top 10 (Acc@10), and MRR. For the GeoQuery and Sportscaster experiments, the goal is to see if our models can generate correct meaning representations for unseen input. For GeoQuery, we follow Andreas et al. (2013) in evaluating extrinsically by checking that each representation evaluates to the same answer as the gold representation when executed against the Geobase database. For Sportscaster, we evaluate by exact match to a gold representation. Mixed GeoQuery and Sportscaster We run experiments on the GeoQuery 880 corpus using the splits from Andreas et al. (2013), which includes geography queries for English, Greek, Thai, and German paired with formal database queries, as well as a seed lexicon or NP list for each language. In addition to training models on each individual dataset, we also learn poly"
N18-1066,C08-5001,0,0.0346971,"olyglot decoding, i.e., generating any output language by starting at the initial state 0 (e.g., C and Clojure). We formulate the decoding problem using a variant of the well-known single source shortest path (SSSP) algorithm for directed acyclic graphs (DAGs) (Johnson (1977)). This involves a graph G = (V, E) (nodes V and labeled edges E, see graph in Figure 2), and taking an off-line topological sort of the graph’s vertices. Using a data structure d ∈ R|V |(initialized as ∞|V |, as shown in Figure 2), the standard SSSP algorithm (which is the forward update variant of the Viterbi algorithm (Huang, 2008)) works by searching forward through the graph in sorted order and finding for each node v an incoming labeled edge u, with label z, that solves the following recurrence: n o d(v) = min d(u) + w(u, v, z) (2) Input: Input x of size n, DAG G = (V, E), lexical translation function pt , source node b with initial score o. Output: Shortest component path 1: d[V [G]] ← ∞, π[V [G]] ← N il, d[b] ← o 2: s[V [G], n] ← 0.0 . Shortest path sums at each node 3: for each vertex u ≥ b ∈ V [G] in sorted order do 4: for each vertex and label  Qn(v, z) ∈ Adj[u] do  5: score ← −log i pt (xi |z) + s[u, i] 6: if"
N18-1066,E09-2008,0,0.0321984,"r English, Greek, Thai, and German paired with formal database queries, as well as a seed lexicon or NP list for each language. In addition to training models on each individual dataset, we also learn polyglot models trained on all datasets concatenated together. We also created a new mixed language test set that was built by re726 Sportscaster 5.3 Mixed Table 1: Test results on the Stdlib and Py27 tasks averaged over all datasets and compared against the best monolingual results from Richardson and Kuhn (2017b,a), or RK Implementation and Model Details We use the Foma finite-state toolkit of Hulden (2009) to construct all graphs used in our experiments. We also use the Cython version of Dynet (Neubig et al., 2017) to implement all the neural models (see supp. materials for more details). In the results tables, we refer to the lexical and neural models introduced in Section 4 as Lexical Shortest Path and Neural Shortest Path, where models that use copying (+ copy) and lexical biasing (+ bias) are marked accordingly. We also experimented with adding a discriminative reranker to our lexical models (+ rerank), using the approach from Richardson and Kuhn (2017b), which uses additional lexical (e.g."
N18-1066,P16-1195,0,0.0302132,"; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more genera"
N18-1066,D13-1160,1,0.747,"idual APIs may be too small or poorly documented to build individual models or QA applications, and will in some way need to bootstrap off of more general models or resources. 2 Related Work Our approach builds on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al"
N18-1066,P16-1002,0,0.181603,"on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation ("
N18-1066,D11-1131,0,0.0737932,"Missing"
N18-1066,C14-1122,0,0.103952,"API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously. Our ultimate goal is to build polyglot translation models (cf. Johnson et al. (2016)), or models with shared representations that can translate any input text to any output programming language, regardless of whether such language pairs were encountered explicitly during trai"
N18-1066,P12-1051,0,0.57328,"Missing"
N18-1066,P17-1005,0,0.0813129,"ls introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Ri"
N18-1066,J99-4005,0,0.145502,"hin this 721 pendent word generation process: When building models that can translate between unobserved language pairs, we use the term zeroshot translation from Johnson et al. (2016). 3 |x ||z| 1 YX p(x |z) = pt (xj |zi ) |A| Baseline Semantic Translator where each pt defines a multinomial distribution over a given component term z for all words x. The decoding problem for the above translation model involves finding the most likely output zˆ, which requires solving an arg maxz over Equation 1. In the general case, this problem is known to be N P-complete for the models under consideration (Knight, 1999) largely due to the large space of possible predictions z. Richardson and Kuhn (2017b) avoid these issues by exploiting the finiteness of the target component search space (an idea we also pursue here and discuss more below), and describe a constrained decoding algorithm that runs in time O(|C |log |C|). While this works well for small APIs, it becomes less feasible when dealing with large sets of APIs, as in the polyglot case, or with more complex semantic languages typically used in SP (Liang, 2013). Problem Formulation Throughout the paper, we refer to target code representations as API com"
N18-1066,knight-al-onaizan-1998-translation,0,0.0889234,"our first model, we use the lexical translation model and probability function pt in Equation 1 as 723 ← − ← − den states ( h , ..., h |x |) for the input sequence (x1 , ..., x|x |). Standardly, each word is then represented as the concatenation of its forward and → − ← − backward states: hj = [ h j , h j ]. the weighting function, which can be learned efficiently off-line using the EM algorithm. When attempting to use the SSSP procedure to compute this equation for a given source input x, we immediately have the problem that such a computation requires a complete component representation z (Knight and Al-Onaizan, 1998). We use an approximation1 that involves ignoring the normalizer |A |and exploiting the word independence assumption of the model, which allows us to incrementally compute translation scores for individual source words given output translations corresponding to shortest paths during the SSSP search. The full decoding algorithm in shown in Algorithm 1, where the red highlights the adjustments made to the standard SSSP search as presented in Cormen et al. (2009). The main modification involves adding a data structure s ∈ R|V |× |x |(initialized as 0.0|V |×|x |at line 2) that stores a running sum"
N18-1066,deng-chrupala-2014-semantic,0,0.0242857,"not observed during training. While software documentation is easy to find in bulk, if a particular API is not already documented in a language other than English (e.g., Haskell in de), it is unlikely that such a translation will appear without considerable effort by experienced translators. Similarly, many individual APIs may be too small or poorly documented to build individual models or QA applications, and will in some way need to bootstrap off of more general models or resources. 2 Related Work Our approach builds on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in thi"
N18-1066,D16-1116,0,0.0350275,"Missing"
N18-1066,P16-1004,0,0.0550629,"need to bootstrap off of more general models or resources. 2 Related Work Our approach builds on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig,"
N18-1066,D10-1119,0,0.180033,"Missing"
N18-1066,K17-1038,0,0.093166,"Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously. Our ultimate goal is to build polyglot translation models (cf. Johnson et al. (2016)), or models with shared representations that can translate any input text to any output programming language, regardless of whether such language pairs were encountered explicitly during training. Inherent in thi"
N18-1066,J13-2005,0,0.0524802,"e general case, this problem is known to be N P-complete for the models under consideration (Knight, 1999) largely due to the large space of possible predictions z. Richardson and Kuhn (2017b) avoid these issues by exploiting the finiteness of the target component search space (an idea we also pursue here and discuss more below), and describe a constrained decoding algorithm that runs in time O(|C |log |C|). While this works well for small APIs, it becomes less feasible when dealing with large sets of APIs, as in the polyglot case, or with more complex semantic languages typically used in SP (Liang, 2013). Problem Formulation Throughout the paper, we refer to target code representations as API components. In all cases, components will consist of formal representations of functions, or function signatures (e.g., long max(int a, int b)), which include a function name (max), a sequence of arguments (int a, int b), and other information such as a return value (long) and namespace (for more details, see Richardson (2018)). For a given API dataset D = {(xi , zi )}ni=1 of size n, the goal is to learn a model that can generate exactly a correct component sequence z = (z1 , .., z|z |), within a finite"
N18-1066,D15-1166,0,0.0188769,"gi is the decoder’s hidden state at step i, and ci is a contextvector that encodes information about the input x and the encoder annotations. Each context vector ci in turn is a weighted sum of each annotation hj against an attention vector αi,j , or ci = P|x| j=1 αi,j hj , which is jointly learned using an additional single layered multi-layer perceptron defined in the following way: Neural Shortest Path αi,j ∝ exp(ei,j ); Our second set of models use neural networks to compute the weighting function in Equation 2. We use an encoder-decoder model with global attention (Bahdanau et al., 2014; Luong et al., 2015), which has the following two components: ei,j = MLP(gi−1 , hj ) (8) Lexical Bias and Copying In contrast to standard MT tasks, we are dealing with a relatively low-resource setting where the sparseness of the target vocabulary is an issue. For this reason, we experimented with integrating lexical translation scores using a biasing technique from Arthur et al. (2016). Their method is based on the following computation for each token zi : Encoder Model The first is an encoder network, which uses a bi-directional recurrent neural network architecture with LSTM units (Hochreiter and Schmidhuber,"
N18-1066,W17-3516,1,0.852695,"7) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation"
N18-1066,I17-2053,0,0.0975531,"problem of text to function signature translation. Initially, these datasets were proposed as a resource for studying semantic parser induction (Mooney, 2007), or for building models that learn to translate text to formal meaning representations from parallel data. In followup work (Richardson and Kuhn, 2017a), they proposed using the resulting models to do automated question-answering (QA) and code retrieval on target APIs, and experimented with an additional set of software datasets built from 27 open-source Python projects. Introduction Recent work by Richardson and Kuhn (2017a,b); Miceli Barone and Sennrich (2017) considers the problem of translating source code documentation to lower-level code template representations as part of an effort to model the meaning of such documentation. Example documentation for a number of programming languages is shown in Figure 1, where each docstring description in red describes a given function (blue) in the library. While capturing the semantics of docstrings is in general a difficult task, learning the translation from descriptions to formal code representations (e.g., formal representations of functions) is proposed as a reasonable first step towards learning more"
N18-1066,N16-1161,0,0.0280835,"(cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learning from multiple APIs. To deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously. Our ultimate goal is to build polyglot translation models (cf. Johnson et al. (2016)), or models with shared representations that can translate any input text to any output programming l"
N18-1066,N06-1056,0,0.487003,"already documented in a language other than English (e.g., Haskell in de), it is unlikely that such a translation will appear without considerable effort by experienced translators. Similarly, many individual APIs may be too small or poorly documented to build individual models or QA applications, and will in some way need to bootstrap off of more general models or resources. 2 Related Work Our approach builds on the baseline models introduced in Richardson and Kuhn (2017b) (see also Deng and Chrupała (2014)). Their work is positioned within the broader SP literature, where traditionally SMT (Wong and Mooney, 2006a) and parsing (Zettlemoyer and Collins, 2009) methods are used to study the problem of translating text to formal meaning representations, usually centering around QA applications (Berant et al., 2013). More recently, there has been interest in using neural network approaches either in place of (Dong and Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the avai"
N18-1066,P17-1041,0,0.0277633,"nd Lapata, 2016; Koˇcisk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is re"
N18-1066,J03-1002,0,0.0145505,"entations in all languages. Word-based Translation Model The translation models investigated in Richardson and Kuhn (2017b) use a noisy-channel formulation where p(z |x) ∝ p(x |z)p(z) via Bayes rule. By assuming a uniform prior on output components, p(z), the model therefore involves estimating p(x |z), which under a word-translation model is computed using the following formula: P p(x |z) = a∈A p(x, a |z), where the summation ranges over the set of all many-to-one word alignments A from x → z, with |A |equal to (|z |+ 1)|x |. They investigate various types of sequence-based alignment models (Och and Ney, 2003), and find that the classic IBM Model 1 outperforms more complex word models. This model factors in the following way and assumes an inde722 ceil s0 2C 0.00 2Clojure s5 numeric s1 s7 math ∞7 ∞6 ∞5 ∞1 s6 algo ∞2 s2 ∞3 math s3 atan2 atan2 ceil s9 s8 ∞9 ∞8 x ∞4 ∞10 s4 s10 arg s11 y ∞11 x Figure 2: A DAFSA representation for a portion of the component sequence search space C that includes math functions in C and Clojure, and an example path/translation (in bold): 2C numeric math ceil arg. Algorithm 1 Lexical Shortest Path Search Decoding reduces to the problem of finding a path for a given text in"
N18-1066,P09-1110,0,0.0929458,"Missing"
N18-1066,P17-1105,0,0.0280205,"sk´y et al., 2016) or in combination with (Misra and Artzi, 2016; Jia and Liang, 2016; Cheng et al., 2017) these traditional models, the latter idea we look at in this paper. Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github (cf. Allamanis et al. (2017)). Most of this recent work focuses on processing large amounts of API data in bulk (Gu et al., 2016; Miceli Barone and Sennrich, 2017), either for learning longer executable programs from text (Yin and Neubig, 2017; Rabinovich et al., 2017), or solving the inverse problem of code to text generation (Iyer et al., 2016; Richardson et al., 2017). In contrast to our work, these studies do not look explicitly at translating to target APIs, or at non-English documentation. The idea of polyglot modeling has gained some traction in recent years for a variety of problems (Tsvetkov et al., 2016) and has appeared within work in SP under the heading of multilingual SP (Jie and Lu, 2014; Duong et al., 2017). A related topic is learning from multiple knowledge sources or domains (Herzig and Berant, 2017), which is related to our idea of learn"
N18-1066,D16-1163,0,0.0234111,".2 78.0 78.9 79.6 4.2 71.1 75.2 Acc@10 – – – 92.4 94.1 91.1 92.8 92.1 92.9 94.7 91.4 91.7 91.9 18.2 94.3 90.0 74.2 86.0 40.3 70.3 81.9 83.4 83.3 – – 86.8 90.2 94.8 93.9 90.5 Table 2: Test results for the standard (above) and mixed (middle) GeoQuery tasks averaged over all languages, and results for the English Sportscaster task (below). The neural models are strongly outperformed by all other models both in the monolingual and polyglot case (only the latter results shown), even when lexical biasing is applied. While surprising, this is consistent with other studies on low¨ resource neural MT (Zoph et al., 2016; Ostling and Tiedemann, 2017), where datasets of comparable size to ours (e.g., 1 million tokens or less) typically fail against classical SMT models. This result has also been found in relation to neural AMR semantic parsing, where similar issues of sparsity are encountered (Peng et al., 2017). Even by doubling the amount of training data by training on all datasets (results not shown), this did not improve the accuracy, suggesting that much more data is needed (more discussion below). Beyond increases in accuracy, our polyglot models support zero-shot translation as shown in Figure 4, which"
N18-1066,D17-2012,1,0.0587922,"t.de ‡ Tel-Aviv University, Israel joberant@cs.tau.ac.il Abstract 1. (en, Java) Documentation *Returns the greater of two long values public static long max(long a, long b) Traditional approaches to semantic parsing (SP) work by training individual models for each available parallel dataset of text-meaning pairs. In this paper, we explore the idea of polyglot semantic translation, or learning semantic parsing models that are trained on multiple datasets and natural languages. In particular, we focus on translating text to code signature representations using the software component datasets of Richardson and Kuhn (2017a,b). The advantage of such models is that they can be used for parsing a wide variety of input natural languages and output programming languages, or mixed input languages, using a single unified model. To facilitate modeling of this type, we develop a novel graph-based decoding framework that achieves state-of-the-art performance on the above datasets, and apply this method to two other benchmark SP tasks. 1 2. (en, Python) Documentation max(self, a, b): """"""Compares two values numerically and returns the maximum"""""" 3. (en, Haskell) Documentation -- |""The largest element of a non-empty struct"
N18-1066,D16-1162,0,\N,Missing
N18-1066,E17-1035,0,\N,Missing
N18-1066,P16-1162,0,\N,Missing
N18-2075,D14-1162,0,0.0811077,"Missing"
N18-2075,W12-3307,0,0.594313,"g preprocessing steps for each Wikipedia document: • Removed all photos, tables, Wikipedia template elements, and other non-text elements. • Removed single-sentence segments, documents with less than three segments, and documents where most segments were filtered. • Divided each segment into sentences using the P UNKT tokenizer of the NLTK library (Bird et al., 2009). This is necessary for the use of our dataset as a benchmark, as without a well-defined sentence segmentation, it is impossible to evaluate different models. Previous Methods Bayesian text segmentation methods (Chen et al., 2009; Riedl and Biemann, 2012) employ a generative probabilistic model for text. In these models, a document is represented as a set of topics, which are sampled from a topic distribution, and each topic imposes a distribution over the vocabulary. Riedl and Biemann (2012) perform best among this family of methods, where they define a coherence score between pairs of sentences, and compute a segmentation by finding drops in coherence scores between pairs of adjacent sentences. Another noteworthy approach for text segmentation is G RAPH S EG (Glavaˇs et al., 2016), an unsupervised graph method, which performs competitively o"
N18-2075,P01-1064,0,0.733653,"cused on unsupervised methods such as clustering or graph search, due to the paucity in labeled data. In this work, we formulate text segmentation as a supervised learning problem, and present a large new dataset for text segmentation that is automatically extracted and labeled from Wikipedia. Moreover, we develop a segmentation model based on this dataset and show that it generalizes well to unseen natural text. 1 Introduction Text segmentation is the task of dividing text into segments, such that each segment is topically coherent, and cutoff points indicate a change of topic (Hearst, 1994; Utiyama and Isahara, 2001; Brants et al., 2002). This provides basic structure to a document in a way that can later be used by downstream applications such as summarization and information extraction. Existing datasets for text segmentation are small in size (Choi, 2000; Glavaˇs et al., 2016), and are used mostly for evaluating the performance of segmentation algorithms. Moreover, some datasets (Choi, 2000) were synthesized automatically and thus do not represent the natural distribution of text in documents. Because no large labeled dataset exists, prior work on text segmentation tried to either come up with heurist"
N18-2075,N09-1042,0,0.164582,"in size (Choi, 2000; Glavaˇs et al., 2016), and are used mostly for evaluating the performance of segmentation algorithms. Moreover, some datasets (Choi, 2000) were synthesized automatically and thus do not represent the natural distribution of text in documents. Because no large labeled dataset exists, prior work on text segmentation tried to either come up with heuristics for identifying whether two sentences discuss the same topic (Choi, 2000; Glavaˇs et al., 2016), or to model topics explicitly with methods such as LDA (Blei et al., 2003) that assign a topic to each paragraph or sentence (Chen et al., 2009). 2 2.1 Related Work Existing Text Segmentation Datasets The most common dataset for evaluating performance on text segmentation was created by Choi (2000). It is a synthetic dataset containing 920 documents, where each document is a concatena∗ Both authors contributed equally to this paper and the order of authorship was determined randomly. 469 Proceedings of NAACL-HLT 2018, pages 469–473 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics tion of 10 random passages from the Brown corpus. Glavaˇs et al. (2016) created a dataset of their own, which consi"
N18-2075,A00-2004,0,0.912735,"acted and labeled from Wikipedia. Moreover, we develop a segmentation model based on this dataset and show that it generalizes well to unseen natural text. 1 Introduction Text segmentation is the task of dividing text into segments, such that each segment is topically coherent, and cutoff points indicate a change of topic (Hearst, 1994; Utiyama and Isahara, 2001; Brants et al., 2002). This provides basic structure to a document in a way that can later be used by downstream applications such as summarization and information extraction. Existing datasets for text segmentation are small in size (Choi, 2000; Glavaˇs et al., 2016), and are used mostly for evaluating the performance of segmentation algorithms. Moreover, some datasets (Choi, 2000) were synthesized automatically and thus do not represent the natural distribution of text in documents. Because no large labeled dataset exists, prior work on text segmentation tried to either come up with heuristics for identifying whether two sentences discuss the same topic (Choi, 2000; Glavaˇs et al., 2016), or to model topics explicitly with methods such as LDA (Blei et al., 2003) that assign a topic to each paragraph or sentence (Chen et al., 2009)."
N18-2075,P13-1167,0,0.060164,"fined in Beeferman et al. (1999) to evaluate the performance of our model. Pk is the probability that when passing a sliding window of size k over sentences, the sentences at the boundaries of the window will be incorrectly classified as belonging to the same segment (or vice versa). To match the setup of Chen et al. (2009), we also provide the Pk metric for a sliding window over words when evaluating on the datasets from their paper. Following (Glavaˇs et al., 2016), we set k to half of the average segment size in the ground-truth segmentation. For evaluations we used the S EG E VAL package (Fournier, 2013). In addition to segmentation accuracy, we also report runtime when running on a mid-range laptop CPU. We note that segmentation results are not always directly comparable. For example, Chen et al. (2009) require that all documents in the dataset discuss the same topic, and so their method is not directly applicable to W IKI -50. Nevertheless, we attempt a comparison in Table 2. 4.2 5.1 Figure 1: Our model contains a sentence embedding sub-network, followed by a segmentation prediction sub-network which predicts a cut-off probability for each sentence. LSTM. We then apply a fully-connected lay"
N18-2075,S16-2016,0,0.544782,"Missing"
N18-2075,P94-1002,0,0.909546,"egmentation focused on unsupervised methods such as clustering or graph search, due to the paucity in labeled data. In this work, we formulate text segmentation as a supervised learning problem, and present a large new dataset for text segmentation that is automatically extracted and labeled from Wikipedia. Moreover, we develop a segmentation model based on this dataset and show that it generalizes well to unseen natural text. 1 Introduction Text segmentation is the task of dividing text into segments, such that each segment is topically coherent, and cutoff points indicate a change of topic (Hearst, 1994; Utiyama and Isahara, 2001; Brants et al., 2002). This provides basic structure to a document in a way that can later be used by downstream applications such as summarization and information extraction. Existing datasets for text segmentation are small in size (Choi, 2000; Glavaˇs et al., 2016), and are used mostly for evaluating the performance of segmentation algorithms. Moreover, some datasets (Choi, 2000) were synthesized automatically and thus do not represent the natural distribution of text in documents. Because no large labeled dataset exists, prior work on text segmentation tried to"
N18-2088,P17-1171,0,0.101011,"Missing"
N18-2088,W14-4012,0,0.109159,"Missing"
N18-2088,P17-1055,0,0.032513,"2 84.4 84.3 83.9 83.8 83.3 83.1 81.9 70.8 78.7 Model RaSoR + TR + LM(L1) [1] Mnemonic Reader [2] RaSoR + TR [3] MPCM [4] RaSoR (base model) [5] ReasoNet [6] jNet [7] AddSent AddOneSent 47.0 46.6 44.5 40.3 39.5 39.4 37.9 57.0 56.0 53.9 50.0 49.5 50.3 47.0 Table 3: Single-model F1 on adversarial SQuAD. [1,3] This work. [2] Hu et al. (2017) [4] Wang et al. (2016) [5] Lee et al. (2016) [6] Shen et al. (2017) [7] Zhang et al. (2017) Table 2: Single-model results on SQuAD’s test set.3 [1] Peters et al. (2018) [2,7] This work. [3] Liu et al. (2017b) [4] Wang et al. (2017) [5] Huang et al. (2017) [6] Cui et al. (2017) [8] Xiong et al. (2017a) [9] Liu et al. (2017a) [10] Lee et al. (2016) 5 Experimental setup We use pre-trained GloVe embeddings (Pennington et al., 2014) of dimension dw = 300 and produce character-based word representations via dc = 100 convolutional filters over character embeddings as in Seo et al. (2016). For all BiLSTMs, hyper-parameter search included the following values, with model selection being done according to validation set results (underlined): number of stacked BiLSTM layers (1, 2, 3), number of cells dh (50, 100, 200), dropout rate over input (0.4, 0.5, 0.6), dropout rate ove"
N18-2088,D14-1181,0,0.0283668,", . . . , wk with wt ∈ Rdw , the re-embedding of the t-th token wt0 is the result of a Highway layer (Srivastava et al., 2015) and is defined as: wt0 = gt wt + (1 − gt ) zt gt = σ(Wg xt + Ug ut ) zt = tanh(Wz xt + Uz ut ) where xt is a function strictly of the word-type of the t-th token, ut is a function of the enclosing sequence, Wg , Wz , Ug , U z are parameter matrices, and the element-wise product operator. We set xt = [wt ; ct ], a concatenation of wt with ct ∈ Rdc where the latter is a character-based representation of the token’s word-type produced via a CNN over character embeddings (Kim, 2014). We note that word-embeddings wt are pre-trained (Pennington et al., 2014) and are kept fixed during training, as is commonly done in order to reduce model capacity and mitigate overfitting. We next describe different formulations for the contextual term ut . 3 Base model We build upon Lee et al. (2016), who proposed the RaSoR model. For word-embedding inputs q1 , . . . , qm and p1 , . . . , pn of dimension dw , RaSoR consists of the following components: Passage-independent question representation The question is encoded via a BiLSTM {v1 , . . . , vm } = BiLSTM(q1 , . . . , qm ) and the resu"
N18-2088,P17-1018,0,0.0298877,"7.6 76.8 76.5 76.0 75.8 75.8 75.1 73.2 85.8 84.2 84.4 84.3 83.9 83.8 83.3 83.1 81.9 70.8 78.7 Model RaSoR + TR + LM(L1) [1] Mnemonic Reader [2] RaSoR + TR [3] MPCM [4] RaSoR (base model) [5] ReasoNet [6] jNet [7] AddSent AddOneSent 47.0 46.6 44.5 40.3 39.5 39.4 37.9 57.0 56.0 53.9 50.0 49.5 50.3 47.0 Table 3: Single-model F1 on adversarial SQuAD. [1,3] This work. [2] Hu et al. (2017) [4] Wang et al. (2016) [5] Lee et al. (2016) [6] Shen et al. (2017) [7] Zhang et al. (2017) Table 2: Single-model results on SQuAD’s test set.3 [1] Peters et al. (2018) [2,7] This work. [3] Liu et al. (2017b) [4] Wang et al. (2017) [5] Huang et al. (2017) [6] Cui et al. (2017) [8] Xiong et al. (2017a) [9] Liu et al. (2017a) [10] Lee et al. (2016) 5 Experimental setup We use pre-trained GloVe embeddings (Pennington et al., 2014) of dimension dw = 300 and produce character-based word representations via dc = 100 convolutional filters over character embeddings as in Seo et al. (2016). For all BiLSTMs, hyper-parameter search included the following values, with model selection being done according to validation set results (underlined): number of stacked BiLSTM layers (1, 2, 3), number of cells dh (50, 100, 200), dropout rat"
N18-2088,D16-1244,0,0.0862231,"Missing"
N18-2088,K17-1028,0,0.0357359,"Missing"
N18-2088,D14-1162,0,0.0846227,"en wt0 is the result of a Highway layer (Srivastava et al., 2015) and is defined as: wt0 = gt wt + (1 − gt ) zt gt = σ(Wg xt + Ug ut ) zt = tanh(Wz xt + Uz ut ) where xt is a function strictly of the word-type of the t-th token, ut is a function of the enclosing sequence, Wg , Wz , Ug , U z are parameter matrices, and the element-wise product operator. We set xt = [wt ; ct ], a concatenation of wt with ct ∈ Rdc where the latter is a character-based representation of the token’s word-type produced via a CNN over character embeddings (Kim, 2014). We note that word-embeddings wt are pre-trained (Pennington et al., 2014) and are kept fixed during training, as is commonly done in order to reduce model capacity and mitigate overfitting. We next describe different formulations for the contextual term ut . 3 Base model We build upon Lee et al. (2016), who proposed the RaSoR model. For word-embedding inputs q1 , . . . , qm and p1 , . . . , pn of dimension dw , RaSoR consists of the following components: Passage-independent question representation The question is encoded via a BiLSTM {v1 , . . . , vm } = BiLSTM(q1 , . . . , qm ) and the resulting hidden states are summarized via attention (BahdanauP et al., 2015; P"
N18-2088,P17-1161,0,0.018722,"M function. Our formulation is external to that definition, with the specific goal of gating between LSTM hidden states and fixed wordembeddings. Multiple works have shown the efficacy of semi-supervision for NLP tasks (Søgaard, 2013). 3 From SQuAD’s leaderboard per Dec 13, 2017. http: //rajpurkar.github.io/SQuAD-explorer 557 Pre-training a LM in order to initialize the weights of an encoder has been reported to improve generalization and training stability for sequence classification (Dai and Le, 2015) as well as translation and summarization (Ramachandran et al., 2017). Similar to our work, Peters et al. (2017) utilize the same pre-trained LM from J´ozefowicz et al. (2016) for sequence tagging tasks, keeping encoder weights fixed during training. Their formulation includes a backward LM and uses the hidden states from the top-most stacked LSTM layer of the LMs, whereas we also consider reading the hidden states of the bottom one, which substantially improves performance. In parallel to our work, Peters et al. (2018) have successfully leveraged pre-trained LMs for several tasks, including RC, by utilizing representations from all layers of the pre-trained LM. In a transfer-learning setting, McCann et"
N18-2088,N18-1202,0,0.0450056,"[7] DCN+ [8] Conductor-net [9] ··· RaSoR (base model) [10] 78.6 77.6 76.8 76.5 76.0 75.8 75.8 75.1 73.2 85.8 84.2 84.4 84.3 83.9 83.8 83.3 83.1 81.9 70.8 78.7 Model RaSoR + TR + LM(L1) [1] Mnemonic Reader [2] RaSoR + TR [3] MPCM [4] RaSoR (base model) [5] ReasoNet [6] jNet [7] AddSent AddOneSent 47.0 46.6 44.5 40.3 39.5 39.4 37.9 57.0 56.0 53.9 50.0 49.5 50.3 47.0 Table 3: Single-model F1 on adversarial SQuAD. [1,3] This work. [2] Hu et al. (2017) [4] Wang et al. (2016) [5] Lee et al. (2016) [6] Shen et al. (2017) [7] Zhang et al. (2017) Table 2: Single-model results on SQuAD’s test set.3 [1] Peters et al. (2018) [2,7] This work. [3] Liu et al. (2017b) [4] Wang et al. (2017) [5] Huang et al. (2017) [6] Cui et al. (2017) [8] Xiong et al. (2017a) [9] Liu et al. (2017a) [10] Lee et al. (2016) 5 Experimental setup We use pre-trained GloVe embeddings (Pennington et al., 2014) of dimension dw = 300 and produce character-based word representations via dc = 100 convolutional filters over character embeddings as in Seo et al. (2016). For all BiLSTMs, hyper-parameter search included the following values, with model selection being done according to validation set results (underlined): number of stacked BiLSTM l"
N18-2088,D17-1039,0,0.023718,"ons are introduced into the definition of the LSTM function. Our formulation is external to that definition, with the specific goal of gating between LSTM hidden states and fixed wordembeddings. Multiple works have shown the efficacy of semi-supervision for NLP tasks (Søgaard, 2013). 3 From SQuAD’s leaderboard per Dec 13, 2017. http: //rajpurkar.github.io/SQuAD-explorer 557 Pre-training a LM in order to initialize the weights of an encoder has been reported to improve generalization and training stability for sequence classification (Dai and Le, 2015) as well as translation and summarization (Ramachandran et al., 2017). Similar to our work, Peters et al. (2017) utilize the same pre-trained LM from J´ozefowicz et al. (2016) for sequence tagging tasks, keeping encoder weights fixed during training. Their formulation includes a backward LM and uses the hidden states from the top-most stacked LSTM layer of the LMs, whereas we also consider reading the hidden states of the bottom one, which substantially improves performance. In parallel to our work, Peters et al. (2018) have successfully leveraged pre-trained LMs for several tasks, including RC, by utilizing representations from all layers of the pre-trained LM"
N19-1139,N18-1170,0,0.0255673,"e gold label. 1 Figure 1: (A) Using a white-box attack we generate adversarial examples for a source toxicity model S(·). We train our black-box attacker, D IST F LIP, to emulate the white-box attack. (B) We use D IST F LIP to attack a black-box model. Introduction Adversarial examples (Goodfellow et al., 2014) have gained tremendous attention recently, as they elucidate model limitations, and expose vulnerabilities in deployed systems. Work in natural language processing (NLP) either (a) used simple heuristics for generating adversarial examples (Jia and Liang, 2017; Belinkov and Bisk, 2017; Iyyer et al., 2018), or (b) assumed white-box access, where the attacker has access to gradients of the model with respect to the input (Feng et al., 2018; Ebrahimi et al., 2018). In this approach, adversarial examples are constructed through an optimization process that uses gradient descent to search for input examples that maximally change the predictions of a model. However, developing attacks with only black-box access to a model (no access to gradients) is still under-explored in NLP. ∗ Equal contribution Inspired by work in computer vision (Papernot et al., 2016; Liu et al., 2017), we show in this work th"
N19-1139,W14-4012,0,0.0352936,"Missing"
N19-1139,D17-1215,0,0.0328229,"humans maintain high-accuracy in predicting the gold label. 1 Figure 1: (A) Using a white-box attack we generate adversarial examples for a source toxicity model S(·). We train our black-box attacker, D IST F LIP, to emulate the white-box attack. (B) We use D IST F LIP to attack a black-box model. Introduction Adversarial examples (Goodfellow et al., 2014) have gained tremendous attention recently, as they elucidate model limitations, and expose vulnerabilities in deployed systems. Work in natural language processing (NLP) either (a) used simple heuristics for generating adversarial examples (Jia and Liang, 2017; Belinkov and Bisk, 2017; Iyyer et al., 2018), or (b) assumed white-box access, where the attacker has access to gradients of the model with respect to the input (Feng et al., 2018; Ebrahimi et al., 2018). In this approach, adversarial examples are constructed through an optimization process that uses gradient descent to search for input examples that maximally change the predictions of a model. However, developing attacks with only black-box access to a model (no access to gradients) is still under-explored in NLP. ∗ Equal contribution Inspired by work in computer vision (Papernot et al., 20"
N19-1139,P18-2006,0,0.0784981,"IST F LIP, to emulate the white-box attack. (B) We use D IST F LIP to attack a black-box model. Introduction Adversarial examples (Goodfellow et al., 2014) have gained tremendous attention recently, as they elucidate model limitations, and expose vulnerabilities in deployed systems. Work in natural language processing (NLP) either (a) used simple heuristics for generating adversarial examples (Jia and Liang, 2017; Belinkov and Bisk, 2017; Iyyer et al., 2018), or (b) assumed white-box access, where the attacker has access to gradients of the model with respect to the input (Feng et al., 2018; Ebrahimi et al., 2018). In this approach, adversarial examples are constructed through an optimization process that uses gradient descent to search for input examples that maximally change the predictions of a model. However, developing attacks with only black-box access to a model (no access to gradients) is still under-explored in NLP. ∗ Equal contribution Inspired by work in computer vision (Papernot et al., 2016; Liu et al., 2017), we show in this work that a neural network can learn to emulate the optimization process of a white-box attack and generalize well to new examples. Figure 1 gives a high-level overvi"
N19-1139,P18-1079,0,0.016586,"of our generated sentences are misclassified by the API, while humans agree that the sentences are toxic. Our code can be downloaded from http:// github.com/orgoro/white-2-black. 2 Background Adversarial examples have been extensively used recently in NLP for probing and understanding neural models (Jia and Liang, 2017; Weber et al., 2018). Methods for generating such examples include adding random or heuristically constructed noise (Belinkov and Bisk, 2017; Rodriguez and Rojas-Galeano, 2018; Gao et al., 2018), meaningpreserving modifications that change the surface form (Iyyer et al., 2018; Ribeiro et al., 2018), and human-in-the-loop generation (Wallace et al., 2018). A weakness of such models is that they do not directly try to modify the prediction of a model, which can reduce efficacy (Kurakin et al., 2016). In the white-box setting, Feng et al. (2018) have changed the meaning of an input without changing model output using access to gradients, and Ebrahimi et al. (2018) proposed H OT F LIP, the aforementioned white-box attack that we emulate for flipping input characters. In computer vision, Papernot et al. (2016) and Liu et al. (2017) have shown that adversarial examples generated by a white-bo"
N19-1139,P18-3018,0,0.0147851,"while humans agree that the sentences are toxic. Our code can be downloaded from http:// github.com/orgoro/white-2-black. 2 Background Adversarial examples have been extensively used recently in NLP for probing and understanding neural models (Jia and Liang, 2017; Weber et al., 2018). Methods for generating such examples include adding random or heuristically constructed noise (Belinkov and Bisk, 2017; Rodriguez and Rojas-Galeano, 2018; Gao et al., 2018), meaningpreserving modifications that change the surface form (Iyyer et al., 2018; Ribeiro et al., 2018), and human-in-the-loop generation (Wallace et al., 2018). A weakness of such models is that they do not directly try to modify the prediction of a model, which can reduce efficacy (Kurakin et al., 2016). In the white-box setting, Feng et al. (2018) have changed the meaning of an input without changing model output using access to gradients, and Ebrahimi et al. (2018) proposed H OT F LIP, the aforementioned white-box attack that we emulate for flipping input characters. In computer vision, Papernot et al. (2016) and Liu et al. (2017) have shown that adversarial examples generated by a white-box model can be helpful for a black-box attack. Generating"
N19-1139,W18-1004,0,0.0555845,"Missing"
N19-1193,P09-1010,0,0.640489,"ams represented by the search state. Moreover, we search not in the space of programs but in a more compressed state of program executions, augmented with recent entities and actions. On the SCONE dataset, we show that our algorithm dramatically improves performance on all three domains compared to standard beam search and other baselines. 1 Introduction Training models that can understand natural language instructions and execute them in a realworld environment is of paramount importance for communicating with virtual assistants and robots, and therefore has attracted considerable attention (Branavan et al., 2009; Vogel and Jurafsky, 2010; Chen and Mooney, 2011). A prominent approach is to cast the problem as semantic parsing, where instructions are mapped to a high-level programming language (Artzi and Zettlemoyer, 2013; Long et al., 2016; Guu et al., 2017). Because annotating programs at scale is impractical, it is desirable to train a model from instructions, an initial world state, and a target world state only, letting the program itself be a latent variable. Learning from such weak supervision results in a difficult search problem at training time. The model must search for a program that when e"
N19-1193,N18-1177,0,0.0476981,"Missing"
N19-1193,P18-1168,1,0.832145,"ork employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but recent success of sequence-to-sequence models (Sutskever et al., 2014) shifted most of the burden to learning. Search is often performed simply using beam search, where program tokens are emitted from left-to-right, or program trees are generated top-down (Krishnamurthy et al., 2017; Yin and Neubig, 2017; Cheng et al., 2017; Rabinovich et al., 2017) or bottom-up (Liang et al., 2017; Guu et al., 2017; Goldman et al., 2018). Nevertheless, when instructions are long and complex and reward is sparse, the model may never find enough correct programs, and training will fail. In this paper, we propose a novel search algorithm for mapping a sequence of natural language instructions to a program, which extends the standard beam-search in two ways. First, we capitalize on the target world state being available at training time and train a critic network that given the language instructions, current world state, and target world state estimates the expected future reward for each search state. In contrast to traditional"
N19-1193,P17-1097,0,0.090555,"performance on all three domains compared to standard beam search and other baselines. 1 Introduction Training models that can understand natural language instructions and execute them in a realworld environment is of paramount importance for communicating with virtual assistants and robots, and therefore has attracted considerable attention (Branavan et al., 2009; Vogel and Jurafsky, 2010; Chen and Mooney, 2011). A prominent approach is to cast the problem as semantic parsing, where instructions are mapped to a high-level programming language (Artzi and Zettlemoyer, 2013; Long et al., 2016; Guu et al., 2017). Because annotating programs at scale is impractical, it is desirable to train a model from instructions, an initial world state, and a target world state only, letting the program itself be a latent variable. Learning from such weak supervision results in a difficult search problem at training time. The model must search for a program that when executed leads to the correct target state. Early work employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but rece"
N19-1193,P17-1167,0,0.0246324,"Missing"
N19-1193,N03-1016,0,0.0706726,"over, intermediate evaluations were used to compute gradient updates, rather than for guiding search. Guiding search with both policy and value networks was done in Monte-Carlo Tree Search (MCTS) for tasks with a sparse reward (Silver et al., 2017; T. A. and and Barber, 2017; Shen et al., 2018). In MCTS, value network evaluations are refined with backup updates to improve policy scores. In this work, we gain this advantage by using the target denotation. The use of an actor and a critic is also reminiscent of A∗ where states are scored by past cost and an admissible heuristic for future cost (Klein and Manning, 2003; Pauls and Klein, 2009; lee et al., 2016). In semantic parsing, Misra et al. (2018) recently proposed a critic distribution to improve the policy, which is based on 8 Conclusions In this work, we propose a new training algorithm for mapping instructions to programs given denotation supervision only. Our algorithm exploits the denotation at training time to train a critic network used to rank search states on the beam, and performs search in a compact execution space rather than in the space of programs. We evaluated on three different domains from SCONE, and found that it dramatically improve"
N19-1193,D17-1160,0,0.0616823,"s in a difficult search problem at training time. The model must search for a program that when executed leads to the correct target state. Early work employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but recent success of sequence-to-sequence models (Sutskever et al., 2014) shifted most of the burden to learning. Search is often performed simply using beam search, where program tokens are emitted from left-to-right, or program trees are generated top-down (Krishnamurthy et al., 2017; Yin and Neubig, 2017; Cheng et al., 2017; Rabinovich et al., 2017) or bottom-up (Liang et al., 2017; Guu et al., 2017; Goldman et al., 2018). Nevertheless, when instructions are long and complex and reward is sparse, the model may never find enough correct programs, and training will fail. In this paper, we propose a novel search algorithm for mapping a sequence of natural language instructions to a program, which extends the standard beam-search in two ways. First, we capitalize on the target world state being available at training time and train a critic network that given the language ins"
N19-1193,P17-1005,0,0.0164851,"The model must search for a program that when executed leads to the correct target state. Early work employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but recent success of sequence-to-sequence models (Sutskever et al., 2014) shifted most of the burden to learning. Search is often performed simply using beam search, where program tokens are emitted from left-to-right, or program trees are generated top-down (Krishnamurthy et al., 2017; Yin and Neubig, 2017; Cheng et al., 2017; Rabinovich et al., 2017) or bottom-up (Liang et al., 2017; Guu et al., 2017; Goldman et al., 2018). Nevertheless, when instructions are long and complex and reward is sparse, the model may never find enough correct programs, and training will fail. In this paper, we propose a novel search algorithm for mapping a sequence of natural language instructions to a program, which extends the standard beam-search in two ways. First, we capitalize on the target world state being available at training time and train a critic network that given the language instructions, current world state, and target"
N19-1193,D12-1069,0,0.224334,"level programming language (Artzi and Zettlemoyer, 2013; Long et al., 2016; Guu et al., 2017). Because annotating programs at scale is impractical, it is desirable to train a model from instructions, an initial world state, and a target world state only, letting the program itself be a latent variable. Learning from such weak supervision results in a difficult search problem at training time. The model must search for a program that when executed leads to the correct target state. Early work employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but recent success of sequence-to-sequence models (Sutskever et al., 2014) shifted most of the burden to learning. Search is often performed simply using beam search, where program tokens are emitted from left-to-right, or program trees are generated top-down (Krishnamurthy et al., 2017; Yin and Neubig, 2017; Cheng et al., 2017; Rabinovich et al., 2017) or bottom-up (Liang et al., 2017; Guu et al., 2017; Goldman et al., 2018). Nevertheless, when instructions are long and complex and reward is sparse, the model may never find enough correct p"
N19-1193,W10-2903,0,0.209449,"where instructions are mapped to a high-level programming language (Artzi and Zettlemoyer, 2013; Long et al., 2016; Guu et al., 2017). Because annotating programs at scale is impractical, it is desirable to train a model from instructions, an initial world state, and a target world state only, letting the program itself be a latent variable. Learning from such weak supervision results in a difficult search problem at training time. The model must search for a program that when executed leads to the correct target state. Early work employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but recent success of sequence-to-sequence models (Sutskever et al., 2014) shifted most of the burden to learning. Search is often performed simply using beam search, where program tokens are emitted from left-to-right, or program trees are generated top-down (Krishnamurthy et al., 2017; Yin and Neubig, 2017; Cheng et al., 2017; Rabinovich et al., 2017) or bottom-up (Liang et al., 2017; Guu et al., 2017; Goldman et al., 2018). Nevertheless, when instructions are long and complex and rewar"
N19-1193,D13-1161,0,0.024128,"0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 All States High Reward States 0 5000 10000 15000 20000 25000 30000 35000 40000 Train step Figure 6: The difference between the prediction of the value network and the expected reward (estimated from the discovered paths) during training. We report the average difference for all of the states (blue) and for the high reward states only (&gt; 0.7, orange). The results are averaged over 6 runs with different random seeds (best viewed in color). 7 Related Work prior domain knowledge (that is not learned). Training from denotations has been extensively investigated (Kwiatkowski et al., 2013; Pasupat and Liang, 2015; Bisk et al., 2016), with a recent emphasis on neural models (Neelakantan et al., 2016; Krishnamurthy et al., 2017). Improving beam search has been investigated by proposing specialized objectives (Wiseman and Rush, 2016), stopping criteria (Yang et al., 2018), and using continuous relaxations (Goyal et al., 2018). Bahdanau et al. (2017) and Suhr and Artzi (2018) proposed ways to evaluate intermediate predictions from a sparse reward signal. Bahdanau et al. (2017) used a critic network to estimate expected BLEU in translation, while Suhr and Artzi (2018) used edit-dis"
N19-1193,H94-1010,0,0.663329,"cution space rather than in the space of programs. We evaluated on three different domains from SCONE, and found that it dramatically improves performance compared to strong baselines across all domains. VBS I X is applicable to any task that supports graph-search exploration. Specifically, for tasks that can be formulated as an MDP with a deterministic transition function, which allow efficient execution of multiple partial trajectories. Those tasks include a wide range of instruction mapping (Branavan et al., 2009; Vogel and Jurafsky, 2010; Anderson et al., 2018) and semantic parsing tasks (Dahl et al., 1994; Iyyer et al., 2017; Yu et al., 2018). Therefore, evaluating VBS I X on other domains is a natural next step for our research. Acknowledgments We thank the anonymous reviewers for their constructive feedback. This work was completed in fulfillment for the M.Sc degree of the first author. This research was partially supported by The Israel Science Foundation grant 942/16, the Blavatnik Computer Science Research Fund, and The Yandex Initiative for Machine Learning. 1950 References P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S¨underhauf, I. Reid, S. Gould, and A. van den Hengel. 2018."
N19-1193,D16-1262,0,0.0181639,"ute gradient updates, rather than for guiding search. Guiding search with both policy and value networks was done in Monte-Carlo Tree Search (MCTS) for tasks with a sparse reward (Silver et al., 2017; T. A. and and Barber, 2017; Shen et al., 2018). In MCTS, value network evaluations are refined with backup updates to improve policy scores. In this work, we gain this advantage by using the target denotation. The use of an actor and a critic is also reminiscent of A∗ where states are scored by past cost and an admissible heuristic for future cost (Klein and Manning, 2003; Pauls and Klein, 2009; lee et al., 2016). In semantic parsing, Misra et al. (2018) recently proposed a critic distribution to improve the policy, which is based on 8 Conclusions In this work, we propose a new training algorithm for mapping instructions to programs given denotation supervision only. Our algorithm exploits the denotation at training time to train a critic network used to rank search states on the beam, and performs search in a compact execution space rather than in the space of programs. We evaluated on three different domains from SCONE, and found that it dramatically improves performance compared to strong baselines"
N19-1193,P17-1003,1,0.839658,"s to the correct target state. Early work employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but recent success of sequence-to-sequence models (Sutskever et al., 2014) shifted most of the burden to learning. Search is often performed simply using beam search, where program tokens are emitted from left-to-right, or program trees are generated top-down (Krishnamurthy et al., 2017; Yin and Neubig, 2017; Cheng et al., 2017; Rabinovich et al., 2017) or bottom-up (Liang et al., 2017; Guu et al., 2017; Goldman et al., 2018). Nevertheless, when instructions are long and complex and reward is sparse, the model may never find enough correct programs, and training will fail. In this paper, we propose a novel search algorithm for mapping a sequence of natural language instructions to a program, which extends the standard beam-search in two ways. First, we capitalize on the target world state being available at training time and train a critic network that given the language instructions, current world state, and target world state estimates the expected future reward for each"
N19-1193,P11-1060,0,0.0237684,"re mapped to a high-level programming language (Artzi and Zettlemoyer, 2013; Long et al., 2016; Guu et al., 2017). Because annotating programs at scale is impractical, it is desirable to train a model from instructions, an initial world state, and a target world state only, letting the program itself be a latent variable. Learning from such weak supervision results in a difficult search problem at training time. The model must search for a program that when executed leads to the correct target state. Early work employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but recent success of sequence-to-sequence models (Sutskever et al., 2014) shifted most of the burden to learning. Search is often performed simply using beam search, where program tokens are emitted from left-to-right, or program trees are generated top-down (Krishnamurthy et al., 2017; Yin and Neubig, 2017; Cheng et al., 2017; Rabinovich et al., 2017) or bottom-up (Liang et al., 2017; Guu et al., 2017; Goldman et al., 2018). Nevertheless, when instructions are long and complex and reward is sparse, the mod"
N19-1193,P16-1138,0,0.637988,"amatically improves performance on all three domains compared to standard beam search and other baselines. 1 Introduction Training models that can understand natural language instructions and execute them in a realworld environment is of paramount importance for communicating with virtual assistants and robots, and therefore has attracted considerable attention (Branavan et al., 2009; Vogel and Jurafsky, 2010; Chen and Mooney, 2011). A prominent approach is to cast the problem as semantic parsing, where instructions are mapped to a high-level programming language (Artzi and Zettlemoyer, 2013; Long et al., 2016; Guu et al., 2017). Because annotating programs at scale is impractical, it is desirable to train a model from instructions, an initial world state, and a target world state only, letting the program itself be a latent variable. Learning from such weak supervision results in a difficult search problem at training time. The model must search for a program that when executed leads to the correct target state. Early work employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoy"
N19-1193,D18-1266,0,0.365591,"iding search. Guiding search with both policy and value networks was done in Monte-Carlo Tree Search (MCTS) for tasks with a sparse reward (Silver et al., 2017; T. A. and and Barber, 2017; Shen et al., 2018). In MCTS, value network evaluations are refined with backup updates to improve policy scores. In this work, we gain this advantage by using the target denotation. The use of an actor and a critic is also reminiscent of A∗ where states are scored by past cost and an admissible heuristic for future cost (Klein and Manning, 2003; Pauls and Klein, 2009; lee et al., 2016). In semantic parsing, Misra et al. (2018) recently proposed a critic distribution to improve the policy, which is based on 8 Conclusions In this work, we propose a new training algorithm for mapping instructions to programs given denotation supervision only. Our algorithm exploits the denotation at training time to train a critic network used to rank search states on the beam, and performs search in a compact execution space rather than in the space of programs. We evaluated on three different domains from SCONE, and found that it dramatically improves performance compared to strong baselines across all domains. VBS I X is applicable"
N19-1193,P10-1083,0,0.229924,"search state. Moreover, we search not in the space of programs but in a more compressed state of program executions, augmented with recent entities and actions. On the SCONE dataset, we show that our algorithm dramatically improves performance on all three domains compared to standard beam search and other baselines. 1 Introduction Training models that can understand natural language instructions and execute them in a realworld environment is of paramount importance for communicating with virtual assistants and robots, and therefore has attracted considerable attention (Branavan et al., 2009; Vogel and Jurafsky, 2010; Chen and Mooney, 2011). A prominent approach is to cast the problem as semantic parsing, where instructions are mapped to a high-level programming language (Artzi and Zettlemoyer, 2013; Long et al., 2016; Guu et al., 2017). Because annotating programs at scale is impractical, it is desirable to train a model from instructions, an initial world state, and a target world state only, letting the program itself be a latent variable. Learning from such weak supervision results in a difficult search problem at training time. The model must search for a program that when executed leads to the corre"
N19-1193,P15-1142,0,0.0750724,"2 0.1 0.0 All States High Reward States 0 5000 10000 15000 20000 25000 30000 35000 40000 Train step Figure 6: The difference between the prediction of the value network and the expected reward (estimated from the discovered paths) during training. We report the average difference for all of the states (blue) and for the high reward states only (&gt; 0.7, orange). The results are averaged over 6 runs with different random seeds (best viewed in color). 7 Related Work prior domain knowledge (that is not learned). Training from denotations has been extensively investigated (Kwiatkowski et al., 2013; Pasupat and Liang, 2015; Bisk et al., 2016), with a recent emphasis on neural models (Neelakantan et al., 2016; Krishnamurthy et al., 2017). Improving beam search has been investigated by proposing specialized objectives (Wiseman and Rush, 2016), stopping criteria (Yang et al., 2018), and using continuous relaxations (Goyal et al., 2018). Bahdanau et al. (2017) and Suhr and Artzi (2018) proposed ways to evaluate intermediate predictions from a sparse reward signal. Bahdanau et al. (2017) used a critic network to estimate expected BLEU in translation, while Suhr and Artzi (2018) used edit-distance between the current"
N19-1193,P16-1003,0,0.0129017,"a different program prefix, and the path’s final state represents its execution result. Program search can therefore be reduced to execution search: given an example (c, u, y) and a model πθ , we can use πθ to explore in execution space, discover correct terminal states, i.e. states corresponding to correct full programs, and extract paths leading to those states. As the number of paths may be exponential in the size of the graph, we can use beam-search to extract the most probable correct programs (according to the model) in the discovered graph. Our approach is similar to the DPD algorithm (Pasupat and Liang, 2016), where CKYstyle search is performed in denotation space, followed by search in a pruned space of programs. However, DPD was used without learning, and the search was not guided by a trained model, which is a major part of our algorithm. 4.2 Value-based Beam Search in Execution Space We propose Value-based Beam Search in eXecution space (VBS I X), a variant of beam 1945 Algorithm 1 Program Search with VBS I X 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: function P ROGRAM S EARCH(c, u, y, πθ , Vφ ) G, T ← VBS I X(c, u, y, πθ , Vφ ) Z ← Find paths in G t"
N19-1193,P09-1108,0,0.0392076,"tions were used to compute gradient updates, rather than for guiding search. Guiding search with both policy and value networks was done in Monte-Carlo Tree Search (MCTS) for tasks with a sparse reward (Silver et al., 2017; T. A. and and Barber, 2017; Shen et al., 2018). In MCTS, value network evaluations are refined with backup updates to improve policy scores. In this work, we gain this advantage by using the target denotation. The use of an actor and a critic is also reminiscent of A∗ where states are scored by past cost and an admissible heuristic for future cost (Klein and Manning, 2003; Pauls and Klein, 2009; lee et al., 2016). In semantic parsing, Misra et al. (2018) recently proposed a critic distribution to improve the policy, which is based on 8 Conclusions In this work, we propose a new training algorithm for mapping instructions to programs given denotation supervision only. Our algorithm exploits the denotation at training time to train a critic network used to rank search states on the beam, and performs search in a compact execution space rather than in the space of programs. We evaluated on three different domains from SCONE, and found that it dramatically improves performance compared"
N19-1193,D14-1162,0,0.0832546,"Missing"
N19-1193,P17-1105,0,0.0233292,"h for a program that when executed leads to the correct target state. Early work employed lexicons and grammars to constrain the search space (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but recent success of sequence-to-sequence models (Sutskever et al., 2014) shifted most of the burden to learning. Search is often performed simply using beam search, where program tokens are emitted from left-to-right, or program trees are generated top-down (Krishnamurthy et al., 2017; Yin and Neubig, 2017; Cheng et al., 2017; Rabinovich et al., 2017) or bottom-up (Liang et al., 2017; Guu et al., 2017; Goldman et al., 2018). Nevertheless, when instructions are long and complex and reward is sparse, the model may never find enough correct programs, and training will fail. In this paper, we propose a novel search algorithm for mapping a sequence of natural language instructions to a program, which extends the standard beam-search in two ways. First, we capitalize on the target world state being available at training time and train a critic network that given the language instructions, current world state, and target world state estimates the"
N19-1193,P18-1193,0,0.116948,"ence of natural language instructions, and y is the target state of the environment after following the instructions, which we refer to as denotation. The model is trained to map the instructions u to a program z such that executing z in the context c results in the denotation y, which we denote by JzKc = y. Thus, the program z is a latent variable we must search for at both training and test time. When the sequence of instructions is long, search becomes hard, particularly in the early stages of training. Recent work tackled the training problem using variants of reinforcement learning (RL) (Suhr and Artzi, 2018; Liang et al., 2018) or maximum marginal likelihood (MML) (Guu et al., 2017; Goldman et al., 2018). We now briefly describe MML training, which we base our training procedure on, and outperformed RL in past work under comparable conditions (Guu et al., 2017). We denote by πθ (·) a model, parameterized by θ, that generates the program z token by token from left to right. The model πθ receives the context c, instructions u and previously predicted tokens z1...t−1 , and returns a distribution over the next program token zt . The probability of a program Qt prefix is defined to be: pθ (z1...t |u,"
N19-1193,D16-1137,0,0.0182165,"d paths) during training. We report the average difference for all of the states (blue) and for the high reward states only (&gt; 0.7, orange). The results are averaged over 6 runs with different random seeds (best viewed in color). 7 Related Work prior domain knowledge (that is not learned). Training from denotations has been extensively investigated (Kwiatkowski et al., 2013; Pasupat and Liang, 2015; Bisk et al., 2016), with a recent emphasis on neural models (Neelakantan et al., 2016; Krishnamurthy et al., 2017). Improving beam search has been investigated by proposing specialized objectives (Wiseman and Rush, 2016), stopping criteria (Yang et al., 2018), and using continuous relaxations (Goyal et al., 2018). Bahdanau et al. (2017) and Suhr and Artzi (2018) proposed ways to evaluate intermediate predictions from a sparse reward signal. Bahdanau et al. (2017) used a critic network to estimate expected BLEU in translation, while Suhr and Artzi (2018) used edit-distance between the current world and the goal for SCONE. But, in those works stronger supervision was assumed: Bahdanau et al. (2017) utilized the gold sequences, and Suhr and Artzi (2018) used intermediate worlds states. Moreover, intermediate eva"
N19-1233,P18-1008,0,0.0235404,"to approximate the distribution of text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than stateof-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation. 1 Introduction Neural networks have revolutionized the field of text generation, in machine translation (Sutskever et al., 2014; Neubig, 2017; Luong et al., 2015; Chen et al., 2018), summarization (See et al., 2017), image captioning (You et al., 2016) and many other applications (Goldberg, 2017). Traditionally, text generation models are trained by going over a gold sequence of symbols (characters or words) from left-to-right, and maximizing the probability of the next symbol given the history, namely, a language modeling (LM) objective. A commonly discussed drawback of such LM-based text generation is exposure bias (Ranzato et al., 2015): during training, the model predicts the next token conditioned on the ground truth history, while at test time prediction is based o"
N19-1233,P02-1040,0,0.114519,"not guarantee an improvement in an extrinsic downstream task that uses a language model. However, perplexity often correlates with extrinsic measures (Jurafsky and Martin, 2018), and is the de-facto metric for evaluating the quality of language models today. GAN-based Text Generation Evaluation. By definition, a text GAN outputs a discrete sequence of symbols rather than a probability distribution. As a result, LM metrics cannot be applied to evaluate the generated text. Consequently, other metrics have been proposed: • N-gram overlap: (Yu et al., 2017; Press et al., 2017): Inspired by BLEU (Papineni et al., 2002), this measures whether n-grams generated by the model appear in a held-out corpus. A major drawback is that this metric favors conservative models that always generate very common text (e.g., “it is”). To mitigate this, selfBLEU has been proposed (Lu et al., 2018) as an additional metric, where overlap is measured between two independently sampled texts from the model. • LM score: The probability of generated text according to a pre-trained LM. This has the same problem of favoring conservative models. • Zhao et al. (2017) suggested an indirect score by training a LM on GAN-generated text, an"
N19-1233,W17-2629,0,0.367569,"ow et al., 2014) offer a solution for exposure bias. ∗ The authors contributed equally Originally introduced for images, GANs leverage a discriminator, which is trained to discriminate between real images and generated images via an adversarial loss. In such a framework, the generator is not directly exposed to the ground truth data, but instead learns to imitate it using global feedback from the discriminator. This has led to several attempts to use GANs for text generation, with a generator using either a recurrent neural network (RNN) (Yu et al., 2017; Guo et al., 2017; Press et al., 2017; Rajeswar et al., 2017), or a Convolutional Neural Network (CNN) (Gulrajani et al., 2017; Rajeswar et al., 2017). However, evaluating GANs is more difficult than evaluating LMs. While in language modeling, evaluation is based on the log-probability of a model on held-out text, this cannot be straightforwardly extended to GAN-based text generation, because the generator outputs discrete tokens, rather than a probability distribution. Currently, there is no single evaluation metric for GAN-based text generation, and existing metrics that are based on n-gram overlap are known to lack robustness and have low correlation"
N19-1233,P17-1099,0,0.0335636,"text generated by a GAN, which permits evaluating them with traditional probability-based LM metrics. We apply our approximation procedure on several GAN-based models and show that they currently perform substantially worse than stateof-the-art LMs. Our evaluation procedure promotes better understanding of the relation between GANs and LMs, and can accelerate progress in GAN-based text generation. 1 Introduction Neural networks have revolutionized the field of text generation, in machine translation (Sutskever et al., 2014; Neubig, 2017; Luong et al., 2015; Chen et al., 2018), summarization (See et al., 2017), image captioning (You et al., 2016) and many other applications (Goldberg, 2017). Traditionally, text generation models are trained by going over a gold sequence of symbols (characters or words) from left-to-right, and maximizing the probability of the next symbol given the history, namely, a language modeling (LM) objective. A commonly discussed drawback of such LM-based text generation is exposure bias (Ranzato et al., 2015): during training, the model predicts the next token conditioned on the ground truth history, while at test time prediction is based on predicted tokens, causing a trai"
N19-1348,P15-1153,0,0.028306,"that pretraining on D ISCO F USE substantially improves performance on W EB S PLIT when viewed as a sentence fusion task. 1 Figure 1: Example for two independent sentences, and their fusion. The modifications applied are pronominalization (blue) and connective insertion (red). Introduction Sentence fusion is the task of combining several independent sentences into a single coherent text (Barzilay and McKeown, 2005). Sentence fusion is important in many NLP applications, including retrieval-based dialogue (Song et al., 2018; Yan and Zhao, 2018), text summarization (Barzilay and McKeown, 2005; Bing et al., 2015) and question answering (Li et al., 2018; Marsi and Krahmer, 2005). Such systems retrieve multiple sentences from different sources, documents or paragraphs, and use them to construct a coherent text. ∗ † Work done during internship at Google AI. Work done at Google AI. Sentence fusion is challenging because it requires understanding the discourse semantics between the input sentences. Consider the example in Figure 1: a coherent fusion of the sentences requires understanding that the second sentence contrasts the first one, in order to insert the discourse connective “However”. In addition, t"
N19-1348,D18-1080,0,0.0749343,"Keown et al. (2010) introduced a human-generated corpus of 3,000 examples. Elsner and Santhanam (2011) extracted around 300 fusion examples from pre- and postediting news articles. Thadani and McKeown (2013) constructed 1,858 examples from summarization tasks. Such datasets are too small to train modern data-hungry neural models. Related to sentence fusion is its “inverse” task of sentence splitting. Collados (2013) automatically constructed a Spanish simplification dataset by splitting single sentences into several simpler ones. Recently, two larger datasets for text splitting were released (Botha et al., 2018; Narayan et al., 2017; Aharoni and Goldberg, 2018). However, using these datasets for the “mirror” task of sentence fusion is problematic. First, sentence splitting often involves removing content from the original sentence for simplification, and this content is impossible to recover in the fusion direction. Second, these datasets do not focus on discourse and thus prominent discourse phenomena may be missed. Last, our new dataset is more than an order of magnitude larger than the above sentence splitting datasets. Another related line of recent work focused on predicting discourse connectiv"
N19-1348,I17-2071,0,0.0293507,"fusion is challenging because it requires understanding the discourse semantics between the input sentences. Consider the example in Figure 1: a coherent fusion of the sentences requires understanding that the second sentence contrasts the first one, in order to insert the discourse connective “However”. In addition, the gender and syntactic role of the entity “Zeitler” needs to be inferred to insert the pronoun “he”. Prior work on sentence fusion (Barzilay and McKeown, 2005; Turner and Charniak, 2005; Filippova, 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013; Bing et al., 2015; Chali et al., 2017) utilized very small amounts of labeled data, which are insufficient to train modern neural models. In this work, we propose a method for automatically generating sentence fusion examples at scale from raw text corpora. To this end, we go over sentences and contiguous pairs of sentences in a corpus, and apply a set of manually-constructed rules, which identify the occurrence of prevalent fusion operations. The rules specify how to modify the sentences such that they are “unfused” into two independent sentences. E.g., in Figure 1 one rule will delete the discourse connective “However”, and anot"
N19-1348,L18-1260,1,0.863729,"Missing"
N19-1421,N18-2017,0,0.0840187,"Missing"
N19-1421,P17-1147,0,0.031819,"e was Simon when he heard the lawn mower?”, one can infer that the lawn mower is close to Simon, and that it is probably outdoors and situated at street level. This type of knowledge seems trivial for humans, but is still out of the reach of current natural language understanding (NLU) systems. The authors contributed equally Lo AtLo At Introduction ∗ At Work on Question Answering (QA) has mostly focused on answering factoid questions, where the answer can be found in a given context with little need for commonsense knowledge (Hermann et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017). Small benchmarks such as the Winograd Scheme Challenge (Levesque, 2011) and COPA (Roemmele et al., 2011), targeted common sense more directly, but have been difficult to collect at scale. Recently, efforts have been invested in developing large-scale datasets for commonsense reasoning. In SWAG (Zellers et al., 2018b), given a textual description of an event, a probable subsequent event needs to be inferred. However, it 4149 Proceedings of NAACL-HLT 2019, pages 4149–4158 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics has been quickly realized t"
N19-1421,P17-2097,0,0.0138164,"ned LM on the target task (Radford et al., 2018; Devlin et al., 2018). Investigations of commonsense datasets, and of natural language datasets more generally, have revealed the difficulty in creating benchmarks that measure the understanding of a program rather than its ability to take advantage of distributional biases, and to model the annotation process (Gururangan et al., 2018; Poliak et al., 2018). Annotation artifacts in the Story Cloze Test, for example, allow models to achieve high performance while only looking at the proposed endings and ignoring the stories (Schwartz et al., 2017; Cai et al., 2017). Thus, the development of benchmarks for common sense remains a difficult challenge. Researchers have also investigated question answering that utilizes common sense. Science questions often require common sense, and have recently received attention (Clark et al., 2018; Mihaylov et al., 2018; Ostermann et al., 2018); however, they also need specialized scientific knowledge. In contrast to these efforts, our work studies common sense without requiring additional information. SQUABU created a small handcurated test of common sense and science questions (Davis, 2016), which are difficult for cur"
N19-1421,D16-1264,0,0.0831656,"tions. For instance, given the question “Where was Simon when he heard the lawn mower?”, one can infer that the lawn mower is close to Simon, and that it is probably outdoors and situated at street level. This type of knowledge seems trivial for humans, but is still out of the reach of current natural language understanding (NLU) systems. The authors contributed equally Lo AtLo At Introduction ∗ At Work on Question Answering (QA) has mostly focused on answering factoid questions, where the answer can be found in a given context with little need for commonsense knowledge (Hermann et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017). Small benchmarks such as the Winograd Scheme Challenge (Levesque, 2011) and COPA (Roemmele et al., 2011), targeted common sense more directly, but have been difficult to collect at scale. Recently, efforts have been invested in developing large-scale datasets for commonsense reasoning. In SWAG (Zellers et al., 2018b), given a textual description of an event, a probable subsequent event needs to be inferred. However, it 4149 Proceedings of NAACL-HLT 2019, pages 4149–4158 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computation"
N19-1421,P11-2057,0,0.105689,"Missing"
N19-1421,D18-1260,0,0.125724,"dvantage of distributional biases, and to model the annotation process (Gururangan et al., 2018; Poliak et al., 2018). Annotation artifacts in the Story Cloze Test, for example, allow models to achieve high performance while only looking at the proposed endings and ignoring the stories (Schwartz et al., 2017; Cai et al., 2017). Thus, the development of benchmarks for common sense remains a difficult challenge. Researchers have also investigated question answering that utilizes common sense. Science questions often require common sense, and have recently received attention (Clark et al., 2018; Mihaylov et al., 2018; Ostermann et al., 2018); however, they also need specialized scientific knowledge. In contrast to these efforts, our work studies common sense without requiring additional information. SQUABU created a small handcurated test of common sense and science questions (Davis, 2016), which are difficult for current techniques to solve. In this work, we create similarly well-crafted questions but at a larger scale. 3 Dataset Generation Our goal is to develop a method for generating questions that can be easily answered by humans without context, and require commonsense knowledge. We generate multipl"
N19-1421,N16-1098,0,0.0329131,"hich of two alternatives best reflects a cause or effect relation to the premise. For both datasets, scalability is an issue when evaluating modern modeling approaches. 4150 With the recent adoption of crowdsourcing, several larger datasets have emerged, focusing on predicting relations between situations or events in natural language. JHU Ordinal Commonsense Inference requests a label from 1-5 for the plausibility that one situation entails another (Zhang et al., 2017). The Story Cloze Test (also referred to as ROC Stories) pits ground-truth endings to stories against implausible false ones (Mostafazadeh et al., 2016). Interpolating these approaches, Situations with Adversarial Generations (SWAG), asks models to choose the correct description of what happens next after an initial event (Zellers et al., 2018b). LM-based techniques achieve very high performance on the Story Cloze Test and SWAG by fine-tuning a pre-trained LM on the target task (Radford et al., 2018; Devlin et al., 2018). Investigations of commonsense datasets, and of natural language datasets more generally, have revealed the difficulty in creating benchmarks that measure the understanding of a program rather than its ability to take advanta"
N19-1421,L18-1564,0,0.0238509,"nal biases, and to model the annotation process (Gururangan et al., 2018; Poliak et al., 2018). Annotation artifacts in the Story Cloze Test, for example, allow models to achieve high performance while only looking at the proposed endings and ignoring the stories (Schwartz et al., 2017; Cai et al., 2017). Thus, the development of benchmarks for common sense remains a difficult challenge. Researchers have also investigated question answering that utilizes common sense. Science questions often require common sense, and have recently received attention (Clark et al., 2018; Mihaylov et al., 2018; Ostermann et al., 2018); however, they also need specialized scientific knowledge. In contrast to these efforts, our work studies common sense without requiring additional information. SQUABU created a small handcurated test of common sense and science questions (Davis, 2016), which are difficult for current techniques to solve. In this work, we create similarly well-crafted questions but at a larger scale. 3 Dataset Generation Our goal is to develop a method for generating questions that can be easily answered by humans without context, and require commonsense knowledge. We generate multiple-choice questions in a p"
N19-1421,D14-1162,0,0.0822894,"Missing"
N19-1421,K17-1004,0,0.0483245,"fine-tuning a pre-trained LM on the target task (Radford et al., 2018; Devlin et al., 2018). Investigations of commonsense datasets, and of natural language datasets more generally, have revealed the difficulty in creating benchmarks that measure the understanding of a program rather than its ability to take advantage of distributional biases, and to model the annotation process (Gururangan et al., 2018; Poliak et al., 2018). Annotation artifacts in the Story Cloze Test, for example, allow models to achieve high performance while only looking at the proposed endings and ignoring the stories (Schwartz et al., 2017; Cai et al., 2017). Thus, the development of benchmarks for common sense remains a difficult challenge. Researchers have also investigated question answering that utilizes common sense. Science questions often require common sense, and have recently received attention (Clark et al., 2018; Mihaylov et al., 2018; Ostermann et al., 2018); however, they also need specialized scientific knowledge. In contrast to these efforts, our work studies common sense without requiring additional information. SQUABU created a small handcurated test of common sense and science questions (Davis, 2016), which ar"
N19-1421,H89-1033,0,0.796975,"dels on C OMMONSENSE QA, showing that humans substantially outperform current models. The dataset can be downloaded from www. tau-nlp.org/commonsenseqa. The code for all our baselines is available at github. com/jonathanherzig/commonsenseqa. 2 Related Work Machine common sense, or the knowledge of and ability to reason about an open ended world, has long been acknowledged as a critical component for natural language understanding. Early work sought programs that could reason about an environment in natural language (McCarthy, 1959), or leverage a world-model for deeper language understanding (Winograd, 1972). Many commonsense representations and inference procedures have been explored (McCarthy and Hayes, 1969; Kowalski and Sergot, 1986) and large-scale commonsense knowledge-bases have been developed (Lenat, 1995; Speer et al., 2017). However, evaluating the degree of common sense possessed by a machine remains difficult. One important benchmark, the Winograd Schema Challenge (Levesque, 2011), asks models to correctly solve paired instances of coreference resolution. While the Winograd Schema Challenge remains a tough dataset, the difficulty of generating examples has led to only a small availabl"
N19-1421,D18-1009,0,0.745359,"ed equally Lo AtLo At Introduction ∗ At Work on Question Answering (QA) has mostly focused on answering factoid questions, where the answer can be found in a given context with little need for commonsense knowledge (Hermann et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017). Small benchmarks such as the Winograd Scheme Challenge (Levesque, 2011) and COPA (Roemmele et al., 2011), targeted common sense more directly, but have been difficult to collect at scale. Recently, efforts have been invested in developing large-scale datasets for commonsense reasoning. In SWAG (Zellers et al., 2018b), given a textual description of an event, a probable subsequent event needs to be inferred. However, it 4149 Proceedings of NAACL-HLT 2019, pages 4149–4158 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics has been quickly realized that models trained on large amounts of unlabeled data (Devlin et al., 2018) capture well this type of information and performance on SWAG is already at human level. VCR (Zellers et al., 2018a) is another very recent attempt that focuses on the visual aspects of common sense. Such new attempts highlight the breadth of"
N19-1421,N18-1202,0,0.0609154,"d is element-wise product. Then, the model predicts an answer score using a feed forward layer: hW2 + b2 . Average pre-trained embeddings and softmax are used to train the model. e ESIM We use ESIM, a strong NLI model (Chen et al., 2016). Similar to Zellers et al. (2018b), we change the output layer size to the number of candidate answers, and apply softmax to train with cross-entropy loss. f B I DAF++ A state-of-the-art RC model, that uses the retrieved Google web snippets (Section 3) as context. We augment B I DAF (Seo et al., 2016) with a self-attention layer and ELMo representa4154 tions (Peters et al., 2018; Huang et al., 2018). To adapt to the multiple-choice setting, we choose the answer with highest model probability. g G ENERATIVE P RE - TRAINED T RANS FORMER (GPT) Radford et al. (2018) proposed a method for adapting pre-trained LMs to perform a wide range of tasks. We applied their model to C OMMONSENSE QA by encoding each question and its candidate answers as a series of delimiterseparated sequences. For example, the question “If you needed a lamp to do your work, where would you put it?”, and the candidate answer “bedroom” would become “[start] If ... ? [sep] bedroom [end]”. The hidden re"
N19-1421,S18-2023,0,0.0618399,"Missing"
P10-1124,D07-1017,0,0.44882,"entailment that can be used for semantic inference (Budanitsky and Hirst, 2006). WordNet has also been exploited to automatically generate a training set for a hyponym classifier (Snow et al., 2005), and we make a similar use of WordNet in Section 5.1. Lexicographic resources are accurate but tend to have low coverage. Therefore, distributional similarity is used to learn broad-scale resources. Distributional similarity algorithms predict a semantic relation between two predicates by comparing the arguments with which they occur. Quite a few methods have been suggested (Lin and Pantel, 2001; Bhagat et al., 2007; Yates and Etzioni, 2009), which differ in terms of the specifics of the ways in which predicates are represented, the features that are extracted, and the function used to compute feature vector similarity. Details on such methods are given in Section 5.1. Global learning It is natural to describe entailment relations between predicates by a graph. Nodes represent predicates, and edges represent entailment between nodes. Nevertheless, using a graph for global learning of entailment between predicates has attracted little attention. Recently, Szpektor and Dagan (2009) presented the resource A"
P10-1124,J06-1003,0,0.00428957,"entailment rules between predicates: lexicographic resources and distributional similarity resources. Lexicographic 1220 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220–1229, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics resources are manually-prepared knowledge bases containing information about semantic relations between lexical items. WordNet (Fellbaum, 1998), by far the most widely used resource, specifies relations such as hyponymy, derivation, and entailment that can be used for semantic inference (Budanitsky and Hirst, 2006). WordNet has also been exploited to automatically generate a training set for a hyponym classifier (Snow et al., 2005), and we make a similar use of WordNet in Section 5.1. Lexicographic resources are accurate but tend to have low coverage. Therefore, distributional similarity is used to learn broad-scale resources. Distributional similarity algorithms predict a semantic relation between two predicates by comparing the arguments with which they occur. Quite a few methods have been suggested (Lin and Pantel, 2001; Bhagat et al., 2007; Yates and Etzioni, 2009), which differ in terms of the spec"
P10-1124,P08-2012,0,0.0635342,"traints, linear programming is a natural choice, enabling the use of state of the art optimization packages. We describe two formulations of integer linear programs that learn the edges: one maximizing a global score function, and another maximizing a global probability function. Let Iuv be an indicator denoting the event that node u entails node v. Our goal is to learn the edges E over a set of nodes V . We start by formulating the constraints and then the target functions. The first constraint is that the graph must respect transitivity. Our formulation is equivalent to the one suggested by Finkel and Manning (2008) in a coreference resolution task: ∀u,v,w∈V Iuv + Ivw − Iuw ≤ 1 In addition, for a few pairs of nodes we have strong evidence that one does not entail the other and so we add the constraint Iuv = 0. Combined with the constraint of transitivity this implies that there must be no path from u to v. This is done in the following two scenarios: (1) When two nodes u and v are identical except for a pair of words wu and wv , and wu is an antonym of wv , or a hypernym of wv at distance ≥ 2. (2) When two nodes u and v are transitive opposites, that is, if u = subj obj obj subj X ←−− w −−→ Y and v = X ←"
P10-1124,P09-1039,0,0.0332457,"that maximize the probability of the evidence while respecting the transitivity constraint. In this paper we tackle a similar problem of learning a transitive relation, but we use linear programming. A Linear Program (LP) is an optimization problem, where a linear function is minimized (or maximized) under linear constraints. If the variables are integers, the problem is termed an Integer Linear Program (ILP). Linear programming has attracted attention recently in several fields of NLP, such as semantic role labeling, summarization and parsing (Roth and tau Yih, 2005; Clarke and Lapata, 2008; Martins et al., 2009). In this paper we formulate the entailment graph learning problem as an Integer Linear Program, and find that this leads to an optimal solution with respect to the target function in our experiment. 3 Entailment Graph This section presents an entailment graph structure, which resembles the graph in (Szpektor and Dagan, 2009). The nodes of an entailment graph are propositional templates. A propositional template is a path in a dependency tree between two arguments of a common predicate1 (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Note that in a dependency parse, such a path passes throug"
P10-1124,I05-5011,0,0.192819,"Missing"
P10-1124,P05-1044,0,0.0101971,"aining set of positive (entailing) and negative (non-entailing) template pairs. Let T be the set of propositional templates extracted from the corpus. For each ti ∈ T with two variables and a single predicate word w, we extract from WordNet the set H of direct hypernyms and synonyms of w. For every h ∈ H, we generate a new template tj from ti by replacing w with h. If tj ∈ T , we consider (ti , tj ) to be a positive example. Negative examples are generated analogously, by looking at direct co-hyponyms of w instead of hypernyms and synonyms. This follows the notion of “contrastive estimation” (Smith and Eisner, 2005), since we generate negative examples that are semantically similar to positive examples and thus focus the classifier’s attention on identifying the boundary between the classes. Last, we filter training examples for which all features are zero, and sample an equal number of positive and negative examples (for which we compute similarity features), since classifiers tend to perform poorly on the minority class when trained on imbalanced data (Van Hulse et al., 2007; Nikulin, 2008). 5.2 Global learning of edges Once the entailment classifier is trained we learn the graph edges given its nodes."
P10-1124,P06-1101,0,0.243251,"Missing"
P10-1124,N07-1031,0,0.114355,"nectivity component2 of the graph, all nodes are synonymous. Moreover, if we merge every strong connectivity component to a single node, the graph becomes a Directed Acyclic Graph (DAG), and the graph nodes can be sorted and presented hierarchically. Next, we show an application that leverages this property. 4 Motivating Application In this section we propose an application that provides a hierarchical view of propositions extracted from a corpus, based on an entailment graph. Organizing information in large collections has been found to be useful for effective information access (Kaki, 2005; Stoica et al., 2007). It allows for easier data exploration, and provides a compact view of the underlying content. A simple form of structural presentation is by a single hierarchy, e.g. (Hofmann, 1999). A more complex approach is hierarchical faceted metadata, where a number of concept hierarchies are created, corresponding to different facets or dimensions (Stoica et al., 2007). Hierarchical faceted metadata categorizes concepts of a domain in several dimensions, but does not specify the relations between them. For example, in the health-care domain we might have facets for categories such as diseases and symp"
P10-1124,C08-1107,1,0.871124,"erence relation between two text fragments (when the rule is bidirectional this is known as paraphrasing). An important type of entailment rule refers to propositional templates, i.e., propositions comprising a predicate and arguments, possibly replaced by variables. The rule required for the previous example would be ‘X reduce Y → X affect Y’. Because facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task. This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g. (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008). Previous work has focused on learning each entailment rule in isolation. However, it is clear that there are interactions between rules. A prominent example is that entailment is a transitive relation, and thus the rules ‘X → Y ’ and ‘Y → Z’ imply the rule ‘X → Z’. In this paper we take advantage of these global interactions to improve entailment rule learning. First, we describe a structure termed an entailment graph that models entailment relations between propositional templates (Section 3). Next, we show that we can present propositions according to an entailment hierarchy derived from t"
P10-1124,W09-2504,1,0.834018,"ggested (Lin and Pantel, 2001; Bhagat et al., 2007; Yates and Etzioni, 2009), which differ in terms of the specifics of the ways in which predicates are represented, the features that are extracted, and the function used to compute feature vector similarity. Details on such methods are given in Section 5.1. Global learning It is natural to describe entailment relations between predicates by a graph. Nodes represent predicates, and edges represent entailment between nodes. Nevertheless, using a graph for global learning of entailment between predicates has attracted little attention. Recently, Szpektor and Dagan (2009) presented the resource Argument-mapped WordNet, providing entailment relations for predicates in WordNet. Their resource was built on top of WordNet, and makes simple use of WordNet’s global graph structure: new rules are suggested by transitively chaining graph edges, and verified against corpus statistics. The most similar work to ours is Snow et al.’s algorithm for taxonomy induction (2006). Snow et al.’s algorithm learns the hyponymy relation, under the constraint that it is a transitive relation. Their algorithm incrementally adds hyponyms to an existing taxonomy (WordNet), using a greed"
P10-1124,W04-3206,1,0.853995,"an (2008) suggested learning over templates with subj one variable (unary templates) such as X ←−− affect, and using them to estimate a score for binary templates. Feature representation The features of a template are some representation of the terms that instantiated the argument variables in a corpus. Two representations are used in our experiment (see Section 6). Another variant occurs when using binary templates: a template may be represented by a pair of feature vectors, one for each variable (Lin and Pantel, 2001), or by a single vector, where features represent pairs of instantiations (Szpektor et al., 2004; Yates and Etzioni, 2009). The former variant reduces sparsity problems, while Yates and Etzioni showed the latter is more informative and performs favorably on their data. Similarity function We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). The former is a symmetric measure and the latter is asymmetric. Therefore, information about the direction of entailment is provided by the BInc measure. We then generate for any (t1 , t2 ) features that are the 12 distributional similarity scores usin"
P10-1124,W07-1401,1,\N,Missing
P11-1062,P98-1013,0,0.107617,"Missing"
P11-1062,P10-2045,1,0.863712,"hs, resulting in improved coverage. 2 Background Most work on learning entailment rules between predicates considered each rule independently of others, using two sources of information: lexicographic resources and distributional similarity. Lexicographic resources are manually-prepared knowledge bases containing semantic information on predicates. A widely-used resource is WordNet (Fellbaum, 1998), where relations such as synonymy and hyponymy can be used to generate rules. Other resources include NomLex (Macleod et al., 1998; Szpektor and Dagan, 2009) and FrameNet (Baker and Lowe, 1998; Ben Aharon et al., 2010). Lexicographic resources are accurate but have 1 The resource can be downloaded from http://www.cs.tau.ac.il/˜jonatha6/homepage files/resources /ACL2011Resource.zip 611 low coverage. Distributional similarity algorithms use large corpora to learn broader resources by assuming that semantically similar predicates appear with similar arguments. These algorithms usually represent a predicate with one or more vectors and use some function to compute argument similarity. Distributional similarity algorithms differ in their feature representation: Some use a binary representation: each predicate is"
P11-1062,P10-1124,1,0.843876,"rule ‘X annex Y → X control Y’ helps recognize that the text ‘Japan annexed Okinawa’ answers the question ‘Which country controls Okinawa?’. Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010). Most past work took a “local learning” approach, learning each entailment rule independently of others. It is clear though, that there are global interactions between predicates. Notably, entailment is a transitive relation and so the rules A → B and B → C imply A → C. Recently, Berant et al. (2010) proposed a global graph optimization procedure that uses Integer Linear Programming (ILP) to find the best set of entailment rules under a transitivity constraint. Imposing this constraint raised two challenges. The first of ambiguity: transitivity does not always hold when predicates are ambiguous, e.g., X buy Y → X acquire Y and X acquire Y → X learn Y, but X buy Y 9 X learn Y since these two rules correspond to two different senses of acquire. The second challenge is scalability: ILP solvers do not scale well since ILP is an NP-complete problem. Berant et al. circumvented these issues by l"
P11-1062,D07-1017,0,0.179692,"9). This representation performs well, but suffers when data is sparse. The binary-DIRT representation deals with sparsity by representing a predicate with a pair of vectors, one for each argument (Lin and Pantel, 2001). Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). Different algorithms also differ in their similarity function. Some employ symmetric functions, geared towards paraphrasing (bi-directional entailment), while others choose directional measures more suited for entailment (Bhagat et al., 2007). In this paper, We employ several such functions, such as Lin (Lin and Pantel, 2001), and BInc (Szpektor and Dagan, 2008). Schoenmackers et al. (2010) recently used distributional similarity to learn rules between typed predicates, where the left-hand-side of the rule may contain more than a single predicate (horn clauses). In their work, they used Hearst-patterns (Hearst, 1992) to extract a set of 29 million (argument, type) pairs from a large web crawl. Then, they employed several filtering methods to clean this set and automatically produced a mapping of 1.1 million arguments into 156 type"
P11-1062,D10-1107,0,0.0492637,"tion Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensively used in NLP lately (Clarke and Lapata, 2008; Martins et al., 2009; Do and Roth, 2010). 3 Typed Entailment Graphs Given a set of typed predicates, entailment rules can only exist between predicates that share the same (unordered) pair of types (such as place and country)3 . Hence, every pair of types defines a graph that describes the entailment relations between predicates sharing those types (Figure 1). Next, we show how to represent entailment rules between typed predicates in a structure termed typed entailment graph, which will be the learning goal of our algorithm. A typed entailment graph is a directed graph where the nodes are typed predicates. A typed predicate is a tr"
P11-1062,P08-2012,0,0.113385,"city) and common in(disease,place)), and learn 30,000 rules between these predicates2 . In this paper we will learn entailment rules over the same data set, which was generously provided by 2 The rules and the mapping of arguments into types can be downloaded from http://www.cs.washington.edu/research/ sherlock-hornclauses/ Schoenmackers et al. As mentioned above, Berant et al. (2010) used global transitivity information to learn small entailment graphs. Transitivity was also used as an information source in other fields of NLP: Taxonomy Induction (Snow et al., 2006), Co-reference Resolution (Finkel and Manning, 2008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensively used in NLP lately (Clarke and Lapata, 2008; Mart"
P11-1062,C92-2082,0,0.136804,"lgorithms also differ in their similarity function. Some employ symmetric functions, geared towards paraphrasing (bi-directional entailment), while others choose directional measures more suited for entailment (Bhagat et al., 2007). In this paper, We employ several such functions, such as Lin (Lin and Pantel, 2001), and BInc (Szpektor and Dagan, 2008). Schoenmackers et al. (2010) recently used distributional similarity to learn rules between typed predicates, where the left-hand-side of the rule may contain more than a single predicate (horn clauses). In their work, they used Hearst-patterns (Hearst, 1992) to extract a set of 29 million (argument, type) pairs from a large web crawl. Then, they employed several filtering methods to clean this set and automatically produced a mapping of 1.1 million arguments into 156 types. Examples for (argument, type) pairs are (EXODUS, book), (CHINA, country) and (ASTHMA, disease). Schoenmackers et al. then utilized the types, the mapped arguments and tuples from TextRunner (Banko et al., 2007) to generate 10,672 typed predicates (such as conquer(country,city) and common in(disease,place)), and learn 30,000 rules between these predicates2 . In this paper we wi"
P11-1062,P09-1039,0,0.017584,"008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensively used in NLP lately (Clarke and Lapata, 2008; Martins et al., 2009; Do and Roth, 2010). 3 Typed Entailment Graphs Given a set of typed predicates, entailment rules can only exist between predicates that share the same (unordered) pair of types (such as place and country)3 . Hence, every pair of types defines a graph that describes the entailment relations between predicates sharing those types (Figure 1). Next, we show how to represent entailment rules between typed predicates in a structure termed typed entailment graph, which will be the learning goal of our algorithm. A typed entailment graph is a directed graph where the nodes are typed predicates. A typ"
P11-1062,P10-1031,0,0.0074745,"ailment rules over the same data set, which was generously provided by 2 The rules and the mapping of arguments into types can be downloaded from http://www.cs.washington.edu/research/ sherlock-hornclauses/ Schoenmackers et al. As mentioned above, Berant et al. (2010) used global transitivity information to learn small entailment graphs. Transitivity was also used as an information source in other fields of NLP: Taxonomy Induction (Snow et al., 2006), Co-reference Resolution (Finkel and Manning, 2008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensively used in NLP lately (Clarke and Lapata, 2008; Martins et al., 2009; Do and Roth, 2010). 3 Typed Entailment Graphs Given a set of typed predicates, entailment rules can"
P11-1062,W06-1616,0,0.0298882,"ACT ← ACT ∪ VIO 6: until |VIO |= 0 how typed entailment graphs benefit from decomposition given different prior values. From a more general perspective, this algorithm can be applied to any problem of learning a sparse transitive binary relation. Such problems include Co-reference Resolution (Finkel and Manning, 2008) and Temporal Information Extraction (Ling and Weld, 2010). Last, the algorithm can be easily parallelized by solving each component on a different core. 4.4 Incremental ILP Another solution for scaling ILP is to employ incremental ILP, which has been used in dependency parsing (Riedel and Clarke, 2006). The idea is that even if we omit the transitivity constraints, we still expect most transitivity constraints to be satisfied, given a good local entailment classifier. Thus, it makes sense to avoid specifying the constraints ahead of time, but rather add them when they are violated. This is formalized in Algorithm 2. Line 1 initializes an active set of constraints and a violated set of constraints (ACT;VIO). Line 3 applies the ILP solver with the active constraints. Lines 4 and 5 find the violated constraints and add them to the active constraints. The algorithm halts when no constraints are"
P11-1062,D10-1106,0,0.263522,"l jonatha6@post.tau.ac.il dagan@cs.biu.ac.il goldbej@eng.biu.ac.il Abstract Extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference. In this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates. We model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs. We apply the algorithm over a large data set of extracted predicate instances, from which a resource of typed entailment rules has been recently released (Schoenmackers et al., 2010). Our results show that using global transitivity information substantially improves performance over this resource and several baselines, and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs. 1 Introduction Generic approaches for applied semantic inference from text gained growing attention in recent years, particularly under the Textual Entailment (TE) framework (Dagan et al., 2009). TE is a generic paradigm for semantic inference, where the objective is to recognize whether a target meaning can be inferred from a given text. A crucial comp"
P11-1062,I05-5011,0,0.106093,"d from a given text. A crucial component of inference systems is extensive resources of entailment rules, also known as inference rules, i.e., rules that specify a directional inference relation between fragments of text. One important type of rule is rules that specify entailment relations between predicates and their arguments. For example, the rule ‘X annex Y → X control Y’ helps recognize that the text ‘Japan annexed Okinawa’ answers the question ‘Which country controls Okinawa?’. Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010). Most past work took a “local learning” approach, learning each entailment rule independently of others. It is clear though, that there are global interactions between predicates. Notably, entailment is a transitive relation and so the rules A → B and B → C imply A → C. Recently, Berant et al. (2010) proposed a global graph optimization procedure that uses Integer Linear Programming (ILP) to find the best set of entailment rules under a transitivity constraint. Imposing this constraint raised two challenges. The first of ambiguity: transi"
P11-1062,P06-1101,0,0.456438,"72 typed predicates (such as conquer(country,city) and common in(disease,place)), and learn 30,000 rules between these predicates2 . In this paper we will learn entailment rules over the same data set, which was generously provided by 2 The rules and the mapping of arguments into types can be downloaded from http://www.cs.washington.edu/research/ sherlock-hornclauses/ Schoenmackers et al. As mentioned above, Berant et al. (2010) used global transitivity information to learn small entailment graphs. Transitivity was also used as an information source in other fields of NLP: Taxonomy Induction (Snow et al., 2006), Co-reference Resolution (Finkel and Manning, 2008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensive"
P11-1062,C08-1107,1,0.814176,"stributional similarity algorithms differ in their feature representation: Some use a binary representation: each predicate is represented by one feature vector where each feature is a pair of arguments (Szpektor et al., 2004; Yates and Etzioni, 2009). This representation performs well, but suffers when data is sparse. The binary-DIRT representation deals with sparsity by representing a predicate with a pair of vectors, one for each argument (Lin and Pantel, 2001). Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). Different algorithms also differ in their similarity function. Some employ symmetric functions, geared towards paraphrasing (bi-directional entailment), while others choose directional measures more suited for entailment (Bhagat et al., 2007). In this paper, We employ several such functions, such as Lin (Lin and Pantel, 2001), and BInc (Szpektor and Dagan, 2008). Schoenmackers et al. (2010) recently used distributional similarity to learn rules between typed predicates, where the left-hand-side of the rule may contain more than a single predicate (horn clauses). In their work, they used Hear"
P11-1062,W09-2504,1,0.938877,"text. A crucial component of inference systems is extensive resources of entailment rules, also known as inference rules, i.e., rules that specify a directional inference relation between fragments of text. One important type of rule is rules that specify entailment relations between predicates and their arguments. For example, the rule ‘X annex Y → X control Y’ helps recognize that the text ‘Japan annexed Okinawa’ answers the question ‘Which country controls Okinawa?’. Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010). Most past work took a “local learning” approach, learning each entailment rule independently of others. It is clear though, that there are global interactions between predicates. Notably, entailment is a transitive relation and so the rules A → B and B → C imply A → C. Recently, Berant et al. (2010) proposed a global graph optimization procedure that uses Integer Linear Programming (ILP) to find the best set of entailment rules under a transitivity constraint. Imposing this constraint raised two challenges. The first of ambiguity: transitivity does not always hol"
P11-1062,W04-3206,1,0.657482,"ed from http://www.cs.tau.ac.il/˜jonatha6/homepage files/resources /ACL2011Resource.zip 611 low coverage. Distributional similarity algorithms use large corpora to learn broader resources by assuming that semantically similar predicates appear with similar arguments. These algorithms usually represent a predicate with one or more vectors and use some function to compute argument similarity. Distributional similarity algorithms differ in their feature representation: Some use a binary representation: each predicate is represented by one feature vector where each feature is a pair of arguments (Szpektor et al., 2004; Yates and Etzioni, 2009). This representation performs well, but suffers when data is sparse. The binary-DIRT representation deals with sparsity by representing a predicate with a pair of vectors, one for each argument (Lin and Pantel, 2001). Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). Different algorithms also differ in their similarity function. Some employ symmetric functions, geared towards paraphrasing (bi-directional entailment), while others choose directional measures more"
P11-1062,W03-1011,0,0.555143,"y score estimating whether p1 entails p2 . We compute 11 distributional similarity scores for each pair of predicates based on the arguments appearing in the extracted arguments. The first 6 scores are computed by trying all combinations of the similarity functions Lin and BInc with the feature representations unary, binary-DIRT and binary (see Section 2). The other 5 scores were provided by Schoenmackers et al. (2010) and include SR (Schoenmackers et al., 2010), LIME (McCreath and Sharma, 1997), M-estimate (Dzeroski and Brakto, 1992), the standard G-test and a simple implementation of Cover (Weeds and Weir, 2003). Overall, the rationale behind this representation is that combining various scores will yield a better classifier than each single measure. 3) Training We train over an equal number of positive and negative examples, as classifiers tend to perform poorly on the minority class when trained on imbalanced data (Van Hulse et al., 2007; Nikulin, 2008). 4.2 ILP formulation Once the classifier is trained, we would like to learn all edges (entailment rules) of each typed entailment graph. Given a set of predicates V and an entailment score function f : V × V → R derived from the classifier, we want"
P11-1062,C98-1013,0,\N,Missing
P12-1013,P10-2045,1,0.884125,"h may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob and Rambow, 2009; Ben Aharon et al., 2010), and others assumed that semantic relations between predicates can be deduced from their co-occurrence in a corpus via manually-constructed 118 patterns (Chklovski and Pantel, 2004). Recently, Berant et al. (2010; 2011) formulated the problem as the problem of learning global entailment graphs. In entailment graphs, nodes are predicates (e.g., ‘X attack Y’) and edges represent entailment rules between them (‘X invade Y → X attack Y’). For every pair of predicates i, j, an entailment score wij was learned by training a classifier over distributional similarity features. A positive wij indicate"
P12-1013,P10-1124,1,0.934057,"prominent generic paradigm for textual inference is Textual Entailment (TUE) (Dagan et al., 2009). In TUE, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text. For example, the text “Cyprus was invaded by the Ottoman Empire in 1571” implies the hypothesis “The Ottomans attacked Cyprus”. Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran Textual entailment is inherently a transitive relation , that is, the rules ‘x → y’ and ‘y → z’ imply the rule ‘x → z’. Accordingly, Berant et al. (2010) formulated the problem of learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules that respect transitivity. Since finding the optimal set of edges respecting transitivity is NP-hard, they employed Integer Linear Programming (ILP) to find the exact solution. Indeed, they showed that applying global transitivity constraints improves rule learning comparing to methods that ignore graph structure. More recently, Berant et al. (Berant et al., 2011) introduced a more efficient exact algorithm, which decomposes the graph into connec"
P12-1013,P11-1062,1,0.797113,"ation , that is, the rules ‘x → y’ and ‘y → z’ imply the rule ‘x → z’. Accordingly, Berant et al. (2010) formulated the problem of learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules that respect transitivity. Since finding the optimal set of edges respecting transitivity is NP-hard, they employed Integer Linear Programming (ILP) to find the exact solution. Indeed, they showed that applying global transitivity constraints improves rule learning comparing to methods that ignore graph structure. More recently, Berant et al. (Berant et al., 2011) introduced a more efficient exact algorithm, which decomposes the graph into connected components and then applies an ILP solver over each component. Despite this progress, finding the exact solution remains NP-hard – the authors themselves report they were unable to solve some graphs of rather moderate size and that the coverage of their method is limited. Thus, scaling their algorithm to data sets with tens of thousands of predicates (e.g., the extractions of Fader et al. (2011)) is unlikely. 117 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages"
P12-1013,W04-3205,0,0.194733,"rs and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob and Rambow, 2009; Ben Aharon et al., 2010), and others assumed that semantic relations between predicates can be deduced from their co-occurrence in a corpus via manually-constructed 118 patterns (Chklovski and Pantel, 2004). Recently, Berant et al. (2010; 2011) formulated the problem as the problem of learning global entailment graphs. In entailment graphs, nodes are predicates (e.g., ‘X attack Y’) and edges represent entailment rules between them (‘X invade Y → X attack Y’). For every pair of predicates i, j, an entailment score wij was learned by training a classifier over distributional similarity features. A positive wij indicated that the classifier believes i → j and a negative wij indicated that the classifier believes i 9 j. Given the graph nodes V (corresponding to the predicates) and the weighting func"
P12-1013,D10-1107,0,0.0548624,"nd applies an ILP solver on each component separately using a cutting-plane procedure (Riedel and Clarke, 2006). Although this method is exact and improves scalability, it does not guarantee an efficient solution. When the graph does not decompose into sufficiently small components, and the weights generate many violations of transitivity, solving Max-Trans-Graph becomes intractable. To address this problem, we present in this paper a method for approximating the optimal set of edges within each component and show that it is much more efficient and scalable both theoretically and empirically. Do and Roth (2010) suggested a method for a related task of learning taxonomic relations between terms. Given a pair of terms, a small graph is constructed and constraints are imposed on the graph structure. Their work, however, is geared towards scenarios where relations are determined on-the-fly for a given pair of terms and no global knowledge base is explicitly constructed. Thus, their method easily produces solutions where global constraints, such as transitivity, are violated. Another approximation method that violates transitivity constraints is LP relaxation (Martins et al., 2009). In LP relaxation, the"
P12-1013,D11-1142,0,0.0406764,"nstraints improves rule learning comparing to methods that ignore graph structure. More recently, Berant et al. (Berant et al., 2011) introduced a more efficient exact algorithm, which decomposes the graph into connected components and then applies an ILP solver over each component. Despite this progress, finding the exact solution remains NP-hard – the authors themselves report they were unable to solve some graphs of rather moderate size and that the coverage of their method is limited. Thus, scaling their algorithm to data sets with tens of thousands of predicates (e.g., the extractions of Fader et al. (2011)) is unlikely. 117 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 117–125, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics In this paper we present a novel method for learning the edges of entailment graphs. Our method computes much more efficiently an approximate solution that is empirically almost as good as the exact solution. To that end, we first (Section 3) conjecture and empirically show that entailment graphs exhibit a “tree-like” property, i.e., that they can be reduced into a structure similar t"
P12-1013,P08-2012,0,0.0309496,"Linear Program (LP), which is polynomial. An LP solver is then applied on the problem, and variables xij that are assigned a fractional value are rounded to their nearest integer and so many violations of transitivity easily occur. The solution when applying LP relaxation is not a transitive graph, but nevertheless we show for comparison in Section 5 that our method is much faster. Last, we note that transitive relations have been explored in adjacent fields such as Temporal Information Extraction (Ling and Weld, 2010), Ontology Induction (Poon and Domingos, 2010), and Coreference Resolution (Finkel and Manning, 2008). 3 Forest-reducible Graphs The entailment relation, described by entailment graphs, is typically from a “semantically-specific” predicate to a more “general” one. Thus, intuitively, the topology of an entailment graph is expected to be “tree-like”. In this section we first formalize this intuition and then empirically analyze its validity. This property of entailment graphs is an interesting topological observation on its own, but also enables the efficient approximation algorithm of Section 4. For a directed edge i → j in a directed acyclic graphs (DAG), we term the node i a child of node j,"
P12-1013,P09-1039,0,0.0330329,"etically and empirically. Do and Roth (2010) suggested a method for a related task of learning taxonomic relations between terms. Given a pair of terms, a small graph is constructed and constraints are imposed on the graph structure. Their work, however, is geared towards scenarios where relations are determined on-the-fly for a given pair of terms and no global knowledge base is explicitly constructed. Thus, their method easily produces solutions where global constraints, such as transitivity, are violated. Another approximation method that violates transitivity constraints is LP relaxation (Martins et al., 2009). In LP relaxation, the constraint xij ∈ {0, 1} is replaced by 0 ≤ xij ≤ 1, transforming the problem from an ILP to a Linear Program (LP), which is polynomial. An LP solver is then applied on the problem, and variables xij that are assigned a fractional value are rounded to their nearest integer and so many violations of transitivity easily occur. The solution when applying LP relaxation is not a transitive graph, but nevertheless we show for comparison in Section 5 that our method is much faster. Last, we note that transitive relations have been explored in adjacent fields such as Temporal In"
P12-1013,P10-1031,0,0.0861357,"is not guaranteed, the area under the precision-recall curve drops by merely a point. To conclude, the contribution of this paper is twofold: First, we define a novel modeling assumption about the tree-like structure of entailment graphs and demonstrate its validity. Second, we exploit this assumption to develop a polynomial approximation algorithm for learning entailment graphs that can scale to much larger graphs than in the past. Finally, we note that learning entailment graphs bears strong similarities to related tasks such as Taxonomy Induction (Snow et al., 2006) and Ontology induction (Poon and Domingos, 2010), and thus our approach may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob"
P12-1013,P02-1006,0,0.287434,"Missing"
P12-1013,W06-1616,0,0.0335633,"and the constraint xij + xjk − xik ≤ 1 on the binary variables enforces that whenever xij = xjk = 1, then also xik = 1 (transitivity). Since ILP is NP-hard, applying an ILP solver directly does not scale well because the number of variables is O(|V |2 ) and the number of constraints is O(|V |3 ). Thus, even a graph with ∼80 nodes (predicates) has more than half a million constraints. Consequently, in (Berant et al., 2011), they proposed a method that efficiently decomposes the graph into smaller components and applies an ILP solver on each component separately using a cutting-plane procedure (Riedel and Clarke, 2006). Although this method is exact and improves scalability, it does not guarantee an efficient solution. When the graph does not decompose into sufficiently small components, and the weights generate many violations of transitivity, solving Max-Trans-Graph becomes intractable. To address this problem, we present in this paper a method for approximating the optimal set of edges within each component and show that it is much more efficient and scalable both theoretically and empirically. Do and Roth (2010) suggested a method for a related task of learning taxonomic relations between terms. Given a"
P12-1013,D10-1106,0,0.463293,"a6@post.tau.ac.il {dagan,goldbej}@{cs,eng}.biu.ac.il adlerm@cs.bgu.ac.il Abstract and Hovy, 2002; Shinyama and Sekine, 2006) or equivalently inference rules, that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, e.g., the entailment rule ‘X invade Y → X attack Y’ can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010). Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a “tree-like” property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration takes linear time. We compare o"
P12-1013,I05-5011,0,0.0553455,"Engineering, Bar-Ilan University jonatha6@post.tau.ac.il {dagan,goldbej}@{cs,eng}.biu.ac.il adlerm@cs.bgu.ac.il Abstract and Hovy, 2002; Shinyama and Sekine, 2006) or equivalently inference rules, that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, e.g., the entailment rule ‘X invade Y → X attack Y’ can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010). Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a “tree-like” property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edge"
P12-1013,N06-1039,0,0.114428,"Missing"
P12-1013,P06-1101,0,0.0775781,"rithm, and that though an optimal solution is not guaranteed, the area under the precision-recall curve drops by merely a point. To conclude, the contribution of this paper is twofold: First, we define a novel modeling assumption about the tree-like structure of entailment graphs and demonstrate its validity. Second, we exploit this assumption to develop a polynomial approximation algorithm for learning entailment graphs that can scale to much larger graphs than in the past. Finally, we note that learning entailment graphs bears strong similarities to related tasks such as Taxonomy Induction (Snow et al., 2006) and Ontology induction (Poon and Domingos, 2010), and thus our approach may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as Wor"
P12-1013,C08-1107,1,0.906093,"Bar-Ilan University jonatha6@post.tau.ac.il {dagan,goldbej}@{cs,eng}.biu.ac.il adlerm@cs.bgu.ac.il Abstract and Hovy, 2002; Shinyama and Sekine, 2006) or equivalently inference rules, that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, e.g., the entailment rule ‘X invade Y → X attack Y’ can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010). Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a “tree-like” property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration ta"
P12-1013,W09-2504,1,0.89564,"ntology induction (Poon and Domingos, 2010), and thus our approach may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob and Rambow, 2009; Ben Aharon et al., 2010), and others assumed that semantic relations between predicates can be deduced from their co-occurrence in a corpus via manually-constructed 118 patterns (Chklovski and Pantel, 2004). Recently, Berant et al. (2010; 2011) formulated the problem as the problem of learning global entailment graphs. In entailment graphs, nodes are predicates (e.g., ‘X attack Y’) and edges represent entailment rules between them (‘X invade Y → X attack Y’). For every pair of predicates i, j, an entailment score wij was learned by training a classifier over"
P12-1013,W04-3206,1,0.826086,"e past. Finally, we note that learning entailment graphs bears strong similarities to related tasks such as Taxonomy Induction (Snow et al., 2006) and Ontology induction (Poon and Domingos, 2010), and thus our approach may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob and Rambow, 2009; Ben Aharon et al., 2010), and others assumed that semantic relations between predicates can be deduced from their co-occurrence in a corpus via manually-constructed 118 patterns (Chklovski and Pantel, 2004). Recently, Berant et al. (2010; 2011) formulated the problem as the problem of learning global entailment graphs. In entailment graphs, nodes are predicates (e.g., ‘X attack Y’) and edges represent ent"
P12-2031,P10-1124,1,0.824564,"Missing"
P12-2031,D07-1017,0,0.101112,"Missing"
P12-2031,D11-1142,0,0.0413378,"Missing"
P12-2031,P11-2057,0,0.0181794,"ematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association"
P12-2031,N10-1045,0,0.012713,"application of a rule in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an application and yields high inter-annotator agreement. Unfortunately, their method requires lengthy guidelines and substantial annotator training effort, which are time consuming and costly. Thus, a simple, robust and replicable evaluation method is needed. Recently, crowdsourcing services such as Amazon Mechanical Turk (AMT) and CrowdFlower (CF)1 have been employed for semantic inference annotation (Snow et al., 2008; Wang and CallisonBurch, 2010; Mehdad et al., 2010; Negri et al., 2011). These works focused on generating and annotating RTE text-hypothesis pairs, but did not address annotation and evaluation of inference rules. In this paper, we propose a novel instance-based evaluation framework for inference rules that takes advantage of crowdsourcing. Our method substantially simplifies annotation of rule applications and avoids annotator training completely. The novelty in our framework is two-fold: (1) We simplify instance-based evaluation from a complex decision scenario to two independent binary decisions. (2) We apply methodological principles tha"
P12-2031,D11-1062,0,0.0204538,"in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an application and yields high inter-annotator agreement. Unfortunately, their method requires lengthy guidelines and substantial annotator training effort, which are time consuming and costly. Thus, a simple, robust and replicable evaluation method is needed. Recently, crowdsourcing services such as Amazon Mechanical Turk (AMT) and CrowdFlower (CF)1 have been employed for semantic inference annotation (Snow et al., 2008; Wang and CallisonBurch, 2010; Mehdad et al., 2010; Negri et al., 2011). These works focused on generating and annotating RTE text-hypothesis pairs, but did not address annotation and evaluation of inference rules. In this paper, we propose a novel instance-based evaluation framework for inference rules that takes advantage of crowdsourcing. Our method substantially simplifies annotation of rule applications and avoids annotator training completely. The novelty in our framework is two-fold: (1) We simplify instance-based evaluation from a complex decision scenario to two independent binary decisions. (2) We apply methodological principles that efficiently communi"
P12-2031,P02-1006,0,0.0781013,"a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators. 1 Introduction Inference rules are an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006), describing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms"
P12-2031,P10-1122,0,0.0283976,"wever, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, c Jeju, Republic of Korea, 8-14"
P12-2031,D10-1106,0,0.0624181,"ectional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), One option for evaluating inference rule resources is to measure their impact on an end task, as that is what ultimately interests an inference system developer. However, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablati"
P12-2031,I05-5011,0,0.0849676,"scribing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), One option for evaluating inference rule resources is to measure their impact on an end task, as that is what ultimately interests an inference system developer. However, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was establishe"
P12-2031,N06-1039,0,0.0333793,"er, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators. 1 Introduction Inference rules are an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006), describing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 20"
P12-2031,D08-1027,0,0.105078,"Missing"
P12-2031,C08-1107,1,0.8349,"Missing"
P12-2031,P07-1058,1,0.77055,"system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Bhagat et al. (2007) proposed “instance-based evaluation”, in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an applicatio"
P12-2031,W10-0725,0,0.0864527,"Missing"
P12-2031,W03-1011,0,0.542702,"Missing"
P12-3014,P10-1124,1,0.82554,"f documents for a given query, but do not allow any exploration of the thematic structure in the retrieved information. Thus, the need for tools that allow to effectively sift through a target set of documents is becoming ever more important. Faceted search (Stoica and Hearst, 2007; K¨aki, 2005) supports a better understanding of a target domain, by allowing exploration of data according to multiple views or facets. For example, given a set of documents on Nobel Prize laureates we might have different facets corresponding to the laureate’s nationality, the year when the prize was awarded, the Berant et al. (2010) proposed an exploration scheme that focuses on relations between concepts, which are derived from a graph describing textual entailment relations between propositions. In their setting a proposition consists of a predicate with two arguments that are possibly replaced by variables, such as ‘X control asthma’. A graph that specifies an entailment relation ‘X control asthma → X affect asthma’ can help a user, who is browsing documents dealing with substances that affect asthma, drill down and explore only substances that control asthma. This type of exploration can be viewed as an extension of"
P12-3014,D11-1142,0,0.0128819,"oposition-based exploration, or equivalently, statement-based exploration. In our model, it is the entailment relation between propositional templates which determines the granularity of the viewed information space. We first describe the inputs to the system and then detail our proposed exploration scheme. 3.1 System Inputs Corpus A collection of documents, which form the search space of the system. Extracted Propositions A set of propositions, extracted from the corpus document. The propositions are usually produced by an extraction method, such as TextRunner (Banko et al., 2007) or ReVerb (Fader et al., 2011). In order to support the exploration process, the documents are indexed by the propositional templates and argument terms of the extracted propositions. Entailment graph for predicates The nodes of the entailment graph are propositional templates, where edges indicate entailment relations between templates (Section 2.2). In order to avoid circularity in the exploration process, the graph is transformed into a DAG, by merging ‘equivalent’ nodes that are in the same strong connectivity component (as suggested by Berant et al. (2010)). In addition, for clarity and simplicity, edges that can be i"
P12-3014,N07-1031,0,0.0778192,"ate its benefit on the health-care domain. To the best of our knowledge this is the first implementation of an exploration system at the statement level that is based on the textual entailment relation. 1 Introduction Finding information in a large body of text is becoming increasingly more difficult. Standard search engines output a set of documents for a given query, but do not allow any exploration of the thematic structure in the retrieved information. Thus, the need for tools that allow to effectively sift through a target set of documents is becoming ever more important. Faceted search (Stoica and Hearst, 2007; K¨aki, 2005) supports a better understanding of a target domain, by allowing exploration of data according to multiple views or facets. For example, given a set of documents on Nobel Prize laureates we might have different facets corresponding to the laureate’s nationality, the year when the prize was awarded, the Berant et al. (2010) proposed an exploration scheme that focuses on relations between concepts, which are derived from a graph describing textual entailment relations between propositions. In their setting a proposition consists of a predicate with two arguments that are possibly r"
P13-1131,P11-1062,1,0.885173,"nce for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an inference rule may depend on the context in which it is applied, such as t"
P13-1131,D07-1017,0,0.284558,"mainly initiated by the highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. Howeve"
P13-1131,D10-1113,0,0.655432,"lied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a context-sensitive score for each particular"
P13-1131,C10-2029,0,0.519782,"lied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a context-sensitive score for each particular"
P13-1131,E09-1025,0,0.200537,"tion Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distributional setting. This research line was mainly initiated by the highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicate"
P13-1131,P12-2023,0,0.0224889,"el from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adapt our method to compute contextbiased lexical similarities accordingly. Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. In a different NLP task, Eidelman et al. (2012) utilize a similar approach to ours for improving the performance of statistical machine translation (SMT). They learn an LDA model on the source language side of the training corpus with the purpose of identifying implicit sub-domains. Then they utilize the distribution over topics inferred for each document in their corpus to compute separate per-topic translation probability tables. Finally, they train a classifier to translate a given target word based on these tables and the inferred topic distribution of the given document in which the target word appears. A notable difference between ou"
P13-1131,D08-1094,0,0.0973353,"Missing"
P13-1131,P10-2017,0,0.0700603,"Missing"
P13-1131,D11-1142,0,0.0780829,"010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rulesets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. The Lin similarity measure is described in Equation 2. Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpektor and Dagan, 2008). To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al., 2011), a large scale publicly available webbased open extractions data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pred, y), containing pred, a verb predicate, x, the argument instantiation of the template’s slot X, and y, the instantiation of the template’s slot Y . ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example ‘X can accommodate up to Y’. Yet, many of these templates share a similar meaning, e.g. ‘X accommodate up to Y’, ‘X can accommod"
P13-1131,P98-2127,0,0.928942,"d word space, context-sensitive methods represent them as vectors at the level of latent topics. This raises the question of whether such coarse-grained topic vectors might be less informative in determining the semantic similarity between the two predicates. To address this hypothesized caveat of prior context-sensitive rule scoring methods, we propose a novel generic scheme that integrates wordlevel and topic-level representations. Our scheme can be applied on top of any context-insensitive “base” similarity measure for rule learning, which operates at the word level, such as Cosine or Lin (Lin, 1998). Rather than computing a single context-insensitive rule score, we compute a distinct word-level similarity score for each topic in an LDA model. Then, when applying a rule in a given context, these different scores are weighed together based on the specific topic distribution under the given context. This way, we calculate similarity over vectors in the original word space, while biasing them towards the given context via a topic model. In order to promote replicability and equal-term comparison with our results, we based our experiments on publicly available datasets, both for unsupervised"
P13-1131,S07-1009,0,0.243789,"provement is statistically significant at p < 0.01 for BInc and Lin, and p < 0.015 for Cosine, using paired ttest. This shows that our model indeed successfully leverages contextual information beyond the basic context-agnostic rule scores and is robust across measures. Surprisingly, both baseline topic-level contextsensitive methods, namely DC and SC, underperformed compared to their context-insensitive baselines. While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over contextinsensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. However, our result suggests that topiclevel models might not be robust enough when applied to a random sample of inferences. An interesting indication of the differences between our word-topic model, WT, and topic-only models, DC and SC, lies in the optimal number of LDA topics required for each method. The number of topics in the range 25-100 performed almost equally well under the WT model for all base measures, with a moderate decline for higher numbers. 1337 The need for this rather small number of topics is due"
P13-1131,I05-5011,0,0.334593,"s data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pred, y), containing pred, a verb predicate, x, the argument instantiation of the template’s slot X, and y, the instantiation of the template’s slot Y . ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example ‘X can accommodate up to Y’. Yet, many of these templates share a similar meaning, e.g. ‘X accommodate up to Y’, ‘X can accommodate up to Y’, ‘X will accommodate up to Y’, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word cooccurrence statistics per predicate. Next, we applied some clean-up preprocessing to the ReVerb extractions. This includes discarding stop words, rare words and non-alphabetical words instantiating either the X or the Y arguments. In addition, we discarded all predicates that co-occur with less than 100 unique argument words in each slot. The remaining corpus consists of 7 million unique extractions and 2,155 verb predicates. Finally, w"
P13-1131,N06-1039,0,0.0362764,"and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set. 1 Introduction Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distributional setting. This research line was mainly initiated by t"
P13-1131,C08-1107,1,0.967985,"he highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an infer"
P13-1131,P08-1078,1,0.952819,"may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a"
P13-1131,P10-1097,0,0.0638171,"Missing"
P13-1131,P12-2031,1,0.821446,"h learned rule under each LDA topic, as specified in our context-sensitive model. We release a rule set comprising the top 500 context-sensitive rules that we learned for each of the verb predicates in our learning corpus, along with our trained LDA 3 ReVerb is available washington.edu/ 1336 at http://reverb.cs. Method Valid Invalid Total Lin 266 545 811 BInc 254 523 777 Cosine 272 539 811 Method CI DC SC WT Table 3: Sizes of rule application test set for each learned rule-set. model.4 4.2 Evaluation Task To evaluate the performance of the different methods we chose the dataset constructed by Zeichner et al. (2012). 5 This publicly available dataset contains about 6,500 manually annotated predicate template rule applications, each one labeled as correct or incorrect. For example, ‘Jack agree with Jill 9 Jack feel sorry for Jill’ is a rule application in this dataset, labeled as incorrect, and ‘Registration open this month → Registration begin this month’ is another rule application, labeled as correct. Rule applications were generated by randomly sampling extractions from ReVerb, such as (‘Jack’,‘agree with’,‘Jill’) and then sampling possible rules for each, such as ‘agree with → feel sorry for’. Hence,"
P13-1131,P08-1028,0,0.0596509,"l similarity and substitution scenarios in context. While we focus on lexical-syntactic predicate templates and instantiations of their argument slots as context, lexical similarity methods consider various lexical units that are not necessarily predicates, with their context typically being the collection of words in a window around them. Various approaches have been proposed to address lexical similarity. A number of works are based on a compositional semantics approach, where a prior representation of a target lexical unit is composed with the representations of words in its given context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010). Other works (Erk and Pad´o, 2010; Reisinger and Mooney, 2010) use a rather large word window around target words and compute similarities between clusters comprising instances of word windows. In addition, (Dinu and Lapata, 2010a) adapted the predicate inference topic model from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adap"
P13-1131,D12-1104,0,0.0505574,"ond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acq"
P13-1131,N07-1071,0,0.681951,"of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the gi"
P13-1131,P02-1006,0,0.103873,"tivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set. 1 Introduction Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distribu"
P13-1131,N10-1013,0,0.0879068,"ethods. 1333 Consider the application of an inference rule ‘LHS → RHS’ in the context of a particular pair of arguments for the X and Y slots, denoted by wx and wy , respectively. Denoting by l and r the predicates appearing in the two rule sides, the reliability score of the topic-level model is defined as follows (we present a geometric mean formulation for consistency with DIRT): scoreTopic (LHS → RHS, wx , wy ) q (3) = sim(dxl , dxr , wx ) · sim(dyl , dyr , wy ) where sim(d, d0 , w) is a topic-distribution similarity measure conditioned on a given context word. Specifically, Ritter et al. (2010) utilized the dot product form for their similarity measure: simDC (d, d0 , w) = Σt [p(t|d, w) · p(t|d0 , w)] (4) (the subscript DC stands for double-conditioning, as both distributions are conditioned on the argument word, unlike the measure below). Dinu and Lapata (2010b) presented a slightly different similarity measure for topic distributions that performed better in their setting as well as in a related later paper on context-sensitive scoring of lexical similarity (Dinu and Lapata, 2010a). In this measure, the topic distribution for the right hand side of the rule is not conditioned on w"
P13-1131,P10-1044,0,0.143123,"Missing"
P13-1131,D10-1106,0,0.105017,"argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X"
P13-1131,P10-1045,0,0.112294,"Missing"
P13-1131,C98-2122,0,\N,Missing
P13-1131,P08-1000,0,\N,Missing
P14-1133,D13-1160,1,0.864467,"candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instance, the utterances “W"
P14-1133,P13-1042,0,0.48901,"raphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instan"
P14-1133,N10-1066,0,0.0293492,"a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2012). Lastly, Freebas"
P14-1133,P09-1053,0,0.447958,"orpus, combined with a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2"
P14-1133,C04-1051,0,0.439373,"problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector space model: φpr (x, c)> θpr = φas (x, c)> θas + φvs (x, c)> θvs . x : What Association model The goal of the association model is to determine whether x and c contain phrases that are likely to be paraphrases. Given an utterance x = hx0 , x1 , .., xn−1 i, we denote by xi:j the span from token i to token j. For each pair of utterances (x, c), we go through all spans of x and c and"
P14-1133,D11-1142,0,0.0345079,"“Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instance, the utterances “Where is ACL in 2014?” and “What is the location of ACL 2014?” cannot be used in traditional semantic parsing methods, since the KB does not contain an entity ACL2014, but this pair clearly contains valuable linguistic information. As another reference point, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). In this paper, we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB (Figure 1). Our approach targets factoid questions with a modest amount of compositionality. Given an input utterance, we first use a simple deterministic procedure to construct a manageable set of candidate logical forms (ideally, we would generate canonical utterances for all possible logical forms, but this is intractable). Next, we heuris1415 Proceedings of the 52nd Annual Mee"
P14-1133,P13-1158,0,0.694577,"s, and a vector space model, which represents each utterance as a vector and learns a similarity score between them. The entire system is trained jointly from question-answer pairs only. Our work relates to recent lines of research in semantic parsing and question answering. Kwiatkowski et al. (2013) first maps utterances to a domain-independent intermediate logical form, and then performs ontology matching to produce the final logical form. In some sense, we approach the problem from the opposite end, using an intermediate utterance, which allows us to employ paraphrasing methods (Figure 2). Fader et al. (2013) presented a QA system that maps questions onto simple queries against Open IE extractions, by learning paraphrases from a large monolingual parallel corpus, and performing a single paraphrasing step. We adopt the idea of using paraphrasing for QA, but suggest a more general paraphrase model and work against a formal KB (Freebase). We apply our semantic parser on two datasets: W EB Q UESTIONS (Berant et al., 2013), which contains 5,810 question-answer pairs with common questions asked by web users; and 2 Setup Our task is as follows: Given (i) a knowledge base K, and (ii) a training set of que"
P14-1133,P05-1045,0,0.00789813,"Missing"
P14-1133,H05-1049,0,0.0119153,"monolingual parallel corpus, combined with a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of"
P14-1133,P06-1114,0,0.0147557,"was the gallipoli campaign waged?” is Galipoli and not GalipoliCampaign. Last, PARA S EMPRE does not handle temporal information, which causes errors in questions like “Where did Harriet Tubman live after the civil war?” 7 Discussion In this work, we approach the problem of semantic parsing from a paraphrasing viewpoint. A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment (Dagan et al., 2013). While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al., 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. Our paraphrase model emphasizes simplicity and efficiency, but the framework is agnostic to the internals of the paraphrase method. On the semantic parsing side, our work is most related to Kwiatkowski et al. (2013). The main challenge in semantic parsing is coping with the mismatch between language and the KB. In both Kwiatkowski et al. (2013) and this work, an intermediate representation is employed to handle the mismatch, but while they use"
P14-1133,N10-1145,0,0.139126,"ssociate natural language phrases that co-occur frequently in a monolingual parallel corpus, combined with a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature"
P14-1133,P12-1092,0,0.0624549,"ctor space model The association model relies on having a good set of candidate associations, but mining associations suffers from coverage issues. We now introduce a vector space (VS) model, which assigns a vector representation for each utterance, and learns a scoring function that ranks paraphrase candidates. We start by constructing vector representations of words. We run the WORD 2 VEC tool (Mikolov et al., 2013) on lower-cased Wikipedia text (1.59 billion tokens), using the CBOW model with a window of 5 and hierarchical softmax. We also experiment with publicly released word embeddings (Huang et al., 2012), which were trained using both local and global context. Both result in kdimensional vectors (k = 50). Next, we construct a vector vx ∈ Rk for each utterance x by simply averaging the vectors of all content words (nouns, verbs, and adjectives) in x. We can now estimate a paraphrase score for two utterances x and c via a weighted combination of the components of the vector representations: vx> W vc = k X wij vx,i vc,j i,j=1 where W ∈ Rk×k is a parameter matrix. In terms of our earlier notation, we have θvs = vec(W ) and φvs (x, c) = vec(vx vc> ), where vec(·) unrolls a matrix into a vector. In"
P14-1133,P03-1054,0,0.0553387,"Missing"
P14-1133,D10-1119,0,0.111259,"y? Type.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClay Whig Party Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pair"
P14-1133,D13-1161,0,0.286014,"m (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instance, the utterances “Where is ACL in 2014?” and “"
P14-1133,J13-2005,1,0.499567,"ry predicate; e.g., Founded.Microsoft denotes the entities that are Microsoft founders. In PlaceOfBirth.Seattle u Founded.Microsoft, an intersection operator allows us to denote the set of Seattle-born Microsoft founders. A reverse operator reverses the order of arguments: R[PlaceOfBirth].BillGates denotes Bill Gates’s birthplace (in contrast to PlaceOfBirth.Seattle). Lastly, count(Founded.Microsoft) denotes set cardinality, in this case, the number of Microsoft founders. The denotation of a logical form z with respect to a KB K is given by JzKK . For a formal description of simple λ-DCS, see Liang (2013) and Berant et al. (2013). 3 Model overview We now present the general framework for semantic parsing via paraphrasing, including the model and the learning algorithm. In Sections 4 and 5, we provide the details of our implementation. Canonical utterance construction Given an utterance x and the KB, we construct a set of candi1416 date logical forms Zx , and then for each z ∈ Zx generate a small set of canonical natural language utterances Cz . Our goal at this point is only to generate a manageable set of logical forms containing the correct one, and then generate an appropriate canonical utt"
P14-1133,W12-3016,0,0.00963061,"d Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2012). Lastly, Freebase formulas have types (see Section 4), and we conjoin the type of z with the first word of x, to capture the correlation between a word (e.g., “where”) with the Freebase type (e.g., Location). Learning As our training data consists of question-answer pairs (xi , yi ), we maximize the log-likelihood of the correct answer. The probability of an answer y is obtained by marginalizing over canonical utterances c and logical forms z whose denotation is y. Formally, our objective function O(θ) is as follows: exp{φ(x, c, z)> θ} pθ (c, z |x) = P , 0 0 > z 0 ∈Zx ,c0 ∈Cz exp{φ(x, c , z )"
P14-1133,J04-4002,0,0.0223798,"mma(xi:j ) denote the POS tag and lemma sequence of xi:j . corpora, containing 18 million pairs of question paraphrases from wikianswers.com, which were tagged as having the same meaning by users. PARALEX is suitable for our needs since it focuses on question paraphrases. For example, the phrase “do for a living” occurs mostly in questions, and we can extract associations for this phrase from PARALEX. Paraphrase pairs in PAR ALEX are word-aligned using standard machine translation methods. We use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) to all 5-grams. This results in a phrase table with approximately 1.3 million phrase pairs. We let A denote this set of mined candidate associations. For a pair (x, c), we also consider as candidate associations the set B (represented implicitly), which contains token pairs (xi , ci0 ) such that xi and ci0 share the same lemma, the same POS tag, or are linked through a derivation link on WordNet (Fellbaum, 1998). This allows us to learn paraphrases for words that appear in our datasets but are not covered by the phrase table, and to handle nominalizations for phrase pairs such as “Who designe"
P14-1133,E06-1052,0,0.00946005,"ot GalipoliCampaign. Last, PARA S EMPRE does not handle temporal information, which causes errors in questions like “Where did Harriet Tubman live after the civil war?” 7 Discussion In this work, we approach the problem of semantic parsing from a paraphrasing viewpoint. A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment (Dagan et al., 2013). While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al., 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. Our paraphrase model emphasizes simplicity and efficiency, but the framework is agnostic to the internals of the paraphrase method. On the semantic parsing side, our work is most related to Kwiatkowski et al. (2013). The main challenge in semantic parsing is coping with the mismatch between language and the KB. In both Kwiatkowski et al. (2013) and this work, an intermediate representation is employed to handle the mismatch, but while they use a logical representation, we opt for a text-b"
P14-1133,R11-1063,0,0.0374621,"phrases that co-occur frequently in a monolingual parallel corpus, combined with a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity fre"
P14-1133,N03-1033,0,0.0202947,"Missing"
P14-1133,W09-0607,0,0.0323385,"rasing method that maps a test question to a question for which the answer is already known in a single step. We propose a general paraphrasing framework and instantiate it with two paraphrase models. Lastly, Fader et al. handle queries with only one property and entity whereas we generalize to more types of logical forms. Since our generated questions are passed to a paraphrase model, we took a very simple approach, mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization. Research on generation (Dale et al., 2003; Reiter et al., 2005; Turner et al., 2009; Piwek and Boyer, 2012) typically focuses on generating natural utterances for human consumption, where fluency is important. In conclusion, the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model. We achieve state-of-the-art results on two recently released datasets. We believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB. With more sophisticated generation and paraphrase, we hope to tackle compositionally richer utterances"
P14-1133,U06-1019,0,0.250324,"urts accuracy by only a modest amount. 5 Paraphrasing type c : What is of the music musical did Richard genres of Wagner Richard play Wagner Figure 3: Token associations extracted for a paraphrase pair. Blue and dashed (red and solid) indicate positive (negative) score. Line width is proportional to the absolute value of the score. 5.1 Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model"
P14-1133,C10-1131,0,0.0606614,"er Richard play Wagner Figure 3: Token associations extracted for a paraphrase pair. Blue and dashed (red and solid) indicate positive (negative) score. Line width is proportional to the absolute value of the score. 5.1 Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector space model: φpr (x, c)> θpr = φas (x, c)> θas + φvs (x, c)> θvs . x : Wh"
P14-1133,P07-1121,0,0.760219,"ed the people Henry Clay? Type.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClay Whig Party Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic"
P15-1129,D11-1039,0,0.00781714,"compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an impor"
P15-1129,Q13-1005,0,0.079901,"person that is author of the most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Figure 1: Functionality-driven process for building semantic par"
P15-1129,P14-1133,1,0.955127,"al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014). In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule. In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014). Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain. Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny,"
P15-1129,D13-1160,1,0.752966,"ion date argmax(type.article, publicationDate) person that is author of the most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Figure 1:"
P15-1129,P13-1042,0,0.085091,"Missing"
P15-1129,P11-1060,1,0.833794,"the largest publication date argmax(type.article, publicationDate) person that is author of the most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to"
P15-1129,J04-4002,0,0.041243,"bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the P UBLICATIONS domain include fewest– least number and by–whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013). 7 Experimental Evaluation We evaluated our functionality-driven process on the seven domains described in Section 5 and one new domain we describe in Secti"
P15-1129,P15-1142,1,0.816375,"ch search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance “article” would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015). Training. We train our model by maximizing the regularized log-likelihood O(θ) = 1337 Domain C ALENDAR # pred. 22 # ex. 837 Phenomena temporal language B LOCKS 19 1995 spatial language H OUSING 24 941 measurement units R ESTAURANTS 32 1657 long unary relations P UBLICATIONS 15 801 S OCIAL 45 4419 multi-arity relations BASKETBALL 24 1952 parentheticals sublexical compositionality Example x: “Show me meetings after the weekly standup day” c: “meeting whose date is at least date of weekly standup” z: type.meeting u date. > R(date).weeklyStandup x: “Select the brick that is to the furthest left."
P15-1129,W10-2903,0,0.0167669,"n answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or cont"
P15-1129,Q14-1030,0,0.145706,"specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the"
P15-1129,P13-1158,0,0.0467733,"emantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by"
P15-1129,N13-1092,0,0.0451735,"j) ∈ A. (xi:i , cj−1:j ) ∀(i, j), (i + 1, j + 1) ∈ A. (xi:i+1 , cj:j+1 ) all unaligned words in x and c (xi:j , ci0 :j 0 ) if in phrase table Table 4: Features for the paraphrasing model. pos(xi:i ) is the POS tag; type(JzKw ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c. P (x,c,z)∈D log pθ (z, c |x, w) − λkθk1 . To optimize, we use AdaGrad (Duchi et al., 2010). Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phr"
P15-1129,Q13-1016,0,0.00746168,"onsiderable room for improvement in the paraphrasing model. 8 Related work and discussion Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphr"
P15-1129,C10-2128,0,0.00740735,"to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance. In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic implications. We believe that our methodology is a"
P15-1129,J82-3002,0,0.153592,"denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance. In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic"
P15-1129,N13-1103,0,0.217487,"most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Figure 1: Functionality-driven process for building semantic parsers. The two red boxes are t"
P15-1129,P07-1121,0,0.0103617,"the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 197"
P15-1129,D13-1161,0,0.154588,"article, publicationDate) person that is author of the most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Figure 1: Functionality-driven proc"
P15-1129,N06-1014,1,0.127494,"s for the paraphrasing model. pos(xi:i ) is the POS tag; type(JzKw ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c. P (x,c,z)∈D log pθ (z, c |x, w) − λkθk1 . To optimize, we use AdaGrad (Duchi et al., 2010). Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the P UBLICATIONS domain include fewest– least number and by–whose author is. Note that we do not build a hard lexicon but onl"
P15-1129,J13-2005,1,\N,Missing
P17-1003,N16-1181,0,0.1022,"Missing"
P17-1003,P16-1002,0,0.217076,"Figure 1: The main challenges of training a semantic parser from weak supervision: (a) compositionality: we use variables (v0 , v1 , v2 ) to store execution results of intermediate generated programs. (b) search: we prune the search space and augment REINFORCE with pseudo-gold programs. bolic representation that is executed by an executor, through weak supervision remains challenging. This is because the model must interact with a symbolic executor through non-differentiable operations to search over a large program space. In semantic parsing, recent work handled this (Dong and Lapata, 2016; Jia and Liang, 2016) by training from manually annotated programs and avoiding program execution at training time. However, annotating programs is known to be expensive and scales poorly. In program induction, attempts to address this problem (Graves et al., 2014; Reed and de Freitas, 2016; Kaiser and Sutskever, 2015; Graves et al., 2016b; Andreas et al., 2016) either utilized low-level memory (Zaremba and Sutskever, 2015), or required memory to be differentiable (Neelakantan et al., 2015; Yin et al., 2015) so that the model can be trained with backpropagation. This makes it difficult to use the efficient discret"
P17-1003,D13-1160,1,0.219306,"port language compositionality, we augment the standard seq2seq model with a key-variable memory to save and reuse intermediate execution results (Figure 1). This is a novel application of pointer networks (Vinyals et al., 2015) to compositional semantics. Second, to alleviate the search problem of finding correct programs when training from questionanswer pairs,we use the computer to execute partial programs and prune the programmer’s search space by checking the syntax and semantics of generated programs. This generalizes the weakly supervised semantic parsing framework (Liang et al., 2011; Berant et al., 2013) by leveraging semantic denotations during structural search. Third, to train from weak supervision and directly maximize the expected reward we turn to the REINFORCE (Williams, 1992) algorithm. Since learning from scratch is difficult for REINFORCE, we combine it with an iterative maximum likelihood (ML) training process, where beam search is used to find pseudo-gold programs, which are then used to augment the objective of REINFORCE. On the W EB Q UESTIONS SP dataset (Yih et al., 2016), NSM achieves new state-of-the-art results with weak supervision, significantly closing the gap between wea"
P17-1003,Q15-1039,1,0.669596,"sions in a large search space. Our solution employs two techniques (a) a symbolic “computer” helps find good programs by pruning the search space (b) an iterative ML training process, where beam search is used to find pseudogold programs. Wiseman and Rush (Wiseman and Rush, 2016) proposed a max-margin approach to train a sequence-to-sequence scorer. However, their training procedure is more involved, and we did not implement it in this work. MIXER (Ranzato et al., 2015) also proposed to combine ML training and REINFORCE, but they only considered tasks with full supervisions. Berant and Liang (Berant and Liang, 2015) applied imitation learning to semantic parsing, but still requires hand crafted grammars and features. 5 Conclusion We propose the Manager-Programmer-Computer framework for neural program induction. It integrates neural networks with a symbolic nondifferentiable computer to support abstract, scalable and precise operations through a friendly neural computer interface. Within this framework, we introduce the Neural Symbolic Machine, which integrates a neural sequence-to-sequence “programmer” with key-variable memory, and a symbolic Lisp interpreter with code assistance. Because the interpreter"
P17-1003,D14-1179,0,0.0107889,"Missing"
P17-1003,D11-1049,1,0.0251387,"uitable for representing semantics. Earlier works such as OOPS (Schmidhuber, 2004) has desirable characteristics, for example, the ability to define new functions. These remain to be future improvements for NSM. ever, their strategy is not applicable to a large KB such as Freebase, which contains about 100M entities, and more than 20k properties. Instead, NSM chooses a more scalable approach, where the “computer” saves intermediate results, and the neural network only refers to them with variable names (e.g., “R1 ” for all cities in the US). NSM is similar to the Path Ranking Algorithm (PRA) (Lao et al., 2011) in that semantics is encoded as a sequence of actions, and denotations are used to prune the search space during learning. NSM is more powerful than PRA by 1) allowing more complex semantics to be composed through the use of a key-variable memory; 2) controlling the search procedure with a trained neural network, while PRA only samples actions uniformly; 3) allowing input questions to express complex relations, and then dynamically generating action sequences. PRA can combine multiple semantic representations to produce the final prediction, which remains to be future work for NSM. We formula"
P17-1003,D16-1127,0,0.00967479,"ics to be composed through the use of a key-variable memory; 2) controlling the search procedure with a trained neural network, while PRA only samples actions uniformly; 3) allowing input questions to express complex relations, and then dynamically generating action sequences. PRA can combine multiple semantic representations to produce the final prediction, which remains to be future work for NSM. We formulate NSM training as an instance of reinforcement learning (Sutton and Barto, 1998) in order to directly optimize the task reward of the structured prediction problem (Norouzi et al., 2016; Li et al., 2016; Yu et al., 2017). Compared to imitation learning methods (Daume et al., 2009; Ross et al., 2011) that interpolate a model distribution with an oracle, NSM needs to solve a challenging search problem of training from weak supervisions in a large search space. Our solution employs two techniques (a) a symbolic “computer” helps find good programs by pruning the search space (b) an iterative ML training process, where beam search is used to find pseudogold programs. Wiseman and Rush (Wiseman and Rush, 2016) proposed a max-margin approach to train a sequence-to-sequence scorer. However, their tra"
P17-1003,P11-1060,0,0.248285,"difficult to use the efficient discrete operations and memory of a traditional computer, and limited the application to synthetic or small knowledge bases. In this paper, we propose to utilize the memory and discrete operations of a traditional comDeep neural networks have achieved impressive performance in supervised classification and structured prediction tasks such as speech recognition (Hinton et al., 2012), machine translation (Bahdanau et al., 2014; Wu et al., 2016) and more. However, training neural networks for semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) or program induction, where language is mapped to a sym† (Hop v0 CityIn) Large Search Space Introduction ⇤ v1 ← Work done while the author was interning at Google Work done while the author was visiting Google 23 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 23–33 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1003 puter in a novel Manager-Programmer-Computer (MPC) framework for neural program induction, which integrates three components: 1. A “manager” that provides"
P17-1003,P16-1004,0,0.492526,"lation ) Size Elevation Figure 1: The main challenges of training a semantic parser from weak supervision: (a) compositionality: we use variables (v0 , v1 , v2 ) to store execution results of intermediate generated programs. (b) search: we prune the search space and augment REINFORCE with pseudo-gold programs. bolic representation that is executed by an executor, through weak supervision remains challenging. This is because the model must interact with a symbolic executor through non-differentiable operations to search over a large program space. In semantic parsing, recent work handled this (Dong and Lapata, 2016; Jia and Liang, 2016) by training from manually annotated programs and avoiding program execution at training time. However, annotating programs is known to be expensive and scales poorly. In program induction, attempts to address this problem (Graves et al., 2014; Reed and de Freitas, 2016; Kaiser and Sutskever, 2015; Graves et al., 2016b; Andreas et al., 2016) either utilized low-level memory (Zaremba and Sutskever, 2015), or required memory to be differentiable (Neelakantan et al., 2015; Yin et al., 2015) so that the model can be trained with backpropagation. This makes it difficult to use"
P17-1003,P15-1142,0,0.113679,"rates “)”), it gets executed, and the result is stored as the value of a new variable in the “computer”. This variable is keyed by the GRU hidden state at that step. When a new variable R1 with key embedding v1 is added into the key-variable memory, By introducing variables that save the intermediate results of execution, the program naturally models language compositionality and describes from left to right a bottom-up derivation of the full meaning of the natural language input, which is convenient in a seq2seq model (Figure 1). This is reminiscent of the floating parser (Wang et al., 2015; Pasupat and Liang, 2015), where a derivation tree that is not grounded in the input is incrementally constructed. The set of programs defined by our functions is equivalent to the subset of -calculus presented in (Yih et al., 2015). We did not use full Lisp programming language here, because constructs like control flow and loops are unnecessary for most current semantic parsing tasks, and it is simple to add more functions to the model when necessary. To create a friendly neural computer interface, the interpreter provides code assistance to the programmer by producing a list of valid tokens at each step. First, a v"
P17-1003,P15-1128,0,0.119198,"added into the key-variable memory, By introducing variables that save the intermediate results of execution, the program naturally models language compositionality and describes from left to right a bottom-up derivation of the full meaning of the natural language input, which is convenient in a seq2seq model (Figure 1). This is reminiscent of the floating parser (Wang et al., 2015; Pasupat and Liang, 2015), where a derivation tree that is not grounded in the input is incrementally constructed. The set of programs defined by our functions is equivalent to the subset of -calculus presented in (Yih et al., 2015). We did not use full Lisp programming language here, because constructs like control flow and loops are unnecessary for most current semantic parsing tasks, and it is simple to add more functions to the model when necessary. To create a friendly neural computer interface, the interpreter provides code assistance to the programmer by producing a list of valid tokens at each step. First, a valid token should not cause a syntax error: e.g., if the previous token is “(”, the next token must be a function name, and if the previous token is “Hop”, the next token must be a variable. More importantly"
P17-1003,P16-2033,0,0.064073,"nerated programs. This generalizes the weakly supervised semantic parsing framework (Liang et al., 2011; Berant et al., 2013) by leveraging semantic denotations during structural search. Third, to train from weak supervision and directly maximize the expected reward we turn to the REINFORCE (Williams, 1992) algorithm. Since learning from scratch is difficult for REINFORCE, we combine it with an iterative maximum likelihood (ML) training process, where beam search is used to find pseudo-gold programs, which are then used to augment the objective of REINFORCE. On the W EB Q UESTIONS SP dataset (Yih et al., 2016), NSM achieves new state-of-the-art results with weak supervision, significantly closing the gap between weak and full supervision for this task. Unlike prior works, it is trained end-toend, and does not require feature engineering or domain-specific knowledge. 2 Neural Symbolic Machines We now introduce NSM by first describing the “computer”, a non-differentiable Lisp interpreter that executes programs against a large KB and provides code assistance (Section 2.1). We then propose a seq2seq model (“programmer”) that supports compositionality using a key-variable memory to save and reuse interm"
P17-1003,P17-1172,1,0.0362347,"d through the use of a key-variable memory; 2) controlling the search procedure with a trained neural network, while PRA only samples actions uniformly; 3) allowing input questions to express complex relations, and then dynamically generating action sequences. PRA can combine multiple semantic representations to produce the final prediction, which remains to be future work for NSM. We formulate NSM training as an instance of reinforcement learning (Sutton and Barto, 1998) in order to directly optimize the task reward of the structured prediction problem (Norouzi et al., 2016; Li et al., 2016; Yu et al., 2017). Compared to imitation learning methods (Daume et al., 2009; Ross et al., 2011) that interpolate a model distribution with an oracle, NSM needs to solve a challenging search problem of training from weak supervisions in a large search space. Our solution employs two techniques (a) a symbolic “computer” helps find good programs by pruning the search space (b) an iterative ML training process, where beam search is used to find pseudogold programs. Wiseman and Rush (Wiseman and Rush, 2016) proposed a max-margin approach to train a sequence-to-sequence scorer. However, their training procedure is"
P17-1003,P15-1129,1,0.0364946,"., the decoder generates “)”), it gets executed, and the result is stored as the value of a new variable in the “computer”. This variable is keyed by the GRU hidden state at that step. When a new variable R1 with key embedding v1 is added into the key-variable memory, By introducing variables that save the intermediate results of execution, the program naturally models language compositionality and describes from left to right a bottom-up derivation of the full meaning of the natural language input, which is convenient in a seq2seq model (Figure 1). This is reminiscent of the floating parser (Wang et al., 2015; Pasupat and Liang, 2015), where a derivation tree that is not grounded in the input is incrementally constructed. The set of programs defined by our functions is equivalent to the subset of -calculus presented in (Yih et al., 2015). We did not use full Lisp programming language here, because constructs like control flow and loops are unnecessary for most current semantic parsing tasks, and it is simple to add more functions to the model when necessary. To create a friendly neural computer interface, the interpreter provides code assistance to the programmer by producing a list of valid token"
P17-1003,D16-1137,0,0.030246,"to directly optimize the task reward of the structured prediction problem (Norouzi et al., 2016; Li et al., 2016; Yu et al., 2017). Compared to imitation learning methods (Daume et al., 2009; Ross et al., 2011) that interpolate a model distribution with an oracle, NSM needs to solve a challenging search problem of training from weak supervisions in a large search space. Our solution employs two techniques (a) a symbolic “computer” helps find good programs by pruning the search space (b) an iterative ML training process, where beam search is used to find pseudogold programs. Wiseman and Rush (Wiseman and Rush, 2016) proposed a max-margin approach to train a sequence-to-sequence scorer. However, their training procedure is more involved, and we did not implement it in this work. MIXER (Ranzato et al., 2015) also proposed to combine ML training and REINFORCE, but they only considered tasks with full supervisions. Berant and Liang (Berant and Liang, 2015) applied imitation learning to semantic parsing, but still requires hand crafted grammars and features. 5 Conclusion We propose the Manager-Programmer-Computer framework for neural program induction. It integrates neural networks with a symbolic nondifferen"
P17-1003,D14-1162,0,\N,Missing
P17-1020,P16-1153,0,0.00471278,"th the TREC QA (Voorhees and Tice, 2000), WikiQA (Yang et al., 2016b) and SelQA (Jurczyk et al., 2016) datasets. Recently, neural networks models (Wang and Nyberg, 2015; Severyn and 216 Figure 5: For a random subset of documents in the development set, we visualized the learned attention over the sentences (p(sl |d, x)). bine supervision signals. Reinforcement learning recently gained popularity in tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al., 2015; He et al., 2016). Moschitti, 2015; dos Santos et al., 2016) achieved improvements on TREC datsaet. Sultan et al. (2016) optimized the answer sentence extraction and the answer extraction jointly, but with gold labels for both parts. Trischler et al. (2016b) proposed a model that shares the intuition of observing inputs at multiple granularities (sentence, word), but deals with multiple choice questions. Our model considers answer sentence selection as latent and generates answer strings instead of selecting text spans, and we found that WIKIREADING dataset suits our purposes best with some pruning, which stil"
P17-1020,P16-1145,1,0.937857,"running them over long documents is prohibitively slow because it is difficult to parallelize over sequences. Inspired by how people first skim the document, identify relevant parts, and carefully read these parts to produce an answer, we combine a coarse, fast model for selecting relevant sentences and a more expensive RNN for producing the answer from those sentences. We treat sentence selection as a latent variable trained jointly from the answer only using reinforcement learning. Experiments demonstrate the state of the art performance on a challenging subset of the W IKIREADING dataset (Hewlett et al., 2016) and on a new dataset, while speeding up the model by 3.5x-6.7x. 1 Sentence Selection (Latent) ˆ Document Summary (d) Answer Generation (RNN) Answer (y) Figure 1: Hierarchical question answering: the model first selects relevant sentences that produce a document summary ˆ for the given query (x), and then generates an answer (y) (d) ˆ and the query x. based on the summary (d) (RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016; Xiong et al., 2016). While such models have access to all the rel"
P17-1020,N16-1181,0,0.0123817,"; Wang et al., 2016; Xiong Answer sentence selection is studied with the TREC QA (Voorhees and Tice, 2000), WikiQA (Yang et al., 2016b) and SelQA (Jurczyk et al., 2016) datasets. Recently, neural networks models (Wang and Nyberg, 2015; Severyn and 216 Figure 5: For a random subset of documents in the development set, we visualized the learned attention over the sentences (p(sl |d, x)). bine supervision signals. Reinforcement learning recently gained popularity in tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al., 2015; He et al., 2016). Moschitti, 2015; dos Santos et al., 2016) achieved improvements on TREC datsaet. Sultan et al. (2016) optimized the answer sentence extraction and the answer extraction jointly, but with gold labels for both parts. Trischler et al. (2016b) proposed a model that shares the intuition of observing inputs at multiple granularities (sentence, word), but deals with multiple choice questions. Our model considers answer sentence selection as latent and generates answer strings instead of selecting text spans, and we found that WIKIREADING"
P17-1020,P16-1086,0,0.0551691,"performance on a challenging subset of the W IKIREADING dataset (Hewlett et al., 2016) and on a new dataset, while speeding up the model by 3.5x-6.7x. 1 Sentence Selection (Latent) ˆ Document Summary (d) Answer Generation (RNN) Answer (y) Figure 1: Hierarchical question answering: the model first selects relevant sentences that produce a document summary ˆ for the given query (x), and then generates an answer (y) (d) ˆ and the query x. based on the summary (d) (RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016; Xiong et al., 2016). While such models have access to all the relevant information, they are slow because the model needs to be run sequentially over possibly thousands of tokens, and the computation is not parallelizable. In fact, such models usually truncate the documents and consider only a limited number of tokens (Miller et al., 2016; Hewlett et al., 2016). Inspired by studies on how people answer questions by first skimming the document, identifying relevant parts, and carefully reading these parts to produce an answer (Masson, 1983), we propose a coarse-to-fine model for question answ"
P17-1020,P16-1223,0,0.0198913,"ments demonstrate the state of the art performance on a challenging subset of the W IKIREADING dataset (Hewlett et al., 2016) and on a new dataset, while speeding up the model by 3.5x-6.7x. 1 Sentence Selection (Latent) ˆ Document Summary (d) Answer Generation (RNN) Answer (y) Figure 1: Hierarchical question answering: the model first selects relevant sentences that produce a document summary ˆ for the given query (x), and then generates an answer (y) (d) ˆ and the query x. based on the summary (d) (RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016; Xiong et al., 2016). While such models have access to all the relevant information, they are slow because the model needs to be run sequentially over possibly thousands of tokens, and the computation is not parallelizable. In fact, such models usually truncate the documents and consider only a limited number of tokens (Miller et al., 2016; Hewlett et al., 2016). Inspired by studies on how people answer questions by first skimming the document, identifying relevant parts, and carefully reading these parts to produce an answer (Masson, 1983), we propose"
P17-1020,P16-1046,0,0.00918336,"2016b) proposed a model that shares the intuition of observing inputs at multiple granularities (sentence, word), but deals with multiple choice questions. Our model considers answer sentence selection as latent and generates answer strings instead of selecting text spans, and we found that WIKIREADING dataset suits our purposes best with some pruning, which still provided 1.97 million examples compared to 2K questions for TREC dataset. Hierarchical models which treats sentence selection as a latent variable have been applied text categorization (Yang et al., 2016b), extractive summarization (Cheng and Lapata, 2016), machine translation (Ba et al., 2014) and sentiment analysis (Yessenalina et al., 2010; Lei et al., 2016). To the best of our knowledge, we are the first to use the hierarchical nature of a document for QA. Finally, our work is related to the reinforcement learning literature. Hard and soft attention were examined in the context of caption generation (Xu et al., 2015). Curriculum learning was investigated in Sachan and Xing (2016), but they focused on the ordering of training examples while we com8 Conclusion We presented a coarse-to-fine framework for QA over long documents that quickly foc"
P17-1020,P14-1062,0,0.0293513,"an advantage since it samples a “real” sentence without mixing words from different sentences. Conversely, soft attention is trained more easily, and has the capacity to learn a low-entropy distribution that is similar to hard attention. p(c = cl,j |x, d), with the same parameters as in the BoW model. Convolutional Neural Network Model While our sentence selection model is designed to be fast, we explore a convolutional neural network (CNN) that can compose the meaning of nearby words. A CNN is still efficient, since all filters can be computed in parallel. Following previous work (Kim, 2014; Kalchbrenner et al., 2014), we concatenate the embeddings of tokens in the query x and the sentence sl , and run a convolutional layer with F filters and width w over the concatenated embeddings. This results in F features for every span of length w, and we employ max-over-time-pooling (Collobert et al., 2011) to get a final representation hl ∈ RF . We then compute p(s = sl |x, d) by passing hl through a single layer feed-forward network as in the BoW model. 4.2 |d| X 4.3 Document Summary After computing attention over sentences, we create a summary that focuses on the document parts related to the question using deter"
P17-1020,D14-1181,0,0.00600041,"attention an advantage since it samples a “real” sentence without mixing words from different sentences. Conversely, soft attention is trained more easily, and has the capacity to learn a low-entropy distribution that is similar to hard attention. p(c = cl,j |x, d), with the same parameters as in the BoW model. Convolutional Neural Network Model While our sentence selection model is designed to be fast, we explore a convolutional neural network (CNN) that can compose the meaning of nearby words. A CNN is still efficient, since all filters can be computed in parallel. Following previous work (Kim, 2014; Kalchbrenner et al., 2014), we concatenate the embeddings of tokens in the query x and the sentence sl , and run a convolutional layer with F filters and width w over the concatenated embeddings. This results in F features for every span of length w, and we employ max-over-time-pooling (Collobert et al., 2011) to get a final representation hl ∈ RF . We then compute p(s = sl |x, d) by passing hl through a single layer feed-forward network as in the BoW model. 4.2 |d| X 4.3 Document Summary After computing attention over sentences, we create a summary that focuses on the document parts related"
P17-1020,D16-1245,0,0.015382,"ynthetic dataset that captures various aspects of reasoning; and SQuAD (Rajpurkar et al., 2016; Wang et al., 2016; Xiong Answer sentence selection is studied with the TREC QA (Voorhees and Tice, 2000), WikiQA (Yang et al., 2016b) and SelQA (Jurczyk et al., 2016) datasets. Recently, neural networks models (Wang and Nyberg, 2015; Severyn and 216 Figure 5: For a random subset of documents in the development set, we visualized the learned attention over the sentences (p(sl |d, x)). bine supervision signals. Reinforcement learning recently gained popularity in tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al., 2015; He et al., 2016). Moschitti, 2015; dos Santos et al., 2016) achieved improvements on TREC datsaet. Sultan et al. (2016) optimized the answer sentence extraction and the answer extraction jointly, but with gold labels for both parts. Trischler et al. (2016b) proposed a model that shares the intuition of observing inputs at multiple granularities (sentence, word), but deals with multiple choice questions. Our model considers answer sentence selection as latent a"
P17-1020,D16-1011,0,0.196594,"setup is that an answer does not necessarily appear verbatim in the input (the genre of a movie can be determined even if not mentioned explicitly). Furthermore, the answer often appears multiple times in the document in spurious contexts (the year ‘2012’ can appear many times while only once in relation to the question). Thus, we treat sentence selection as a latent variable that is trained jointly with the answer generation model from the answer only using reinforcement learning. Treating sentence selection as a latent variable has been explored in classification (Yessenalina et al., 2010; Lei et al., 2016), however, to our knowledge, has not been applied for question answering. 3 Data We evaluate on W IKI R EADING, W IKI R EADING L ONG, and a new dataset, W IKI S UGGEST. W IKIREADING (Hewlett et al., 2016) is a QA dataset automatically generated from Wikipedia and Wikidata: given a Wikipedia page about an entity and a Wikidata property, such as PROFES SION , or GENDER , the goal is to infer the target value based on the document. Unlike other recently released large-scale datasets (Rajpurkar et al., 2016; Trischler et al., 2016a), W IKIREAD ING does not annotate answer spans, making sentence se"
P17-1020,Q16-1009,0,0.024095,"16) datasets. Recently, neural networks models (Wang and Nyberg, 2015; Severyn and 216 Figure 5: For a random subset of documents in the development set, we visualized the learned attention over the sentences (p(sl |d, x)). bine supervision signals. Reinforcement learning recently gained popularity in tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al., 2015; He et al., 2016). Moschitti, 2015; dos Santos et al., 2016) achieved improvements on TREC datsaet. Sultan et al. (2016) optimized the answer sentence extraction and the answer extraction jointly, but with gold labels for both parts. Trischler et al. (2016b) proposed a model that shares the intuition of observing inputs at multiple granularities (sentence, word), but deals with multiple choice questions. Our model considers answer sentence selection as latent and generates answer strings instead of selecting text spans, and we found that WIKIREADING dataset suits our purposes best with some pruning, which still provided 1.97 million examples compared to 2K questions for TREC dataset. Hierarchical models which t"
P17-1020,D16-1147,0,0.0226778,"cument summary ˆ for the given query (x), and then generates an answer (y) (d) ˆ and the query x. based on the summary (d) (RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016; Xiong et al., 2016). While such models have access to all the relevant information, they are slow because the model needs to be run sequentially over possibly thousands of tokens, and the computation is not parallelizable. In fact, such models usually truncate the documents and consider only a limited number of tokens (Miller et al., 2016; Hewlett et al., 2016). Inspired by studies on how people answer questions by first skimming the document, identifying relevant parts, and carefully reading these parts to produce an answer (Masson, 1983), we propose a coarse-to-fine model for question answering. Our model takes a hierarchical approach (see Figure 1), where first a fast model is used to select a few sentences from the document that are relevant for answering the question (Yu et al., 2014; Yang et al., 2016a). Then, a slow RNN is employed to produce the final answer from the selected sentences. The RNN is run over a fixed numb"
P17-1020,P16-1041,0,0.251461,"is employed to produce the final answer from the selected sentences. The RNN is run over a fixed number of tokens, regardless of the length of the document. Empirically, our model encodes the Introduction Reading a document and answering questions about its content are among the hallmarks of natural language understanding. Recently, interest in question answering (QA) from unstructured documents has increased along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016; Nguyen et al., 2016; Trischler et al., 2016a). Current state-of-the-art approaches for QA over documents are based on recurrent neural networks † Document (d) Work done while the authors were at Google. 209 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 209–220 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1020 s1 : The 2011 Joplin tornado was a catastrophic EF5rated multiple-vortex tornado that struck Joplin, Missouri . . . d: s4 : It was the third tornado to strike Joplin since May 1971. s5 : Overall, the to"
P17-1020,D15-1001,0,0.00610636,"e selection is studied with the TREC QA (Voorhees and Tice, 2000), WikiQA (Yang et al., 2016b) and SelQA (Jurczyk et al., 2016) datasets. Recently, neural networks models (Wang and Nyberg, 2015; Severyn and 216 Figure 5: For a random subset of documents in the development set, we visualized the learned attention over the sentences (p(sl |d, x)). bine supervision signals. Reinforcement learning recently gained popularity in tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al., 2015; He et al., 2016). Moschitti, 2015; dos Santos et al., 2016) achieved improvements on TREC datsaet. Sultan et al. (2016) optimized the answer sentence extraction and the answer extraction jointly, but with gold labels for both parts. Trischler et al. (2016b) proposed a model that shares the intuition of observing inputs at multiple granularities (sentence, word), but deals with multiple choice questions. Our model considers answer sentence selection as latent and generates answer strings instead of selecting text spans, and we found that WIKIREADING dataset suits our purposes best with some p"
P17-1020,D16-1261,0,0.0127765,"reasoning; and SQuAD (Rajpurkar et al., 2016; Wang et al., 2016; Xiong Answer sentence selection is studied with the TREC QA (Voorhees and Tice, 2000), WikiQA (Yang et al., 2016b) and SelQA (Jurczyk et al., 2016) datasets. Recently, neural networks models (Wang and Nyberg, 2015; Severyn and 216 Figure 5: For a random subset of documents in the development set, we visualized the learned attention over the sentences (p(sl |d, x)). bine supervision signals. Reinforcement learning recently gained popularity in tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al., 2015; He et al., 2016). Moschitti, 2015; dos Santos et al., 2016) achieved improvements on TREC datsaet. Sultan et al. (2016) optimized the answer sentence extraction and the answer extraction jointly, but with gold labels for both parts. Trischler et al. (2016b) proposed a model that shares the intuition of observing inputs at multiple granularities (sentence, word), but deals with multiple choice questions. Our model considers answer sentence selection as latent and generates answer strings instead of selecting t"
P17-1020,P15-2116,0,0.0810742,"Missing"
P17-1020,D16-1241,0,0.08793,"014; Yang et al., 2016a). Then, a slow RNN is employed to produce the final answer from the selected sentences. The RNN is run over a fixed number of tokens, regardless of the length of the document. Empirically, our model encodes the Introduction Reading a document and answering questions about its content are among the hallmarks of natural language understanding. Recently, interest in question answering (QA) from unstructured documents has increased along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016; Nguyen et al., 2016; Trischler et al., 2016a). Current state-of-the-art approaches for QA over documents are based on recurrent neural networks † Document (d) Work done while the authors were at Google. 209 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 209–220 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1020 s1 : The 2011 Joplin tornado was a catastrophic EF5rated multiple-vortex tornado that struck Joplin, Missouri . . . d: s4 : It was the third tornado to strik"
P17-1020,D16-1264,0,0.733174,"e question (Yu et al., 2014; Yang et al., 2016a). Then, a slow RNN is employed to produce the final answer from the selected sentences. The RNN is run over a fixed number of tokens, regardless of the length of the document. Empirically, our model encodes the Introduction Reading a document and answering questions about its content are among the hallmarks of natural language understanding. Recently, interest in question answering (QA) from unstructured documents has increased along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016; Nguyen et al., 2016; Trischler et al., 2016a). Current state-of-the-art approaches for QA over documents are based on recurrent neural networks † Document (d) Work done while the authors were at Google. 209 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 209–220 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1020 s1 : The 2011 Joplin tornado was a catastrophic EF5rated multiple-vortex tornado that struck Joplin, Missouri . . . d: s4 : It was the t"
P17-1020,D13-1020,0,0.0431752,"OW and CNN. 7 et al., 2016) and NewsQA (Trischler et al., 2016a) are QA datasets where the answer is a span in the document. Compared to Wikireading, some datasets covers shorter passages (average 122 words for SQuAD). Cloze-style question answering datasets (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2015) assess machine comprehension but do not form questions. The recently released MS MARCO dataset (Nguyen et al., 2016) consists of query logs, web documents and crowd-sourced answers. Related Work There has been substantial interest in datasets for reading comprehension. MCTest (Richardson et al., 2013) is a smaller-scale datasets focusing on common sense reasoning; bAbi (Weston et al., 2015) is a synthetic dataset that captures various aspects of reasoning; and SQuAD (Rajpurkar et al., 2016; Wang et al., 2016; Xiong Answer sentence selection is studied with the TREC QA (Voorhees and Tice, 2000), WikiQA (Yang et al., 2016b) and SelQA (Jurczyk et al., 2016) datasets. Recently, neural networks models (Wang and Nyberg, 2015; Severyn and 216 Figure 5: For a random subset of documents in the development set, we visualized the learned attention over the sentences (p(sl |d, x)). bine supervision si"
P17-1020,P16-1043,0,0.0209428,"et. Hierarchical models which treats sentence selection as a latent variable have been applied text categorization (Yang et al., 2016b), extractive summarization (Cheng and Lapata, 2016), machine translation (Ba et al., 2014) and sentiment analysis (Yessenalina et al., 2010; Lei et al., 2016). To the best of our knowledge, we are the first to use the hierarchical nature of a document for QA. Finally, our work is related to the reinforcement learning literature. Hard and soft attention were examined in the context of caption generation (Xu et al., 2015). Curriculum learning was investigated in Sachan and Xing (2016), but they focused on the ordering of training examples while we com8 Conclusion We presented a coarse-to-fine framework for QA over long documents that quickly focuses on the relevant portions of a document. In future work we would like to deepen the use of structural clues and answer questions over multiple documents, using paragraph structure, titles, sections and more. Incorporating coreference resolution would be another important direction for future work. We argue that this is necessary for developing systems that can efficiently answer the information needs of users over large quantiti"
P17-1020,N16-1174,0,0.180044,"s not parallelizable. In fact, such models usually truncate the documents and consider only a limited number of tokens (Miller et al., 2016; Hewlett et al., 2016). Inspired by studies on how people answer questions by first skimming the document, identifying relevant parts, and carefully reading these parts to produce an answer (Masson, 1983), we propose a coarse-to-fine model for question answering. Our model takes a hierarchical approach (see Figure 1), where first a fast model is used to select a few sentences from the document that are relevant for answering the question (Yu et al., 2014; Yang et al., 2016a). Then, a slow RNN is employed to produce the final answer from the selected sentences. The RNN is run over a fixed number of tokens, regardless of the length of the document. Empirically, our model encodes the Introduction Reading a document and answering questions about its content are among the hallmarks of natural language understanding. Recently, interest in question answering (QA) from unstructured documents has increased along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016; N"
P17-1020,D10-1102,0,0.131596,"ning characteristic of our setup is that an answer does not necessarily appear verbatim in the input (the genre of a movie can be determined even if not mentioned explicitly). Furthermore, the answer often appears multiple times in the document in spurious contexts (the year ‘2012’ can appear many times while only once in relation to the question). Thus, we treat sentence selection as a latent variable that is trained jointly with the answer generation model from the answer only using reinforcement learning. Treating sentence selection as a latent variable has been explored in classification (Yessenalina et al., 2010; Lei et al., 2016), however, to our knowledge, has not been applied for question answering. 3 Data We evaluate on W IKI R EADING, W IKI R EADING L ONG, and a new dataset, W IKI S UGGEST. W IKIREADING (Hewlett et al., 2016) is a QA dataset automatically generated from Wikipedia and Wikidata: given a Wikipedia page about an entity and a Wikidata property, such as PROFES SION , or GENDER , the goal is to infer the target value based on the document. Unlike other recently released large-scale datasets (Rajpurkar et al., 2016; Trischler et al., 2016a), W IKIREAD ING does not annotate answer spans,"
P17-1020,D15-1237,0,\N,Missing
P17-2098,P14-1133,1,0.722811,"with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015). A fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains. To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014). 623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 623–628 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2098 (Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016). We find that by providing the decoder with a representation of the domain, we can train a sing"
P17-2098,Q15-1039,1,0.833763,"h language and logical form are represented with similar abstract vector representations in all domains. We build on their work and examine models that share representations across domains during encoding of language and decoding of logical form, inspired by work on domain adaptation Introduction Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015). A fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains. To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014). 623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 623–628 c Vancouver, Canada, July 30 - August 4, 201"
P17-2098,W10-2903,0,0.141633,"al form, inspired by work on domain adaptation Introduction Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015). A fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains. To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014). 623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 623–628 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2098 (Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016). We"
P17-2098,P07-1033,0,0.117192,"Missing"
P17-2098,P16-1004,0,0.366738,"es from multiple datasets in different domains, each corresponding to a separate knowledge-base (KB), and train a model over all examples. This is motivated by an observation that while KBs differ in their entities and properties, the structure of language composition repeats across domains (Figure 1). E.g., a superlative in language will correspond to an ‘argmax’, and a verb followed by a noun often denotes a join operation. A model that shares information across domains can improve generalization compared to a model that is trained on a single domain only. Recently, Jia and Liang (2016) and Dong and Lapata (2016) proposed sequence-to-sequence models for semantic parsing. Such neural models substantially facilitate information sharing, as both language and logical form are represented with similar abstract vector representations in all domains. We build on their work and examine models that share representations across domains during encoding of language and decoding of logical form, inspired by work on domain adaptation Introduction Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces"
P17-2098,P16-1002,0,0.401937,"solution: to pool examples from multiple datasets in different domains, each corresponding to a separate knowledge-base (KB), and train a model over all examples. This is motivated by an observation that while KBs differ in their entities and properties, the structure of language composition repeats across domains (Figure 1). E.g., a superlative in language will correspond to an ‘argmax’, and a verb followed by a noun often denotes a join operation. A model that shares information across domains can improve generalization compared to a model that is trained on a single domain only. Recently, Jia and Liang (2016) and Dong and Lapata (2016) proposed sequence-to-sequence models for semantic parsing. Such neural models substantially facilitate information sharing, as both language and logical form are represented with similar abstract vector representations in all domains. We build on their work and examine models that share representations across domains during encoding of language and decoding of logical form, inspired by work on domain adaptation Introduction Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing"
P17-2098,Q13-1005,0,0.0475547,"e information sharing, as both language and logical form are represented with similar abstract vector representations in all domains. We build on their work and examine models that share representations across domains during encoding of language and decoding of logical form, inspired by work on domain adaptation Introduction Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015). A fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains. To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014). 623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 623–628 c Vancouver, Canada"
P17-2098,D11-1140,0,0.495149,"ng. Such neural models substantially facilitate information sharing, as both language and logical form are represented with similar abstract vector representations in all domains. We build on their work and examine models that share representations across domains during encoding of language and decoding of logical form, inspired by work on domain adaptation Introduction Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015). A fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains. To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014). 623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
P17-2098,J13-2005,0,0.0786585,"Missing"
P17-2098,P11-1060,0,0.0288147,"stantially facilitate information sharing, as both language and logical form are represented with similar abstract vector representations in all domains. We build on their work and examine models that share representations across domains during encoding of language and decoding of logical form, inspired by work on domain adaptation Introduction Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015). A fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains. To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014). 623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), page"
P17-2098,D15-1166,0,0.0275501,"ontext sensitive embeddings b1 , . . . , bm using a bidirectional RNN (Bahdanau et al., 2015): a forward RNN generates hidden states hF1 , . . . , hFm by applying the LSTM recurrence (Hochreiter and Schmidhuber, 1997): hFi = LST M (φ(in) (xi ), hFi−1 ), where φ(in) is an embedding function mapping a word xi to a fixed-dimensional vector. A backward RNN B similarly generates hidden states hB m , . . . , h1 by processing the input sequence in reverse. Finally, for each input position i, the representation bi is the concatenation [hFi , hB i ] . An attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) generates output tokens one at a time. At each time step j, it generates yj based on the current hidden state sj , then updates the hidden state sj+1 based on sj and yj . Formally, the decoder is defined by the following equations: 3.1 sj+1 = LST M ([φ(out) (yj ), cj , dk ], sj ). (a) eji = s&gt; bi , j W (1) p(yj = w |x, y1:j−1 ) ∝ exp(U [sj , cj ]), sj+1 = LST M ([φ (2) Recently Johnson et al. (2016) used a similar intuition for neural machine translation, where they added an artificial token at the beginning of each source sentence to specify the target language. We implemented their approach"
P17-2098,Q14-1030,0,0.0460092,"loping conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015). A fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains. To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014). 623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 623–628 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2098 (Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016). We find that by providing the decoder with a representation of the domain, we can train a single model over multiple domains and substantially improve accuracy compared to models trained on each domai"
P17-2098,P15-1129,1,0.14768,"ge utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015). A fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains. To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014). 623 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 623–628 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2098 (Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016). We find that by providing the decoder with a representation of the domain, we can train a single model over multip"
P17-2098,D12-1069,0,\N,Missing
P17-2098,N16-1101,0,\N,Missing
P18-1168,P16-1231,0,0.0140047,"t Int Set BoolFunc BoolFunc Bool Bool Bool Bool Item Figure 2: An example for the state of the type stack s while decoding a program z for an utterance x. Given the constrains on valid programs, our model p0θ (z |x) is defined as: Y t p (z |x, z ) · 1(zt |z1:t−1 ) P θ t 0 1:t−1 , 0 p (z | x, z 1:t−1 ) · 1(z |z1:t−1 ) z0 θ where 1(zt |z1:t−1 ) indicates whether a certain program token is valid given the program prefix. Discriminative re-ranking: The above model is a locally-normalized model that provides a distribution for every decoded token, and thus might suffer from the label bias problem (Andor et al., 2016; Lafferty et al., 2001). Thus, we add a globally-normalized re-ranker pψ (z |x) that scores all |B |programs in the final beam produced by p0θ (z |x). Our globally-normalized model is: pgψ (z |x) ∝ exp(sψ (x, z)), and is normalized over all programs in the beam. The scoring function sψ (x, z) is a neural network with identical architecture to the locallynormalized model, except that (a) it feeds the decoder with the candidate program z and does not generate it. (b) the last hidden state is inserted to a feed-forward network whose output is sψ (x, z). Our final ranking score is p0θ (z|x)pgψ (z"
P18-1168,Q13-1005,0,0.0660937,"ning data includes (x, k, y) triplets. supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007). However, collecting such training examples at scale has quickly turned out to be difficult, because expert annotators who are familiar with formal languages are required. This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see Fig. 1). Naturally, collecting denotations is much easier, because it can be performed by non-experts. Introduction The goal of semantic parsing is to map language utterances to executable programs. Early work on statistical learning of semantic parsers utilized ∗ Authors equally contributed to this work. Training semantic parsers from denotations rather than programs complicates training in two ways: (a) Search: The algorithm must le"
P18-1168,D13-1160,1,0.614264,"ults in the correct denotation y. Our training data includes (x, k, y) triplets. supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007). However, collecting such training examples at scale has quickly turned out to be difficult, because expert annotators who are familiar with formal languages are required. This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see Fig. 1). Naturally, collecting denotations is much easier, because it can be performed by non-experts. Introduction The goal of semantic parsing is to map language utterances to executable programs. Early work on statistical learning of semantic parsers utilized ∗ Authors equally contributed to this work. Training semantic parsers from denotations rather than programs complicates trai"
P18-1168,P13-1042,0,0.0518139,"enotation y. Our training data includes (x, k, y) triplets. supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007). However, collecting such training examples at scale has quickly turned out to be difficult, because expert annotators who are familiar with formal languages are required. This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see Fig. 1). Naturally, collecting denotations is much easier, because it can be performed by non-experts. Introduction The goal of semantic parsing is to map language utterances to executable programs. Early work on statistical learning of semantic parsers utilized ∗ Authors equally contributed to this work. Training semantic parsers from denotations rather than programs complicates training in two ways: (a)"
P18-1168,P17-1005,0,0.0452502,"ences that refer to the size of shapes. This is due to the data distribution, which includes many examples of the former case and fewer examples of the latter. 7 Related Work Training semantic parsers from denotations has been one of the most popular training schemes for scaling semantic parsers since the beginning of the decade. Early work focused on traditional log-linear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), but recently denotations have been used to train neural semantic parsers (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017). Visual reasoning has attracted considerable attention, with datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al., 2017a). The advantage of CNLVR is that language utterances are both natural and compositional. Treating visual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b). Our method for generating training data resembles data re-combination ideas in Jia and Liang (2016), where examples are generated automatically by replacing entities with their categories. While spuriousness is central to semantic"
P18-1168,P05-1033,0,0.0288132,"mpute beam of programs of length 1 6: for t = 2 . . . T do // Decode with cache 7: Bt ← construct beam from Bt−1 8: At = truncate(A, t) 9: Bt .add(de-abstract(At )) 10: for z ∈ BT do //Update cache 11: Update rewards in C[¯ x] using (¯ z , R(z, y)) 12: return BT ∪ de-abstract(A). to the program of the first utterance, with IsBlue replacing IsYellow. More generally, we can sample any abstract example and instantiate the abstract clusters that appear in it by sampling pairs of utterance-program tokens for each abstract cluster. Formally, this is equivalent to a synchronous context-free grammar (Chiang, 2005) that has a rule for generating each manually-annotated abstract utteranceprogram pair, and rules for synchronously generating utterance and program tokens from the seven clusters. We generated 6,158 (x, z) examples using this method and trained a standard sequence to sequence parser by maximizing log p0θ (z|x) in the model above. Although these are generated from a small set of 106 abstract utterances, they can be used to learn a model with higher coverage and accuracy compared to the rule-based parser, as our evaluation demonstrates.3 The resulting parser can be used as a standalone semantic"
P18-1168,W10-2903,0,0.166952,"iven an image rendered from a KB k and an utterance x, our goal is to parse x to a program z that results in the correct denotation y. Our training data includes (x, k, y) triplets. supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007). However, collecting such training examples at scale has quickly turned out to be difficult, because expert annotators who are familiar with formal languages are required. This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see Fig. 1). Naturally, collecting denotations is much easier, because it can be performed by non-experts. Introduction The goal of semantic parsing is to map language utterances to executable programs. Early work on statistical learning of semantic parsers utilized ∗ Authors equally contr"
P18-1168,P17-1097,0,0.554929,"at training time, in order to find the correct program. This is a difficult search problem due to the combinatorial nature of the search space. (b) Spurious1809 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1809–1819 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ness: Incorrect programs can lead to correct denotations, and thus the learner can go astray based on these programs. Of the two mentioned problems, spuriousness has attracted relatively less attention (Pasupat and Liang, 2016; Guu et al., 2017). Recently, the Cornell Natural Language for Visual Reasoning corpus (CNLVR) was released (Suhr et al., 2017), and has presented an opportunity to better investigate the problem of spuriousness. In this task, an image with boxes that contains objects of various shapes, colors and sizes is shown. Each image is paired with a complex natural language statement, and the goal is to determine whether the statement is true or false (Fig. 1). The task comes in two flavors, where in one the input is the image (pixels), and in the other it is the knowledge-base (KB) from which the image was synthesized."
P18-1168,P16-1002,0,0.15705,"notations have been used to train neural semantic parsers (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017). Visual reasoning has attracted considerable attention, with datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al., 2017a). The advantage of CNLVR is that language utterances are both natural and compositional. Treating visual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b). Our method for generating training data resembles data re-combination ideas in Jia and Liang (2016), where examples are generated automatically by replacing entities with their categories. While spuriousness is central to semantic parsing when denotations are not very informative, there has been relatively little work on explicitly tackling it. Pasupat and Liang (2015) used manual rules to prune unlikely programs on the W IK I TABLE Q UESTIONS dataset, and then later utilized crowdsourcing (Pasupat and Liang, 2016) to eliminate spurious programs. Guu et al. (2017) proposed R ANDOMER, a method for increasing exploration and handling spuriousness by adding randomness to beam search and a prop"
P18-1168,D17-1160,0,0.146532,"sing. To combat this challenge we apply several techniques. First, we use beam search at decoding time and when training from weak supervision (see Sec. 4), similar to prior work (Liang et al., 2017; Guu et al., 2017). At each decoding step we maintain a beam B of program prefixes of length n, expand them exhaustively to programs of length n+1 and keep the top-|B |program prefixes with highest model probability. Second, we utilize the semantic typing system to only construct programs that are syntactically valid, and substantially prune the program search space (similar to type constraints in Krishnamurthy et al. (2017); Xiao et al. (2016); Liang et al. (2017)). We maintain a stack that keeps track of the expected semantic type at each decoding step. The stack is initialized with the type Bool. Then, at each decoding step, only tokens that return the semantic type at the top of the stack are allowed, the stack is popped, and if the decoded token is a function, the semantic types of its arguments are pushed to the stack. This dramatically reduces the search space and guarantees that only syntactically valid programs will be produced. Fig. 2 illustrates the state of the stack when decoding a program for an inp"
P18-1168,D12-1069,0,0.0198746,"utterance x, our goal is to parse x to a program z that results in the correct denotation y. Our training data includes (x, k, y) triplets. supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007). However, collecting such training examples at scale has quickly turned out to be difficult, because expert annotators who are familiar with formal languages are required. This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see Fig. 1). Naturally, collecting denotations is much easier, because it can be performed by non-experts. Introduction The goal of semantic parsing is to map language utterances to executable programs. Early work on statistical learning of semantic parsers utilized ∗ Authors equally contributed to this work. Training semantic parsers from de"
P18-1168,D13-1161,0,0.17207,"x to a program z that results in the correct denotation y. Our training data includes (x, k, y) triplets. supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007). However, collecting such training examples at scale has quickly turned out to be difficult, because expert annotators who are familiar with formal languages are required. This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see Fig. 1). Naturally, collecting denotations is much easier, because it can be performed by non-experts. Introduction The goal of semantic parsing is to map language utterances to executable programs. Early work on statistical learning of semantic parsers utilized ∗ Authors equally contributed to this work. Training semantic parsers from denotations rather than prog"
P18-1168,P17-1003,1,0.223245,"−1 ) ∝ i &gt; exp(φzt Ws [qt ; ct ]), where φz is an embedding for program token z, vˆ is a bag-of-words vector for the tokens in x, zi:j = (zi , . . . , zj ) is a history vector of size K, the matrices Wq , Wα , Ws are learned parameters (along with the LSTM parameters and embedding matrices), and ’;’ denotes concatenation. Search: Searching through the large space of programs is a fundamental challenge in semantic parsing. To combat this challenge we apply several techniques. First, we use beam search at decoding time and when training from weak supervision (see Sec. 4), similar to prior work (Liang et al., 2017; Guu et al., 2017). At each decoding step we maintain a beam B of program prefixes of length n, expand them exhaustively to programs of length n+1 and keep the top-|B |program prefixes with highest model probability. Second, we utilize the semantic typing system to only construct programs that are syntactically valid, and substantially prune the program search space (similar to type constraints in Krishnamurthy et al. (2017); Xiao et al. (2016); Liang et al. (2017)). We maintain a stack that keeps track of the expected semantic type at each decoding step. The stack is initialized with the typ"
P18-1168,P11-1060,0,0.829361,"d from a KB k and an utterance x, our goal is to parse x to a program z that results in the correct denotation y. Our training data includes (x, k, y) triplets. supervised learning, where training examples included pairs of language utterances and programs (Zelle and Mooney, 1996; Kate et al., 2005; Zettlemoyer and Collins, 2005, 2007). However, collecting such training examples at scale has quickly turned out to be difficult, because expert annotators who are familiar with formal languages are required. This has led to a body of work on weaklysupervised semantic parsing (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013; Cai and Yates, 2013; Artzi and Zettlemoyer, 2013). In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the environment (see Fig. 1). Naturally, collecting denotations is much easier, because it can be performed by non-experts. Introduction The goal of semantic parsing is to map language utterances to executable programs. Early work on statistical learning of semantic parsers utilized ∗ Authors equally contributed to this work."
P18-1168,P15-1142,0,0.0841618,"hnson et al., 2017a). The advantage of CNLVR is that language utterances are both natural and compositional. Treating visual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b). Our method for generating training data resembles data re-combination ideas in Jia and Liang (2016), where examples are generated automatically by replacing entities with their categories. While spuriousness is central to semantic parsing when denotations are not very informative, there has been relatively little work on explicitly tackling it. Pasupat and Liang (2015) used manual rules to prune unlikely programs on the W IK I TABLE Q UESTIONS dataset, and then later utilized crowdsourcing (Pasupat and Liang, 2016) to eliminate spurious programs. Guu et al. (2017) proposed R ANDOMER, a method for increasing exploration and handling spuriousness by adding randomness to beam search and a proposing a “meritocratic” weighting scheme for gradients. In our work we found that random exploration during beam search did not improve results while meritocratic updates slightly improved performance. 8 Discussion In this work we presented the first semantic parser for th"
P18-1168,P16-1003,0,0.792218,"he huge space of programs at training time, in order to find the correct program. This is a difficult search problem due to the combinatorial nature of the search space. (b) Spurious1809 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1809–1819 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ness: Incorrect programs can lead to correct denotations, and thus the learner can go astray based on these programs. Of the two mentioned problems, spuriousness has attracted relatively less attention (Pasupat and Liang, 2016; Guu et al., 2017). Recently, the Cornell Natural Language for Visual Reasoning corpus (CNLVR) was released (Suhr et al., 2017), and has presented an opportunity to better investigate the problem of spuriousness. In this task, an image with boxes that contains objects of various shapes, colors and sizes is shown. Each image is paired with a complex natural language statement, and the goal is to determine whether the statement is true or false (Fig. 1). The task comes in two flavors, where in one the input is the image (pixels), and in the other it is the knowledge-base (KB) from which the ima"
P18-1168,P17-1105,0,0.102385,", but struggled with sentences that refer to the size of shapes. This is due to the data distribution, which includes many examples of the former case and fewer examples of the latter. 7 Related Work Training semantic parsers from denotations has been one of the most popular training schemes for scaling semantic parsers since the beginning of the decade. Early work focused on traditional log-linear models (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013), but recently denotations have been used to train neural semantic parsers (Liang et al., 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017; Cheng et al., 2017). Visual reasoning has attracted considerable attention, with datasets such as VQA (Antol et al., 2015) and CLEVR (Johnson et al., 2017a). The advantage of CNLVR is that language utterances are both natural and compositional. Treating visual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR (Hu et al., 2017; Johnson et al., 2017b). Our method for generating training data resembles data re-combination ideas in Jia and Liang (2016), where examples are generated automatically by replacing entities with their categories. While spuriousness i"
P18-1168,P17-2034,0,0.0636273,"with learning rate of 0.001, and mini-batch size of 8. use their lemmatized form. We also use spelling correction to replace words that contain typos. After pre-processing we replace every word that occurs less than 5 times with an UNK symbol. Pre-processing Because the number of utterances is relatively small for training a neural model, we take the following steps to reduce sparsity. We lowercase all utterance tokens, and also Baselines We compare our models to the M A JORITY baseline that picks the majority class (T RUE in our case). We also compare to the stateof-the-art model reported by Suhr et al. (2017) Evaluation We evaluate on the public development and test sets of CNLVR as well as on the hidden test set. The standard evaluation metric is accuracy, i.e., how many examples are correctly classified. In addition, we report consistency, which is the proportion of utterances for which the decoded program has the correct denotation for all 4 images/KBs. It captures whether a model consistently produces a correct answer. 1815 Model M AJORITY M AX E NT RULE S UP. S UP.+D ISC W EAK S UP. W.+D ISC Acc. 55.3 68.0 66.0 67.7 77.7 84.3 85.7 Dev. Con. 29.2 36.7 52.4 66.3 67.4 Test-P Acc. Con. 56.2 67.7"
P18-1168,P16-1127,0,0.0277062,"e we apply several techniques. First, we use beam search at decoding time and when training from weak supervision (see Sec. 4), similar to prior work (Liang et al., 2017; Guu et al., 2017). At each decoding step we maintain a beam B of program prefixes of length n, expand them exhaustively to programs of length n+1 and keep the top-|B |program prefixes with highest model probability. Second, we utilize the semantic typing system to only construct programs that are syntactically valid, and substantially prune the program search space (similar to type constraints in Krishnamurthy et al. (2017); Xiao et al. (2016); Liang et al. (2017)). We maintain a stack that keeps track of the expected semantic type at each decoding step. The stack is initialized with the type Bool. Then, at each decoding step, only tokens that return the semantic type at the top of the stack are allowed, the stack is popped, and if the decoded token is a function, the semantic types of its arguments are pushed to the stack. This dramatically reduces the search space and guarantees that only syntactically valid programs will be produced. Fig. 2 illustrates the state of the stack when decoding a program for an input utterance. 1811 x"
P18-1168,D07-1071,0,0.65515,"Missing"
P19-1448,P17-1005,0,0.0439472,"1 Types are tables, string columns, number columns, etc. Encoder A Bidirectional LSTM (Hochreiter and Schmidhuber, 1997) provides a contextualized representation hi for each question word xi . Importantly, the encoder input at time step i is [wxi ; li ]: the concatenation of the word embedding P P for xi and li = τ v∈Vτ plink (v |xi ) · rv , where rv is a learned embedding for the schema item v, based on the type of v and its schema neighbors. Thus, plink (v |xi ) augments every word xi with information on the schema items it should link to. Decoder We use a grammar-based (Xiao et al., 2016; Cheng et al., 2017; Yin and Neubig, 2017; Rabinovich et al., 2017) LSTM decoder with attention on the input question (Figure 2). At each decoding step, a non-terminal of type τ is expanded using one of the grammar rules. Rules are either schema-independent and generate nonterminals or SQL keywords, or schema-specific and generate schema items. At each decoding step j, the decoding LSTM takes a vector gj as input, which is an embedding of the grammar rule decoded in the previous step, and outputs a vector oj . If this rule is schemaindependent, gj is a learned global embedding. If it is schema-specific, i.e., a"
P19-1448,D14-1179,0,0.0555322,"Missing"
P19-1448,N19-1240,0,0.0708628,"Missing"
P19-1448,P18-1033,0,0.208367,"Missing"
P19-1448,W18-2501,1,0.823585,"Missing"
P19-1448,P17-1089,0,0.235403,"Missing"
P19-1448,D17-1160,1,0.921424,"Missing"
P19-1448,P17-1105,0,0.0425241,"r columns, etc. Encoder A Bidirectional LSTM (Hochreiter and Schmidhuber, 1997) provides a contextualized representation hi for each question word xi . Importantly, the encoder input at time step i is [wxi ; li ]: the concatenation of the word embedding P P for xi and li = τ v∈Vτ plink (v |xi ) · rv , where rv is a learned embedding for the schema item v, based on the type of v and its schema neighbors. Thus, plink (v |xi ) augments every word xi with information on the schema items it should link to. Decoder We use a grammar-based (Xiao et al., 2016; Cheng et al., 2017; Yin and Neubig, 2017; Rabinovich et al., 2017) LSTM decoder with attention on the input question (Figure 2). At each decoding step, a non-terminal of type τ is expanded using one of the grammar rules. Rules are either schema-independent and generate nonterminals or SQL keywords, or schema-specific and generate schema items. At each decoding step j, the decoding LSTM takes a vector gj as input, which is an embedding of the grammar rule decoded in the previous step, and outputs a vector oj . If this rule is schemaindependent, gj is a learned global embedding. If it is schema-specific, i.e., a schema item v was generated, gj is a learned emb"
P19-1448,C18-1280,0,0.0545797,"Missing"
P19-1448,P16-1127,0,0.0613657,"y-crafted features. 1 Types are tables, string columns, number columns, etc. Encoder A Bidirectional LSTM (Hochreiter and Schmidhuber, 1997) provides a contextualized representation hi for each question word xi . Importantly, the encoder input at time step i is [wxi ; li ]: the concatenation of the word embedding P P for xi and li = τ v∈Vτ plink (v |xi ) · rv , where rv is a learned embedding for the schema item v, based on the type of v and its schema neighbors. Thus, plink (v |xi ) augments every word xi with information on the schema items it should link to. Decoder We use a grammar-based (Xiao et al., 2016; Cheng et al., 2017; Yin and Neubig, 2017; Rabinovich et al., 2017) LSTM decoder with attention on the input question (Figure 2). At each decoding step, a non-terminal of type τ is expanded using one of the grammar rules. Rules are either schema-independent and generate nonterminals or SQL keywords, or schema-specific and generate schema items. At each decoding step j, the decoding LSTM takes a vector gj as input, which is an embedding of the grammar rule decoded in the previous step, and outputs a vector oj . If this rule is schemaindependent, gj is a learned global embedding. If it is schem"
P19-1448,P17-1041,0,0.0712993,"string columns, number columns, etc. Encoder A Bidirectional LSTM (Hochreiter and Schmidhuber, 1997) provides a contextualized representation hi for each question word xi . Importantly, the encoder input at time step i is [wxi ; li ]: the concatenation of the word embedding P P for xi and li = τ v∈Vτ plink (v |xi ) · rv , where rv is a learned embedding for the schema item v, based on the type of v and its schema neighbors. Thus, plink (v |xi ) augments every word xi with information on the schema items it should link to. Decoder We use a grammar-based (Xiao et al., 2016; Cheng et al., 2017; Yin and Neubig, 2017; Rabinovich et al., 2017) LSTM decoder with attention on the input question (Figure 2). At each decoding step, a non-terminal of type τ is expanded using one of the grammar rules. Rules are either schema-independent and generate nonterminals or SQL keywords, or schema-specific and generate schema items. At each decoding step j, the decoding LSTM takes a vector gj as input, which is an embedding of the grammar rule decoded in the previous step, and outputs a vector oj . If this rule is schemaindependent, gj is a learned global embedding. If it is schema-specific, i.e., a schema item v was gene"
P19-1448,D18-1193,0,0.118416,"Missing"
P19-1448,D18-1425,0,0.229563,"Missing"
P19-1485,C16-1236,0,0.194777,"ork (and term this TQA-U). In addition, we replace Bing web snippets with Google web snippets (and term this TQA-G). 5. H OTPOT QA (Yang et al., 2018): Crowdsourcing workers were shown pairs of related Wikipedia paragraphs and asked to author questions that require multi-hop reasoning over the paragraphs. There are two versions of H OTPOT QA: the first where the context includes the two gold paragraphs and eight “distractor” paragraphs, and a second, where 10 paragraphs retrieved by an information retrieval (IR) system are given. Here, we use the latter version. The small datasets are: 1. CQ (Bao et al., 2016): Questions are real Google web queries crawled from Google Suggest, originally constructed for querying the KB Freebase (Bollacker et al., 2008). However, the dataset was also used as a RC task with retrieved web snippets (Talmor et al., 2017). 2. CWQ (Talmor and Berant, 2018c): Crowdsourcing workers were shown compositional formal queries against Freebase and were asked to re-phrase them in natural language. Thus, questions require multi-hop reasoning. The original work assumed models contain an IR component, but the authors also provided default web snippets, which we use here. The repartit"
P19-1485,D14-1159,1,0.826906,"al representations from BERT (Devlin et al., 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose M ULTI QA, a BERTbased model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community. 1 Introduction Reading comprehension (RC) is concerned with reading a piece of text and answering questions about it (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Rajpurkar et al., 2016). Its appeal stems both from the clear application it proposes, but also from the fact that it allows to probe many aspects of language understanding, simply by posing questions on a text document. Indeed, this has led to the creation of a large number of RC datasets in recent years. While each RC dataset has a different focus, there is still substantial overlap in the abilities required to answer questions across these datasets. Nevertheless, there has been relatively little work (Min et al., 2017; Chung et al., 2018; Sun et al., 2018) that explo"
P19-1485,D18-1241,0,0.0282032,"atasets. The uniform format datasets can be downloaded from www.tau-nlp.org/multiqa. The code for the AllenNLP models is available at http://github.com/alontalmor/ multiqa. 2 Datasets We describe the 10 datasets used for our investigation. Each dataset provides question-contextanswer triples {(qi , ci , ai )}N i=1 for training, and a model maps an unseen question-context pair (q, c) to an answer a. For simplicity, we focus on the single-turn extractive setting, where the answer a is a span in the context c. Thus, we do not evaluate abstractive (Nguyen et al., 2016) or conversational datasets (Choi et al., 2018; Reddy et al., 2018). We broadly distinguish large datasets that include more than 75K examples, from small datasets that contain less than 75K examples. In §4, we will fix the size of the large datasets to control for size effects, and always train on exactly 75K examples per dataset. We now shortly describe the datasets, and provide a summary of their characteristics in Table 1. The table shows the original size of each dataset, the source for the context, how questions were generated, and whether the dataset was specifically designed to probe multi-hop reasoning. The large datasets used ar"
P19-1485,D17-1215,0,0.0940464,"such as C OM QA, CWQ, and CQ. Moreover, in N EWS QA, M UL TI QA surpasses human performance as measured by the creators of those datasets. (46.5 EM, 69.4 F1) (Trischler et al., 2017)), improving upon previous state-of-the-art by a large margin. To conclude, M ULTI QA is able to improve state-of-the-art performance on multiple datasets. Our results suggest that in many NLU tasks the size of the dataset is the main bottleneck rather than the model itself. Does training on multiple datasets improve resiliency against adversarial attacks? Finally, we evaluated M ULTI QA on the adversarial SQ UAD (Jia and Liang, 2017), where a misleading sentence is appended to each conshows roughly 67 F1 on the development set, but no exact number. For CQ we compare against SOTA achieved on the web snippets context. On the Freebase context SOTA is 42.8 F1 . (Luo1 et al., 2018) 4918 Dataset C OM QA CQ BERT-large Dev. Prec. Rec. F1 45.8 42.0 42.9 32.8 M ULTI QA Dev. Prec. Rec. F1 51.9 47.2 48.2 46.6 M ULTI QA Test Prec. Rec. F1 44.4 40.0 40.8 42.4 Prec. 21.2 - SOTA Rec. 38.4 - F1 22.4 39.72 Table 7: Results for datasets where the evaluation metric is average recall/precision/F1 . CQ evaluates with F1 only. dataset , even in"
P19-1485,N19-1213,0,0.0585399,"Missing"
P19-1485,P17-1147,0,0.083439,"ng workers were shown Wikipedia paragraphs and were asked to author questions about their content. Questions mostly require soft matching of the language in the question to a local context in the text. 2. N EWS QA (Trischler et al., 2017): Crowdsourcing workers were shown a CNN article (longer than SQ UAD) and were asked to au4912 thor questions about its content. 3. S EARCH QA (Dunn et al., 2017): Trivia questions were taken from Jeopardy! TV show, and contexts are web snippets retrieved from Google search engine for those questions, with an average of 50 snippets per question. 4. T RIVIAQA (Joshi et al., 2017): Trivia questions were crawled from the web. In one variant of T RIVIAQA (termed TQA-W), Wikipedia pages related to the questions are provided for each question. In another, web snippets and documents from Bing search engine are given. For the latter variant, we use only the web snippets in this work (and term this TQA-U). In addition, we replace Bing web snippets with Google web snippets (and term this TQA-G). 5. H OTPOT QA (Yang et al., 2018): Crowdsourcing workers were shown pairs of related Wikipedia paragraphs and asked to author questions that require multi-hop reasoning over the paragr"
P19-1485,N18-1143,0,0.115827,"it (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Rajpurkar et al., 2016). Its appeal stems both from the clear application it proposes, but also from the fact that it allows to probe many aspects of language understanding, simply by posing questions on a text document. Indeed, this has led to the creation of a large number of RC datasets in recent years. While each RC dataset has a different focus, there is still substantial overlap in the abilities required to answer questions across these datasets. Nevertheless, there has been relatively little work (Min et al., 2017; Chung et al., 2018; Sun et al., 2018) that explores the relations between the different datasets, including whether a model trained on one dataset generalizes to another. This research gap is highlighted by the increasing interest in developing and evaluating the generalization of language understanding models to new setups (Yogatama et al., 2019; Liu et al., 2019). In this work, we conduct a thorough empirical analysis of generalization and transfer across 10 RC benchmarks. We train models on one or more source RC datasets, and then evaluate their performance on a target test set, either without any additional"
P19-1485,D17-1082,0,0.0946285,"Missing"
P19-1485,P18-1078,0,0.295289,"ne dataset generalizes to another. This research gap is highlighted by the increasing interest in developing and evaluating the generalization of language understanding models to new setups (Yogatama et al., 2019; Liu et al., 2019). In this work, we conduct a thorough empirical analysis of generalization and transfer across 10 RC benchmarks. We train models on one or more source RC datasets, and then evaluate their performance on a target test set, either without any additional target training examples (generalization) or with additional target examples (transfer). We experiment with D OC QA (Clark and Gardner, 2018), a standard and popular RC model, as well as a model based on BERT (Devlin et al., 2019), which provides powerful contextual representations. Our generalization analysis confirms findings that current models over-fit to the particular training set and generalize poorly even to similar datasets. Moreover, BERT representations substantially improve generalization. However, we find that the contribution of BERT is much more pronounced on Wikipedia (which BERT was trained on) and Newswire, but quite moderate when documents are taken from web snippets. We also analyze the main causes for poor gene"
P19-1485,P18-1161,0,0.0277674,"). Table 7 shows results for datasets where the evaluation metric is average recall/precision/F1 between the list of predicted answers and the list of gold answers. We compare M ULTI QA to BERT-large, a model that does not train on M ULTI -375K, but only fine-tunes BERT-large on the target dataset. We also show the state-of-the-art (SOTA) result for all datasets for reference.1 M ULTI QA improves state-of-the-art performance on fivedatasets, although it does not even train on all examples in the large datasets.2 M UL 1 State-of-the-are-results were found in (Tay et al., 2018) for N EWS QA, in Lin et al. (2018), for S EARCH QA, in Das et al. (2019) for TQA-U, in (Talmor and Berant, 2018b) for CWQ, in Ding et al. (2019) for H OTPOT QA, in (Abujabal et al., 2018) for C OM QA, and in Bao et al. (2016) for CQ. 2 We compare only to models for which we found a publication. For TQA-U, Figure 4 in Clark and Gardner (2018) TI QA improves performance compared to BERTlarge in all cases. This improvement is especially noticeable in small datasets such as C OM QA, CWQ, and CQ. Moreover, in N EWS QA, M UL TI QA surpasses human performance as measured by the creators of those datasets. (46.5 EM, 69.4 F1) (Trischle"
P19-1485,P19-1441,0,0.0260277,"RC datasets in recent years. While each RC dataset has a different focus, there is still substantial overlap in the abilities required to answer questions across these datasets. Nevertheless, there has been relatively little work (Min et al., 2017; Chung et al., 2018; Sun et al., 2018) that explores the relations between the different datasets, including whether a model trained on one dataset generalizes to another. This research gap is highlighted by the increasing interest in developing and evaluating the generalization of language understanding models to new setups (Yogatama et al., 2019; Liu et al., 2019). In this work, we conduct a thorough empirical analysis of generalization and transfer across 10 RC benchmarks. We train models on one or more source RC datasets, and then evaluate their performance on a target test set, either without any additional target training examples (generalization) or with additional target examples (transfer). We experiment with D OC QA (Clark and Gardner, 2018), a standard and popular RC model, as well as a model based on BERT (Devlin et al., 2019), which provides powerful contextual representations. Our generalization analysis confirms findings that current model"
P19-1485,N19-1423,0,0.613181,"nalysis has been done on whether they generalize to one another, and the extent to which existing datasets can be leveraged for improving performance on new ones. In this paper, we conduct such an investigation over ten RC datasets, training on one or more source RC datasets, and evaluating generalization, as well as transfer to a target RC dataset. We analyze the factors that contribute to generalization, and show that training on a source RC dataset and transferring to a target dataset substantially improves performance, even in the presence of powerful contextual representations from BERT (Devlin et al., 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose M ULTI QA, a BERTbased model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community. 1 Introduction Reading comprehension (RC) is concerned with reading a piece of text and answering questions about it (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Rajpurk"
P19-1485,D18-1242,0,0.0436101,"Missing"
P19-1485,P19-1259,0,0.0128546,"list of predicted answers and the list of gold answers. We compare M ULTI QA to BERT-large, a model that does not train on M ULTI -375K, but only fine-tunes BERT-large on the target dataset. We also show the state-of-the-art (SOTA) result for all datasets for reference.1 M ULTI QA improves state-of-the-art performance on fivedatasets, although it does not even train on all examples in the large datasets.2 M UL 1 State-of-the-are-results were found in (Tay et al., 2018) for N EWS QA, in Lin et al. (2018), for S EARCH QA, in Das et al. (2019) for TQA-U, in (Talmor and Berant, 2018b) for CWQ, in Ding et al. (2019) for H OTPOT QA, in (Abujabal et al., 2018) for C OM QA, and in Bao et al. (2016) for CQ. 2 We compare only to models for which we found a publication. For TQA-U, Figure 4 in Clark and Gardner (2018) TI QA improves performance compared to BERTlarge in all cases. This improvement is especially noticeable in small datasets such as C OM QA, CWQ, and CQ. Moreover, in N EWS QA, M UL TI QA surpasses human performance as measured by the creators of those datasets. (46.5 EM, 69.4 F1) (Trischler et al., 2017)), improving upon previous state-of-the-art by a large margin. To conclude, M ULTI QA is able t"
P19-1485,N19-1246,0,0.0839062,"ch we use here. The repartitioned version 1.1 was used. (Talmor and Berant, 2018a) 3. W IKI H OP (Welbl et al., 2017) Questions are entity-relation pairs from Freebase, and are not phrased in natural language. Multiple Wikipedia paragraphs are given as context, and the dataset was constructed such that multi-hop reasoning is needed for answering the question. 4. C OM QA (Abujabal et al., 2018): Questions are real user questions from the WikiAnswers community QA platform. No contexts are provided, and thus we augment the questions with web snippets retrieved from Google search engine. 5. DROP (Dua et al., 2019): Contexts are Wikipedia paragraphs and questions are authored by crowdsourcing workers. This dataset focuses on quantitative reasoning. Because most questions are not extractive, we only use the 33,573 extractive examples in the dataset (but evaluate on the entire development set). 3 Models We carry our empirical investigation using two models. The first is D OC QA (Clark and Gardner, 2018), and the second is based on BERT (Devlin et al., 2019), which we term B ERT QA. We now describe the pre-processing on the datasets, and provide a brief description of the models. We emphasize that in all o"
P19-1485,P17-2081,0,0.0208763,"g questions about it (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Rajpurkar et al., 2016). Its appeal stems both from the clear application it proposes, but also from the fact that it allows to probe many aspects of language understanding, simply by posing questions on a text document. Indeed, this has led to the creation of a large number of RC datasets in recent years. While each RC dataset has a different focus, there is still substantial overlap in the abilities required to answer questions across these datasets. Nevertheless, there has been relatively little work (Min et al., 2017; Chung et al., 2018; Sun et al., 2018) that explores the relations between the different datasets, including whether a model trained on one dataset generalizes to another. This research gap is highlighted by the increasing interest in developing and evaluating the generalization of language understanding models to new setups (Yogatama et al., 2019; Liu et al., 2019). In this work, we conduct a thorough empirical analysis of generalization and transfer across 10 RC benchmarks. We train models on one or more source RC datasets, and then evaluate their performance on a target test set, either wi"
P19-1485,W18-2501,0,0.0164069,"oreover, we find that when using the high capacity BERT-large, one can train a single model on multiple RC datasets, and obtain close to or better than state-of-the-art performance on all of them, without fine-tuning to a particular dataset. Armed with the above insights, we train a large RC model on multiple RC datasets, termed M UL TI QA. Our model leads to new state-of-the-art results on five datasets, suggesting that in many language understanding tasks the size of the dataset is the main bottleneck, rather than the model itself. Last, we have developed infrastructure (on top of AllenNLP (Gardner et al., 2018)), where experimenting with multiple models on multiple RC datasets, mixing datasets, and performing finetuning, are trivial. It is also simple to expand the infrastructure to new datasets and new setups (abstractive RC, multi-choice, etc.). We will open source our infrastructure, which will help researchers evaluate models on a large number of datasets, and gain insight on the strengths and shortcoming of their methods. We hope this will accelerate progress in language understanding. To conclude, we perform a thorough investigation of generalization and transfer in reading comprehension over"
P19-1485,D18-1232,0,0.015326,"48.2 46.6 M ULTI QA Test Prec. Rec. F1 44.4 40.0 40.8 42.4 Prec. 21.2 - SOTA Rec. 38.4 - F1 22.4 39.72 Table 7: Results for datasets where the evaluation metric is average recall/precision/F1 . CQ evaluates with F1 only. dataset , even in the presence of BERT representatext (A DD S ENT variant). M ULTI QA obtained tions. It also leads to substantial reduction in the 66.7 EM and 73.1 F1 , outperforming BERT-large number of necessary training examples for a fixed (60.4EM, 66.3F1) by a significant margin, and performance. also substantially improving state-of-the-art results (56.0 EM, 61.3 F1 , (Hu et al., 2018) and 52.1 • Training the high-capacity BERT-large representations over multiple RC datasets leads to good EM, 62.7 F1 , (Wang et al., 2018)). performance on all of the trained datasets without 6 Related Work having to fine-tune on each dataset separately. • BERT representations improve generalization, but Prior work has shown that RC performance can be their effect is moderate when the source of the improved by training on a large dataset and transcontext is web snippets compared to Wikipedia ferring to a smaller one, but at a small scale (Min and newswire. et al., 2017; Chung et al., 2018). S"
P19-1485,D14-1162,0,0.0818428,"Missing"
P19-1485,N18-1202,0,0.0315332,"d has recently shown this in a larger experiment for by retrieving web snippets for all questions and multi-choice questions, where they first fine-tuned adding them as examples (context augmentation). BERT on RACE (Lai et al., 2017) and then finetuned on several smaller datasets. Acknowledgments Interest in learning general-purpose representaWe thank the anonymous reviewers for their contions for natural language through unsupervised, structive feedback. This work was completed in multi-task and transfer learning has been skypartial fulfillment for the PhD degree of Alon Talrocketing lately (Peters et al., 2018; Radford et al., mor. This research was partially supported by 2018; McCann et al., 2018; Chronopoulou et al., The Israel Science Foundation grant 942/16, The 2019; Phang et al., 2018; Wang et al., 2019; Xu Blavatnik Computer Science Research Fund and et al., 2019). In parallel to our work, studies that The Yandex Initiative for Machine Learning. focus on generalization have appeared on publication servers, empirically studying generalization to multiple tasks (Yogatama et al., 2019; Liu et al., References 2019). Our work is part of this research thread on generalization in natural langauge u"
P19-1485,D16-1264,0,0.510723,", 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose M ULTI QA, a BERTbased model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community. 1 Introduction Reading comprehension (RC) is concerned with reading a piece of text and answering questions about it (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Rajpurkar et al., 2016). Its appeal stems both from the clear application it proposes, but also from the fact that it allows to probe many aspects of language understanding, simply by posing questions on a text document. Indeed, this has led to the creation of a large number of RC datasets in recent years. While each RC dataset has a different focus, there is still substantial overlap in the abilities required to answer questions across these datasets. Nevertheless, there has been relatively little work (Min et al., 2017; Chung et al., 2018; Sun et al., 2018) that explores the relations between the different dataset"
P19-1485,N19-1271,0,0.0328474,"Missing"
P19-1485,D18-1259,0,0.0657517,"ow, and contexts are web snippets retrieved from Google search engine for those questions, with an average of 50 snippets per question. 4. T RIVIAQA (Joshi et al., 2017): Trivia questions were crawled from the web. In one variant of T RIVIAQA (termed TQA-W), Wikipedia pages related to the questions are provided for each question. In another, web snippets and documents from Bing search engine are given. For the latter variant, we use only the web snippets in this work (and term this TQA-U). In addition, we replace Bing web snippets with Google web snippets (and term this TQA-G). 5. H OTPOT QA (Yang et al., 2018): Crowdsourcing workers were shown pairs of related Wikipedia paragraphs and asked to author questions that require multi-hop reasoning over the paragraphs. There are two versions of H OTPOT QA: the first where the context includes the two gold paragraphs and eight “distractor” paragraphs, and a second, where 10 paragraphs retrieved by an information retrieval (IR) system are given. Here, we use the latter version. The small datasets are: 1. CQ (Bao et al., 2016): Questions are real Google web queries crawled from Google Suggest, originally constructed for querying the KB Freebase (Bollacker e"
P19-1485,D13-1020,0,0.0720787,"ence of powerful contextual representations from BERT (Devlin et al., 2019). We also find that training on multiple source RC datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new RC dataset. Following our analysis, we propose M ULTI QA, a BERTbased model, trained on multiple RC datasets, which leads to state-of-the-art performance on five RC datasets. We share our infrastructure for the benefit of the research community. 1 Introduction Reading comprehension (RC) is concerned with reading a piece of text and answering questions about it (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Rajpurkar et al., 2016). Its appeal stems both from the clear application it proposes, but also from the fact that it allows to probe many aspects of language understanding, simply by posing questions on a text document. Indeed, this has led to the creation of a large number of RC datasets in recent years. While each RC dataset has a different focus, there is still substantial overlap in the abilities required to answer questions across these datasets. Nevertheless, there has been relatively little work (Min et al., 2017; Chung et al., 2018; Sun et"
P19-1485,N18-1059,1,0.934121,"-hop reasoning over the paragraphs. There are two versions of H OTPOT QA: the first where the context includes the two gold paragraphs and eight “distractor” paragraphs, and a second, where 10 paragraphs retrieved by an information retrieval (IR) system are given. Here, we use the latter version. The small datasets are: 1. CQ (Bao et al., 2016): Questions are real Google web queries crawled from Google Suggest, originally constructed for querying the KB Freebase (Bollacker et al., 2008). However, the dataset was also used as a RC task with retrieved web snippets (Talmor et al., 2017). 2. CWQ (Talmor and Berant, 2018c): Crowdsourcing workers were shown compositional formal queries against Freebase and were asked to re-phrase them in natural language. Thus, questions require multi-hop reasoning. The original work assumed models contain an IR component, but the authors also provided default web snippets, which we use here. The repartitioned version 1.1 was used. (Talmor and Berant, 2018a) 3. W IKI H OP (Welbl et al., 2017) Questions are entity-relation pairs from Freebase, and are not phrased in natural language. Multiple Wikipedia paragraphs are given as context, and the dataset was constructed such that m"
P19-1485,S17-1020,1,0.847511,"r questions that require multi-hop reasoning over the paragraphs. There are two versions of H OTPOT QA: the first where the context includes the two gold paragraphs and eight “distractor” paragraphs, and a second, where 10 paragraphs retrieved by an information retrieval (IR) system are given. Here, we use the latter version. The small datasets are: 1. CQ (Bao et al., 2016): Questions are real Google web queries crawled from Google Suggest, originally constructed for querying the KB Freebase (Bollacker et al., 2008). However, the dataset was also used as a RC task with retrieved web snippets (Talmor et al., 2017). 2. CWQ (Talmor and Berant, 2018c): Crowdsourcing workers were shown compositional formal queries against Freebase and were asked to re-phrase them in natural language. Thus, questions require multi-hop reasoning. The original work assumed models contain an IR component, but the authors also provided default web snippets, which we use here. The repartitioned version 1.1 was used. (Talmor and Berant, 2018a) 3. W IKI H OP (Welbl et al., 2017) Questions are entity-relation pairs from Freebase, and are not phrased in natural language. Multiple Wikipedia paragraphs are given as context, and the da"
P19-1485,W17-2623,0,0.356992,"examples per dataset. We now shortly describe the datasets, and provide a summary of their characteristics in Table 1. The table shows the original size of each dataset, the source for the context, how questions were generated, and whether the dataset was specifically designed to probe multi-hop reasoning. The large datasets used are: 1. SQ UAD (Rajpurkar et al., 2016): Crowdsourcing workers were shown Wikipedia paragraphs and were asked to author questions about their content. Questions mostly require soft matching of the language in the question to a local context in the text. 2. N EWS QA (Trischler et al., 2017): Crowdsourcing workers were shown a CNN article (longer than SQ UAD) and were asked to au4912 thor questions about its content. 3. S EARCH QA (Dunn et al., 2017): Trivia questions were taken from Jeopardy! TV show, and contexts are web snippets retrieved from Google search engine for those questions, with an average of 50 snippets per question. 4. T RIVIAQA (Joshi et al., 2017): Trivia questions were crawled from the web. In one variant of T RIVIAQA (termed TQA-W), Wikipedia pages related to the questions are provided for each question. In another, web snippets and documents from Bing search"
P19-1485,P18-1158,0,0.0122035,"e the evaluation metric is average recall/precision/F1 . CQ evaluates with F1 only. dataset , even in the presence of BERT representatext (A DD S ENT variant). M ULTI QA obtained tions. It also leads to substantial reduction in the 66.7 EM and 73.1 F1 , outperforming BERT-large number of necessary training examples for a fixed (60.4EM, 66.3F1) by a significant margin, and performance. also substantially improving state-of-the-art results (56.0 EM, 61.3 F1 , (Hu et al., 2018) and 52.1 • Training the high-capacity BERT-large representations over multiple RC datasets leads to good EM, 62.7 F1 , (Wang et al., 2018)). performance on all of the trained datasets without 6 Related Work having to fine-tune on each dataset separately. • BERT representations improve generalization, but Prior work has shown that RC performance can be their effect is moderate when the source of the improved by training on a large dataset and transcontext is web snippets compared to Wikipedia ferring to a smaller one, but at a small scale (Min and newswire. et al., 2017; Chung et al., 2018). Sun et al. (2018) • Performance over an RC dataset can be improved has recently shown this in a larger experiment for by retrieving web snip"
Q15-1039,P11-1158,0,0.0165153,"e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order parsing, we turn to agenda-based parsing (Kay, 1986; Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for structured prediction (Daume et al., 2009; Ross et al., 2011; Goldberg and"
Q15-1039,P14-1091,0,0.674948,"Missing"
Q15-1039,P14-1133,1,0.716721,"the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the structure of the natural language and the logical langua"
Q15-1039,D13-1160,1,0.830462,"0; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the structure of the natural language and the logical language. As a result, the space of possible semantic parses for even a short utterance grows quickly. For example, consider the utterance “what city was abraham lincoln born in”. Figure 1 illustrates the number of possible semantic parses that can be constructed over some of the uttera"
Q15-1039,P11-1045,0,0.120641,"as F IXED O RDER with beam size 10–20. For the chosen beam size (K = 200), AGENDA IL is 9x faster than F IXE D O RDER . For K = 1, performance is poor for AGENDA IL and zero for F IXED O RDER. This highlights the inherent difficulty of mapping to logical forms compared to more shallow tasks, as maintaining just a single best derivation for each parsing state is not sufficient. A common variant on beam parsing is to replace the fixed beam size K with a threshold α, and prune any derivation whose probability is at least α times smaller than the best derivation in that state (Zhang et al., 2010; Bodenstab et al., 2011). We implemented this baseline and compared it to AGENDA IL 30 FixedOrder AgendaIL 25 3.0 30 20 2.5 2.0 1.5 1.0 10 0 1 3.5 time (sec) accuracy 40 4.0 FixedOrder AgendaIL #derivations (thousands) 50 0.0 1 20 15 10 5 0.5 10 20 50 100 200 400 beam size FixedOrder AgendaIL (scored) AgendaIL (popped) 10 20 50 100 200 400 beam size 0 1 10 20 50 100 200 400 beam size Figure 8: Comparing AGENDA IL and F IXED O RDER for various beam sizes (left: accuracy, middle: parsing time at test time in seconds, right: number of thousands of derivations scored and popped). The x-axis is on a logarithmic scale. and"
Q15-1039,D14-1067,0,0.277654,"Missing"
Q15-1039,P13-1042,0,0.223438,"this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the struc"
Q15-1039,J98-2004,0,0.501273,"d beam search, where the number of parses (see Figure 2) for each chart cell (e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order parsing, we turn to agenda-based parsing (Kay, 1986; Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for stru"
Q15-1039,W10-2903,0,0.133798,"g question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the structure of the natural language and the logical language. As a result, the space of possible semantic parses for even a short utterance grows quickly. For example, consider the utterance “what city was a"
Q15-1039,D13-1197,0,0.0335917,"Missing"
Q15-1039,Q13-1033,0,0.0703911,"Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for structured prediction (Daume et al., 2009; Ross et al., 2011; Goldberg and Nivre, 2013; Chang et al., 2015). Specifically, we cast agenda-based semantic parsing as a Markov decision process, where the goal is to learn a policy, that given a state (i.e., the current chart and agenda), chooses the best next action (i.e., the parse to pop from the agenda). The supervision signal is used to generate a sequence of 546 oracle actions, from which the model is trained. Our work bears a strong resemblance to Jiang et al. (2012), who applied imitation learning to agendabased parsing, but in the context of syntactic parsing. However, two new challenges arise in semantic parsing. First, sy"
Q15-1039,W05-1506,0,0.0403074,"2012), who had just a single item in each chart cell. 7 Lazy Agenda As we saw in Section 3, a single semantic function (e.g., L EX, B RIDGE) can create hundreds of derivations. Scoring all these derivations when adding them to the agenda is wasteful, because most have low probability. In this section, we assume semantic functions return a derivation stream, i.e., an iterator that lazily computes derivations on demand. Our lazy agenda G will hold derivation streams rather than derivations, and the actual agenda Q will be defined only implicitly. The intuition is similar to lazy K-best parsing (Huang and Chiang, 2005), but is applied to agenda-based semantic parsing. Our main assumption is that every derivation stream g = [d1 , d2 , . . . ], is sorted by decreasing score: s(d1 ) ≥ s(d2 ) ≥ · · · (in practice, this is only approximated as we explain at the end of this section). We define the score of a derivation stream as s(g) = s(d1 ). At test time the only change to Algorithm 1 is in line 4, where instead of popping the G s(g[1]) |g| U [d1 ] 7 1 0.88 5 100 11.92 [d2 , d3 , d4 , . . . ] G s(g[1]) |g| U [d1 ] 7 1 0.88 [d2 ] 5 1 0.12 1 1 0.006 [d3 ] [d4 , . . . ] −2 98 0.004 Figure 7: Unrolling a derivation"
Q15-1039,P07-1019,0,0.00809241,"in imitation learning (Abbeel and Ng, 2004; Daume et al., 2009; Ross et al., 2011; Goldberg and Nivre, 2013) has shown that interpolating with the model (corresponding to smaller β) can improve generalization. We were unable to improve accuracy by annealing β from 1000 to 0, so understanding this dynamic remains an open question. Parsing. In this paper, we avoided computing K derivations in each chart cell using an agenda and learning a scoring function for choosing agenda items. A complementary and purely algorithmic solution is lazy K-best parsing (Huang and Chiang, 2005), or cube growing (Huang and Chiang, 2007), which do not involve learning or an agenda. Similar to our work, cube growing approximates the best derivations in each chart cell in the case where features do not decompose Work in the past attempted to speed up inference using a simple model that is trained separately and used to prune the hypotheses considered by the main parsing model (Bodenstab et al., 2011; FitzGerald et al., 2013). We on the other hand speed up inference by training a single model that learns to follow good parsing actions. Work in agenda-based syntactic parsing (Klein and Manning, 2003; Pauls and Klein, 2009) focuse"
Q15-1039,P08-1067,0,0.0130397,"ore), and a completion estimate (outside score). Good estimates for the outside score result in a decrease in the number of derivations. Currently actions depend on the inside score, but we could add features based on chart derivations to provide “outside” information. Adding such features would present computational challenges as scores on the agenda would have to be updated as the agenda and chart are modified. Semantic parsing has been gaining momentum in recent years, but still there has been relatively little work on developing faster algorithms, especially compared to syntactic parsing (Huang, 2008; Kummerfeld et al., 2010; Rush and Petrov, 2012; Lewis and Steedman, 2014). While we have obtained significant speedups, we hope to encourage new ideas that exploit the structure of semantic parsing to yield better algorithms. Reproducibility. All code,8 data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x8fdfad310dd84b7baf683b520b4b64d5/. Acknowledgments We thank the anonymous reviewers and the action editor, Jason Eisner, for their thorough reviews and constructive feedback. We also gratefully acknowledge the support of the DAR"
Q15-1039,N03-1016,0,0.377514,"r of parses (see Figure 2) for each chart cell (e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order parsing, we turn to agenda-based parsing (Kay, 1986; Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for structured prediction (Daume"
Q15-1039,D12-1069,0,0.709011,"on batch: 10/2015; Published 11/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Root:Type.City u PlaceOfBirthOf.AbeLincoln Intersect what Set:Type.City was Set:PlaceOfBirthOf.AbeLincoln Join Lex city Entity:AbeLincoln Lex abraham lincoln Binary:PlaceOfBirthOf Lex born in Figure 2: An example semantic parse, or derivation, for the utterance “what city was abraham lincoln born in”. Each node in the tree has a category (e.g., E NTITY) and a logical form (e.g., AbeLincoln). hard search problem. To manage this combinatorial explosion, past approaches (Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013) used beam search, where the number of parses (see Figure 2) for each chart cell (e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order p"
Q15-1039,P10-1036,0,0.026314,"ompletion estimate (outside score). Good estimates for the outside score result in a decrease in the number of derivations. Currently actions depend on the inside score, but we could add features based on chart derivations to provide “outside” information. Adding such features would present computational challenges as scores on the agenda would have to be updated as the agenda and chart are modified. Semantic parsing has been gaining momentum in recent years, but still there has been relatively little work on developing faster algorithms, especially compared to syntactic parsing (Huang, 2008; Kummerfeld et al., 2010; Rush and Petrov, 2012; Lewis and Steedman, 2014). While we have obtained significant speedups, we hope to encourage new ideas that exploit the structure of semantic parsing to yield better algorithms. Reproducibility. All code,8 data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x8fdfad310dd84b7baf683b520b4b64d5/. Acknowledgments We thank the anonymous reviewers and the action editor, Jason Eisner, for their thorough reviews and constructive feedback. We also gratefully acknowledge the support of the DARPA Communicating with Com"
Q15-1039,D10-1119,0,0.0394942,"art for the utterance “what city was abraham lincoln born in”. Numbers in chart cells indicate the number of possible semantic parses constructed over that span, and arrows point to some of the logical forms that were constructed. There are more than one million possible semantic parses for this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (B"
Q15-1039,D13-1161,0,0.183508,"duction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the structure of the natural langua"
Q15-1039,D14-1107,0,0.0332052,"es for the outside score result in a decrease in the number of derivations. Currently actions depend on the inside score, but we could add features based on chart derivations to provide “outside” information. Adding such features would present computational challenges as scores on the agenda would have to be updated as the agenda and chart are modified. Semantic parsing has been gaining momentum in recent years, but still there has been relatively little work on developing faster algorithms, especially compared to syntactic parsing (Huang, 2008; Kummerfeld et al., 2010; Rush and Petrov, 2012; Lewis and Steedman, 2014). While we have obtained significant speedups, we hope to encourage new ideas that exploit the structure of semantic parsing to yield better algorithms. Reproducibility. All code,8 data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x8fdfad310dd84b7baf683b520b4b64d5/. Acknowledgments We thank the anonymous reviewers and the action editor, Jason Eisner, for their thorough reviews and constructive feedback. We also gratefully acknowledge the support of the DARPA Communicating with Computers (CwC) program under ARO prime contract no."
Q15-1039,P11-1060,1,0.904504,"t city was abraham lincoln born in”. Numbers in chart cells indicate the number of possible semantic parses constructed over that span, and arrows point to some of the logical forms that were constructed. There are more than one million possible semantic parses for this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013)."
Q15-1039,P14-5010,0,0.00359529,"Missing"
Q15-1039,P09-1108,0,0.271725,") for each chart cell (e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order parsing, we turn to agenda-based parsing (Kay, 1986; Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for structured prediction (Daume et al., 2009; Ross et a"
Q15-1039,N12-1054,0,0.0505466,"de score). Good estimates for the outside score result in a decrease in the number of derivations. Currently actions depend on the inside score, but we could add features based on chart derivations to provide “outside” information. Adding such features would present computational challenges as scores on the agenda would have to be updated as the agenda and chart are modified. Semantic parsing has been gaining momentum in recent years, but still there has been relatively little work on developing faster algorithms, especially compared to syntactic parsing (Huang, 2008; Kummerfeld et al., 2010; Rush and Petrov, 2012; Lewis and Steedman, 2014). While we have obtained significant speedups, we hope to encourage new ideas that exploit the structure of semantic parsing to yield better algorithms. Reproducibility. All code,8 data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x8fdfad310dd84b7baf683b520b4b64d5/. Acknowledgments We thank the anonymous reviewers and the action editor, Jason Eisner, for their thorough reviews and constructive feedback. We also gratefully acknowledge the support of the DARPA Communicating with Computers (CwC) program un"
Q15-1039,P07-1121,0,0.086672,"Figure 1: A parsing chart for the utterance “what city was abraham lincoln born in”. Numbers in chart cells indicate the number of possible semantic parses constructed over that span, and arrows point to some of the logical forms that were constructed. There are more than one million possible semantic parses for this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in"
Q15-1039,D14-1071,0,0.479256,"Missing"
Q15-1039,P14-1090,0,0.673614,"Missing"
Q15-1039,P15-1128,0,0.736716,"AGENDA F IXED +AGENDA α = 1000 α = 100 α = 10 p+w θ p+c θ pθ -B INARYA ND L EMMA Table 1: Test set results for the standard fixed-order parser (F IXED O RDER) and our new agenda-based parser (AGENDA IL), which substantially reduces parsing time and the number of parsing actions at no cost to accuracy. System YV14 BCFL13 BDZZ14 BWC14 BL14 YDZR14 BWC14+ BL14 WYWH14 WYWH14 YCHG15 F IXED O RDER AGENDA IL Authors Yao and Van-Durme (2014) Berant et al. (2013) Bao et al. (2014) Bordes et al. (2014) Berant and Liang (2014) Yang et al. (2014) Bordes et al. (2014) Wang et al. (2014) Wang et al. (2014) Yih et al. (2015) this work this work Acc. 35.4 35.7 37.5 39.2 39.9 41.3 41.8 45.3 45.3 52.5 49.6 49.7 Dev. 48.0 49.1 45.9 47.1 47.8 35.6 27.0 43.3 36.8 1.2 40.5 |Act.| 1,421 18,259 6,211 6,281 11,279 3,858 1,604 1,706 3,758 12,302 1,561 |Feat.| 1,912 18,259 6,320 6,615 11,279 3,858 1,604 2,121 4,278 15,524 2,110 Time 214 1,972 419 775 1,216 174 78 238 358 1,497 167 Table 3: Development set results for variants of AGENDA IL. ates a larger space of derivations. 9.2 Analysis To gain insight into our system components, we perform extensive experiments on the development set. Table 2: Results on the W EB Q UESTION"
Q15-1039,D07-1071,0,0.172777,"indicate the number of possible semantic parses constructed over that span, and arrows point to some of the logical forms that were constructed. There are more than one million possible semantic parses for this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from de"
Q15-1039,C10-2168,0,0.0159143,"oughly as efficient as F IXED O RDER with beam size 10–20. For the chosen beam size (K = 200), AGENDA IL is 9x faster than F IXE D O RDER . For K = 1, performance is poor for AGENDA IL and zero for F IXED O RDER. This highlights the inherent difficulty of mapping to logical forms compared to more shallow tasks, as maintaining just a single best derivation for each parsing state is not sufficient. A common variant on beam parsing is to replace the fixed beam size K with a threshold α, and prune any derivation whose probability is at least α times smaller than the best derivation in that state (Zhang et al., 2010; Bodenstab et al., 2011). We implemented this baseline and compared it to AGENDA IL 30 FixedOrder AgendaIL 25 3.0 30 20 2.5 2.0 1.5 1.0 10 0 1 3.5 time (sec) accuracy 40 4.0 FixedOrder AgendaIL #derivations (thousands) 50 0.0 1 20 15 10 5 0.5 10 20 50 100 200 400 beam size FixedOrder AgendaIL (scored) AgendaIL (popped) 10 20 50 100 200 400 beam size 0 1 10 20 50 100 200 400 beam size Figure 8: Comparing AGENDA IL and F IXED O RDER for various beam sizes (left: accuracy, middle: parsing time at test time in seconds, right: number of thousands of derivations scored and popped). The x-axis is on"
Q15-1039,J13-2005,1,\N,Missing
S17-1020,D13-1161,0,0.0510116,"tions involving conjunctions, but struggles on questions that involve relation composition and superlatives. 1 Introduction Question answering (QA) has witnessed a surge of interest in recent years (Hill et al., 2015; Yang et al., 2015; Pasupat and Liang, 2015; Chen et al., 2016; Joshi et al., 2017), as it is one of the prominent tests for natural language understanding. QA can be coarsely divided into semantic parsingbased QA, where a question is translated into a logical form that is executed against a knowledgebase (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Kwiatkowski et al., 2013; Reddy et al., 2014; Berant and Liang, 2015), and unstructured QA, where a question is answered directly from some relevant text 161 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 161–167, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We test this model on C OMPLEX Q UESTIONS (Bao et al., 2016), a dataset designed to require more compositionality compared to earlier datasets, such as W EB Q UESTIONS (Berant et al., 2013) and S IMPLE Q UESTIONS (Bordes et al., 2015). We find that a simple QA model, despi"
S17-1020,D13-1160,1,0.748774,"ase (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Kwiatkowski et al., 2013; Reddy et al., 2014; Berant and Liang, 2015), and unstructured QA, where a question is answered directly from some relevant text 161 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 161–167, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We test this model on C OMPLEX Q UESTIONS (Bao et al., 2016), a dataset designed to require more compositionality compared to earlier datasets, such as W EB Q UESTIONS (Berant et al., 2013) and S IMPLE Q UESTIONS (Bordes et al., 2015). We find that a simple QA model, despite having no access to the target KB, performs reasonably well on this dataset (∼35 F1 compared to the state-of-the-art of 41 F1 ). Moreover, for the subset of questions for which the right answer can be found in one of the web snippets, we outperform the semantic parser (51.9 F1 vs. 48.5 F1 ). We analyze results for different types of compositionality and find that superlatives and relation composition constructions are challenging for a webbased QA system, while conjunctions and events with multiple arguments"
S17-1020,Q15-1039,1,0.837992,"questions that involve relation composition and superlatives. 1 Introduction Question answering (QA) has witnessed a surge of interest in recent years (Hill et al., 2015; Yang et al., 2015; Pasupat and Liang, 2015; Chen et al., 2016; Joshi et al., 2017), as it is one of the prominent tests for natural language understanding. QA can be coarsely divided into semantic parsingbased QA, where a question is translated into a logical form that is executed against a knowledgebase (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Kwiatkowski et al., 2013; Reddy et al., 2014; Berant and Liang, 2015), and unstructured QA, where a question is answered directly from some relevant text 161 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 161–167, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We test this model on C OMPLEX Q UESTIONS (Bao et al., 2016), a dataset designed to require more compositionality compared to earlier datasets, such as W EB Q UESTIONS (Berant et al., 2013) and S IMPLE Q UESTIONS (Bordes et al., 2015). We find that a simple QA model, despite having no access to the target KB, perform"
S17-1020,P11-1060,0,0.0623128,"well on complex questions involving conjunctions, but struggles on questions that involve relation composition and superlatives. 1 Introduction Question answering (QA) has witnessed a surge of interest in recent years (Hill et al., 2015; Yang et al., 2015; Pasupat and Liang, 2015; Chen et al., 2016; Joshi et al., 2017), as it is one of the prominent tests for natural language understanding. QA can be coarsely divided into semantic parsingbased QA, where a question is translated into a logical form that is executed against a knowledgebase (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Kwiatkowski et al., 2013; Reddy et al., 2014; Berant and Liang, 2015), and unstructured QA, where a question is answered directly from some relevant text 161 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 161–167, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We test this model on C OMPLEX Q UESTIONS (Bao et al., 2016), a dataset designed to require more compositionality compared to earlier datasets, such as W EB Q UESTIONS (Berant et al., 2013) and S IMPLE Q UESTIONS (Bordes et al., 2015). We find tha"
S17-1020,P14-5010,0,0.0054065,"Missing"
S17-1020,D16-1241,0,0.0363473,"6; Seo et al., 2016). In semantic parsing, background knowledge has already been compiled into a knowledge-base (KB), and thus the challenge is in interpreting the question, which may contain compositional constructions (“What is the second-highest mountain in Europe?”) or computations (“What is the difference in population between France and Germany?”). In unstructured QA, the model needs to also interpret the language of a document, and thus most datasets focus on matching the question against the document and extracting the answer from some local context, such as a sentence or a paragraph (Onishi et al., 2016; Rajpurkar et al., 2016; Yang et al., 2015). Since semantic parsing models excel at handling complex linguistic constructions and reasoning over multiple facts, a natural way to examine whether a benchmark indeed requires modeling these properties, is to train an unstructured QA model, and check if it under-performs compared to semantic parsing models. If questions can be answered by examining local contexts only, then the use of a knowledge-base is perhaps unnecessary. However, to the best of our knowledge, only models that utilize the KB have been evaluated on common semantic parsing benchm"
S17-1020,W02-1033,0,0.145658,"way to examine whether a benchmark indeed requires modeling these properties, is to train an unstructured QA model, and check if it under-performs compared to semantic parsing models. If questions can be answered by examining local contexts only, then the use of a knowledge-base is perhaps unnecessary. However, to the best of our knowledge, only models that utilize the KB have been evaluated on common semantic parsing benchmarks. The goal of this paper is to bridge this evaluation gap. We develop a simple log-linear model, in the spirit of traditional web-based QA systems (Kwok et al., 2001; Brill et al., 2002), that answers questions by querying the web and extracting the answer from returned web snippets. Thus, our evaluation scheme is suitable for semantic parsing benchmarks in which the knowledge required for answering questions is covered by the web (in contrast with virtual assitants for which the knowledge is specific to an application). Semantic parsing shines at analyzing complex natural language that involves composition and computation over multiple pieces of evidence. However, datasets for semantic parsing contain many factoid questions that can be answered from a single web document. In"
S17-1020,P15-1142,0,0.0366569,"cts the answer only from web snippets, without access to the target knowledge-base. We investigate this approach on C OMPLEX Q UESTIONS, a dataset designed to focus on compositional language, and find that our model obtains reasonable performance (∼35 F1 compared to 41 F1 of state-of-the-art). We find in our analysis that our model performs well on complex questions involving conjunctions, but struggles on questions that involve relation composition and superlatives. 1 Introduction Question answering (QA) has witnessed a surge of interest in recent years (Hill et al., 2015; Yang et al., 2015; Pasupat and Liang, 2015; Chen et al., 2016; Joshi et al., 2017), as it is one of the prominent tests for natural language understanding. QA can be coarsely divided into semantic parsingbased QA, where a question is translated into a logical form that is executed against a knowledgebase (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Kwiatkowski et al., 2013; Reddy et al., 2014; Berant and Liang, 2015), and unstructured QA, where a question is answered directly from some relevant text 161 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 161–"
S17-1020,D14-1162,0,0.0799427,"Missing"
S17-1020,P16-1223,0,0.0373468,"web snippets, without access to the target knowledge-base. We investigate this approach on C OMPLEX Q UESTIONS, a dataset designed to focus on compositional language, and find that our model obtains reasonable performance (∼35 F1 compared to 41 F1 of state-of-the-art). We find in our analysis that our model performs well on complex questions involving conjunctions, but struggles on questions that involve relation composition and superlatives. 1 Introduction Question answering (QA) has witnessed a surge of interest in recent years (Hill et al., 2015; Yang et al., 2015; Pasupat and Liang, 2015; Chen et al., 2016; Joshi et al., 2017), as it is one of the prominent tests for natural language understanding. QA can be coarsely divided into semantic parsingbased QA, where a question is translated into a logical form that is executed against a knowledgebase (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Kwiatkowski et al., 2013; Reddy et al., 2014; Berant and Liang, 2015), and unstructured QA, where a question is answered directly from some relevant text 161 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 161–167, c Vancouver, C"
S17-1020,D16-1264,0,0.0551793,"In semantic parsing, background knowledge has already been compiled into a knowledge-base (KB), and thus the challenge is in interpreting the question, which may contain compositional constructions (“What is the second-highest mountain in Europe?”) or computations (“What is the difference in population between France and Germany?”). In unstructured QA, the model needs to also interpret the language of a document, and thus most datasets focus on matching the question against the document and extracting the answer from some local context, such as a sentence or a paragraph (Onishi et al., 2016; Rajpurkar et al., 2016; Yang et al., 2015). Since semantic parsing models excel at handling complex linguistic constructions and reasoning over multiple facts, a natural way to examine whether a benchmark indeed requires modeling these properties, is to train an unstructured QA model, and check if it under-performs compared to semantic parsing models. If questions can be answered by examining local contexts only, then the use of a knowledge-base is perhaps unnecessary. However, to the best of our knowledge, only models that utilize the KB have been evaluated on common semantic parsing benchmarks. The goal of this p"
S17-1020,P17-1171,0,0.0358423,"Missing"
S17-1020,Q14-1030,0,0.0208969,"ns, but struggles on questions that involve relation composition and superlatives. 1 Introduction Question answering (QA) has witnessed a surge of interest in recent years (Hill et al., 2015; Yang et al., 2015; Pasupat and Liang, 2015; Chen et al., 2016; Joshi et al., 2017), as it is one of the prominent tests for natural language understanding. QA can be coarsely divided into semantic parsingbased QA, where a question is translated into a logical form that is executed against a knowledgebase (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Kwiatkowski et al., 2013; Reddy et al., 2014; Berant and Liang, 2015), and unstructured QA, where a question is answered directly from some relevant text 161 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 161–167, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We test this model on C OMPLEX Q UESTIONS (Bao et al., 2016), a dataset designed to require more compositionality compared to earlier datasets, such as W EB Q UESTIONS (Berant et al., 2013) and S IMPLE Q UESTIONS (Bordes et al., 2015). We find that a simple QA model, despite having no access"
S17-1020,P16-1145,0,0.118932,"Missing"
S17-1020,D15-1237,0,0.311176,"ckground knowledge has already been compiled into a knowledge-base (KB), and thus the challenge is in interpreting the question, which may contain compositional constructions (“What is the second-highest mountain in Europe?”) or computations (“What is the difference in population between France and Germany?”). In unstructured QA, the model needs to also interpret the language of a document, and thus most datasets focus on matching the question against the document and extracting the answer from some local context, such as a sentence or a paragraph (Onishi et al., 2016; Rajpurkar et al., 2016; Yang et al., 2015). Since semantic parsing models excel at handling complex linguistic constructions and reasoning over multiple facts, a natural way to examine whether a benchmark indeed requires modeling these properties, is to train an unstructured QA model, and check if it under-performs compared to semantic parsing models. If questions can be answered by examining local contexts only, then the use of a knowledge-base is perhaps unnecessary. However, to the best of our knowledge, only models that utilize the KB have been evaluated on common semantic parsing benchmarks. The goal of this paper is to bridge th"
S17-1020,P17-1147,0,0.0340715,"ut access to the target knowledge-base. We investigate this approach on C OMPLEX Q UESTIONS, a dataset designed to focus on compositional language, and find that our model obtains reasonable performance (∼35 F1 compared to 41 F1 of state-of-the-art). We find in our analysis that our model performs well on complex questions involving conjunctions, but struggles on questions that involve relation composition and superlatives. 1 Introduction Question answering (QA) has witnessed a surge of interest in recent years (Hill et al., 2015; Yang et al., 2015; Pasupat and Liang, 2015; Chen et al., 2016; Joshi et al., 2017), as it is one of the prominent tests for natural language understanding. QA can be coarsely divided into semantic parsingbased QA, where a question is translated into a logical form that is executed against a knowledgebase (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Kwiatkowski et al., 2013; Reddy et al., 2014; Berant and Liang, 2015), and unstructured QA, where a question is answered directly from some relevant text 161 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 161–167, c Vancouver, Canada, August 3-4, 20"
S17-1020,W14-2416,1,0.936475,"Missing"
S17-1020,P15-1128,0,0.0256309,"ver the candidates in A, and is used at test time to find the most probable answers. 4 Experiments C OMPLEX Q UESTIONS contains 1,300 training examples and 800 test examples. We performed 5 random 70/30 splits of the training set for development. We computed POS tags and named entities with Stanford CoreNLP (Manning et al., 2014). We did not employ any co-reference resolution tool in this work. If after candidate extraction, we do not find the gold answer in the topK(=140) candidates, we discard the example, resulting in a training set of 856 examples. We compare our model, W EB QA, to STAGG (Yih et al., 2015) and C OMP Q (Bao et al., 2016), which are to the best of our knowledge the highest performing semantic parsing models on both C OMPLEX Q UESTIONS and W EB Q UES TIONS . For these systems, we only report test F1 numbers that are provided in the original papers, as we do not have access to the code or predictions. We evaluate models by computing average F1 , the official evaluation metric defined for C OM PLEX Q UESTIONS. This measure computes the F1 Candidate Extraction We extract all 1-grams, 2-grams, 3-grams and 4-grams (lowercased) that appear in R, yielding roughly 5,000 candidates per que"
S17-1020,P16-1086,0,0.0538137,"Missing"
S17-1020,C16-1236,0,\N,Missing
W14-2416,P14-1090,1,0.581306,"Missing"
W14-2416,P14-1133,1,0.0366645,"ase, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics provide richer, open-domain challenges. While the vocabulary increased, our analysis suggests that compositionality and complexity decreased. We therefore conclude that the semantic parsing community should target"
W14-2416,D13-1160,1,0.915633,"schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically derived over (English) text, it is not surprising that IE researchers would start including such “features” in their models. Our question is then: what is the difference between an IE system with access to syntax, as compared to a semantic parser, when both are targeting a factoid-extraction style task? While our conclusions should hold generally for similar KBs, we will focus on Freebase, such as explored by Krishnamurthy and Mitchell (2012), and then others such as Cai and Yates (2013a) and Berant et al. (2013). We compare two open-source, state-ofthe-art systems on the task of Freebase QA: the semantic parsing system SEMPRE (Berant et al., 2013), and the IE system jacana-freebase (Yao and Van Durme, 2014). We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of Berant et al. (2013) and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as well as integrate disparate pieces of information. This representational c"
W14-2416,D07-1071,0,0.0117893,"(QA) from structured data, such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland US"
W14-2416,P09-1110,0,0.0175891,"h as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Associati"
W14-2416,P13-1042,0,0.479728,"f a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically derived over (English) text, it is not surprising that IE researchers would start including such “features” in their models. Our question is then: what is the difference between an IE system with access to syntax, as compared to a semantic parser, when both are targeting a factoid-extraction style task? While our conclusions should hold generally for similar KBs, we will focus on Freebase, such as explored by Krishnamurthy and Mitchell (2012), and then others such as Cai and Yates (2013a) and Berant et al. (2013). We compare two open-source, state-ofthe-art systems on the task of Freebase QA: the semantic parsing system SEMPRE (Berant et al., 2013), and the IE system jacana-freebase (Yao and Van Durme, 2014). We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of Berant et al. (2013) and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as well as integrate disparate pieces of informati"
W14-2416,D12-1069,0,0.0385127,"ugh meaning in order to populate a database with factoids of a form matching a given schema.1 Given the ease with which reasonably accurate, deep syntactic structure can be automatically derived over (English) text, it is not surprising that IE researchers would start including such “features” in their models. Our question is then: what is the difference between an IE system with access to syntax, as compared to a semantic parser, when both are targeting a factoid-extraction style task? While our conclusions should hold generally for similar KBs, we will focus on Freebase, such as explored by Krishnamurthy and Mitchell (2012), and then others such as Cai and Yates (2013a) and Berant et al. (2013). We compare two open-source, state-ofthe-art systems on the task of Freebase QA: the semantic parsing system SEMPRE (Berant et al., 2013), and the IE system jacana-freebase (Yao and Van Durme, 2014). We find that these two systems are on par with each other, with no significant differences in terms of accuracy between them. A major distinction between the work of Berant et al. (2013) and Yao and Van Durme (2014) is the ability of the former to represent, and compose, aggregation operators (such as argmax, or count), as we"
W14-2416,D10-1119,0,0.0289785,"), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Lingu"
W14-2416,D11-1140,0,0.00845976,"al., 2008) and Yago2 (Hoffart et al., 2011), has drawn significant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics provide richer, ope"
W14-2416,D13-1161,0,0.0426815,"ficant interest from both knowledge base (KB) and semantic parsing (SP) researchers. The majority of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics provide richer, open-domain challenges. While the vocabulary increased, our ana"
W14-2416,P11-1060,0,0.0293974,"ity of such work treats the KB as a database, to which standard database queries (SPARQL, MySQL, etc.) are issued to retrieve answers. Language understanding is modeled as the task of converting natural language questions into queries through intermediate logical forms, with the popular two approaches including: CCG parsing (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013a), and dependencybased compositional semantics (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014). 1 So-called Open Information Extraction (OIE) is simply a further blurring of the distinction between IE and SP, where the schema is allowed to grow with the number of verbs, and other predicative elements of the language. 82 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 82–86, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics provide richer, open-domain challenges. While the vocabulary increased, our analysis suggests that compositionality and complexity decreased. We therefore conclude tha"
