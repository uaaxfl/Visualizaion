2020.acl-demos.43,P17-1123,0,0.0237783,"idden to the student in Quac). The student must ask questions of the teacher, and the teacher answers using extracts of the document. In our system, the questions asked by the user are answered automatically, introducing potential errors, and the user can choose to ask questions or not. In this work, the focus is not on the collection of naturally occurring questions, but in putting a Q&A system in use in a news dialogue system, and observing the extent of its use. Question Generation (QG) has become an active area for text generation. A common approach is to use a sequence to sequence model (Du et al., 2017), encoding the paragraph (or context), an optional target answer (answer-aware (Sun et al., 2018)), and decoding a paired question. This common approach focuses on the generation of a single article, from a single piece of context, often a paragraph. We argue that our framing of the QG problem as the generation of a series of questions spanning several (possibly redundant) documents is a novel task. Krishna and Iyyer (2019) build a hierarchy of questions generated for a single document; the document is then reorganized into a “Squashed” document, where paragraphs and questions are interleaved."
2020.acl-demos.43,P19-1224,0,0.015163,"logue system, and observing the extent of its use. Question Generation (QG) has become an active area for text generation. A common approach is to use a sequence to sequence model (Du et al., 2017), encoding the paragraph (or context), an optional target answer (answer-aware (Sun et al., 2018)), and decoding a paired question. This common approach focuses on the generation of a single article, from a single piece of context, often a paragraph. We argue that our framing of the QG problem as the generation of a series of questions spanning several (possibly redundant) documents is a novel task. Krishna and Iyyer (2019) build a hierarchy of questions generated for a single document; the document is then reorganized into a “Squashed” document, where paragraphs and questions are interleaved. Because our approach is based on using multiple documents as the source, compiling all questions into a single document would be long to read, so we opt for a chatbot. 2 https://www.messenger.com/t/BBCPolitics https://www.messenger.com/t/quartznews 4 https://www.messenger.com/t/cnn 3 385 6 Discussion During the usability study, we obtained direct and indirect feedback from our users, and we summarize limitations that could"
2020.acl-demos.43,W17-2701,1,0.837596,"m/watch?v=eze9hpEPUgo. 2 System Description This section describes the components of the chatbot: the content source, the user interface, the supported user actions and the computed system answers. Appendix A lists library and data resources used in the system. 2.1 Content Sources We form the content for the chatbot from a set of news sources. We have collected an average of 2,000 news articles per day from 20 international news sources starting in 2010. The news articles are clustered into stories: groups of news articles about a similar evolving topic, and each story is automatically named (Laban and Hearst, 2017). Some of the top stories at the time of writing are shown in Figure 1(a). 2.2 User Interface The chatbot supports information-seeking: the user is seeking information and the system delivers in380 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 380–387 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics (a) Homepage (b) Initiating a Chatroom (c) Chatroom Q&A Figure 1: Screenshots of the news chatbot (a) Homepage lists most recently active chatrooms (Zone 1 is an example chatroom) (b) Newly opened chatroom: Zone 2 is an event"
2020.acl-demos.43,2020.acl-main.460,1,0.743749,"to conversations, and (2) limiting the scope of each dialogue is helpful from both a usability and a technical standpoint, as it helps reduce ambiguity and search scope. For example, answering a question like: “What is the total cost to insurers so far?” is easier when knowing the scope is the Australia Fires, compared to all of news. Articles in a story are grouped into events, corresponding to an action that occurred in a particular time and place. For each event, the system forms an event message by combining the event’s news article headlines generated by an abstractive summarizer model (Laban et al., 2020). Zone 2 in Figure 1(b) gives an example of an event message. The event messages form a chronological timeline in the story. Because of the difference in respective roles, we expect user messages to be shorter than system responses, which we aim to be around 30 words. 2.3 User Actions During the conversation, the user can choose among different kinds of actions. Explore the event timeline. A chatroom conversation starts with the system showing the two most recent event messages of the story (Figure 1(b)). These messages give minimal context to the user necessary to start a conversation. When t"
2020.acl-demos.43,2021.ccl-1.108,0,0.0768927,"Missing"
2020.acl-demos.43,P18-2124,0,0.0613794,"ve burned?”, and can therefore be considered redundant. Our procedure to track the knowledge state of a news conversation consists of the following steps: (1) generate candidate questions spanning the knowledge in the story, (2) build a graph connecting paragraphs with questions they answer, (3) during a conversation, use the graph to track what questions have been answered already, and avoid using paragraphs that do not answer new questions. Question Candidate Generation. We fine-tune a GPT2 language model (Radford et al., 2019) on the task of question generation using the SQuAD 2.0 dataset (Rajpurkar et al., 2018). At training, the model reads a paragraph from the training set, and learns to generate a question associated with the paragraph. For each paragraph in each article of the story (the paragraph set), we use beam search to generate K candidate questions. In our experience, using a large beam size (K=20) is important, as one paragraph can yield several valid questions. Beam search enforces exploration, with the first step of beam search often containing several interrogative words (what, where...). For a given paragraph, we reduce the set of questions by deduplicating questions that are lexicall"
2020.acl-demos.43,Q19-1016,0,0.0183168,"ch turn, the user can choose from a list of replies, deciding which track of the dialogue-article is followed. CNN4 has also experimented with choose-your-own adventure articles, with the added ability for small talk. Relevant Q&A datasets. NewsQA (Trischler et al., 2017) collected a dataset by having a crowdworker read the summary of a news article and ask a follow-up question. Subsequent crowd-workers answered the question or marked it as not-answerable. NewsQA’s objective was to collect a dataset, and we focus on building a usable dialogue interface for the news with a Q&A component. CoQA (Reddy et al., 2019) and Quac (Choi et al., 2018) are two datasets collected for questions answering in the context of a dialogue. For both datasets, two crowd-workers (a student and a teacher) have a conversation about a piece of text (hidden to the student in Quac). The student must ask questions of the teacher, and the teacher answers using extracts of the document. In our system, the questions asked by the user are answered automatically, introducing potential errors, and the user can choose to ask questions or not. In this work, the focus is not on the collection of naturally occurring questions, but in putt"
2020.acl-demos.43,D18-1427,0,0.0490229,"swers using extracts of the document. In our system, the questions asked by the user are answered automatically, introducing potential errors, and the user can choose to ask questions or not. In this work, the focus is not on the collection of naturally occurring questions, but in putting a Q&A system in use in a news dialogue system, and observing the extent of its use. Question Generation (QG) has become an active area for text generation. A common approach is to use a sequence to sequence model (Du et al., 2017), encoding the paragraph (or context), an optional target answer (answer-aware (Sun et al., 2018)), and decoding a paired question. This common approach focuses on the generation of a single article, from a single piece of context, often a paragraph. We argue that our framing of the QG problem as the generation of a series of questions spanning several (possibly redundant) documents is a novel task. Krishna and Iyyer (2019) build a hierarchy of questions generated for a single document; the document is then reorganized into a “Squashed” document, where paragraphs and questions are interleaved. Because our approach is based on using multiple documents as the source, compiling all questions"
2020.acl-demos.43,W17-2623,0,0.0329361,"incorporated into the chatbot. On BBC’s Messenger chatbot2 , a user can enter search queries, such as “latest news” or “Brexit news” and obtain a list of latest BBC articles matching the search criteria. In the chatbot produced by Quartz3 , journalists hand-craft news stories in the form of pre-written dialogues (aka choose-yourown adventure). At each turn, the user can choose from a list of replies, deciding which track of the dialogue-article is followed. CNN4 has also experimented with choose-your-own adventure articles, with the added ability for small talk. Relevant Q&A datasets. NewsQA (Trischler et al., 2017) collected a dataset by having a crowdworker read the summary of a news article and ask a follow-up question. Subsequent crowd-workers answered the question or marked it as not-answerable. NewsQA’s objective was to collect a dataset, and we focus on building a usable dialogue interface for the news with a Q&A component. CoQA (Reddy et al., 2019) and Quac (Choi et al., 2018) are two datasets collected for questions answering in the context of a dialogue. For both datasets, two crowd-workers (a student and a teacher) have a conversation about a piece of text (hidden to the student in Quac). The"
2020.acl-main.446,D18-1431,0,0.360033,"produce banal responses such as “Yeah.” While this may be an appropriate response to a chitchat conversation, to keep a human participant engaged, diversity of responses is important. Diverse models vary the language used and the content referenced, and the generated utterances differ from the most typical conversation responses some proportion of the time. A model which only generates “Yeah,” “No,” and “I don’t know” is not diverse and is not be engaging to converse with. Past work has improved model diversity with innovation on model architectures and decoding strategies (Li et al., 2016a; Baheti et al., 2018; Li et al., 2017; Shao et al., 2017; Cao and Clark, 2017; Serban et al., 2017; Zhao et al., 2017). We build upon this work to propose a novel method to collect and determine more diverse data to train these models with. Our method can be used in conjunction with existing generation-specific model innovations. Some prior work on data collection processes has prioritized diversity. For instance, Rashkin et al. (2019) prompts crowdworkers to choose an underused emotion class to generate dialogue. This work encourages coverage of emotion classes, but does not consider the likelihood that some cro"
2020.acl-main.446,E17-2029,0,0.135123,"e an appropriate response to a chitchat conversation, to keep a human participant engaged, diversity of responses is important. Diverse models vary the language used and the content referenced, and the generated utterances differ from the most typical conversation responses some proportion of the time. A model which only generates “Yeah,” “No,” and “I don’t know” is not diverse and is not be engaging to converse with. Past work has improved model diversity with innovation on model architectures and decoding strategies (Li et al., 2016a; Baheti et al., 2018; Li et al., 2017; Shao et al., 2017; Cao and Clark, 2017; Serban et al., 2017; Zhao et al., 2017). We build upon this work to propose a novel method to collect and determine more diverse data to train these models with. Our method can be used in conjunction with existing generation-specific model innovations. Some prior work on data collection processes has prioritized diversity. For instance, Rashkin et al. (2019) prompts crowdworkers to choose an underused emotion class to generate dialogue. This work encourages coverage of emotion classes, but does not consider the likelihood that some crowdworkers are better at producing certain types of data t"
2020.acl-main.446,D18-2029,0,0.0387578,"Missing"
2020.acl-main.446,E17-2068,0,0.0103455,"d cross-validation over 10 trials: higher is better. The task is classification of emotions from a set of 32 possible given the text of dialogue responses in subc . † and * indicate p&lt;0.05 and 0.001 respectively compared to Random Population. task is to predict which of the 32 emotions is expressed from a given utterance. Following Larson et al. (2019), we use two classification models: • Bag-of-Words SVM • FastText classifier Bag-of-Words SVM is an SVM using TF-IDF word features for prediction. The FastText classifier uses a neural classification model on top of fastText sentence embeddings (Joulin et al., 2017). The sub-corpora we collect using the different methods serve as the datasets to train these classification models. 4.2.2 Classification Results Classification task results are summarized in Table 4. Reported scores are averaged 5-fold crossvalidation and averaged over 10 runs of datasets collected from each method. While most conditions show Diverse Population significantly outperforms Random Population, it performs worse than Random Population with Entropy SVM and Entropy FastText and performs the same in Mean IDF FastText. Above Mean Population, on the other hand, outperforms the Random Po"
2020.acl-main.446,N18-3005,0,0.284204,"-fly collection of new datasets via crowdworking or similar methods. We implement DIDC with three diversity metrics: Outlier, Entropy, and Mean-IDF. Diversity-Informed Data Collection also provides a new method for finding an upper bound on a current corpus’s diversity via a Corpus-Wide Oracle which has access to information about which utterances are most diverse across the corpus. Prior work has not used corpus-level statistics to enhance the diversity of the collected data. Instead, when collecting data with crowdworkers, researchers have sought more diverse responses by altering the task (Kang et al., 2018) or by altering the stimulus (Larson et al., 2019). Prior work that trains neural dialogue models has not made use of subsets of existing datasets that exhibit properties 4958 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4958–4968 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of diversity. Our experiments show this strategy yields significantly more diverse data than baseline collection processes. It also yields better, more diverse model output on two downstream tasks. Additionally, this method can be implemented for othe"
2020.acl-main.446,P17-4012,0,0.014358,"esses collect utterances for subc . To train generation models, the input is the most recent parent utterance for each utt in subc , and utt is the target sentence to generate. When utt is the starting utterance in a conversation, the input is the situation associated with the conversation (such as planning a vacation). We train Sequence-to-Sequence models (Sutskever et al., 2014) with a 2-layer bidirectional encoder, hidden size 500, word vector size 64, Adam optimizer (Kingma and Ba, 2014), learning rate 0.001, trained for 3000 steps with batch size 32. Models are implemented using OpenNMT (Klein et al., 2017). We opt to use a standard model as it has fewer parameters to learn from smaller sub-corpora. We use the same parameter settings for all trained models. 4.3.2 Generation Results Generation task results are summarized in Table 5. We report on both mean and median length of model responses. Distinct-1 and Distinct-2 measure the proportion of unigrams and bigrams respectively in the set of model responses which are unique (Li et al., 2016a). We also report diversity of the generated responses calculated by the metrics used in subc collection (see Table 2). Our method results in models which prod"
2020.acl-main.446,N19-1051,0,0.0352007,"Missing"
2020.acl-main.446,N16-1014,0,0.667944,"pproaches, which modify model architectures to solve the problem, this method uses dynamically computed corpuslevel statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpuslevel metrics. 1 Introduction It is well-documented that neural dialogue models struggle with generating engaging, relevant responses (Li et al., 2016a) and often produce banal responses such as “Yeah.” While this may be an appropriate response to a chitchat conversation, to keep a human participant engaged, diversity of responses is important. Diverse models vary the language used and the content referenced, and the generated utterances differ from the most typical conversation responses some proportion of the time. A model which only generates “Yeah,” “No,” and “I don’t know” is not diverse and is not be engaging to converse with. Past work has improved model diversity with innovation on model architectures and decoding strategies (Li et"
2020.acl-main.446,D17-1230,0,0.154364,"ses such as “Yeah.” While this may be an appropriate response to a chitchat conversation, to keep a human participant engaged, diversity of responses is important. Diverse models vary the language used and the content referenced, and the generated utterances differ from the most typical conversation responses some proportion of the time. A model which only generates “Yeah,” “No,” and “I don’t know” is not diverse and is not be engaging to converse with. Past work has improved model diversity with innovation on model architectures and decoding strategies (Li et al., 2016a; Baheti et al., 2018; Li et al., 2017; Shao et al., 2017; Cao and Clark, 2017; Serban et al., 2017; Zhao et al., 2017). We build upon this work to propose a novel method to collect and determine more diverse data to train these models with. Our method can be used in conjunction with existing generation-specific model innovations. Some prior work on data collection processes has prioritized diversity. For instance, Rashkin et al. (2019) prompts crowdworkers to choose an underused emotion class to generate dialogue. This work encourages coverage of emotion classes, but does not consider the likelihood that some crowdworkers are bet"
2020.acl-main.446,P19-1534,0,0.194905,"know” is not diverse and is not be engaging to converse with. Past work has improved model diversity with innovation on model architectures and decoding strategies (Li et al., 2016a; Baheti et al., 2018; Li et al., 2017; Shao et al., 2017; Cao and Clark, 2017; Serban et al., 2017; Zhao et al., 2017). We build upon this work to propose a novel method to collect and determine more diverse data to train these models with. Our method can be used in conjunction with existing generation-specific model innovations. Some prior work on data collection processes has prioritized diversity. For instance, Rashkin et al. (2019) prompts crowdworkers to choose an underused emotion class to generate dialogue. This work encourages coverage of emotion classes, but does not consider the likelihood that some crowdworkers are better at producing certain types of data than others. This paper introduces Diversity-Informed Data Collection (DIDC), a new strategy for creating a dataset of conversational utterances via selecting which participants’ data to include in the collection. The strategy progressively builds up a more diverse sub-corpus from an existing larger collection. The main idea is to grow the sub-corpus by adding"
2020.acl-main.446,P17-1061,0,0.142282,"nversation, to keep a human participant engaged, diversity of responses is important. Diverse models vary the language used and the content referenced, and the generated utterances differ from the most typical conversation responses some proportion of the time. A model which only generates “Yeah,” “No,” and “I don’t know” is not diverse and is not be engaging to converse with. Past work has improved model diversity with innovation on model architectures and decoding strategies (Li et al., 2016a; Baheti et al., 2018; Li et al., 2017; Shao et al., 2017; Cao and Clark, 2017; Serban et al., 2017; Zhao et al., 2017). We build upon this work to propose a novel method to collect and determine more diverse data to train these models with. Our method can be used in conjunction with existing generation-specific model innovations. Some prior work on data collection processes has prioritized diversity. For instance, Rashkin et al. (2019) prompts crowdworkers to choose an underused emotion class to generate dialogue. This work encourages coverage of emotion classes, but does not consider the likelihood that some crowdworkers are better at producing certain types of data than others. This paper introduces Diversi"
2020.acl-main.460,P18-3015,0,0.0177273,"pervised, not requiring parallel data, but only a group of documents and a group of summaries. In contrast, our work does not require any summaries, and is trained using only documents. Radford et al. (2019) summarize documents using a language model (GPT2) in a Zeroshot learning setting. The model reads the document followed by a special token “TL/DR”, and is tasked with continuing the document with a summary. Our work is an extension of this work: we initialize our Summarizer model with a GPT2 and specialize it with a second unsupervised method. Summarization and Q&A. Eyal et al. (2019) and Arumae and Liu (2018) turn reference summaries into fill-in-the-blank (FIB) questions, either as an evaluation metric or to train an extractive summarization model. In this work, we directly generate FIB questions on the document being summarized, bypassing the need for a reference summary. Scialom et al. (2019)’s work stays closer to a Q&A scenario, and uses a Question Generation module to generate actual questions about the document, answered by a Squad-based (Rajpurkar et al., 2018) model using the generated summary. We refrain from using actual questions because question generation remains a challenge, and it"
2020.acl-main.460,P18-1063,0,0.0399781,"on module to generate actual questions about the document, answered by a Squad-based (Rajpurkar et al., 2018) model using the generated summary. We refrain from using actual questions because question generation remains a challenge, and it is unclear how many questions should be generated to assess the quality of a summary. RL in Summarization. Paulus et al. (2018) introduced Reinforcement Learning (RL) to neural summarization methods by optimizing for ROUGE scores, leading to unreadable summaries. Since then, Reinforcement Learning has been used to select sentences with high ROUGE potential (Chen and Bansal, 2018), or optimize modified versions of ROUGE that account for readability (Pasunuru and Bansal, 2018). In all cases, the reward being computed relies on a reference summary, making the methods supervised. We craft a reward that does not require a target summary allowing our training process to remain unsupervised. 3 The Summary Loop For this work, the definition of a summary is: 5136 “A summary is a brief, fluent text that covers the main points of an original document.” Optimization 6 Summary score 1.1 Brevity, fluency and coverage are the three pillars of a good summary. Under a length constrain"
2020.acl-main.460,N19-1423,0,0.0179284,"vised). In order to remove this dependency, we use the first 50 words of the unmasked 5138 Summary Dataset Empty String Headline First 10 words Newsroom First 24 words CNN/DM First 46 words Summary Length 0 9.59 10.0 23.41 24.0 45.75 46.0 Raw Coverage 0.334 0.478 0.428 0.525 0.537 0.726 0.649 Norm. Coverage 0 0.144 0.094 0.191 0.203 0.392 0.315 Table 1: Analysis of the raw and normalized coverage of three existing human-written summary datasets, as well as first-k word baselines. document (D[: 50]) as a proxy for document summaries. The Coverage Model is initialized with a trained BERT model (Devlin et al., 2019), and trained using (D, D[: 50]) pairs on the coverage task. Because BERT is already trained on the similar MLM task, the Coverage model is able to leverage knowledge accrued by BERT. The Coverage Model converges after roughly 5 hours of training on a Titan X GPU. 3.4.3 Analysis of Coverage We present properties of the raw and normalized coverage through the analysis of existing humanwritten summary datasets. We focus our analysis on three datasets in the news domain: (1) a headline dataset obtained from common US news websites (Laban and Hearst, 2017), (2) the Newsroom dataset (Grusky et al.,"
2020.acl-main.460,N19-1409,0,0.0235134,"n-generated summaries and automatic summaries in terms of length of copied spans. 2 Related Work Supervised Abstractive Summarization. Sequence-to-sequence (seq2seq) (Sutskever et al., 2014) models trained using teacher-forcing are the most common approach to abstractive summarization (Nallapati et al., 2016). A common architecture is the Pointer-Generator (See et al., 2017). Performance can further be improved by constraining the attention (Gehrmann et al., 2018; Gui et al., 2019; Wang et al., 2019) and using pretrained Transformer-based language models (Lewis et al., 2019; Chi et al., 2019; Edunov et al., 2019). Through architectural changes, the training procedure remains constant: using a large corpus of document-summary pairs, the model is trained to reproduce target summaries. Unsupervised Summarization. Most unsupervised summarization work is extractive: sentences deemed relevant are pulled out of the original document and stitched into a summary, based on a heuristic for a sentence’s relevance (Mihalcea and Tarau, 2004; Barrios et al., 2015; West et al., 2019). Nikolov and Hahnloser (2019)’s abstractive approach is partially unsupervised, not requiring parallel data, but only a group of docume"
2020.acl-main.460,N19-1395,0,0.0381151,"Missing"
2020.acl-main.460,D18-1443,0,0.0198576,"n techniques (compression, merging, etc.) than prior automatic work and achieve higher levels of abstraction, reducing by almost half the gap between human-generated summaries and automatic summaries in terms of length of copied spans. 2 Related Work Supervised Abstractive Summarization. Sequence-to-sequence (seq2seq) (Sutskever et al., 2014) models trained using teacher-forcing are the most common approach to abstractive summarization (Nallapati et al., 2016). A common architecture is the Pointer-Generator (See et al., 2017). Performance can further be improved by constraining the attention (Gehrmann et al., 2018; Gui et al., 2019; Wang et al., 2019) and using pretrained Transformer-based language models (Lewis et al., 2019; Chi et al., 2019; Edunov et al., 2019). Through architectural changes, the training procedure remains constant: using a large corpus of document-summary pairs, the model is trained to reproduce target summaries. Unsupervised Summarization. Most unsupervised summarization work is extractive: sentences deemed relevant are pulled out of the original document and stitched into a summary, based on a heuristic for a sentence’s relevance (Mihalcea and Tarau, 2004; Barrios et al., 2015; W"
2020.acl-main.460,N18-1065,0,0.0383839,"Missing"
2020.acl-main.460,D19-1117,0,0.0119063,"on, merging, etc.) than prior automatic work and achieve higher levels of abstraction, reducing by almost half the gap between human-generated summaries and automatic summaries in terms of length of copied spans. 2 Related Work Supervised Abstractive Summarization. Sequence-to-sequence (seq2seq) (Sutskever et al., 2014) models trained using teacher-forcing are the most common approach to abstractive summarization (Nallapati et al., 2016). A common architecture is the Pointer-Generator (See et al., 2017). Performance can further be improved by constraining the attention (Gehrmann et al., 2018; Gui et al., 2019; Wang et al., 2019) and using pretrained Transformer-based language models (Lewis et al., 2019; Chi et al., 2019; Edunov et al., 2019). Through architectural changes, the training procedure remains constant: using a large corpus of document-summary pairs, the model is trained to reproduce target summaries. Unsupervised Summarization. Most unsupervised summarization work is extractive: sentences deemed relevant are pulled out of the original document and stitched into a summary, based on a heuristic for a sentence’s relevance (Mihalcea and Tarau, 2004; Barrios et al., 2015; West et al., 2019)."
2020.acl-main.460,P18-1064,0,0.0214747,"ring to include grammaticality, either by using a parsing model, or leveraging the Corpus of Linguistic Acceptability (Warstadt et al., 2019) could prove useful. Summarization in the wild. Because our method is unsupervised, it can be applied to new domains and languages. In this work, we benefited from pretrained BERT and GPT2 models in English, which do not yet exist publicly for other languages. Once they become available in other languages, the Summary Loop can be ported over. Abstraction dangers. Recent work around measuring factuality in generated text, using Natural Language Inference (Guo et al., 2018) or rule-based fact extraction (Zhang et al., 2019b) becomes increasingly important with summaries that are more abstractive. This work can be naturally included into the Summary Loop, with a fact-checker model generating an accuracy score. 7 Conclusion In this work we present a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. When tested on common news summarization datasets, our method significantly outperforms previous unsupervised methods, and gets within the range of competitive supervised metho"
2020.acl-main.460,W17-2701,1,0.861836,"Missing"
2020.acl-main.460,2020.acl-main.703,0,0.139014,"Missing"
2020.acl-main.460,W04-3252,0,0.0117812,"constraining the attention (Gehrmann et al., 2018; Gui et al., 2019; Wang et al., 2019) and using pretrained Transformer-based language models (Lewis et al., 2019; Chi et al., 2019; Edunov et al., 2019). Through architectural changes, the training procedure remains constant: using a large corpus of document-summary pairs, the model is trained to reproduce target summaries. Unsupervised Summarization. Most unsupervised summarization work is extractive: sentences deemed relevant are pulled out of the original document and stitched into a summary, based on a heuristic for a sentence’s relevance (Mihalcea and Tarau, 2004; Barrios et al., 2015; West et al., 2019). Nikolov and Hahnloser (2019)’s abstractive approach is partially unsupervised, not requiring parallel data, but only a group of documents and a group of summaries. In contrast, our work does not require any summaries, and is trained using only documents. Radford et al. (2019) summarize documents using a language model (GPT2) in a Zeroshot learning setting. The model reads the document followed by a special token “TL/DR”, and is tasked with continuing the document with a summary. Our work is an extension of this work: we initialize our Summarizer mode"
2020.acl-main.460,K16-1028,0,0.112759,"Missing"
2020.acl-main.460,2020.lrec-1.819,0,0.0632068,"Missing"
2020.acl-main.460,N18-2102,0,0.0202085,"ar et al., 2018) model using the generated summary. We refrain from using actual questions because question generation remains a challenge, and it is unclear how many questions should be generated to assess the quality of a summary. RL in Summarization. Paulus et al. (2018) introduced Reinforcement Learning (RL) to neural summarization methods by optimizing for ROUGE scores, leading to unreadable summaries. Since then, Reinforcement Learning has been used to select sentences with high ROUGE potential (Chen and Bansal, 2018), or optimize modified versions of ROUGE that account for readability (Pasunuru and Bansal, 2018). In all cases, the reward being computed relies on a reference summary, making the methods supervised. We craft a reward that does not require a target summary allowing our training process to remain unsupervised. 3 The Summary Loop For this work, the definition of a summary is: 5136 “A summary is a brief, fluent text that covers the main points of an original document.” Optimization 6 Summary score 1.1 Brevity, fluency and coverage are the three pillars of a good summary. Under a length constraint, a good quality summary should contain as much information about the original document as possi"
2020.acl-main.460,P18-2124,0,0.0368506,"our Summarizer model with a GPT2 and specialize it with a second unsupervised method. Summarization and Q&A. Eyal et al. (2019) and Arumae and Liu (2018) turn reference summaries into fill-in-the-blank (FIB) questions, either as an evaluation metric or to train an extractive summarization model. In this work, we directly generate FIB questions on the document being summarized, bypassing the need for a reference summary. Scialom et al. (2019)’s work stays closer to a Q&A scenario, and uses a Question Generation module to generate actual questions about the document, answered by a Squad-based (Rajpurkar et al., 2018) model using the generated summary. We refrain from using actual questions because question generation remains a challenge, and it is unclear how many questions should be generated to assess the quality of a summary. RL in Summarization. Paulus et al. (2018) introduced Reinforcement Learning (RL) to neural summarization methods by optimizing for ROUGE scores, leading to unreadable summaries. Since then, Reinforcement Learning has been used to select sentences with high ROUGE potential (Chen and Bansal, 2018), or optimize modified versions of ROUGE that account for readability (Pasunuru and Ban"
2020.acl-main.460,D19-1320,0,0.0193995,"e model reads the document followed by a special token “TL/DR”, and is tasked with continuing the document with a summary. Our work is an extension of this work: we initialize our Summarizer model with a GPT2 and specialize it with a second unsupervised method. Summarization and Q&A. Eyal et al. (2019) and Arumae and Liu (2018) turn reference summaries into fill-in-the-blank (FIB) questions, either as an evaluation metric or to train an extractive summarization model. In this work, we directly generate FIB questions on the document being summarized, bypassing the need for a reference summary. Scialom et al. (2019)’s work stays closer to a Q&A scenario, and uses a Question Generation module to generate actual questions about the document, answered by a Squad-based (Rajpurkar et al., 2018) model using the generated summary. We refrain from using actual questions because question generation remains a challenge, and it is unclear how many questions should be generated to assess the quality of a summary. RL in Summarization. Paulus et al. (2018) introduced Reinforcement Learning (RL) to neural summarization methods by optimizing for ROUGE scores, leading to unreadable summaries. Since then, Reinforcement Le"
2020.acl-main.460,P17-1099,0,0.0854557,"Missing"
2020.acl-main.460,D19-1304,0,0.0185631,"than prior automatic work and achieve higher levels of abstraction, reducing by almost half the gap between human-generated summaries and automatic summaries in terms of length of copied spans. 2 Related Work Supervised Abstractive Summarization. Sequence-to-sequence (seq2seq) (Sutskever et al., 2014) models trained using teacher-forcing are the most common approach to abstractive summarization (Nallapati et al., 2016). A common architecture is the Pointer-Generator (See et al., 2017). Performance can further be improved by constraining the attention (Gehrmann et al., 2018; Gui et al., 2019; Wang et al., 2019) and using pretrained Transformer-based language models (Lewis et al., 2019; Chi et al., 2019; Edunov et al., 2019). Through architectural changes, the training procedure remains constant: using a large corpus of document-summary pairs, the model is trained to reproduce target summaries. Unsupervised Summarization. Most unsupervised summarization work is extractive: sentences deemed relevant are pulled out of the original document and stitched into a summary, based on a heuristic for a sentence’s relevance (Mihalcea and Tarau, 2004; Barrios et al., 2015; West et al., 2019). Nikolov and Hahnlos"
2020.acl-main.460,Q19-1040,0,0.0259975,"onstraint by summarizing the same document under three different length constraints. Each model adapts to its word budget. However, length is only one way to customize summaries. One might want to summarize based on point of view, chronology, theme, etc. Fluency vs. Grammaticality. By choosing to represent the validity of summaries with a Language model, we encourage fluent summaries (i.e., with likely sequences of words) but not necessarily grammatical ones. Extending the scoring to include grammaticality, either by using a parsing model, or leveraging the Corpus of Linguistic Acceptability (Warstadt et al., 2019) could prove useful. Summarization in the wild. Because our method is unsupervised, it can be applied to new domains and languages. In this work, we benefited from pretrained BERT and GPT2 models in English, which do not yet exist publicly for other languages. Once they become available in other languages, the Summary Loop can be ported over. Abstraction dangers. Recent work around measuring factuality in generated text, using Natural Language Inference (Guo et al., 2018) or rule-based fact extraction (Zhang et al., 2019b) becomes increasingly important with summaries that are more abstractive"
2020.acl-main.460,D19-1389,0,0.0407296,"8; Gui et al., 2019; Wang et al., 2019) and using pretrained Transformer-based language models (Lewis et al., 2019; Chi et al., 2019; Edunov et al., 2019). Through architectural changes, the training procedure remains constant: using a large corpus of document-summary pairs, the model is trained to reproduce target summaries. Unsupervised Summarization. Most unsupervised summarization work is extractive: sentences deemed relevant are pulled out of the original document and stitched into a summary, based on a heuristic for a sentence’s relevance (Mihalcea and Tarau, 2004; Barrios et al., 2015; West et al., 2019). Nikolov and Hahnloser (2019)’s abstractive approach is partially unsupervised, not requiring parallel data, but only a group of documents and a group of summaries. In contrast, our work does not require any summaries, and is trained using only documents. Radford et al. (2019) summarize documents using a language model (GPT2) in a Zeroshot learning setting. The model reads the document followed by a special token “TL/DR”, and is tasked with continuing the document with a summary. Our work is an extension of this work: we initialize our Summarizer model with a GPT2 and specialize it with a sec"
2020.bea-1.5,P16-1154,0,0.0609068,"Missing"
2020.bea-1.5,D18-1547,0,0.0384349,"s the Cornell Movie Script Dataset which includes 300,000 utterances (Danescu-Niculescu-Mizil and Lee, 2011). By contrast, goal oriented dialogue systems have a specific task to complete, such as restaurant (Wen et al., 2017) and movie (Yu et al., 2017c) recommendations as well as restaurant reservations (Bordes et al., 2017). Neural goal-oriented dialogue systems require large amounts of data to train. Bordes et al. (2017) include 6 restaurant reservation tasks, with 1,000 training dialogues in each dataset. Multidomain datasets such as MultiWOZ include 10k dialogues spanning multiple tasks (Budzianowski et al., 2018). For longer-term interactions, a dataset involving medical diagnosis has approximately 200 conversations per disease (Wei et al., 2018). By contrast, prior work in the field of intelligent tutoring dialogues has widely relied on large rule-based systems injected with human-crafted domain knowledge (Anderson et al., 1995; Aleven et al., 2001; Graesser et al., 2001; VanLehn et al., 2002; Rus et al., 2015b). Many of these systems involve students answering multiple choice or fillin-the-blank questions and being presented with a hint or explanation when they answer incorrectly. However, curating"
2020.bea-1.5,E17-1042,0,0.0461785,"Missing"
2020.bea-1.5,P02-1040,0,0.106982,"<EOC&gt; Tutor: Well, “bunny” is “coniglio” Student: il gatto e di fronte al coniglio. In this representation, domain information and an intended set of actions to take is separated with a special token <EOC&gt; from two sentences of conversation. Model training details are in Appendix E. We split the data along conversations into 2296 train, 217 development, and 107 test utterances. 7.1 Generation Quality Results One benefit of CIMA is the ability to compare generated text to multiple distinct reference sentences in order to measure quality. We apply two standard generation quality measures: BLEU (Papineni et al., 2002) and BERT F1 Score (Zhang* et al., 5 As we did not see a gain in quality when including the full conversation, we simplify the task to responding to the most recent tutor and student utterance. BERT F1 0.45 0.53 Table 5: Generation quality results comparing a rulebased baseline to the neural Generation model. Table 4: For each student action(s), percentage of tutor groups who agree on a unified action set in response. 7 BLEU 0.34 0.31 7.2 Action Evaluation Results In addition to quality, we examine whether the Generation model is able to generate utterances consistent with the set of actions i"
2020.bea-1.5,W16-3643,0,0.0174867,"et al., 2017b), which is a corpus of humans tutoring each other with the names of colored shapes in a made-up foreign language. In each session, an image is given to help scaffold the dialogue. The corpus contains 177 conversations with 2454 turns in total. This corpus has been utilized to ground deep learning model representations of visual attributes (colors and shapes) in dialogue via interacting with a simulated tutor (Ling and Fidler, 2017; Yu et al., 2017b). Follow-up work has used this data to model a student learning names and colors of shapes using a reinforcement learning framework (Yu et al., 2016, 2017a). Our approach differs from that of Yu et al. (2017b) in several ways, including that we tie the colored shape tutoring interactions to the more complex domain of prepositional phrases. Additionally, by using a real foreign language (Italian) we are able to leverage words with similar morphological properties in addition to well-defined grammar rules. 2.3 Crowdwork Dialogue Role-Playing Prior work has shown that crowdworkers are effective at role-playing. Self-dialogue, where a single crowdworker role-plays both sides of a conversation, has been used to collect chit-chat data (Krause 1"
2020.bea-1.5,D14-1162,0,0.0820928,"Missing"
2020.bea-1.5,W17-2802,0,0.314596,"conditioned on a desired set of tutoring actions. 2 Modern work in dialogue falls into two categories: chit-chat models and goal-oriented models. Chit-chat models aim to creating interesting, diversely-worded utterances which further a conversation and keep users engaged. These models have the advantage of leveraging large indirectly-collected datasets, such as the Cornell Movie Script Dataset which includes 300,000 utterances (Danescu-Niculescu-Mizil and Lee, 2011). By contrast, goal oriented dialogue systems have a specific task to complete, such as restaurant (Wen et al., 2017) and movie (Yu et al., 2017c) recommendations as well as restaurant reservations (Bordes et al., 2017). Neural goal-oriented dialogue systems require large amounts of data to train. Bordes et al. (2017) include 6 restaurant reservation tasks, with 1,000 training dialogues in each dataset. Multidomain datasets such as MultiWOZ include 10k dialogues spanning multiple tasks (Budzianowski et al., 2018). For longer-term interactions, a dataset involving medical diagnosis has approximately 200 conversations per disease (Wei et al., 2018). By contrast, prior work in the field of intelligent tutoring dialogues has widely relied"
2020.bea-1.5,W17-2001,0,0.362925,"conditioned on a desired set of tutoring actions. 2 Modern work in dialogue falls into two categories: chit-chat models and goal-oriented models. Chit-chat models aim to creating interesting, diversely-worded utterances which further a conversation and keep users engaged. These models have the advantage of leveraging large indirectly-collected datasets, such as the Cornell Movie Script Dataset which includes 300,000 utterances (Danescu-Niculescu-Mizil and Lee, 2011). By contrast, goal oriented dialogue systems have a specific task to complete, such as restaurant (Wen et al., 2017) and movie (Yu et al., 2017c) recommendations as well as restaurant reservations (Bordes et al., 2017). Neural goal-oriented dialogue systems require large amounts of data to train. Bordes et al. (2017) include 6 restaurant reservation tasks, with 1,000 training dialogues in each dataset. Multidomain datasets such as MultiWOZ include 10k dialogues spanning multiple tasks (Budzianowski et al., 2018). For longer-term interactions, a dataset involving medical diagnosis has approximately 200 conversations per disease (Wei et al., 2018). By contrast, prior work in the field of intelligent tutoring dialogues has widely relied"
2020.bea-1.5,P18-2033,0,0.016462,"systems have a specific task to complete, such as restaurant (Wen et al., 2017) and movie (Yu et al., 2017c) recommendations as well as restaurant reservations (Bordes et al., 2017). Neural goal-oriented dialogue systems require large amounts of data to train. Bordes et al. (2017) include 6 restaurant reservation tasks, with 1,000 training dialogues in each dataset. Multidomain datasets such as MultiWOZ include 10k dialogues spanning multiple tasks (Budzianowski et al., 2018). For longer-term interactions, a dataset involving medical diagnosis has approximately 200 conversations per disease (Wei et al., 2018). By contrast, prior work in the field of intelligent tutoring dialogues has widely relied on large rule-based systems injected with human-crafted domain knowledge (Anderson et al., 1995; Aleven et al., 2001; Graesser et al., 2001; VanLehn et al., 2002; Rus et al., 2015b). Many of these systems involve students answering multiple choice or fillin-the-blank questions and being presented with a hint or explanation when they answer incorrectly. However, curating this domain knowledge is timeexpensive, rule-based systems can be rigid, and the typical system does not include multiple rephrasings of"
2020.emnlp-demos.18,W04-1213,0,0.148197,"e the graph by clicking nodes to further explore new associations (e.g., clicking liver damage to potentially discover more related drugs and diseases). Navigation is known to help facilitate exploration (Kairam et al., 2015), such as when users do not have a pinpointed query in mind (White and Roth, 2009). Entity extraction and selection To extract entities we use S2ORC-BERT (Lo et al., 2020), a new language model pre-trained on a large cor5 pus of scientific papers. This model is fine-tuned on two separate biomedical named entity recognition (NER) tasks (BC5CDR (Li et al., 2016) and JNLPBA (Kim et al., 2004)), enabling us to extract spans of text corresponding to proteins, genes, cells, drugs, and diseases from across the corpus. We extract entities only from titles and abstracts of papers to reduce noise and focus on the more salient entities in each paper. We show only entities collocated at least twice with other enti(b) Figure 1: (a) Collocation explorer: corpus-wide associations between biomedical entities, such as drugs and conditions. Highlighted in the figure is the edge between Chloroquine and liver injury. (b) Exploratory search of connections between patient characteristics and interve"
2020.emnlp-demos.18,2021.ccl-1.108,0,0.0391281,"Missing"
2020.emnlp-demos.18,2020.acl-main.447,1,0.746087,"g our example, Marc can search for Chloroquine and see its network of associations, such as a potential connection to liver damage, or its connection to other drugs such as the anti-viral drug Ribavirin. Marc can navigate the graph by clicking nodes to further explore new associations (e.g., clicking liver damage to potentially discover more related drugs and diseases). Navigation is known to help facilitate exploration (Kairam et al., 2015), such as when users do not have a pinpointed query in mind (White and Roth, 2009). Entity extraction and selection To extract entities we use S2ORC-BERT (Lo et al., 2020), a new language model pre-trained on a large cor5 pus of scientific papers. This model is fine-tuned on two separate biomedical named entity recognition (NER) tasks (BC5CDR (Li et al., 2016) and JNLPBA (Kim et al., 2004)), enabling us to extract spans of text corresponding to proteins, genes, cells, drugs, and diseases from across the corpus. We extract entities only from titles and abstracts of papers to reduce noise and focus on the more salient entities in each paper. We show only entities collocated at least twice with other enti(b) Figure 1: (a) Collocation explorer: corpus-wide associat"
2020.emnlp-demos.18,2020.nlpcovid19-acl.1,1,0.872394,"Missing"
2020.sdp-1.22,Q15-1038,0,0.0236572,"mance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications. 1 are softmax-normalized weights and the scalar [...] Introduction Automatic definition detection is an important task in natural language processing (NLP). Definitions can be used for a variety of downstream tasks, such as ontology matching and construction (Bovi et al., 2015), paraphrasing (Hashimoto et al., 2011), and word sense disambiguation (Banerjee and Pedersen, 2002; Huang et al., 2019). Prior work in automated definition detection has addressed the domain of scholarly articles (Reiplinger et al., 2012; Jin et al., 2013; Espinosa-Anke and Schockaert, 2018; Vanetik et al., 2020; Veyseh et al., 2020). Definition detection is especially important for scholarly papers because they often use unfamiliar technical terms that readers must understand to properly comprehend the article. In formal terms, definition detection is comprised of two tasks: classifying sent"
2020.sdp-1.22,N19-1423,0,0.0107514,"us token spans) seem likely to be highly effective to address the errors in Table 3, such as discarding output that does not successfully predict both a term and a definition. In the next section, we implement the first three solution types on top of the state-ofthe-art system and report the resulting performance improvements. Tokens TL is fine-tuned on the task of definition extraction, whereas the joint model encodes input from a combination of a graph convolutional network and a 2 BERT encoder without fine-tuning. We evaluate several state-of-the-art encoders for this task, including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), and SciBERT (Beltagy et al., 2019). Second, HEDDEx is provided with additional syntactic features as input. These features include parts of speech, syntactic dependencies, and the token-level labels provided by entity recognizers and abbreviation detectors (Schwartz and Hearst, 2003). The features are extracted using off-the3 shelf tools like Spacy and SciSpacy (Neumann et al., 2019). Third, the output of the CRF and sentence classifier is refined using heuristic rules. The rules clean up the slot tags produced by the CRF, and override predictions made by the sen"
2020.sdp-1.22,N18-2061,0,0.0300599,"Missing"
2020.sdp-1.22,W06-2609,0,0.0802545,"like definition detection still poses significant challenges to NLP, and that this is an area that needs more focus in the scholarly document processing community. 2 Related Work Definition detection has been tackled in several ways in prior research. The traditional rule-based systems (Muresan and Klavans, 2002; Westerhout and Monachesi, 2008; Westerhout, 2009a) used hand-written definition patterns (e.g., “is defined as“) and linguistic features (e.g., pronoun, verb, punctuation), providing high precision but low recall detection. To address the low recall problem, model-driven approaches (Fahmi and Bouma, 2006; Westerhout, 2009b; Navigli and Velardi, 2010; Reiplinger et al., 2012) were developed using statistical and syntactic features such as bagof-words, sentence position, part-of-speech (POS) tags, and their combination with hand-written rules. Notably, Jin et al. (2013) used conditional random field (CRF) (Lafferty et al., 2001) to predict tags of each token in a sentence such as TERM for term tokens, DEF for definition tokens, and O for neither. Recently, sophisticated neural models such as convolutional networks (Espinosa-Anke and Schockaert, 2018) and graph convolutional networks (Veyseh et"
2020.sdp-1.22,P11-1109,0,0.0748146,"Missing"
2020.sdp-1.22,D19-1355,0,0.0137206,"ty of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications. 1 are softmax-normalized weights and the scalar [...] Introduction Automatic definition detection is an important task in natural language processing (NLP). Definitions can be used for a variety of downstream tasks, such as ontology matching and construction (Bovi et al., 2015), paraphrasing (Hashimoto et al., 2011), and word sense disambiguation (Banerjee and Pedersen, 2002; Huang et al., 2019). Prior work in automated definition detection has addressed the domain of scholarly articles (Reiplinger et al., 2012; Jin et al., 2013; Espinosa-Anke and Schockaert, 2018; Vanetik et al., 2020; Veyseh et al., 2020). Definition detection is especially important for scholarly papers because they often use unfamiliar technical terms that readers must understand to properly comprehend the article. In formal terms, definition detection is comprised of two tasks: classifying sentences as containing definitions or not, and identifying which spans within these sentences contain terms and definitions"
2020.sdp-1.22,D13-1073,0,0.111709,"provements, and potential issues for the development of reading aid applications. 1 are softmax-normalized weights and the scalar [...] Introduction Automatic definition detection is an important task in natural language processing (NLP). Definitions can be used for a variety of downstream tasks, such as ontology matching and construction (Bovi et al., 2015), paraphrasing (Hashimoto et al., 2011), and word sense disambiguation (Banerjee and Pedersen, 2002; Huang et al., 2019). Prior work in automated definition detection has addressed the domain of scholarly articles (Reiplinger et al., 2012; Jin et al., 2013; Espinosa-Anke and Schockaert, 2018; Vanetik et al., 2020; Veyseh et al., 2020). Definition detection is especially important for scholarly papers because they often use unfamiliar technical terms that readers must understand to properly comprehend the article. In formal terms, definition detection is comprised of two tasks: classifying sentences as containing definitions or not, and identifying which spans within these sentences contain terms and definitions. As the performance of definition extractors continues to improve, these algorithms could pave the way for new types of intelligent ass"
2020.sdp-1.22,P19-1233,0,0.269825,"be highly effective to address the errors in Table 3, such as discarding output that does not successfully predict both a term and a definition. In the next section, we implement the first three solution types on top of the state-ofthe-art system and report the resulting performance improvements. Tokens TL is fine-tuned on the task of definition extraction, whereas the joint model encodes input from a combination of a graph convolutional network and a 2 BERT encoder without fine-tuning. We evaluate several state-of-the-art encoders for this task, including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), and SciBERT (Beltagy et al., 2019). Second, HEDDEx is provided with additional syntactic features as input. These features include parts of speech, syntactic dependencies, and the token-level labels provided by entity recognizers and abbreviation detectors (Schwartz and Hearst, 2003). The features are extracted using off-the3 shelf tools like Spacy and SciSpacy (Neumann et al., 2019). Third, the output of the CRF and sentence classifier is refined using heuristic rules. The rules clean up the slot tags produced by the CRF, and override predictions made by the sentence classifier. The rules"
2020.sdp-1.22,2021.ccl-1.108,0,0.0571394,"Missing"
2020.sdp-1.22,2020.acl-main.447,1,0.699146,"F1) over the combination of Joint and SciBERT. Given the modest improvement in term and definition tagging, we suspect that much of this improvement can be accounted for by the correction of position markers in the slot tags (i.e., distinguishing between B and I in the tag assignments). In the following experiments, we call HEDDEx the combination of three components: the encoder (SciBERT or RoBERTa), syntactic features, and heuristic filters. Error Analysis on Predicted Definitions To assess how well HEDDEx works at the document level, we randomly sampled 50 ACL papers from the S2ORC dataset (Lo et al., 2020), a large corpus of 81.1M English-language academic papers spanning many academic disciplines. We ran the pretrained HEDDEx model on every sentence of every document; if the model detected a term/definition pair, the corresponding sentence was output for assessment. (Note that this analysis can estimate precision but not recall, as false negatives are not detected.) 200 Macro P/R/F TERM P/R/F DEF P/R/F Partial F Clsf. DefMiner (Jin et al., 2013) LSTM-CRF (Li et al., 2016) GCDT (Liu et al., 2019a) Joint (Veyseh et al., 2020) Joint* (Veyseh et al., 2020) 52.5 / 49.5 / 50.5 57.1 / 55.9 / 56.2 57."
2020.sdp-1.22,muresan-klavans-2002-method,0,0.210079,"x outperforms the state of the art, while revealing opportunities for future improvements. In summary, this paper draws attention to the work yet to be done in addressing the task of document-level definition detection for scholarly documents. We draw attention to the fact that a seemingly straightforward task like definition detection still poses significant challenges to NLP, and that this is an area that needs more focus in the scholarly document processing community. 2 Related Work Definition detection has been tackled in several ways in prior research. The traditional rule-based systems (Muresan and Klavans, 2002; Westerhout and Monachesi, 2008; Westerhout, 2009a) used hand-written definition patterns (e.g., “is defined as“) and linguistic features (e.g., pronoun, verb, punctuation), providing high precision but low recall detection. To address the low recall problem, model-driven approaches (Fahmi and Bouma, 2006; Westerhout, 2009b; Navigli and Velardi, 2010; Reiplinger et al., 2012) were developed using statistical and syntactic features such as bagof-words, sentence position, part-of-speech (POS) tags, and their combination with hand-written rules. Notably, Jin et al. (2013) used conditional random"
2020.sdp-1.22,P10-1134,0,0.0319101,"nificant challenges to NLP, and that this is an area that needs more focus in the scholarly document processing community. 2 Related Work Definition detection has been tackled in several ways in prior research. The traditional rule-based systems (Muresan and Klavans, 2002; Westerhout and Monachesi, 2008; Westerhout, 2009a) used hand-written definition patterns (e.g., “is defined as“) and linguistic features (e.g., pronoun, verb, punctuation), providing high precision but low recall detection. To address the low recall problem, model-driven approaches (Fahmi and Bouma, 2006; Westerhout, 2009b; Navigli and Velardi, 2010; Reiplinger et al., 2012) were developed using statistical and syntactic features such as bagof-words, sentence position, part-of-speech (POS) tags, and their combination with hand-written rules. Notably, Jin et al. (2013) used conditional random field (CRF) (Lafferty et al., 2001) to predict tags of each token in a sentence such as TERM for term tokens, DEF for definition tokens, and O for neither. Recently, sophisticated neural models such as convolutional networks (Espinosa-Anke and Schockaert, 2018) and graph convolutional networks (Veyseh et al., 2020) have been applied to obtain better"
2020.sdp-1.22,W19-5034,0,0.027201,"ncodes input from a combination of a graph convolutional network and a 2 BERT encoder without fine-tuning. We evaluate several state-of-the-art encoders for this task, including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), and SciBERT (Beltagy et al., 2019). Second, HEDDEx is provided with additional syntactic features as input. These features include parts of speech, syntactic dependencies, and the token-level labels provided by entity recognizers and abbreviation detectors (Schwartz and Hearst, 2003). The features are extracted using off-the3 shelf tools like Spacy and SciSpacy (Neumann et al., 2019). Third, the output of the CRF and sentence classifier is refined using heuristic rules. The rules clean up the slot tags produced by the CRF, and override predictions made by the sentence classifier. The rules include, among other rules: transfer learning Feature Extraction POS · NP / VP entity · acronym · ... Syntactic Features NNP B-T VBZ O NN B-D NNP I-D Encoders CRF Slot Tags B-T O B-D I-D Sentence Classifier Heuristic Rules repair tags · filter bad predictions B-T Output &quot;TL&quot; term O B-D I-D &quot;transfer learning&quot; definition 0/1 decision • Do not classify a sentence as a definition if it onl"
2020.sdp-1.22,passonneau-2006-measuring,0,0.0723989,"a document, allowing assessment of recall as well as precision. Two annotators annotated two full papers using an annotation scheme similar to that used in DEFT (Spala et al., 2019) except for omitting cross-sentence links. We chose to annotate two award-winning ACL papers: ELMo (Peters et al., 2018) and LISA (Strubell et al., 2018) resulting in 485 total sentences from which we identified 98 definitional and 387 non-definitional sentences. Similar to DEFT (Spala et al., 2019), we measured inter-annotator agreement using Krippendorff’s alpha (Krippendorff, 2011) with the MASI distance metric (Passonneau, 2006). We obtained 0.626 for terms and 201 0.527 for definitions, where the agreement score for terms is lower than those in DEFT annotations (0.80). This may be because our annotations for terms include various types such as textual terms, acronyms, and math symbols, while terms in DEFT are only textual terms. The task was quite difficult: each annotator takes two and half hours to annotate a single paper. Future work will include refining the annotation scheme to ensure more consistency among annotators and to annotate more documents. 5.3 Evaluation on Document-level Definitions We evaluated docu"
2020.sdp-1.22,E09-3011,0,0.0206824,"nities for future improvements. In summary, this paper draws attention to the work yet to be done in addressing the task of document-level definition detection for scholarly documents. We draw attention to the fact that a seemingly straightforward task like definition detection still poses significant challenges to NLP, and that this is an area that needs more focus in the scholarly document processing community. 2 Related Work Definition detection has been tackled in several ways in prior research. The traditional rule-based systems (Muresan and Klavans, 2002; Westerhout and Monachesi, 2008; Westerhout, 2009a) used hand-written definition patterns (e.g., “is defined as“) and linguistic features (e.g., pronoun, verb, punctuation), providing high precision but low recall detection. To address the low recall problem, model-driven approaches (Fahmi and Bouma, 2006; Westerhout, 2009b; Navigli and Velardi, 2010; Reiplinger et al., 2012) were developed using statistical and syntactic features such as bagof-words, sentence position, part-of-speech (POS) tags, and their combination with hand-written rules. Notably, Jin et al. (2013) used conditional random field (CRF) (Lafferty et al., 2001) to predict ta"
2020.sdp-1.22,westerhout-monachesi-2008-creating,0,0.0220595,"the art, while revealing opportunities for future improvements. In summary, this paper draws attention to the work yet to be done in addressing the task of document-level definition detection for scholarly documents. We draw attention to the fact that a seemingly straightforward task like definition detection still poses significant challenges to NLP, and that this is an area that needs more focus in the scholarly document processing community. 2 Related Work Definition detection has been tackled in several ways in prior research. The traditional rule-based systems (Muresan and Klavans, 2002; Westerhout and Monachesi, 2008; Westerhout, 2009a) used hand-written definition patterns (e.g., “is defined as“) and linguistic features (e.g., pronoun, verb, punctuation), providing high precision but low recall detection. To address the low recall problem, model-driven approaches (Fahmi and Bouma, 2006; Westerhout, 2009b; Navigli and Velardi, 2010; Reiplinger et al., 2012) were developed using statistical and syntactic features such as bagof-words, sentence position, part-of-speech (POS) tags, and their combination with hand-written rules. Notably, Jin et al. (2013) used conditional random field (CRF) (Lafferty et al., 2"
2020.sdp-1.22,N18-1202,0,0.180053,"definition annotation collections select unrelated sentences from across a document collection. As mentioned in the introduction, we are interested in annotating full papers, which requires finding every definition within a given paper. Therefore, we created a new collection in which we annotate every sentence within a document, allowing assessment of recall as well as precision. Two annotators annotated two full papers using an annotation scheme similar to that used in DEFT (Spala et al., 2019) except for omitting cross-sentence links. We chose to annotate two award-winning ACL papers: ELMo (Peters et al., 2018) and LISA (Strubell et al., 2018) resulting in 485 total sentences from which we identified 98 definitional and 387 non-definitional sentences. Similar to DEFT (Spala et al., 2019), we measured inter-annotator agreement using Krippendorff’s alpha (Krippendorff, 2011) with the MASI distance metric (Passonneau, 2006). We obtained 0.626 for terms and 201 0.527 for definitions, where the agreement score for terms is lower than those in DEFT annotations (0.80). This may be because our annotations for terms include various types such as textual terms, acronyms, and math symbols, while terms in DEFT"
2020.sdp-1.22,W12-3206,0,0.0999298,"n detection, ideas for improvements, and potential issues for the development of reading aid applications. 1 are softmax-normalized weights and the scalar [...] Introduction Automatic definition detection is an important task in natural language processing (NLP). Definitions can be used for a variety of downstream tasks, such as ontology matching and construction (Bovi et al., 2015), paraphrasing (Hashimoto et al., 2011), and word sense disambiguation (Banerjee and Pedersen, 2002; Huang et al., 2019). Prior work in automated definition detection has addressed the domain of scholarly articles (Reiplinger et al., 2012; Jin et al., 2013; Espinosa-Anke and Schockaert, 2018; Vanetik et al., 2020; Veyseh et al., 2020). Definition detection is especially important for scholarly papers because they often use unfamiliar technical terms that readers must understand to properly comprehend the article. In formal terms, definition detection is comprised of two tasks: classifying sentences as containing definitions or not, and identifying which spans within these sentences contain terms and definitions. As the performance of definition extractors continues to improve, these algorithms could pave the way for new types"
2020.sdp-1.22,W19-4015,0,0.0205096,"Missing"
2020.sdp-1.22,D18-1548,0,0.028839,"Missing"
2020.sdp-1.22,2020.lrec-1.256,0,0.0671101,"f reading aid applications. 1 are softmax-normalized weights and the scalar [...] Introduction Automatic definition detection is an important task in natural language processing (NLP). Definitions can be used for a variety of downstream tasks, such as ontology matching and construction (Bovi et al., 2015), paraphrasing (Hashimoto et al., 2011), and word sense disambiguation (Banerjee and Pedersen, 2002; Huang et al., 2019). Prior work in automated definition detection has addressed the domain of scholarly articles (Reiplinger et al., 2012; Jin et al., 2013; Espinosa-Anke and Schockaert, 2018; Vanetik et al., 2020; Veyseh et al., 2020). Definition detection is especially important for scholarly papers because they often use unfamiliar technical terms that readers must understand to properly comprehend the article. In formal terms, definition detection is comprised of two tasks: classifying sentences as containing definitions or not, and identifying which spans within these sentences contain terms and definitions. As the performance of definition extractors continues to improve, these algorithms could pave the way for new types of intelligent assistance for readers of dense technical documents. For exam"
2020.sdp-1.22,W09-4410,0,0.0165648,"nities for future improvements. In summary, this paper draws attention to the work yet to be done in addressing the task of document-level definition detection for scholarly documents. We draw attention to the fact that a seemingly straightforward task like definition detection still poses significant challenges to NLP, and that this is an area that needs more focus in the scholarly document processing community. 2 Related Work Definition detection has been tackled in several ways in prior research. The traditional rule-based systems (Muresan and Klavans, 2002; Westerhout and Monachesi, 2008; Westerhout, 2009a) used hand-written definition patterns (e.g., “is defined as“) and linguistic features (e.g., pronoun, verb, punctuation), providing high precision but low recall detection. To address the low recall problem, model-driven approaches (Fahmi and Bouma, 2006; Westerhout, 2009b; Navigli and Velardi, 2010; Reiplinger et al., 2012) were developed using statistical and syntactic features such as bagof-words, sentence position, part-of-speech (POS) tags, and their combination with hand-written rules. Notably, Jin et al. (2013) used conditional random field (CRF) (Lafferty et al., 2001) to predict ta"
2021.acl-long.498,2020.emnlp-main.506,0,0.0386032,"Missing"
2021.acl-long.498,C18-1039,0,0.0164619,"that without explicitly representing edits, the KiS model easily learns to copy (using attention heads) and deviate from the original text. Outputs can be post-processed into edits, if desired. Unsupervised Simplification has mostly been limited to lexical simplification. Recently Surya et al. (2019) (Unsup NTS) proposed a system that 3 6366 https://newsela.com/ can perform both lexical and syntactic simplification, with a joint encoder, and two decoders (simple and complex). We directly compare our unsupervised approach to Unsup NTS. RL for Simplification. Prior work (Zhang and Lapata, 2017; Guo et al., 2018) used Reinforcement Learning (RL)-based simplification. However, in both cases, components of the reward or training procedure involved reference simplifications, requiring an aligned dataset. By designing a reference-free reward, we are able to train our model with RL without supervision. Evaluation of Simplification. This usually falls into two categories: automatic offline evaluation, and human evaluation. Automatic evaluations usually involve using n-gram overlap calculations such as BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016)). SARI was shown to correlate better with human jud"
2021.acl-long.498,C18-1121,0,0.0599255,"Missing"
2021.acl-long.498,W17-2623,0,0.0237833,", and KiS model output (right). Participants read a text and answered comprehension questions (bottom). Average completion time was 160 seconds (original) and 136 seconds (KiS model output). technical terms increase the impact of simplification. Section length. We chose document length of 3-4 paragraphs (or 200 words), and five comprehension questions. Document length should not be too short (makes some questions trivial), or too long (adds a retrieval component to the task). Selection of questions. Questions were generated via a GPT2 question generation model finetuned on the NewsQA dataset (Trischler et al., 2017). We select questions answerable by both the original and Newsela references, attempting to have both factoid (answer is entity) and reasoning questions. Re-submission until correct. When submitting answers, participants received feedback on which were incorrect, and were required to re-submit until all answers were correct. This aligns the objective of the participant (i.e., finishing the task rapidly), with the task’s objective (i.e., measuring participant’s efficiency at understanding). This also gives a way to discourage participants from “bruteforcing” the task, re-submitting many combina"
2021.acl-long.498,2020.acl-main.450,0,0.0320914,"Missing"
2021.acl-long.498,P12-1107,0,0.0728909,"Missing"
2021.acl-long.498,P02-1040,0,0.110812,"d approach to Unsup NTS. RL for Simplification. Prior work (Zhang and Lapata, 2017; Guo et al., 2018) used Reinforcement Learning (RL)-based simplification. However, in both cases, components of the reward or training procedure involved reference simplifications, requiring an aligned dataset. By designing a reference-free reward, we are able to train our model with RL without supervision. Evaluation of Simplification. This usually falls into two categories: automatic offline evaluation, and human evaluation. Automatic evaluations usually involve using n-gram overlap calculations such as BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016)). SARI was shown to correlate better with human judgements of simplicity than BLEU, and it has since become a standard (Zhang and Lapata, 2017; Surya et al., 2019; Martin et al., 2020). In our experiments, we report both SARI and BLEU. Human evaluation is typically done in an intrinsic way – e.g., by directly rating factors like fluency, simplicity and relevance of model outputs (Surya et al., 2019; Wubben et al., 2012). In this work, we propose an extrinsic, task-based protocol. In our comprehension study, we directly measure how much simplified texts can help a hu"
2021.acl-long.498,2020.acl-main.240,0,0.0259349,"airs have a positive ∆Z(W1 , W2 ), with a median value of 0.4. We use this median as the target Zipf shift in the LScore , and use a ramp shape similar to the SScore , clipped between 0 and 1 (denoted as [·]+ ): #+ "" LScore (W1 , W2 ) = 1 − 3.2 |∆Z(W1 ,W2 )−0.4| 0.4 (2) Fluency We use two sub-components for the fluency component: a pre-trained language-model, and a discriminator trained dynamically with the generator. 3.2.1 Language-Model Fluency Language models assign a probability to a sequence of words. This probability is often used to measure fluency of generated text (Kann et al., 2018; Salazar et al., 2020). The KiS fluency score is based on a language model in a way similar way to Laban et al. (2020). The language model is used to obtain a likelihood of the original paragraph (LM (p)) and of the generated output LM (q). We use average log-likelihood, for numerical stability. The language model fluency score is then: h LM (p) − LM (q) i+ LMScore (p, q) = 1 − (3) λ λ is a tunable hyper-parameter. If the LM (q) is lower than LM (p) by λ or more, LMScore (p, q) = 0. If LM (q) is above or equal to LM (p), then LMScore (p, q) = 1, and otherwise, it is a linear interpolation. We set λ = 1.3 as it is t"
2021.acl-long.498,2020.emnlp-main.418,0,0.0288105,"et with coarse-level alignments for calibration. Lexical Simplification focuses on the substitution of single words or phrases with simpler equivalents, with diverse approaches using lexical databases such as WordNet (Thomas and Anderson, 2012), to using contextualized word vectors (Qiang et al., 2020). These methods tend to be limited, as they do not consider syntactic complexity, and have no direct way of modeling deletions and insertions. We incorporate a lexical score (LScore ) as one of the rewards in our simplicity component. Text-edit for Simplification. Recent work (Dong et al., 2019; Stahlberg and Kumar, 2020) has modeled text simplification as a text-edit task, learning sequences of word-edits that transform the input into the output. Text editing offers explainability, at the cost of added model complexity. We find that without explicitly representing edits, the KiS model easily learns to copy (using attention heads) and deviate from the original text. Outputs can be post-processed into edits, if desired. Unsupervised Simplification has mostly been limited to lexical simplification. Recently Surya et al. (2019) (Unsup NTS) proposed a system that 3 6366 https://newsela.com/ can perform both lexica"
2021.acl-long.498,P19-1198,0,0.120049,"licity component. Text-edit for Simplification. Recent work (Dong et al., 2019; Stahlberg and Kumar, 2020) has modeled text simplification as a text-edit task, learning sequences of word-edits that transform the input into the output. Text editing offers explainability, at the cost of added model complexity. We find that without explicitly representing edits, the KiS model easily learns to copy (using attention heads) and deviate from the original text. Outputs can be post-processed into edits, if desired. Unsupervised Simplification has mostly been limited to lexical simplification. Recently Surya et al. (2019) (Unsup NTS) proposed a system that 3 6366 https://newsela.com/ can perform both lexical and syntactic simplification, with a joint encoder, and two decoders (simple and complex). We directly compare our unsupervised approach to Unsup NTS. RL for Simplification. Prior work (Zhang and Lapata, 2017; Guo et al., 2018) used Reinforcement Learning (RL)-based simplification. However, in both cases, components of the reward or training procedure involved reference simplifications, requiring an aligned dataset. By designing a reference-free reward, we are able to train our model with RL without superv"
2021.acl-long.498,Q15-1021,0,0.0265698,"ly, we train our models to simplify full paragraphs, and evaluate our models in a human evaluation on short documents (i.e., 3-4 paragraphs). Through rigorous empirical evaluation, we demonstrate the strong performance of our approach; automated results show that this unsupervised approach is able to outperform strong supervised models by 4 SARI points or more. We publicly released the code and model checkpoints1 . 2 Related Work Simplification Datasets. Early datasets were first based on Simple Wikipedia2 : WikiSmall (Zhu et al., 2010), later expanded into WikiLarge (Zhang and Lapata, 2017). Xu et al. (2015) show there are quality concerns with Simple Wikipedia datasets, 1 https://github.com/tingofurro/keep_ it_simple 2 https://simple.wikipedia.org/ and propose Newsela3 as a replacement. Newsela is a project led by educators re-writing news articles targeting different school grade levels. We view Newsela as the gold-standard for our work, and use the public Newsela release of 1,911 groups of articles to design and evaluate our work. Using a coarse paragraph alignment algorithm, we extract 40,000 paired simple/complex paragraphs targeting a separation of 4 grade levels. We call this dataset the p"
2021.acl-long.498,Q16-1029,0,0.12737,"s of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6365–6378 August 1–6, 2021. ©2021 Association for Computational Linguistics formulate a reference-free reward for text simplification and directly optimize it, circumventing the need for aligned data. Our main contribution is the Keep it Simple (KiS) procedure, a novel unsupervised method for text simplification. Applied to the English news domain, KiS outperforms several supervised models on common simplification metrics such as SARI (Xu et al., 2016) and the Flesch-Kincaid Grade Level (Kincaid et al., 1975). A second contribution is a new algorithm for RLbased training of text generators, k-SCST, which is an extension of Self-Critical Sequence Training (Rennie et al., 2017). For each input, we generate k sampled outputs (vs. 2 in SCST), and use the mean population reward as a baseline. We show in Section 4 that in our domain, k-SCST outperforms models trained with SCST. A third contribution is a novel evaluation method for text simplification. Based on the assumption that simplified text should enable faster reading with better understand"
2021.acl-long.498,D19-1253,0,0.0227871,")/k, which we use as the baseline, instead of ˆ The loss L becomes: R. 8-SCST 6-SCST 4-SCST SCST L= (R¯S −RSj ) j=1 2 4 6 8 10 Hours of Training 12 Figure 5: Training KiS models comparing SCST with k-SCST. We try 4, 6 and 8 as values for k. Increasing k improves performance and stability. 4 k X KiS Training Rennie et al. (2017) introduced Self-Critical Sequence Training (SCST) as an effective algorithm for reward-based training of text generators, successfully applying it to image captioning. The efficacy of SCST was later confirmed on other text generation tasks such as question generation (Zhang and Bansal, 2019), and summarization (Celikyilmaz et al., 2018; Laban et al., 2020). In SCST, a probabilistic model is used to generate two distinct candidates: C S , a candidate constructed by samˆ by pling the word distribution at each step, and C, taking the argmax of the word distribution at each step. Each candidate is scored, obtaining rewards ˆ respectively, and the loss is: of RS and R, ˆ − RS ) L = (R N X S log p(wiS |w1S ...wi−1 , P ) (5) i=0 Sj log p(wiSj |w1Sj ...wi−1 ,P) i=0 (6) We use a GPT2-medium for the generator, initialized with the released pre-trained checkpoint. Experimental details such"
2021.acl-long.498,D17-1062,0,0.0826632,"ll sentences). Specifically, we train our models to simplify full paragraphs, and evaluate our models in a human evaluation on short documents (i.e., 3-4 paragraphs). Through rigorous empirical evaluation, we demonstrate the strong performance of our approach; automated results show that this unsupervised approach is able to outperform strong supervised models by 4 SARI points or more. We publicly released the code and model checkpoints1 . 2 Related Work Simplification Datasets. Early datasets were first based on Simple Wikipedia2 : WikiSmall (Zhu et al., 2010), later expanded into WikiLarge (Zhang and Lapata, 2017). Xu et al. (2015) show there are quality concerns with Simple Wikipedia datasets, 1 https://github.com/tingofurro/keep_ it_simple 2 https://simple.wikipedia.org/ and propose Newsela3 as a replacement. Newsela is a project led by educators re-writing news articles targeting different school grade levels. We view Newsela as the gold-standard for our work, and use the public Newsela release of 1,911 groups of articles to design and evaluate our work. Using a coarse paragraph alignment algorithm, we extract 40,000 paired simple/complex paragraphs targeting a separation of 4 grade levels. We call"
2021.acl-long.498,2020.acl-main.458,0,0.0775658,"Missing"
2021.acl-long.498,C10-1152,0,0.0520007,"e.g., the deletion, insertion or re-ordering of full sentences). Specifically, we train our models to simplify full paragraphs, and evaluate our models in a human evaluation on short documents (i.e., 3-4 paragraphs). Through rigorous empirical evaluation, we demonstrate the strong performance of our approach; automated results show that this unsupervised approach is able to outperform strong supervised models by 4 SARI points or more. We publicly released the code and model checkpoints1 . 2 Related Work Simplification Datasets. Early datasets were first based on Simple Wikipedia2 : WikiSmall (Zhu et al., 2010), later expanded into WikiLarge (Zhang and Lapata, 2017). Xu et al. (2015) show there are quality concerns with Simple Wikipedia datasets, 1 https://github.com/tingofurro/keep_ it_simple 2 https://simple.wikipedia.org/ and propose Newsela3 as a replacement. Newsela is a project led by educators re-writing news articles targeting different school grade levels. We view Newsela as the gold-standard for our work, and use the public Newsela release of 1,911 groups of articles to design and evaluate our work. Using a coarse paragraph alignment algorithm, we extract 40,000 paired simple/complex parag"
2021.acl-short.134,J08-1001,0,0.119669,"Missing"
2021.acl-short.134,N19-1423,0,0.0747657,"Missing"
2021.acl-short.134,P11-2022,0,0.0393028,"ntences. The task is often restricted to generative models, as it is prohibitively expensive to score all combinations to extract a best-scoring order (Logeswaran et al., 2018). 2.2 Models for the Shuffle Test Figure 2 is a timeline of models that have led to progress on the Shuffle Test since its introduction. The Entity Grid (model A in Fig. 2) was introduced by Barzilay and Lapata (2008). A text is transformed into an entity grid, a matrix (#sentences x #entities) indicating presence of an entity in a sentence. The entity grid is featurized and used to train a predictor on coherence tasks. Elsner and Charniak (2011) (model B) extended the entity grid by adding linguistic features such as named-entity type. Nguyen and Joty (2017) (model C) introduce the first neural approach, using a convolutional neural network (CNN) to operate over the entity grid, and Joty et al. (2018) (model D) added word embeddings to entity-grid features. Most recently, Moon et al. (2019) (model E) replaced traditional word vectors with ELMO (Peters et al., 2018) contextual word vectors. Crucially, all these models are directly trained on the Shuffle Test, and with each iteration of improvement, model capacity (i.e., the number of"
2021.acl-short.134,J97-1003,1,0.233217,"e in Long Text. We limited our analysis to texts with up to 512 words, a common constraint in pre-trained Transformers. Recent progress in model architectures open the possibility to process longer text, with models such as the Reformer (Kitaev et al., 2019), Longformer (Beltagy et al., 2020) and Big Bird (Zaheer et al., 2020) processing sequences of several thousand words. With longer sequences, one can further increase the block-size of the k-Blocked Shuffle Test (i.e., k=20) to gain understanding of model’s ability to discern global coherence (Van Dijk, 1985), or main topics and subtopics (Hearst, 1997). Specialized Coherence Models. In this work, we limit our analysis to popular models out-ofthe-box, establishing baseline performances for the Zero-Shot KBST. Future work should establish whether performance can be further improved, for instance using word-level likelihood signals and surprisal profiles. 6 Conclusion In this work, we discuss a potential limitation in the framing of the Shuffle Test, the most commonly used task to evaluate models for textual coherence. We show that a RoBERTa model can be finetuned to achieve near-perfect performance without necessarily measuring coherence, and"
2021.acl-short.134,P18-1031,0,0.0527083,"Missing"
2021.acl-short.134,P18-1052,0,0.0372848,"Missing"
2021.acl-short.134,D19-5406,0,0.060267,"Missing"
2021.acl-short.134,2021.ccl-1.108,0,0.0580495,"Missing"
2021.acl-short.134,D19-1231,0,0.0263873,"Missing"
2021.acl-short.134,P17-1121,0,0.0489281,"Missing"
2021.acl-short.134,N18-1202,0,0.122402,"Missing"
2021.acl-short.134,2020.acl-main.240,0,0.0716117,"Missing"
2021.bea-1.17,2020.coling-main.509,0,0.184267,"Missing"
2021.bea-1.17,2020.acl-main.69,0,0.0325175,"Missing"
2021.bea-1.17,D17-1090,0,0.0330094,"Missing"
2021.bea-1.17,Q18-1023,0,0.0310898,"Missing"
2021.bea-1.17,W19-4113,0,0.0252573,"causal extraction is sparse and thus most of the work is still tied to a focus on extracting relations based on specific linguistic features (Asghar, 2016). We utilize Cao et al. (2016), which is aimed at extracting causal relations from academic papers via a series of structured syntactic patterns tied to a linguistic typology (Altenberg, 1984). Causal relation extraction has been applied to inform question answering models (Girju, 2003; Breja and Jain, 2020). Some work has used neural networks to generate explanations from opendomain “why” questions without using external knowledge sources (Nie et al., 2019). • An evaluation framework, accompanied by preliminary experimental results showing that fine-tuning on a small, auxiliary dataset of cause-and-effect questions substantially improves both question generation and question answering models, 2.2 Neural question answering (QA) is a widelyexplored area with many large datasets of crowdworker-created questions. SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2017) each include over 100,000 crowdworkercreated questions from Wikipedia and CNN articles, respectively. NarrativeQA includes questions aimed at larger narrative events, which"
2021.bea-1.17,N19-4009,0,0.04438,"Missing"
2021.bea-1.17,P02-1040,0,0.108736,"Missing"
2021.bea-1.17,2020.findings-emnlp.217,0,0.0251631,"k has combined a syntactic question generator with backtranslation, to improve robustness and reduce grammatical errors of generated questions (Dhole and Manning, 2020). While Syn-QG is able to reliably generate types of causal questions using two specific patterns, the system is limited in diversity of question wording by syntactic rules. Question answering datasets have been used to train neural models to generate questions directly from input text. Question generation approaches include specialized attention mechanisms (Zhao et al., 2018) as well as generating via large transformer models (Qi et al., 2020; Chan and Fan, 2019). Jointly training QA and QG models have also been explored (Wang et al., 2017a). Past work has also trained neural QG systems on questions generated via a rule-based system (De Kuthy et al., 2020). Our pipeline can take in freeform text and automatically extract a cause and effect to generate questions. We feed passages from the SQuAD 2.0 development set (Rajpurkar et al., 2018) and the Textbook Question Answering (Kembhavi et al., 2017) datasets into our causal extraction pipeline for evaluation experiments. SQuAD 2.0 consists of over 100,000 questions from over 500 Wiki"
2021.bea-1.17,P18-2124,0,0.132167,"g causal extraction system (Cao et al., 2016), which uses a series of syntactic rules to extract causes and effects from unstructured text. We utilize the resulting cause and effect as intended answers for a neural question generation system. For each cause and effect, we generate one question, to test each direction of the causal relationship. Our work sets the stage for scalable generation of cause-and-effect questions because it automatically generates causal questions and answers from freeform text. In this paper, we evaluate our approach on two English datasets: SQuAD Wikipedia articles (Rajpurkar et al., 2018) and middle school science textbooks in the Textbook Question Answering (TQA) dataset (Kembhavi et al., 2017). Our research contributions include: • A novel cause-and-effect question generation pipeline, including an improved causal extraction system based on a linguistic typology (Cao et al., 2016; Altenberg, 1984), questions, with open source code to apply the pipeline to other text collections, allowing for future work to examine the educational impact of automatically-generated cause-and-effect questions.1 2 Related Work In this section, we discuss past work in causal extraction, question"
2021.bea-1.17,D16-1264,0,0.311525,"ctual questions, which involve recalling information, fall on the lowest level (Recall) (Beatty Jr, 1975; Anderson et al., 2001). By contrast, cause and effect questions are categorized at level 6 – Analysis – according to the original Bloom’s taxonomy (Beatty Jr, 1975) or level 2 – 158 Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 158–170 April 20, 2021 ©2021 Association for Computational Linguistics Understanding – in the commonly used revised model (Anderson et al., 2001). The rise of large question answering datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), and HotPotQA (Yang et al., 2018), have created the ability to train wellperforming neural QG systems. However, due to the nature of their training data, current question generation systems mainly generate factual questions characteristic of Bloom’s level 1. Questions which test knowledge of a causal relation can assess a deeper level of mastery beyond surface-level factual questions. Altenberg (1984) states that “[a] causal relation can be said to exist between two events or states of affairs if one is understood as the cause of or reason for the other.” We a"
2021.bea-1.17,W17-5034,1,0.920132,"ataset because it contains educational textbook text, which often express causal relationships; however, we do not use the TQA questions since many are tied to an entire lesson in the textbook and include visual diagrams. 2.4 3.2 2.3 Question Generation Question Generation Applications Past work has explored educational applications of question generation (Kurdi et al., 2020). QGNet utilizes a pointer-generator model trained on SQuAD to automatically generate questions from textbooks (Wang et al., 2018). Additional work has aimed at generating educational questions from a structured ontology (Stasaski and Hearst, 2017). QuizBot exposes students to questions via a dialogue interface, where pre-set questions are chosen in an intelligent order (Ruan et al., 2019). Causal Extraction We use and improve a pre-existing causal extractor to identify cause and effect pairs in unstructured input text, see Causal Extractor in Figure 1 (Cao et al., 2016).2 The system achieves approximately 0.85 recall of hand-labeled cause-and-effect relationships over 3 academic articles, as reported in Cao et al. (2016). The extractor relies on a series of hand-crafted patterns based on syntax cues and 160 2 https://github.com/Angela7"
2021.bea-1.17,W17-2623,0,0.0226064,"calling information, fall on the lowest level (Recall) (Beatty Jr, 1975; Anderson et al., 2001). By contrast, cause and effect questions are categorized at level 6 – Analysis – according to the original Bloom’s taxonomy (Beatty Jr, 1975) or level 2 – 158 Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 158–170 April 20, 2021 ©2021 Association for Computational Linguistics Understanding – in the commonly used revised model (Anderson et al., 2001). The rise of large question answering datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), and HotPotQA (Yang et al., 2018), have created the ability to train wellperforming neural QG systems. However, due to the nature of their training data, current question generation systems mainly generate factual questions characteristic of Bloom’s level 1. Questions which test knowledge of a causal relation can assess a deeper level of mastery beyond surface-level factual questions. Altenberg (1984) states that “[a] causal relation can be said to exist between two events or states of affairs if one is understood as the cause of or reason for the other.” We aim to generate cause-and-effect q"
2021.bea-1.17,P17-1018,0,0.0342548,"Missing"
2021.bea-1.17,D18-1259,0,0.0208145,"level (Recall) (Beatty Jr, 1975; Anderson et al., 2001). By contrast, cause and effect questions are categorized at level 6 – Analysis – according to the original Bloom’s taxonomy (Beatty Jr, 1975) or level 2 – 158 Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 158–170 April 20, 2021 ©2021 Association for Computational Linguistics Understanding – in the commonly used revised model (Anderson et al., 2001). The rise of large question answering datasets, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), and HotPotQA (Yang et al., 2018), have created the ability to train wellperforming neural QG systems. However, due to the nature of their training data, current question generation systems mainly generate factual questions characteristic of Bloom’s level 1. Questions which test knowledge of a causal relation can assess a deeper level of mastery beyond surface-level factual questions. Altenberg (1984) states that “[a] causal relation can be said to exist between two events or states of affairs if one is understood as the cause of or reason for the other.” We aim to generate cause-and-effect questions, which test knowledge of"
2021.bea-1.17,W17-2603,0,0.0179958,"SQuAD. This is somewhat expected, as Syn-QG’s syntax rules result in questions that are similarly-structured and include the cause or effect. We also note slightly higher scores when an effect is the intended answer. Future work should isolate whether this is due to the model’s ability to generate better questions from effects or whether the extractor is better at extracting effects than causes from passages. In Section 7, we explore this further by having humans label the quality of the QG model’s output. 6.2 Question Answering System Performance Following past work on evaluating QG models (Yuan et al., 2017), we measure whether a QA system is able to accurately answer the generated cause-and-effect questions from the passage. If the QG model produces an ill-formed or incorrect ques164 Model Base Syn-QG C 0.52 0.71 TQA E 0.57 0.72 T 0.55 0.72 C 0.32 0.54 SQuAD E T 0.42 0.37 0.57 0.56 Type Adv. Prep. Sub. C-I Table 5: Average cause and effect recall in questions generated by ProphetNet Base and fine-tuned on SynQG (out of 1.0, higher is better). C indicates questions where a cause is the intended answer, E indicates questions where the effect is the intended answer, and T indicates the total for bo"
2021.bea-1.17,D18-1424,0,0.156109,"Missing"
2021.findings-emnlp.266,2020.lrec-1.269,0,0.0967462,"Missing"
2021.findings-emnlp.266,D19-1609,0,0.0632955,"Missing"
2021.findings-emnlp.266,W16-1508,0,0.0610303,"Missing"
2021.naacl-main.255,2020.acl-demos.40,0,0.0122365,". Pronoza et al. (2015) build a corpus of Russian headlines pairs, but limit pairs in the dataset by filtering out headlines that are distant syntactically. We find that headline groups often contain syntactically distant headlines (see Figure 3). Bouamor et al. (2012) and Shinyama et al. (2002) present a simple strategy, relying on the assumption that all articles on a topic published on the same day form a group. As will be shown below, this assumption is not always correct (see Figure 2). Several of the most-used news aggregators, such as Yahoo News 2 , Google News3 , and Bloomberg’s NSTM (Bambrick et al., 2020) present headlines in groups. As these systems do not have published algorithms, we cannot comment on their methods; nonetheless we hope that the release of the HLG dataset offers a common evaluation test-bed to benchmark systems. cles originating from 34 international news sources. The timelines range in size from 80 to 274 news articles, and span 18 days to 10 years. We choose to use timelines as the source for the dataset for two reasons. First, news timelines center around a theme, and as successive events occur, many pairs of headlines will be semantically close, yielding challenging samp"
2021.naacl-main.255,N18-1101,0,0.228658,"hmarks such as the General Language Understanding Evaluation collection (GLUE), paired with fast-paced progress in Transformer-based architectures has led to models outperforming human baseline performance on many tasks, such as paraphrase identification (Dolan et al., 2004), semantic similarity (Cer et al., 2017), and extractive question-answering (QA) (Rajpurkar et al., 2018). This success has led to the questioning of the composition of benchmarks, and the subsequent creation of ever-more challenging datasets, for example by increasing the diversity of texts in textual entailment datasets (Williams et al., 2018), or introducing unanswerable questions in QA datasets (Rajpurkar et al., 2018). 1.1 HeadLine Grouping Definition In this paper, we propose the novel task of HeadLine Grouping. Although news articles may discuss several topics, because of length constraints, lucasbanheadlines predominantly describe a single event. 3186 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3186–3198 June 6–11, 2021. ©2021 Association for Computational Linguistics Therefore, for the task of headline grouping, we defin"
A94-1013,J93-1004,0,0.0405588,"Missing"
A94-1013,H89-2048,0,0.0518108,"e costs negligible for a system that must in any case record these probabilities for use in its part-of-speech tagger. This approach appears to incur a cycle: because most part-of-speech taggers require pre-determined sentence boundaries, sentence labeling must be done before tagging. But if sentence labeling is done before tagging, no part-of-speech assignments are available for the boundary-determination algorithm. Instead of assigning a single part-of-speech to each word, our algorithm uses ~he prior probabilities of all parts-of-speech for that word. This is in contrast to Riley&apos;s method (Riley, 1989) which requires probabilities to be found for every lexical item (since it records the number of times every token has been seen before and after a period). Instead, we suggest making use of the unchanging prior probabilities for each word already stored in the system&apos;s lexicon. The rest of this section describes the algorithm in more detail. 2.1 input layer is fully connected to a hidden layer consisting of j hidden units with a sigmoidal squashing activation function. The hidden units in turn feed into one output unit which indicates the results of the function. 4 The output of the network,"
A94-1013,A88-1019,0,0.105909,"ch the middle token appears. If the middle token is a potential end-of-sentence punctuation mark, the descriptor arrays for the context tokens are input to the network and the output result indicates the appropriate label, subject to the thresholds to and t 1. Section 3 describes experiments which vary the size of k and the number of hidden units. Assignment of Descriptors The first stage of the process is lexical analysis, which breaks the input text (a stream of characters) into tokens. Our implementation uses a slightlymodified version of the tokenizer from the PARTS part-of-speech tagger (Church, 1988) for this task. A token can be a sequence of alphabetic characters, a sequence of digits (numbers containing periods acting as decimal points are considered a single token), or a single non-alphanumeric character. A lookup module then uses a lexicon with part-of-speech tags for each token. This lexicon includes information about the frequency with which each word occurs as each possible part-of-speech. The lexicon and the frequency counts were also taken from the PARTS tagger, which derived the counts from the Brown corpus (Francis and Kucera, 1982). For the word adult, for example, the lookup"
A94-1013,A92-1018,0,\N,Missing
A94-1013,J93-1006,0,\N,Missing
C92-2082,P86-1018,0,0.140654,"Missing"
C92-2082,P88-1027,0,0.146211,"Missing"
C92-2082,J87-3001,0,0.122769,"from the frame An Lo is a (kind of) L1. Here Lt is the hypernym of Lo and the relationship is reflexive and transitive, but not symmetric. This example shows a way to discover a hyponymic lexical relationship between two or more noun phrases in a naturally-occurring text. This approach is simllar in spirit to the pattern-based interpretation techniques being used in MRD processing. For example, t All examples in this paper are real text, taken from Grolter's Amerwan Acaderntc Encyclopedia(Groher tg00) PROC. OVCOLING-92, NhNTIIS,AUG.23-28, 1992 phrases. For example, discovering the predicate (Alshawi 1987), in interpreting LDOCE definitions, uses a hierarchy of patterns which consist mainly of part-of-speech indicators and wildcard characters. (Markowitz e~ al. 1986), (Jensen & Binot 1987), and (Nakamura & Nagao 1988) also use pattern recognition to extract semantic relations such as taxonomy from various dictionaries. (Ahlswede & Evens I988) compares an approach based on parsing Webster's 7th definitions with one based on pattern recognition, and finds that for finding simple semantic relations, pattern recognition [s far more accurate and efficient than parsing. The general feeling is that th"
C92-2082,J91-1002,0,0.207077,"Missing"
C92-2082,C88-2098,0,0.117407,"ip between two or more noun phrases in a naturally-occurring text. This approach is simllar in spirit to the pattern-based interpretation techniques being used in MRD processing. For example, t All examples in this paper are real text, taken from Grolter's Amerwan Acaderntc Encyclopedia(Groher tg00) PROC. OVCOLING-92, NhNTIIS,AUG.23-28, 1992 phrases. For example, discovering the predicate (Alshawi 1987), in interpreting LDOCE definitions, uses a hierarchy of patterns which consist mainly of part-of-speech indicators and wildcard characters. (Markowitz e~ al. 1986), (Jensen & Binot 1987), and (Nakamura & Nagao 1988) also use pattern recognition to extract semantic relations such as taxonomy from various dictionaries. (Ahlswede & Evens I988) compares an approach based on parsing Webster's 7th definitions with one based on pattern recognition, and finds that for finding simple semantic relations, pattern recognition [s far more accurate and efficient than parsing. The general feeling is that the structure and function of MRDs makes their interpretation amenable to pattern-recognition techniques. hyponym( ""broken bone"", ""injury"") indicates that tbe term ""broken bone"" can be understood at some level as an ""i"
C92-2082,P90-1032,0,0.14257,"sign case roles to lexical items, and (Jacobs & Zernik 1988) uses extensive domain knowledge to fill in missing category information for unknown words. When comparing a result hyponym(No,Nt) to the contents of WordNet's noun hierarchy, three kinds of outcomes are possible: Verify. If both No and Nt are in WordNet, and if the relation byponym(No,N1) is in the hierarchy (possibly througi~ transitive closure) then the thesaurus is verified. Work on acquisition of syntactic information from text corpora includes Brent's (Brent 1991) verb subcategorization frame recognition technique and Smadja's (Smadja & McKeown 1990) collocation acquisition algorithm. (Calzolari & Bindi 1990) use corpus-based statistical association ratios to determine lexical information such as prepositional complementation relations, modification relations, and significant compounds. C r i t i q u e . If both No and N1 are in WordNet, and if the relation hyponym(No, N1) is not in the hierarchy (even through transitive closure) then the thesaurus is critiqued, i.e., a new set of hyponym connections is suggested. Our methodology is similar to Brent's in its effort to distinguish clear pieces of evidence from ambiguous ones. The assumptio"
C92-2082,C90-3010,0,0.12426,") uses extensive domain knowledge to fill in missing category information for unknown words. When comparing a result hyponym(No,Nt) to the contents of WordNet's noun hierarchy, three kinds of outcomes are possible: Verify. If both No and Nt are in WordNet, and if the relation byponym(No,N1) is in the hierarchy (possibly througi~ transitive closure) then the thesaurus is verified. Work on acquisition of syntactic information from text corpora includes Brent's (Brent 1991) verb subcategorization frame recognition technique and Smadja's (Smadja & McKeown 1990) collocation acquisition algorithm. (Calzolari & Bindi 1990) use corpus-based statistical association ratios to determine lexical information such as prepositional complementation relations, modification relations, and significant compounds. C r i t i q u e . If both No and N1 are in WordNet, and if the relation hyponym(No, N1) is not in the hierarchy (even through transitive closure) then the thesaurus is critiqued, i.e., a new set of hyponym connections is suggested. Our methodology is similar to Brent's in its effort to distinguish clear pieces of evidence from ambiguous ones. The assumption is that that given a large enough corpus, the algorithm ca"
C92-2082,P89-1023,0,0.136296,"text in which the proper noun occurs in order to identify some relevant semantic attributes. For instance. Coates-Stephens mentions that ""known as"" can explicitly introduce meanings for terms, as can appositives. We also have considered these markers, hut the tbrmer often does not cleanly indicate ""another name for"" and the latter is difficult to recognize accurately. FUNES differs quite strongly from our approach in that, because it is able to fill in many kinds of frame roles, it requires a parser that produces a detailed structure, and it requires a domain-dependent knowlege base/lexicon. (Velardi & Pazienza 1989) makes use of hand-coded selection restriction and conceptual relation rules in order to assign case roles to lexical items, and (Jacobs & Zernik 1988) uses extensive domain knowledge to fill in missing category information for unknown words. When comparing a result hyponym(No,Nt) to the contents of WordNet's noun hierarchy, three kinds of outcomes are possible: Verify. If both No and Nt are in WordNet, and if the relation byponym(No,N1) is in the hierarchy (possibly througi~ transitive closure) then the thesaurus is verified. Work on acquisition of syntactic information from text corpora incl"
C92-2082,P90-1034,0,0.150065,"f MRDs makes their interpretation amenable to pattern-recognition techniques. hyponym( ""broken bone"", ""injury"") indicates that tbe term ""broken bone"" can be understood at some level as an ""injury"" without having to determine the correct senses of the component words and how they combine. Note also that a term like ""broken bone"" is not likely to appear in a dictionary or lexicon, although it is a common locution. S e m a n t i c R e l a t e d n e s s I n f o r m a t i o n . There bas recently been work in the detection of semantically related nouns via, for example, shared argument structures (Hindle 1990), and shared dictionary definition context (Wilks e¢ al. 1990). These approaches attempt to infer relationships among [exical terms by looking at very large text samples and determining which ones are related in a statistically significant way. The technique introduced in this paper can be seen as having a similar goal but an entirely different approach, since only one sample need be found in order to determine a salient relationship (and that sample may be infrequently occurring or nonexistent). Thus one could say by interpreting sentence (S1) according to (In-b) we are applying pattern-based"
C92-2082,J87-3005,0,\N,Missing
C92-2082,H91-1067,0,\N,Missing
C92-2082,A92-1018,0,\N,Missing
C92-2082,P91-1027,0,\N,Missing
D07-1089,P06-1009,0,0.140908,"alignment, and even more so in the case of MCA. Second, generative models explicitly model the inter-dependence of different features, which reduces the ability to incorporate multiple arbitrary features into the model. Since orthographic similarity is not a strong enough indication for semantic homology in MCA, we would like to be able to incorporate multiple inter-dependent features into a single model, including orthographic, contextual, ontological, and lexical features. Recently, several authors have described discriminative SMT alignment models (Moore, 2005; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006). However, to the best of our knowledge only the model of Blunsom & Cohn (2006), which is based on a Conditional Random Field (CRF) (Lafferty et al., 2001), can compute word indices pairs’ directional posterior probabilities, like those computed by the HMM models. Therefore, we decided to adopt the CRF-based model to the MCA problem. 3.1 Conditional random fields for word alignment The model of Blunsom & Cohn (2006) is based on a linear chain CRF, which can be viewed as the undirected version of an HMM. The CRF models a many-to-one pairwise alignment, in which every source word can get aligned"
D07-1089,N06-1014,0,0.014579,"rget sentence given a source sentence. In other words they only model many-to-one alignments, recovering the many-tomany alignments in a preprocessing step. Therefore, SMT HMMs can only compute the posterior probabilities P (cik ; cjl |C i , C j ) and P (cjl ; cik |C i , C j ), where the relation ; represents the (directional) event that a source word is translated into a target word. Nevertheless, recently such posterior probabilities have been used in SMT word alignment system as an alternative to Viterbi decoding, and helped to improve the performance of such systems (Matusov et al., 2004; Liang et al., 2006). Generative models like HMMs have several limitations. First, they require relatively large train849 ing data, which is difficult to attain in case of SMT word alignment, and even more so in the case of MCA. Second, generative models explicitly model the inter-dependence of different features, which reduces the ability to incorporate multiple arbitrary features into the model. Since orthographic similarity is not a strong enough indication for semantic homology in MCA, we would like to be able to incorporate multiple inter-dependent features into a single model, including orthographic, contex"
D07-1089,C04-1032,0,0.0166933,"ity of generating a target sentence given a source sentence. In other words they only model many-to-one alignments, recovering the many-tomany alignments in a preprocessing step. Therefore, SMT HMMs can only compute the posterior probabilities P (cik ; cjl |C i , C j ) and P (cjl ; cik |C i , C j ), where the relation ; represents the (directional) event that a source word is translated into a target word. Nevertheless, recently such posterior probabilities have been used in SMT word alignment system as an alternative to Viterbi decoding, and helped to improve the performance of such systems (Matusov et al., 2004; Liang et al., 2006). Generative models like HMMs have several limitations. First, they require relatively large train849 ing data, which is difficult to attain in case of SMT word alignment, and even more so in the case of MCA. Second, generative models explicitly model the inter-dependence of different features, which reduces the ability to incorporate multiple arbitrary features into the model. Since orthographic similarity is not a strong enough indication for semantic homology in MCA, we would like to be able to incorporate multiple inter-dependent features into a single model, including"
D07-1089,W03-0301,0,0.031109,"1 0.9 0.8 Precision 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.3 0.4 0.5 0.6 0.7 0.8 Recall Viterbi_Intersection Viterbi_Union Posterior decoding Figure 3: Recall/Precision curve of pairwise citance alignments comparing Viterbi to posterior decoding. of the system in general. Therefore, we concentrate our discussion on general trends, and do not claim that the specific performance numbers we report here are statistically significant. As a point of comparison, the SMT community has been evaluating performance of word-alignment systems on an even smaller dataset of 447 pairs of non-overlapping sentences (Mihalcea and Pedersen, 2003). We first analyze the performance of the system on pairwise citance alignments. Instead of taking the equivalence closure of ;p we take only the symmetric closure. The result is 990 manyto-many pairwise alignments. In order to evaluate the effectiveness of the posterior-decoding algorithm, we generate the Viterbi alignments using the same CRF model. The Viterbi many-to-many pairwise alignments are then generated by combining equivalent pairs of many-to-one alignments using three different standard symmetrization methods for word-alignment—union, intersection, and the refined method of Och & N"
D07-1089,H05-1011,0,0.0126608,"s difficult to attain in case of SMT word alignment, and even more so in the case of MCA. Second, generative models explicitly model the inter-dependence of different features, which reduces the ability to incorporate multiple arbitrary features into the model. Since orthographic similarity is not a strong enough indication for semantic homology in MCA, we would like to be able to incorporate multiple inter-dependent features into a single model, including orthographic, contextual, ontological, and lexical features. Recently, several authors have described discriminative SMT alignment models (Moore, 2005; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006). However, to the best of our knowledge only the model of Blunsom & Cohn (2006), which is based on a Conditional Random Field (CRF) (Lafferty et al., 2001), can compute word indices pairs’ directional posterior probabilities, like those computed by the HMM models. Therefore, we decided to adopt the CRF-based model to the MCA problem. 3.1 Conditional random fields for word alignment The model of Blunsom & Cohn (2006) is based on a linear chain CRF, which can be viewed as the undirected version of an HMM. The CRF models a many-to-one pairwise"
D07-1089,J03-1002,0,0.0479101,"es (e.g., genotoxic stress and DNA damage). Unlike global multiple sequence alignment (MSA) in genomics, where each character can be aligned to at most one character in every other sequence, in multiple citance alignment, each word can be aligned to any number of words in other sentences. Another major difference between the two problems is the fact that while the sequential ordering of characters must be maintained in multiple sequence alignment, this is not the case for multiple citance alignment. MCA is also related to the problem of word alignment in statistical machine translation (SMT) (Och and Ney, 2003). However, unlike SMT alignment, MCA aligns multiple citances in the same language rather than a pair of sentences in different languages. In this paper we present an MCA algorithm that is based on an extension to the posterior decoding algorithm for MSA called AMAP (Schwartz et al., 2006; Schwartz and Pachter, 2007), with an underlying pairwise alignment model based on the CRF SMT alignment of Blunsom & Cohn (2006). 2 Multiple citance alignments Let G , {C 1 , C 2 , . . . , C K } be a group of K citances that cite the same target paper, where the ith citance is a sequence of words C i , C1i C"
D07-1089,N06-1015,0,0.0223238,"o attain in case of SMT word alignment, and even more so in the case of MCA. Second, generative models explicitly model the inter-dependence of different features, which reduces the ability to incorporate multiple arbitrary features into the model. Since orthographic similarity is not a strong enough indication for semantic homology in MCA, we would like to be able to incorporate multiple inter-dependent features into a single model, including orthographic, contextual, ontological, and lexical features. Recently, several authors have described discriminative SMT alignment models (Moore, 2005; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006). However, to the best of our knowledge only the model of Blunsom & Cohn (2006), which is based on a Conditional Random Field (CRF) (Lafferty et al., 2001), can compute word indices pairs’ directional posterior probabilities, like those computed by the HMM models. Therefore, we decided to adopt the CRF-based model to the MCA problem. 3.1 Conditional random fields for word alignment The model of Blunsom & Cohn (2006) is based on a linear chain CRF, which can be viewed as the undirected version of an HMM. The CRF models a many-to-one pairwise alignment, in which every so"
H05-1092,W05-1308,0,0.0224025,"Missing"
H05-1092,W05-1307,0,0.052875,"Missing"
H05-1092,P04-1055,1,0.820343,"interaction  get proteins (see Section 5.4). Thus the evaluation on the test set is done at the document level (to determine if the algorithm can predict the interaction that a curator would assign to a document as a whole given the protein pair). Note that we assume here that the papers that provide the evidence for the interactions are given – an assumption not usually true in practice. 4 Models For assigning interactions, we used two generative graphical models and a discriminative model. Figure 1 shows the generative dynamic model, based on previous work on role and relation extraction (Rosario and Hearst, 2004) where the task was to extract the entities TREATMENT and DISEASE and the relationships between them. The nodes labeled “Role” represent the entities (in this case the choices are PROTEIN and NULL); the children of the role nodes are the words (which act as features), thus there are as many role states as there are words in the sentence; this model consists of a Markov sequence of states where each state generates one or multiple 735 observations. This model makes the additional assumption that there is an interaction present in the sentence (represented by the node “Inter.”) that generates th"
H05-1092,W02-1010,0,0.010165,"ortant task, the extraction of the interactions between proteins from free text. We use graphical models and a neural net that were found to achieve high accuracy in the related task of extracting the reRelated work There has been little work in general NLP on trying to identify different relations between entities. Many papers that claim to be doing relationship recognition in actuality address the task of role extraction: (usually two) entities are identified and the relationship is implied by the co-occurrence of these entities or by some linguistic expression (Agichtein and Gravano, 2000; Zelenko et al., 2002). The ACE competition2 has a relation recognition subtask, but assumes a particular type of relation holds between particular entity types (e.g., if the two entities in question are an EMP and an ORG, then an employment relation holds between them; which type of employment relation depends on the type of entity, e.g., staff person vs partner). 1 2 www.ncbi.nlm.nih.gov/RefSeq/HIVInteractions/index.html http://www.itl.nist.gov/iaui/894.01/tests/ace/ 732 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 732–"
H05-1105,P92-1003,0,0.540421,"ctions can link two words, two constituents (e.g., NPs), two clauses or even two sentences. Thus, the first challenge is to identify the boundaries of the conjuncts of each coordination. The next problem comes from the interaction of the coordinations with other constituents that attach to its conjuncts (most often prepositional phrases). In the example above we need to decide between [health and [quality of life]] and [[health and qual839 ity] of life]. From a semantic point of view, we need to determine whether the or in chronic diseases or disabilities really means or or is used as an and (Agarwal and Boggess, 1992). Finally, we need to choose between a non-elided and an elided reading: [[chronic diseases] or disabilities] vs. [chronic [diseases or disabilities]]. Below we focus on a special case of the latter problem: noun compound (NC) coordination. Consider the NC car and truck production. Its real meaning is car production and truck production. However, due to the principle of economy of expression, the first instance of production has been compressed out by means of ellipsis. By contrast, in president and chief executive, president is simply linked to chief executive. There is also an all-way coordi"
H05-1105,P01-1005,0,0.087387,"Introduction Resolution of structural ambiguity problems such as noun compound bracketing, prepositional phrase (PP) attachment, and noun phrase coordination requires using information about lexical items and their cooccurrences. This in turn leads to the data sparseness problem, since algorithms that rely on making decisions based on individual lexical items must have statistics about every word that may be encountered. Past approaches have dealt with the data sparseness problem by attempting to generalize from semantic classes, either manually built or automatically derived. More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. They demonstrate the idea on a lexical disambiguation problem for which labeled examples are available “for free”. The problem is to choose which of 2-3 commonly confused In a related strand of work, Lapata and Keller (2004) show that computing n-gram statistics over very large corpora yields results that are competitive with if not better than the best supervised and knowledge-based approaches on a wide range of NLP tasks. For example, they show that for"
H05-1105,C94-2195,0,0.0230055,"Missing"
H05-1105,J82-3004,0,0.168565,"Missing"
H05-1105,W95-0103,0,0.077544,"Missing"
H05-1105,P97-1003,0,0.016244,"ans of ellipsis. By contrast, in president and chief executive, president is simply linked to chief executive. There is also an all-way coordination, where the conjunct is part of the whole, as in Securities and Exchange Commission. More formally, we consider configurations of the kind n1 c n2 h, where n1 and n2 are nouns, c is a coordination (and or or) and h is the head noun4 . The task is to decide whether there is an ellipsis or not, independently of the local context. Syntactically, this can be expressed by the following bracketings: [[n1 c n2 ] h] versus [n1 c [n2 h]]. (Collins’ parser (Collins, 1997) always predicts a flat NP for such configurations.) In order to make the task more 4 The configurations of the kind n h1 c h2 (e.g., company/n cars/h1 and/c trucks/h2 ) can be handled in a similar way. realistic (from a parser’s perspective), we ignore the option of all-way coordination and try to predict the bracketing in Penn Treebank (Marcus et al., 1994) for configurations of this kind. The Penn Treebank brackets NCs with ellipsis as, e.g., (NP car/NN and/CC truck/NN production/NN). and without ellipsis as (NP (NP president/NN) and/CC (NP chief/NN executive/NN)) The NPs with ellipsis are"
H05-1105,P99-1081,0,0.813486,"Missing"
H05-1105,J93-1005,0,0.56583,"els; (i) Pr(p|n1 ) vs. Pr(p|v) (ii) Pr(p, n2 |n1 ) vs. Pr(p, n2 |v). Each of these was computed two different ways: using Pr (probabilities) and # (frequencies). We estimate the n-gram counts using exact phrase queries (with inflections, derived from WordNet 2.0) using the MSN Search Engine. We also allow for determiners, where appropriate, e.g., between the preposition and the noun when querying for #(p, n2 ). We add up the frequencies for all possible variations. Web frequencies were reliable enough and did not need smoothing for (i), but for (ii), smoothing using the technique described in Hindle and Rooth (1993) led to better recall. We also tried back-off from (ii) to (i), as well as back-off plus smoothing, but did not find improvements over smoothing alone. We found n-gram counts to be unreliable when pronouns appear in the test set rather than nouns, and disabled them in these cases. Such examples can still be handled by paraphrases or surface features (see below). 837 2.2.2 Web-Derived Surface Features Authors sometimes (consciously or not) disambiguate the words they write by using surface-level markers to suggest the correct meaning. We have found that exploiting these markers, when they occur"
H05-1105,C92-1029,0,0.051725,"Missing"
H05-1105,N04-1016,0,0.22874,"must have statistics about every word that may be encountered. Past approaches have dealt with the data sparseness problem by attempting to generalize from semantic classes, either manually built or automatically derived. More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. They demonstrate the idea on a lexical disambiguation problem for which labeled examples are available “for free”. The problem is to choose which of 2-3 commonly confused In a related strand of work, Lapata and Keller (2004) show that computing n-gram statistics over very large corpora yields results that are competitive with if not better than the best supervised and knowledge-based approaches on a wide range of NLP tasks. For example, they show that for the problem of noun compound bracketing, the performance of an n-gram based model computed using search engine statistics was not significantly different from the best supervised algorithm whose parameters were tuned and which used a taxonomy. They find however that these approaches generally fail to outperform supervised state-of-the-art models that are trained"
H05-1105,W05-0603,1,0.331429,"on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 835–842, Vancouver, October 2005. 2005 Association for Computational Linguistics of individual lexical items. The trick is to figure out how to use information that is latent in the web as a corpus, and web search engines as query interfaces to that corpus. In this paper we describe two techniques – surface features and paraphrases – that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. In recent work (Nakov and Hearst, 2005) we showed that a variation of the techniques, when applied to the problem of noun compound bracketing, produces higher accuracy than Lapata and Keller (2004) and the best supervised results. In this paper we adapt the techniques to the structural disambiguation problems of prepositional phrase attachment and noun compound coordination. 2 Prepositional Phrase Attachment A long-standing challenge for syntactic parsers is the attachment decision for prepositional phrases. In a configuration where a verb takes a noun complement that is followed by a PP, the problem arises of whether the PP attach"
H05-1105,P00-1014,0,0.0612618,"Missing"
H05-1105,H94-1048,0,0.0846923,"Missing"
H05-1105,P98-2177,0,0.035961,"Missing"
H05-1105,W97-0109,0,0.0763764,"Missing"
H05-1105,J93-2004,0,\N,Missing
H05-1105,C98-2172,0,\N,Missing
J02-1002,W97-0703,0,0.0306315,"for subtopic segmentation (Nomoto and Nitta 1994; Hasnah 1996; Richmond, Smith, and Amitay 1997; Heinonen 1998; Boguraev and Neff 2000). Other techniques use clustering and/or similarity matrices based on word co-occurrences (Reynar 1994; Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cue words, or hand-selected cue words to detect segment boundaries (Passonneau and Litman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998). Researchers have explored the use of this kind of document segmentation to improve automated summarization (Salton et al. 1994; Barzilay and Elhadad 1997; Kan, Klavans, and McKeown 1998; Mittal et al. 1999; Boguraev and Neff 2000) and automated genre detection (Karlgren 1996). Text segmentation issues are also important for passage retrieval, a subproblem of information retrieval (Hearst and Plaunt 1993; Salton, Allan, and Buckley 1993; Callan 1994; Kaszkiel and Zobel 1997). More recently, a great deal of interest has arisen in using automatic segmentation for the detection of topic and story boundaries in news feeds (Mani et al. 1997; Merlino, Morey, and Maybury 1997; Ponte and Croft 1997; Hauptmann and Witbrock 1998; Allan et al. 1998; Beefe"
J02-1002,W97-0304,0,0.689628,"Missing"
J02-1002,A00-2004,0,0.642017,"technique, or slight variations ∗ Harvard University, 380 Leverett Mail Center, Cambridge, MA 02138. E-mail: pevzner@post.harvard.edu † University of California, Berkeley, 102 South Hall #4600, Berkeley, CA 94720. E-mail: hearst@sims.berkeley.edu c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 1 of it, for subtopic segmentation (Nomoto and Nitta 1994; Hasnah 1996; Richmond, Smith, and Amitay 1997; Heinonen 1998; Boguraev and Neff 2000). Other techniques use clustering and/or similarity matrices based on word co-occurrences (Reynar 1994; Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cue words, or hand-selected cue words to detect segment boundaries (Passonneau and Litman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998). Researchers have explored the use of this kind of document segmentation to improve automated summarization (Salton et al. 1994; Barzilay and Elhadad 1997; Kan, Klavans, and McKeown 1998; Mittal et al. 1999; Boguraev and Neff 2000) and automated genre detection (Karlgren 1996). Text segmentation issues are also important for passage retrieval, a subproblem of information retrieval (Hearst"
J02-1002,P94-1002,1,0.959896,"Missing"
J02-1002,J97-1003,1,0.87084,"umber of boundaries within the window does not match the true number of boundaries for that window of text. 1. Introduction Text segmentation is the task of determining the positions at which topics change in a stream of text. Interest in automatic text segmentation has blossomed over the last few years, with applications ranging from information retrieval to text summarization to story segmentation of video feeds. Early work in multiparagraph discourse segmentation examined the problem of subdividing texts into multiparagraph units that represent passages or subtopics. An example, drawn from Hearst (1997), is a 21paragraph science news article, called “Stargazers,” whose main topic is the existence of life on earth and other planets. Its contents can be described as consisting of the following subtopic discussions (numbers indicate paragraphs): 1–3 4–5 6–8 9–12 13 14–16 17–18 19–20 21 Introduction: The search for life in space The moon’s chemical composition How early earth-moon proximity shaped the moon How the moon helped life evolve on earth Improbability of the earth-moon system Binary/trinary star systems make life unlikely The low probability of nonbinary/trinary systems Properties of ea"
J02-1002,P98-2244,0,0.110661,"istribution; subtopic boundaries are assumed to occur at the point in the documents at which large shifts in vocabulary occur. Many others have used this technique, or slight variations ∗ Harvard University, 380 Leverett Mail Center, Cambridge, MA 02138. E-mail: pevzner@post.harvard.edu † University of California, Berkeley, 102 South Hall #4600, Berkeley, CA 94720. E-mail: hearst@sims.berkeley.edu c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 1 of it, for subtopic segmentation (Nomoto and Nitta 1994; Hasnah 1996; Richmond, Smith, and Amitay 1997; Heinonen 1998; Boguraev and Neff 2000). Other techniques use clustering and/or similarity matrices based on word co-occurrences (Reynar 1994; Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cue words, or hand-selected cue words to detect segment boundaries (Passonneau and Litman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998). Researchers have explored the use of this kind of document segmentation to improve automated summarization (Salton et al. 1994; Barzilay and Elhadad 1997; Kan, Klavans, and McKeown 1998; Mittal et al. 1999; Boguraev and Neff 2000) and auto"
J02-1002,P96-1038,0,0.0507844,"Callan 1994; Kaszkiel and Zobel 1997). More recently, a great deal of interest has arisen in using automatic segmentation for the detection of topic and story boundaries in news feeds (Mani et al. 1997; Merlino, Morey, and Maybury 1997; Ponte and Croft 1997; Hauptmann and Witbrock 1998; Allan et al. 1998; Beeferman, Berger, and Lafferty 1997, 1999). Sometimes segmentation is done at the clause level, for the purposes of detecting nuances of dialogue structure or for more sophisticated discourse-processing purposes (Morris and Hirst 1991; Passonneau and Litman 1993; Litman and Passonneau 1995; Hirschberg and Nakatani 1996; Marcu 2000). Some of these algorithms produce hierarchical dialogue segmentations whose evaluation is outside the scope of this discussion. 1.1 Evaluating Segmentation Algorithms There are two major difficulties associated with evaluating algorithms for text segmentation. The first is that since human judges do not always agree where boundaries should be placed and how fine grained an analysis should be, it is difficult to choose a reference segmentation for comparison. Some evaluations circumvent this difficulty by detecting boundaries in sets of concatenated documents, where there can be n"
J02-1002,P95-1015,0,0.154781,"n, Allan, and Buckley 1993; Callan 1994; Kaszkiel and Zobel 1997). More recently, a great deal of interest has arisen in using automatic segmentation for the detection of topic and story boundaries in news feeds (Mani et al. 1997; Merlino, Morey, and Maybury 1997; Ponte and Croft 1997; Hauptmann and Witbrock 1998; Allan et al. 1998; Beeferman, Berger, and Lafferty 1997, 1999). Sometimes segmentation is done at the clause level, for the purposes of detecting nuances of dialogue structure or for more sophisticated discourse-processing purposes (Morris and Hirst 1991; Passonneau and Litman 1993; Litman and Passonneau 1995; Hirschberg and Nakatani 1996; Marcu 2000). Some of these algorithms produce hierarchical dialogue segmentations whose evaluation is outside the scope of this discussion. 1.1 Evaluating Segmentation Algorithms There are two major difficulties associated with evaluating algorithms for text segmentation. The first is that since human judges do not always agree where boundaries should be placed and how fine grained an analysis should be, it is difficult to choose a reference segmentation for comparison. Some evaluations circumvent this difficulty by detecting boundaries in sets of concatenated d"
J02-1002,J91-1002,0,0.642054,"nformation retrieval (Hearst and Plaunt 1993; Salton, Allan, and Buckley 1993; Callan 1994; Kaszkiel and Zobel 1997). More recently, a great deal of interest has arisen in using automatic segmentation for the detection of topic and story boundaries in news feeds (Mani et al. 1997; Merlino, Morey, and Maybury 1997; Ponte and Croft 1997; Hauptmann and Witbrock 1998; Allan et al. 1998; Beeferman, Berger, and Lafferty 1997, 1999). Sometimes segmentation is done at the clause level, for the purposes of detecting nuances of dialogue structure or for more sophisticated discourse-processing purposes (Morris and Hirst 1991; Passonneau and Litman 1993; Litman and Passonneau 1995; Hirschberg and Nakatani 1996; Marcu 2000). Some of these algorithms produce hierarchical dialogue segmentations whose evaluation is outside the scope of this discussion. 1.1 Evaluating Segmentation Algorithms There are two major difficulties associated with evaluating algorithms for text segmentation. The first is that since human judges do not always agree where boundaries should be placed and how fine grained an analysis should be, it is difficult to choose a reference segmentation for comparison. Some evaluations circumvent this diff"
J02-1002,C94-2187,0,0.210363,"Missing"
J02-1002,P93-1020,0,0.207016,"versity of California, Berkeley, 102 South Hall #4600, Berkeley, CA 94720. E-mail: hearst@sims.berkeley.edu c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 1 of it, for subtopic segmentation (Nomoto and Nitta 1994; Hasnah 1996; Richmond, Smith, and Amitay 1997; Heinonen 1998; Boguraev and Neff 2000). Other techniques use clustering and/or similarity matrices based on word co-occurrences (Reynar 1994; Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cue words, or hand-selected cue words to detect segment boundaries (Passonneau and Litman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998). Researchers have explored the use of this kind of document segmentation to improve automated summarization (Salton et al. 1994; Barzilay and Elhadad 1997; Kan, Klavans, and McKeown 1998; Mittal et al. 1999; Boguraev and Neff 2000) and automated genre detection (Karlgren 1996). Text segmentation issues are also important for passage retrieval, a subproblem of information retrieval (Hearst and Plaunt 1993; Salton, Allan, and Buckley 1993; Callan 1994; Kaszkiel and Zobel 1997). More recently, a great deal of interest has arisen in using autom"
J02-1002,P94-1050,0,0.762245,"ny others have used this technique, or slight variations ∗ Harvard University, 380 Leverett Mail Center, Cambridge, MA 02138. E-mail: pevzner@post.harvard.edu † University of California, Berkeley, 102 South Hall #4600, Berkeley, CA 94720. E-mail: hearst@sims.berkeley.edu c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 1 of it, for subtopic segmentation (Nomoto and Nitta 1994; Hasnah 1996; Richmond, Smith, and Amitay 1997; Heinonen 1998; Boguraev and Neff 2000). Other techniques use clustering and/or similarity matrices based on word co-occurrences (Reynar 1994; Yaari 1997; Choi 2000), and still others use machine learning techniques to detect cue words, or hand-selected cue words to detect segment boundaries (Passonneau and Litman 1993; Beeferman, Berger, and Lafferty 1997; Manning 1998). Researchers have explored the use of this kind of document segmentation to improve automated summarization (Salton et al. 1994; Barzilay and Elhadad 1997; Kan, Klavans, and McKeown 1998; Mittal et al. 1999; Boguraev and Neff 2000) and automated genre detection (Karlgren 1996). Text segmentation issues are also important for passage retrieval, a subproblem of infor"
J02-1002,W97-0305,0,0.0388378,"Missing"
J97-1003,J96-2004,0,0.573347,"against author-specified markups, but unfortunately, as discussed above, authors usually do not specify the kind of subtopic information desired. As mentioned above, Hearst (1995) and Hearst and Plaunt (1993) show how to use TextTiles in information retrieval tasks, although this work does not show whether or not the results of these algorithms produce better performance than the results of some other segmentation strategy would. 6.1 Reader Judgments There is a growing concern surrounding issues of intercoder reliability when using human judgments to evaluate discourse-processing algorithms (Carletta 1996; Condon and Cech 1995). Proposals have recently been made for protocols for the collection of human discourse segmentation data (Nakatani et al. 1995) and for how to evaluate the validity of judgments so obtained (Carletta 1996; Isard and Carletta 1995; Ros6 1995; Passonneau and Litman 1993; Litman and Passonneau 1995). Recently, Hirschberg 52 Hearst TextTiling and Nakatani (1996) have reported promising results for obtaining higher interjudge agreement using their collection protocols. For the evaluation of the TextTiling algorithms, judgments were obtained from seven readers for each of 12"
J97-1003,P92-1032,0,0.0298452,"Missing"
J97-1003,H92-1045,0,0.473029,"Missing"
J97-1003,J86-3001,0,0.139856,"remains, then, of how to detect subtopic shift. Brown and Yule (1983) consider in detail two markers: adverbial clauses and certain kinds of prosodic markers. By contrast, the next subsection will show that lexical co-occurrence patterns can be used to identify subtopic shift. 3.2 Relationship to Segmentation in Hierarchical Discourse Models Much of the current work in empirical discourse processing makes use of hierarchical discourse models, and several prominent theories of discourse assume a hierarchical segmentation model. Foremost among these are the attentional/intentional structure of Grosz and Sidner (1986) and the Rhetorical Structure Theory of Mann and Thompson (1987). The building blocks for these theories are phrasal or clausal units, and the targets of the analyses are usually very short texts, typically one to three paragraphs in length. 5 Many problems in discourse analysis, such as dialogue generation and turntaking (Moore and Pollack 1992; Walker and Whittaker 1990), require fine-grained, hierarchical models that are concerned with utterance-level segmentation. Progress is being made in the automatic detection of boundaries at this level of granularity using machine learning techniques"
J97-1003,P94-1002,1,0.221091,"tive at each sentence gap. This approach is illustrated in Figure 3(c). A lexical chain for term t is considered active across a sentence gap if instances of t occur within some distance threshold of one another. In the figure, all three instances of the word A occur within the distance threshold. The third B, however, follows too far after the second B to continue the chain. The score for the gap between 2 and 3 is simply the number of active chains that span this gap. Boundaries are determined as specified in Section 5. This variation of the TextTiling algorithm is explored and evaluated in Hearst (1994b). 4.4 Vector Space Similarity Comparisons As mentioned in Section 2, Salton and Allan (1993) report work in the automatic detection of hypertext links and theme generation from large documents, focusing primarily on encyclopedia text. They describe the application of similarity comparisons between articles, sections, and paragraphs within an encyclopedia, both for creating links among related passages, and for better facilitating retrieval of articles in response to user queries. Their approach finds similarities among the paragraphs of large documents using normalized tfidf term weighting,"
J97-1003,J93-3003,0,0.0247109,"Missing"
J97-1003,P96-1038,0,0.0573924,"detection of boundaries at this level of granularity using machine learning techniques combined with a variety of well-chosen discourse cues (Litman and Passonneau 1995). In contrast, TextTiling has the goal of identifying major subtopic boundaries, attempting only a linear segmentation. We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed. The work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration (Hirschberg and Nakatani 1996), discourse markers such as oh, well, ok, however (Schiffrin 1987; Litman and Passonneau 1995), pronoun reference resolution (Passonneau and Litman 1993; Webber 1988) and tense and aspect (Webber 1987; Hwang and Schubert 1992). From a computational viewpoint, deducing textual topic structure from lexical occurrence information alone is appealing, both because it is easy to compute, and because discourse cues are sometimes misleading with respect to the topic structure (Brown and Yule 1983, Section 3). 4. Detecting Subtopic Change via Lexical Co-occurrence Patterns TextTiling assumes that a set"
J97-1003,P92-1030,0,0.0463547,"ndaries, attempting only a linear segmentation. We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed. The work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration (Hirschberg and Nakatani 1996), discourse markers such as oh, well, ok, however (Schiffrin 1987; Litman and Passonneau 1995), pronoun reference resolution (Passonneau and Litman 1993; Webber 1988) and tense and aspect (Webber 1987; Hwang and Schubert 1992). From a computational viewpoint, deducing textual topic structure from lexical occurrence information alone is appealing, both because it is easy to compute, and because discourse cues are sometimes misleading with respect to the topic structure (Brown and Yule 1983, Section 3). 4. Detecting Subtopic Change via Lexical Co-occurrence Patterns TextTiling assumes that a set of lexical items is in use during the course of a given subtopic discussion, and when that subtopic changes, a significant proportion of the vocabulary changes as well. The algorithm is designed to recognize episode boundarie"
J97-1003,P93-1041,0,0.181926,"tO ~0 0 ~0 O 00 .wt LO u3 t&apos;- ,r-t 0 l&quot;- o ,¢--I It) 0 ,¢-t O ¢.O L.O 0 LO LO &quot;,~ O ¢&apos;1 LO I.O ,q4 ,¢-t 0 P. CO ,¢-t P. 0 O 03 ,r--t (&apos;,1 m C4 O,I 0 O ,r.t v-I O CO .i-I 0 v-I -,-I O • ,--I ~ ~&apos;~ ~ ~ 0 ¢g r--I -~ I~l 0 • I~ • 0 O O ,:::1~ O0 u~ LO LO ~1~ O0 b.- ¢,0 ¢.0 1 ~ ,~ Ob 1,~. O0 b.. O0 ¢ . 0 0 0 ¢,0 ~ Figure 2 Distribution of selected terms from the Stargazer text, with a single digit frequency per sentence number (blanks indicate a frequency of zero). 42 Hearst TextTiling variation in the way concepts are expressed, and so may require that thesaural relations be used as well, as in (Kozima 1993). It should be noted that other researchers have experimented with the display of patterns of cohesion cues other than lexical cohesion as tools for analyzing discourse structure. Grimes (1975, Chapter 6) introduces span charts to show the interaction of various thematic devices such as character identification, setting, and tense. Stoddard (1991) creates cohesion maps by assigning to each word a location on a twodimensional grid corresponding to the word&apos;s position in the text. To summarize, many discourse analysis tasks require a fine-grained, hierarchical model, and consequently require man"
J97-1003,P95-1015,0,0.078064,"on (1987). The building blocks for these theories are phrasal or clausal units, and the targets of the analyses are usually very short texts, typically one to three paragraphs in length. 5 Many problems in discourse analysis, such as dialogue generation and turntaking (Moore and Pollack 1992; Walker and Whittaker 1990), require fine-grained, hierarchical models that are concerned with utterance-level segmentation. Progress is being made in the automatic detection of boundaries at this level of granularity using machine learning techniques combined with a variety of well-chosen discourse cues (Litman and Passonneau 1995). In contrast, TextTiling has the goal of identifying major subtopic boundaries, attempting only a linear segmentation. We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed. The work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration (Hirschberg and Nakatani 1996), discourse markers such as oh, well, ok, however (Schiffrin 1987; Litman and Passonneau 1995), pronoun reference resolution (Passonneau and"
J97-1003,C90-2048,0,0.065359,"Missing"
J97-1003,J92-4007,0,0.0299777,"h of the current work in empirical discourse processing makes use of hierarchical discourse models, and several prominent theories of discourse assume a hierarchical segmentation model. Foremost among these are the attentional/intentional structure of Grosz and Sidner (1986) and the Rhetorical Structure Theory of Mann and Thompson (1987). The building blocks for these theories are phrasal or clausal units, and the targets of the analyses are usually very short texts, typically one to three paragraphs in length. 5 Many problems in discourse analysis, such as dialogue generation and turntaking (Moore and Pollack 1992; Walker and Whittaker 1990), require fine-grained, hierarchical models that are concerned with utterance-level segmentation. Progress is being made in the automatic detection of boundaries at this level of granularity using machine learning techniques combined with a variety of well-chosen discourse cues (Litman and Passonneau 1995). In contrast, TextTiling has the goal of identifying major subtopic boundaries, attempting only a linear segmentation. We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and al"
J97-1003,J91-1002,0,0.841474,"it is quite difficult to draw conclusions at a paragraph-sized granularity. I 6 This might be explainedin part by Stark (1988) who shows that readers disagree measurablyabout where to place paragraph boundaries when presented with texts with those boundaries removed. 45 Computational Linguistics Volume 23, Number 1 plot the score at every sentence gap, thus eliminating the wide variation that is seen when measuring after each word. Results for this approach are reported in Section 6. 4.3 Lexical Chains Morris and Hirst&apos;s pioneering work on computing discourse structure from lexical relations (Morris and Hirst 1991; Morris 1988) is a precursor to the work reported on here. Influenced by Halliday and Hasan&apos;s (1976) theory of lexical coherence, Morris developed an algorithm that finds chains of related terms via a comprehensive thesaurus (Roget&apos;s Fourth Edition). 7 For example, the words residential and apartment both index the same thesaural category and can thus be considered to be in a coherence relation with one another. The chains are used to structure texts according to the attentional/intentional theory of discourse structure (Grosz and Sidner 1986) discussed above. The extent of the lexical chains"
J97-1003,C94-2187,0,0.145937,"Missing"
J97-1003,P93-1020,0,0.0788465,"ssonneau 1995). In contrast, TextTiling has the goal of identifying major subtopic boundaries, attempting only a linear segmentation. We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed. The work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration (Hirschberg and Nakatani 1996), discourse markers such as oh, well, ok, however (Schiffrin 1987; Litman and Passonneau 1995), pronoun reference resolution (Passonneau and Litman 1993; Webber 1988) and tense and aspect (Webber 1987; Hwang and Schubert 1992). From a computational viewpoint, deducing textual topic structure from lexical occurrence information alone is appealing, both because it is easy to compute, and because discourse cues are sometimes misleading with respect to the topic structure (Brown and Yule 1983, Section 3). 4. Detecting Subtopic Change via Lexical Co-occurrence Patterns TextTiling assumes that a set of lexical items is in use during the course of a given subtopic discussion, and when that subtopic changes, a significant proportion of the vocabulary"
J97-1003,P94-1050,0,0.379683,"local coherence&quot; (p. 286) in narrative text. Kozima (1993) presents a very elaborate algorithm for computing the lexical cohesiveness of a window of words, using spreading activation in a semantic network created from an English dictionary. The cohesion score is plotted against words and smoothed, and boundaries are considered to fall at the lowest-scoring words. This complex computation, as opposed to simple term repetition, may be necessary when working with narrative texts, but no comparison of methods is done. The algorithm&apos;s results are shown on one text, but are not evaluated formally. Reynar (1994) describes an algorithm similar to that of Hearst (1993) and Hearst and Plaunt (1993) with a difference in the way in which the size of the blocks of adjacent regions are chosen. A greedy algorithm is used: the algorithm begins with no boundaries, then a boundary b (between two sentences) is chosen which maximizes the lexical score resulting from comparing the block on the left whose extent ranges from b to the closest existing boundary on the left, and similarly for the right. This process is repeated until a prespecified number of boundaries have been chosen. This seems problematic, since th"
J97-1003,C92-1054,0,0.0620688,"Missing"
J97-1003,P90-1010,0,0.0548054,"empirical discourse processing makes use of hierarchical discourse models, and several prominent theories of discourse assume a hierarchical segmentation model. Foremost among these are the attentional/intentional structure of Grosz and Sidner (1986) and the Rhetorical Structure Theory of Mann and Thompson (1987). The building blocks for these theories are phrasal or clausal units, and the targets of the analyses are usually very short texts, typically one to three paragraphs in length. 5 Many problems in discourse analysis, such as dialogue generation and turntaking (Moore and Pollack 1992; Walker and Whittaker 1990), require fine-grained, hierarchical models that are concerned with utterance-level segmentation. Progress is being made in the automatic detection of boundaries at this level of granularity using machine learning techniques combined with a variety of well-chosen discourse cues (Litman and Passonneau 1995). In contrast, TextTiling has the goal of identifying major subtopic boundaries, attempting only a linear segmentation. We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed. The work des"
J97-1003,P87-1021,0,0.0527501,"ying major subtopic boundaries, attempting only a linear segmentation. We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed. The work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration (Hirschberg and Nakatani 1996), discourse markers such as oh, well, ok, however (Schiffrin 1987; Litman and Passonneau 1995), pronoun reference resolution (Passonneau and Litman 1993; Webber 1988) and tense and aspect (Webber 1987; Hwang and Schubert 1992). From a computational viewpoint, deducing textual topic structure from lexical occurrence information alone is appealing, both because it is easy to compute, and because discourse cues are sometimes misleading with respect to the topic structure (Brown and Yule 1983, Section 3). 4. Detecting Subtopic Change via Lexical Co-occurrence Patterns TextTiling assumes that a set of lexical items is in use during the course of a given subtopic discussion, and when that subtopic changes, a significant proportion of the vocabulary changes as well. The algorithm is designed to r"
J97-1003,P88-1014,0,0.0337508,"TextTiling has the goal of identifying major subtopic boundaries, attempting only a linear segmentation. We should expect to see, in grouping together paragraph-sized units instead of utterances, a decrease in the complexity of the feature set and algorithm needed. The work described here makes use only of lexical distribution information, in lieu of prosodic cues such as intonational pitch, pause, and duration (Hirschberg and Nakatani 1996), discourse markers such as oh, well, ok, however (Schiffrin 1987; Litman and Passonneau 1995), pronoun reference resolution (Passonneau and Litman 1993; Webber 1988) and tense and aspect (Webber 1987; Hwang and Schubert 1992). From a computational viewpoint, deducing textual topic structure from lexical occurrence information alone is appealing, both because it is easy to compute, and because discourse cues are sometimes misleading with respect to the topic structure (Brown and Yule 1983, Section 3). 4. Detecting Subtopic Change via Lexical Co-occurrence Patterns TextTiling assumes that a set of lexical items is in use during the course of a given subtopic discussion, and when that subtopic changes, a significant proportion of the vocabulary changes as we"
J97-1003,C92-2070,0,0.025864,"is not stated as a concern in Morris&apos;s work, perhaps because the texts were short, and presumably, if a word were ambiguous, the correct thesaurus class would nevertheless be chosen because the chained-to words would share only the correct thesaurus class. However, my experimentation with an implemented version of Morris&apos; algorithm that made use of Roget&apos;s 1911 thesaurus (which is admittedly less structured than the thesaurus used by Morris), when run on longer texts, found ambiguous links to be a common occv&apos;,:ence and detrimental to the algorithm. A thesaurus-based disambiguation algorithm (Yarowsky 1992) may help alleviate this problem (this option is revisited in Section 7), but another solution is to move away from thesaurus classes and use simple word co-occurrence instead, since within a given text a word is usually used with only one sense (Gale, Church, and Yarowsky 1992b). The potential downside of this approach is that many useful links may be missed. Another limitation of the Morris algorithm is that it does not take advantage of, or discuss how to account for, the tendency for multiple simultaneous chains to occur over the same intention (each chain corresponds to one intention). Re"
J97-2002,A88-1019,0,0.0552867,"munication with Joe Zhou. 7 ""Satz"" is the German word for ""sentence."" 246 Palmer and Hearst Multilingual Sentence Boundary Input Text Tokenization Part-of-speech Lookup Descriptor array construction Classificationby learning algorithm Text withsentence boundaries disambiguated Figure 1 The Satz architecture. 3.1 Tokenization The first stage of the process is lexical analysis, which breaks the input text (a stream of characters) into tokens. The Satz tokenizer is implemented using the UNIX tool LEX (Lesk and Schmidt 1975) and is modeled on the tokenizer used by the PARTS part-of-speech tagger (Church 1988). The tokens returned by the LEX program can be a sequence of alphabetic characters, a sequence of digits, 8 or a sequence of one or more non-alphanumeric characters such as periods or quotation marks. 3.2 Part-of-Speech Lookup The individual tokens are next assigned a series of possible parts of speech, based on a lexicon and simple heuristics described below. 3.2.1 Representing Context. The context surrounding a punctuation mark can be represented in various ways. The most straightforward is to use the individual words preceding and following the punctuation mark, as in this example: at the"
J97-2002,C94-2178,0,0.00944247,"at the e n d of a sentence. Ellipsis, a series of p e r i o d s (...), can o c c u r b o t h w i t h i n sentences a n d at * 202 Burlington Road, Bedford, MA 01730. E-maih palmer@mitre.org. Some of the work reported here was done while the author was at the University of California, Berkeley. The views and opinions in this paper are those of the authors and do not reflect the MITRE Corporation's current work position. t 3333 Coyote Hill Rd., Palo Alto, CA 94304. E-marl: hearst@parc.xerox.com 1 There is some recent research in aligning bilingual corpora without relying on sentence boundaries (Fung and Church 1994; Fung and McKeown 1994). 2 In this article, we will consider only the period, the exclamation point, and the question mark to be possible ""end-of-sentence punctuation marks,"" and all references to ""ptmctuation marks"" will refer to these three. Although the colon, the semicolon, and conceivably the comma can also delimit grammatical sentences, their usage is beyond the scope of this work. (~) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 sentence boundaries. The ambiguity of these punctuation marks is illustrated in the following difficult cases:"
J97-2002,1994.amta-1.11,0,0.00964401,"ence. Ellipsis, a series of p e r i o d s (...), can o c c u r b o t h w i t h i n sentences a n d at * 202 Burlington Road, Bedford, MA 01730. E-maih palmer@mitre.org. Some of the work reported here was done while the author was at the University of California, Berkeley. The views and opinions in this paper are those of the authors and do not reflect the MITRE Corporation's current work position. t 3333 Coyote Hill Rd., Palo Alto, CA 94304. E-marl: hearst@parc.xerox.com 1 There is some recent research in aligning bilingual corpora without relying on sentence boundaries (Fung and Church 1994; Fung and McKeown 1994). 2 In this article, we will consider only the period, the exclamation point, and the question mark to be possible ""end-of-sentence punctuation marks,"" and all references to ""ptmctuation marks"" will refer to these three. Although the colon, the semicolon, and conceivably the comma can also delimit grammatical sentences, their usage is beyond the scope of this work. (~) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 sentence boundaries. The ambiguity of these punctuation marks is illustrated in the following difficult cases: (1) The group included D"
J97-2002,J93-1004,0,0.0350833,"ion mark occurring in its context is indeed the end of a sentence. Two adjustable sensitivity thresholds, to and tl, are used to classify the results of the disambiguation. If the output is less than to, the punctuation mark is not a sentence boundary; if the output is greater than or equal to tl, it is a sentence boundary. Outputs which fall between the thresholds cannot be disambiguated by the network (which may indicate that the mark is inherently ambiguous) and are marked accordingly, so they can be treated specially in later processing. 13 For example, the sentence alignment algorithm in Gale and Church (1993) allows a distinction between hard and soft boundaries, where soft boundaries are movable by the alignment program. In our case, punctuation marks remaining ambiguous after processing by Satz can be treated as soft boundaries while unambiguous punctuation marks (as well as paragraph boundaries) can be treated as hard boundaries, thus allowing the alignment program greater flexibility. A neural network is trained by presenting it with input data paired with the desired output. For Satz, the input is the context surrounding the punctuation mark to be disambiguated, and the output is a score indi"
J97-2002,P95-1037,0,0.0118364,"pass through all the training data. Training times for all experiments reported in this article were less than one minute and were obtained on a DEC Alpha 3000 workstation, unless otherwise noted. In Sections 4.1-4.9 we present results of testing the Satz system with a neural network, including investigations of the effects of varying network parameters such as hidden layer size, threshold values, and amount of training data. 3.4.2 Decision Tree. Algorithms for decision tree induction (Quinlan 1986; Bahl et al. 1989) have been successfully applied to NLP problems such as parsing (Resnik 1993; Magerman 1995) and discourse analysis (Siegel and McKeown 1994; Soderland and Lehnert 1994). We tested the Satz system using the c4.5 (Quinlan 1993) decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network. These results are discussed in Section 4.10. The induction algorithm proceeds by e;caluating the information content of a series of binary attributes and iteratively building a tree from the attribute values, with the leaves of the decision tree being the values of the goal attributes. At each step in the learning procedure, t"
J97-2002,C90-3038,0,0.0132846,"e descriptor arrays are used to train the parameters of the learning algorithm. We investigated the effectiveness of two separate algorithms: (1) back-propagation training of neural networks, and (2) decision tree induction. The learning algorithms are described in the next two sections, and the results obtained with the algorithms are presented in Section 4. 3.4.1 Neural Network. Artificial neural networks have been successfully applied for many years in speech recognition applications (Bourland and Morgan 1994; Lippmann 1989), and more recently in NLP tasks such as word category prediction (Nakamura et al. 1990) and part-of-speech tagging (Schmid 1994). Neural networks in the context of machine learning provide a well-tested training algorithm (back-propagation) that has achieved high success rates in pattern-recognition problems similar to the problem posed by sentence boundary disambiguation (Hertz, Krogh, and Palmer 1991). For Satz, we used a fully-connected feed-forward neural network, as shown in Figure 3. The network accepts k • 20 input values, where k is the size of the context and 20 is the number of elements in the descriptor array described in Section 3.3. The 250 Palmer and Hearst Multili"
J97-2002,A94-1013,1,0.891064,"Missing"
J97-2002,H93-1054,0,0.0173685,"is a single pass through all the training data. Training times for all experiments reported in this article were less than one minute and were obtained on a DEC Alpha 3000 workstation, unless otherwise noted. In Sections 4.1-4.9 we present results of testing the Satz system with a neural network, including investigations of the effects of varying network parameters such as hidden layer size, threshold values, and amount of training data. 3.4.2 Decision Tree. Algorithms for decision tree induction (Quinlan 1986; Bahl et al. 1989) have been successfully applied to NLP problems such as parsing (Resnik 1993; Magerman 1995) and discourse analysis (Siegel and McKeown 1994; Soderland and Lehnert 1994). We tested the Satz system using the c4.5 (Quinlan 1993) decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network. These results are discussed in Section 4.10. The induction algorithm proceeds by e;caluating the information content of a series of binary attributes and iteratively building a tree from the attribute values, with the leaves of the decision tree being the values of the goal attributes. At each step in the learn"
J97-2002,H89-2048,0,0.486821,"s. The Satz system offers a robust, rapidly trainable alternative to existing systems, which usually require extensive manual effort and are tailored to a specific text genre or a particular language. By using part-of-speech frequency data to represent the context in which the punctuation mark appears, the system offers significant savings in parameter estimation and training time over word-based methods, while at the same 264 Palmer and Hearst Multilingual Sentence Boundary time producing a very low error rate (see Table 10 for a summary of the best results for each language). The systems of Riley (1989) and Wasson report what seem to be slightly better error rates, but these results are not directly comparable since they were evaluated on other collections. Furthermore, the Wasson system required nine staff months of development, and the Riley system required 25 million word tokens for training and storage of probabilities for all corresponding word types. By comparison, the Satz approach has the advantages of flexibility for application to new text genres, small training sets (and thereby fast training times), relatively small storage requirements, and little manual effort. The training tim"
J97-2002,C94-1027,0,0.01361,"rs of the learning algorithm. We investigated the effectiveness of two separate algorithms: (1) back-propagation training of neural networks, and (2) decision tree induction. The learning algorithms are described in the next two sections, and the results obtained with the algorithms are presented in Section 4. 3.4.1 Neural Network. Artificial neural networks have been successfully applied for many years in speech recognition applications (Bourland and Morgan 1994; Lippmann 1989), and more recently in NLP tasks such as word category prediction (Nakamura et al. 1990) and part-of-speech tagging (Schmid 1994). Neural networks in the context of machine learning provide a well-tested training algorithm (back-propagation) that has achieved high success rates in pattern-recognition problems similar to the problem posed by sentence boundary disambiguation (Hertz, Krogh, and Palmer 1991). For Satz, we used a fully-connected feed-forward neural network, as shown in Figure 3. The network accepts k • 20 input values, where k is the size of the context and 20 is the number of elements in the descriptor array described in Section 3.3. The 250 Palmer and Hearst Multilingual Sentence Boundary DA DA. • • DA DA"
J97-2002,1993.tmi-1.28,0,\N,Missing
J97-2002,M95-1012,0,\N,Missing
J97-2002,A92-1018,0,\N,Missing
J97-2002,P93-1001,0,\N,Missing
J97-2002,J93-1006,0,\N,Missing
N04-4030,C92-2082,1,\N,Missing
N04-4030,P99-1016,0,\N,Missing
N07-1031,P99-1016,0,0.0467431,"s may be easier than the full ontology inference problem. 3 Related Work There is a large literature on document classification and automated text categorization (Sebastiani, 2002). However, that work assumes that the categories of interest are already known, and tries to assign documents to categories. In contrast, in this paper we focus on the problem of determining the categories of interest. Another thread of work is on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and free text (Hearst, 1992; Caraballo, 1999). That work focuses on finding the right position for a word within a lexicon, rather than building up comprehensible and coherent faceted hierarchies. A major class of solutions for creating subject hierarchies uses data clustering. The Scatter/Gather system (Cutting et al., 1992) uses a greedy global agglomerative clustering algorithm where an initial set of clusters is recursively re-clustered until only documents remain. Hofmann (1999) proposes the probabilistic latent semantic analysis algorithm (pLSA), a probabilistic version of clustering that uses latent semantic analysis for grouping"
N07-1031,C92-2082,1,0.024219,"cet hierarchies may be easier than the full ontology inference problem. 3 Related Work There is a large literature on document classification and automated text categorization (Sebastiani, 2002). However, that work assumes that the categories of interest are already known, and tries to assign documents to categories. In contrast, in this paper we focus on the problem of determining the categories of interest. Another thread of work is on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and free text (Hearst, 1992; Caraballo, 1999). That work focuses on finding the right position for a word within a lexicon, rather than building up comprehensible and coherent faceted hierarchies. A major class of solutions for creating subject hierarchies uses data clustering. The Scatter/Gather system (Cutting et al., 1992) uses a greedy global agglomerative clustering algorithm where an initial set of clusters is recursively re-clustered until only documents remain. Hofmann (1999) proposes the probabilistic latent semantic analysis algorithm (pLSA), a probabilistic version of clustering that uses latent semantic anal"
N07-1031,magnini-cavaglia-2000-integrating,0,0.0958717,"Missing"
N07-1031,N04-4030,1,0.502266,"llection.       4 Method The main idea behind the Castanet algorithm1 is to carve out a structure from the hypernym (IS-A) relations within the WordNet (Fellbaum, 1998) lexical database. The primary unit of representation in WordNet is the synset, which is a set of words that are considered synonyms for a particular concept. Each synset is linked to other synsets via several types of lexical and semantic relations; we only use hypernymy (IS-A relations) in this algorithm. 1 A simpler, un-evaluated version of this algorithm was presented previously in a short paper (Stoica and Hearst, 2004). entity 4.1 Algorithm Overview substance, matter The Castanet algorithm assumes that there is text associated with each item in the collection, or at least with a representative subset of the items. The textual descriptions are used both to build the facet hierarchies and to assign items (documents, images, citations, etc.) to the facets. The text does not need to be particularly coherent for the algorithm to work; we have applied it to fragmented image annotations and short journal titles, but if the text is impoverished, the information items will not be labeled as thoroughly as desirable a"
N16-1134,D13-1181,0,0.020491,"lyzed text quality using what may be called “high-level” features — i.e., moving beyond the specific lexical, syntactic, or phonemic properties of sentences to explore the overall structure of sentences or even the discursive relationships between them. Feng et al. (2012) engineered features from sentences’ CFG parse trees to describe their overall structure and classify them in terms of a priori rhetorical categories, such as loose vs. periodic. They found that these features were helpful for authorship classification, and a later study used similar features to predict the success of novels (Ashok et al., 2013). Wible and Tsao (2010) and Gianfortoni et al. (2011) designed n-gram based features to capture local lexico-syntactic sequences within sentences. Looking at the level of discourse, Louis and Nenkova (2012) found that adjacent sentences “exhibit stable patterns of syntactic co-occurence” — i.e., certain types of sentences tend to follow certain other types of sentences. Furthermore, they demonstrated that sentences with similar communicative purposes are syntactically similar, so syntax can be taken as a proxy for communicative purpose. We build on both of these observations. 3 Data-sets Quota"
N16-1134,W12-2510,0,0.188662,"Missing"
N16-1134,D14-1082,0,0.0170481,"d by a verb phrase followed by punctuation. This feature, they argue, provides an interpretable representation of the general syntactic structure of a sentence. We directly employ this feature. General/Abstract Words. Through qualitative analysis of our data, we noticed that many quotation make pronouncements about nouns that might be considered as generalizations or abstractions. For 1141 instance: “Peace comes from within. Do not seek it without.” In this case, the abstract noun “peace” is the subject of the first sentence. To capture such nouns, we first use the Stanford Dependency Parser (Chen and Manning, 2014) to extract all words in the head position of nominal subject dependencies (excluding stopwords). Using WordNet, we check whether the word’s most common synset is both within the hyponym hierarchy of the synset “abstraction.n.06” and within a minimum distance (5) of it2 ; if so, we consider this noun Abstract.3 The most common such nouns in the Quotations 1 corpus are not necessarily concept words like “peace”. For instance, the word “men” appears in this list; many quotations use the word to evoke a generalized (male) subject.4 We consider a nominal subject to be General if it is within a min"
N16-1134,P12-1094,0,0.0277496,"pes” or “figures,” patterns of language from the level of the phoneme (as in the case of rhyme or alliteration) to higher-level syntactic and even logical structures (Peacham, 1954). In the figure of epistrophe, successive clauses end with the same words. In pysma, the speaker (or writer) launches a series of sharp and vehement questions; this and other rhetorical figures describe language at the level of discourse—that is, the relationship between linguistic elements across sentences. Recent studies of quotations have described what makes certain fragments of text more memorable than others (Danescu-Niculescu-Mizil et al., 2012; 2.1 Related Work Style of Memorable Language Danescu-Niculescu-Mizil et al. (2012) used a variety of features to distinguish popular movie quotations from unmemorable lines from the same movie. By modeling word and part-of-speech sequences of quotes and non-quotes, they found that quotations tended to use less common words but that these words were placed in more common syntactic patterns. The researchers evocatively hint at some “common syntactic scaffolding” that structures quotations, and we build on this finding by characterizing these patterns. They also found that quotations tend to co"
N16-1134,D12-1139,0,0.073051,"Missing"
N16-1134,W11-2606,0,0.0710238,"Missing"
N16-1134,N15-1172,0,0.016759,"on syntactic scaffolding” that structures quotations, and we build on this finding by characterizing these patterns. They also found that quotations tend to contain linguistic features that make them more “generalizable,” such as tendency toward indefinite over definite articles. Other researchers have attempted to analyze quotations in ways that evoke the traditional figures 1139 Proceedings of NAACL-HLT 2016, pages 1139–1144, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics of rhetoric. Exploring the same movie quotes corpus as well as other corpora, Guerini et al. (2015) found that memorable quotes were more euphonic, with more instances of rhyme and alliteration than their non-memorable counterparts. Kuznetsova et al. (2013) developed several methods for quantifying the creativity of word combinations like “disadvantageous peace,” and found that quotes were more likely to contain creative combinations than non-quotes. In terms of classical rhetoric, such unexpected word combinations could embody figures such as oxymoron. 2.2 High-Level Style To investigate quotations in a new way, we are inspired by researchers who have analyzed text quality using what may b"
N16-1134,W12-2502,0,0.063314,"Missing"
N16-1134,P03-1054,0,0.0151369,"sorted by χ2 value; dominant sentence position is in bold. Highest χ2 represents the top five unigrams. Other Unigrams represents other select examples. NQ2 is whether feature is also ordered in the same way in the comparison corpus (“x” if this is the case). terns in quotations, regardless of what other more classical rhetorical figures they may contain. Unigrams. As a baseline feature, we note in which sentence, 1 or 2, a unigram occurs. High-Level Syntax. Feng et al. (2012) found that the top level of syntactic parse trees (in the case of Stanford PCFG Parser’s output, which we also used (Klein and Manning, 2003), two levels beneath ROOT) provided a useful feature for authorship identification. For instance, the sentence “Forgotten is forgiven.” can be represented by the construction NP + VP + ., a noun phrase followed by a verb phrase followed by punctuation. This feature, they argue, provides an interpretable representation of the general syntactic structure of a sentence. We directly employ this feature. General/Abstract Words. Through qualitative analysis of our data, we noticed that many quotation make pronouncements about nouns that might be considered as generalizations or abstractions. For 114"
N16-1134,D13-1124,0,0.0372176,"Missing"
N16-1134,D12-1106,0,0.166235,"s or even the discursive relationships between them. Feng et al. (2012) engineered features from sentences’ CFG parse trees to describe their overall structure and classify them in terms of a priori rhetorical categories, such as loose vs. periodic. They found that these features were helpful for authorship classification, and a later study used similar features to predict the success of novels (Ashok et al., 2013). Wible and Tsao (2010) and Gianfortoni et al. (2011) designed n-gram based features to capture local lexico-syntactic sequences within sentences. Looking at the level of discourse, Louis and Nenkova (2012) found that adjacent sentences “exhibit stable patterns of syntactic co-occurence” — i.e., certain types of sentences tend to follow certain other types of sentences. Furthermore, they demonstrated that sentences with similar communicative purposes are syntactically similar, so syntax can be taken as a proxy for communicative purpose. We build on both of these observations. 3 Data-sets Quotations 1. We gathered a data-set of quotations from the social network Tumblr (Chang et al., 2014). Like other social networks, Tumblr has become a place where users frequently share quotations. In fact, use"
N16-1134,P11-1029,0,0.139468,"Missing"
N16-1134,W10-0804,0,0.0136924,"ing what may be called “high-level” features — i.e., moving beyond the specific lexical, syntactic, or phonemic properties of sentences to explore the overall structure of sentences or even the discursive relationships between them. Feng et al. (2012) engineered features from sentences’ CFG parse trees to describe their overall structure and classify them in terms of a priori rhetorical categories, such as loose vs. periodic. They found that these features were helpful for authorship classification, and a later study used similar features to predict the success of novels (Ashok et al., 2013). Wible and Tsao (2010) and Gianfortoni et al. (2011) designed n-gram based features to capture local lexico-syntactic sequences within sentences. Looking at the level of discourse, Louis and Nenkova (2012) found that adjacent sentences “exhibit stable patterns of syntactic co-occurence” — i.e., certain types of sentences tend to follow certain other types of sentences. Furthermore, they demonstrated that sentences with similar communicative purposes are syntactically similar, so syntax can be taken as a proxy for communicative purpose. We build on both of these observations. 3 Data-sets Quotations 1. We gathered a"
P02-1032,P98-1015,0,0.803623,"Missing"
P02-1032,W97-0205,0,0.0119864,"problem using a very small training set. That approach is bottom Using Lexical Hierarchies Many approaches attempt to automatically assign semantic roles (such as case roles) by computing semantic similarity measures across a large lexical hierarchy; primarily using WordNet (Fellbaum, 1998). Budanitsky & Hirst (2001) provide a comparative analysis of such algorithms. However, it is uncommon to simply use the hierarchy directly for generalization purposes. Many researchers have noted that WordNet’s words are classified into senses that are too fine-grained for standard NLP tasks. For example, Buitelaar (1997) notes that the noun book is assigned to seven different senses, including fact and section, subdivision. Thus most users of WordNet must contend with the sense disambiguation issue in order to use the lexicon. The most closely related use of a lexical hierarchy that we know of is that of Li & Abe (1998), which uses an information-theoretic measure to make a cut through the top levels of the noun portion of WordNet. This is then used to determine acceptable classes for verb argument structure, and for the prepositional phrase attachment problem and is found to perform as well as or better than"
P02-1032,J98-2002,0,0.0216806,"Hirst (2001) provide a comparative analysis of such algorithms. However, it is uncommon to simply use the hierarchy directly for generalization purposes. Many researchers have noted that WordNet’s words are classified into senses that are too fine-grained for standard NLP tasks. For example, Buitelaar (1997) notes that the noun book is assigned to seven different senses, including fact and section, subdivision. Thus most users of WordNet must contend with the sense disambiguation issue in order to use the lexicon. The most closely related use of a lexical hierarchy that we know of is that of Li & Abe (1998), which uses an information-theoretic measure to make a cut through the top levels of the noun portion of WordNet. This is then used to determine acceptable classes for verb argument structure, and for the prepositional phrase attachment problem and is found to perform as well as or better than existing algorithms. Additionally, Boggess et al. (1991) “tag” veterinary text using a small set of semantic labels, assigned in much the same way a parser works, and describe this in the context of prepositional phrase attachment. 7 Conclusions and Future Work We have provided evidence that the upper l"
P02-1032,J93-2005,0,0.0233784,"Missing"
P02-1032,W01-0511,1,0.828939,"Missing"
P02-1032,C94-2125,0,0.619507,"Missing"
P02-1032,J93-1005,0,\N,Missing
P02-1032,P95-1007,0,\N,Missing
P02-1032,P98-2127,0,\N,Missing
P02-1032,C98-2122,0,\N,Missing
P02-1032,C98-1015,0,\N,Missing
P02-1032,W93-0307,1,\N,Missing
P04-1055,J95-4004,0,0.0134969,"g with hand-derived syntactic and semantic constraints, and recognizes a wide range of relationships between biological molecules. vous System Diseases). When there are multiple MeSH terms for one word, we simply choose the first one. These semantic features are shown to be very useful for our tasks (see Section 4.3). Rosario et al. (2002) demonstrate the usefulness of MeSH for the classification of the semantic relationships between nouns in noun compounds. The results reported in this paper were obtained with the following features: the word itself, its part of speech from the Brill tagger (Brill, 1995), the phrase constituent the word belongs to, obtained by flattening the output of a parser (Collins, 1996), and the word’s MeSH ID (if available). In addition, we identified the sub-hierarchies of MeSH that tend to correspond to treatments and diseases, and convert these into a tri-valued attribute indicating one of: disease, treatment or neither. Finally, we included orthographic features such as ‘is the word a number’, ‘only part of the word is a number’, ‘first letter is capitalized’, ‘all letters are capitalized’. In Section 4.3 we analyze the impact of these features. 3 4 Data and Featur"
P04-1055,P96-1025,0,0.00653868,"een biological molecules. vous System Diseases). When there are multiple MeSH terms for one word, we simply choose the first one. These semantic features are shown to be very useful for our tasks (see Section 4.3). Rosario et al. (2002) demonstrate the usefulness of MeSH for the classification of the semantic relationships between nouns in noun compounds. The results reported in this paper were obtained with the following features: the word itself, its part of speech from the Brill tagger (Brill, 1995), the phrase constituent the word belongs to, obtained by flattening the output of a parser (Collins, 1996), and the word’s MeSH ID (if available). In addition, we identified the sub-hierarchies of MeSH that tend to correspond to treatments and diseases, and convert these into a tri-valued attribute indicating one of: disease, treatment or neither. Finally, we included orthographic features such as ‘is the word a number’, ‘only part of the word is a number’, ‘first letter is capitalized’, ‘all letters are capitalized’. In Section 4.3 we analyze the impact of these features. 3 4 Data and Features For our experiments, the text was obtained from MEDLINE 20012 . An annotator with biology expertise cons"
P04-1055,P02-1032,1,0.430205,"h hand-built rules. Feldman et al. (2002) use hand-built rules that make use of syntactic and lexical features and semantic constraints to find relations between genes, proteins, drugs and diseases. The GENIES system (Friedman et al., 2001) uses a hand-built semantic grammar along with hand-derived syntactic and semantic constraints, and recognizes a wide range of relationships between biological molecules. vous System Diseases). When there are multiple MeSH terms for one word, we simply choose the first one. These semantic features are shown to be very useful for our tasks (see Section 4.3). Rosario et al. (2002) demonstrate the usefulness of MeSH for the classification of the semantic relationships between nouns in noun compounds. The results reported in this paper were obtained with the following features: the word itself, its part of speech from the Brill tagger (Brill, 1995), the phrase constituent the word belongs to, obtained by flattening the output of a parser (Collins, 1996), and the word’s MeSH ID (if available). In addition, we identified the sub-hierarchies of MeSH that tend to correspond to treatments and diseases, and convert these into a tri-valued attribute indicating one of: disease,"
P04-1055,W02-1010,0,0.00639472,"t relations, but instead are usually used to identify examples of one relation. In the related work for statistical models there has been, to the best of our knowledge, no attempt to distinguish between different relations that can occur between the same semantic entities. In Agichtein and Gravano (2000) the goal is to extract pairs such as (Microsoft, Redmond), where Redmond is the location of the organization Microsoft. Their technique generates and evaluates lexical patterns that are indicative of the relation. Only the relation location of is tackled and the entities are assumed given. In Zelenko et al. (2002), the task is to extract the relationships person-affiliation and organization-location. The classification (done with Support Vector Machine and Voted Perceptron algorithms) is between positive and negative sentences, where the positive sentences contain the two entities. In the bioscience NLP literature there are also efforts to extract entities and relations. In Ray and Craven (2001), Hidden Markov Models are applied to MEDLINE text to extract the entities PROTEINS and LOCATIONS in the relationship subcellular-location and the entities GENE and DISORDER in the relationship disorderassociati"
P04-1055,A00-1026,0,\N,Missing
P05-3017,X96-1039,0,0.048854,"’ {NO ORDER, ALLOW GAPS} [layer=’shallow_parse’ && tag_type=’NP’ [layer=’chemicals’] AS chem $ ] [layer=’shallow_parse’ && tag_type=’NP’ [layer=’MeSH’ && label BELOW ""C""] AS dis $ ] ] AS sent SELECT chem.content,dis.content,sent.content This looks for sentences containing two NPs in any order without overlaps (NO ORDER) and separated by any number of intervening elements. We further require one of the NPs to end (ensured by the $ symbol) with a chemical, and the other (the disease) to end with a MeSH term from the C subtree. 4 System Architecture Our basic model is similar to that of TIPSTER (Grishman, 1996): each annotation is stored as a record, which specifies the character-level beginning and ending positions, the layer and the type. The basic table9 contains the following columns: (1) annotation id; (2) doc id; (3) section: title, abstract or body; (4) layer id: layer identifier (word, POS, shallow parse, sentence, etc.); (5) start char pos: beginning character position, relative to section and doc id; (6) end char pos: ending character position; (7) tag type: a layer-specific token identifier. After evaluating various different extensions of the structure above, we have arrived at one with"
P05-3017,U04-1019,0,0.0772853,"oolean expressions over nodes and regular expressions inside nodes. Matching uses a binary index and is performed recursively starting at the top node in the query. TIGERSearch2 is associated with the German syntactic corpus TIGER. The tool is more typed than TGrep2 and allows search over discontinuous constituents that are common in German. TIGERSearch stores the corpus in a Prolog-like logical form and searches using unification matching. LPath is an extension of XPath with three features: immediate precedence, subtree scoping and edge alignment. The queries are executed in an SQL database (Lai and Bird, 2004). Other tree query languages include CorpusSearch, Gsearch, Linguist’s Search Engine, Netgraph, TIQL, VIQTORYA etc. Some tools go beyond the tree model and allow multiple intersecting hierarchies. Emu (Cassidy and Harrington, 2001) supports sequential levels of annotations over speech datasets. Hierarchical relations may exist between tokens in different levels, but precedence is defined only between elements within the same level. The queries cannot 1 2 http://tedlab.mit.edu/∼dr/Tgrep2/ http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/ express immediate precedence and are executed u"
P05-3017,ma-etal-2002-models,0,\N,Missing
P05-3017,bird-etal-2000-towards,0,\N,Missing
P05-3017,P02-1032,1,\N,Missing
P08-1052,J94-4005,0,0.0190161,"ama et al. (2002), who use named entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 2.4 Freq. 2205 1923 771 382 189 189 169 148 106 81 78 77 66 66 58 48 47 45 ... 4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characterize the semantic relation between them by leveraging the vast size of the Web to build linguistically-motivated lexically-specific features. We mine the Web for sentences containing the target nouns, and we extract the connecting verbs, prepositions, and coordinating conjunctions, which we use in a vector-space model to measure relational similarity. The"
P08-1052,P98-1015,0,0.119469,"Missing"
P08-1052,J06-1003,0,0.00331044,"translation, word sense disambiguation, and information extraction. For example, a relational search engine like TextRunner, which serves queries like “find all X such that X causes wrinkles”, asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT. While there are not many success stories so far, measuring semantic similarity has proven its advantages for textual entailment (Tatu and Moldovan, 2005). Introduction Despite the tremendous amount of work on word similarity (see (Budanitsky and Hirst, 2006) for an overview), there is surprisingly little research on the important related problem of relational similarity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who ∗ After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach o"
P08-1052,S07-1003,1,0.387472,"Missing"
P08-1052,C94-2119,0,0.093553,"amed entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 2.4 Freq. 2205 1923 771 382 189 189 169 148 106 81 78 77 66 66 58 48 47 45 ... 4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characterize the semantic relation between them by leveraging the vast size of the Web to build linguistically-motivated lexically-specific features. We mine the Web for sentences containing the target nouns, and we extract the connecting verbs, prepositions, and coordinating conjunctions, which we use in a vector-space model to measure relational similarity. The process of extraction starts wi"
P08-1052,P06-2064,0,0.45477,"Missing"
P08-1052,J07-2002,0,0.0179384,"Missing"
P08-1052,W01-0511,1,0.855213,"Missing"
P08-1052,P02-1032,1,0.865229,"Missing"
P08-1052,H05-1047,0,0.00788142,"l similarity problems, including but not limited to question answering, information retrieval, machine translation, word sense disambiguation, and information extraction. For example, a relational search engine like TextRunner, which serves queries like “find all X such that X causes wrinkles”, asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT. While there are not many success stories so far, measuring semantic similarity has proven its advantages for textual entailment (Tatu and Moldovan, 2005). Introduction Despite the tremendous amount of work on word similarity (see (Budanitsky and Hirst, 2006) for an overview), there is surprisingly little research on the important related problem of relational similarity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who ∗ After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achie"
P08-1052,N03-1033,0,0.0203025,"n the query changes the set of returned results and their ranking. For each query, we collect the text snippets from the result set (up to 1,000 per query). We split them into sentences, and we filter out all incomplete ones and those that do not contain the target nouns. We further make sure that the word sequence following the second mentioned target noun is nonempty and contains at least one nonnoun, thus ensuring the snippet includes the entire noun phrase: snippets representing incomplete sentences often end with a period anyway. We then perform POS tagging using the Stanford POS tagger (Toutanova et al., 2003) 1 454 POS P V V V V V V V V V V P C V V V V V ... V Direction 2→1 1→2 1→2 2→1 2→1 1→2 1→2 1→2 2→1 1→2 1→2 2→1 1→2 1→2 1→2 2→1 1→2 2→1 ... 2→1 Table 1: The most frequent Web-derived features for committee member. Here V stands for verb (possibly +preposition and/or +particle), P for preposition and C for coordinating conjunction; 1 → 2 means committee precedes the feature and member follows it; 2 → 1 means member precedes the feature and committee follows it. and shallow parsing with the OpenNLP tools2 , and we extract the following types of features: Verb: We extract a verb if the subject NP"
P08-1052,P06-1040,0,0.101838,"rity (see (Budanitsky and Hirst, 2006) for an overview), there is surprisingly little research on the important related problem of relational similarity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who ∗ After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER. We further apply it to (1) characterizing noun-noun compounds using abstract linguistic predicates like CAUSE, USE, and FROM, and (2) classifying the relation between pairs of nominals in context. 452 Proceedings of ACL-08: HLT, pages 452–460, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2 2.1 Related Work Characterizing Semantic Relations Turney and Littman (2005) characterize the relationship between two words"
P08-1052,J06-3003,0,0.213011,"rity (see (Budanitsky and Hirst, 2006) for an overview), there is surprisingly little research on the important related problem of relational similarity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who ∗ After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER. We further apply it to (1) characterizing noun-noun compounds using abstract linguistic predicates like CAUSE, USE, and FROM, and (2) classifying the relation between pairs of nominals in context. 452 Proceedings of ACL-08: HLT, pages 452–460, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2 2.1 Related Work Characterizing Semantic Relations Turney and Littman (2005) characterize the relationship between two words"
P08-1052,C98-1015,0,\N,Missing
P14-2045,Y10-1086,0,0.121935,"Missing"
P14-2045,E03-1074,0,0.0605062,"Missing"
P14-2045,levy-andrew-2006-tregex,0,0.113292,"Missing"
P14-2045,de-marneffe-etal-2006-generating,0,0.0912054,"Missing"
P14-2045,D08-1027,0,0.0101717,"Missing"
P15-1120,W15-0610,0,0.0223765,"f the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be marked up manually by the instructor before the feedback is accurate enough to be reliably used for a given question; a recent study found on the order of 500800 items per question had to be marked up at minimum in order to obtain acceptable correlations with human scorers (Heilman and Madnani, 2015). This high initial cost makes the development of hundreds of practice questions for a given conceptual unit a daunting task for instructors. Recent research in Learning at Scale has produced some interesting approaches to improving “feedback at scale.” One approach (Brooks et al., 2014) uses a variation on hierarchical text clustering in tandem with a custom user interface that allows instructors to rapidly view clusters and determine which contain correct answers, incorrect answers, and partially correct answers. This greatly speeds up the markup time and allows instructors to assign explana"
P15-1120,W11-2838,0,0.0341834,"gs in the case of second language learning. This suggests a different approach for building a parser than what is the current standard. I am not claiming that this has not been suggested in the past; for instance Schwind (1988) designed a parser to explain errors to learners. However, because of the renewed interest in technology for teaching, this may be a pivotal time to reconsider how we develop parsing technology: perhaps we should think fundamentally about parsers as coaches rather than parsers as critics. This inversion can apply to other aspects of NLP technology as well. For instance, Dale and Kilgarriff (2011) have held a series of workshop to produce algorithms to identify errors introduced into texts by non-native writers in the warmly named “Helping Our Own” shared task (Dale et al., 2012). Using the technology developed for tasks like these, the challenge is to go beyond recognizing and correcting the errors to helping the writer understand why the choices they are making are not correct. Another option is to target practice questions tailored for learners based on errors in a fun manner (as described below). Of course, for decades, the field of Intelligent Tutoring Systems (ITS) (VanLehn, 2011"
P15-1120,W12-2006,0,0.015351,"past; for instance Schwind (1988) designed a parser to explain errors to learners. However, because of the renewed interest in technology for teaching, this may be a pivotal time to reconsider how we develop parsing technology: perhaps we should think fundamentally about parsers as coaches rather than parsers as critics. This inversion can apply to other aspects of NLP technology as well. For instance, Dale and Kilgarriff (2011) have held a series of workshop to produce algorithms to identify errors introduced into texts by non-native writers in the warmly named “Helping Our Own” shared task (Dale et al., 2012). Using the technology developed for tasks like these, the challenge is to go beyond recognizing and correcting the errors to helping the writer understand why the choices they are making are not correct. Another option is to target practice questions tailored for learners based on errors in a fun manner (as described below). Of course, for decades, the field of Intelligent Tutoring Systems (ITS) (VanLehn, 2011) has developed technology for this purpose, so what is new about what I am suggesting? First, we know as NLP researchers that language analysis requires specific technology beyond stand"
P15-1120,W14-1701,0,0.0237059,"Missing"
P15-1120,W15-0628,0,0.0172138,"feedback on short answer test questions. There has been significant work in this space (Kukich, 2000; Hirschman et al., 2000). The standard approach builds on the classic successful model of essay scoring which compares the student’s text to model essays using a similarity-based technique such as LSA (Landauer et al., 2000; Mohler and Mihalcea, 2009) or careful authoring of the answer (Leacock and Chodorow, 2003). Recent techniques pair with learning techniques like Inductive Logic Programming with instructor editing to induce logic rules that describe permissible answers with high accuracy (Willis, 2015). Unfortunately most approaches require quite a large number of students’ answers to be marked up manually by the instructor before the feedback is accurate enough to be reliably used for a given question; a recent study found on the order of 500800 items per question had to be marked up at minimum in order to obtain acceptable correlations with human scorers (Heilman and Madnani, 2015). This high initial cost makes the development of hundreds of practice questions for a given conceptual unit a daunting task for instructors. Recent research in Learning at Scale has produced some interesting ap"
P15-1120,W14-1812,0,0.0551689,"Missing"
P15-1120,C88-2127,0,0.484631,"rnational Joint Conference on Natural Language Processing, pages 1245–1252, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics can teach principles and lessons for the conceptual stage that the learner is currently at. Different grammars could be developed for learners at different competency levels, as well as for different first-second language pairings in the case of second language learning. This suggests a different approach for building a parser than what is the current standard. I am not claiming that this has not been suggested in the past; for instance Schwind (1988) designed a parser to explain errors to learners. However, because of the renewed interest in technology for teaching, this may be a pivotal time to reconsider how we develop parsing technology: perhaps we should think fundamentally about parsers as coaches rather than parsers as critics. This inversion can apply to other aspects of NLP technology as well. For instance, Dale and Kilgarriff (2011) have held a series of workshop to produce algorithms to identify errors introduced into texts by non-native writers in the warmly named “Helping Our Own” shared task (Dale et al., 2012). Using the tec"
P15-1120,E09-1065,0,\N,Missing
P15-1120,W13-3601,0,\N,Missing
P94-1002,P93-1022,0,0.00674291,"Missing"
P94-1002,P92-1032,0,0.0189912,"to not only identify the extents of the subtopical units, but to label their contents as well. This paper focusses only on the discovery of subtopic structure, leaving determination of subtopic content to future work. Most discourse segmentation work is done at a finer granularity than that suggested here. However, for lengthy written expository texts, multi-paragraph segmentation has many potential uses, including the improvement of computational tasks that make use of distributional information. For example, disambiguation algorithms that train on arbitrary-size text windows, e.g., Yarowsky (1992) and Gale et ai. (1992b), and algorithms that use lexical co-occurrence to determine semantic relatedness, e.g., Schfitze (1993), might benefit from using windows with motivated boundaries instead. Information retrieval algorithms can use subtopic structuring to return meaningful portions of a text if paragraphs are too short and sections are too long (or are not present). Motivated segments can also be used as a more meaningful unit for indexing long texts. Salton et al. (1993), working with encyclopedia text, find that comparing a query against sections and then paragraphs is more successful"
P94-1002,J86-3001,0,0.444579,"Missing"
P94-1002,C92-2070,0,0.0798933,"Missing"
P94-1002,P93-1041,0,0.879677,"Missing"
P94-1002,P93-1020,0,0.381621,"Missing"
P94-1002,J91-1002,0,\N,Missing
P94-1002,J93-3003,0,\N,Missing
P94-1002,C92-1054,0,\N,Missing
P99-1001,W98-0719,0,0.0136681,"ine headache; a hypothesis which did not exist in the literature at the time Swanson found these links. The hypothesis has to be tested via non-textual means, b u t the important point is that a new, potentially plausible medical hypothesis was derived from a combination of text fragments and the explorer&apos;s medical expertise. (According to Swanson (1991), subsequent s t u d y found support for the magnesiummigraine hypothesis ( R a m a d a n et al., 1989).) This approach has been only partially automated. There is, of course, a potential for combinatorial explosion of potentially valid links. Beeferman (1998) has developed a flexible interface and analysis tool for exploring certain kinds of chains of links among lexical relations within WordNet. 2 However, sophisticated new algorithms are needed for helping in the pruning process, since a good pruning algorithm will want to take into account various kinds of semantic constraints. This m a y be an interesting area of investigation for computational linguists. Further narrowing its focus, the study set aside patents given to schools and governments and zeroed in on those awarded to industry. For 2,841 patents issued in 1993 and 1994, it examined th"
P99-1001,P93-1032,0,\N,Missing
S07-1080,P06-2064,0,0.0116933,"7. 2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publication (Nakov and Hearst, 2006), we make the claim that the relation between the nouns in a noun-noun compound can be characterized by the set of intervening verbs extracted from the Web. 3 Method Given an entity-annotated example sentence, we reduce the target entities e1 and e2 to single nouns noun1 and noun2 , by keeping their last nouns only, which we assume to be the heads. We then mine the Web for sentences containing both noun1 and noun2 , from which"
S07-1080,P02-1032,1,0.836987,"Missing"
S07-1080,P06-1040,0,0.0385558,"cattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, c Prague, June 2007. 2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publication (Nakov and Hearst, 2006), we make the claim that the relation between the nouns in a noun-noun compound can be characterized by the set of intervening verbs extracted from the Web. 3 Method Given an entity-annotated example sentence, we reduce the target entities e1 and e2 to single nouns nou"
S07-1080,P98-1015,0,\N,Missing
S07-1080,C98-1015,0,\N,Missing
W01-0511,C94-2195,0,0.00904418,"into statistical processing, primarily for the problem of prepositional phrase (PP) attachment. The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with. Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb. It is not clear how to use the results of such analysis after they are found; the semantics of the rela1 Nominalizations are compounds whose head noun is a nominalized verb and whose modiﬁer is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation. tionship between the terms must still be determined. In our framework we would cast this problem as ﬁnding the relationship R(p, n2) that best characterizes the preposition and the NP that follows it, an"
W01-0511,J93-1005,0,0.440811,"issues surrounding previously unseen proper nouns, which are often important for information extraction tasks. There have been several eﬀorts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (PP) attachment. The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with. Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb. It is not clear how to use the results of such analysis after they are found; the semantics of the rela1 Nominalizations are compounds whose head noun is a nominalized verb and whose modiﬁer is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation. tionship between the terms"
W01-0511,J98-2002,0,0.018677,"ing, primarily for the problem of prepositional phrase (PP) attachment. The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with. Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb. It is not clear how to use the results of such analysis after they are found; the semantics of the rela1 Nominalizations are compounds whose head noun is a nominalized verb and whose modiﬁer is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation. tionship between the terms must still be determined. In our framework we would cast this problem as ﬁnding the relationship R(p, n2) that best characterizes the preposition and the NP that follows it, and then seeing if th"
W01-0511,J93-2005,0,0.0181102,"for automatically assigning semantic relations to noun compounds, and report the results of experiments using this method. Section 6 concludes the paper and discusses future work. 2 Related Work Several approaches have been proposed for empirical noun compound interpretation. Lauer and Dras (1994) point out that there are three components to the problem: identiﬁcation of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of ﬁnding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on hand-written rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates"
W01-0511,W93-0307,1,0.717001,"rate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (PP) attachment. The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with. Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb. It is not clear how to use the results of such analysis after they are found; the semantics of the rela1 Nominalizations are compounds whose head noun is a nominalized verb and whose modiﬁer is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation. tionship between the terms must still be determined. In our framework we would cast this problem as ﬁnding the relationship R(p, n2) that best characterizes the preposition and th"
W01-0511,W95-0105,0,0.0542896,"Missing"
W01-0511,C94-2125,0,0.602994,"the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of ﬁnding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on hand-written rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindﬂesch et al. (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text. Lapata (2000) classiﬁes nominalizations according to whether the modiﬁer is the subject or the object of the underlying verb expressed by the head noun.1 In the related sub-area of information extraction (Cardie, 1997; Riloﬀ, 1996), the main goal is to ﬁnd every instance of part"
W05-0101,W04-3205,0,0.0176227,"only the top-scoring results would be named. Furthermore, innovative approaches that perhaps did not do as well as some others were also highlighted. Students were required to try at least 2 different types of features and 3 different classifiers. This assignment was quite successful, as the stu3 Available at http://www.sims.berkeley.edu/courses/is2902/f04/assignments/assignment3.html 4 Available at http://www.sims.berkeley.edu/courses/is2902/f04/lectures/lecture11.ppt choose a verb and then analyze verb-argument structure (they were provided with two relevant papers (Church and Hanks, 1990; Chklovski and Pantel, 2004)). As mentioned above, most of the MIMS students were not familiar with regular expressions, so I should have done a longer unit on this topic, at the expense of boring the CS students. The students learned a great deal from working to improve the grammar rules, but the verb-argument analysis portion was not particularly successful, in part because the corpus analyzed was too small to yield many sentences for a given verb and because we did not have code to automatically find regularities about the semantics of the arguments of the verbs. Other causes of difficulty were the students’ lack of l"
W05-0101,J90-1003,0,0.0424314,"s was kept very low-key; only the top-scoring results would be named. Furthermore, innovative approaches that perhaps did not do as well as some others were also highlighted. Students were required to try at least 2 different types of features and 3 different classifiers. This assignment was quite successful, as the stu3 Available at http://www.sims.berkeley.edu/courses/is2902/f04/assignments/assignment3.html 4 Available at http://www.sims.berkeley.edu/courses/is2902/f04/lectures/lecture11.ppt choose a verb and then analyze verb-argument structure (they were provided with two relevant papers (Church and Hanks, 1990; Chklovski and Pantel, 2004)). As mentioned above, most of the MIMS students were not familiar with regular expressions, so I should have done a longer unit on this topic, at the expense of boring the CS students. The students learned a great deal from working to improve the grammar rules, but the verb-argument analysis portion was not particularly successful, in part because the corpus analyzed was too small to yield many sentences for a given verb and because we did not have code to automatically find regularities about the semantics of the arguments of the verbs. Other causes of difficulty"
W05-0101,W02-0109,0,0.0877919,"guage(s) to work with. Scripting tools such as python are fast and easy to prototype with, but require the students to learn a new programming language. Java is attractive because many 2 tools are written in it and the MIMS students were familiar with java – they are required to use it for two of their required courses but still tend to struggle with it. I did not consider perl since python is a more principled language and is growing in acceptance and in tool availability. In the end I decided to require the students to learn python because I wanted to use NLTK, the Natural Language Toolkit (Loper and Bird, 2002). One goal of NLTK is to remove the emphasis on programming to enable students to achieve results quickly; and this aligned with my primary goal. NLTK seemed promising because it contained some well-written tutorials on n-grams, POS tagging and chunking, and contained text categorization modules. (I also wanted support for entity extraction, which NLTK does not supply.) NLTK is written in python, and so I decided to try it and have the students learn a new programming language. As will be described in detail below, our use of NLTK was somewhat successful, but we experienced numerous problems a"
W05-0101,J97-2002,1,0.717084,"the command-line interface, and not taking advantage of its rich functionality. As with NLTK, the documentation for Weka was incomplete and out of date, and it was difficult to determine how to use the more advanced features. We performed extended experimentation with the system and developed a detailed tutorial on how to use the system; this tutorial should be of general use.4 For the categorization task, we used the “twenty newsgroups” collection that was supplied with NLTK. Unfortunately, it was not preprocessed into sentences, so I also had to write some sentence splitting code (based on Palmer and Hearst (1997)) so students could make use of their tokenizer and tagger code. We selected one pair of newsgroups which contained very different content (rec.motorcycles vs. sci.space). We called this the diverse set. We then created two groups of newsgroups with more homogeneous content (a) rec.autos, rec.motorcycles, rec.sport.baseball, rec.sport.hockey, and (b) sci.crypt, sci.electronics, sci.med.original, sci.space. The intention was to show the students that it is easier to automatically distinguish the heterogeneous groups than the homogeneous ones. We set up the code to allow students to adjust the s"
W05-0101,N04-4030,1,0.795964,"e successful than those who developed their own. Those who tried other topics were often too ambitious and had trouble getting meaningful results. However, several of those students were trying ideas that they planned to apply to their capstone projects, and so it was highly valuable for them to get a preview of what worked and what did not. One suggestion I made was to create a back-ofthe-book indexer, specifically for a recipe book, and one team did a good job with this project. Another was to improve on or apply an automatic hierarchy generation tool that we have developed in our research (Stoica and Hearst, 2004). Students working on a project to collect metadata for camera phone images successfully applied this tool to this problem. Again, social networking analysis topics were popular but not particularly successful; NLP tools are not advanced enough yet to meet the needs of this intriguing topic area. Not surprisingly, when students started with a new (interesting) text collec7 tion, they were bogged down in the preprocessing stage before they could get much interesting work done. 6.4 Reflecting on Assignments Although students were excited about the Enron collection and we created a resource that"
W05-0101,P06-4018,0,\N,Missing
W05-0603,J03-3005,0,0.249046,"are kept together in the expanded version. However, this NC is ambiguous, and can also be paraphrased as cells from the brain stem, implying a left bracketing. Some NCs’ meaning cannot be readily expressed with a prepositional paraphrase (Warren, 1978). An alternative is the copula paraphrase, as in office building that/which is a skyscraper (right bracketing), or a verbal paraphrase such as pain associated with arthritis migraine (left). Other researchers have used prepositional paraphrases as a proxy for determining the semantic relations that hold between nouns in a compound (Lauer, 1995; Keller and Lapata, 2003; Girju et al., 2005). Since most NCs have a prepositional paraphrase, Lauer builds a model trying to choose between the most likely candidate prepositions: of, for, in, at, on, from, with and about (excluding like which is mentioned by Warren). This could be problematic 21 though, since as a study by Downing (1977) shows, when no context is provided, people often come up with incompatible interpretations. In contrast, we use paraphrases in order to make syntactic bracketing assignments. Instead of trying to manually decide the correct paraphrases, we can issue queries using paraphrase pattern"
W05-0603,N04-1016,0,0.584491,"es that are used in science fiction. 4 4.1 Evaluation Lauer’s Dataset We experimented with the dataset from (Lauer, 1995), in order to produce results comparable to those of Lauer and Keller & Lapata. The set consists of 244 unambiguous 3-noun NCs extracted from Grolier’s encyclopedia; however, only 216 of these NCs are unique. Lauer (1995) derived n-gram frequencies from the Grolier’s corpus and tested the dependency and the adjacency models using this text. To help combat data sparseness issues he also incorporated a taxonomy and some additional information (see Related Work section above). Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer’s using simple lexical models. 4.2 Biomedical Dataset We constructed a new set of noun compounds from the biomedical literature. Using the Open NLP 6 In addition to the articles (a, an, the), we also used quantifiers (e.g. some, every) and pronouns (e.g. this, his). tools,7 we sentence splitted, tokenized, POS tagged and shallow parsed a set of 1.4 million MEDLINE abstracts (citations between 1994 and 2003). Then we extracted all 3-noun sequences falling in the last three positions of noun phrases (NPs) found in the sha"
W05-0603,J93-2005,0,0.351437,"Missing"
W07-0730,W06-3123,0,0.0196787,"Table 2: Summary of our submissions. All runs are for the News Commentary test data. The official submissions are marked with a star. applied rule (8), since its Spanish/French equivalent que (as well as the German daß) is always obligatory. These transformations affected 927 out of the 2007 test sentences. We also used this transformed data set when translating to German (however, German uses NCs as much as English does). 3.4 Other Non-standard Settings Below we discuss some non-standard settings that differ from the ones suggested by the organizers in their baseline system. First, following Birch et al. (2006), who found that higher-order LMs give better results2 , we used a 5-gram LM for News Commentary, and 7-gram LM for Europarl (as opposed to 3-gram, as done normally). Second, for all runs we trained our systems on all sentences of length up to 100 (rather than 40, as suggested in the baseline system). Third, we used a maximum phrase length limit of 10 (rather than 7, as typically done). Fourth, we used both a lexicalized and distance-based reordering models (as opposed to lexicalized only, as in the baseline system). Finally, while we did not use any resources other than the ones provided by t"
W07-0730,P03-1054,0,0.00530773,", but syntactically different from the phrases learned on training, and the potential for a high-quality translation is missed. We address this problem by using nearly equivalent syntactic paraphrases of the original sentences. Each paraphrased sentence is paired with the foreign translation that is associated with the original sentence in the training data. This augmented training corpus can then be used to train an SMT system. Alternatively, we can paraphrase the test sentences making them closer to the target language syntax. Given an English sentence, we parse it with the Stanford parser (Klein and Manning, 2003) and then generate paraphrases using the following syntactic transformations: 1. [NP NP1 P NP2 ] ⇒ [NP NP2 NP1 ]. inequality in income ⇒ income inequality. 2. [NP NP1 of NP2 ] ⇒ [NP NP2 poss NP1 ]. inequality of income ⇒ income’s inequality. 3. NPposs ⇒ NP. income’s inequality ⇒ income inequality. 4. NPposs ⇒ NPP Pof . income’s inequality ⇒ inequality of income. 5. NPN C ⇒ NPposs . income inequality ⇒ income’s inequality. 6. NPN C ⇒ NPP P . income inequality ⇒ inequality in incomes. 212 Proceedings of the Second Workshop on Statistical Machine Translation, pages 212–215, c Prague, June 2007. 2"
W07-0730,W06-3114,0,0.0700457,"California at Berkeley Berkeley, CA 94720 hearst@ischool.berkeley.edu 2 Introduction Modern Statistical Machine Translation (SMT) systems are trained on aligned sentences of bilingual corpora, typically from one domain. When tested on text from that same domain, such systems demonstrate state-of-the art performance; however, on out-of-domain text the results can get significantly worse. For example, on the WMT 2006 Shared Task evaluation, the French to English translation BLEU scores dropped from about 30 to about 20 for nearly all systems, when tested on News Commentary rather than Europarl (Koehn and Monz, 2006). Therefore, this year the shared task organizers have provided 1M words of bilingual News Commentary training data in addition to the Europarl data (about 30M words), thus challenging the participants to experiment with domain adaptation. Below we describe our domain adaptation experiments, trying to achieve better results on the News Monolingual Syntactic Paraphrasing In many cases, the testing text contains “phrases” that are equivalent, but syntactically different from the phrases learned on training, and the potential for a high-quality translation is missed. We address this problem by us"
W07-0730,N03-2016,0,0.0852024,"ems were trained on both corpora. • Language models. We used two language models (LM) – a small in-domain one (trained on News Commentary) and a big out-of-domain one (trained on Europarl). For example, for EN → ES (from English to Spanish), on the lowercased tuning data set, using in-domain LM only achieved a BLEU of 0.332910, while using both LMs yielded 0.354927, a significant effect. • Cognates. Previous research has found that using cognates can help get better word alignments (and ultimately better MT results), especially in case of a small training set. We used the method described in (Kondrak et al., 2003) in order to extract cognates from the two data sets. We then added them as sentence pairs to the News Commentary corpus before training the word alignment models1 for ucb3, ucb4 and ucb5. 1 Following (Kondrak et al., 2003), we considered words of length 4 or more, we required the length ratio to be between 7 and 10 , and we accepted as potential cognates all pairs for 10 7 which the longest common subsequence ratio (LCSR) was 0.58 or more. We repeated 3 times the cognate pairs extracted from the Europarl, and 4 times the ones from News Commentary. 214 • Phrases. The ucb5 system uses the Europ"
W07-0730,N04-1016,0,0.0242204,"ith internal possessive marker; NP that is a Noun Compound. While the first four and the last two transformations are purely syntactic, (5) and (6) are not. The algorithm must determine whether a possessive marker is feasible for (5) and must choose the correct preposition for (6). In either case, for noun compounds (NCs) of length 3 or more, it also needs to choose the position to modify, e.g., inquiry’s committee chairman vs. inquiry committee’s chairman. In order to ensure accuracy of the paraphrases, we use statistics gathered from the Web, using a variation of the approaches presented in Lapata and Keller (2004) and Nakov and Hearst (2005). We use patterns to generate possible prepositional or copula paraphrases in the context of the preceding and the following word in the sentence, First we split the NC into two parts N1 and N2 in all possible ways, e.g., beef import ban lifting would be split as: (a) 213 &quot;lt &quot;lt &quot;lt &quot;lt N1 N2 N2 N2 poss prep that that N2 rt&quot; det N10 rt&quot; be det N10 rt&quot; be prep det N10 rt&quot; where: lt is the word preceding N1 in the original sentence or empty if none, rt is the word following N2 in the original sentence or empty if none, poss is a possessive marker (’s or ’), that is t"
W07-0730,P95-1007,0,0.0772654,"Missing"
W07-0730,W05-0603,1,0.85274,"r; NP that is a Noun Compound. While the first four and the last two transformations are purely syntactic, (5) and (6) are not. The algorithm must determine whether a possessive marker is feasible for (5) and must choose the correct preposition for (6). In either case, for noun compounds (NCs) of length 3 or more, it also needs to choose the position to modify, e.g., inquiry’s committee chairman vs. inquiry committee’s chairman. In order to ensure accuracy of the paraphrases, we use statistics gathered from the Web, using a variation of the approaches presented in Lapata and Keller (2004) and Nakov and Hearst (2005). We use patterns to generate possible prepositional or copula paraphrases in the context of the preceding and the following word in the sentence, First we split the NC into two parts N1 and N2 in all possible ways, e.g., beef import ban lifting would be split as: (a) 213 &quot;lt &quot;lt &quot;lt &quot;lt N1 N2 N2 N2 poss prep that that N2 rt&quot; det N10 rt&quot; be det N10 rt&quot; be prep det N10 rt&quot; where: lt is the word preceding N1 in the original sentence or empty if none, rt is the word following N2 in the original sentence or empty if none, poss is a possessive marker (’s or ’), that is that, which or who, be is is"
W07-0730,P03-1021,0,0.0129077,"sing parameters were filled with 1e-40. The ucb5 system is also trained on Europarl, yielding a third lexicalized re-ordering model and adding 4 new parameters to the phrase table entries. Unfortunately, longer sentences (up to 100 tokens, rather than 40), longer phrases (up to 10 tokens, rather than 7), two LMs (rather than just one), higher-order LMs (order 7, rather than 3), multiple higher-order lexicalized re-ordering models (up to 3), etc. all contributed to increased system’s complexity, and, as a result, time limitations prevented us from performing minimum-error-rate training (MERT) (Och, 2003) for ucb3, ucb4 and ucb5. Therefore, we used the MERT parameter values from ucb1 instead, e.g. the first 4 phrase weights of ucb1 were divided by two, copied twice and used in ucb3 as the first 8 phrase-table parameters. The extra 4 parameters of ucb5 came from training a separate MT system on the Europarl data (scaled accordingly). 3.3 Paraphrasing the Test Set In some of our experiments (ucb2 and ucb4), given a test sentence, we generated the single most-likely paraphrase, which makes it syntactically closer to Spanish and French. Unlike English, which makes extensive use of noun compounds,"
W07-0730,2006.amta-papers.2,0,\N,Missing
W07-1010,W06-3310,0,0.058706,"Missing"
W09-3608,magnini-cavaglia-2000-integrating,0,0.0215124,"appears especially promising for de65 Figures 4 and 5 show the output of the Castanet algorithm when applied to the titles of journals from the bioscience literature. Note that even the highly ambiguous common anatomy words are successfully grouped using this algorithm, presumably because of the requirement that each word occur in only one location in the ontology and because the anatomy part of the ontology is strongly favored during the part of the process in which the core tree is built with unambiguous terms. (Although some versions of Castanet use an advanced version of WordNet Domains (Magnini, 2000), they were not used in the construction of this category set.) termining which words of long documents to include in building facet systems. 2.2 Castanet Applied to Journal Titles The main idea behind the Castanet algorithm is to carve out a structure from the hypernym (“isa”) relations within the WordNet (Fellbaum, 1998) lexical database (Stoica et al., 2007; Stoica and Hearst, 2004). The Castanet algorithm assumes that there is text associated with each item in the collection, or at least with a representative subset of the items. The textual descriptions are used both to build the facet hi"
W09-3608,N04-4030,1,0.919299,"ovary, and further refining by disease type. This ability to “mix and match” both for describing the articles and for navigating the category structure is key. to NLP, including: • How to automatically or semi-automatically create rich subject-oriented faceted metadata for scholarly text? • How to automatically assign information items to faceted category labels? This paper describes the results of applying Castanet, a semi-automated approach to creating faceted metadata, to a scholarly collection. (In past work it has been shown to work well on a different kind of text (Stoica et al., 2007; Stoica and Hearst, 2004).) It then discusses some open problems in building navigation structures for scholarly digital libraries. 2 Creating Faceted Metadata This section first defines faceted metadata, and then describes the CastaNet algorithm. More details about the algorithm can be found in a prior publication (Stoica et al., 2007). Rather than one large category hierarchy, faceted metadata consists of a set of categories (flat or hierarchical), each of which corresponds to a different facet (dimension or feature type) relevant to the collection to be navigated. After the facets are designed, each item in the col"
W09-3608,N07-1031,1,0.830171,"n, then navigating to ovary, and further refining by disease type. This ability to “mix and match” both for describing the articles and for navigating the category structure is key. to NLP, including: • How to automatically or semi-automatically create rich subject-oriented faceted metadata for scholarly text? • How to automatically assign information items to faceted category labels? This paper describes the results of applying Castanet, a semi-automated approach to creating faceted metadata, to a scholarly collection. (In past work it has been shown to work well on a different kind of text (Stoica et al., 2007; Stoica and Hearst, 2004).) It then discusses some open problems in building navigation structures for scholarly digital libraries. 2 Creating Faceted Metadata This section first defines faceted metadata, and then describes the CastaNet algorithm. More details about the algorithm can be found in a prior publication (Stoica et al., 2007). Rather than one large category hierarchy, faceted metadata consists of a set of categories (flat or hierarchical), each of which corresponds to a different facet (dimension or feature type) relevant to the collection to be navigated. After the facets are desi"
W16-0203,S12-1047,0,0.0766063,"Missing"
W16-0203,2015.lilt-12.3,0,0.0464006,"Missing"
W16-0203,W14-1618,0,0.0632342,"Missing"
W16-0203,N13-1090,0,0.0948168,"Missing"
W16-0203,C08-1119,0,0.0464512,"Missing"
W16-0526,W15-0622,0,0.140556,"difficulty level and prerequisites of a textbook to select sections that are both relevant and reflect the concepts that the reader has already encountered. 1 Introduction A learner who is studying material from an online course, such as a video from a Khan academy physics sequence, may desire additional reading material to supplement the current video or exercise. It can be distracting to do a web search to find relevant material, and furthermore, the material that is found may be described at the wrong level or may assume prerequisite knowledge that the learner does not have. To this point, Mathew et al. (2015) note that online encyclopedic resources, such as Wikipedia, are not pedagogically organized and tend to have many cyclic dependencies among articles. 2 Related Work There has been some related work in aligning textbook content to other content. Contractor et al. (2015) identify the need to automatically label instructional materials with learning standards, which are defined hierarchically from general goals down to lists of instructions that define the skills that students should learn during a course or within a curriculum. They develop an algorithm for representing the content within a lis"
W17-2701,W16-5703,0,0.0291407,"of articles from multiple sources across a decade from scratch, 2. A topic detection method that handles interruption in topics, 3. A novel way to name stories, and 4. A method for clustering, rating, and displaying quotations associated with the stories. 1 Proceedings of the Events and Stories in the News Workshop, pages 1–9, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics ence model named Storylines that clusters articles into storylines which are assigned to broader topics. Emphasis is put on scalability with a goal of processing one article per second. Poghosyan and Ifrim (2016) leverage keywords in social media to generate storylines, and Vossen et al. (2015) propose to use news timelines to build storylines, structured index of events in the timeline, and event relationships. Visualizing news stories focuses on building a user interface to present a given story to help the user digest a complex story. Using the EMM dataset, Krstaji´c et al. (2013) propose a visual analytics system to represent relationships between news stories and their evolution over time. Each story element is represented as a tile in a vertical list. Over time (x-axis), the placement of story e"
W17-2701,W08-1408,0,0.0381017,"techniques to automatically process a streamable dataset of text into related groups called topics. In the context of news, the topics detected and tracked are commonly called stories. Swan and Allan (2000) use the Topic Detection and Tracking (TDT) and TDT2 datasets, consisting of 50,000 news articles to produce 146 stories, called clusters. The clustering process is done using named entities and noun phrases, as opposed to unigrams. They report an inability to merge clusters if there are large gaps in time with no articles, and their algorithm does not group documents in an online fashion. Pouliquen et al. (2008) build a large dataset of news articles, named the Europe Media Monitor (EMM). Their topic detection creates local clusters in each language. The monolingual stories are then linked across languages to form global stories. A reported drawback is the clustering cannot handle merging and splitting between disparate topics and cannot mend gaps between stories that last more than 8 days. Ahmed et al. (2011) propose an online inferIntroduction Complex news events unfold over months, and the sequence of events over time can be thought of as forming stories. Our objective is to generate, from publicl"
W17-2701,W15-4507,0,0.0283285,"hod that handles interruption in topics, 3. A novel way to name stories, and 4. A method for clustering, rating, and displaying quotations associated with the stories. 1 Proceedings of the Events and Stories in the News Workshop, pages 1–9, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics ence model named Storylines that clusters articles into storylines which are assigned to broader topics. Emphasis is put on scalability with a goal of processing one article per second. Poghosyan and Ifrim (2016) leverage keywords in social media to generate storylines, and Vossen et al. (2015) propose to use news timelines to build storylines, structured index of events in the timeline, and event relationships. Visualizing news stories focuses on building a user interface to present a given story to help the user digest a complex story. Using the EMM dataset, Krstaji´c et al. (2013) propose a visual analytics system to represent relationships between news stories and their evolution over time. Each story element is represented as a tile in a vertical list. Over time (x-axis), the placement of story elements is adjusted on the vertical axis according to the level of activity in the"
W17-2701,D12-1072,0,0.0661731,"Missing"
W17-5034,D14-1159,0,0.0773446,"Missing"
W17-5034,H05-1103,0,0.0440803,"izable (Abacha et al., 2016). Prior work aimed at generating educational practice questions has generated questions directly from text using a series of manual translations and a ranking procedure to determine quality (Heilman and Smith, 2010, 2009; Heilman, 2011). Other work has focused on question generation, independent of an educational context. A large-scale question generation task posed to the community prompted a focus on factual question generation from texts and knowledge bases (Rus et al., 2008; Graesser et al., 2012). Approaches have included factual generation directly from text (Brown et al., 2005; Mannem et al., 2010; Mazidi and Tarau, 2016; Yao et al., 2012) as well as generation from knowledge bases (Olney et al., 2012a). Recent advances in text generation have used neural generative models to create interestingly worded questions (Serban et al., 2016; Indurthi et al., 2017). However, because we are using a human created ontology and lack specialized training data, we utilize hand-crafted rules for generation. 3 Figure 1: Selected part of the Biology ontology. for educational applications. While more resources could be used to accomplish this task, we only utilize the ontology to ex"
W17-5034,N10-1086,0,0.0731338,"However, these approaches mainly generate questions for a single fact and do not combine multiple pieces of information together to create more complex questions. There is the potential to explore other, more complex, types of question generation procedures from the ontology. Approaches have also utilized online questions for ontology-driven generation, but this is less generalizable (Abacha et al., 2016). Prior work aimed at generating educational practice questions has generated questions directly from text using a series of manual translations and a ranking procedure to determine quality (Heilman and Smith, 2010, 2009; Heilman, 2011). Other work has focused on question generation, independent of an educational context. A large-scale question generation task posed to the community prompted a focus on factual question generation from texts and knowledge bases (Rus et al., 2008; Graesser et al., 2012). Approaches have included factual generation directly from text (Brown et al., 2005; Mannem et al., 2010; Mazidi and Tarau, 2016; Yao et al., 2012) as well as generation from knowledge bases (Olney et al., 2012a). Recent advances in text generation have used neural generative models to create interestingly"
W17-5034,E17-1036,0,0.0144324,"ocused on question generation, independent of an educational context. A large-scale question generation task posed to the community prompted a focus on factual question generation from texts and knowledge bases (Rus et al., 2008; Graesser et al., 2012). Approaches have included factual generation directly from text (Brown et al., 2005; Mannem et al., 2010; Mazidi and Tarau, 2016; Yao et al., 2012) as well as generation from knowledge bases (Olney et al., 2012a). Recent advances in text generation have used neural generative models to create interestingly worded questions (Serban et al., 2016; Indurthi et al., 2017). However, because we are using a human created ontology and lack specialized training data, we utilize hand-crafted rules for generation. 3 Figure 1: Selected part of the Biology ontology. for educational applications. While more resources could be used to accomplish this task, we only utilize the ontology to explore the efficacy of this question generation approach. By utilizing an expert-curated ontology instead of an automatically generated one, we operate under the assumption that the ontology is correct and complete. Future work can explore utilizing this method in conjunction with other"
W17-5034,W16-6609,0,0.0303542,"aimed at generating educational practice questions has generated questions directly from text using a series of manual translations and a ranking procedure to determine quality (Heilman and Smith, 2010, 2009; Heilman, 2011). Other work has focused on question generation, independent of an educational context. A large-scale question generation task posed to the community prompted a focus on factual question generation from texts and knowledge bases (Rus et al., 2008; Graesser et al., 2012). Approaches have included factual generation directly from text (Brown et al., 2005; Mannem et al., 2010; Mazidi and Tarau, 2016; Yao et al., 2012) as well as generation from knowledge bases (Olney et al., 2012a). Recent advances in text generation have used neural generative models to create interestingly worded questions (Serban et al., 2016; Indurthi et al., 2017). However, because we are using a human created ontology and lack specialized training data, we utilize hand-crafted rules for generation. 3 Figure 1: Selected part of the Biology ontology. for educational applications. While more resources could be used to accomplish this task, we only utilize the ontology to explore the efficacy of this question generatio"
W17-5034,W11-1414,0,0.024431,"Although much research exists in automated question generation the techniques needed for educational applications require a level of precision that is not always present in these approaches. Ontologies have the potential to be uniquely beneficial for educational question generation because they allow concepts to be connected in nontraditional ways. Questions can be generated about different concepts’ properties which span 2 Related Work Prior work has explored both automatically generating educational ontologies from text and utilizing expert-created ontologies for other tasks. For instance, Olney et al. (2011) explored extracting 303 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 303–312 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics nodes and relationships from text to build a concept map ontology automatically from textbooks. Other work has also attempted to build ontologies from non-educational texts (Benafia et al., 2015; Szulman et al., 2010) and has explored utilizing crowd-sourcing to build an ontology from text (Getman and Karasiuk, 2014). Prior approaches to question generation from ontologies"
W17-5034,P16-1056,0,0.0565399,"Missing"
W19-3001,Q16-1033,0,0.122397,"ead of more recent embedding methods (Bojanowski et al., 2016; Conneau et al., 2017; Pennington et al., 2014; Peters et al., 2018; Subramanian et al., 2018), which we explore. Related Work Considerable potential for automating a counselor was shown with the initial rule-based Eliza system (Weizenbaum, 1966) and recent developments have sought to target systems for delivering cognitive behavioral therapy (Fitzpatrick et al., 2017). Other studies have looked at the effect of suicide prevention counselor training (Gould et al., 2013), identifying patterns of successful crisis hotline counselors (Althoff et al., 2016), automating counselor evaluation (P´erez-Rosas et al., 2017), and building a dashboard for crisis counselors (Dinakar et al., 2015). There is additional work to identify supportive and distressed behaviors and language in online forums (Balani and De Choudhury, 2015; De Choudhury and De, 2014; Wang and Jurgens, 2018) and support forum moderators (Hussain et al., 2015). Most similar to our study, was one study that showed the potential for an avatar system to help train medical doctors to deliver news to patients (Andrade et al., 2010). However, this study did not target counselors or train co"
W19-3001,W18-2501,0,0.0189146,"arg maxj sim(mi , mj ) where j indexes over the messages in the training set. Similarity is commonly calculated as cosine similarity between TF-IDF vector representations of the input (i.e., counselor) message mi and messages in the training set. We compare the TFIDF representation with additional vector representations of the counselor input. Exhaustive comparison of embedding methods is not feasible, so we chose popular, successful, and diverse embeddings: GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2016), Attract-Repel (Vuli´c et al., 2017), and ELMo (Peters et al., 2018; Gardner et al., 2018). We also consider two sentence embeddings: InferSent (Conneau et al., 2017) and GenSen (Subramanian et al., 2018). Messages are embedded by summing the embeddings of their elements, e.g., across words or sentences for appropriate embeddings. For the second retrieval approach, we select the response from the training data that is most probable, i.e, j 0 = arg maxj P (rj |mi ) where mi is again the input message and j indexes over training examples. With this approach, which we will refer to as S2S-retrieve, the probability of a response is calculated by a Seq2Seq model trained on counselor-vis"
W19-3001,P12-3007,0,0.0304232,"before interacting with individuals in need. Well, my parents have been fighting a lot for the past few months and I got a C ri on a test today Figure 1: A conversation snippet showing a visitor’s response ri to a counselor’s message mi with preceding context, i.e., a visitor’s message ci . dialogue. Some studies have looked at defining or learning scoring functions over IDF weights to construct retrieval scores (Krause et al., 2017; Ritter et al., 2011). Most similar to our work is a system that considered similarities of full histories of dialogues in addition to a previous turn of context (Banchs and Li, 2012) and another study that hand-tuned weights in a scoring function on IDF weights to include additional messages of context (Sordoni et al., 2015). However, these works used similarities calculated over TF-IDF (Baeza-Yates et al., 2011) and bag-of-words of representations, instead of more recent embedding methods (Bojanowski et al., 2016; Conneau et al., 2017; Pennington et al., 2014; Peters et al., 2018; Subramanian et al., 2018), which we explore. Related Work Considerable potential for automating a counselor was shown with the initial rule-based Eliza system (Weizenbaum, 1966) and recent deve"
W19-3001,D17-1070,0,0.0414999,"functions over IDF weights to construct retrieval scores (Krause et al., 2017; Ritter et al., 2011). Most similar to our work is a system that considered similarities of full histories of dialogues in addition to a previous turn of context (Banchs and Li, 2012) and another study that hand-tuned weights in a scoring function on IDF weights to include additional messages of context (Sordoni et al., 2015). However, these works used similarities calculated over TF-IDF (Baeza-Yates et al., 2011) and bag-of-words of representations, instead of more recent embedding methods (Bojanowski et al., 2016; Conneau et al., 2017; Pennington et al., 2014; Peters et al., 2018; Subramanian et al., 2018), which we explore. Related Work Considerable potential for automating a counselor was shown with the initial rule-based Eliza system (Weizenbaum, 1966) and recent developments have sought to target systems for delivering cognitive behavioral therapy (Fitzpatrick et al., 2017). Other studies have looked at the effect of suicide prevention counselor training (Gould et al., 2013), identifying patterns of successful crisis hotline counselors (Althoff et al., 2016), automating counselor evaluation (P´erez-Rosas et al., 2017),"
W19-3001,P17-4012,0,0.0178921,"(Subramanian et al., 2018). Messages are embedded by summing the embeddings of their elements, e.g., across words or sentences for appropriate embeddings. For the second retrieval approach, we select the response from the training data that is most probable, i.e, j 0 = arg maxj P (rj |mi ) where mi is again the input message and j indexes over training examples. With this approach, which we will refer to as S2S-retrieve, the probability of a response is calculated by a Seq2Seq model trained on counselor-visitor message-response pairs. All Seq2Seq models were trained in the OpenNMT framework (Klein et al., 2017). 4.2.1 4.3 Response Generation For generating a response to a single counselor message, we consider a Seq2Seq model (Sutskever et al., 2014). When considering an additional message of context, we first use the Seq2Seq model with the preceding messages concatenated into a single input. Second, we use a Variational Hierarchical Conversation RNN (VHCR) that explicitly models prior conversation state with a hierarchical structure of latent variables (Park et al., 2018). This model has been shown to improve on other models that adjust for context when there is more than one preceding utterance (Pa"
W19-3001,N18-1169,0,0.0329454,"Missing"
W19-3001,D16-1230,0,0.0604988,"Missing"
W19-3001,N15-1020,0,0.0881207,"Missing"
W19-3001,P17-1103,0,0.018488,"e judges were crowdworkers on Amazon Mechanical Turk1 who had been granted Masters status and were located in the United States. Crowdworkers were presented with instructions, labeled examples, and batches of 10 cases where they were asked to judge responses to messages. To evaluate methods for the first research question, crowdworkers were given a single message and a response and asked to judge the response. For the second research question, crowdworkers were given two messages of context and a highlighted response and asked to judge the response. In contrast to studies that rank on scales (Lowe et al., 2017), we directly asked the workers to decide if a response made sense or not. In addition to indicating that a response did or did not make sense, we allowed a third class for workers to indicate if they were unsure without additional context. We found these classes to be sufficiently descriptive to consistently label messages between researchers. In preliminary trials with crowdworkers, there was insufficient agreement on labels. This instability of labels could stem from a variety of causes, including uncertainty about whether a change of topic should be considered a coherent response. To surmo"
W19-3001,N18-1162,0,0.0129204,"ing a counselor input message and preceding conversation context. For responding to a single counselor input message, we consider two approaches: one based on cosine similarity of vector representations and the other based on likelihood. For responding to a counselor message when considering additional conversation context, we extend retrieval to consider additional messages of context, i.e., an additional message preceding the counselor’s last message. For generating responses, we consider a popular Seq2Seq model (Sutskever et al., 2014; Vinyals and Le, 2015) and a hierarchical neural model (Park et al., 2018). 4.1 Data Preprocessing Names were standardized to be popular American male or female baby names from the last 5 decades. Entire messages were tokenized with appropriate tokenizers for each embedding method and converted to lowercase, as appropriate. 3 4.2 Response Retrieval Considering a Single Message lated over the sum of the previous messages embeddings, i.e., considering contexts ci and cj that precede a test message mi and a training message mj respectively, we choose rj 0 such that j 0 = arg maxj sim(mi + ci , mj + cj ). As a second approach, we measure context similarity as the weight"
W19-3001,D14-1162,0,0.08158,"ghts to construct retrieval scores (Krause et al., 2017; Ritter et al., 2011). Most similar to our work is a system that considered similarities of full histories of dialogues in addition to a previous turn of context (Banchs and Li, 2012) and another study that hand-tuned weights in a scoring function on IDF weights to include additional messages of context (Sordoni et al., 2015). However, these works used similarities calculated over TF-IDF (Baeza-Yates et al., 2011) and bag-of-words of representations, instead of more recent embedding methods (Bojanowski et al., 2016; Conneau et al., 2017; Pennington et al., 2014; Peters et al., 2018; Subramanian et al., 2018), which we explore. Related Work Considerable potential for automating a counselor was shown with the initial rule-based Eliza system (Weizenbaum, 1966) and recent developments have sought to target systems for delivering cognitive behavioral therapy (Fitzpatrick et al., 2017). Other studies have looked at the effect of suicide prevention counselor training (Gould et al., 2013), identifying patterns of successful crisis hotline counselors (Althoff et al., 2016), automating counselor evaluation (P´erez-Rosas et al., 2017), and building a dashboard"
W19-3001,P17-1006,0,0.0312592,"Missing"
W19-3001,E17-1106,0,0.0457236,"Missing"
W19-3001,D18-1004,0,0.0201076,"nt developments have sought to target systems for delivering cognitive behavioral therapy (Fitzpatrick et al., 2017). Other studies have looked at the effect of suicide prevention counselor training (Gould et al., 2013), identifying patterns of successful crisis hotline counselors (Althoff et al., 2016), automating counselor evaluation (P´erez-Rosas et al., 2017), and building a dashboard for crisis counselors (Dinakar et al., 2015). There is additional work to identify supportive and distressed behaviors and language in online forums (Balani and De Choudhury, 2015; De Choudhury and De, 2014; Wang and Jurgens, 2018) and support forum moderators (Hussain et al., 2015). Most similar to our study, was one study that showed the potential for an avatar system to help train medical doctors to deliver news to patients (Andrade et al., 2010). However, this study did not target counselors or train conversation strategies. To our knowledge, there has been no work on automating the individual seeking help to improve counselor training. 2.1 ci This is a safe place to talk. Tell me more about what is going on to make you mi feel sad and stressed RQ2 Can we extend retrieval baseline models to consider more than one tu"
W19-3001,N18-1202,0,0.0879501,"al scores (Krause et al., 2017; Ritter et al., 2011). Most similar to our work is a system that considered similarities of full histories of dialogues in addition to a previous turn of context (Banchs and Li, 2012) and another study that hand-tuned weights in a scoring function on IDF weights to include additional messages of context (Sordoni et al., 2015). However, these works used similarities calculated over TF-IDF (Baeza-Yates et al., 2011) and bag-of-words of representations, instead of more recent embedding methods (Bojanowski et al., 2016; Conneau et al., 2017; Pennington et al., 2014; Peters et al., 2018; Subramanian et al., 2018), which we explore. Related Work Considerable potential for automating a counselor was shown with the initial rule-based Eliza system (Weizenbaum, 1966) and recent developments have sought to target systems for delivering cognitive behavioral therapy (Fitzpatrick et al., 2017). Other studies have looked at the effect of suicide prevention counselor training (Gould et al., 2013), identifying patterns of successful crisis hotline counselors (Althoff et al., 2016), automating counselor evaluation (P´erez-Rosas et al., 2017), and building a dashboard for crisis counselor"
W19-3001,W18-5713,0,0.0197528,"ctice with during training. We found that retrieval methods became more competitive with improved embedding methods and surpassed generative methods when more context was considered. We also found that context had impact on how difficult it was for crowdworkers to evaluate responses. As a next step, we plan to explore better leveraging rich structure in the conversations, with a focus on the protocol that the counselors are trained to follow. There has been increased interest in blending retrieval and generation approaches by modifying prototypes retrieved from training data (Li et al., 2018; Weston et al., 2018). It is possible that such an approach would enable modifying and thus tailoring responses to similar contexts. The results we present are on a specific, datalimited setting, but the implications of our results may be broader both for other important applications, which commonly have data limitations, and for retrieval baselines that are used to assess generative models. As embeddings have improved, so too have retrieval baselines, which need to be updated for appropriate evaluation of generative models in any language generation setting. Our results are not without limitations. The data-limit"
W93-0106,J87-3001,0,0.012837,"names, assign more specific senses to broadly defined terms, and classify new words into existing categories. We also describe how to use these statistics to assign schema-like information to the categories and show how the new categories improve a text-labeling algorithm. In effect, we provide a mechanism for successfully combining a hand-built lexicon with knowledge-free, statistically-derived information. 1 Introduction Much effort is being applied to the creation of lexicons and the acquisition of semantic and syntactic a t t r i b u t e s of the lexical items t h a t comprise them, e.g, [1], [4],[7],[8], [11], [16], [18], [20]. However, a lexicon as given m a y not suit the requirements of a p a r t i c u l a r c o m p u t a t i o n a l task. Because lexicons are expensive to build, rather t h a n create new ones from scratch, it is preferable to a d j u s t existing ones to meet an a p p l i c a t i o n ' s needs. In this p a p e r we describe such an effort: we add associational information to a hierarchically structured lexicon in order to better serve a text labeling task. An a l g o r i t h m for p a r t i t i o n i n g a full-length expository text into a sequence of subto"
W93-0106,C90-3010,0,0.0128769,"s, assign more specific senses to broadly defined terms, and classify new words into existing categories. We also describe how to use these statistics to assign schema-like information to the categories and show how the new categories improve a text-labeling algorithm. In effect, we provide a mechanism for successfully combining a hand-built lexicon with knowledge-free, statistically-derived information. 1 Introduction Much effort is being applied to the creation of lexicons and the acquisition of semantic and syntactic a t t r i b u t e s of the lexical items t h a t comprise them, e.g, [1], [4],[7],[8], [11], [16], [18], [20]. However, a lexicon as given m a y not suit the requirements of a p a r t i c u l a r c o m p u t a t i o n a l task. Because lexicons are expensive to build, rather t h a n create new ones from scratch, it is preferable to a d j u s t existing ones to meet an a p p l i c a t i o n ' s needs. In this p a p e r we describe such an effort: we add associational information to a hierarchically structured lexicon in order to better serve a text labeling task. An a l g o r i t h m for p a r t i t i o n i n g a full-length expository text into a sequence of subtopieal"
W93-0106,C92-2082,1,0.0456068,"n more specific senses to broadly defined terms, and classify new words into existing categories. We also describe how to use these statistics to assign schema-like information to the categories and show how the new categories improve a text-labeling algorithm. In effect, we provide a mechanism for successfully combining a hand-built lexicon with knowledge-free, statistically-derived information. 1 Introduction Much effort is being applied to the creation of lexicons and the acquisition of semantic and syntactic a t t r i b u t e s of the lexical items t h a t comprise them, e.g, [1], [4],[7],[8], [11], [16], [18], [20]. However, a lexicon as given m a y not suit the requirements of a p a r t i c u l a r c o m p u t a t i o n a l task. Because lexicons are expensive to build, rather t h a n create new ones from scratch, it is preferable to a d j u s t existing ones to meet an a p p l i c a t i o n ' s needs. In this p a p e r we describe such an effort: we add associational information to a hierarchically structured lexicon in order to better serve a text labeling task. An a l g o r i t h m for p a r t i t i o n i n g a full-length expository text into a sequence of subtopieal discuss"
W93-0106,P86-1018,0,0.0194991,"e specific senses to broadly defined terms, and classify new words into existing categories. We also describe how to use these statistics to assign schema-like information to the categories and show how the new categories improve a text-labeling algorithm. In effect, we provide a mechanism for successfully combining a hand-built lexicon with knowledge-free, statistically-derived information. 1 Introduction Much effort is being applied to the creation of lexicons and the acquisition of semantic and syntactic a t t r i b u t e s of the lexical items t h a t comprise them, e.g, [1], [4],[7],[8], [11], [16], [18], [20]. However, a lexicon as given m a y not suit the requirements of a p a r t i c u l a r c o m p u t a t i o n a l task. Because lexicons are expensive to build, rather t h a n create new ones from scratch, it is preferable to a d j u s t existing ones to meet an a p p l i c a t i o n ' s needs. In this p a p e r we describe such an effort: we add associational information to a hierarchically structured lexicon in order to better serve a text labeling task. An a l g o r i t h m for p a r t i t i o n i n g a full-length expository text into a sequence of subtopieal discussions i"
W93-0106,P87-1024,0,0.0293212,"ific senses to broadly defined terms, and classify new words into existing categories. We also describe how to use these statistics to assign schema-like information to the categories and show how the new categories improve a text-labeling algorithm. In effect, we provide a mechanism for successfully combining a hand-built lexicon with knowledge-free, statistically-derived information. 1 Introduction Much effort is being applied to the creation of lexicons and the acquisition of semantic and syntactic a t t r i b u t e s of the lexical items t h a t comprise them, e.g, [1], [4],[7],[8], [11], [16], [18], [20]. However, a lexicon as given m a y not suit the requirements of a p a r t i c u l a r c o m p u t a t i o n a l task. Because lexicons are expensive to build, rather t h a n create new ones from scratch, it is preferable to a d j u s t existing ones to meet an a p p l i c a t i o n ' s needs. In this p a p e r we describe such an effort: we add associational information to a hierarchically structured lexicon in order to better serve a text labeling task. An a l g o r i t h m for p a r t i t i o n i n g a full-length expository text into a sequence of subtopieal discussions is desc"
W93-0106,P93-1034,0,0.0443883,"enses to broadly defined terms, and classify new words into existing categories. We also describe how to use these statistics to assign schema-like information to the categories and show how the new categories improve a text-labeling algorithm. In effect, we provide a mechanism for successfully combining a hand-built lexicon with knowledge-free, statistically-derived information. 1 Introduction Much effort is being applied to the creation of lexicons and the acquisition of semantic and syntactic a t t r i b u t e s of the lexical items t h a t comprise them, e.g, [1], [4],[7],[8], [11], [16], [18], [20]. However, a lexicon as given m a y not suit the requirements of a p a r t i c u l a r c o m p u t a t i o n a l task. Because lexicons are expensive to build, rather t h a n create new ones from scratch, it is preferable to a d j u s t existing ones to meet an a p p l i c a t i o n ' s needs. In this p a p e r we describe such an effort: we add associational information to a hierarchically structured lexicon in order to better serve a text labeling task. An a l g o r i t h m for p a r t i t i o n i n g a full-length expository text into a sequence of subtopieal discussions is described"
W93-0106,C92-2070,0,0.0600102,"t and how many unmarked children it had at any time, but each synset is assigned to only one category. The function &quot;mark&quot; places the synset and all its descendents that have not yet been entered into a category into a new category. Note that #descendents is recalculated in the third-to-last line in case any of the children of N have been entered into categories. In the end there may be isolated small pieces of hierarchy that aren't stored in any category, but this can be fixed by a cleanup pass, if desired. 3 A Topic Labeler We are using a version of the disambiguation algorithm described in [21] to assign topic labels to coherent passages of text. Yarowsky defines word senses as the categories listed for a word in Roger's Thesaurus (Fourth Edition), where a category is something like TOOLS/MACHINERY. For each category, the algorithm • Collects contexts that are representative of the category. • Identifies salient words in the collective contexts and determines the weight for each word. • Uses the resulting weights to predict the appropriate category for a word occurring in a novel context. The proper use of this algorithm is to choose among the categories to which a particular ambigu"
W93-0307,C90-3029,0,0.106126,"Missing"
W93-0307,J87-3005,0,0.149145,"Missing"
W93-0307,H93-1054,1,0.817587,"Missing"
W93-0307,P84-1054,0,0.0409969,"Missing"
W93-0307,P90-1004,0,0.0866302,"Missing"
W93-0307,J82-3004,0,0.214561,"Missing"
W93-0307,J93-2006,0,\N,Missing
W93-0307,P91-1030,0,\N,Missing
W93-0307,J92-4003,0,\N,Missing
